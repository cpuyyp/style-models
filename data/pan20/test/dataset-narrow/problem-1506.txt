This sounds like a challenge for trees with sufficient interaction depth, as you've found that the year interacts with other factors that results in improvements. Ordinary least squares regression here do not capture that type of interaction well. I would suggest setting up the following regression model: 

Decide on the KPI to be measured as the objective, is that revenue per customer? Profit per customer? Or total profit per store? Is the algorithm designed to increase per customer KPI or more customers? Check that the stores selected have some history to give assurance that they were similar in terms of the selected KPI before the implementation of the tests. Remember to remove outlier customers / transactions. Check that enough stores are selected in control vs. test buckets to detect a desired threshold increase that is statistically significant. 

The probability threshold can be changed and you would obtain different results for accuracy / recall / precision / F1 score (the objective). You could see this as an aspect of the parameters you have tuned to obtain the best objective. Similarly for CV, the out-of-training fold objective would be reported together with the other parameters, such as number of trees and predictor subset size. One reason I could think of regarding the instability of threshold across CV folds is that the CV folds were not split up using stratified sampling, resulting in different proportions of positives observed across the folds. 

Let's say we have a neural network with one input neuron and one output neuron. The training data $(x, f(x))$ is generated by a process $$f(x) = ax + \mathcal{N}(b, c)$$ with $a, b, c \in \mathbb{R}^+$, e.g. something like 

So there has only to be one way to place three points such that all possible class distributions among this point-placement can be classified the correct way. If you don't place the three points on a line, the perception gets it right. But there is no way to get the perception classify all possible class distributions of 4 points, no matter how you place the points Your example Your features are in $\mathbb{R}$. Every classifier has at least dimension 1. VC-Dimension 2: It can classify all four situations correctly. 

both of them might have the advantage, that the filter does not detect a border, where no border is. 

Question 2: Tagging texts This can be treated the same way like question 1. Question 3: Finding locations Download a database of countries / cities (e.g. maxmind) and just search for a match. 

Outliers In an earlier version of this question I wrote "outlier" when I meant "label noise". For outliers, there is: 

10/75 class A is mispredicted as B, 11/60 class B is mispredicted as A. Contrast this to The misclassification seen in C & D: 9 / 70 class C is mispredicted as D And only 3 / 90 class D is mispredicted as C. There is actually more cases of class D that is mispredicted as A than C. Hence we cannot argue that D is confused with class C. 

I would agree with the suggestion with holding out the 2016 data to check the external agency's work. Without inspecting the code, you just can't be sure that the k-fold cross-validation process has been performed properly. Another benefit of using the 2016 held out set is that you can see if the model trained on past years' data work well in future years data. There might be a concept drift in the new year as the true relationship between Y and Xs changes. In cross validation, each fold belongs to the same time period as the training data set. 

A correlation matrix would help discover any pair-wise correlation (between any 2 variables). I think that would be a great place to start, as it is just one line of code for initial analysis. After that, if you want to investigate "an increase in parameter 1 and parameter 2 is correlated with a fall in parameter 10", you could write code to form interaction variables like sum / product / divide and build a correlation matrix between the interaction variables vs. the original variables. Personally, I found the corrplot R package useful in visualizing the correlation matrix. Hope this helps. 

Classes which have less than 100 examples are pretty much useless. Lets remove them from the plot. Also increase the bin size to 25: 

Source: Wikipedia So both, clustering and association rule mining (ARM), are in the field of unsupervised machine learning. Clustering is about the data points, ARM is about finding relationships between the attributes of those datapoints. However, I wonder if there are more relationships. For example, given a clustering, can this enhance / simplify ARM or vice versa? 

No. Assume you initialize all weights with the same value. Then all gradients (in the same layer) will be the same. Always. Hence the network effectively only learns one parameter per layer. It is possible (and likely) that this is neither a gloabl nor a local minimum of the network (which has more parameters). Yes, as the learning rate is "small enough for all practical purposes". (No, if you use SGD or mini-batch gradient descent) Usure. I think the correct answer is "No, the network can make more mistakes in between with cross entropy.". It is certainly sure to improve CE loss while at the same time getting worse at accuracy (see proof below). However, I'm not sure if the gradient would ever lead to such a result. 

After the models are deployed in production, I'd monitor the following: (1) The same metric you used to evaluate the performance of your model, in some cases it is accuracy, or it could be precision, recall, RMSE. I'd plot a daily time series charting the metric and see that it is still performing above a satisfactory threshold. There might be seasonality within the calender, the model performs well around certain months and not so well in other months. I'd compare the performance against the test/validation sets of the same months to account for seasonality. (2) Apart from looking at the performance of the model, especially if one is working with shared computing resources, I'd also recommend keeping a close tab on the data aggregation runtime, model runtime, success rates of models runs over the past period. 

The confusion matrix suggests that you are performing classification rather than regression. RMSE and R-square are measures associated with continuous variables; For categorical variables, I'd suggest using Accuracy / Recall / Precision / F1 score to measure the performance of the model. $URL$ 

Transform the date column into (CurrentYear - YearOfDateStamp) AS NumOfYearsAway. I'd recommend leaving this as a numerical feature rather than a categorical one. This would allow the tree based model to select cuts like NumOfYearsAway >= 5.5 instead of NumOfYearsAway in (6,7,10). This could also be more helpful when NumOfYearsAway = 0 occurs in your scoring dataset, where you might not have training data for current year data set. Fit a tree based model, I'd pick XGBoost, with the usual CV to tune the hyperparameters such as interaction depth. 

Always on the same data, of course. For example, I think the linear SVM can find more decision boundaries than a perceptron due to slack variables. While the perceptron finds just an arbitrary hyperplane which separates the data linearly (if such a hyperplane exists), the linear SVM will always find the same hyperplane due to the optimality criterium. 

First, neural networks are good in dealing with"label noise". I'm currently on mobile/vacation, so remind me to search the paper on Friday. Second, the more important question is how to get a good ground truth. Without a good ground truth you can't evaluate your models, no matter how good they might be. I see the ways: (1) have multiple experts label the stuff. Then you can make the ground truth a probability, not a simple label. If 9 experts say it is cancer and 1 says it is not, you would label it with 90% (2) wait. If you can access the patients data, it will likely be more obvious in a year (especially if it was not treated) (3) other diagnostic methods: I'm not a medical doctor, but I'm pretty sure there are invasive methods to diagnose cancer which are reliable 

Instead of thinking it in terms of GBs, unless you're asking about the physical limits of JVM, my suggested approach towards this question would be to plot the cross-validated accuracy vs. the number of rows used in the modelling dataset through stratified sampling. You might observe a point of diminishing return where additional rows do not contribute towards higher accuracy. In addition, breadth-wise in terms of the number of variables used, you could plot a variable importance plot and plot the cross-validated accuracy vs. the top X important variables. In this case, you might even observe that accuracy increases when you use fewer variables as the noisy variables are discarded. You could then select the top X variables that give you the highest cross-validated accuracy. 

Couple of suggestions: Your training set is smaller than your test set? It should be the other way around. You should also tune the RF hypermeters using the held out set or Cross Validation. The two parameter people tune on RF are ntrees and mtry. 

A partial dependence plot might be what you are after: these plots show the relationship between the probability and the input variable. The mlr package in R takes care of this. Discussion of Partial Dependence on XGBoost Git: $URL$ General Tutorial using mlr: $URL$ 

Now we know that $\beta$ errors are 123.4 times as expensive as $\alpha$ errors. But we only have a given set of data. In this set we have $n_1=10000$ customers who paid back the credit and $n_2 = 100$ customers who didn't. How can the training of the decision tree be adjusted to account for the fact that $\beta$ errors are more expensive? (Note: This is a theoretical question to learn about decision trees. I know about other classifiers like neural networks and I know of ensembles. However, I only want to know about decision trees here.) 

The OPTICS clustering algorithm defines $$\text{core-dist}_{\varepsilon,MinPts}(p)=\begin{cases}\text{UNDEFINED} & \text{if } |N_\varepsilon(p)| < MinPts\\ MinPts\text{-th smallest distance to } N_\varepsilon(p) & \text{otherwise}\end{cases}$$ and $$\text{reachability-dist}_{\varepsilon,MinPts}(o,p) = \begin{cases}\text{UNDEFINED} & \text{if } |N_\varepsilon(p)| < MinPts\\ \max(\text{core-dist}_{\varepsilon,MinPts}(p), \text{dist}(p,o)) & \text{otherwise}\end{cases}$$ why isn't it simply $$\text{reachability-dist}_{\varepsilon,MinPts}(o,p) = \begin{cases}\text{UNDEFINED} & \text{if } |N_\varepsilon(p)| < MinPts\\ \text{dist}(p,o) & \text{otherwise}\end{cases}$$ 

I've posted this link on Reddit and got a lot of feedback. Some have posted their answers here, others didn't. This answer should sum the reddit post up. (I made it community wiki, so that I don't get points for it) 

I've understood that SVMs are binary, linear classifiers (without the kernel trick). They have training data $(x_i, y_i)$ where $x_i$ is a vector and $y_i \in \{-1, 1\}$ is the class. As they are binary, linear classifiers the task is to find a hyperplane which separates the data points with the label $-1$ from the data points with the label $+1$. Assume for now, that the data points are linearly separable and we don't need slack variables. Now I've read that the training problem is now the following optimization problem: 

What is the x and y axis of this scatter plot? If precision and recall are on these axes, then the range of both axes would be 0 - 1. I'm assuming one point would then represent one model, in this case you'd want to select the model on the top right, I.e. where precision and recall are both high. As for the concepts behind precision and recall, wiki has a good write up $URL$ 

The drawback of picking XGBoost in your application however, is that the interpretation of the impact of a particular variable on the target variable is not obvious. You'd need partial dependence plot to observe how the target variable vary with the bespoke input variable. If interpretability is very important, one could pick a single tree regression model like rpart. 

I would recommend against switch back as the purchase data is likely to exhibit seasonality (e.g. 2pm - 6pm / tuesdays exhibit lower volume, December exhibit higher volumes, company running advertisements during certain months etc.), which would become an unnecessary exogenous factor that complicates the post implementation analysis. I am inclined towards the causal impact approach. However, there are certain things that should be checked before selecting the stores as control vs. test (it is a kick-start list, please feel free to adapt to your business domain). 

I would like to see a simple example for this. Example Assume you have a fully connected network. It has only an input layer and and output layer. The input layer has 3 nodes, the output layer has 2 nodes. This network has $3 \cdot 2 = 6$ parameters. To make it even more concrete, lets say you have a ReLU activation function in the output layer and the weight matrix $$ \begin{align} W &= \begin{pmatrix} 0 & 1 & 1\\ 2 & 3 & 5\\ \end{pmatrix} \in \mathbb{R}^{2 \times 3}\\ b &= \begin{pmatrix}8\\ 13\end{pmatrix} \in \mathbb{R}^2 \end{align} $$ So the network is $f(x) = ReLU(W \cdot x + b)$ with $x \in \mathbb{R}^3$. How would the convolutional layer have to look like to be the same? What does LeCun mean with "full connection table"? I guess to get an equivalent CNN it would have to have exactly the same number of parameters. The MLP from above has $2 \cdot 3 + 2 = 8$ parameters. 

The output of the model is a probability that the given person is obese. The higher, the more likely. Now I know, if everything else stays the same, that a higher weight should always be the same or a higher output probability. But I don't know the exact relationship, only that it changes monothonically. How can I include this knowledge into the network and force it to show the desired behaviour? 

I think this is more of a finance question. Assuming portfolio here refers to an investment of two assets, then To determine the portfolio risk therefore, you'd need to either predict, or use historicals for $\sigma_1$, $\sigma_2$, $Cov_{1,2}$ Formula from: $URL$ 

To break the problem down, I think you are seeking to predict the capacity of the 5 suppliers ahead of time. So that you could allocate the order sizes for each supplier that is smaller than the capacity so that there is no need for reallocation. However, I doubt that you would be able to know what is the full capacity of each supplier, as you have mentioned, each supplier also have other clients so you don't know how much more could each supplier handle, on top of your previous order. As a (big) compromise, you could model as though each supplier is operating at 100% capacity when they serviced your past orders. With that, you have the series of Y to feed into your regression model. As for the inputs, you could add seasonality e.g. Time of year. I'm not sure as a outsider, you would have enough meaningful input variables to train the model. Maybe a naive approach of Mean(OrderSize) + 2StdDev(OrderSize) might work better. Hope this helps.