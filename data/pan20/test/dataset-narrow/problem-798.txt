It seems, there is also a way to install SQL*Loader with instant clients: $URL$ I'm quite sure it is not supported by Oracle and you need a client installation like DCookie explained. But it could be a solution when you need to install it on multiple servers. The alternative solution would be to make a installation like DCookie explained and create a rpm/deb from it and push it to multiple servers. 

i don't know the check point vpn, but in f-secure it was possible to configure the vpn profile to only route some defined networks/subnets over the vpn. 

we had the same problem: to much information for one image. what helped a lot are layers. you can put the items on different layers. we than added some buttons with vba scripts to make layers visible or hide them. this way you can put all information in one place but only show the information you currently need. 

Have you asked your provider which ports are allowed? Perhaps the provider has a central firewall which only allows some port like http or ftp for the vps. 

Call both commands with the same block size: (for both commmands). Perhaps they have different configurations. 

to get the broadcast correct, guessing is your correct broadcast address. but be aware that the ip address will vanish with the next reboot. you have to specify it in an interface file to make it permanent. in suse it is e.g. . there are normally also some gui tools to do the network configuration, in suse it is , in redhat it is . 

do you have a working backup? when yes, restore your bin folder. otherwise, look at an other box where you installed the same version of ubuntu and to what you find on the working installation. 

Take a look at the docs What's the Difference between Rewriting and Redirecting?. This will explain the difference whether the client will see a change in address bar or not. IMO you neither have to put or at the end of your rule. 

so change the value to a value matching your environment. but be sure to synchronize the value with the value or equivalent as explained in this documentation 

We're planning a forklift upgrade of our NAS and were going to use a utility like to move the files over. However we'd like to preserve Windows' "Previous Versions" shadow copies. Any way to do this? Will it happen automatically? I can't find any information about Shadow Copies with Robocopy one way or another. Edit with clarification: Both NAS devices are EMC, although we were not planning on using the EMC upgrade tool. (Maybe we should reconsider that decision, but that's another question.) The shares are CIFS/SMB and are accessed by Windows domain users. End users can access previous versions of files in Windows Explorer by selecting a file and going to "Properties" -> "Previous Versions". So somehow Windows is aware of the alternate versions. My understanding was that previous version of the file was stored in something like a NTFS resource stream, which it why I think of it as basically file metadata. But maybe that's not right, or not how EMC does it. I don't know what filesystem EMC uses under the SMB covers. But if Windows can access the old file versions over SMB, why can't Robocopy? 

Our company is trying to rethink our approach to managing permissions among employees for access to project files in our domain. We're considering creating a new AD Group for each office project and then adding users to the groups as employees work on projects. (Right now, user accounts are individually added or removed from relevant project folders by a script when they join or leave a project.) The concern is that we have ~300 new projects a year, so there would potentially be thousands of these groups. Also, users may work on many projects over the years, so each user would potentially be a member of hundreds of groups. Are either of those numbers a concern? We don't want to create a situation that causes the domain controller to struggle or push the limits of AD. 

I cannot for the life of me figure out how to create a LAG between these switches. The ports in question are all on the same subnet (marked blue in the diagram) so there is no VLAN tagging / trunking required, I think. I just want all the blue ports to behave as one switch. I also cannot figure out how and whether LACP comes into play. Can someone please either provide me with specific instructions or guide me to a specific tutorial about what settings I should be using? I prefer using the web admin GUI but I do have serial access to the devices if need be. Any advice or guidance would be much appreciated. 

Have you checked the order of your INPUT rules? Perhaps the traffic is allowed by your first INPUT rule. 

afaik, it is not possible with pure rpm. rpm provides this only for installed packages with option . when you can use yum, try to search in the repository. 

your files no longer belong to your account, because you copied them as root. this way root became the owner. to get back to sudo, mount your root partition from a rescue environment and edit . add your user account to the admin group and you should have sudo access again. after that you can change ownership of the copied files. do you have another ubuntu installation where to look for the correct ownership? 

Perhaps this nagios plugin (Jmx4perl) may help you. It has a java web application which has to be deployed in your WebLogic server. With this app you can access the JMX server of your WebLogic with http. Perhaps your WebLogic has this sort of application already deployed. JBoss has it: JmxConsole 

Are you sure the process is not a zombie? When it is a zombie, in most cases only a reboot will help. You can read more about zombies here. 

as long as you have swap space available, there is no need for the os to free swap space. It will be freed when there is no space left. Bu when you get in this situation you definitely have a problem. 

Only the lightweight server needs the certificate, because it is handling the ssl traffic and the client is only communication with this server. A ssl certificate is normally bound to the fqdn of the server, unless you have a wildcard certificate, like *.example.com. This said, you can use the same certificate for multiple servers on the same machine as long as they all have the name of the cn tag of the certificate. Otherwise you would get an error message in the browser/client, telling you server name and certifiacate name do not match. 

to disable reverse lookups, disbale HostnameLookup. when this does not help, be sure to not use hostnames in , , deny/allow rules and in . 

Our office uses a Windows Domain with Active Directory to manage user access to machines and network resources. The IT staff maintains a record of everyone user's password, which is used mainly for troubleshooting. E.g. sometimes problems appear only when logged in as a "regular" user, not an admin. Also, this lets IT admins configure software for local users, check settings, etc. Is it considered bad practice to keep this list of passwords? In theory, only administrators have access to it. Is there some way to use admin credentials to log in as a local user, which would obviate the need to store the user's password? (A little background: the office has about 30 users, with 2 IT admins. Some users have remote access via VPN.) 

We have a laptop with intermittent wifi connectivity problems. Rebooting often helps. Today while digging around I noticed that its DHCP IP address and gateway are not addresses on our subnet. The DCHP lease it's using hasn't expired but based on the timestamps it was probably obtained while on a different network with a different SSID. For some reason the laptop is trying to use that DHCP lease on our network but has no connectivity because the gateway doesn't match. I suspect if I do a it will get a new IP address correctly, but I want to know why it's trying to use a lease from a different DHCP server. Is this a misconfiguration on the laptop somehow? Or a problem with our office DHCP server? Edit with more info: Our company wifi network has a non-generic SSID and a subnet of . The DHCP and gateway are both on that subnet. Wifi access points are set to "bridge" mode (our Windows domain controllers do DHCP for the LAN including Wifi). The user of the laptop came in today saying he had been using the laptop at a client's office that morning (don't know the SSID but I can pretty well guarantee it wasn't the same as ours). The laptop connects to our wifi network but gives a "no connectivity" error and has no access. reveals that the wireless card has a DHCP-assigned IP address of and a gateway of . Not our network! The lease was issued that morning, presumably by the client's network, and is still valid. But obviously it's the wrong network. Edit 2: The Plot Thickens We disconnected from the company wifi and connected to a different one, and the laptop works. It's given an IP address of (different network, different DHCP range). Wifi works fine. When we bring it back onto the company network it doesn't work again, but this time shows that it's holding onto the IP address!! So something about that laptop or our DHCP server is causing the laptop to just use a random old DHCP lease when connecting. 

Is there a way to migrate from an old dedicated server to a new one without losing any data in-between - and with no downtime? In the past, I've had to lose MySQL data between the time when the new server goes up (i.e., all files transferred, system up and ready), and when I take the old server down (data still transferred to old until new one takes over). There is also a short period where both are down for DNS, etc., to refresh. Is there a way for MySQL/root to easily transfer all data that was updated/inserted between a certain time frame? 

Migrated from Server A to Server B from different providers, with different IP's for hostname, nameservers, etc. I've updated the nameservers for a number of domains, and they seem to be propagating, resolving, etc... for a short period of time, and then later, it looks like some of the domains have "jumped" back to the old nameservers... Any idea what might be causing this? Solutions? 

I am trying to migrate from my old server (Server 1) from provider 1 to a new server (Server B) at provider 2, keeping the process as seamless as possible. One of the first things I noticed in the test folder I migrated is that several PHP functions are not supported with Server 2 -- apache_request_headers(), for example. This is supposedly because PHP was not compiled as an Apache module on Server 2. There might be other differences that may cause fatal script errors, that I haven't yet found. Both servers run CentOS with WHM. Is there a way to configure the new server to be exactly the same as the old, without this ad hoc checking? 

PHP, WHM, and several other services are already installed on a CentOS x64 server I am trying to migrate data to. Many of my existing PHP scripts are dependent on PHP's apache_request_headers() function, which the current server's PHP configuration does not support. Apparently, compiling PHP as an Apache module is one solution, but are there other ways to enable this (without uninstalling PHP, reinstalling, etc., and all dependent services), perhaps as easy as modifying php.ini, somehow? 

This will instruct to add the service to the runlevels 3 and 5, with a start position of 90 and a kill position of 10. 

when you have different clients you can put multiple first nodes in your openldap, but when the different clients are only different organisational units you should create them as subnodes of one organisation. do you have an example what types of first level nodes you want to create? perhaps this way you get some more precise answers. 

because the number of connections is ip bound in most browsers, you can spread your content across multiple ips/domains to enhance performance. you can find a more detailed explanation at yahoo. there are also some more performance tips for your frontend performance. a good tool for messauring frontend performance in firefox is yslow. it does not only messaure performance, but also gives hints about how to improve your performance. 

you could use checksums. but the safest way is to restore from your backup. you do not have to test every backup, but once in a wile you should restore a backup on a different machine. this way you will know for sure whether your backup works or not. 

this way all request to will be forwarded to your first jboss instance, and will be forwarded to your second jboss instance, and so on. 

Some application servers already have a JMX proxy. In JBoss you can access it with this URL: $URL$ For more details see this link. When your application server does not have such a http based way to access JMX, you could create it on your own. Perhaps you can use this nagios plugin (Jmx4perl) as a starting point. It uses a small java web application, which has to be deployed. After that you cann access the JMX server with http. It also has a special proxy mode, with a dedicated proxy server. 

your scheduling is right. i'm not sure why you can't edit the crontab. do you get any error message? 

i haven't tested it by myself, but this sounds promising: $URL$ i will try this for myself. EDIT: finally i had time to test for myself. i tested it on suse, but should be the same for ubuntu, except perhaps package and path names. first install xvfb package, on suse it is called start xvfb in background with display, screen and resolution parameters: