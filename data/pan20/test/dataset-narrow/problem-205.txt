to generate your static random generator start point the $usersrandomnumber acts as a starting seed see the mysql documentation here: $URL$ it will involve you storing the user's seed in a table somewhere, or cookie /local/server storage but will get you what you're after 

TLDR; Check for conversations being left open completely. In our system we re-use conversations and have a table dedicated to holding these conversations that are usable, however the dev team setup a new service broker without my knowledge ages ago while I was off, didn't set up these conversation points and didn't set any thresholds on the alerting. When the new system has been turned on the conversations are being opened but not closed properly and since there aren't any in the pool it is just creating a new conversation (we reached 7.1 million conversations for one service broker) My steps for fixing was to create and record the 20 conversation handlers that I require for that service broker and record them into our table. This stopped the growth of the tempDB to stop the risk of the DB going down. Then came the long process of closing off all the un-used conversations 

I think it's obvious what it does but to make it clear let me give a brief explanation. In my table there's DOCNUMBER column. I assign a value to this column in Before Update trigger. And in the above trigger, which of course should fire after the Before Update Trigger, I check if the DOCNUMBER has value (and if it's for the first time) and if it does, I write that value to another table called . The problem is, the above trigger sometimes, although very rarely, fails to insert the to the table. That is, sometimes I see a record in table with DOCNUMBER value that does not exist in table.The table has only one column called DOCID and it's part of a UNIQUE KEY constrsraint. Do you see any problem with this trigger? EDIT: I think you'll get a better idea if I post the BEFORE TRIGGER too: 

EXP(SUM(LN(YR.RATE))) is just a trick to get the multiplication of the column values as there's no ready function available in Oracle. 

Note, are you wanting those default values in there? would make more sense (in my opinion) to have it as a required field on the execution of the proc so you don't accidentally run it with no values and presume its right 

User 2 can see the entry created by user 1 Now user 1 disconnects or runs a forced rollback their entries to the table are removed so if user 2 re-runs their query the data is no longer there 

You could try putting the remoteData into a temp table before doing the merge. a lot of your time will be taken up with comparisons of data not the actual retrieval so if you run the 

I'm putting down my recommendations for your two questions and additional considerations for your index. 1 I'd keep the timestamp as one field, breaking it into two separate fields for date and a smallint for the hour will reduce the space by 2 bytes per row (aka 43 MB with your 22k row estimation) having to compare two fields for date /time doesn't seem worth the small space gain. 2 22k rows may not be many in comparison to some databases, if you're picking out individual occurrences though then you still want it as fast as possible. finding a specific value from a heap requires a full table scan which can take time, if it's indexed then you do an index seek which means you can find records a lot faster. Additional If the data is broken up to different rows by location (eg, '2015/10/13 00:00', 'London', 'sunny') then I'd advise a composite key between the timestamp and location I don't believe that is the case, it sounds like the data will be stored in one long line, so (datetime timestamp, London (data), Paris(data)), for this just index the timestamp field, as an index would suffice Hope that helps 

Now, there might be multiple entries for one customer_id value in the table B. I need the latest date to be updated into the table A. I wonder, if I order the result by last_login_date column in Ascending order, will the merge statement eventually update the record in A with the latest last_login_date? 

I have two tables, namely Employees and Payments. Below is the contents of the two: The Employees table 

The title was best I could find to explain my question, I don't think it helps though. Anyways, I have two tables: 

After the process is over, I run queries against all the tables. Everything goes fine with 9 of them, but with one of the tables the executed query hangs. The next day, when we run the same query against the same table, the execution takes only a moment. I have no idea what the problem is. I tried not adding indexes but it didn't help. I know he way we copy table content might seem weird. 

Is there any way to free memory without restarting SQL? Could it be the issue with memory leak as described in: Memory leak occurs... ? 

We need to move SQL Server instalation from physical cluster (SQL Server 2005 SP3 ans 2008 R2) to virtual non-cluster enviroment. As this is very old instalation (several databases with unknown purposes) the plan is to create enviroment as similar as possible to the old one (same SQL versions, components etc.), move user databases and in some point turn off old cluster and change IP addresses/names on the new machine to match the old one. The problem we have are system databases. I believe we could transfer (backup/restore or attach/detach) msdb database (I did it few times in the past) to transfer all jobs. But what about master database? Is the same approach has even chance to work? In the past all I done was just transfering logins but this time there are many user defined objects in master database. Also we are affraid that moving master database from cluster to non-cluster enviroment would cause problems in the future. 

I'm not sure what your dynamic columns are so I cant 100% promise this will work, but if you get any column errors you should just be able to put ts. infront of it to declare its from that table Ste 

NOTE: I've put section Y and Z as your additional sections as they don't appear in your example with what they are If that doesn't work directly here's some external links with more detailed examples: 

I may be understanding this slightly wrong so I apologise if I am. If you have two databases that you need to be identical and are on 2014 then use the AlwaysOn High Availability Group. Since you're data centres are at separate locations use the Async mode This will mean the database is kept completely up to date (all be it possibly with a few second delay) and you can have the secondary node as a read-only replica, this means that your alarm system can read into that database run all the checks etc you would normally. the Always on system keeps everything up to date, so if the connection drops, when it comes back online it will merge over all changes It also means that if your main centre goes down you can set it to automatically failover to the secondary, when the main datacentre comes back online it will re-sync with the (now) primary node, at which point you can fail it back over to your main centre. You can run this on multiple databases, so we have our main DB and our Admin DB synced across our nodes, however what runs all the jobs and direct actions on each side is not replicated so can stay independent of each other 

I know that there's a one to one relationship between the document types and Instructions, so according to database normalization I should not have a separate table for it. But on the other hand I'll have to include the same columns in two different tables. What do you think? P.S. I won't be able to create a foreign key constraint between Instructions table and the two other. 

But this way, I'll end up first fetching all the employees whose positions are 1 or 3 and then extract the 10 rows among them. I find this a bit inefficient. Is there any way to achieve this without making full scan first? 

I have two schemas namely A and B. I regularly copy contents of 10 tables from A to B. Here's is how I do it 

I've been asked this question but neither I seem to be able to answer it on my own, nor can I find anything related on the web. So what are the cases that might cause a runtime exception when committing a transaction in Oracle?The only thing that I can think of is the low disk space. Are there any other? 

The problem you have is that the where clause is executed before the Select clause of a query, so when it hits the where section it has no concept of what the muni_score is There are two simple workarounds (not taking any performance into account here) and I'll note down a slightly more complex one. The easiest is to simply replace the muni_score in the where clause with what makes is up, so: 

(Or create a procedure to run that) NOTE: this will NOT work for any sysadmin account as their current_user is always 'dbo' EDIT: Warning, view server state will grant permission to view anyone who is connected, not just people with the current user name so you may want to check around what potential additional security you wish to put in place for that if need be 

you cant have multiple databases 'in use' at one time, as there are a lot of things which change between databases even within the same instance. You can access databases outside of the currently accessed database like so: