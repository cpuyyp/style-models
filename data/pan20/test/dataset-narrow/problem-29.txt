The common way to render transparent polygons in a rasterizer is by use of Alpha Blending, which basically combines the colour of the supposedly transparent pixel with the colour of the background at that pixel's position, this way rendering the pixel transparently onto what was already rendered before. This technique, however, requires your polygons (or at least the transparent ones) to be sorted from back to front, at least in general non-trivial cases, since you can only combine the transparent colour with what has already been rendered behind it. But since the Painter's Algorithm already requires ploygons to be sorted from back to front (which is pretty much the entire essence of the algorithm), this just fits naturally into the rest of the algorithm. Thus, supporting transparency becomes rather straight forward by simply doing any variant of alpha blending of the pixel with the background instead of overwriting the background. 

I think I found your misunderstanding, but it's IMHO based on a little inconsistency (or at least lack of clarity) in the book. 

Of course the answer isn't complete without some general advise, that you should really make sure you actually need double precision vertex attributes, or even double precision computation. Even on modern hardware that's still significantly slower and not what those GPUs are really made for, not to speak of the doubled memory consumption for the vertex data. More precision is not always better if it isn't actually necessary and you might be surprised what a lousy little is able to accomplish when used well. 

In unextended OpenGL that's indeed a little difficult, if possible at all. There is really no actual notion of "GLSL assembly code", or not one that's remotely vendor-independent, let alone a way to retrieve that. There are a few extensions that might help you get that information, though, albeit in a not entirely intuitive or even platform-dependent way (but well, you're talking about micro-optimizations, so we're firmly in platform-dependent territory anyway). First of all, there's GL_ARB_get_program_binary (core since 4.1), which let's you retrieve a platform-dependent binary version of the program (after linking). At least on NVidia hardware (maybe others, too, but I wouldn't know) this binary blob also contains ASCII assembly listings of all the contained shaders in NVidia-extended ARB syntax (continuously refined in the extensions). Those listings are quite easy to find in the binary when opening it with a normal text editor and the syntax is quite straight forward if you know your way around shaders and general assembly programming principles. As said, other vendors might offer similar listings inside their binaries. Then there's also external programs that might compile your shader into some intermediate form, the most platform-independent of those being SPIR-V, for which Khronos-approved external compilers exist. Being an intermediate language this is somewhat of a "high-level assembly language" with an "assembly-like" structure but with many high-level abstractions that builds a bit of a compromise between the easy interpretability of assembly and the easy programability and platform-independence of a high-level language (think of Java bytecode). With the GL_ARB_spirv extension (core since 4.6) you can directly use (externally precompiled) SPIR-V programs in OpenGL in place of GLSL shaders. I don't know if there is currently a way to let your driver compile GLSL into SPIR-V and then retrieve that. Otherwise some external SPIR-V module created from a reference compiler would not necessarily be equal to the machine code that your driver's actual GLSL compiler produces. It might also be that such an intermediate representation might already be too high-level for analysing micro-optimizations anyway. All in all, and especially if you just want to use it for debugging/analyzing purposes on a dedicated machine, trying to look for a whatever-natured assembly listing in your driver's binary blob retrieved with might be the easiest and most "close-to-the metal" way to get some insight into the compiled product. I use it occasionally just out of interest. But of course no answer to this question would be complete without a warning that you shouldn't make too much out of it either. As on the CPU, generally trusting your compiler is a good default approach and you shouldn't let micro-optimizations like this guide your practice too much, let alone rely on any of that guidance being remotely portable (let alone assume that the assembly listed in the binary is actually what's uses as machine code). But if you want to know what your driver might make of it, it can be quite interesting and helpful, especially when deciding between two equally meaningful ways to write some function that might have unexpected differences in the "assembly". 

Those are just a few thoughts that I happened to ponder on just recently and deemed worth sharing. As said, if they are appropriate for your use-case has to be evaluated, possibly with some timing and quality measurements. 

So what you do depends on the space in which your light direction is stored in the scene. If it is stored in world space (e.g. the sun as the directional light example par-excellence), you just have to transform it into view space (e.g. by multiplying it with the camera/view matrix) right before uploading it into the shader (i.e. likely each frame, since it's view dependent). If it is actually view independent (e.g. a kind of "flashlight" always shining straight into the camera direction), you don't really have to transform it at all. And if it's local to some specific scene object (more appropriate for point light sources, though), you transform it by that object's modelview matrix. 

First disregard the sparseness and tiling and basically just store a full boolean grid where each pixel just stores if it is green or not, could basically just be a 0/1 uchar per pixel. Fortunately, since it's a post-processing step and you're not e.g. working on rendered geometry directly, you know that you don't need more storage for possible fragments than the image resolution. Then perform a so-called stream compaction step, which is a common step in GPU computing where you basically compact the full array of flags into a tight array of indices (i.e. coordinates) of the actually set pixels. This is usually done by two sub-steps: 

But of course all this also depends on how you actually want to proceed with processing your found per-tile generated green pixels. This way you still end up with a varying number of pixels in each tile, but at least they're laid out linearly in memory and you alleviated the impact of that "dynamicality" on the generation of those per-tile lists, however you go about processing them now. But maybe all you want is just the sum of all green pixels or some avergae weight of their coordinates? An option you might also consider is simply ignoring all that and just restructuring the next processing step in a way that works on a sparse boolean array. This gives you a fixed number of pixels but a scattered layout of the relevant data. As said, heavily depends on how you proceed with the data. 

Of course it still has to be evaluated if this is a better solution, since texture access isn't entirely free either, as well as what the best combination of texture size and format would be. As to filling the texture with data and adressing one of your comments, you have to consider that texture filtering returns the exact value at the texel center, i.e. a texture coordinate off by half the texel size. So yes, you should generate values at texels, i.e. something like this in application code: 

That's just a result from simplifying the calculation based on the fact that is nothing else than . Because the depth range in NDC space is $[-1,1]$, which has a width of $2$. It doesn't. These values have nothing to do with the near and far values you used to construct your projection matrix, in fact you could use any arbitrary projection matrix (or none at all). They are the depth range values set with , which have a totally different meaning (okay, let's say slightly different but related). Basically, and special situations notwithstanding, the projection matrix defines where you near and far clipping planes are in eye-space, i.e. which eye-space range is mapped to the $[-1,1]$ NDC depth range. Then the depth range defines what these NDC near/far values of -1 and 1 are mapped to in the actual depth buffer. So the names "near"/"far" do relate to the near and far clipping planes. But both transformations are basically independent of each other, using the general NDC (i.e. the normalized device space) as a mediator between both stages. 

You're basically mixing up your spaces. If you look at the documentation for , you see that it actually expects coordinates in window space. And this makes sense, since you explicitly query the viewport (i.e. basically the window's pixel dimensions) and give that to the function. But with your ratio computation above you already transformed your coordinates from that space into normalized device space, i.e. the $[-1,1]$ square. So this whole computation isn't necessary at all and is already done for you by . Likewise are the near and far distances from world/eye space not the same in window space, which uses the normalized $[0,1]$ depth range. If we assume that your viewport is your whole window, then all you need to do is put those pixel coordinates you clicked on into (still accounting for the different Y-convention, though): 

This isn't necessarily due to the orthographic projection per se, but rather the classic two-sided lighting problem together with a little optimization that the old OpenGL did. When the eye space origin is inside the object, you are basically looking at the object from the inside and it's only the peculiarities of orthographic projection and the near/far planes you chose that makes the outsides appear. It depends on how you actually implement your shader. I assume though, when you compute your , you just use the vector from the eye space vertex to the origin (or rather the negative normalized eye space vertex position). You are basically trying to light the backfaces of the sphere and if you haven't taken account for that, the angle between the view vector and the normal will thus be larger than 90°, usually resulting in a negative or 0-clamped specular term (or whatever other unexpected results). A common way to accomodate for that would be to negate the normal vector whenever it is facing away from the camera, or maybe the light, or when dealing with backfacing fragments,... But this depends on your specific use-case. Now, why doesn't this happen in old deprecated OpenGL when using orthographic projection? For this we have to look at the specificities of how it did its lighting computations. There was a function which controlled some of the global properties of lighting computations. Specifically, the paramter . This controls an optimization where the specular computation does not use the actual vector from the vertex to the camera, but always just uses the z-axis instead. i.e. as you said it assumes an infinitely far away viewer along the z-axis. This is the reason why even in perspective projection the specular highlight on an object is always at the same position, even if you move it around in the view. If you change this setting from the default of to , it should perform an actual view vector computation and you should get the same results as with your own shader. There was also another option that controls a property related to your general issue, too. enables two-sided lighting, which means that back-facing polygons will be lit like their front faces (however, possibly using different material settings) by using the inverted normal. If you activate this, this still won't change anything in your orthographic scene, since you are seeing the front faces of the sphere here. But once you change to perspective projection, the lighting should be black, but this time because you're seeing the backfaces, which you can enable lighting for with this option.