You've asked a question which often results in real projects (and I freely admit to being a consultant in this area). I'm just going to give you some things to think about: 

The answer is "yes". You can do this with a filtered index (see here for documentation). For instance, you can do: 

Databases often run out of space when they are in full recovery mode. If you are not doing transactions and manual backups are sufficient, then you can change the recovery mode to simple. You might also need to recover space from the log file . . . it can be emptied but still use space. However, you are better off asking this question to DBAs who may offer more solutions. 

I am going to intelligently guess (and in the process probably attract a SQL Server guru who might give a really detailed answer). The first query approaches the execution as: 

It chooses this path because you have a clause on the primary key. It never gets to the second step, so the query does not fail. The second doesn't have a primary key to run on, so it approaches the query as: 

You are doing only equality comparisons in the and then sorting by . A single index on: should allow the entire query to be satisfied only by the index -- first finding the right rows, then sorting by them, and then fetching the id. 

This has generally fixed this problem for me in the past. What may be happening is that subtle changes to the table (or to the availability of temporary space) cause the SQL optimize to prefer a slower join algorithm. This can be quite subtle and quite sudden. When you create a temporary table, the optimizer has more information about the table (such as its size), so it can generate a better plan. 

Could return two rows in any order (although, in practice on any database I know of, 'A' will come before 'B'). In SQL Server, this turns into an execution plan using a "concatenation" physical operation. I could easily imagine that the concatenation operation would scan its inputs, returning whatever input has records available. However, I found the following statement on the web (here): 

The idea is to pair each row in with the matching row in (if any). Note that the matching row could be above or below, by the logic you give. That is why the last two conditions are bi-directional. 

First, get rid of the subquery. It is not needed and it can interfere with optimization. Second, you don't need a , because the clause is requiring matches. 

Your second method is the right way to represent the data in the database. If you want a user/date pair to appear only once, then create a unique index/constraint to enforce this: 

Question: Is this true in practice? Is this guaranteed to be true? I haven't found any reference in Microsoft documentation that the inputs are scanned in order, from the first to the last. On the other hand, whenever I try running it, the results suggest that the inputs are, indeed, processed in order. Is there a way to have the engine process more than one input at a time? My tests (using much more complicated expressions than constants) are on a parallel-enabled 8-core machine, and most queries do take advantage of the parallelism. 

Partitioning could quite possibly help you in this situation. By putting the data into a partition, the database will have a smaller table for each client to deal with. This could help performance a lot. Here is an example of how it would help. The data in the table would otherwise be interleaved, so different clients would have rows on a data page. Reading the data from a single client would then "clutter up" the available page cache with records from other clients. However, it will not help very much if all the clients are accessing data concurrently, so all are competing for available memory. Partitioning probably will not make this worse, but it won't necessarily help. Also, MySQL automatically partitions the indexes on the table, which is generally a good thing. Often partitioning is applied to a date field, so older data gets put into less used partitions. This may be a case where another field is useful. But, if all the clients need all their data at the same time, then the partitioning may not give you much of a gain. 

Where is the stored procedure call. The goal is to get the return value from the stored procedure. If this is possible without an (or, more specifically, without starting a transaction), that would be great. I cannot modify the stored procedures in general to store the value in a table, because there are too many of them. One of them is failing, and I can modify that. My current best solution is something like: 

This is a bane of running complex queries in SQL Server. Fortunately, it doesn't happen that often. Look at the query plan for the query (when it is running slow). I am guessing you will find a nested loop join occurring one or more times on tables with no indexes for the join. This really slows things down. To fast forward, the way to fix this is with a hint. Add the following at the end of the query: 

This is perhaps the worst error message I've encountered. It seems to really mean "You did not handle an error in a nested transaction." If I put in the , then I get the message: 

How can I handle exceptions generated in the stored procedure and still continue processing? The following code illustrates the problem. What I want to do is return 0 or -1 depending on the success or failure of the internal call: 

Tables in SQL have a pre-defined structure. Database systems (for better or worse) are not designed to hold arbitrary numbers of columns. In cases where this is a requirement (and your problem is not one of these cases), then a JSON representation (or something similar) can be used. 

The issue isn't the NULL values. It is the selectivity of the index. In your example, the selectivity of is better than the selectivity of just . It covers more of the conditions in the clause, so it is more likely to reduce page hits. You may think that reducing the number of rows by 50% is enough, but it really isn't. The benefit of indexes in a clause is to reduce the number of pages being read. If a page has, on average, at least one record with a non-NULL value, then there is no gain to using the index. And, if there are 10 records per page, then almost every page will have one of those records. You might try an index on . The optimizer should pick that one up. In the end, though, if the clause is keeping lost of records -- there is not rule but let's say 20% -- then the index probably won't help. One exception would be when the index contains all the columns needed by the query. Then it can satisfy the query without bringing in the data page for each record. And, if an index gets used and the selectivity is high, then performance with the index could be worse than performance without it. 

The first is a covering index for . The index should be used for the query with no need for the data tables. The second is a covering index for . The first three will satisfy the clause. The four column will be used for the and the final is needed for the . 

The solution to your problem is a MySQL capability called "partitioning". The documentation is here. What partitioning does is store a single table in separate "partitions". These are defined by a particular expression, usually a column value or range. In your case, this would probably be based on -- assuming that it is known when a record is created and it doesn't change. You would store a day's worth of in each partition. Then the deletion step would be truncating a partition rather than deleting a bunch of rows in a big table. The partition truncation would be a much faster method. 

Your statement includes . This is an aggregation function. Because there is no , this means that all the rows are treated as one group, and hence one row is returned. Perhaps you mean: 

For 10 values where you have no index, this is such a micro-optimization that you shouldn't worry about it. The database has to read all the rows, and those reads are going to be less efficient that doing a few comparison operations. The first can more efficiently be written as: 

You then repeat rows for each statistic. This is usually less efficient than the single-table approach. But it can be beneficial, particularly when you have lots of statistics, and most systems have different sets of statistics. 

This is only a partial answer to your question, because you don't provide enough information on how the database will be used. I also think this question may be more appropriate on the database administrators site. For instance, if the data is loaded nightly and the database is only used during the day for querying, then you want the fill factor to be 100%. That will be the most efficient approach for querying, minimizing the number of pages. Page splitting in indexes occurs when a new record lands on a page that is full. The page is split into two pages, which are then, roughly, 50% full. Once the page is split, there is more space for those values. If the data is being updated with multiple updates or inserts per second, then page splitting is unnecessary overhead. However, on indexes, this will probably happen the first time a new record lands on an index page. Note that page splitting is going to happen anyway, once pages are filled. This would happen anyway, regardless of the fill factor. The major question is "start-up" time, because initially basically all inserts/updates will result in page splits. But this should quickly dampen. The overhead also depends on the characteristics of the implementation. If the indexes fit into memory, or if your disk system is solid-state disk, then the overhead might not be noticeable at all. My guess is that this is not the most important problem, unless consistency of performance when the new database first goes online is paramount. 

If the statistics maintained for each system were very different from other systems, then it would be more efficient to store only the appropriate columns. If system requirements mandated data separation or permissioning. The latter is easier to do on the table level. 

Of the two options mentioned, without any doubt, the first is better. MySQL is fine at handling large tables. There is no reason whatsoever to break the data into separate "equivalent" tables. In fact, having multiple tables with the same layout is usually an indication of a poor database design. For performance, you can then add indexes on the system and date (or both in one index). You can also learn about performance. There are some reasons why you would split the data into separate tables per system. Here are two: 

This should have the same efficiency as the version for most database engines. The better question is which way of describing these values better matches the business problem? You want the code to be easily understood and maintainable. Use a list if that makes sense. Use a range if that makes sense. If you had hundreds of values, then definitely go with the second approach. It is cleaner and easier to understand and write and should be faster. How you write the query does affect optimization, and that in turn depends on the database engine. Some databases are smarter about optimization than others. Most of the effect would be on the use of indexes and the effect that other filters in the clause have on index usage. 

If is a simple junction table that is never referenced, then you can make the two columns a primary key. This solves the problem of creating a unique index on the two columns. If other tables could be referring to a row in , then don't use a composite primary key. Composite primary keys just complicate foreign key references, SQL queries, and can sometimes become unmaintainable. I strongly prefer auto-incremented or as single-column primary keys in tables. These are efficient for indexes. They are easy to reference as foreign keys. They provide information about the ordering of rows. For a clustered index, they are easy, because they always go on the last page. The latter is a slight downside compared to GUIDs, because they have slightly more information about each entity. Admittedly, in a simple junction table, they are also redundant. Note that in MySQL the primary key is automatically the clustered index. Hence, you do not need an additional index on the first key in the primary key. In your example, is redundant, unless you really care about the performance difference between a B-tree and a hash table. 

My problem is the . The success path is fine. If I leave out the try/catch block in the stored procedure, then the error is raised and the insert fails. However, what I want to do is to handle the error and return a nice value. The code as is returns the message: