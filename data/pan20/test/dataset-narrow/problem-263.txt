I have about 26 applications interacting with our database, some of these will store email addresses in mixed case. Since I made the mistake of using varchar as the column type and not a case insensitive one, I would like to make some kind of trigger or rule, which will convert all attempts to save email addresses as mixed case, into lower case. I could achieve the same using the following query: 

I am considering a model where I use PostgreSQL COPY to copy data from a file into a table. I was wondering what kind of performance to expect on high-end hardware. An interval in MB/s would be nice so I can start estimating. If you just know the answer on other databases, I would be interested to compare. 

But that would throw error messages in all the applications, and potentially stopping new customers from registering. Therefore, I am looking for this auto-converting rule/trigger which will handle any updates or inserts into this table. Is it possible to do this at all, so it won't come back and bite me later? 

What this achieves is that I could now modify the new "matches" table and for example add a column or rows, which do not need to get presented in the "reconciliations" view (by adding a where clause to filter them out). I am on Postgres 9.5 so the view is automatically updatable. Initial tests shows that there are no immediate problems with this, so I am asking this question to know what kind of problems I should be looking for. Performance is not a big issue. 

We are creating SAAS where we will at most have 50.000 customers. We are considering creating a user in the Postgres database for each customer. We will map each user that logs into our service to a user in the database in order to be very sure that they only have access to their own data. We also want to implement an audit trail directly in the database by this solutions, which utilizes triggers. If each customer has its own database user, then it would be very easy to see who did what, even if two customers would share the same data. Will we be running into some unexpected problems because we have 50.000 users in our database? Performance-wise or administration-wise. Maybe connection pooling would be more difficult, but I do not really know whether we would need it. 

This is to ensure the distribution statistics of your table are up-to-date and thus help the query optimiser. Make sure you have physical resources for such a command to run, it will hit CPU and disk quite hard Is there some combination of fields in your table that are unique? If so then this will cause the InnoDB engine to use it to create the clustered index for the table. Clustered indexes are very fast when it comes to range scans which will play to your advantage if one of those fields that make up uniqueness is timestamp. A table can only ever have one clustered index. In MySQL the precedence for the DB Engine creating a clustered index is as follows:- 

The technical limit on the number of columns depends on the engine. InnoDB allows 1017 in MySQL5.7 Those 1017 columns can cover a maximum of 65535 bytes. Records are stored on "pages". InnoDB allows the page size to be configured to 4, 8, 16, 32, 64Kb. Your record must fit on a page so you can't stick a 5K record on a 4K page. The problems with having wide records is that when the DB engine retrieves records it does so in pages. You can get few wide records on a page so retrieval performance decreases. DBs pull the results through memory so subsequent retrievals will see if the data remains in memory before falling back to storage. Having many records on a page means that the first physical retrieval of a page is more likely to load into memory records which can be logically (and much faster) read from memory. From a design perspective it depends on what your use case is. In an OLTP system then I would feel uncomfortable with 450+ columns. A database is not a dumb store. It can be used to enforce rules on the structure of information and the relationships between different data entities. This is an incredibly powerful weapon to have in your arsenal. In a data warehouse supporting certain analytical systems 450+ sounds like a lot however I have seen some wide denormalised tables used to feed OLAP cube technologies. If I saw a 450+ column table I should also ask questions about security. When I grant access to that table do I want everyone with access to have access to all 450+ columns? In addition to storage efficiency/performance normalisation can also factor in a security design. Consider performance. Of those 450+ columns which ones get retrieved the majority of the time? Do you really want to have the expense of retrieving 450+ columns if only 32 are used on a regular basis? The answer I have given assumes that InnoDB (the default) is used. 

This fails with the error: ERROR: syntax error at or near "RETURNING" What I am trying to achieve is to be able to run an update statement on table t with a returning clause. I know that I can achieve this if the statement run in the rule was an INSERT or an UPDATE (by adding a RETURNING clause), but I do not know how to achieve this when it is a SELECT statement. If I remove the RETURNING part of the query above, then I get the following message: cannot perform UPDATE RETURNING on relation "t" query: UPDATE t SET c = true WHERE id IN (1000460) RETURNING id hint: You need an unconditional ON UPDATE DO INSTEAD rule with a RETURNING clause. It was this error message that lead me to attempt the syntax in the beginning of this post. 

Now I would like to utilize the test view or any other view in a GUI which simply presents an editable table. This is possible with the recently introduced automatically updatable views. I want the table to contain drop-downs containing all possible values every time there is a column with constrains on it, like the column with a foreign key in this case. How can this be achieved? I know that I could utilize enums and I do that today, but on Amazon RDS they are unreasonably hard to modify. I had an idea of (ab)using the "EXPLAIN SELECT" query for each column and check which table and column is being queried. 

Another solution could be to add aliases, so after renaming a_table to b_table then querying a_table would still work, but all queries to the a_table would get logged and I could search for those queries that get run towards the old table. This would mean that there would be no risk involved in renaming a table. The same goes for columns, stored procedures etc. Does such a solution exist? I realize that the first solution requires major work in the IDE but having such an IDE would mean that I could safely rename tables and columns at will. Maybe a quick-fix solution to this would be a Mac/Windows hack where if any text editor presents the following text to me: 1UL0Aii1aaErbU then that will always get auto-replaced with a_column no matter where it is presented. I believe AutoHotKey.com could achieve this on Windows. 

Personally I would rehearse it within a test environment and expect to do it during a quiet time out of business hours. 

db_datareader is a built in role that grants read access to everything. db_datawriter is its equivalent for writes. If you want to grant limited CRUD access then you need to create a new role and use the GRANT statement to assign permissions to that role. After you have done that you can use the statements above to make your user a member of your new role as well. 

Let us suppose that you have built a reference data set that has two (or more) fields. For the sake of argument lets call the table geography 

Potential pitfalls to watch out for are the data types, size and collation discrepancies between data_table.field1 and geography.Area_Code. If geography.Area_Code is unique and must always be present make sure you stick a primary key on it. It is probably worth adding an index to data_table.field1. Again, if it is mandatory and unique make it a primary key. 

A VARCHAR(8000) column won't be indexed as you can only index up to 900 bytes prior to SQL Server 2016 so you don't need to worry about indexes. You need to consider who and what will be accessing the table when you make that change. As far as SQL Server is concerned such a change is considered a change of data type so will take longer than a change in VARCHAR size which is considered a metadata change. What is happening under the hood is that the data in your VARCHAR(8000) column will be shifted out of row and your record will now have a pointer to the data represented by VARCHAR(MAX). Time to execute will depend on many things such as 

It is possible to generate the GRANT statements dynamically but on any database with a security sensitivity I would be very careful doing so. 

Failing that try putting an index across timestamp, experiment_id and bucket_label. Again, you had best do this in quiet/down time and make sure you have the physical resource available to do it. On a separate point be very careful using field names that are reserved words such as timestamp. You can get some peculiar exceptions thrown in applications that are very hard to track down. 

It didn't mention about backups. For the meantime, you can change log backup param to as a workaround. Or consider moving to another backup tools (eg. Minion Backup or dbatools) or roll your own custom code because the last update of Ola's Maintenance Plan was on Oct 7, 2016. There's a github if you like to raise an issue/enhancement. 

You might be doing a large batch of transaction processing that could cause log reader agent to slowly read the t-log. Or NOT properly maintaining the t-log that could cause increasingly huge t-log. Ask around what's being process at that time. (watch out for index maintenance - Log Reader Agent will appear hung as it scans more log records.) Then check for VLF (Virtual Log File) by issuing on the publisher database. See if you have a lot of VLFs. Keep VLFs in check and t-log size at minimum. The key is to properly manage your transaction log when dealing with replication. Read on how to Optimize the Transaction Log. On your Log Reader Agent Profile, you can change the and to higher value. Be careful on these 2 parameters as it can cause Log Reader Agent to scan more transactions and could spend more time scanning and slowly delivering to distribution database. It's rarely you need to change parameters (even the ) on Log Reader Agent so use it with caution. 

I would highly recommend you to capture a baseline on replication commands stats (Distribution Agent). We can use Perfmon to monitor the following: 

For 7GB to 9GB of database, I would advise you to just use backup and restore strategy. This is the most simplest and effective way of migrating small database. I highly recommend you look at dbatools to help you automate your migration (it also migrate your objects like logins). The most important part of your migration is you practice your rollback and making sure application/users can still connect to your database after migration. Always test before you go live. 

It could be someone or a process/apps deleted and inserted the data in subscriber db (while cmds are being applied). Review your subscriber security settings and check what process/apps are accessing the subscriber db. Multiple publications connected to subscriber db. pub1 (deleted the data first) then pub2 tried to UPDATE/DELETE the data which could cause an error 20598. Triggers on subscriber tables that could delete/insert/update the data. 

We faced exactly the same scenario. Our solution was to have a SQL Server Agent job with a steps to call stored procs co-ordinating the delete/archive. To make our lives easier we had the application that persisted the data in the first place write a DateTimeCreated field in each related table with exactly the same date/time. For example an ProductEnquiry record and associated ProductResults record would get exactly the same date/time. Our clustered index was on that DateTimeCreated so DELETE FROM ProductResults WHERE DateTimeCreated BETWEEN... made use of the clustered index. As the database grew we found that the purge jobs had to have a pre/post step to disable/re-enable the FK constraints. We also had to start using a loop so we purged 50,000 records at a time and kept going until there were no more qualifying records to delete. When we move to Enterprise edition we started using partition switching which had a dramatic performance improvement on purge activity and massively reduced IO on our SAN. 

If you want a user to be able to read any table and view in your database then you would run the following from SQL Server 2012 onwards 

The higher the cardinality the more selective the index is. event_impression_ibfk_1 is being used on experiment_id and bucket_label neither of which appear to be particularly selective. If I am reading the explain plan correctly it thinks it needs to scan 14.9 million or your 40 million row table. If you have some quiet time or preferably downtime available it might be worth running 

It depends how complex your spreadsheet is, how many worksheets, whether someone has put all sorts of merged cells, fancy titles etc I've had some success using Apache Tikka as a content extraction tool with basic Linux bash utilities such as grep, awk, sort etc. I've had to do this to determine which spreadsheets might contain GDPR sensitive data. Tikka can extract data from over 1400 file formats and is a JAR file that can be called just like any other Java program. The useful output from a spreadsheet will be tab delimited. The name of the sheet will without leading tabs. Cells will be separated by tabs and the first column in any sheet will be prefixed by a tab. This makes it really easy to grab what output you need and use the MySQL COPY FROM statement to ingest it.