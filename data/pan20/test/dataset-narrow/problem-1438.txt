I don't understand why you wouldn't want to use an off-the-shelf loader. PNG, for example, is a good choice for a format but is complex to write a general purpose loader for (and probably not worth the effort of writing one that only loads the specific subset of PNG formats you care about). Given that somewhat unusual requirement, TGA is probably your best bet. TGA 2.0 has an alpha channel and is relatively simple compared to PNG. 

You can do this in C#, but you'll likely need to P/Invoke a lot of the functionality you'd need. C or C++ is probably more well-suited to the task -- you'll have fewer hoops to jump through. There are a few open source "cheat engine" projects out there you can look at for a better idea of what you're going to have to do. 

And so on. The purpose of a deliverable is almost always to demonstrate to some outside party what progress has been made -- since this is a school project that's almost certainly the case. External parties will not necessarily be intimately familiar with the details of the development between any two successive deliverables, so providing clear and understandable goals makes their job -- evaluating your progress -- easier. They will appreciate that. It will also help you, because its gives you goals and a plan, and helps you identify rough high-level dependencies, and let you check and measure your own progress. I use techniques very similar to this to schedule blocks of work for teams in my actual, real job making commercial games. 

To send constant values to shaders without using the effect framework, you create constant buffers and bind them to the pipeline with (for example) . For example: 

You should output vertices in clip space from the vertex shader (this is generally where things end up after the projection matrix is applied). The hardware will take the output of the vertex shader, perform clipping, and then perform the perspective division by and continue along the pipeline. 

If you define the ball's position as it's center, and the ball exposes its position and size information, you can easily compute the appropriate place to draw the ball's sprite from the code that does the rendering (which is partly why I feel like the sprite itself need not be part of the ball object). (In other words, use your second constructor, the one that takes the position of the ball and not the screen size.) 

There are simple solutions and tricks you can use (for example, if your slopes will all be a fixed angle, just author additional sprites for sloped movement and idle, provide a mechanism for your game code to know if you are on sloped surface, and render accordingly -- this is usually what older sprite-based games did). A general solution, expanding on your multi-jointed sprite idea, is to use inverse kinematics to solve for the placement of a characters feet (see also). This involves defining a logical skeletal structure for your characters, represented visually by several distinct sprites (for the torso, as many leg parts as you need, and foot). The end-effector in this system is the character's foot position on the slope, which can be computed from the (fixed) torso position and some idea of which foot is in front, based on the current walk cycle frame. Between those two known positions, an IK system can solve for the parameters to the leg joints and create a reasonable approximation of how any real biped might stabilize itself on a sloped surface. IK is often expressed as a 3D problem, but it can be reduced to 2D space relatively easily. In 2D you don't need to futz around with nearly as many joint rotation constraints to create realistic movement as you would in 3D, which is nice. This is the method used by many modern 2D games, especially given the trend of skeletal-based sprite compositions for characters, rather than the older technique of hand authoring every frame. 

Then that's all you should enforce -- and if you want to enforce other things, encode that into the interface as well. There is no need to, for example, enforce that implementations actually affect stats -- what if you wanted to make a "booby-trapped healing potion" that instead of healing anybody when used, set off a bunch of explosions? 

The array contains objects, which have a player ID field you can use to compare against the current player's ID to determine if they are the player holding the top score or not. 

This file () defines each frame of the animation, space-separated, as a number which refers to a sprite index in the sprite sheet. Frames 1 through 6 of the animation correspond exactly to sprites 1 through 6 in the sheet, but frame 7 of the animation is sprite 5, 8 is 4, and so on. At the cost of some additional data overhead, this approach allows you keep your current, simple animation code (that just loops the animation). It also affords you the flexibility of using an entire sprite sheet for potentially many distinct animations, and easily having variants of an animation (for example a "fast" and "slow" punching animation, whether the "fast" animation omits some less-than-critical frames). 

Normally when we render 3D graphics, we make use of a depth buffer to achieve the proper visual appearance of solid objects in relation to each other regardless of the rendering order of said objects. If, as in your case, you do not want this behavior, simply disable it somewhere in your initialization code. Do this by creating a new state object that controls the depth and stencil buffers and setting the appropriate member to , then assign your state object to the graphics deviceenter link description here: 

I'd recreate them every time they are needed. You are right to be concerned about the downsides of both choices, but in the end, you should err on the side of being less user-hostile. Having the ad constantly active in the background (assuming that the ad cannot be downloaded and then "suspended" until needed) will cause unnecessary processing during your games runtime, and the potential impact on performance and the user's battery life is almost certainly going to be worse in the long run for you than the potential impact of an ad not downloading fast enough and consequently not displaying long enough for the user to comprehend it (or see it at all). If your levels are linear enough you can start to pre-load the ads as the user nears the end, if such capability is provided by the API and does not violate any usage terms you may have agreed to. 

Storing components within the entity objects, and then doing a "for each entity, for each component, update" loop is a fairly straightforward and obvious solution. It's also not very good for cache coherency and creates problems for concurrent update scheduling. Consider an outboard storage approach, where component instances are stored in large homogeneous lists within the subsystem that is responsible for them. For example, physics components all live in what is effectively a big within the physics simulation object. In this system, entities only store IDs (possibly simple integers) or some other similar form of reference to components that comprise the entity. In fact, it's possible (and in some cases beneficial) to take this to the Nth degree and eschew an entity object altogether, binding components together only implicitly by storing an entity ID number. To update components, you simply update the owning system (that is, or similar). This update traverses each component in order and does whatever updating is needed. This is much more cache-friendly, and affords you a much coarser organization of update dependencies. This allows you to more easily reason about which systems have dependencies on the updated data from other systems and order their relative updates accordingly. It allow allows you to easily process updates for entire batches of components in parallel if you know the two relevant systems do not need to directly interact in the same frame. 

So long as the matrix is invertible (which it generally will be, unless you're doing something very unusual), then computing the matrix inverse of will give you a matrix that does what you want. That is, if performs some transformation, performs the "opposite" transformation. Most matrix/vector libraries provide a means for computing the inverse. 

The vertex format can be as simple or complex as you need -- you can simply decide to pre-define a set of vertex formats your game will use ({position, color}; {position, color, normal, texture coordinates}; et cetera) and represent the format as enumerated type. At the other end of the spectrum you can implement a free-form object that allows specification of arbitrary vertex element semantics at arbitrary byte offsets within a vertex. The actual vertex data should be represented as an and you can cast to an appropriate vertex format structure if needed, based on the vertex format enumeration. This allows you to store any shape/mesh in any vertex format without having to create subclasses -- having to create those subclasses is actually a design and maintainability problem. Once you have this in place, it should be pretty straightforward to then add a to the class. If you give the class a vertex format as well, you can even enforce correct bindings between materials and geometry: you can raise errors when you try to assign a material whose shader requires {position, color, normal} to a geometry object that only has {position, color} for example. This also allows you to externalize data definitions and thus make your APIs more data-driven. You can now construct objects with arbitrary collections of vertices. In terms of that API you can then implement another interface (it should be separate from ) that loads vertex lists from a data file format of your choice, or an interface that generates a list of vertices that form a cube, or sphere, or whatever other procedural primitive you'd like, with whatever vertex properties you desire. EDIT: A clarification on some ways you can handle vertex formats: The object should accept and store only raw, untyped bytes of vertex data. You can work directly with these raw bytes, although I do understand the appeal of having a 'vertex structure' type that represents an individual vertex for ease of manipulation. In order to be useful that type must perforce be a POD (or you couldn't send arrays of them to the GPU anyhow), and so it is sane to be casting between the raw bytes and the vertex structure, provided the data in the bytes does actually match up to the structure's schema. You could thus provide an interface to the vertex data in the geometry object like: