Try every combinaison of every size. Could actually work, but would take some (long (long)) time. Solution considered B: Build a table of levenshtein distances of suffixes The intuition is that there may exist some local minima of distance when computing the distance from every suffix to every suffix. The distance function is the Levenshtein distance, but we will be able to customize it in the future in order to take in account the probability of being of a certain types, instead of having a fixed type for each token. In order to stay simple in that demonstration, we will use fixed-type tokens, and use the classic levenshtein to compute the distance between tokens. e.g. Lets have the input sequence . We compute the distance of every suffix with every suffix (cropped to be of equal size): 

Solution considered A: Convolution of a patch of tokens This solution consists in applying a convolution with several patches of tokens, and take the one that creates the least conflicts. The hard part here is to find potential patches to roll along the observe sequence. Few ideas for this one, but nothing very satisfying: 

The goal is to figure out that syntax without prior knowledge of it. From now, new line is considered as a token as well. A document can then be represented as a 1-dimension sequence of tokens: 

Drawback: since a markov model has no memory, we will lose the orders of transition. e.g. If the repeated sequence is , we lose the fact that A->B occurs before C->B. 

I think I've worked something out. Basically I'm looking for an approach that works in a map/reduce type environment and I think this approach does it. So, 

I'm trying to build a cosine locality sensitive hash so I can find candidate similar pairs of items without having to compare every possible pair. I have it basically working, but most of the pairs in my data seem to have cosine similarity in the -0.2 to +0.2 range so I'm trying to dice it quite finely and pick things with cosine similarity 0.1 and above. I've been reading Mining Massive Datasets chapter 3. This talks about increasing the accuracy of candidate pair selection by Amplifying a Locality-Sensitive Family. I think I just about understand the mathematical explanation, but I'm struggling to see how I implement this practically. What I have so far is as follows 

Comparing this to 3.6.3 of mmds, my AND step is when I look at bands of r bits - a pair of movies pass the AND step if the r bits have the same value. My OR step happens in the buckets: movies are candidate pairs if they are both in any of the buckets. The book suggests I can "amplify" my results by adding more AND and OR steps, but I'm at a loss for how to do this practically as the explanation of the construction process for further layers is in terms of checking pairwise equality rather than coming up with bucket numbers. Can anyone help me understand how to do this? 

and in the end identify all the Items, even thought they dont exactly match. In order to stay short and readable, let's use instead some aliases A, B, C, D, ... to designate those token types. e.g. 

This seems to be used extensively in biology in order to analyse nucleobases (GTAC) in DNA/RNA. Drawback: Suffix trees are good for exact matching of exact tokens (e.g. characters). We have neither exact sequences, nor exact tokens. 

Now we can clearly see 2 clear diagonal lines emerge. There are 3 Items (Item1, Item2, Item3) in that sequence. The longest line represents the matching between Item1 vs Item2 and Item2 vs Item3. The second longest represents the matching between Item1 vs Item3. Now I am not sure on the best way to exploit that data. Is it as simple as taking the highest diagonal lines? Let's assume it is. Lets compute the average value of the diagonal line that start from each token. We can see the result on the following picture (the vector below the matrix) : 

Clearly now, our vector of diagonal averages is messed up, and we cannot exploit it anymore... My assumption is that this could be solved by a customized distance function (instead of Levenshtein), where the insertion of a whole block may not be so much penalized. That is what I am not sure of. Conclusion None of the explored convolution-based solutions seem to fit our problem. The levenshtein-distance-based solution seems promising, especially because it is compatible with probability-based-type tokens. But I am not sure yet about how to exploit the results of it. I would be very grateful if you have experience in a related field, and a couple of good hints to give us, or other techniques to explore. Thank you very much in advance. 

I have say 1000 movies each with ratings from some selection of 1M users. Each movie is represented by a sparse vector of user scores (row number = user ID, value = user's score) I build N random vectors. The vector length matches the length of the movie vectors (i.e. the number of users). The vector values are +1 or -1. I actually encode these vectors as binary to save space, with +1 mapped to 1 and -1 mapped to 0 I build sketch vectors for each movie by taking the dot product of the movie and each of the N random vectors (or rather, if I create a matrix R by laying the N random vectors horizontally and layering them on top of each other then the sketch for movie m is R*m), then taking the sign of each element in the resulting vector, so I end with a sketch vector for each movie of +1s and -1s, which again I encode as binary. Each vector is length N bits. Next I look for similar sketches by doing the following 

I split the sketch vector into b bands of r bits Each band of r bits is a number. I combine that number with the band number and add the movie to a hash bucket under that number. Each movie can be added to more than one bucket. I then look in each bucket. Any movies that are in the same bucket are candidate pairs. 

So now I have a workable solution, and all I need to do is work out whether using 3 steps like this will actually help me get a better result with fewer overall hash bits or better overall performance... 

Now, its obvious that any suffix compared to itself will have a null distance. But we are not interested by suffix (exactly or partially) matching itself, so we crop that part. 

Since the suffixes are cropped to the same size, comparing long string will always yield a bigger distance than comparing smaller strings. We need to compensate that by a smooth penalty starting from the right (+P), fading out linearely to the left. I am not sure yet how to choose a good penalty function that would fit all cases. Here we apply a (+P=6) penalty on the extreme right, fading out to 0 to the left. 

because it is the one that matches the best the sequence. The syntax (token types and orders) can vary a lot from one document to another. e.g. another document may have that list 

Here the repeated sequence would be because it is the token that creates the least conflicts. Let's complexify it a bit. From now each token has no determined type. In the real world, we are not always 100% sure of some token's type. Instead, we give it a probability of having a certain type. 

I have text documents which contain mainly lists of Items. Each Item is a group of several token from different types: FirstName. LastName, BirthDate, PhoneNumber, City, Occupation, etc. A token is a group of words. Items can lie on several lines. Items from a document have about the same token syntax, but they dont necessarely have to be exactly the same. They may be some more/less tokens between Items, as well as within Items.