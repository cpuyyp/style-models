No, if you are talking about partitioned tables, sql uses the where clause to filter rows. if your query doesn't have the where clause it will return all rows. if you can't test your changes you probably shouldn't be making them. 

Is it possible to use historic information to show disk usage growth? I haven't keep much in the way of database setup and disk usage information. So I can't say how much disk growth has occurred in the last 6/12/18 months. Can I reconstruct that picture from info stored somewhere in SQL Server? 

By default auto_update_statistics is disabled. A quick search returns plenty of links about it. According to Brent Ozar not everyone can agree if turning it on or off is a good idea. $URL$ If out if date stats is causing you problems maybe you should consider turning this feature on. To do this: 

I believe we've found another workaround. I'm posting my answer as I think it may be useful, and it's different enough from wBob's suggestion. We've changed the insert part of the merge statement so that it inserts to a temp table rather than the original target. Once the merge statement executes, we then insert from the #table into the target. It's not ideal, but at least the merge still handles the complexity of the 'upsert' by marking rows that have been retired/expired. We found this to be an acceptable trade-off compared with completely re-writing the merge as separate inserts and updates. 

I know there are a number of topics on this question, but I'm always seeking more insights. I have a large table with a billion+ records. The amount of records could be reduced and archived, but the size will still be large. My task is to change a existing data type of a single column where the old value of data is safe to convert into the type. Here are some of my approaches: 1 - Drop the constraints on the table that impact the targeted column, drop the indexes that also impact the targeted column, add a new column with NULL's at the end of the table, update the new column with the old column values in chunks of 10K, 50K or 100K increments, drop the old column when data has been copied and reapply indexes for that column only. 2 - Copy all data into a new table with the data type change in chunks as before, verify data is completed, drop the old table, rename the new table to the old and apply indexes. 3 - Import all data from another data source like a flat file to a new table with the data type change using BULK INSERT and MERGE SP's, basically like option 2 with having 2 duplicate tables, verify data, drop old to replace with new table and apply indexes. What would be the fastest and safest option? Are there other options I'm not considering? I've updated 100 million records for other tables really well with option 1. The bigger the table, the harder option 1 becomes due to the time duration of updating. 

This is difficult question to answer. The right solution for you will depend upon several factors such as cost and availablity of resources and expertise. But the solution should be driven by the requirements. Before you try to select a technical solution it is worth considering what the end result should look like. Where is the data going and for what purpose? Do you want reporting and analytics? Who is going to use this data and how? (Executives? Analysts? Marketing? Sales? Customer Service?) What level detail does each group need? (Raw ? Aggregated?) How often? When? How fast? Does it need to be integrated with Salesforce? If you decide the solution is a data warehouse I would seriously consider a cloud based alternative. As you already have cloud based software and you are spread across different countries. 

If you are comfortable with excel you could import the data directly with an odbc connection. If you wanted olap capability you can do that with a tabular model within excel. In general most BI solutions will work with any relational database via an OLE or ODBC connection. If the product doesnt have a native adapter you could use a 3rd party ETL tool to move the raw data to a db that is supported. 

Note: it is possible to have multiple tabs open which can be connected to different databases or servers. If you have multiple tabs open and tiled it can get a bit confusing. As an added precaution you could disconnect and close other tabs to prevent any accidents. It is possible that a sql script can contain many sql statements, and could include another use database command which could change the connection part way through the script. You should read the script or search for this before executing scripts supplied by other people. One last warning: it is also possible to connect to multiple database servers simultaneously using registered servers. However unless you deliberately chose to do this its extremely unlikely you would ever encounter this even accidentally. 

The only way I can think of is, using "Application Name" in the connection string. That is how SQL server recognises different applications. I think, you can append application name in the connection string at runtime using your v$session column. 

I'll always prefer to store that as a single column unless there is some specific business/application demand. Below are my points - 

I am connecting to a database server(MSSQL 2008R2) using SSMS 2016 and for simple queries even like "USE DBNAME", it's taking 4-5 secs. However time stat shows "CPU time = 0 ms, elapsed time = 0 ms", not sure why? On the other hand, If I connect the same server using SSMS 2008R2, it gets executed instantly. I compared client statistics for both SSMS2008R2 AND 2016 but both shows almost same stats. Why my SSMS2016 is behaving like this? 

I am tuning an SP and re-written that to optimize. Now I am comparing old code and new code in terms of timing using "setting statistics time on/off". While setting time statistics off, my new code is performing well at least 4 times better than old code (Old code is taking 4 secs and new code is taking 1 secs to execute), but when I am setting time statistics On, my new code is taking approx 12 secs and old code is taking around 7 secs. Why new code is performing badly after setting time stats on? Looks like there is some cost of time stats also and in my new code, that cost is very higher in comparison to old code. Am I right? If so what is that? 

Yes you can use MS Access as a front end to SQL Server. Its quite common. I have done this myself in the past. No, Access itself won't corrupt data. However your Access application needs to be designed so that it respects SQL Server constraints and rules. Also how will your Application stop users from making simple mistakes, like deleting records if they're not permitted to. The most common problem I had was dealing with concurrency. When 2 clients are doing things at the same time such as inserting a record. Do you let Access generate the primary key? or do you let SQL Server manage it? In either case after you've inserted rows how do you ensure that each client is still dealing with the same row? This is a consideration for anyone developing a multi user system. It just takes some time and thought. Some advice: Make sure that you define the primary key in SQL Server. If you don't do this Access won't allow you to update records as it can't guarantee a lock on a single record. Primary keys and indexes also have a huge impact on performance. Edit: I've just been thinking about your Question title a bit more. You mention "security breach". If security is a concern, You need to consider how users will connect to the database. Domain authentication? or SQL authentication, and how you log actions taken by users. 

For question 2. I think I will break the large table up into many different tables and then use a view to join them together. This will allow me to have something like table partitioning without the feature of table partitioning as outlined in this article. $URL$ View with schemabinding that unions all the tables together. Then I can insert data and select data from this view as if it was the primary fact table. I would only need to ensure all my SELECT queries on this view include the column I chose to partition the tables with to get the full benefit. 

Interesting find today, I have a file group that is used for indexing. It has one file that has a .ldf extension, which as you know for SQL Server, is the extension for the transaction log files. My understanding is the extensions don't really matter. Whatever is first is first and anything else is secondary regardless of the extension. Does that apply to .ldf in this case when clearly it's being used for the clustered indexes? I ask because I would assume SQL Server treats .ldf differently than say, mdf's. (And before you ask, yes, there is already a .ldf assigned for the transaction log on another spindle) 

I have a bulk loading process that loads millions of records into a few fact tables within a warehouse. Those tables are primarily clustered on time. The non-clustered indexes are in place for how the data is used to speed up performance. I typically drop some of the non-clustered indexes to both speed up inserts and to reduce index fragmentation after large data loads. However, this process of dropping and rebuilding is causing a huge amount of time as the data grows. Example: One table took 2 hours to apply a new non-clustered index on 100m+ rows. Likewise, if I leave the non-clustered indexes in place, they will increase the inserts 3x to 10x in some cases that force your hand to drop and rebuild. While it's awesome to drop and rebuild indexes, they don't really pan out that well as data grows in those tables. What options are available to me? Should I bulk up the server with more memory (currently 32GB) and cpu (4 vCPU)? Should I rethink my indexes? Should I find a balance of keeping some indexes on for reorganization versus dropping and rebuilding? (Note: I don't have enterprise edition.) I'm thinking my only option is enterprise edition with table partitioning where I can rebuild indexes per partition as opposed to the whole table. 

A curve could have anywhere from a dozen to hundreds of values, and the points may not be ordered. What is the cleanest/simplest way of doing this? 

No i disagree. I dont believe it should be faster on your laptop given the spec you have just described. I also dont believe a difference in version or flags should explain it either. A 9000% difference suggests something is going very very wrong. You should expect to get at least the same or similar result on the server, or even better. At this point there are too many variables in your post. I think you need to work through a process of elimination to pin down where issue exists. Is it the server hardware? Is it the sql instance? Is it db specific? Is it the table/indexes/statistics? Is it the query itself? Is it anthing to do with load or external influences? I would start by performing a simple test under the same conditions on both the server and laptop. For example a simple script which inserts a million rows of test data into a table. Run the same script on a few dbs to get a benchmark. If you get comparable performance, then start looking at your tables. Create a maintenance script to rebuild your indexes and update stats. Run the same script on both dbs before you run your query. When you run your query, check the execution plans in both environments. Are they the same? 

Changing the below setting and uninstalling some add ons for SSMS 2016 helped me - HKEY_CURRENT_USER\Software\Microsoft\SQL Server Management Studio\13.0\UserFeedbackOptIn Thanks @Kin. 

Just by looking at the query and predicate ("income in (32,33,34) and status_id = 6;"), you can't be sure that SQL will use covering index. It all depends on cost that SQL thinks is the lowest one. Not sure how many rows your predicate will have, but obviously, using Clustered index scan is cheaper than covering index. 

I am working on the optimization of a SP which contains some business logic using looping. I have removed the looping and converted those piece of code into some simple insert/update statements. Now I've to do benchmarking and compare old and new code in terms of execution time and logical/physical reads. My problem is because of the loop in my old code, how can I determine what is the total no of logical/physical reads. Because in SSMS, I can see thousands of IO stats statements like: "Table 'Employee'. Scan count 1, logical reads 3, physical reads 0, read-ahead reads 0, lob logical reads 43, lob physical reads 0, lob read-ahead reads 0." 

Page write is just an indicator of logical writes as well as physical writes. Logical write is an operation that happens when the page is being written in the buffer(dirty buffer) and physical write is an operation that happens when the same dirty buffer is converted to clean buffer and is being written on the physical disk. Definitely, this is not a single metric to judge memory pressure. You need to combine different metrics for coming to a conclusion. But in your case, it doesn't seem to be a problem as your page life expectancy is quite high which indicates a page will remain in the buffer for around 37k secs. So you don't need to worry about memory pressure I think. 

I have been given the task of improving the performance of the first report connection each day. Similar to this post. We have a simple SSRS report that acts as a landing page. It has hyperlinks to subsequent reports. I have tried: 

I dont think there are any hard rules around this. It's really up to you to decide what convention suits you best. The options available to you may be influenced by the database software you are using. The 3 suggestions i have are: 

There are lots of tools and methods available for loading data. But the easier you want this to be for end users the more complex and sophisticated your dataloading routine will become. Data validation and business rules need to be defined and implemented. Somebody has to do it. No software is smart enough to do that "auto-magically". You could build data checking into the spreadsheet. Hopefully this would stop errors before you load. It is possible to import data directly from a db query and there is a variety of lookup and validation tools in excel. You could build an import routine in ssis and apply validations, filters, lookups and substitutions as part of the import process. It can even handle errors and redirect invalid records back to the user. But this takes time and effort. My preferred approach would be to use a db to begin with rather than excel. That way datatypes, dependencies, rules and constraints can be enforced from the start. You could develop an interface in MS-access. Unfortunately all of these things take time. The more sophisticated you want the app to be more time it will take to develop. 

I have a document that was compressed in gunzip from a unknown source system. It was downloaded and decompressed using a 7zip console application. The document is a CSV file that appears to be encoded in UTF-8. It's then uploaded to Azure Data Lake Store right after compression. Then there is a U-SQL job setup to simply copy it from one folder to another folder. This process fails and raises a UTF-8 encoding error for a value: Ã©e Testing I downloaded the document from the store and removed all records but that one with the value flagged by Azure. In Notepad++, it shows the document as UTF-8. I save the document as UTF-8 again and upload it back to the store. I run the process again and the process succeeds with that value as UTF-8 What am I missing here? Is it possible the original document is not truly UTF-8? Is there something else causing a false positive? I'm a bit baffled. Possibilities 

I have a two part question really for anyone working with data warehousing where they have large fact tables. Question 1 Lets say you have a table that has 500 million or more records in it that is clustered on time. You are only posting incremental records for the last 24 hours on a daily basis to this table. How do you handle inserting delayed records that are a month or older to that table? Would you do nothing unless the insert caused a lot of fragmentation or would you attempt to drop the indexes, insert and rebuild? I do not have enterprise edition available to me for table partitioning. Question 2 If you have a fact table that is growing large like the above example, would it be wise to split the fact table up into multiple tables or would it be better to look towards adding additional files to the filegroup of said table if table partitioning is not an option? Thanks in advance. I'm dealing with some large growth and trying to approach it the right way.