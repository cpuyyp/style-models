I have hosted VSO build agents building our exe and that pipeline setup. I am trying to deploy the produced exe to a dev testing environment. I have a deployment group set up and the files are being successfully copied to the correct location. What I'm trying to do now is launch the exe after it was copied. I have an inline powershell script task running consisting of 

I have a CI environment for a DLL. On code check in, it automatically builds and publish's with a version such as 1.0.0-dev01. Once it is QA approved I don't want to rebuild this DLL with a new version number, but instead strip off the '-dev01'. This all happens in within my VSO environment. Does anyone have any suggestions on this? 

the release log displays but on the computer the script is running, the process isn't launched. If I run via powershell on the computer from the A1 folder, the process starts as expected. What am I missing? EDIT: Revisiting this after a bit. I have redone the script to use start-process . I have used combinations of using runas with credentials setting working directorys. The process does start...sort of. My target application is a WPF window. It launches but doesn't any any GUI drawn. I know the process is running because i can use which returns a valid process. Does anyone know how to launch wpf/console application and have it draw it's GUI? 

So I wrote this powershell script to be ran by the deployment group agents who have access to the server locally. 

From the perspective of build automation, I find that it is better to have multiple repositories. This allows for smaller configurations and more granular control of your build/release process. You can allows reference and pull in source code to build or release (of course you can also ignore source code as well in VSTS build pipeline). This leads to being able to release just the project you want or need to your desired environment. You can of course pull in artifacts from other builds and include them in your release if you need to. To summarize, I tend to go as small as possible and and only group things into the same repository if they are indeed part of the same project and no part of the project can be used elsewhere. 

Relatively speaking, the concept of devops is new and still defining itself in my opinion. I currently fulfill a devops engineer role. For me, this means I facilitate and develop the tools and processes used by both our dev and ops teams freeing them to focus on the product that generates revenue for the company. The ops and dev teams spin up their own servers and such as needed. I just hook up the CI for our products, ensure our processes makes sense and seek out what process can be improved/automated. I meet with all of our departments, from sales, to warehouse, to developers and operations (QA and release managers) to see what they are doing and how I can improve their process. 

Image generation and distribution quickly rises to O(n^2) or higher, and as terraform is explicitly focused on instantiation it relies on external provisioners for internal state. It will work at a smaller scale, but will greatly complicate a hybrid-cloud model and will have problems if the systems are long lived. It also tends to cause difficulties with refactoring an iterating but these would be dependent on your use case. For example, lets say you wanted to deploy a new version of a python module. If you had a wheel or a requirements.txt you could deploy this to cloud provider a,b,c on OS version c and d. But with fully baked images you would have to produce and test images a_c, b_c, c_c, a_d, b_d,c_d. If you have to do this per iteration the costs to developer productivity grow very quickly. Note that you may be assuming that the images will be fully immutable, but without a configuration management tool you have no way to ensure that they are. As containers in general right now work under a trusted model, and it is fairly trivial for any process or person who has access to launch a machine under frameworks like docker to make arbitrary changes to anything on the system (every API user is root, even on the host for docker) this is a contingency you want to account for. If you keep the loose coupling between cloud provider, application and base OS it is much easier to iterate over time and vendor mitigation is far easier. If you maintain a terraform configuration for GCE, AWS and Azure, a packer configuration per OS then an application deployment method separately it may increase the initial complexity but those costs will quickly amortize away over the total life cycle of the project. I personally find a tool like ansible, which can be run without centralized infrastructure, and which can make orchestrated changes based on state to be more flexible in a pure deployment model but I have always had to retrofit in some form of configuration management at some point. This may not directly map to your specific use case but it is a very good reason that it is a common practice. 

This means that the container can access all resources including the hosts disks and hardware. It is security through obscurity, which is not security. Anyone who knows how to use or walk the /sys and /proc trees could easily compromise the host and all containers with zero logging. The reality is that docker shifts both complexity and the security boundaries. All users who can launch container or hit the API need to be restricted to trusted users in that security context. disables all apparmor and selinux policies which is actually far less secure than a native package. Namespaces are not a security function and depend on apparmor and selinux to enforce reasonable constraints. Docker notes this on this page. $URL$ 'First of all, only trusted users should be allowed to control your Docker daemon.' The security functions of docker/kube are not administrative boundaries like classical unix permissions, they are tools to prevent non-privileged containers from breaking out. In a docker world the administrative boundary becomes the host, and the selection and segmentation required to isolate applications or users within that context needs to be applied at that boundary. The benefits of this shift in complexity and responsibility generally outweigh the risks if implemented with those changes in mind. TLDR Docker API user == Sudo ALL user Running a container with the --privlaged flag == running a web service as suid or as the root user. Edited per the OP's request for additional information The referenced issue with breakout int he OP's edit was an non uid0 privilege escalation. Unfortunately, due to the need to perform root only actions Docker needs to enable some capabilities so that apt/dnf can install packages etc... This need does pose a risk if production workloads are run in this default configuration and one should adopt the security principle of least privilege for production workloads. disables apparmor/selinux and opens up capabilities I am using ubuntu but it may be useful to work through the following steps. First start a default container with docker run -i --rm -t debian bash From the parent host find the PID for bash using ps and note that the process is owned root. If you look in you will see the contexts it is running under. 

I have a few CI environment in VSTS. One for dev, QA, staging, and production. my builds are created in the cloud by VSTS build agents. these builds are not obfuscated by design and need to be deployed to my dev and test environments as such. When they are approved by QA in the test env, they go to staging. What I need to have happen is for the build to be obfuscated before being put in this environment. Is it possible for me to run a docker container in VSTS release pipeline to obfuscate the build in the container and then download the result in a deployment group? My main questions are boiled down to this: I would like to have a container image in the cloud running a tool installed on the image. During my VSTS release pipeline, I would like for my release agent to pull down this image, pass in the build artifact, and then do something with the results of running my artifact through this tool. I have an Azure subscription. I've tried reading the documentation but I'm confused as to what I need to set up. This container is not hosting any web app, but is being used as a way to run my tool locally on the release agent. I see that I can run a docker run command as a VSTS pipeline action. What do I need to set up on my azure subscription to host this image for the VSTS agent to pull it down? 

passing in the a personal API key (created by Jenkins user manager to use by visual studio team services), this script first checks if it is can access the server using local connections, if so set the variables appropriately. From there we create local handlers for environmental variables. Our release artifact name defined in VSTS is the same as that in our jenkins server. The build number used is created by VSTS and passed to jenkins and stored by the VSTS plugin(Jenkins TFS Plugin download). We get this version number from the environment as the artifact build number. We also ensure that our team project path url is windows URL encoded brief explanation. Next we hold our authentication which is stored as a variable in our VSTS variables. The is marked as secret. As such, we must pass it in as it isn't visible as an environment variable. We then encode the authentication and add it to our request headers. We then add a debug line to print what target we are looking for and what project we are looking in, just to make sure we are looking for the right thing. We create a variable to hold the build information that we find and do one more debug write stating what are query url is so that we can manually run it if we run into any issues. Next we then run this url query against the build server and iterate over all the builds returned. The query filters out all the JSON to just the build ID, it's absolute url, and the VSTS build number. We check to see if the build we are examining has the same build number as the artifact we are deploying, if so store it in the variable, print the was found for debugging, and exit the loop. Coming out of the loop we check to see if we found a result. If we did not find a build (due to it being cleaned up most likely) we inform the user that the artifact needs to be rebuild from source. Otherwise we download the artifact zip to a temporary location from the jenkins server and extract it to the target directory, defined as a variable. Lastly we delete the downloaded zip. This is ran as a powershell script task and due to it's length, can not be an inline script. So to deploy this, i have a devops git repo for my scripts and I just include this as an artifact that is downloaded and ran. This task is preceded by other tasks which stop the current process from running, and delete the current files in place before running this task. After this task is ran, more scripts are ran to run the downloaded artifacts. I hope this helps someone else trying to coordinate and link VSTS builds with Jenkins artifacts. 

But be very careful adding them as an example. cap_sys_module will allow a container to add or remove kernel modules from the parent host. cap_sys_rawio will open memory and all block devices to attack cap_sys_admin is super dangerous. So in this case I would see if you can make things run in this context. $ docker run -i --rm -t --cap-drop=all -u nobody:nogroup -t debian bash Hopefully the robustness of apparmor and selinux profiles improves over time, but if that is not enough for your security needs you can look in there too. Really, avoiding the --privileged flag and using principle of least privilege will make the most impact. Especially if you take advantage of the ephemeral nature of containers to keep packages up to date. If you need more Red Hat covers some basic seccomp options here. $URL$ 

Compared to just adding the user to the docker group the second links solution is not any more secure. Note how that page still can launch with the flag, and that it is running unconfined. 

Assuming that "publicly available" is on the public internet it will be difficult. DNS isn't really that reactive to changes and is challenging to run in a secure fashion on the public internet. It also depends on what protocol you are running, because of an oddity around SRV records where pre-existing protocols were excluded from supporting it in the RFC. But in general the public and the private solution I would implement are similar with an additional layer for a public internet solution. (assuming no use of a 3rd party or geo specific answers) On the internet edge use Unbound DNS server on the edge, and have it relay queries to consul which would handle service registration. If this is for private usage this would allow for some data-center awareness and fail-over. If it is just for internal use you can just use consul and either configure your internal DNS servers to delegate subdomains or forward to the default domain. 

This is an area where Kubernetes has the correct model, there should be a load balancer between all systems which should have functional health checks. Once you start to add Nagios, Zabbix or other types of monitoring to the system you start to build a large state machine. This will break the loose coupling model and introduce inter-dependencies that inhibit the ease refactoring. While not set in stone the key differentiation between microservices and other variants of SOA is this loose coupling. If the services are fine-grained and perform a single function, implement a health check at an upstream load balancer, then monitor the active pool members. As an example in HAproxy 

Note it is all zeros, from CapInh. Then if I had to enable features for the application I would use after the Example, will break ping: 

While it may not be the answer you are looking for, puppet's DSL lacks verbs on purpose. Puppet's idempotent philosophy is a huge reason it is so useful for some use cases but for this type of need you will want to use a tool which is more targeted to that role. When you hit limitations due to the idempotent philosophy of Puppet it is a good indication that another tool is required for that need. I would look for publicly maintained tools for the task maybe in Ansible or Terraform. We leveraged the "puppet kick" functionality of puppet and then tried to extend it after it was deprecated and the pain of trying to ignore their design philosophy is an expensive one.