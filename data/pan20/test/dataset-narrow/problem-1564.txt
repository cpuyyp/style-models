The images of the data points and of the targets are from the same source: the target image of a data point appears in other data points, and vice-versa. What I have done: For now I have built a perceptron with one hidden layer and the output layer gives the pixels of the prediction. The two layers are dense and made of sigmoid neurons and I used the mean squared error as the objective. As the images are fairly simple and do not vary much, this does the job well: with 200-300 examples and 50 hidden units, I get a good error value (0.06) and good predictions on test data. The network is trained with gradient descent (with learning rate scaling). Here are the kinds of learning curves I get, and the error evolution with number of epochs: What I am trying to do: This is all good, but now I would like to reduce the dimensionality of the data set so that it would scale to bigger images and more examples. So I applied PCA. However I did not apply it on the list of data points but on the list of images, for two reasons: 

As the images look all similar, I managed to reduce their size from 4800 (40x40x3) to 36 while loosing only 1e-6 of the variance. What does not work: When I feed my reduced data set and its reduced targets to the network, the gradient descent converges very fast to a high error (around 50 !). You can see the equivalent plots as the ones above: I had not imagined that a learning curve could start at a high value, and then go down and back up... And what are the usual causes of gradient descent stopping so fast? Could it be linked to parameter initialization (I use GlorotUniform, the default of the lasagne library). Then I noticed that if I feed the reduced data but the original (uncompressed) targets, I get back the initial performance. So it seems that applying PCA on the target images was not a good idea. Why is that? After all, I just multiplied the inputs and the targets by the same matrix, so the training input and target are still linked in a manner that the neural network should be able to figure out, no? What am I missing? Even if I introduce and extra layer of 4800 units so that there is the same total number of sigmoid neurons, I get the same results. To sum up, I have tried: 

I often use Nose, Tox or Unittest when testing my python code, specially when it has to be integrated with other modules or other pieces of code. However, now that I've found myself using R more than python for ML modelling and development. I realized that I don't really test my R code (And more importantly I really don't know how to do it well). So my question is, what are good packages that allow you to test R code in a similar manner as Nose, Tox or Unittest do in Python. Additional references such as tutorials will be greatly appreciated as well. Bonus points for packages in R similar to 

Spell checking is not really in the realm of train-and-predict models in data science. Mainly because embeddings require known words in the case of word2vec/GloVe, known contexts in the case of doc2vec or known stems in the case of Fasttext plus a significantly large training dataset. Spell checking is instead more traditional software engineering. It's broad but fortunately, you can start small and increase it bit by bit. If words are commonly known i.e. common nouns and pronouns you can use a package similar to autocorrect in Python but if you are looking for a specific list of words, in your case ports which are proper nouns, you must use a real spell check procedure. Since it's a broad topic you should start by following the answer provided by Stack Overflow about Peter Norvig's post and start building from there according to your needs. 

As I understand it, this option only calculates the loss function differently without training the model with weights (sample importance) so how do I train a Keras model with different importance (weights) for different samples. PD. This is a similar question xgboost: give more importance to recent samples but I would like an applicable answer to Keras. 

Also, check this this question on StackOverflow for more information, like solutions with match and the qdap package. 

You can use either sparse matrices or feature hashing. Sparse Matrix I suppose that using a sparse matrix is the only choice. I suspect that this line of code will work. This uses the Matrix package. 

Kaggle is more contest platform with a jobs board more than a freelance platform. Try upwork and freelancer.com. They are the ebay of freelancing and they support a wide range of payments although some of their fees have been going up for a while and search is favoring fees over matching to best results. Still you will be able to sort by skills like visualization, machine learning, data cleaning. It will depend on what skills you have to offer. I'm sure there are others better out there and probably others without too much traction. AngelList also offer contracts for data scientists if you're looking for something more structured. 

Let's start by answering your first question. Is it required to balance the dataset? Absolutely, the reason is simple in failing to do so you end up with algorithmic bias. This means that if you train your classifier without balancing the classifier has a high chance of favoring one of the classes with the most examples. This is especially the case with boosted trees. Even normal decision trees, in general, have the same effect. So it is always important to balance the dataset Now let's discuss the three different scenarios placed. Choice A): This would be what I explained all along. I'm not saying necessarily you will have a bias. It depends on the dataset itself. If the nature of the dataset has a very fine distinction with the boundaries then the chance of misclassification is reduced, you might get a decent result but it's still not recommended. Also if the data does not have good boundaries then the rate of misclassification rises a lot. Choice B): Since you are placing weights for each sample you are trying to overcome the bias with a penalty. This is also called as an Asymmetric method. Normally these methods increase the accuracy of a model by a slight margin but that mostly depends on the machine learning algorithm you are using. In examples like Adaboost such a model the effectivity of the model increases. This method is also called Asymmetric Adaboost. But this might not necessarily work with all algorithms. Choice C): Assuming you have weighted the samples accordingly it should do the same as either choice A or choice B. I'll leave this for you to extrapolate based on my previous explanations. 

This should mostly do the job. Use the arr1 ,arr2,arr3 in the function you mentioned. They are the 1d array of the columns you split 

You can try something like SMOTE and see how your newly generated data fits your requirements. If your data has a statistical model you can use an appropriate parametric model to generate data. You can even try methods like Non Parametric estimation such as Parzen windows etc. All of this depends on the statistical fit of your image data which you have processed so far. Other Methods similar to SMOTE: ADASYN, Boundary SMOTE etc (Look them up on the internet) Note: Apply SMOTE on the already processed image data such as the training data you already have not directly on the image itself. 

And combined with some ggplot2 magic using the results of that apply function you can plot a graphical representation of the search. In this plot lighter colors represent lower error and each block represents a unique combination of column sampling and row sampling. So if you want to perform an additional search of say eta (or tree depth) you will end up with one of these plots for each eta parameters tested. I see you have a different evaluation metric (RMPSE), just plug that in the cross validation function and you'll get the desired result. Besides that I wouldn't worry too much about fine tuning the other parameters because doing so won't improve performance too much, at least not so much compared to spending more time engineering features or cleaning the data. 

You could use the ratios of bugs (or any other variable) divided by lines of code (LOC). Since ratios may vary a lot for instance bugs vs. vulnerabilities the resulting plot oftentimes doesn't look as good so you could use a normalization procedure, in fact, is the recommended procedure. In your case Bugs and a few vulnerabilities divided by LOC doesn't look too bad so I included the plot of raw ratios too in this example. I'm using a count overlapping points plot which is a variant of a barplot that doesn't fill bars and only cares about the value as a point. You could use any normalization that you feel gives you the best result. Here I'm only centering the data. In both plots the highest value means higher complexity according to the ratio metric. Raw Ratios of variables divided by LOC 

You will need a package that supports sparse matrices for measuring the net revenue though. Fortunately, most modern mainstream packages support sparse matrices. Feature Hashing Here is a great explanation of feature hashing in R (among other techniques) which is also an alternative, specially useful when you have hundreds of thousands or millions of multiple levels. $URL$ 

The ratio of vocabulary vs embedding length to determine the size of other layers in a neural network doesn't really matter. Word embeddings are always around 100 and 300 in length, longer embedding vectors don't add enough information and smaller ones don't represent the semantics well enough. What matters more is the network architecture, the algorithm(s) and the dataset size. A simple way to understand this concept is that a bidirectional LSTM model with 50 neurons (nodes) followed by a fully connected layer of 70 neurons will outperform a simple MLP of 1000 neurons (nodes) connected to a embedding layer simply due to its architecture. Adding dropout will improve performance as well. In addition, even if the vocabulary is just 300 words, using pre-trained embeddings will probably yield better results than training the embeddings directly on the dataset. The same applies to data size, a dataset with more samples will make a better classifier than a dataset with just a couple thousand samples. In summary, it is preferable to try many architectures and cross-validate them (and/or ensemble them depending if you have a large enough dataset) with the smallest number of neurons possible and then start building up in size, depending on what computational resources you have and the speed of development you need. Large models slow down development speed whereas small models speed it up. This goes whether your vocabulary is the size of common crawl or just 300. As usual, try feature engineering (sentence length, special characters, etc.) and increase the dataset size as doing so often helps in whatever task you're trying to predict. 

Neural Networks basically act as a high memory-based machine learning algorithm. So for a given dataset the chance of it perfectly aligned with all the data at a given instance is high, as it most likely just ends up remembering every data point you give. Overfitting occurs precisely because of this, when a new expansive data set is introduced there is no way it can adjust its fit to the new data, graphically it ends up missing more of the values than its supposed to fit. In conclusion, it does not work well in the case where the scoring population is significantly different compared to training sample. 

You could try modeling it as a discrete distribution and then try obtaining the random samples. Try making a function p(x) and deriving the CDF from that. In the example you gave the CDF graph would look like this Once you obtained your CDF you can try using Inverse Transform Sampling. This method allows you to obtain random samples for the distribution specified. This link would be helpful for your understanding of that technique. These random samples can fill those missing values as per your requirement of probabilities. Note: There are other techniques as well, you could search and explore along the lines of random sample generation from discrete distributions. It might be the case that your actual data might fit for example something like Poisson's distribution etc. For all such cases, you can find various examples on how to generate random samples for those particular distributions 

First and foremost you need to know the difference between the type of data you are trying to predict. The two general categories are discrete and continuous. Most people tend to miss out that classification is at its core a discrete "regression". The values are predicted for discrete variables by considering a decision-based approach which ultimately leads to finding or predicting the variable in question. You can call this classification. For continuous variables, however, since the values can have a certain range it most closely mirrors a curve fit with the addition of an error boundary. I think this should clarify what you're essentially trying to ask. 

There is very little information in this question. I will try to answer this in the most generic sense. Let's start by defining Noise. Noise here as you probably know is unwanted data. Any data which you are not looking for while evaluating a problem or scenario can be considered as noise. Examples for amplifying noise: Amplifying noise might occur in cases and scenarios where there is a small data set and you are trying to supersample the dataset or another example could be while working with waveforms. In order to detect weaker signals. Disadvantages of Amplifying noise The biggest disadvantage of amplifying noise from a data science perspective is that the model used to perform various operations on the data such as Regression, Classification etc will be less efficient. For example having noise based on supersampling in Classification may affect the model. If we were to use decision trees for classification you might create a bias in the algorithm which just pertains to noise while training. So your accuracy for classification also decreases. Similarly, in regression when you train with noise you might choose a wrong model because the noise alters the goodness of the fit. 

1.- Ideal true vs false ratios don't exist and they should reflect the the reality the best they can, you can always remove negatives if the ratio is too skewed to improve training speed though. Let me explain it with an example. Ads CTR is as old as the internet and it's skewed to less than 1% positives vs. plus 99% negatives. Yet, data scientists prefer to train it on the entire dataset because many negatives will include information that models couldn't find otherwise. They might not provide a lot of information as a positive one but they are still somewhat important. There are approaches where CTR ratios get artificially rebalanced by sampling in case you want a swifter training and it will still work. In your case, positives are 0.4% which resemble CTR on ads so you can: gather more data to increase the number of positives in order to better understand what makes an article interesting. In case that is not possible trying ensembles which often improve prediction performance. 2.- Clustering is an unsupervised approach so you would be losing information by doing so (training labels) besides, sentence embeddings (representations) of one big cluster of negatives and a tiny cluster of positives do not convey information as well as word embeddings which have already been trained on billions of documents. In addition, running k-means on categorical variables will yield anomalous clusters because it's meant to be used with continuous variables. You can find more information about the topic on the following links: $URL$ K-Means clustering for mixed numeric and categorical data $URL$ $URL$ Therefore, you should use high dimensional embeddings or representations to cluster meanings together, this has been explored in word meanings but for sentences or articles, a vector representation becomes more complicated to implement. One possible approach is the Word Moversâ€™ Distance but there are many more possible approaches, you should google them. In addition a non-linear clustering algorithm such as t-sne will probably yield better results than k-means using the embeddings approach. A better approach is: 1) to use multiple models and compare their performance on this dataset. I have the impression that there will be certain keywords that make articles interesting, so a bag of words will still be helpful, even as a starter model. 2)Use feature engineering. Your model might be overloooking important features, such as article length, reading time, number of paragraphs, ratio of complex words (measured by length), etc. Feature engineering is always important in case you haven't used it yet. 3)Use pretrained embeddings. CNN and RNN models can use pretrained embeddings such as GloVe, Word2Vec or FastText so you use better representations plus other complex layers later on in the architecture. This is extremely important to increase accuracy. 4)Use metrics to measure improvement and ranks to check for the best predicted interesting articles.