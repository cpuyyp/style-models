Create three different procedures - GetMonthHighScores, GetWeekHighScores and GetDayHighScores. Call the appropriate one for the given parameter. 

With some clever jiggery-pokery with partitioning and filegroups you could get a particular sub-set of your data onto just one of the physical files used by a particular database. Trying to attach that into an existing database at the recipient would be "challengining", I think. In 20-some years of doing this stuff I've not heard of anyone trying that.* My suggestions would be a) copy your DB, delete the stuff you don't want to pass on, send a backup of that to the recipient b) use BCP with the queryout option to dump just the required data to external files. Send those to the recipient. *I'm always willing to learn, however. 

SQL Server 2016 introduced the STRING_SPLIT() function. It has two parameters - the string to be chopped up and the separator. The output is one row per value returned. For the given example 

There's a lot of CONVERT_IMPLICIT going on in there. Chances are one of your DBs is overflowing one of these intermediate calculations. I don't see an error on my small development box. To debug I would comment out each of your calculated values in turn to see which is throwing the error. Then I'd filter out large DBs using WHERE. If it works for small DBs that would be a clue. Next strip the calculation down to its minimum and run it for just the the largest DB. Add the CASTs back one at a time and compare the Defined Values for arrangements where it works and where it fails. My feeling is a single CAST to INT around the whole calculation is likely to be your best option. This article state ARITHABORT setting has a bearing, too. 

You could create additional statistics for the data. Include a WHERE clause so each new statistics object references only a subset of the table. By bucketing the rows appropriately (big, medium, small?) the filtered statistics will be less bad than the whole-table ones, leading to better plans. The predicate must match that used in queries. Depending on your settings there may be more system activity for stats maintenance. Of course this assumes the table contains (or could change to contain) some combination of columns on which rows can be meaningfully aggregated. 

A) Scale The middle tier can be scaled easily - hence the web farm concept. Scaling out the DB tier is much more difficult. While some products can do this it is not yet trivial and mainstream. B) Cost Typically web servers are common-or-garden boxes. DB servers, however, tend to be larger, more complex and more resilient. This all translates to "expensive." A recent employer estimated a CPU tick on the DB was ten times more expensive than one on an application server. C) Reuse Logic embodied in a stored procedure cannot simply be linked into, say, a stand-alone mobile app. Changing the SP affects every application which uses that DB, whether that application is ready for the change or not. D) Reuse Logic embodied in a stored procedure is common to all applications which use the DB. Programmers cannot side-step the rules at a whim. Changing the SP affects every application which uses that DB, ensuring consistency across the enterprise. E) Tooling There are more, and more sophisticated, languages, tools and techniques available for development in the application tier than there are in the database (from comments, with thanks). F) Network traffic Typically a business function will require many reads and / or writes. Often the output of one statement will be the input to a following statement. If SQL statements are held in the application each will require a network round-trip to the server. If the SQL is held in a stored procedure there will be a single network trip, sending the parameters in and receiving only the final result back; the network cost of the intermediate results is avoided. 

This way the network transfer will be a minimal part of the elapsed time and you can eliminate it from your enquiries. 

Fill factor applies to the leaf pages. PAD_INDEX will determine what happens to non-leaf pages. From the same BoL page: 

Change tracking offers the functionality you're looking for. When switched on, SQL Server creates additional internal tables. Changes to your data tables are noted in these internal tables. System functions allow you to pull changes out of these internal tables and transfer only changed rows. A related technology - Change Data Capture - allows you to see the "before" and "after" values of changes. 

If your DBs reside on a SAN or other shared storage that platform may have logs you can reference. For local storage I know of nothing held by default. A couple of years ago we started tracking DB size and disk free/used space. It has proved very valuable. A couple of Powershell scripts, a scheduled task and a small table has helped us avoid several incidents. Eventually we had enough data to make Monte Carlo prediction workable. 

Now to the body of the SP. If you use the names as the primary key in tables and the statements are straightforward. Assuming you have primary keys and foreign keys defined -- you should -- you have to insert in the correct sequence to respect these key definitions i.e. into and and only then into the mapping table. 

You could stop the passive node, drop it out of the cluster, unplug its LAN cable or otherwise disable the passive node before restarting the acitve services. The consequences could be .. um .. "intertesting", depending on how cleanly this process is handled and how skilled the tech team that configured the cluster were. 

As the two dates are compared inside a function this may prevent the optimiser using an index on the date. Changing to 

In this post the writer runs a query several times. I notice the logical reads vary a little across executions. There is a difference of about 2 pages in a total of a few thousand pages read. It seems clear to me from the context that there would be no write activity in between times. If the plan had changed I would have expected a larger variation than a fraction of a percent. Q: what factors will cause SQL Server to report different logical read counts for the same query in the absence of data writes? 

Apologies for the slightly Heath Robinson presentation. Hopefully the below will give you some pointers nonetheless. I'm using a "numbers" table. There are any number of aricles on the interwebs explaining what this is, why it's a good thing and how to get one efficiently. I take the columns to be and as . Other types would work with suitable adjustments to the respective functions. You don't say what version of SQL Server. I've written for 2008R2 since that's what I had to hand. Newer versions have nicer syntax for some of the things I've done. I've laid it out as a series of CTEs simply because that's how the ideas came to me and what I could manage in the time available. Undoubtedly further thought would give a prettier, simpler, more effieient structure. This would not be resilient against anomolies in . Any gaps or overlaps would cause all sorts of problems with my solution. Your examples had a job lasting, at most, overnight. This will cover a job of any duration. Anyhoo, thanks for the challenge! 

You could use an extended event to capture the end of the job. Then have that initiate your desired processing, perhaps via Service Broker and Activation. 

When creating a data model a good place to start is to have an entity type for each noun in the description of your problem. The attributes of these entity types will be the values you wish to store about those nouns and verbs in the description will become relationships in your data model. In the database the entity types become tables, the attributes are columns and relationships end up as foreign keys. Of course there are a lot of subtlties but this should get you going. Copying your description above into these terms you will have: 

By growing upwards the tree maintains identical depth in every branch. This is important for predictable performance. (Some say the B in B-Tree stands for "balanced" for this very reason.) 

Ball park - it will have to read 60Gb out of the log file, write 60Gb to the backup file, do some metadata operations for the log truncation itself and (optionally) move what's left of the log file if you shrink the ldf. For your average disk subsystem I'd expect you would be done and dusted in couple of hours, tops. 

No, the way you've written it is the "best" way. Unless, of course, that way doesn't work. The joy and frustration of using a declarative language is the optimiser. It is your best friend when it works and worst enemy when it doesn't. One way to kick the optimiser into doing the right thing is to re-write your query in a semantically identically way such as: 

The sub-query will run once per row in Events, so performance may be compromised for very large sets. Make sure there are indexes on Price.price_time. 

A simple approach would be to split Job A's step 8 (let's call them A8a and A8b). A8a checks B's status from the msdb tables and does a if B has not completed, or stops if B has errored. This gets a bit tricky in restart scenarios, or if history gets purged from the tables before A8a runs. You can avoid the busy wait by using a flag. This can be a simple one-row, one-column table. This flag is checked and set by both jobs like this: each job reads the flag. If it is set the other job has finished and step A8b can process. If it is not set, set it and end. Job A will need a new step at A7b to do this; job B will do this as its last step. If the flag is set job B can start A8b by using sp_start_job. You'll have to fiddle with A7's statements and "On success" / "on failure" actions to get it to work. You will need a new step at the start of the whole thing (A0) to ensure the flag's correctly set for each run. Effectively you've split your stream into 4 by doing this: A1-3, A4-7, B* and A8-10. It would be cleaner to set each up as a separate job, have A3 kick off both A4-7 and B* and then do the "is the flag set" thing at the tail of each of these parallel jobs before starting A8-10. Of course the real solution is to get a decent scheduler. Other posters have listed some 3rd party ones.