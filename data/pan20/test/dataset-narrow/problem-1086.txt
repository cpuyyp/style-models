There are a few different things you could mean by "prove that my typechecker works". Which, I suppose, is part of what your question is asking ;) One half of this question is proving that your type theory is good enough to prove whatever properties about the language. Andrej's answer tackles this area very well imo. The other half of the question is —supposing the language and its type system are already fixed— how can you prove that your particular type checker does in fact implement the type system correctly? There are two main perspectives I can see taking here. One is: how can we ever trust that some particular implementation matches its specification? Depending on the degree of assurances you want, you may be happy with a large test suite, or you may want some sort of formal verification, or more likely a mixture of both. The upside of this perspective is that it really highlights the importance of setting boundaries on the claims you're making: what exactly does "correct" mean? what portion of the code is checked, vs what part is the assumed-correct TCB? etc. The downside is that thinking too hard about this leads one down philosophical rabbit holes— well, "downside" if you don't enjoy those rabbit holes. The second perspective is a more mathematical take on correctness. When dealing with languages in maths we often set up "models" for our "theories" (or vice versa) and then try to prove: (a) everything we can do in the theory we can do in the model, and (b) everything we can do in the model we can do in the theory. (These are Soundness and Completeness theorems. Which one's which depends on whether you "started out" from the syntactic theory or from the semantic model.) With this mindset we can think of your type-checking implementation as being a particular model for the type theory in question. So you'd want to prove this two-way correspondence between what your implementation can do and what the theory says you should be able to do. The upside of this perspective is that it really focuses on whether you've covered all the corner cases, whether your implementation is complete in the sense of not leaving out any programs it should accept as type-safe, and whether your implementation is sound in the sense of not letting in any programs it should reject as ill-typed. The downside is your proof of correspondence is likely to be fairly separated from the implementation itself, so it'll only help prove the gross structure of your implementation is good, it may not help catch subtle implementation bugs. 

The class $PR$ (primitive recursive functions) strictly contains the class $ELEMENTARY = TIME(2^n) \cup TIME(2^{2^n}) \cup TIME(2^{2^{2^n}}) \cup \dots$. You say "Perhaps NP-hard problems are solvable with primitive recursion but not as efficiently." This is true. Even the total recursive function model of computation does not carry a notion of time complexity that agrees with that of Turing Machines. To see a simple example, how do you define the predecessor function in primitive recursive terms? You'd have to do something like: use the unbounded search operator to find the smallest $y$ such that $S(y) = x$. That is analogous to computing $f(x) = x-1$ with the function: $i = 0$ $While \, (i + 1 \ne x) \, \, \, \, i++$ Which takes $2^n$ time. Obviously, there is a better way! 

I found one related consequence. Let's say $NEXP$ contains $DTIME(2^{O(t)})$, where $t = n^{\omega(1)}$. It turns out this is just enough time to diagonalize against $P/poly$. Specifically, build the following machine: On input $x$ of length $n$, consider the $n^{th}$ Turing machine $M$. For every possible advice string of length $t$ and every possible bitstring $b$ of length $n$, run $M$ on $b$ with advice $a$, and reject after $t$ steps if you haven't accepted yet. Record your results in a table. This procedure runs in $DTIME(2^{O(t)})$. On input $0^n$, if at least half the advice strings cause $M$ to reject, then instead we define it to be correct for our algorithm to accept (otherwise, it is correct for our algorithm to reject). Any advice strings that caused $M$ to get $0^n$ wrong (that is, at least half the advice strings) now get thrown out of the table. We then repeat the process on input $0^{n-1}1$: if at least half the surviving advice strings cause $M$ to reject, then our algorithm will accept (and reject otherwise). Continue like this for all inputs of length $n$ (although really, only $t$ of them are needed - after that many inputs, we have thrown out all possible advice strings). Clearly this language can be decided in $DTIME(2^{O(t)})$, which we have assumed is in $NEXP$. On the other hand, it cannot be in $P/Poly$: the set of length $n$ inputs diagonalizes against the prospect of $M_n$ being used to decide the language. So we get $NEXP \not \subset P/poly$, which would be interesting. I'm going to leave the question open in case someone comes up with something else. 

If you accept the Curry–Howard correspondence then the question is mainly a philosophical one. "Are proofs and programs different? Of course. How? Well, we call proofs 'proofs' and we call programs 'programs'." Or to put it less flippantly, if there's an isomorphism between proofs and programs —which it seems clearly there is— then your question is asking whether there is any oracle capable of distinguishing the two. Humans categorize them as being different (for the most part), so it's certainly arguable that such an oracle exists. The important question then becomes whether there is any meaningful difference between them, which is up for philosophical debate. What is a "proof"? There is no formal definition of what constitutes a proof; it's a term of art, much like the notion of "effectively calculable" in the Church–Turing thesis. For that matter, "program" has no formal definition either. These are words of natural language used to categorize different fields of mathematical enquiry. What Curry and Howard observed is that these two different fields where in fact studying the same thing. Noticing this connection is important because it says that these different researchers should be talking to one another. But on another level, to notice the connection is to belie the difference between them. When tackling a problem, sometimes it is more beneficial to think of it as a programming problem, whereas other times it is more beneficial to think of it as a logical problem. This difference in perspective is, I think, the important difference between them. But whether a difference of perspective constitutes a difference of identity is a deep philosophical question which has been explored at least as far back as Frege's Ueber Sinn und Bedeutung. 

I want to remark that the answer is yes if you consider the input to be a clocked Turing machine, i.e., there is a clock that lets the Turing machine perform $p(n)$ steps and then accepts/rejects. Now checking whether the language decided by the machine is in NP is a syntactic property that boils down to deciding whether the machine is a well-formed nondeterministic Turing machine with a polynomial clock. 

With respect to exponential time complexity, general instances and instances with constant maximum degree are equally hard: The sparsification lemma of Impagliazzo, Paturi, Zane (2002) shows that $n$-variable instances of $d$-Sat can be reduced to instances of $d$-Sat with at most $f(d,\epsilon)\cdot n$ clauses in time $\exp(\epsilon n)$. As observed in joint work with Husfeldt and Wahlén, the sparsification lemma works for the counting versions of $d$-Sat, too, and especially for the case of counting $2$-Sat (which is equivalent to counting independent sets and counting vertex covers). Moreover, counting independent sets in an $n$-vertex graph cannot be done in time $\exp(o(n))$ unless the exponential time hypothesis fails. This is a yet unpublished observation announced in a talk during the Dagstuhl Seminar Computational Counting. 

Eickmeyer and Grohe (2010) prove that your candidate construction can be made explicit: take $d$ somewhat linearly independent linear hash functions $h_1,\dots,h_d$ and connect left vertices $v$ with right vertices $h_1(v),\dots,h_d(v)$. Eickmeyer and Grohe show that this construction gives $(k,\epsilon)$-expanders with left degree $d=k(t-1)/(2\epsilon)$, whenever $t$ is an integer, the left vertex set has size $n=q^t$, the right vertex set has size $m=dq$, and $q>d$ is a prime power. The hash functions $h_1,\dots,h_d$ are chosen in such a way that any $t$ of them are linearly independent.