This would mean that CRFs are a special case of MRFs. Definitions Markov Random Field Again, according to Wikipedia 

Let's say we want to predict the probability of rain. So just the binary case: rain or no rain. In many cases it makes sense to have this in the [5%, 95%] interval. And for many applications this will be enough. And it is actually desired to make the classifier not too confident. Hence cross entropy (CE) is chosen: $$H_{y'} (y) := - \sum_{i} y_{i}' \log (y_i)$$ But cross entropy practically makes it very hard for the classifier to learn to predict 0. Is there another objective function that does not behave that extreme around 0? Why it matters There might be cases where it is possible to give the prediction 0% (or at least something much closer to 0 like $10^{-6}$). Like in a desert. And there might be applications where one needs this (close to) zero predictions. For example, when you want to predict the probability that something happens at least once. If the classifier always predicts at least a 1% chance, then having rain at least once in 15 days is $$1 - (1-0.05)^{15} \approx 54\%$$ but if the classifier can practically output 0.1% as well, then this is only $$1 - (1-0.001)^{15} \approx 1.5\%$$ I could also imagine this to be important for medical tests or for videos. 

The trained model is stored; it is pretty fast to get the evaluation results (e.g. accuracy, confuscation matrix). 

In order to find the network, I tried a couple of different small networks: One hidden layer with two neurons This would be the function $$out = \max(a_1 x + b_1, 0) + \max(a_2x+ b_2, 0)$$ $$a_1 = 1, b_1 = 0, a_2=1, b_2 = -1$$ 

By applying and then one can get reflect and symmetric padding. Lasagne () (and probably also Theano): 

Are there better ways to capture the fact that two variables might be connected? I would like to use this in a script for exploratory data analysis, similar to 

Are the target values ordered? Then it is likely to be regression. Otherwise classification. See Level of measurement. Examples Regression 

Any train-test split which has more data in the training set will most likely give you better accuracy as calculated on that test set. So the direct answer to your question is 60:40. And 99:1 would even give better accuracy... However, at the end of the day, you are not interested in the accuracy on your test set. You are interested in the "real" accuracy, which gets estimated by your test set. And you better want to make sure that the test set can predict that accuracy well. So, which split should you pick? 

In Hintons talk "What's wrong about convolutional nets" (Late 2016 or early 2015, I guess) he talks about capsules to make a modular CNN. Is there any publicly available implementation or papers about this idea? 

LDA (linear discriminant analysis), SVMs with a linear kernel, and perceptrons are linear classifiers. Is there any other relationship between them, e.g.: 

Error Level Analysis as described Error Level Analysis, found at $URL$ seems to be one seems to be one way: You exploit that local compression ratios might be different. And it seems to be possible to train neural networks on it. I didn't find a paper which says how well this works so far 

Then '$F(x)$' is then that same possibly multi layer function, minus the residuals. The author of course hypothesizes that $F(x)$ is easier to learn than $H(x)$. So I think that in the figure the $F(x)$ is supposed to refer to everything in the Figure besides the ('$+ x$') part. Note how the F(x) symbol is centered with respect the the network, not attached to either layer. Then $F(x) + x$ references the entire $F(x)$ two layer network above combined with the skip connections. 

The mAP (mean average precision) = $\frac{1}{|classes|}\sum_{c \in classes} \frac{\#TP(c)}{\#TP(c) + \#FP(c)}$ If one wants better proposals, one does increase the IoU from 0.5 to a higher value (up to 1.0 which would be perfect). One can denote this with mAP@p, where $p \in (0, 1)$ is the IoU. But what does (as found in this paper) mean? 

I am thinking of the following image where we have two weights and an error (so we can make a 3D visualization). In this case a "long valley" looks like $x^2$ in the plane of the gradient, but in a perpendicular plane the function can still be minimized: 

They add shortcuts for the gradient. This way the first few layers get updates very quick, the vanishing gradient problem is no longer a problem. So you can easily make networks with a 1000 layers. See Residual Networks paper. There is nothing specific about classification / detection. Its about very deep networks. 

The good solution Or have one network with multiple outputs. See the master Thesis of Marvin Teichmann for example. $URL$ The obvious solution This one should not be done: have multiple classifiers 

Sklearn has a attribute, but this is highly model-specific and I'm not sure how to interpret it as removing the most important feature does not necessarily decrease the models quality most. Is there a model-agnostic way to tell which features are important for a prediction problem? The only way I could see is the following: 

Nice side effect: This method also automatically clusters similar classes together. At home, I will add a picture (Figure 5.12). 

It is just an arbitrary choice. You have to choose one number and the order of magnitude matters, but not the exact value. Powers of two just feel natural. If you don't think so: Evaluate it on a given architecture. Lower the number of neurons from a power of two to a smaller number. If the time increases, you've proven me wrong. 

Restricted Boltzmann machines are stochastic neural networks. The neurons form a complete bipartite graph of visible units and hidden units. The "restricted" is exactly the bipartite property: There may not be a connection between any two visible units and there may not be a connection between two hidden units. Restricted Boltzmann machines are trained with Contrastive Divergence (CD-k, see A Practical Guide to Training Restricted Boltzmann Machines). Now I wonder: How are non-restricted Boltzmann Machines trained? When I google for "Boltzmann Machine", I only finde RBMs. 

Suppose I know that I want to use a ResNet-101 architecture for my specific problem. There are ReseNet-101 models trained on ImageNet. Is there any disadvantage of using those pre-trained models and just resetting the last (few) layers to match the specific number of classes or should that be the default option? Please don't simply post your gut feeling... I have a gut feeling as well, but I want to know. 

Most of the time, mini-batch training seems to be used. So the answer is: No, the neural network learning algorithm is not online algorithm. 

I solved the CartPole-v0 with a CEM agent pretty easily (experiments and code), but I struggle to find a setup which works with DQN. Do you know which parameters should be adjusted so that the mean reward is about 200 for this problem? What I tried 

You can train like a usual classification / regression model and evaluate it the same way. Nothing changes, only the definition of the model. 

Bayes networks Bayes networks are directed acyclical graphs (DAGs) $G = (\mathcal{X}, \mathcal{E})$. The nodes represent random variables $X \in \mathcal{X}$. For every $X$, there is a probability distribution which is conditioned on the parents of $X$: $$P(X|\text{parents}(X))$$ There seem to be (please clarify) two tasks: 

This kind of works for linear functions (at least for the training data, not so much for the testing data outside of the range): 

According to my understanding, all optimization algorithms which are based on the gradient should be "trapped" at the saddle point like SGD is. Why does having a momentum term help in this case? (I'm sorry, I don't know who made those images. More of them are available.) 

I currently tried to figure out which paddings are directly supported by the frameworks: Tensorflow (): 

Image classification is the task of assigning one of $n$ previously known labels to a given image. For example, you know that you will be given a couple of photos and each single image has exactly one of $\{cat, dog, car, stone\}$ in it. The algorithm should say what the photo shows. The benchmark dataset for image classification is ImageNet; especiall thy large scale visual recognition challenge (LSVRC). It has exactly 1000 classes and a huge amount of training data (I think there is a down-sampled version with about 250px x 250px images, but many images seem to be from Flicker). This challenge is typically solved with CNNs (or other neural networks). Is there any paper which tries an approach which does not use neural networks in LSVRC? To clarify the question: Of course, there are other classification algorithms like $k$ nearest neighbors or SVMs. However, I doubt they work at all for that many classes / that much data. At least for $k$-NNs I'm sure that prediction would be extremely slow; for SVMs I guess both fitting and prediction would be much to slow (?). 

It is commonly seen as something bad if the decision boundary of a neural network is too sharp, meaning if slight changes in the input change the class prediction completely. Given a trained CNN, is it possible to measure / calculate the "sharpness" of its decision boundaries? Did somebody do that already? 

I recently wrote this as a list of what I have seen / can think of as problem sources that make it hard to reproduce (replicate?) an experiment. I think I have seen most of them, except for hardware errors. My thought was this: A typical computer vision experiment might run a training on a GPU for many hours / several days. Even if a bit-flip happens only once every $10^9$ FLOPs, that would be 11500 errors per second with an Nvidia Titan GTX 1080 Ti. I don't know how this error would affect later calculations (how the problem is numerically conditioned). So: Are there any reports on Hardware errors affecting experiments? (Blog posts, journal articles, posters?) 

Dynamic Bayes networks I guess dynamic Bayes networks (DBNs) are also directed probabilistic graphical models. The variability seems to come from the network changing over time. However, it seems to me that this is equivalent to only copying the same network and connecting every node at time $t$ with every the corresponding node at time $t+1$. Is that the case? 

When reading about SVMs (e.g. on the German Wikipedia) there is a sentence like "an svm is a large-margin classifier). Are there other large margin classifiers than SVMs? 

The curse of dimensionality means that your intuition fails at a certain number of features. See Average Distance of Random Points in a Unit Hypercube for some examples. It does not matter how many points you have. For example, at around 100 dimensions every two points randomly sampled points are really close. 

Explaining the prediction of a black-box model by fancy occlusion analysis (from "Why should I trust you?"): 

They are both about visualizing features being learned in CNNs. Although both papers have an introduction where you can read things like: 

What is an "residual output error"? Is $V_p$ simply the activation of the unit given the pattern $p$? What does the term $S$ mean and why do we want to maximize it? 

In the case of neural networks, the loss function depends on the weights and the biases (which are often not mentioned extra; they are also weights). The loss function itself is parametrized heavily. Its parameters is the data (input and labels). Its variables (by which the derivatives are calculated) are weights. 

Dividing by 0: Add a small number to the denominator Logarithm of 0 / negativ number: Like dividing by 0 

Neural networks get top results in Computer Vision tasks (see MNIST, ILSVRC, Kaggle Galaxy Challenge). They seem to outperform every other approach in Computer Vision. But there are also other tasks: 

I was working on classification problems $$E(W) = \frac{1}{2} \sum_{x \in E}(t_x-o(x))^2$$ where $W$ are the weights, $E$ is the evaluation set, $t_x$ is the desired output (the class) of $x$ and $o(x)$ is the given output. This function seems to be commonly called "error function". But while reading about this topic, I've also seen the terms "criterion function" and "objective function". Do they all mean the same for neural nets? 

This is the wrong approach. Choose your tools to solve tasks, not because they sound cool. Imagine a craftsman saying "I want to use a hammer". (However, machine learning might still suit to this task.) 

(What this can't do is to find connected features: Some features might be not exactly the same, but have a common underlying cause which is important for the prediction. Hence removing either of them doesn't change much, but removing both might change a lot. I ask another question for that.) 

I've just read The Cascade-Correlation Learning Architecture by Scott E. Fahlman and Christian Lebiere. I think I've got the overall concept (or at least the "cascade" part - a 4min YouTube video how I think it works): 

Networks like VGGNet have huge numbers of parameters (see Appendix D for details, but it's something like 135 million). Training such a big network takes a lot of data and a lot of time. There is ImageNet, which has 1000 classes and many thousand images. For ImageNet, some people already trained VGGNet and provided the parameters. One way of finetuning is to replace only the last layer (about 4 million parameters) and add a new layer which has the number of classes you need. Then you can freeze all other weights and train on your data. Hence you will have lower training time and you will likely get better results (at least on similar datasets ... for datasets which look very different, there seems not to be documented conclusive evidence) 

I would say the graphic describes $$\varphi \left (W_2 \varphi(W_1 x) + x \right ) \qquad \text{ with } \varphi = ReLU$$ The $\mathcal{F}(x)$ does not make sense to me. Assuming both weight layers are simple MLPs without bias, where the first one has a weight matrix $W_1$ and the second one has a weight matrix $W_2$, what is $\mathcal{F}$? In the text, they define $$\mathcal{F}(x) := \mathcal{H}(x) - x$$ where $\mathcal{H}(x)$ is "the desired underlying mapping" (whatever that exactly means). Also, equation (1) seems strange to me: $$y = \mathcal{F}(x, \{W_i\}) + x$$ In figure 5 they have two weight layers and call this a building block. Why is there only one weight matrix in this equation? My thoughts I think the authors could mean $$\mathcal{F}_i = \varphi(W_i x)$$ In that case, in the image where $\mathcal{F}(x)$ is it should be $$\mathcal{F}_1(x) = \varphi(W_1 x)$$ and where $\mathcal{F}(x) + x$ is should be $$\mathcal{F}_2(\mathcal{F}_1(x)) + x = \varphi \left (W_2 \varphi(W_1 x) + x \right )$$ 

Input: $x \in \mathbb{R}^+$ Expected output: $f(x) = \begin{cases}x &\text{for } x \leq 5\\10 - x &\text{for } x > 5\end{cases}$ So basically you have a neural network with one input neuron and one output neuron. Let's say the output has the identity as an activation. What is the simplest network with ReLU activations that fits this function? 

The sampling bias problem is that your test set is likely not the complete set of things you're interested in. So, no, you can't simply check MSE_1 < MSE_2 and conclude it is always the case when it's "just" for your dataset the case. This is what significance tests are for. (Although this kind of reasoning is super common in machine learning and I did it myself already ) Then the question if the metric is the correct one for your application. Typical choices are: MSE, mean absolute error, custom cost functions