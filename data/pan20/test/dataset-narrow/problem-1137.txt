Perhaps an even better way to see type inference is as a specialization of a single framework: Abstract Interpretation (abbreviated AI). The hallmark of most unification-based type inference algorithms is that they generate the principal type for a term; translated into AI terms, this means that you never need to widen, nor do you need to go to power domains to find the answer. Unification turns out to be the 'join' operation -- there is also a nice categorical interpretation of unification which is even more enlightening. Several comments shed light on this as well: all algorithms can be better understood in terms of generating then solving systems of constraints. Unification is indeed eager, and constraint-solving (and that includes control-flow based approaches) tend to be lazier. Perhaps an even better way to see type inference is as an alternate interpreter for a program: rather than producing values, it produces types. It is very easy to see type inference as a form a partial evaluation. Perhaps an easy introduction to that circle of ideas is via Aaron Stump's A rewriting view of simple typing. 

There are other items too, but I have not gotten to reading that part of the Agda formalization yet... But by and large, most of HoTT can be nicely formalized in both Agda and Coq. More importantly, both teams of developers are actively working on adapting their systems so that more of HoTT can be handled, at least whenever there is a clear theory of how to implement the needed features. That has turned out to be challenging in parts. 

Schaefer's dichotomy theorem shows that each CSP problem over $\{0,1\}$ is either solvable in polynomial time or is NP-complete. This applies only for CSP problems of bounded width, excluding SAT and Horn-SAT, for example. General CSP problems of unbounded width may be very difficult (even uncomputable), so let's restrict ourselves to problems which are "natural" and are in NP. Given a CSP problem of unbounded width, for each $k$, we can look at the restriction of the problem to clauses of width up to $k$. Schaefer's theorem now applies, and the restricted problem is either in P or NP-complete. If for some $k$, the $k$-restricted problem is NP-complete, then so is the unrestricted problem. The situation is less clear when for all $k$, the $k$-restricted problem is in P. Schaefer's dichotomy theorem relies on four (or so) different algorithms that solve all the easy cases. Suppose that for a given CSP problem, the $k$-restricted problem is always solvable by algorithm A. It might be the case that algorithm A can be used to solve the unrestricted problem as well. Or it might be that algorithm A isn't polynomial time in the unrestricted case, and then we're ignorant as to the hardness of the problem. Has this kind of problem been considered? Are there examples in which we arrive at the "ignorant" spot? 

A kind reviewer sent me a paper which proves exactly the same lower bound I do, in exactly the same way. The paper is 

Most proof assistants have a formalization of the concept of "finite set". These formalizations, however, differ wildly (although one hopes that they are all essentially equivalent!). What I don't understand at this point is the design space involved, and what are the pros and cons of each formalization. In particular, I would like to understand the following: 

Does anyone know if the verified software repository is still alive? Bar that, are there other repositories out there of verified (or just certified) software? The archive of formal proofs is nice, but not quite 'right'. Finding singular examples of verified software is possible (with seL4 and CompCert being obvious examples), but finding whole groups of them is hard. I would have expected to be able to find whole repositories of certified software components by now, but I have been unable to do so. [While not TCS per se, the amount of underlying TCS issues involved in software verification/certification is enormous, so hopefully this will be considered to be on topic] 

The Canadian CS community is seriously under-server and under-organized. The ACM (in the US) completely overshadows us, and since it (nominally anyways) tries to be international, that leaves little room for Canadians to get organized. So typical community-driven issues, like aggregating jobs, does not happen. The community is (now) large enough to be able to sustain such an organization (I know, I belong to the CMS, the Canadian Mathematical Society, and it manages just fine, and the sizes involved are in the same ballpark). I realize that this answer is a bit of a non-answer. But the (unfortunate!) answer to your question is 'no'. So rather than just saying that, I figured I would elaborate. 

This should be #W[1]-hard by a standard interpolation argument. Here is a rough sketch. First, consider the multicolored version of the biclique problem: given a graph whose set of vertices is partitioned into classes $X_1,\dots, X_{2k}$, find a biclique containing exactly one vertex from each set. Unlike Biclique, whose FPT status is open, this multicolored version is known to be W[1]-hard: there is an easy reduction from clique. I believe it should also be #W[1]-hard. Given a graph $G$ and partition as above, let us obtain a new graph $G'$ by replacing every vertex of $X_i$ with an independent set of size $x_i$ (and replacing each edge between $X_i$ and $X_j$ by an $x_i\times x_j$ biclique). Now the number of $k\times k$ bicliques in $G'$ is a function of the $2k$ variables $x_1,\dots,x_{2k}$. In fact, one can see that this function is a polynomial of degree at most $2k$ and the coefficient of the term $x_1\cdot\dots\cdot x_{2k}$ is exactly the number of multicolored bicliques in $G$. Thus by substituting sufficiently many combination of values into the variables $x_i$ and counting the number of bicliques in $G'$, we can evaluate this polynomial at sufficiently many places to recover its coefficients by interpolation. 

Determining the winner of a "parity game" is known to be in $NP\cap coNP$, but it is an outstanding open problem whether it is in $P$. See e.g., $URL$ Note, however, that a parity game is defined by an annotated directed graph, so you might not want to consider it a "natural graph problem." 

The other reason (than the one already stated) for are the (all too often unstated) requirements that 

As far as I understand, in Agda it is possible to represent all of that (i.e. all of Chapter 2 -- there is a library on github which does; AFAIK, the same is true of Coq). It is only when you get to later chapters that things get dicey. There are two obvious items: 

It is that last requirement which really 'seals the deal' in forcing , since that is the choice which minimizes the distance to not being monotone decreasing. 

You really should read Gowers' essay carefully - it cleanly details the reasons why you need a basis in general. So if there is going to be an algebraic account of Gauss-Jordan, it will necessarily involve some additional conditions. You can get an idea of the kinds of conditions by reading the n-lab page on dual vector spaces. There it shows that there are a lot of spaces which are isomorphic to their double duals. As you mention, this is related to double-negation elimination. But let's spell that out. The isomorphism is linear CPS transform, as it is the set of functions $\left(V\multimap R\right)\multimap R$. You are seeking `unCPS'. Which highlights how miraculous the structure of $V$ must be for this to be feasible at all. The page on the n-lab gives some of these miracles. [Turns out that, syntactically, this works for the duality measure $\leftrightarrow$ linear functional on programs if you choose your programming language "just right"; if you're curious, see my work with Chung-chieh Shan at PADL 2016, where we implement $\texttt{unCPS}$]. Another way, more computational, to look at this is to look at more general versions of LU decomposition. For LU with full pivoting, you can decompose $A$ as $PAQ = LDU$ with $P$ and $Q$ as permutation matrices, $D$ diagonal, $L$ unit lower triangular and $U$ unit upper triangular. You can generalize even more to various kinds of rings (see Fraction free factors by Zhou and Jeffrey). The point is that $P$ and $Q$ are arbitrary, but $L,D,U$ depend on them -- another way to see the base dependence. Clever choices of $P$ and $Q$ allow you to deal with whatever defects the expression of $A$ in the original basis were, and 'rotate' things in better position -- be it for stability, sparsity, expression size blowup, etc. 

The number field sieve has never been analyzed rigorously. The complexity that you quote is merely heuristic. The only subexponential algorithm which has been analyzed rigorously is Dixon's factorization algorithm, which is very similar to the quadratic sieve. According to Wikipedia, Dixon's algorithm runs in time $e^{O(2\sqrt{2}\sqrt{\log n\log\log n})}$. Dixon's algorithm is randomized. All (heuristically) known subexponential algorithms require randomization. Dixon's algorithm needs to find integers $x$ such that $x^2 \pmod{n}$ is smooth (can be factored into a product of small primes) and "random", and the number-field sieve has similar but more complicated requirements. The elliptic curve method needs to find an elliptic curve modulo $n$ whose order modulo some factor of $n$ is smooth. In both cases it seems hard to derandomize the algorithms. The nominal worst-case complexity of all these algorithms is infinity: in the case of the quadratic sieve and the number-field sieve you might always be generating the same $x$, while in the elliptic curve method you may always be generating the same elliptic curve. There are many ways around this, for example running an exponential time algorithm in parallel. 

Consider a BFS exploration process, which proceeds in $k$ stages. Put $V_0 = \{u\}$. Given $V_0,\ldots,V_i$, explore all edges from $V_i$ to $V \setminus \bigcup_{j=0}^i V_j$ (where $V$ is the set of all vertices), and set $V_{i+1}$ to consist of all vertices reached in this fashion; their number has a binomial distribution which can easily be calculated. After $k$ steps, check whether the vertex $v$ belongs to $\bigcup_{j=0}^k V_j$. Note that this process is exactly the same in both the undirected and the directed case. Hence the answer, whatever it is, is identical for both models. Presumably in the undirected case the answer is known and can be looked up. Otherwise you can try to estimate it by estimating the sizes $|V_i|$ and so the probability $\frac{1}{n-1} \sum_{i=1}^k |V_i|$ that $v$ belongs to $\bigcup_{i=1}^k V_i$.