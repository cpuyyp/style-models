Which in your case gets you to like 5000 neurons, but since you are oversampling your classes, I would also suggest under-sampling instead, and that gets you maybe to around 1024, which is fairly standard. The [67, 512, 5] configuration is pure gut reaction. 

From what I have seen dropout for LSTMs should not be so high as 0.5 Recommendations are 0.1 or less. This paper - Where to Apply Dropout in Recurrent Neural Networks for Handwriting Recognition has a detailed study on how to use Dropout with RNNs, and one of the recommendations is that 'it is almost always better before the lstm layer than inside or after it' (pg 3), so you can try that. However, I believe this depends on kind of data you have. I remember reading a paper that had dropout for LSTM only being useful for a large LSTM (like 4096 unit x 4 layers). I cannot find it now, but in this one, the authors suggest something similar - showing dropout having better results on a 1500 unit x 2 layer lstm than a 650 unit by 2 layer, while the smaller network would just overfit. The lesson I guess depends on how big your lstm is and what you are trying to achieve - the use case being preventing overfitting while increasing model size. 

At 3:30 in video, he mentions this switching. What he is referring to by 'switch around' is that he is specifying the size of the LSTM being used. The 1024 refers to number of cells (or size) of the LSTM, and the 3 is the number of layers. In the tutorial page being referenced I the snippet he is referencing is: 

Regarding the second part, since the first operation you are really doing is subtraction, it implied the tensors for input and output are the same size. This can be found if you check the implementation code - which is better than the docs in this case - go to line 3056: $URL$ 

Another way of writing this part would (which I think is more illustrative of what's happening, but probably less performant in Keras land): 

Agreed with Fadi. It is just not very efficient. You have to split up the input and weights tensor, into tensor.size()/k chunks and then do a separate mult,add operation on each pair of chunks. Even if you had an efficient indexing scheme of concatenating the chunks to mul-add in one go, you would then have to un-index output to a flat batched tensor. A few things to think about would be pathnet - pytorch and this (I have a half-working implementation somewhere if you are interested), but both are variations on the efficient version of what you are talking about, which is some form of routing. 

Agreed with Emre. One thing that can be helpful when asking the question is to look at comparable datasets since there's usually some around. For spectrograms for example, there are 150,000 samples w/ 12 labels here, 2890 samples here, and 1 million samples with various labels here. Find a few examples and gauge a rough order of magnitude of samples per class given how close the tasks are to yours. That should give you at-least a starting point. Then, if lets say one set is very large, usually someone will have written a paper using that set, and that can help start both the architecture and size jumping off point (and maybe a baseline for transfer learning) of your task. 

Note 1: I believe the technical term for what you are looking for is '(sub)word salience'. First thought (which is not the best method, but could be worth a quick try if your examples are smallish) - Run the text with all words through fastText, and get the baseline most likely labels with probabilities (predict-prob function). Then remove each word that you are interested in testing, (maybe removing common words like 'a' 'the' etc), and compare the predicted probabilities of each obtained set of predicted_probs. The ones that give the greatest difference between baseline and missing wordX can be interpreted as the most informative. A permutation of this idea is to get the embedding for the interesting words, and then get a distance measure (I assume cosine distance) to the labels you have. Then you have a problem, as they may be distributed evenly across labels in terms of distance, but if there is a group of words that are close to a particular label, that can be interpreted as informative. I am sure there is a better technique to do this, so I am curious as well :). You may want to check these out (but they use additional modeling to get salience) Learning Neural Word Salience Scores. A NEW METHOD OF REGION EMBEDDING FOR TEXT CLASSIFICATION - this deals with the exact problems of how to identify the useful bits of an input text 

If you are using a RNN and the 5000 features refers to size of your dictionary, I would recommend you consider an embedding of your words, eg word2vec. Each word would then result in a maybe 200-300 - sized vector, to which you can concatenate the single smile feature, and then run that through your RNN. If that is not doing much, you can run the embedded text through an RNN, and then take the result of that (lets say 10-20 features per time-step) and concatenate the smile feature, and run those through a second RNN. The idea is the first one is a further compression over time, and the second is your 'final' RNN which works at a higher level of abstraction. In general, this is a classic case of combining a high-dimensional feature set with a low dimensional one, to which there are many approaches, the question being, when do you join the features. Take a look at this paper, which covers a few ideas on multi-modal features aka when and how to merge them (diagram 3.2), though I think it is more instructive than useful since their features are more rich than what you have with the single additional feature. Another thing to try is to soften the 'smile' feature if it is not so already, aka if it is a 0 or 1, make it a random (0, 0.1) and (0.9, 1) respectively. 

T-SNE is another dimensionality reduction algorithm not mentioned in the article in the other answer. Used for VERY high dimensional data, if you have trained some embeddings for your dataset. Reference Here . Python standard library here. cheers 

You do not have enough parameters in your hidden Layer to learn. Try something like [67, 512, 5] or expand deeper with something like [67, 1024, 256, 5]. The idea is that your hidden layer may be too small to capture interactions between the attributes given the amount of training data you have. 

Lets say a paper is published which describes a data science algorithm, and the paper is made available on arxiv (no patent or anything else mentioned in paper). The paper is by a university researcher, and the algorithm is not something well known. A. Is it considered ethical to use my own implementation of that algorithm for: 

In the case of face detection in particular it can get very use case specific: 1a) Let's say I am facebook, and I have a billion possible faces of users to tag. With the one-shot approach, I am storing a representation of each new face as you suggest, I assume it is something like One-shot Learning with Memory-Augmented Neural Networks. Now I have a problem with a 'memory' that there are billion faces, and that is a lot of computation to do for lookup. The representations learned must increase in size w.r.t to the dataset in order to be able to distinguish between instances, so I get hit again. 1b) With Deepface, which uses the siamese architecture you mention I have the advantage that to classify a new user, I just have to run it on maybe 1k images of friends around the network where that image comes from. I do not have to retrain the network if a new human is found, since after the initial training, there is not such a problem with doing a comparison between images, as the network is doing a one-vs-one, not a one-vs-all query. The network is not learning classes, but representations and the differences between representations that are most indicative of two examples being different classes. 2a) Now let's say I am a modeling agency with 800 employees, and I have 1000 examples per person. In that case, the one shot learning approach is feasible, especially if you have downloaded a pretrained network, and do the fine-tuning as you suggested. There is close to no cost for any projected growth (the one-shot learning paper trained on a 4million example dataset). 2b) In this case it may be feasible to finetune a network with just a final layer with 1000 logits (800 for employees, and a 200 for growth). This would be totally infeasible for the billion or even million classes (individual humans) - but still probably not advisable. In short, it depends on how many examples per class you have and the number of classes you have. Many architectures out there only work when you have millions of images for relatively few classes (this applies to all the imagenet entries). Others are designed to learn from 10k examples or less, with 50 examples per class, and yet others are designed for not explicitly using the notion of classes, when even that becomes too big. For a comprehensive review of face detection check out Deep Face Recognition: A Survey. New on arxiv 4/18/2018 

EDIT : Deep Face Recognition: A Survey New on arxiv 4/18/2018 looks like best survey of methods over face related tasks :). Beyond Facenet, there are a few approaches which may be good to look at how many faces do you intend to have your system know about - aka netowrks that have a network to directly output which face it is (~ <10K), vs a feature map for clustering (~ 10k to 100K), or comparing any 2 faces (~ >100K) - below are examples of approaches to each... This paper just came out: Exploring Disentangled Feature Representation Beyond Face Identification- 2018 - Reported Accuracy 99.816 . Use an encoder-decoder like scheme to compute features of a face. Then given all the faces you computed this on, do clustering to find which ones are the same face (TSNE - distance in feature space). This paper is cool since it also gives features of each face like 'smiling', and these are used to augment the search. Along similar line is this Before that (other than Facenet) - DeepFace - Accuracy 97.35. Its the facebook lib. If its state-of-the-art enough for them, its state of the art enough for me. Approach is given two images, put them into siamese network, for first detect, then 3d model the face, then project to 2d featuremap, which then combine to label saying whether they are same person or not. Robust Face Recognition via Multimodal Deep Face Representation - 2016, Reported accuracy 98.43. This is interesting because they trained on a relatively small dataset (CASIA WebFace). However this has the last layer being the number of identities in the dataset - so this could be a llimiting factor if you want to recognize millions of identities like facebook does. Otherwise, this looks easiest to implement/mess with. Patch-based Face Recognition using a Hierarchical Multi-label Matcher - not sure what is going on here, but looks interesting. I would think another limiting factor for you is how many examples do you have per face - eg do you have 4K identities and 4mil images like the facebok dataset, or 10k identities and .5mil images (CASIA WebFace), or LFW with like ~5K identities and ~15k images.