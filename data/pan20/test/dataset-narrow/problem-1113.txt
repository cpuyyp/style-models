The only advanced data structures book that I'm aware of is the one by Peter Braß (Advanced Data Structures). It's not a bad book, but I'm not convinced that it's truly advanced at the graduate level. 

The CRA jobs site is still the best place, although it's not theory-centric. Given the proliferation of postdocs in theoryCS, I think it would be nice to have a dedicated place for theory job postings. 

While there are many heuristics (arguably all of them) that take exponential time in the worst-case, what usually makes them attractive (and marketable) is that they "appear" to perform much better in practice, and in fact it's hard to find examples where they are provably exponential. Two canonical examples are the simplex algorithm for linear programming and the $k$-means heuristic for clustering. It seems highly unlikely that you'll be able to show a worst-case running time that's better than exponential for a GA-like heuristic. As Tobias points out, it would be useful to know if there's some kind of guarantee of quality if the heuristic is terminated early - but I'm pessimistic that this can be done as well. 

There's a little bit of confusion that has been introduced into the question by Christoph's comment (and mjqxxxx's comment on the question reflects that). If you just want to map the nodes of X into a weighted tree endowed with the shortest path metric (as the question states), then you're asking about testing whether a metric is a tree metric. I don't know whether this can be done faster than using the 4-point condition. However, if as Christoph's comment indicates, you want to consider embeddings in which the vertices are leaves, then you're asking (sort of) if the metric is an ultrametric. Ultrametrics are a little stronger, because the root-to-leaf distance for ALL nodes is the same, but it's also easier to verify whether a metric is an ultrametric via the MST. See Anupam Gupta's notes for more. 

Yes. But as you get more experienced, you're able to "fail fast" - learn how to test the idea quickly to see if it passes a 'smell test'. 

This is a suggestion for some directions, rather than an answer. The problem you ask seems closely related to the problem of polygon simplification (here's one possible starting point for tracking references) which comes in two flavors. The set up there is that you're given a polygon (or a curve for that matter) and you wish to simplify it. Your parameters are the error $\epsilon$ and the number of segments in the simplification $k$, and you can fix one or the other (the problem structure and solutions change accordingly). If we consider the plane for now, here's an idea towards an approximation algorithm. Let's construct a subroutine that takes fixed parameters $\epsilon, k$ and determines whether an $\epsilon$-kernel exists. We will then binary search to get some approximation of the true $\epsilon$. We can do this because all we need to do is approximate the convex hull of the input point set, and if this convex hull size is strictly smaller than $k$, we can place a lower bound on $\epsilon$ (since at least some adjacent pair of edges in the hull will need to be shortcircuited). So how to create the subroutine ? Firstly, if $\epsilon$ is the correct answer, then you know that the kernel's convex hull must lie within a "shrinking" of the original hull by $\epsilon$ (formally, by running a minkowski sum with a hemisphere of radius $\epsilon$ on the inside). Compute this region. I'll then claim that a greedy strategy should work to verify whether you can solve the problem with $k$ points: the intuition is that convexity should ensure you don't need to backtrack. This idea comes from the polygonal simplification land. Note that even if the above works, and I'm not more than 60% sure it will, it says nothing about hardness. 

What you've defined is the vertex cover problem for 3-uniform hypergraphs. Khot and Regev showed that if the unique games conjecture is true, then it is NP-hard to get an approximation of $3-\epsilon$ for this problem. 

In the realm of sublinear space, there's no explicit class of problems that admit a sublinear space solution, but there are large classes of problems (frequency moment estimation, dimensionality reduction etc) where the existence of a small "sketch" can be shown and this leads to efficient algorithms. But in this space as well, the algorithms are all randomized, and there are strong deterministic lower bounds mostly based on communication complexity. 

This is a very indirect answer, and might not quite address your concern. But FPT and the W hierarchy are closely linked with approximability (FPT problems often have PTASs etc). In that context, note that for any graph, VC = n - MIS, and so an approximation for VC doesn't give an approximation for MIS. This is why you need L-reductions for approximations. I suspect there's an equivalent "kernel-preserving reduction" notion for parameterized complexity as well. 

I am also not sure if this is what you are looking for, but along the lines of mjqxxxx's answer, NAE-SAT always has an even number of solutions because the complement of any satisfying assignment is also satisfying. 

I didn't see the original question, but the new cstheory blog might be a good place, once we have threaded comments 

In your case, the red points mark the holes, and the blue points mark the interior of the polygon(s). In general, two blue regions might be disconnected, but then the minimum equals the sum of the minimum covers for the two disconnected regions. Your problem can be easily mapped to the rectangle cover problem, and conversely the rectangle cover problem can be mapped to your problem assuming that all vertices of the polygon are on a grid, which can be assumed w.lo.g. The bad news is that the minimum rectangle cover is NP-hard. It's also fairly hard to approximate in general (the best current approximation ratio is $O(\sqrt{\log n})$. However, if you had more information about the red and blue points, it's possible you might fall into one of the many special cases of rectangle cover that are easier to solve. 

You have to be careful with some of the methods. Specifically, k-means and k-medoids are iterative schemes that you run "till you're done". SO it's not meaningful to talk about the overall running time of these schemes, but it is meaningful to talk about the running time of a single iteration. For $k$-means, it's easy: each iteration takes $O(kn)$ time to identify the nearest centers and do the new center computation. For $k$-medoids, the new-center computation can take longer: that depends on what procedure you use, and if $k$ is small, the computation of new-centers can dominate the asymptotics. Update: As JeffE points out below in comments, there are certain variants of $k$-means that do have guaranteed convergence in polynomial time, while also yielding quality answers. There's nothing that I'm aware of for $k$-medoids though. 

I don't understand the motivation very well. However, let me provide an answer to a related question. In the property testing framework, you are given two graphs $G$ ad $H$ and wish to distinguish two cases based on parameter $\epsilon$: 

It seems to me that a better approach is to frame this as a large optimization problem and throw it into CPLEX or something like that. 

This is a very vast topic. For nonlinear optimization the book by Bertsekas is a standard reference. For convex optimization you should look at Boyd and Vandenberghe. For linear programming, the book by Schrijver is a reasonable (but dense) source. But you should also look at course material. Each of the authors mentioned above also has course material/lecture slides that might be useful. 

it is known that given a graph G and a tree T, it can be verified in linear time that T is a minimum spanning tree of G. But we don't yet have a deterministic linear time algorithm to compute the MST. Of course the gap is tiny (1 vs $\alpha(n)$), but it's still there :)) 

In the realm of constructive solid geometry, questions like the one you asked can probably be answered. Here, solids are constructed by unions and differences of basic shapes (for example, a sphere with a tunnel can be described as Ball - Cylinder. Shapes are then built up as trees of operations on basic shapes (at the leaves), and the differencing problem can then be thought of as a labelled tree-diff problem (which has been examined among other things in the context of comparing XML documents). I'm not claiming that these questions are already being studied, but CSG is definitely one place where the geometric questions you're asking might be addressed. 

If you merely want separation, then this is solved using linear programming. If you want to maximize the separation, then you're in the land of linear classification problems and max-margin classification in general. 

As Per Vognsen points out, and more generally as well, there are many geometric algorithms that operate as follows: Pick a random sample, and run recursively on the sample and on other structures derived thereof. Clarkson's randomized algorithm for linear programming, as well as Seidel's, and indeed the Matousek-Sharir-Welzl series that Per mentions, all operate in this manner, and Clarkson's paradigm extends to other situations where you build some kind cutting or epsilon-net and recurse. Unfortunately, you're unlikely to get a new result from this, because there are optimal derandomizations of these algorithms, due to work by Matousek, and Chazelle. Chazelle's paper is a good reference point for this work and prior work by Matousek. But it might be a good test of your method: it was hard to come up with these derandomizations, and if your method provides a black box approach starting with the (easier) randomized algorithm, that would be neat. p.s this is probably the most boring example possible, but does your method work on quicksort, or any of the randomized median finding methods ? 

The paper presents a reduction from factoring to a lattice problem. It doesn't go on to claim that the lattice problem can be solved in (probabilistic) polynomial time. My understanding is that Schnorr's assertion instead is that fast implementations for finding short vectors in lattices (independently studied, like LLL etc) can then be employed for fast implementations of factoring solutions (akin in spirit to how SAT solvers can often be used as a fast subroutine for solving other hard problems) 

Related to the DNF/CNF formulation of SAT is the H-rep vs V-rep representation of a polytope for integer programming. If I give you all the vertices, IP can be solved in linear time. But the number of vertices is exponential in the number of constraints, which is the usual way the IP is presented. So the answer to your question is YES. In the case of knot representation, the question sounds interesting, and maybe you could do a specific followup question on the knot problem directly. 

This is a somewhat Euro-centric issue. In the US, there aren't such things as "Ph.D positions". People merely apply to grad school where theory folks are. 

Be careful about intuition. It comes with a great deal of experience, can often be wrong and right at the same time, and is not unique. The point is that everyone brings their own intuition to problems based on their own comfort zones, the needs of the problem, and their background. As Tsuyoshi points out, intuition is really a lot of hard work that's been sublimated into a few concise mental images. So my suggestion would be: just work on problems you enjoy, and try to develop your own ideas even if there's other work out there. You'll build intuition that way. And if a result seems puzzling, it either means you haven't quite understood it yet, or maybe there's a simpler result lurking somewhere beneath, waiting to be uncovered. 

There's a rather large literature on this problem. In brief, the problem you're looking at is called the $1$-median problem for a metric space. Since you're allowing for things "near" the middle, I'll assume approximations are ok. I also don't know what you mean by space less than $n^2$, since the input size itself is $n^2$. Having said that, for an arbitrary metric space you can get a "sublinear" approximation (i.e one that runs in $o(n^2)$ time. Specifically, Piotr Indyk showed that you can get a $(1+\epsilon)$ approximation in time linear in $n$ and polynomial in $1/\epsilon$. For structured spaces you can also do quite well. In Euclidean space, core-set techniques for the $k$-median problem (of which the 1-median is a special case) give you similar bounds (linear in n) to get arbitrarily good approximations. The caveat in this case is that the solution you get might not be one of the input points. 

this is too long for a comment, but it's not an answer One reformulation of your problem might prove to be useful in terms of finding related work. Note that $f$ and $g$ essentially define probability distributions over the $s_i$ and $t_i$. If we treat the $c(e)$ as a "length", then the mincost flow is basically the earthmover distance between the two distributions $f$ and $g$. Now your maximization over $f$ is really saying: find the farthest neighbor to $g$ lying in a box defined over the simplex. Formulating the problem this way suggests that you want some kind of projection from $g$ to the box. 

The paper that people usually cite is Almost Optimal Lower Bounds for Small Depth Circuits, which appears in STOC 1986. The main result pertaining to your question is: 

Again, it would be unfair to single out a single paper, also because it's not exactly clear which model you're working in: best to google 'norm estimation' or 'column sampling' to find some of the related results. This survey by Mahoney is a useful reference for sampling methods. 

Some of us have been reading Michael Nielsen's paper on a geometric approach to using quantum lower bounds (in brief, the construction of a Finsler metric on $SU(2^n)$ such that the geodesic distance from $I$ to an element $U$ is a lower bound on the number of gates in a quantum circuit that computes $U$). I was wondering if there were concrete examples of problems where this program led to a lower bound that came close to, matched or beat prior lower bounds obtained by other means ? 

As I mentioned in the comment, Ipe and/or inkscape gives you really nice figures, and doing overlays is almost trivial. You can either make the entire presentation in ipe/inkscape, or make the figures and embed them in beamer+latex. 

Just heard during the 8F workshop The original proof that network coding can be implemented in a flow network uses Gröbner bases. 

In other words, how should the result on 3SUM change our view of the possibility of getting a significantly lower than $n^2$ upper bound on the complexity of the problem ? 

I don't know what "bidirectional" and "cyclic" mean, but I'll assume that they mean "undirected" and "not a DAG or tree". In that case, the problem is called LONGEST-PATH and is NP-hard because it encodes HAMILTONIAN-PATH 

The notion of bounded doubling dimension appears to be related. Specifically, assume your region S is a ball of radius \rho in the metric space. Then the space is said to have bounded doubling dimension $k$ if any such S can be covered by at most $(1/\epsilon)^k$ balls of radius $\epsilon \rho$. The difference between your definition and this one is the limitation of $S$ to be a ball though. In essence, your parameter $k$ is acting like a kind of dimension of the space. A worthwhile reference is Ken Clarkson's review of different notions of metric space dimension. Separately, I'm a little confused about your definition, since it appears to require a bound unrelated to the size of $S$. This is odd, because $S$ could grow arbitrarily large. 

As Jukka points out, computational geometry is a rich source of problems that can be solved in polynomial time, but we wish to get fast approximations. The classic "ideal" result is an "LTAS" (linear time approximation scheme) whose running time would be of the form $O(n + \text{poly}(1/\epsilon))$ - usually these are obtained by extracting a constant (poly($1/\epsilon$)) sized kernel from the data, and running an expensive algorithm on that kernel, with a guarantee that an exact solution on the kernel is an approximate solution on the entire input. There are a number of tricks, reductions and principles, and Sariel Har-Peled's new book is full of these. I don't think there's a rich complexity theory as such.