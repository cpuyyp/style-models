If you have the data on your local machine, I guess, although quite large, you can perform the operations you want avoiding the Map-Reduce philosophy, thus go for python and scipy. The first thing you should bare is wether you want to perform Map-Reduce. Map-Reduce is a way of manipulating very large amounts of data. Taking it very simply, Map-Reduce divides the data into smaller chunks and the analysis is done over each chunk in parallel (Mapping stage). When all the chunks have been analyzed, the partial results have to be collected and summarized (Reducing-stage). Typically, Map-reduce/Hadoop is performed in both distributed data and computing systems (Amazon S3, etc... One important task of the mapping stage is to orchestrate how the different chunks of data are shared over the nodes that conform the distributed system). Hadoop is a software framework written in Java for doing Map-Reduce. If you decided your needs fit the Map-Reduce, go for Hadoop-Java. It is possible, though, to run Hadoop operations also in python. 

Yes, you can use Keras to build clustering algorithms. Recalling that Keras is a high-level api for Tensorflow or Theano, and that you can build a K-means algorithm (very well known clustering technique) in Tensorflow, you should be able to build it in Keras. Example (in tensorflow):$URL$ Nevertheless, sklearn has in-built clustering algorithms like K-means or Gaussian Mixture Models, so at first, I think a better approach is using these sklearn funcionalities. 

Adding noise is not the same as changing the dimension of the feature space. If the data is linearly separable in the original feature space, it will be also separable although you add an extra random feature. Take a look at figure 1 and figure 2. Figure depicts the scatter plot (var1_1 vs var1_1) of a linear separable data in a one dimensional feature space. Figure 2 depicts the scatter plot of the same feature space with an extra random feature, now the dimension is 2, but the data is still linearly separable. You only have to look at the projection of the data in the var1_1 axis. Figure 1: Scatter plot of separable data in a 1-D feature space 

Example: desired accuracy = 95%. Training accuracy = 93%. Validation accuracy = 82% -> Bias = 2%, Variance = 11% Summarizing: 

It depends on what is understood as noise, since a noise source can be interpreted as any way of corrupting/altering the data. Technically, if you want to add noise to your dataset you can proceed as follows: 

What you are trying to do is Blind Source Separation, and unfortunately it does not works as easy as that. When you are extracting MFCC from an audio file with a mixture of voices, MFCC clusters won't represent two people. For the separation, you should end up with two sound signals with the same number of samples. The MFCC are used as the feature's space for speakaer recognition tasks, for example, but not for separation in general. For more mathematical background about BSS you can visit: $URL$ The most simple case is the so-called Cocktail Party Problem, in where you have at least as many microphones as persons and the mixture has to be instantaneous (no delay between signals, only attenuations due to path-losses). This very simple case can be solved applying Independent Component Analysis, with the FastICA algorithm: $URL$ For delayed mixtures you can try the DUET algorithm: $URL$ For convolved mixtures and undetermined problems (more speakers than microphones) this PARAFAC-based algorithm performs quite well: $URL$ 

If training and test datasets come from the same distribution (I guess yes, since they are all Belgian sign traffics) you can name it Training and Validation datasets. If the same condition holds, you can perform cross-validation over your Training dataset, and carry on naming it Training and Tets datasets. 

The reason is that you have to map the new data to the same feature-space used for the training process, so you have to scale/std it with the same factors, otherwise, you are changing the feature space. 

Add noise to the raw data, i.e, corrupt the raw data with some noise distribution and with certain signal to noise ratio, 

Figure 2:Scatter plot of separable data in a 2-D feature space, which is the same as the previous space plus an extra random feature [ If you want to evaluate the robustness of your prediction model against noise, I will take option 1, since it not straightforward to derive what kind of noise to apply in the feature space. If you are working with images, you can blur them or if you are dealing with audio files, you can add white gaussian noise, or another kind of noise source, for example another mixing the original audio files with other sound sources. 

I wouldn't say you can not tune the hyper-parameters in the trainig dataset, but the purpose of doing so is different than in the validation set. In general, what it is intended with a ML algorithm is how to optimally classify or perform regression given some training data. Once the model is trained, it will be used with new data to obtain predictions, thus: 

We need some training data. With this dataset, the model will look for the optimal weights and biases that minimizes the selected loss function. We need some upper bound to our performance metric: for example, we can decide that our model has to perform like a human, which has showed an average metric of 95% for some specific task. The upper bound can be the best score in a certan benchmark, etc. We start training the model on the training data and we evaluate its metric, lets take the accuracy. If the accuracy is too low, we can tune the hyper-paramters until the accuracy increases in the training data (no evaluation dataset is used here). The difference between our model's accuracy and the upper bound is taken as the bias of the model (this bias has not to be confused with the biases of the neurons). So, the accuracy on the training data gives us the bias of our model. Once we decided our bias is reasonable (optimally 0) arises the question of how the model will perform when it is fed with data that has not be used for training (the real application). The difference between the accuracy on this unseen data and the accuracy on the training data is the variance of the model. This unseen data is the validation set and give us an idea of how the model generalizes to new data. If the variance is high, then the model poorly generalizes to new data. Then, we perform hyper-paramater tunning and evaluating the accuracy on the validation data until the variance is low enough, trying to not worsening the bias (variance-bias trade-off). 

Scale / standarize the input data, but very important, with the scaling/standarization factors stored during the training process. You dont have to compute the min, max or mean values of the new data. Predict 

When working with a 3D laser sensor (LiDAR), the volumetric point density versus distance $\rho_r$ can be theoretically worked out taking into account the physical properties of the laser (number of layers, TOF, etc.). On the other hand, it can also be computed $\hat \rho_r$ from the available training data, yielding into an estimation of this quantity. When this density vs. distance is wanted to be included in a classification problem (in the preprocessing stage as parameter) what value should be regarded? I share my concerns about the two different approaches: 

The classifcation task is approached with CNN. Should the empirical value be used only for sepecific types of scenarios/applications? If a general classification model is itended, should the theoretical value be used, or having enough variability on the training data will make the empirical value work better? 

I'm handling a high imbalanced dataset, thus, I'm weighing the loss function in order to penalize the misclassification of the minority classes, I set the weights in each batch as follows: 

Increse the training data by adding more instances, if available If no more instances are available, perform data augmentation to increase the training dataset Use regularization, for example, dropout. Shorten the network. If there are lots of layers, the network may be learning too specific features from the training data 

Taking it into account, I guess you have to name your datasets as Training and Evaluation. You first train the model with the training data, and then you evaluate how it generalizes with the evaluation data. First you have to decide which metrics you will use, I recommend precision and recall, per class. Once you have your trained model, you perform predictions over the evaluation test and you'll get the metrics for this dataset. For instance, if your results over the evaluation data are worse than on over the training data, your model is overfitting the training dataset. However, I strongly recommend you to use cross-validation on the Training-dataset, visit this link 

Scale / Standarize the training set Store the scaling / standarization factors of the training set Train the model 

What's more unclear is the role of the test dataset. I usually use it when I decided that the model will be no longer changed and I need the final metric that describes the model. The test set can also be used to have an idea about how the model performs with data which have not be intended to work with. In general, the test set gives the power of the model for the inference task. 

Training (on training data): this data is used in the learning stage. The model optimally adjusts the weights in order to minimize the loss function over this dataset. Evaluation (on evaluation data): it serves to fine-tune the model, for instance you can decide if your model overfits Testing (on test data): tells you about the inference power of your model 

When the training loss is lower than the validation loss, the model is said to overfit the training data, i.e. it has learned so much from the training data that it only adjusts well to it and it can't generalize to new data. This phenomena is regarded as the variance of the model. The bias of the model is the difference between the training loss and and the loss you've previoussly selected as the minimum loss reachable, or the desired one. However, this analysts is usually done over other well known metrics, such as precision and recall. You first calculate these metrics on your training data, and then, on the evaluation data. Then you perform the analysis taking the same considerations. In order to reduce the variance/overfitting, there are common techniques: 

It depends on whether the practical application you are designing the algorithm for will encounter those remaining signs. If so, I will give these signs a label, for examle 'Don't care'. The algorithm should have the capability of recognizing your 8 signs and to know that the others are of non importance, but do not fall on your desired targets. On the contrary, if the algorithm will be always fed with these 8 signs, you can use only those images. There would be other approaches: you can train a previous SVM in which you set two groups of signs (binary classifier or Support Vector Data Description), one containing the target 8, and other with the remaining ones. Then, another SVM can recognize the sign only if it has been previously classified among the 8 desired one (this SVM is only trained with the 8 signs). 

Where 'i' goes from 0 to N-1 (for N classes), 'num_total_instances' is the total number of instances in the batch, and 'num_instances_of_class[i]' is the number of instances of class 'i' in the batch. The problem is, as I'm doing it on each batch, It may occur that, in a certain batch there is no instance of class 'j', thus w[j] = inf. Â¿What weigh should I set for class 'j'? At this moment, I'm setting this w[j] to 1E6 I know that there are other approaches to fight against imbalanced datasets and I also know that I can pre-compute the weights for the whole dataset and use them (fixed) cross all batches, but, I'd like the specific answer for this question, whereas it makes sense. For sure, it will be well received other suggestions for the calculation of the weights. 

Theoretical values $\rho_r$ : It will give an upper bound, as it is only taking into account the nature of the sensor. Furthermore, if only physical properties from the data-sheet are used and no statistical information about the uncertainty of the measurements are regarded, this value will be deterministic. Empirical values $\hat \rho$ : they collect both the nature of the sensor and the scenario. A clear disadvantage is that, it will work well for one scenario and not for others. If the estimator is consistent and asymptoticly unbiased, the more data the better the parametrization.