Any uninstall of SQL Server will not delete the data or log directories used by the particular version. The reason your data and log files stayed in the same spot is because you did an inplace upgrade. It will not move the databases to the "new" folders SQL Server 2012 would create (eg. "..\MSSQL11.MSSQLSERVER\DATA"). The only issue you could hit when removing SQL Server is with the shared tools (e.g. SSMS, etc.), but since 2012 changed to using full VS it should not cause any issues. 

A better option might be to script the jobs out instead of taking the information directly out of the system tables. In SSMS just right click each job and it should give you the option to script to a new query window or file. Scripting the jobs out in this manner is probably the most affective, short of just restoring the msdb database to the other instance. You can review the script before running it on your second server and ensure the job steps and schedule looks right. If you have any that run frequently you might adjust the script to have those created as disabled, so you can verify first. There are a few things that might affect why your pocess did not work, one being the order in which you inserted them and ensuring that the records or linked by the jobid like they should be. 

PowerShell A simple one liner can provide the same information you will see in SSCM or the Services Management Console () if you are local to the server: 

That command alone will copy the logins from DB1 over to DB2 and simply sync up anything that changed. So say you add another database to the AG and grant a login access (map a user), then next time you run the above command it will update that login on DB2 for you. The below script is included for completeness just to show what can be done. I built the below process on a client's 4 node AG as a Scheduled Task. As well this offers the ability to exclude replicas if you need; it was something they required. 

Now the above code can easily be wrapped into a function or for each loop that would allow you to easily pass in multiple server names. Edit Depending on what literature you read on PowerShell you will find that most folks that take time to write out a script, will go ahead and spend the time to make it a repeatable one. This is done either through a function (basic level for me) and then modules (more advanced that I have not touched yet). So for your example code: 

As is the command is trying to execute on your LOCAL instance and not REMOTE. You need to have it execute through the linked server itself. You can build the query as a dynamic statement and then simply execute the query against on the linked server. So something like this should: 

Your best option is going to be using Resource Governor. Just create a resource for that vendor app and throttle the memory down for it. You just have to come up with what you are going to identify that workload with, you are allowed: 

Environment Info: SQL Server 2008 R2, running the package through BIDS only at this time. Questions: 

Aaron Bertrand wrote a good post on it that is pretty detailed...How I use PowerShell to collect Performance Counter data. Then Laerte Junior has an excelent walk through on how he finds the counters he wants in a Simple-Talk article: Gathering Perfmon Data with Powershell. This might be where you want to start. It has some cmdlets that he uses to capture the counters for a particular instance I believe. EDIT: See if this is what you need: 

If that did not work I would simply opt for doing this in PowerShell as there is much more control, and less things to configure to get it setup. 

Do you have a connection configured for your project within SSMS? If not right-click under Connections and create a new connection to your instance. Then to create a new query simply right click the connection you created and select "new query". It will then use that connect for your query. It is by design it does this because the query you create has to be associated to some connection, which clicking cancel will obviously create the query with no connection. 

Ok, there are a few things to explain first. (1) The prompt within the properties is only asking for authentication. You cannot specify a database name within the field of that window in order to connect to it. For the server name you specify either or . You would connect to a specific database by setting this via the tab: 

You can use stored procedure to start a job from any server using the parameter. Passing nothing to this parameter means the job is on the local server but you should be able to pass your remote instance name and have it call the job on that server. The only thing I think is you might have to use a run as condition on the job step, under the advanced properties. Otherwise the service account on your local instance would need the appropriate access on that remote instance. Your command would look something like: 

As the error indicates the login does not have permissions on the schema. This would require you to specifically grant to the login, or I believe you could also just for the database. Update Sorry just caught that last sentence. I have not worked with that software but seems it might be using a different login to execute the query if you can execute the same query fine in SSMS. On a side note, since you are in SQL 2005, it would be a better practice to start using the new views for looking at catalog information. 

As stated on MSDN side-by-side installation of SQL 2012 with earlier version is supported. However take note of the instance name, if your SQL 2000 instance is a default instance then your SQL 2012 install will have to be a named instance. I would take note also that some issues may occur with SQL 2000 tools as your SQL Server 2012 installation may attempt to upgrade any shared components it finds. Although I don't know what of SQL 2000 would still exist in SQL 2012. Now, my opinion and not the opinion of StackExchange, or my dog...I would not suggest configuring a side-by-side install with a SQL 2000 instance. Not knowing the setup, I would highly doubt the server configuration of a SQL 2000 instance is very well suited to what Microsoft suggest for a SQL Server 2012 installation. Especially considering the fact that SQL Server 2012 is only supported on Window Server 2008 SP2 and higher. This all depends on how old the server is and such but just my opinion. 

As long as you change the service account within SQL Server Configuration Manager (SSCM) it will apply all the permissions required (registry, files, etc.). Only thing to be aware of are those rare occasions where SSCM process does not apply all permissions as expected, generally from some WMI issue. I so far have not come across this in new versions of SQL Server (2008 R2+). By the name of that service account I am going to assume you are on an OS of Window Server 2008 R2 or higher. Just as a side note, if you do not actually need access to Active Directory resources leaving that account as the system account is perfectly alright. Most security standards will allow it as well if you are running on a current Operating System. These are virtual accounts that offer more security than the previous local system accounts did in previous versions of the OS. They are less chances of someone impersonating those accounts. I use them on most servers I setup for clients now unless they have some reason to need full domain access. 

If your network is fast enough using the option is doable but make sure it does not affect your backup window. I have used third party products that would mirror to another location and often times when the job failed, it was because of the mirror location having a read error during the verification process. The backup wrote quicker to the network share than it did reading it which ended up causing the job to fail all the time. It can cause a few headaches so just be aware. If you want to make sure your data is as safe as possible you should include testing your backup strategy to make sure it works like you think it will, and not just one time. This will also give you a good idea of restore times to meet any SLAs that might be in place. 

Not that I am a SAN/disk expert (there are folks on here that know more than me)...I only share what I've done a little of and mostly read :) Jonathan Kehayias and Ted Krueger wrote a book "Troubleshooting SQL Server" that has some good info on disk performance. You can get the PDF for free from here. (I might buy the printed edition of this too for my desk.) Anyway they have a good query that can be used to check the sys.dm_io_virtual_file_stats and check the average latency on your data files. You may find that RAID10 is not the ideal config for the data files to reside on. 

This is because of the changes affected when you modify the compatibility level of any database within SQL Server. This was an affect seen starting at SQL Server 2008 I believe, at least it shows up in documentation since then. As stated here on MSDN for the ALTER DATABASE SET COMPATIBILITY_LEVEL: 

In order to have it go to one file you have two options, and I think the first will get a little better performance than the second. First you can simply add a parameter to the to append to the file. As long as the columns are consistent (the same) it should work with out issue. 

I will say up front that I do not know why your company would have policies that lock down the OS so much that a Microsoft product cannot be installed and provided the appropriate permissions during an installation. I would think they might be doing that backwards. I supported the military as a contractor for many years and that included secured networks, we did not lock down the OS until after all software was installed on the server. We at most wanted to ensure the software installed itself appropriately without any issue. Outside of that, it is going to depend on what permissions you are not allowing to be set. If it is being restricted on what OS level permissions are set within the local group policy that are going to prevent the services from starting the installation will likely fail. Shorting SQL Server services the documented permissions required to operate might affect your supportability with Microsoft. Just something to keep in mind. Edit/Update There is no in-depth doc that is going to go through what all the installation process does on a server. You can weed through the installation log under the default directory here: . The would be based on which version of SQL Server you are working with: 100 = SQL Server 2008/R2; 110 = SQL Server 2012; 120 = SQL Serve 2014. I don't know anyone has gone through and determined the outcome of each little affect of an OS level permission not existing. All that is needed to understand is that permissions assigned according to documentation show what is required for a supported installation of SQL Server. You are basically going to experience sporadic issues based on what you need to do. Every permissions has a purpose, you just may not have reached or hit that purpose as of yet. 

I have written reports, and actually alerting with it, for clients to monitor multiple servers within their test and production environments. I basically used a SQL Agent Job with a PowerShell step to pull in disk information () into some tables. I then decided to create another PowerShell step that went back and checked if the free space was within a configured threshold (stored in a table). We went with this approach because it was just easier to setup and write fairly quickly. It was also cleaner code to write for a HTML report to be sent via email, than trying to do it in T-SQL. You have DMVs in SQL Server to get disk space usage (e.g. ) but the PowerShell option above allowed us to monitor all drives found on a given server. Now caveat you will have to create a proxy account for the PowerShell execution and that account should have appropriate permissions. An example of just getting a HTML report on free disk space. EDIT Just realized from Mike's comment that I had a function in my profile to, it is written a bit different than his but this is just what worked for me: 

I believe you can create a user account in that database, using WITHOUT LOGIN. This will create it where you can grant execute as permissions to the other users for any particular stored procedure in that common database. They don't have to use a password for it so you control it by who can EXECUTE AS with it. Here is a pretty good write-up on how it could be used, near the end he talks on how it can be used outside the context of the database. 

If you are running to get that list of orphaned users it will return the SID expected. Just take that SID and create your login with the same SID: 

The option is in SSMS but it will not work. SSMS is not Edition smart so to speak, they did not make it smart enough to hide options specific to the Edition you are running. If you try to use it, I believe it just gives you a message relating to "this is an Enterprise only feature"; or something close to it. 

You would need to check the security setting of the package configuration (inside the package). The default for a package is "EncryptSenstiveWithUserKey", which means only that user can modify the package or read the connection manager objects. You also don't provide how the packages are deployed. If they are deployed to the Package Store or SQL Server then the security setting mentioned above does not apply. If they are using file level deployment who executes the job or owns it can play a role in the issue you are having....meaning you will have to adjust the packages. 

The files are created while the the LS process (sqllogship.exe) pulls the file from the primary to your secondary. Once it copies the file the last step of that process it will rename it to . If the process is failing fully copy over the file you have to look at network issues. However, the job that handles that copying should have clear output history as to why it failing. You have to troubleshoot from that output is showing you. I have had one client where we had to write our own copy process because the single-threaded method that does was not sufficient to keep up with the work, mostly due to the client's network between the production and DR site. 

I stand corrected Let me first state I apoligize for not first testing what I was saying as I ended up proving myself wrong. I should have known better. First lesson to learn: Test everything. Second lesson to learn: Not all Microsoft Exam questions are perfect. I would suggest sticking with those test engines that Microsoft supports (e.g. MeasureUp.com) Answer before the details I will first give you what I discovered, then show you how I tested it. So my finding is that basically option D can be correct if you use instead of . This is based on the statement from Bob Beauchemin's whitepaper on SQL Server 2005 Security Best Practices: 

By default when Express Edition is installed the TCP/IP protocol is disabled. You need to go into SQL Server Configuration Manager and enable that protocol. 

Now, with using you can actually cause scripts to receive a prompt when executing the PowerShell script. This may be what is actually generating the error because SQL Agent cannot respond to the prompt or does not know how to handle it. There is really no reason to use that policy setting as is a sufficient policy to allow scripts you wrote and setup on the server to execute without being prompted. I would except if you dug into the full error being returned it might contain text similar to the below message. This is the prompt you can receive when setting the execution policy to : 

The problem you are experiencing is with the PowerShell subsystem in SQL Server Agent. It is a bit flakely with using other modules becuase you are put in the context of the SQL Server PowerShell Provider (SQLPS.exe). So it works the same way as if you opened up and then try to execute your code. One thing to keep in mind with dbatools module is that it will conflict with both SQLPS and the module that MS now maintains separate for SQL Server. Last I checked the main thing that it conflicted against was TEPP that we have in the module now, it just can't load that code. [Caveat: I'm a major contributor to this module.] The dbatools module has custom types and styles built in so when you run the scripts under PowerShell host that also has SQLPS or the SQLServer module imported your results will vary. To utilize dbatools in a SQL Agent step make sure you only use the CmdExec subsystem (step type) and then call PowerShell host to execute your code. If you do not want to maintain a file for each script you can put your code in a SQL Agent CmdExec step in the manner illustrated below, but more complex script it is easier to maintain via files.