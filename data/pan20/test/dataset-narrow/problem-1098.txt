Is there any algorithm which shows that under the assumption of Lipschitz smoothness of the Hessian of a non-convex function one can get to its critical point faster? 

This paper ($URL$ from STOC 2001 is possibly the first paper to show how to convert upperbounds on the $\frac{1}{3}-$approximation degree of a Boolean function into a learning algorithm. Have there been other works in this line? What are the most recent things we know about such conversion from approximate degree to learning algorithms? 

In September $1989$, Renegar had this famous sequence of 3 papers titled, "On the Computational Complexity and Geometry of the First-order Theory of the Reals, Part I/II/III". I was wondering if anyone knew of a smaller textbook/course-notes exposition of the at least maybe the main point there. For example, if I understand correctly, a consequence of all this was an estimate on the running time of solving any polynomial optimization problem (min/maximizing a polynomial with polynomial constraints). If at least this part can be read up from a simpler exposition somewhere else. 

The way I know of to bound generalization error by Rademacher complexity is Theorem 2.4 in this lecture notes, $URL$ Here the quantity on the LHS that Rademacher complexity is trying to upperbound is given as, $L_{\phi}(\hat{f}_{\phi}^*)-\min_{f \in F} L_{\phi}(f)$ where $F$ is some "hypothesis class" of functions mapping $ f :X \rightarrow D$, $\phi : D \times Y \rightarrow [0,1]$ is the "loss function", "$\phi-$"loss of any function $g : X \rightarrow D$ is defined as, $L_\phi(g) = \mathbb{E}[\phi(f(x),y)]$ - where the expectation is taken over some distribution over the points $(x,y) \in X \times Y$ and $\hat{f}_{\phi}^*$ is what the ERM returns over some $m$ samples i.e $\hat{f}_{\phi}^* = argmin_{f \in F} \frac{1}{m} \sum_{i=1}^m \phi(f(x_i),y_i)$. The above setting is called ``agnostic" because at no point was it assumed that, $\exists$ any ground-truth labelling function $L \in F$ such that $y = L(x)$ but rather the class $F$ is to be seen to be trying to learn via empirical risk minimization a distribution, say ${\cal D}$, over $X \times Y$. My question is 3 fold , is there any analogue of this Theorem $2.4$ when, (a) an existence of a $L$ is assumed with $L$ may or maynot be in $F$. (the later is I guess often called the ``realizable setting") (...I have seen some papers trying to bound generalization error of a specific algorithm in the realizable setting but I somehow dont see Rademacher complexity defined in those settings!..) (b) the loss function $\phi$ is not assumed to be bounded above but only assumed to be bounded below. (c) AND most importantly, say I have a class of labelling functions ${\cal L}$ mapping $X \rightarrow Y$ and I want to say the following, "Given a loss function $\phi$, irrespective of which member of ${\cal L}$ labels the data (maybe also irrespective of the distribution over $X$ used to measure $L_{\phi}$) the member of class $F$ obtained via ERM on the data, can never generalize well". Is there a version of Rademacher complexity which captures this? 

Are there lowerbounds known for representing the Tribes function by a circuit consisting of a single layer of polynomial threshold gates feeding into maybe a trivial summing gate? (Even for degree $1$ polynomials?) 

It would be helpful if you could kindly add in other references that you might know of for either provable sparse coding or matching pursuit algorithms. 

Is there any lecture note or review paper which gives a self-contained proof of the Linial-Mansour-Nisan theorem? 

Or if we specialize to Max-CUT do we know of graphs whose Max-CUT polynomial needs high SOS degree to be solved? (Or if we at least know of integrality gaps for it for high SOS degree) 

So Roychowdhury-Orlitsky-Siu had shown that the number of depth $2$ linear threshold gate circuits mapping $\{0,1\}^n \rightarrow \{0,1\}$ and of size $s \geq n$ is $2^{O(n^2s)}$. I think I do understand the proof of this theorem but its not clear to me as to how the following two corollaries seem to be immediately following from this, 

The exposition of that in Ryan O'Donnel's book seems to use terminology and notation which isn't easy to parse unless one reads many pages before that. Most lecture notes I found only at best mention it in the passing. 

I am referring to the proof of Theorem 1.4 in this STOC 2014 paper, $URL$ In particular my question is about the argument that begins in the 8th line of page 9 where the author says the following, "Following the strategy of Theorem 1.2 (and the author's ACC SAT algorithm $URL$ the satisfiability question of (a $AC^0[2]\circ SYM$) C with $n$ inputs and size $poly(s)$ can be efficiently converted into a problem of evaluating a larger $AC^0[2]\circ SYM$ circuit C' where C' has $(n-k)$ inputs, $2^k\times poly(s,M)$ size and $k < \frac{n}{2}$ is a parameter and the $AC^0$ part has depth $6$. More precisely C' is an OR of $2^k$ copies of the depth $5$ circuit $C$ and each copy has its first $k$ inputs assigned to a distinct string from $\{0,1\}^k$." Now given how C' is defined given a circuit C it seems obvious to me that the circuit C' is satisfiable if and only if C is satisfiable. (...EXCEPT that I dont see why $C$ is a depth $5$ circuit given that from whats on the top of page $9$ it follows that its a $AND \circ OR \circ AND \circ XOR \circ AND \circ OR \circ SYM$. Then the $AC^0[2]$ part of $C'$ should be depth $6$ and that matches with the first part of the paragraph. I guess this "depth 5" part of the quoted paragraph is a typo!...) Then to make this argument where is the need for the deeper theorems like Theorem 1.2 of this paper (which is about a fast algorithm to evaluate $ACC \circ THR$ circuits) and the big breakthrough paper thats cited? (At no later stage in this proof do I see these two big results being referred to either.) Am I missing something in this argument? 

Recently in this beautiful paper, $URL$ it has been shown that there is an explicit $Th \circ Th$ function with sign-rank scaling exponentially in dimension. I wanted to get some context for this,