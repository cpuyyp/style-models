In these cases the order may be entered into the system at some time after the transaction took place. In these circumstances it can be difficult to impossible to correctly identify which historical price record should be used - storing the unit price directly on the order is the only feasible option. Multiple channels often bring another challenge - different prices for the same product. Surcharges for phone orders are common - and some customers may negotiate themselves a discount. You may be able to represent all possible prices for all channels in your product schema, but incorporating this into your order tables can become (very) complex. Anywhere that negotiation is allowed it becomes very difficult to link price history to the order price agreed (unlesss agents have very narrow negotiation limits). You need to store the price on the order itself. Even if you only support web transactions and having a relatively simple pricing structure, there's still an interesting problem to overcome - how should price increases be handled for in flight transactions? Does the business insist that the customer must pay increases or do they honour the original price (when the product was added to the basket)? If it's the latter it the technical implementation is complicated - you need to find a way to ensure you're maintaining the price version in the session correctly. Finally, many businesses are starting to use highly dynamic pricing. There may not be one fixed price for a given product - it is always calculated at runtime based on factors such as time of day, demand for the product and so on. In these cases the price may not be stored against the product in the first place! 

When choosing index column order, the overriding concern is: Are there (equality) predicates against this column in my queries? If a column never appears in a where clause, it's not worth indexing(1) OK, so you've got a table and queries against each column. Sometimes more than one. How do you decide what to index? Let's look at an example. Here's a table with three columns. One holds 10 values, another 1,000, the last 10,000: 

For example, say you're allocating a resource for all of March and April. If you just have a single record with start: 01-mar-2013, end: 30-apr-2013, does that mean resources were allocated/used on weekends and over Easter? If you have an entry/day and there's a record for 31-mar-2013, then the resource was allocated on Easter Sunday. If this is wrong, then you can just delete that record. With a range 1/mar - 30/apr, you've either got to update the start or end and insert a new record or maintain a list of "non-allocation days". Either way, these solutions are more awkward than just deleting. You could argue this could be avoided by forcing people to enter the exact dates that will be used (excluding weekends), but eventually someone will get lazy and enter the full range. You've not stated how many resource allocations you expect each day, but I wouldn't worry about the size of the data at this stage unless you're expecting it to be in the high millions. 

When you are not connected then the SQL will fail and the other SQL statements will not get executed. 

The function creates a date with a time. If I remember well this is 12:00. If the date in your table has a different time then this is also taken into account. When you want to test dates then you better put the function around your date like: 

The most 'flexible' way is to store the periods in a separate table together with a type field (Working days, Private off days, etc.). This makes it easier to generate overviews of days present/away because there is no need to check the 32 bits for zero's for each type of day. Also no problem with the different length of the months. There is an other type of days? Just allow it to be entered in the database. No need to change the table. If you need to transfer it into a 32 bit field the I am sure that C++ is capable in doing so. 

If you migrate from RDBMS to NoSQL then you will also have problems. They are not interchangeable for all types of usage. When you start a project then just before you start the creation of your data objects you must choose the one that suits best to your needs. Changing later will nearly always cause problems as soon as you use more then the 'standard' options. When you choose to use a third-party extension then there is always the risk that: 

One of the of goals of a relational database is to eliminate double storage of data. If you can find the right for the through the then you can leave out the reference in one of the tables (they are always the same as you mentioned in a comment). Keeping it in both tables makes your application more complicated. 

Instead of WW you can also use IW. The last one gives the ISO Week. Here the beginning or the end of the year might end up in the week of the previous/next year. Look for it in Wikipedia ($URL$ for more information on this. 

The command gives the CPU usage per core. Since you have 12 I would not create another Linode for that. Check the line with . There you will find the which is a better indication weather or not your server is using it's full CPU capacity. 

Two to the power eighty-six records?! This is about fifteen orders of magnitude larger than any table I've ever worked with - I doubt even Google or Facebook have tables close to this size! Based on this alone, I'd say that having a lookup table of all the possible values is a nonsense. Regarding normalization: I believe your table is already fully normalised (to 5NF), though I think you're missing from your (candidate) key. The co-ordinates of death are than all dependent on a given player and their time of death (). so you don't need to "normalize" these into another table. It can be useful to have lookup tables when you're fully normalized though to help with the following: 

The psuedo-column ORA_ROWSCN is assigned at commit time and can be used to identify the order of commits, with the following caveats: 

As Justin's said (and the links in his post prove), the cardinality rule is a myth. This aside, there's a good reason to use bitmap indexes on fact tables: separate bitmap indexes on can easily be combined by the optimizer to reduce the numbers of rows to access. This is very useful with fact tables with a large number of dimensions. While any single dimension may return a large percentage of the data, when combined with others this may fall dramatically. For example, you may have thousands of orders per day and thousands of customers (with hundreds of orders each), but a given customer is likely to only have 1-2 orders on any given day. This saves you having to create multi-column b-tree indexes. As (ignoring some skip-scan conditions), the leading column in an index must be referenced in the where clause to be used. So with three dimensions you need to create six multi-column b-tree indexes to ensure an index is available for every query your users may throw at you ( ind1: col1, col2, col3; ind2: col1, col3, col2; ind3: col2, col1, col3 etc.) With bitmaps, you just need three single column indexes and can leave the optimizer to decide whether it's beneficial to combine them or not. This example shows how the two single column bitmap indexes are combined, but the b-tree indexes aren't (note Oracle can convert b-tree indexes to bitmaps, but this is rare): 

I have a configuration where one database is log shipping to three different servers hosted in a disaster recovery site. These three disaster recovery servers are joined to the same AlwaysOn availability group (AG). In the event of failover, we recover the database on the server acting as the primary replica of the AG and then add the database to the AG using the 'Join only' synchronization option. Since the databases on the secondary replicas are already in a non-recovered state, the operation succeeds and we end up with a database synchronized across the AG. This is 100% great. New problem: Our monitoring software does not like it when databases are not in a readable state. So while we are in our primary site and are log shipping to our NORECOVERY secondaries at the disaster recovery site, our monitoring software opens high-priority tickets because it thinks the secondary databases are down (because it can't read them). Making the secondaries readable by switching them from NORECOVERY to STANDBY solves this issue, but creates a new one. When we failover to the disaster recovery site and try to add the database to the AG (as outlined above), it fails because the databases on the secondary replicas need to be in NORECOVERY in order to successfully join the AG. If we switch these databases from STANDBY to NORECOVERY before attempting to add the database to the AG, we receive a message saying the databases on the secondary replicas are not restored far enough in order to be joined to the AG and the join fails. If at this point, we take a transaction log backup of the database on the primary replica and apply it to the secondaries with NORECOVERY, we can re-initiate the join procedure successfully. It would seem that changing the secondaries from STANDBY to NORECOVERY is causing the engine to determine the databases are no longer in sync but I can't for the life of me figure out why. Anyone have any ideas? The only thing I can think of is that the act of recovering the primary database itself was enough to bring them out of sync, but if this were true, shouldn't it also be the case when we simply leave the secondaries in NORECOVERY to begin with (like our original plan)? 

The is not correct. You must create 2 of them. One the table for the field and one on the table for the field. 

If your is immutable then it can serve as primary key. Only if it was to hold long values then you could consider to create a separate primary key field to save space in the tables that have a foreign key constraint with this table. 

Check the documentation for more information or type . There is a parameter (TABLE_EXISTS_ACTION) that defines if you want to overwrite existing objects or if you want to append. 

The is the name of the user that runs the application. The is the name of the user defined in the database. Suppose that is logged on his desktop as and runs an application to modify the data. When he is asked to connect he enters the credentials of . In the audit you will find as and as . This explains while you find as since this is the user that is used to run Oracle itself. 

Why do you do an update on ? Anyway you must add a where clause with the primary key of the row to the update. You are now updating all rows in . Also the insert is inserting for every row already in . Why don't you rewrite the insert into something: 

The attributes should be split in 2 tables because the configuration details are not depending on the user but on the configuration of an external system. If a user can have only 1 configuration then you must use an 1-1 relation. If a user can have multiple configurations then you must use the 1-M relation. This is the theory. If you want (in case of a 1-1 relation) put all in 1 table than this is also 'allowed'. Remember well that if one day the relation becomes 1-M that you are in problems. This is also why it is preferred to split the information into 2 tables. 

On the machine where you installed your database you should have SQL*Plus installed. For your question. Create a database link from to and use your Read Data user. Now you can query the data. 

Suppose you want to have the rows that are and . In your first option you get all rows ( for example with all and with all ) and in your second option you only get row 2.