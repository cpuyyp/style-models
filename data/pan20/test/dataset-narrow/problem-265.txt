You need to download ENU\x64\SQLEXPR_x64_ENU.exe because your system is 64 bit from This Link I am not sure error is because of missing Browser or missing Express Database Engine service. Browser service is installed with SQL Express database engine so if you are looking to connect to database engine not Local DB you need to to install Express DB engine. 

Even index rebuild in enterprise edition do takes short term locks as per Paul Randal Article Online index operations need to take two very short-term table locks. An S (Shared) table lock at the start of the operation to force all write plans that could touch the index to recompile, and a SCH-M (Schema-Modification – think of it as an Exclusive) table lock at the end of operation to force all read and write plans that could touch the index to recompile. 

To see all column names which fn_dblog would produce Example: I would show what all things are logged when simple update command is fired 

The message is bit misleading as ideally there cannot be a table with object id 0 and inex ID -1. I would quote from Paul Randal's blog 

There are lot of factors that influence how long it would take for failover cluster to come online on other node. The factors are (but not limited to) 

As a good practice and suggested by Microsoft you should change compatibility level of database to match with server version on which it is hosted. There are few legacy applications which might require version as 80 for that you have to test, I cannot tell upfront whether you change it or not. You should see what all features were deprecated in 2008 R2 as compared to SQL Server 2000 below two articles would help you Breaking Changes to DB feature in SQL server 2008 R2 Deprecated database engine features in SQL Server 2008 R2 Deprecated SQL Server features in SQL Server 2008 R2 Regarding changing compatibility level if you face any issue you can immediately change it back to 80, that is why i said a round of testing is required. If you read Change compatibility Level of database it would really help you 

Suppose there is other request which comes and asks to read same page. SQL Server Buffer Manager will check the data cache in the buffer pool to see if it already has the page cached in memory. If the page is already cached, then the results are passed back to the Access Methods(Access Methods contains set of code which handles I/O requests for rows, indexes, pages, allocations and row versions) to continue with operation as requested. If the page isn’t already in cache, then the Buffer Manager will get the page from the database on disk, put it in the data cache, and pass the results to the Access Methods. 

I hope this will resolve your issue. After installation completes change Service account of SQL Server to some low privileged domain account you can use this link for configuring service account for SQL Server 

No you cannot. In simple recovery the space held by committed transaction is re-utilized in transaction log file. When transaction commits checkpoint truncates the logs and when log is truncated the space used by previous transaction will now be re-utilized by new one so log file will not have to grow instead use the space already there. The problem here is long running transaction you must focus on it, this is holding log hostage and not allowing it to truncate there by increasing space. 

Go to run Type Regedit and registry hive will open Browse to HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Microsoft SQL Server and then Right Click on Microsoft SQL Server Below screen will appear 

The article How to set Log file size will help you set optimum value. Basically you have to spend some time looking at transaction log growth and see how much it grows in a day or week. Currently as per your previous question the autogrowth and initial size of both data and log files are not good. You must refer to to this article to set correct autogrowth setting 

Its increasing to maximum limit because this is how SQL Server is designed to use memory. SQ Server will take as much memory as possible to cache data pages and other objects as well so that when request comes for reading a page it wont have to generate a request for physical I/O and then get the page from disk to memory the whole process is costly and depending on the load on Server can take time. On the contrary if page is avaialble in memory your request would be satisfied immediately. But this is not a drawback it would trim down its memory consumption when SQLOS asks it to do so because it was requested by windows OS that it would require more memory so this whole process is dynamic. So NO NEED to worry whem SQL Server is utilizing memory. For more details about memory please read below article $URL$ PS: Do revert if you still need clarification 

In case of failover, and both nodes A and B down, yes node c will act as owner of both instance running on Node A and Node B. 

SQL Server works in Non preemtive mode which means that if SQLOS asks it to yeild because it got request from windows OS it will ask SQL server to yeild and SQL Server will listen to it and will yeild or will do as SQLOS has asked it to do. This is because SQL server runs as application and is allocated resources by SQLOS which is monitored by windows O. Preemtive wait types occur when SQL server is executing a task and is interrupted by OS and asked to give up the thread it is using so that can be allocated for other tasks and SQL will do it. It will yeild and will wait till the thread is available this waiting will come under PREEMTIVE-XXX waits. What is version fo SQL server here is it patched to latest Service pack there was a bug in SQL server 2008 which points incorrect value of preemtive wait types. Can you run sys.dm_exec_requests DMV and see if the process getting preemtive wait types are suspended or running. Can you please post output of below query(By Jonathan Kehayias) to capture wait stats 

Now on server where max server memory is 28 G if is taking 21.5 G that is definitely a problem. This is what causing the OOM condition. What is MEMORYCLERK_SQLQERESERVATIONS This is a memory clerk in SQL Server which tracks memory allocated to query which involves Sort or hash operations during execution. These operators can be the largest memory consumers for a query. Why OOM error due to this When query involving sort and hash operations is executed it will make a reservation request based on the original query plan which contained a sort or a hash operator. Then as the query executes, it requests the memory and SQL Server will grant that request partially or fully depending on memory availability. There is a memory clerk (accountant) named ‘MEMORYCLERK_SQLQERESERVATIONS’ which tracks memory allocation to such requests . Now in your scenario following could be happening 

If database files are on multiple disks backup operation would initiate on thread per device drive to read the data. In same way if restore is done on multiple drives/mount points backup operation would initiate one thread per drive/mount point Even if you are dumping multiple copies of backup on same drive we would have one thread per backup file dumped. The parallelism associated with backup is related to the stripes. Each stripe gets its own worker thread and that is really the only part of backup/restore that one should consider as parallel operations. The max degree of parallelism has no affect on backup operation. 

This is excellent article on data compression, its big, but would help you in understanding data compression You must remove all SQL Server related folder and files from Antivirus check specially if you have McAfee Antivirus. 

Why Fragmentation Increased Now regarding what could have caused fragmentation to rise we can corroborate it to fact that when pages were getting allocated to the heap they would not be continuous, as you saw above what caused fragmentation value to increase was jump in the PAGE ID's allocated to pages. At the back of head you should also keep in mind that the word fragmentation for HEAP does not have any meaning, how would you define fragmentation for bunch of un-ordered pages. Really Worried about Fragmentation If you really face a scenario where heap table is fragmented and slowing queries it would be better creating a clustered index on table than rebuilding it. The reason is when you rebuild heap all underlying Non Clustered indexes are also rebuilt causing the rebuild process to take much longer time, utilizing lot of resources and bloating transaction log. On a production system one would always try to avoid this. Paul covered this in his Myth Section about heap. PS: Please don't use undocumented command on production system. This was just for demonstration. 

You need to find out what query is running after 2:00 PM and it was still running so it makes me think there is process.job starting at 2:00 PM. This process/job/query is trying to read too many pages which is causing lazywriter to flush too many pages because query is requesting space for such pages It can be that such query is missing index, can be that its creating a bad plan due to skewed statistics, it can be that query needs to be written to get/read only subset of data not whole data. 

Let me ask you what is output of . What is SP and CU level to which your SQL Server is patched. The reason I am asking this is because there was bug in SQl Server 2012 which forced PLE to plummet like what you are observing. Ths bug was fixed in SQL Server 2012 SP1 CU4. Or to be on safer said I would recommend you apply SQL Server 2012 SP2 instead of going for CU4 Its sometime normal for PLE to fluctuate on system having high activity. Actually this is by very virtue how PLE code works in SQL Server. But the fact that its plummetting to zero quite frequently make me believe you might be hitting the bug I have mentioned above. As per Microsoft Bug fix detail 

This will show you how much memory is granted to queries running on the system. If you can see XML actual execution plan you have can you collect this value for costly queries. All the above will show us if there is problem in query or some other issue as to why it is requesting so much memory for execution. 

Since client can afford one day data loss why not change recovery model of database to simple and take ONLY . The differential backup can be taken once a day. when you say client can afford a day data loss I assume client does not needs point in time recovery. If so their is no need to keep database in full recovery model. Keeping in simple recovery will remove hassle of taking frequent log backups. The other approach can be keep database in simple recovery and take weekly full backup and daily differential backup, twice in a day. Just adjust the frequency of diff backup as per RTO agreed. I would prefer daily full backup and a differential backup if enough space is present. Make sure you verify integrity of your backup when you take it, you have option in maintenance plan. It would also be better to restore the backup on other server and run checkdb to make sure the backup is consistent 

This message means that a particular session or query requested for I/O to get data from disk but that session had to wait more than 15 sec and after that request was catered. You can guess that 15 sec is threshold value moment this time is crossed message is dumped in errorlog. This MOSTLY means that disk is not able to cope up with I/O request which is getting generated and in turn means disk might be slow. Since you said 90 % of your errorlog is filled with this information I am forced to believe that underlying hardware is slow or might require a firmware upgrade. This Article will help you in understanding the issue and fixing it 

You can see you are creating temptable with same name.You cannot create temp table with same name in single query or batch this is documented as per BOL document. From Bol document 

If your BCHR is high 90 to 100 Then it points to fact that You don't have memory pressure. Keep in mind that suppose somebody runs a query which request large amount of pages in that case momentarily BCHR might come down to 60 or 70 may be less but that does not means it is a memory pressure it means your query requires large memory and will take it. After that query completes you will see BCHR risiing again 

This is not totally correct. Simple recovery is almost same as full recovery just the log truncations are taken care by Database engine in case of simple recovery. In simple recovery after transaction commits or log file grows 70 % of its size log truncation occurs while in full recovery you need to take log backup to truncate logs. So if you think log file would not grow in simple recovery model you are wrong it can grow and I have seen this often in Index rebuild case. This is because the operation still requires a portion of log and thus not allowing log truncation. Are you aware that setting recovery model to simple and back to full will let you loose point in time recovery. If its is a critcal database with SLA set I would not advise you to use this. There is other option to decrease logging by using bulk logged recovery model but in this as well you loose point in time recovery. Index rebuild is minimally logged Ola Hallengren solution would be good one. You can also choose option SORT_IN_TEMPDB to use tempdb as storage from intermedite index which is created thus avoiding space issue on drive on which index resides. Your approach might work one time but is surely not suitable for long run 

The limitation is for database not for instance. Plus 10G limitation is ONLY for data file, the log file can be 10.5 G or 20G for example. As per this BOL document it says that 

You cannot upgrade from to . Its not supported and wont allow you please see Supported Version and edition upgarde You should not worry much about upgrading from 2008 Sp4 to 2014. The upgrade would not be a much of problem but how would application behave is something on which you should focus. You can download SQL Server 2014 evaluation edition , which is equivalent to enterprise edition for 180 days. Restore database from 2008 to 2014 and start testing. Don't use any enterprise features. When you are satisfied with application performance do an inplace upgrade to SQL Server 2014 standard Before doing in place upgrade make sure you run upgrade advisor and mitigate all issues(if any) pointed by UG advisor. 

For cases where you have just read only databases or databases where you just do select operation and there is no DML operation, in that case you can keep the option to false but again no harm would come if you keep it true. We mostly see database with certain amount of activity. 

There would be no damage as such, what checkpoint does is it writes dirty pages from buffer pool to the disk reducing the amount of recovery time and amount of work SQL Server has to do after restart or crash recovery. In extreme case if database goes under crash recovery the amount of time taken to recover database would increase. In your case it is good you found out that it was actually the index rebuild which produced some good amount of logs and eventually failed because of low disk space on log drive. This would just rollback the operation and would bring fragmentation as it was before the rebuild