...to change my passphrase for my key, but I'm not sure what this means. If I'm encrypting data on box A and decrypting on box B (say with duplicity) do I have to change the passphrase on both ends? Will previous backups still work? Is the passphrase just the key to a sort of encrypted wrapper around the key file? Dumb question, but I don't want to screw this up. Thanks! 

EDIT: It turns out by adding a after the SSH server comes up and before trying it works fine. This seems to be an issue with the configuration of apt on this AMI combined with perhaps some strangeness on Amazon's end, or DNS issues. Here are my entries in : 

You can use the "s3cmd" utility with the "sync" option, although I stumbled on your question because I'm trying to figure out if this syncing mechanism is screwing up my duplicity backups. 

We have a web service running on Amazon EC2. Currently we have some live user data stored on a single disk (EBS). We are considering moving to a RAID0 setup (we don't have to be concerned about the increased failure rate). If we do this migration, what is the quickest (to minimize site unavailability) way to reliably transfer the user data to the RAID array? One idea I had was to take a recent snapshot of the data, copy it over to the new RAID array, then when the site goes down for maintenance use rsync to copy only the changed data over. I'm not sure if this would actually save time or ensure data integrity though. 

This might not be correct in your case. But, typically the answer to this question for a database application is that the database needs vacuumed, optimized or some such as is appropriate for the database in question. From what I can find it looks like mysql has an command. I'd look into that. 

Be sure to try generating lots of different types of failed login attempts. Try from X (gdm or kdm or xdm), try from the console, try from ssh, try from sudo and try from su. Different subsystems can (and will) be configured different ways. It's not uncommon for ssh to be configured to use an internal login command that cuts around the /var/log/btmp business. Try the command as well. You might look in /var/log/secure to see if your failed login attempts are being stored there. But, I'm afraid I don't know the structure of the Debian log directory. Try on anything in the /var/log directory. It's quite likely that ssh is logging something someplace. 

I'm assuming, possibly incorrectly that your users are sitting in from of the windows boxes and need access to some linux gui applications. VNC works very nicely. Most Linux distributions have a the server sides vnc x-server included these days. Get a compatible windows vnc client and you should be good. If you need some additional security look for the ssl-enabled vnc stacks that are starting to crop up. Getting a windows side xserver like the one in cygwin will also works but is more complicated and will be much harder to explain to windows users. If your users are already linux savy this won't be to bad, otherwise go with vnc. 

I have a command that runs a disk snapshot (on EC2, freezing an XFS disk and running an EBS snapshot command), which is set to run on a regular schedule as a cron job. Ideally I would like to be able to have the command delayed for a period of time if the disk is being used heavily at the moment the task is scheduled to run. I'm afraid that using nice/ionice might not have the proper effect, as I would like the script to run with high priority while it is running (i.e. wait for a good time, then finish fast). Thanks. UPDATE: This is what I ended up going with. It checks /proc/diskstats and runs my job when the current IO activity hits 0, or we timeout. I'll probably have to tweak this when I look at what kind of IO activity our servers actually get in production: 

Shouldn't be too many to change by hand, and hopefully your system doesn't differ too much from mine. 

I understand it displays the command with arguments, or when unavailable the command in square brackets. But where do the names come from for processes such as passenger worker ruby instances, which show up as: 

I'm using an Alestic image which disables root SSH logins, but provides a user "ubuntu" with NOPASSWD sudo privileges. See here. In the course of trying to add a new user to the sudoers file I inadvertantly created another line for the "ubuntu" user, this time without NOPASSWD. I have now apparently lost root access to this machine. Is there some way to mount the EBS root volume on a different instance (fixing the sudoers file) and then re-launch the server? Or am I totally screwed? 

Nothing requires that ping be possible between two hosts. It might be that somebody between you and google is dropping ICMP packets. If everything else is working I'd not worry much about this. If you are particularly worried check with whoever runs your networking equipment or firewall and see if they are letting ICMP traffic through. Also check to see if you can ping anybody else in the outside world other than google? 

I'd suggest using the expensive raid controller to do the bulk of the raid work. LSI cards and the software they come with works quite nicely. When properly configured, they will send you email when intereting things happen to the array. Like when disks fail. There is nothing wrong with either of the two linux software raid options, but you've gone out and purchased a somewhat fancy raid card. Let it do the work. Configure the disk array to expose one big device to Linux. If you would like to break up the final device into small volumes use lvm for that. One big physical volume, one big volume group and cut the volume group into whatever number of logical volumes you need. 

This might already be what your doing but, here is a thought. If the contents of sample are only to be accessed via FTP then you can move that directory outside the web server's document root and use your 0775 with owner=user and group=nobody plan. The php script will be able to write, user will be able to use FTP and the outside world will not be able to get at sample through the web server. 

What specificaly is involved in an Elastic Load Balancer health check on an instance? I know that it performas an HTTP(S) "ping"; does it just deem an instance "Unhealthy" if that HTTP request returns an error status number of times in a row? Or does it take other factors into account, such as CPU usage? 

I'm experiencing a strange issue with a fabric script I'm using to bootstrap a server on EC2. I launch a stock Ubuntu 12.04 AMI (ami-3d4ff254), wait for it to start, then proceed with: 

are there any shortcuts that can be taken on this initial "resync", since there's not (AFAIK) any data I need copied between the empty disks can I blithely format and treat it like a normal disk while it's in the middle of its initial sync, as long as I'm okay with it being in a "degraded" state for a bit? 

I'm using fabric for provisioning instances from scratch (a stock ubuntu image) on EC2; it works great. There are a few basic functions for appending to and commenting lines in a file that make modifying config files workable. 

The appears to run fine and gives no errors, however (2/3 of the time or so) installing throws a "no installation candidate" error. When I ssh into the server and run I get the same error. Running by hand, then the package installs fine. Any ideas on what might be happening, or ideas for debugging? 

Before I plunge into the depths of how to synchronize UID's/GID's across my different Linux machines, I would like to know what is actually the benefit? I know that this keeps file synchronization relatively easy (as ownership is "naturally" retained). However this can also be achieved otherwise depending on the transmission service. Is there anything else that would benefit from consistent UIDs/GIDs? 

will trigger a Shibboleth authentication. After a successful auth the request gets passed on. Is that possible? I am asking as I only found this resource $URL$ which I find very ambiguous, as it only says 

I am trying to set up a shibboleth configuration but have now hit an error I do not know how to deal with: When accessing a site, which I configured to be protected by shibboleth, I receive a 500 error, with apache's error log stating: 

I am currently trying to get a webpage of mine running on my local machine. Strangely though, albeit I do use the exact same Directives and RewriteRules, what works on my server, does not work on my local machine. What happens is that every request to a file is handled as a request to a directory, e.g. 

Turns out that IF you use the RequestMapper directive in shibboleth2.xml, even for other sites, then you also HAVE to create a Host directive for Apache, even though that's usually not required. In my case I currently try to configure nginx to talk to shibboleth. Using this nginx module, I had to configure a Host directive, which then also worked. Then, for testing purposes, I wanted to enable Apache's mod_shib again and even though it used to work, it didn't anymore. Specifically adding the Apache Host to shibboleth2.xml solved this problem. It is quite interesting to note that this problem resulted in the above error message... 

The reason your seeing php as having been built without sqlite is so fedora can split the php package and thus not force a big string of dependencies on people who don't want them. For instance you need sqlite and thus likely you do not need postgresql. If fedora was to build the main mod_php application with all --with's turned on you would end up installing postgresql without needing or wanting it. This helps people concerned with both security (only install exactly the software needed) and people concerned with package download bandwidth. 

You should try the Django work without any restarting of apache. Most sever side environments work fine while doing development work without any need for stopping and starting the web server over and over again. But, yes you can run any number of apace instances as long as you make sure your second server's config file is pointing at different resources. Like: 

About serving static files. Yes, you can use a lighter web server to do this. But, before you go to the effort be sure that it will do you any good. Is apache really using resources you need elsewhere? Maybe just configure apache to not start quite so many child processes. Be sure the added complication will pay for itself, because down the road it will almost assuredly confuse somebody when they try to figure out how everything is working.