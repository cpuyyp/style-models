Note that this is a fairly tough image, especially for palette and VQTC methods as it spans much of the RGB colour cube and only 15% of the texels use repeated colours. PC and (post mid 90s) Console Texture compression To reduce data costs, some PC games and early games consoles also made use of palette images, which is a form of Vector Quantisation (VQ). Palette-based approaches make the assumption that a given image only uses relatively small portions of the RGB(A) colour cube. A problem with palette textures is that the compression rates for the achieved quality is generally rather modest. The example texture compressed to "4bpp" (using GIMP) produced 

Possibly "all of the above". I've worked with different generations of the same vendor's hardware which have, variously, required software (either the API/driver or the texture tool) to preorder the texture data into a Morton/tiled layout, and others which, I think, had more "built-in" processing. 

The seam won't be rasterized twice because of the OpenGL or D3D fill/rasterisation rules. What I think Humus is alluding to is that a fragment shader may be executed multiple times on pixels spanning the shared edge, which just implies wasted effort. The moral of the story is "avoid small or long/thin triangles if you want efficient rendering". 

I'm assuming that you've started with homogeneous 'world space' vertices, i.e. something like, $[x,y,z, 1, u,v, R,G,B]$, applied a projection matrix to get $[X_p, Y_p, Z_p, 1, u, v, R, G, B, w]$, and then, after clipping, divided through by w to obtain screen coordinates $[X_p/w, Y_p/w, Z_p/w, 1/w, u/w, v/w, R/w,... ]$. For convenience, let's rename $u/w$ and $v/w$ as $u'$ & $v'$, $1/w$ as $w'$, $R/w$ as $R'$ etc. To do perspective correct texturing I assume you are linearly interpolating, per pixel, the $u'$, $v'$, and $w'$ values, computing the reciprocal of the interpolated $w'$ and multiplying that by $u'$ and $v'$ to obtain the per-pixel U & V coordinates. To do perspective correct Gouraud, simply use the same approach with the interpolated $R'$ , $G'$ and $B'$ values. The most expensive part is typically the division (i.e. reciprocal) but that can be shared with the texturing. 

Assuming a perspective projection and a view point external to the sphere, then the 'boundary' formed by the view point and the circle on the sphere which forms the horizon WRT the view point, will be a cone. Doing a perspective projection (onto a plane) is then equivalent to intersecting this cone with the plane which thus produces a conic section. FYI the four, non-degenerate, cases are shown in this image from Wikipedia An ellipse/circle is thus a possibility, but not the only one - unbounded parabolas or hyperbolas (and I guess if the plane passes through the eye, even degenerate cases ) are possible. 

The key expression you may be looking for is "Post Transform Cache". This is usually effective because of the natural vertex reuse, but reordering your mesh, e.g. with Hoppe's method (note: the page also lists some more recent work) or Forsyth's can improve things further. 

(Promoting "comment" to an answer) It looks to me that it's just finding monotonic runs of edges. Given a defined winding order (in this case anticlockwise), then you can identify {P6, P7, P8, P0} as a decreasing run, as is {P2, P3, P4}. Since the left most vertex, P8, is in (**the middle of) a decreasing run, the decreasing runs define left boundaries and, therefore, increasing runs are right boundaries. **it just occurred to me that if the left most point is at the beginning/end of a monotonic run then a slightly more involved rule will need to be applied involving the edges entering/leaving that point. 

Do you mean haze? FWIW, in Australia the Blue Mountains get their name from the release of oil into the atmosphere from the eucalypts. 

Rather than trying to speed up your shader by fiddling with the code, can I suggest you make use of the GPU's inherent ability to rapidly fill polygons? This will allow it to eliminate the vast bulk of the cases where there is no need to even make a decision. What I suggest you try is something like the following: Create, say, a heptagon (i.e. the blue dashed outline (note: it shares some of the boundary with the green but I just noticed this is not particularly obvious in the diagram)) that starts from the original location and extends to the new position but lies inside the extremes of the new location's circle. The shader for this is then utterly trivial. 

If we find $A_0$ and $A_n$ are outside opposite bounds of B and that $B_0$ and $B_m$ are on the outsides of the bounds of A, then, by the continuity of Beziers, there must be at least one intersection. Case 6 If we can't immediately show either of the above cases, then split each of the Beziers into two "halves", i.e. $A^1, A^2, B^1, B^2$. This is relatively straightforward (left as an exercise to the reader) but is particularly trivial for quadratic Beziers: Recursively compare the 4 combinations: $(A^1,B^1), (A^2, B^1)...(A^2, B^2)$. Clearly if all pass case 1, there is no intersection. If any fail 1, then continue with the rest of the tests with that reduced subset. Case 3 & 5 This is where it becomes slightly more tedious. If "case 3" gets past the "case 1" test, it seems to me that you need to solve for an actual intersection. Given that there is a simple process to map the N control points of a Bezier, A(s), to the N-1 points of the Bezier, A'(s), representing its 1st derivative then (provided care is taken for the relatively rare, so-called "degenerate" situations where the 1st derivative does to zero), then Newton iteration (on one dimension) could be used to find potential solutions. Note also that, since the control points of A'(s) are a bound on the derivative values, there is the potential to do early elimination of some cases. Case 5 seems relatively unlikely, so perhaps only if after a few recursions there is no conclusive proof, one could try each end point of A against curve B and vice versa. This would only give a proof of intersection - not a proof of non-intersection. 

We know the curve starts at $A_0$, terminates at $A_n$, and will lie inside the convex hull. For simplicity let us compute the direction of the line segment $\overline{A_0A_n} $ and the compute the bounds on either side (i.e. take dot products of the remaining control points against the perpendicular to $\overline{A_0A_n}$). If we do the same with curve B we get the following (possible) case: 

*Dreamcast being a notable exception. **for certain definitions of 'long', e.g. a small integer multiple of the screen width/height. 

ETC1: 4bpp RGB Ericsson Texture Compression also works with 4x4 blocks of texels but makes the assumption that, much like YUV, the principal axis of a local set of texels is often very strongly correlated with "luma". The set of texels can then be represented by just an average colour and a highly quantised, scalar 'length' of the projection of the texels onto that assumed axis. Since this reduces the data storage costs relative to say, S3TC, it allows ETC to introduce a partitioning scheme, whereby the 4x4 block is subdivided into a pair of horizontal 4x2 or vertical 2x4 sub-blocks. These each have their own average colour. The example image produces: 

I did implement a ray tracer based on Amantides' work but, as that was years ago, my memory of the paper is a little rusty. However, ignoring this particular case, in general when it comes to working with fractional coverage e.g. Alpha compositing, (see "A over B") my understanding is that the usual assumption is that the items being composited are uncorrelated. Thus if A with X% coverage is on top of B with Y% coverage and C in the background, then it's assumed that one will see X%*A + (100-X%)*Y% * B + (100-X%)(100-Y%)*C Does that make sense? Obviously this will give "leaks" in the case where A and B are strongly correlated. I think I might have put a small bit mask on the rays to avoid these problems, but it was a very long time ago. 

Many years ago I worked on a ray tracer that handled parametric surfaces, so this is unlikely to be state of the art, but, IIRC, I used a combination of interval arithmetic with (binary?) subdivision and Newton-Rhapson. The interval arithmetic + subdivision constructed (conservative) bounding boxes which could be used for intersection rejection. I think I may also have used interval arithmetic on the 1st derivative to help determine when it was safe to launch into Newton-Rhapson. Typically the Newton steps converged very rapidly. You can also pre-dice your parametric surfaces and put them into an acceleration structure (Voxel grid/BVH) to speed up the process. This work was based on/inspired by Daniel Toth's 1985 paper "On Ray Tracing Parametric Surfaces" 

Although this is borderline "off-topic" for the site, there is perhaps a simple answer: Why not use a webgl approach such as this one? $URL$ (parent page here: $URL$ Just rotate the image of the moon around until you have one of the poles visible and you'll see the typical artefacts you get with mapping a rectangle to a sphere. Though I've not tried, you could probably modify the code (use, say, "view page source" to read it, or save a local copy and edit) to load whatever texture your heart desired. 

Ignoring Non-uniform B-splines (rational or not), I have had some experience with rasterisation of Beziers and, since there is a trivial mapping from Uniform B-Splines to Beziers, those too. I have used two different techniques: The first was a scan-line renderer that used Newton-Rhapson to compute the intersection of the current scanline with the curve. This requires the first derivative but, as that can be trivially derived, again as a Bezier representation, from the control points of the parent curve, it's easy to obtain. Further, the bounding box of the control points of the 1st derivative can be useful to detect if the region might have more than 1 intersection with the scan line. The current scanline can use the previous scanline's solution as a starting point - usually that gives a highly accurate solution with one iteration. Alternatively, a second approach is to simply apply binary subdivision of the curve and render it as series of straight line segments with your favourite polygonal renderer. If I recall correctly, at least with cubic Beziers, with each subdivision the error between the line segment joining the end points of the (sub)Bezier and the true curve decreases by a factor of 4. In most cases it therefore doesn't take many subdivisions before the difference between the polygonal approximation and the true curve is insignificant. Finally, if you need info on derivatives of Beziers (or almost everything else related to them), this astounding web-page appeared in my twitter feed recently. 

[Disclaimer: I think the following should work but have not actually coded it myself] I couldn't think of a "trivial" method of producing a yes/no answer but the following would be a reasonable approach to a practical solution to the question. Let's assume our curves are A(s) and B(t) with control points {A0, A1..An} and {B0,..Bm} respectively. It seems to me that, given a pair of 2D Beziers for which we wish to determine do or don't intersect, there are six cases to consider: 

I'm not sure what exactly you are asking but, IIRC, to do CSG with ray tracing, you maintain not just the closest intersection with an object, but a list of ordered pairs of [Inpoint, OutPoint] 'interval' distances. With a single sphere test (or any solid convex primitive for that matter), this list would have at most one [Inpoint, OutPoint] entry or is empty if there is a miss. To implement CSG you then apply the binary operator to a pair of lists, which in turn means applying them to the entries in the lists in a manner akin to merge-sorting two lists. For example, for intersection, if you had [a,b] in one list and [c,d] in the other, compute e=Max(a,c) f=Min(b,d). If e > f there's no intersection else return the intersection, [e,f]. Does that help? 

Case where we can "trivially" determine they do not intersect. Case where they intersect a finite number of times and we can "easily" determine they definitely intersect at least once (but we don't actually care where those intersections occur) One of the Beziers is degenerate, i.e. a point (which will occur if all the control points are identical). We can assume we've already handled the case where both are points. One or more of the curves are closed, e.g.. A0==An. To make life simpler, we'll subdivide such curves and start again. There are an infinite number of points of intersection because each is subset of a "parent" Bezier and they overlap. We aren't certain about the above cases and need further investigation 

Another use, that I think Dan didn't mention, is for filling non-convex polygons, e.g, and The stencil can be used to implement the fill rule which is typically either odd/even (as used in these examples) or non-zero, simply by, say, treating the list of vertices as a triangle fan or, with reordering, a triangle strip, and setting the stencil appropriately. Finally an enclosing object is drawn but restricted to just those pixels that match the appropriate rule. One catch, if you use the non-zero rule, is that you don't want to "overflow" the max stencil value. If, however, you are going to draw the same polygon repeatedly (with or without transformations), it might be more efficient to first apply a triangulation algorithm, which would eliminate the wasted overdraw. (FWIW the supplied images come from a triangulation algorithm I worked on about a decade ago for SVG/OpenVG.) 

Just treat it as a special case of a point light + shadow ray. Whenever a ray hits the screen (or any surface onto which you'd like to project the slide), fire another "projector" ray back towards the projector. If it is not obscured, then use the X & Y angles (relative to the coordinate system of the projector) to look up a texture map of the slide. 

Note that all of the vertices may have inaccuracies. Although the example above shows a crack, the T-junction may instead result in overlap along the edge causing pixels to be drawn twice. This might not seem as bad, but it can cause problems with transparency or stencil operations. You might then think that with floating-point the error introduced will be insignificant, but in a renderer, the screen-space vertex (X,Y) values are nearly always represented by fixed-point numbers and so the displacement from the ideal location will usually be much greater. Further, as the rendering hardware "interpolates" the line segment pixel-by-pixel with its own internal precision, there is even more chance it will diverge from the rounded location of E. If the T-junction is "removed" by, say, also dividing triangle ABC into two, i.e. AEC and EBC, the problem will go away as the shifts introduced by the errors will all be consistent. Now, you might ask why do renderers (especially HW) use fixed-point maths for the vertex XY coordinates? Why don't they use floating-point in order to reduce the problem? Although some did (e.g. Sega's Dreamcast) it can lead to another problem where the triangle set-up maths becomes catastrophically inaccurate, particularly for long-thin triangles, and they change size in unpleasant ways.