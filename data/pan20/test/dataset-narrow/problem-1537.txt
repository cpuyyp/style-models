A Confusion Matrix is an important tool to measure accuracy of a classification algorithm. It compares predicted class of an outcome and actual outcome. 

Also RF isn't a good option when we have severe imbalanceness As Random forests are built on decision trees, and decision trees are sensitive to class imbalance. Therefore each tree will be biased in the same direction and magnitude (on average) by class imbalance. Just a side note, Bagging and Boosting might sound similar but aren't similar.. Xgboost can take care of Imbalanceness whereas RF's don't.. Hope it helps.. 

Bear with me if its a noobish question, Today itself i was trying to fit a model using simple followed by a with for both seperately. But to my surprise, when i switched the parameter in CV, it seemed that the model has changed in terms of performance measure (keeping everything constant as before). I am saying so because I submitted the model predictions on Kaggle and there was a sharp difference between the Public Leader-Board Score. The metric used in the competition was , i had applied and they all gave different scores on LB. So the question is(are) 

Don't use the matrix obtained from your function rather, It looks like is a DataFrame too, so you can simply use : 

A baseline is a method that uses heuristics, simple summary statistics, randomness, or machine learning to create predictions for a dataset. You can use these predictions to measure the baseline's performance (e.g., accuracy)-- this metric will then become what you compare any other machine learning algorithm against. In more detail: A machine learning algorithm tries to learn a function that models the relationship between the input (feature) data and the target variable (or label). When you test it, you will typically measure performance in one way or another. For example, your algorithm may be 75% accurate. But what does this mean? You can infer this meaning by comparing with a baseline's performance. Typical baselines include those supported by scikit-learn's "dummy" estimators: Classification baselines: 

Also you have misunderstood the .. It's not the exponent rather the symbol of inverse of a Matrix Also use for Transpose...(a bit Pythonic Convenience) Just to help you out, Search For your second Query, Refer here Just to add a little, you stop when you see that your loss isn't improving any more as such or is improving at the Hope this helps.. 

Hierarchical clustering is where you build a cluster tree (a dendrogram) to represent data, where each group (or “node”) links to two or more successor groups. The groups are nested and organized as a tree, which ideally ends up as a meaningful classification scheme. 

The method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this. The same can be done with the following line: 

The below piece of code will generate the following Image(your's is Subplotting Three of them, so you will get 3 different axe's and per axes you have to use fill-between) (Kindly ignore the Axis Label's..) 

Some image credits apply...... It's from a blog which applies the idea of Computational Graphs as explained in CS-231N by Karapathay.. What differs from your graph is that we need to take into account $*,+,/,-$ symbols also.. Will have a careful look at yours also and update here, but till then, this graph works as I have also derived the expressions from this one... 

This is what Machine Learning models are used for..(to predict what they think will happen in the near time based on the data they have been fed into....) A simple answer is this The first couple days of data are the most important , you need to gauge and have a watch over your model.. As with any incremental learning, you can learn more recent stuff but also underfit the past significantly more than without learning the new stuff. Therefore, it's why there's a monitoring to perform on models which use incremental learning (especially on production systems). Provided that other things remains similar with time..(same Preprocessing, same scaling etc..) But let's say if you have tons of data everyday, then nearly at the month end, the model can be completely trained on the last months data or else checkpoints needs to be kept 

Add more columns when you are doing group by in the first parentheses.. First we should understand why it's giving this result.. It's similar to Sql,we are applying an aggregate function on a grouped by value, That's why it's giving only one value, If you want to have further information, Add the columns name in the first parentheses in a $list$ It will be something like this 

If you are on Linux/Ubuntu, Then have a look at it will do it for you..(if all your files are concatenated in a single one) Or else, use split the folder manually Or else, use from sklearn Or, write a custom script yourself... (Trust me it will be easier)(had somewhere written the helper script on this forum also..) Spoiler - You will be using modules like is,shutil etc.. 

This comments aren't all mine, I have asked on a slack forum, boxplot is shouting at you: skewness, and also high dispersion. Can not ask much more to a boxplot than location, dispersion and skewness.. Also checkout this term heteroscedasticity(completely suits your case) Try switching the transformation to a log plot or lower.. Also your eda can't be boxplots dependent as prices are involved here..one of the remedy would be doing box-cox transformation $URL$ $URL$ To have a look at it, $URL$ 

How do I represent a color as an activation value within a neuron? ( might be off-topic) I want to detect colors Using Neural Nets In my Knowledge any activation function which is generally used will push the values in between which isn't of any use to me... Just wondering how Windows 10 decides on the theme color change when we switch themes.. (led me to think about this or its just simple averaging on pixel values of the channels?) So seems like an impossible task for Neural Nets then? 

Here we have points, in each of categories with associated numerical values. Instead of using Logarithms, you can also use is "iterated logarithm": 

It's probably this way..(not sure , give it a try) Iputs needs to be reshaped to be [samples, time steps, features] so 

Summary of RF: Random Forests algorithm is a classifier based on primarily two methods - bagging and random subspace method.  Suppose we decide to have number of trees in our forest then we first create S datasets of "same size as original" created from random resampling of data in with-replacement ( times for each dataset). This will result in datasets. Each of these is called a bootstrap dataset. Due to "with-replacement" every dataset can have duplicate data records and Ti can be missing several data records from original datasets. This is called Bagging.  Now, creates trees and uses random subfeatures out of possible features to create any tree. This is called So for each bootstrap dataset you create a tree . If you want to classify some input data you let it pass through each tree and produce outputs (one for each tree) which can be denoted by Final prediction is a majority vote on this set.  Out-of-bag error: After creating the classifiers, for eachin the original training set i.e. , select all which does not include . This subset, pay attention, is a set of boostrap datasets which does not contain a particular record from the original dataset. This set is called out-of-bag examples. There are n such subsets (one for each data record in original dataset T). Out-of-bag estimate for the generalization error is the error rate of the out-of-bag classifier on the training set (compare it with known yi's). 

It would be great if Someone can So that the question can be answered formally.. Based on the above Links, here's what I got,(Came to know that Convolutions are equivalent to Correlation) Image 1 Image 2 Stack doesn't allow me uploading image so using gdrive..(do rotate one of the images, the first one actually) Hope it will give you an idea... 

No you shouldn't do that as the statistics of your training set and your test set isn't the same.. We apply the same transformations on both the data-sets but not using the same statistics.. Also why always filling by mean is preferred? Search for interpolation in data-frames (requires scipy) 

Yes this is definately possible... (Probably Known as ) Split the conv-nets into 2 parallel arch's at the very end , before We generally add a single-dense-layer after Feature Extraction, here you will have a split into two...(the image attached will make it clear) 

I hope I got your question correctly... It’s called “Batch” Normalization because we perform this transformation and calculate the statistics only for a subpart (a batch) of the entire training set not as a whole. 

Based on a credit risk scorecard, application for credit card are classified as “Good” and “Bad”. “Good” indicates applicants paying back dues on credit card and “Bad” indicates customers defaulting on the dues. Now, the customers are compared against actual performance of the customer payment behaviour after say 18 months. So, comparison of Predicted Class (“Good” or “Bad”) to actual customer behaviour state (“Defaulted” or “Regular”). 

If you choose your alternative to Tree based models, then you really have an upper edge here as compared to all other linear/logistic Regressions etc.. People generally use as a process which often generates tables of distances with even more numbers than the original data, but making in fact simplifies our understanding of the data. Distances between objects can be visualized in many simple and evocative ways, one of them is .. 

You don't need to upload them if you have a download link ...( it would be faster if you can upload them all as either ways you have to do so.. So its better to upload them first and then download them in your notebook every-time you run it) If you have a download link then just this 

It's just that it helps when we do the backprop of error while differentiating, as the in the denominator cancels out with the which we will get from the exponent.. So it's just a matter of Mathematical Convenience... 

Just adding some links which I refer to from time to time thinking that someday I will be able to do this from scratch (seems impossible with deepening of Layers though...) 

Adding to The answer given by Toros, These(see below bullets) three are quite similar but with a subtle differences-:(concise and easy to remember) 

Sorry for asking more than one question together but they are kind of related, Thanks. (the difference was around $3\%$, I know it's not a very big number but still Why?) 

It will probably work but less Efficiently(or lower Accuracy) if you train the Network one after the another (like trained for Dog's first , followed by Cats etc..) because 

At first remember that a dictionary is just bunches of keys and value pairs... So, if the key is $a$ and the value is $10$ , you can access the value by doing (d is the dictionary) Beaware that there is no order to storing the values, you can use OrderedDict for that.. Also you can have two values for the same key in a simple dictionary. Here's the complete answer 

Cost Matrix is similar of confusion matrix. It’s just, we are here more concerned about false positives and false negatives .There is no cost penalty associated with True Positive and True Negatives as they are correctly identified. The goal of this method is to choose a classifier with lowest total cost. 

You need to search for nbextensions for jupyter notebooks.. Just install them and search for code folding.. It's exactly what you want It has got markdown, raw, Latex, code etc ... 

If you want to save your list itself, Convert it to numpy format using and then use and reload it whenever required using 

If I were to try to generalize, I'd say that it's all about balancing an increase in the number of parameters of your network without over-fitting. So if e.g. you start with a reasonable architecture and amount of dropout, and you want to try increasing the number of neurons in some layer, you will likely want to increase the dropout proportionately, so that the same number of neurons are not-dropped. If was optimal for neurons, a good first guess for neurons would be to try . Also, I get the feeling that dropout near the top of the network can be more damaging than dropout near the bottom of the network, because it prevents all downstream layers from accessing that information. As to why is generally used, I think its because tuning things like the dropout parameter is really something to be done when all of the big choices about architecture have been settled. It might turn out that is better, but it takes a lot of work to discover that, and if you change the filter size of a conv-net or change the overall number of layers or do things like that, you're going to have to re-optimize that value. So is seen as a sort of placeholder value until we are at the stage where we are chasing down percentage points or fractions of a percentage point. Original Paper and the opening paragraph is from the original paper... 

Suppose our training data set is represented by T and suppose data set has M features (or attributes or variables). 

There is always a trade-off between Type I error/False Positives (accepting Bad Customers) and Type II Error/False Negatives (Rejecting Good Customers). We generally require to optimize between False Positive Rate (Type I Error) and False Negative Rate (Type II Error). So, role of cost matrix comes in picture to find the optimal cut off value for a classification rule.  Now, going back to Credit Risk Model. The cut off value optimize between cost of an opportunity loss (miss to accept a good customer/Type II Error) and cost of accepting a potential defaulter (involved in loss due to default). 

The RandomForestClassifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training observations. The error is the average error for each calculated using predictions from the trees that do not contain  in their respective bootstrap sample. This allows the RandomForestClassifier to be fit and validated whilst being trained.. So we don't actually need to a Validation dataset or k-fold in this case (answer isn't strictly acc to your Question ordering) 

Based on the given Example If they are literally the same everywhere except in a small region, just subtract image 1 from image 2 to find the differences... And then we can check the positive and negative values where they are differing by a considerable margin to classify them 

Removing redundant features One thing that makes it harder to interpret a variable is that there seem to be some variables with very similar meanings(redundant features...) Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy. 

Edit -1 Recently enough, I came across that we can cascade group by together by playing smart and using the Here's the working example (ignore the column names...it's relevant to my dataset) 

They are not using the whole data for cross validation as such ( it's just an illusion) When the argument is an integer, cross_val_score uses the or strategies by default, the latter being used if the estimator derives from .. So it's kind of automated inside the call.. Check this kaggle kernel link 

Edit - (just to make the answer complete) We also have something known as Partial Dependencies while using RF's which is also a very helpful insight to explore even further... 

The label_map variable is a dictionary, Then from this the relation can be derived between the probability scores and class names. Basically, you can create this dictionary by this code. 

Why we prefer mixing of similar Image Groups? It's more of Human Nature to do a task which can en-compass similar things at once, rather than doing the particular tasks repeatedly.. 

Each iteration is supposed to provide an improvement to the training loss. Such improvement is multiplied with the learning rate in order to perform smaller updates. Smaller updates allow to overfit slower the data, but requires more iterations for training. For instance, doing 5 iteations at a learning rate of 0.1 approximately would require doing 5000 iterations at a learning rate of 0.001, which might be obnoxious for large datasets. Typically, we use a learning rate of 0.05 or lower for training, while a learning rate of 0.10 or larger is used for tinkering the hyperparameters... 

A dendrogram is a type of tree diagram showing hierarchical clustering — relationships between similar sets of data. They are frequently used in biology to show clustering between genes or samples, but they can represent any type of grouped data. The columns under the same split at the leaves are somewhat having relationships between them or have similar attributes.., that is what we try to Explore and deepen out understanding about in order to cut down redundant Features.... Sample Dendrogram looks like this one...