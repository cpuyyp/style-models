Control physical access. Ideally your switches, patch panels and servers should be in a secure locking location that has appropriate cooling and power provisions such as an UPS. Keep an on-site key somewhere but resist the pressure to allow your clients access to your equipment - well meaning but ignorant employee's can often create bigger problems out of smaller ones. Make sure you have some kind of access log or audit trail (this can be as simple as incident tracking system). If a network drop is unused, disconnect its patch cable or disable the port on the switch. I really avoid using MAC address based security because inevitably someone switches offices or you have to replace a machine and then you have to update the MAC address table again. MAC addresses make bad authentication tokens anyway as an attacker can easily discover a trusted MAC address via packet sniffing and then change their MAC address to match. Assume that your Wireless Password will be Shared. This is something that smaller deployments really struggle with in my opinion. The "Enterprise" grade solution is 802.1X authentication which requires substantial infrastructure. If you setup a WP2 access point and hand the password over to your clients expect that they will give it out to anyone who asks politely. If the wireless really needs to provide access to the "private" network, then you yourself should setup the client machines and truly keep the secret, well secret. If your clients really need network access for their vendors, contractors and other users setup a separate VLAN and SSID. This can be easily accomplished using Multi-SSID capable access points that understand VLAN tagging. Use a Firewall. This is pretty basic. Have some sort of Layer-3/4 filtering in place between your client's network and the Big Bad Internet and test it to make sure it is doing what you think it is doing. Do not allow your clients to self-host services (or if you do, do it judiciously). Small deployments often don't have the infrastructure to safely host publicly accessible services. If you clients need these sorts of services, you will probably be best served by using an external hosting company that specializes in providing these services (for example, have a company that specializes in web hosting host their website, not an old workstation in a closet somewhere). Do not allow your clients to have Local Administrator (or if you do, do it judiciously). Don't give your clients Local Admin privileges. There are so many good reasons to not do this I won't even bother enumerating them. And do not give them administrator (or any kind of) access to their local server no matter how "experienced" they say their new intern is with technology. 

As contractors come and go they get added to this security group and the Collection/s updates along the way. We then deploy specific Applications they need to that user Collection. I think in the long run this will be easier and simpler to maintain. 

I see you are using DD-WRT. That's unfortunate. I (and others here) have found it to be notoriously unreliable. Insure that DNS is working. From your local network client (192.168.1.104) do a DNS lookup on your domain and make sure the correct 222.111.111.001 address is returned. This will verify that DNS is functioning appropriately for your local clients. Then see if you can access your server from a local client using its "external" IP address (i.e., 222.111.111.001). If you cannot access your server this way, then I suspect your router is the problem. If you can access your server, I suspect DNS or possibly a bad virtual hosting configuration (you mentioned a webserver) could be at fault. 

Background - "Extranet" We have about a dozen users that are on what I would call an Extranet. It is an air-gapped LAN that is physically in the same location as our primary LAN. We share the Extranet LAN with a sister agency which is in another building and while we own the equipment they administratively manage the actual routers and switches. Once the connection leaves our building it travels through our sister agency's infrastructure where our user's traffic is mingled with with their user's traffic. Finally it is routed out to a financial institution's network via a dedicated circuit so all of our users can do financial stuff. We have informal and formal agreements in place with both our sister agency and the financial institution which include these restrictions: 

I thought maybe you could do this with File Group Definitions in File Screening Management functionality of File Server Resource Manager but it appears that the File Group Definitions do not support regular expressions. In Windows Server 2012 R2 there is expanded functionality to File Server Resource Manager to incorporate regular expressions in Automatic Classifications which might help you identify "long file names" reactivity. Since you are on Server 2008 R2 the only other thing I can think of is some PowerShell hackery: 

Except... nothing in IIS changes. I'm either not remembering a step we did previously to move the WSUS IIS Site or something is broken. 

I have a SuperMirco server that is using the MBD-X8-DTL-I-O mainboard which is equipped with one of the lovely Intel ICH10R based fake-RAID controllers (confusingly enough reported as an Adaptec device -- I'm assuming that it is some unholy marriage of the ICH10R chipset for the controller and Adaptec firmware). After some thrashing around I was able to find the correct ACHI drivers and get Windows Server 2008 R2 installed on RAID-1 setup. Part of my confusion comes from the fact that Windows reports the controller as an "Adaptec Serial ATA HostRAID" and the Hard Drive as an "ADAPTEC RAID 1 SCSI Disk Device" whereas during the boot process the Controllers hardware clearly reports that it is actually using the Intel ICH10R ("Controller #00: HostRAID-ICH10r at PCI Bus:00...). I don't expect much from this controller in terms of management but is there any software (3rd party or otherwise) that can monitor the health of the RAID array and notify me (preferably via SNMP to our OpenNMS server)? This server is going off-site and it would nice to know that a drive has died without physically having to look at the Blinking Lights. All I can find is the cheesy Intel RST software which I don't think has any remote monitoring and notification facilities (it also complained that it couldn't be installed due an "incompatibility error" which is strange considering that the controller chipset is an ICH10R - but that is another question). For those of us making do with these fakeraid cards on Windows-based platforms: How do you monitor your RAID array? (If this is just handled by Windows, could you please point me towards a KB or TechNet article? I'm primarily an Unix admin, so if there is a simply a Windows equivalent of mdadm or raidtools I am ignorant of it). 

This is general advice (you asked a general question) but here is the best practices from a parent organization of a previous employer and I have yet to have "Best Practices" summed up so simply. This is pretty much my job in a nutshell. 

Once we had the Collection created we would then deploy our Package-Program against it. I'm trying to replicate the same logic using the Application-Program's Global Conditions and Requirements logic. All my attempts to build my WQL query based Global Condition result in a wbemErrTypeMismatch error (). 

WARNING: You should test this on a workstation you have physical access to before trying it on a remote workstation where your only administrative access to it depends on that interface. 

If one of your laptop goes missing, and you either A) have confidential data on the machine, or B) can't confirm that there is NO confidential data on the laptop then you need some kind of encryption scheme. Of course, you (and your organization) need to decide what criteria you want to use to consider what is confidential and what isn't. I recommend you don't neglect this step; you don't want to go to all this work if it is not necessary, nor do you want to elevate your organization's equivalent of the office's cookie recipe to a level of secrecy that demands AES-256. If you've already been through the process, then you're good to go. 

What am I trying to do? We have a few SCCM clients, mostly publicly facing websites running on Windows Server 2008 R2's IIS 7.5, that are monitored by a system called XYmon. We have recently noticed these servers are rebooting after installing their monthly Windows Updates about an hour early. There is a certain amount of delay inherent in the SCCM Client Actions and the monitoring system but XYmon loses connection with these machines right around 19:20-19:30ish whereas the rest of the monitored machines seem to reboot about an hour later around 20:20-20:30. The Maintenance Window that I expect to be applied starts at 20:00 and ends at 22:00 and reoccurs every Thursday. I am trying to figure out why these machines are installing their updates an hour early. I know that multiple Maintenance Windows are "union-ed" or merged so I suspect there is another Maintenance Window that is also applying to these clients. These machines are also in a non-domain joined DMZ so I'm not going to rule out any timezone / clock skew issues either. What did I try in order to make it happen? I figured the timezone / clock skew issue was the most likely but both machines were in the correct timezone, had synchronized time and managed to handle the Daylight Savings change that happened on March 8th appropriately. My next hypothesis is that we had an errant or old Maintenance Window that was applying to a Collection these machines were in. This seems a little unlikely to me since there is another machine that should be all the same Collections which reboots at the correct time of 20:00-ish. Let's make sure the client is actually rebooting when the monitoring system says it is! If I check a client, shows the boot time at 19:22. The Event Log seems to collaborate this: 

Installing and running Windows Server 2012 R2 and other previous versions of Windows Server on a Compact Flash card or USB is un-supported. The only supported installation of a Windows Server operating system onto a USB device is Hyper-V Server 2012 R2, the dedicated hypervisor of Windows Server which is basically just Hyper-V and the Server Core "management" virtual machine running in the parent partition (Run Hyper-V Server from a USB Flash Drive). As far as I am aware, there is no supported installation path for Compact Flash devices. Big Important Red Flag: 

At this point I'm kind of at a loss on how to continue to troubleshoot this issue. I really want to avoid re-installing the WSUS role and/or SUP if possible due to the imminent release of Microsoft Updates tomorrow. Any advice on further troubleshooting? 

I have a legacy NetWare 6.5 server that offers file shares to about 40 clients. It also acts as the router between our two internal subnets and our parent agency's network. We are planning on replacing the routing functionality with a Juniper product. At the same time we are refactoring our physical network infrastructure - currently the NetWare server has an IP presence on both subnets. I would like to disable two of its three interfaces so it only has one connection to the network. Unfortunately, I cannot figure out how the Novell Client (4.91.5.20080922 on Windows XP SP3) resolves the NDS tree netware_server.department.mycorp to an actual IP address. Testing has shown that if those interfaces are no longer there, clients will fail to "resolve" the NDS object to the correct (still existant) IP address and chaos will ensue. I have tried setting the "Server Cache Timeout" to 0 in an effort to force a "name resolution" of netware_server so I can look at the actual TCP/IP conversation in Wireshark. I have also tried adding an entry to the C:\WINDOWS\system32\drivers\etc\hosts file with the NetWare server's NDS Object Name and then limiting the Novell's Client's Name Space Providers (Properties - Protocol Preferences) to just "Host File" trying to force a new "lookup" using the /etc/hosts file instead of whatever arcane method is currently used. Both of these attempts came from this TID10057730. Both have failed. How does the Novell Client resolve an NDS Object Name like netware_server.department.mycorp to an actual IP address? How can I force the clearing of any client-side "NDS name" cache? How can I force that "NDS name" resolution to always resolve to an IP address that I manually specify? EDIT: First off, we're running pure IP. If you happen to still be running IPX the Novell Client behaves quite differently. 

The consensus (which I agree with) seems to be don't defrag on servers because the benefits aren't worth the performance hit during the actual defragmentation. However, TechNet's article on doing Physical to Virtual conversions recommends defragmentation as a method to reduce the amount of time required to do a P2V. This is especially important if you have a limited maintenance window in which to complete your P2V. 

This is completely unintuitive but I think I understand it correctly. Global Conditions just set up the conditional part of the whole Application-Program logic not any of the evaluative Application-Program logic. For that reason, I'm not doing anything in the WHERE clause. This Global Condition should look in the namespace for the class and "return" the ProductName property. I should now be able to evaluate the value/s of that Property with my Applications' Deployment Type Requirement, right?