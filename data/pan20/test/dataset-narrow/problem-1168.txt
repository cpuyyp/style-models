Are there any problems $P$ such that an $\alpha$-approximation for $P$ (for soem choice of $\alpha$) would imply a better than 2-approximation for VERTEX COVER, which would be hard via the UGC ? 

A problem that one might think would NOT be hard on trees, but is, is the freeze-tag problem in computational geometry: briefly, the problem of scheduling wakeups for robots starting with a single awake 'bot, where makespan is the cost measure. It's known to be NP-hard on weighted star graphs. However, it's open whether the problem is NP-hard in the plane. One might argue that the NP-hardness comes not from 'tree-ness', but from 'arbitrary metric'-ness, but star graphs only give you a limited space of metrics.. 

Dick Lipton has a beautiful post from 2009 on the relationship between the factorial function and factoring. There's a lot there that is unrelated to this question, but one salient point is this theorem: 

It comes from the handbook on theoretical computer science, which had two volumes: A was for algorithms and complexity, and B was for logic and semantics. Jukka, did ICALP predate this ? Or was it in response to this ? As for benefits, I think there's always some utility in taxonomizing areas based on topics of interest, and forms of study. However, as with all taxonomizations, the problem comes when you forget to "go back up the tree and down the other side" :). EDIT: as ICALP explicitly states, this division comes from the Elsevier journal Theoretical Computer Science, which itself predates the handbook, so I think that's a more accurate source. EDIT ++: From the history of the EATCS comes this snippet about TCS, the journal: 

I'm not sure what you mean by 'overlapping points', and even less clear how you construct a 2D scatter plot from 4D data unless by doing some kind of projection. Having said that, what I think you're looking for is a dynamic near-neighbor data structure. Specifically, you want a data structure that processes queries of the form "Who's my nearest neighbor in the current set" and "insert a point into the current set" While there's a ton of work on different kinds of near-neighbor structures, your particular situation (dynamic 4D points) suggests at least starting with a grid-based structure. Make (4D) buckets based on segmenting the coordinates, and hash the points in, and then for a query you can do a spiral search (or something similar) around the cell your query lies in. The next idea would be to build a generalized quad tree data structure (a compressed quad tree) or even something involving space-filling curves, if this first approach doesn't yield the performance you need. There are even more powerful methods, but I wouldn't recommend looking into them till you've exhausted the simple approaches. Bottom line: "dynamic" and "nearest-neighbor" and "low dimensional" are your google friends. 

There's another answer that no one has really brought up. Programming can actually lead to interesting theory. A lot of the recent developments in hashing (especially tabulation hashing) are motivated not by theoretical concerns per se, but by the fact that the theoretically optimal algorithms aren't that great in practice. This of course is something you don't know unless you can write code. Even in the realm of exact exponential time algorithms, a motivation is producing algorithms that can actually work. SAT solvers are the canonical example of this. In short, the ability to code allows you to realize shortcomings and weaknesses in what might look like optimal theoretical results, and that in turn opens up new theoretical research directions. 

Find a professor to guide you, and PUT IN THE TIME ! The hardest thing you'll face is creating the open time to think about problems in the midst of classwork, assignments and exams. But you need to reserve blocks of time for your independent study and research otherwise it will be very hard to make any kind of progress. How you do this is upto you: maybe you can find a professor to meet with you once a week and set intermediate goals for you, or maybe you can set a long term goal (working through X exercises from a text) and work steadily on that. 

The answer below breaks your condition that you don't want serious restrictions placed on your hypergraph, but it might be of interest if only as related work. Your hypergraph (which I'll rename as a range space) has a corresponding dual range space (hypergraph) by interchanging the roles of vertices and edges. Your problem then amounts to coloring the elements of this range space so that a range of cardinality $r$ has $r$ colors. Let's call such a range colorful. There's been some recent work on such "colorful coloring" problems for geometric range spaces, motivated partly by problems in sensor networks. A standard question that's asked is: 

One (old but useful) reference along these lines is Woeginger's 1998 paper on the connection between DPs and approximability: 

A number of theory faculty (David Karger, Tom Leighton, Shang-hua Teng among others) went to Akamai when it started, and then returned. Rina Panigrahy is not theory faculty, but worked at Cisco for many years before returning to "academia" in MSR. Ken Clarkson was at Lucent the whole time before going to IBM, but spent a number of years "essentially" in a business unit working on a wireless project before "returning" to research full time. 

Have you seen this paper by Piotr Indyk ? It's old, but it's a good one. It solves a number of problems including k-median while making only sublinear number of calls to the distance oracle. It's not exactly the model you're looking for, but it does try to reduce the number of distance invocations. 

Since it's Friday, it's time for a CW question. I'm looking for heuristics that have wide use in optimization problems. To limit the scope to more 'theory-friendly' heuristics, here are the rules (some arbitrary, some not) 

Aaron Sterling might be able to say more about this, but clique detection has traditionally been a very important tool in chemoinformatics, where one problem is to identify common substructures between a collection of molecules known to possess certain pharmacological properties. The molecules are viewed as (low treewidth) graphs and clique detection is used to find common structures (the "cliques" are defined across graphs, not within) 

I could be off-base, but in my view some of the better topics to focus on if you only wish to pursue problems as an amateur are in discrete mathematics: combinatorics, graph theory, and even combinatorial geometry. This is because the problems in these realms are quite accessible and easy to state and ponder without too much background. That doesn't mean you can solve them without background: that will take a lot more time. But it's a good place to start. Also, what might limit you is access to literature: papers, books etc if you don't have access to a university library - in that case, working on problems that are more "current" means that you'll be more likely to find papers off researcher websites. It's possible that the days of Fermat-style amateur mathematicians are over, but I really doubt it. I've known people who started doing research as a side hobby and enjoyed it so much they are now full-time researchers. And even if you don't, you'll at least enjoy yourself. As Alessandro points out in comments, this website is a great resource for you to use as well. 

By the simple transformation $p(x_0, y) = p(x_0) p(y | x_0)$ your expression merely becomes $p(x_0) KL(p (y|x), p(y))$, where $KL(p,q)$ is the Kullback-Leibler divergence, and is always positive. therefore, your expression is always positive. 

What you are asking about is the problem of near-neighbor search under the edit distance. You didn't mention whether you're interested in theoretical results or heuristics, so I'll answer the former. The edit distance is somewhat nasty to deal with for building near-neighbor search structures. The main problem is that as a metric, it behaves (sort of) like other well known bad metrics like $\ell_1$ for the purpose of dimensionality reduction and approximation. There's a rather vast body of work to read on this topic, and your best source is the set of papers by Alex Andoni: by following pointers backward (for example from his FOCS 2010 paper) you'll get a good set of sources. 

Of course a much better answer than my previous one is the use of metric embedding theory for solving sparsest cut. A key step in the solution to the sparsest cut problem was the realization that it could be approximated by finding a good embedding of a general metric into an $\ell_1$-normed space. 

(not quite an answer, but too long for a comment) Given your new cost function, I'm wondering if there's a simpler formulatio. Let $W = \sum_{(i,j) \in E} w_{ij}$. Now your goal is to maximize \[ W - \sum_{|\{i,j\} \cap S| = 1} w_{ij} + \sum_{|\{i,j\} \cap S| = 1} w_{ij}u_j\] Collecting terms, your goal is then to minimize \[ \sum_{|\{i,j\} \cap S| = 1} w_{ij} (u_j - 1)\] Setting $w'_{ij} = w_{ij}(u_j - 1)$, your problem now becomes a min cut problem with the twist that it's not a typical min cut (you're not counting edges going from one side to the other, but both sides) 

In certain settings depending on G and the feasible region, this can yield an optimal solution (for example, if you're measuring the distance between two convex polygons), but in general, it's hard to get anything except a local optimum. Update: Based on the OP's clarification, I think a different direction might be more useful. What you're describing begins to sound like a lagrange relaxation of the constrained version of the problem (with a fixed budget). One example of this is a facility-location problem where you want to open facilities to "serve" locations, and while adding more facilities reduces the service cost, it costs more to open each one. There are also variants of this (the buy-at-bulk problem) where the cost for opening more "facilities" is a concave function. I don't understand the stochastic aspects of the problem though. 

There are two examples of a complexity-theoretic study of iterative methods. The class PLS and the recent work by Papadimitriou and Daskalakis on CLS: continuous local search. These might be useful (especially the latter as it deals with continuous domains) 

Like with Nash equilibria, the partition is guaranteed by the theorem, but it's not known if a polytime algorithm exists to find one. Gil Kalai wrote a wonderful series of posts on this topic: One, Two and Three. 

Note: I didn't specify the output domain. Again, I'm flexible, but for now let's say that $p$ and $q$ are defined over a finite domain $[1..M]$. In general, I'd also be interested in the case when they are defined over ${\mathbb R}$ (for example, if they're Gaussians) 

It's not a single problem, but the entire field of analytic combinatorics (see the book by Flajolet and Sedgewick) explores how to analyze the combinatorial complexity of counting structures (or even algorithm running times) by writing down an appropriate generating function and analyzing the structure of the complex solutions. 

At first I thought this would be easy: write down the distances between each pair of holes and each hole and the polygon boundary, and compute an MST. But it's not obvious to me that this is correct. Is this a known problem ? 

Most CS conference publications use the ACM BibTeXstyle file (or the plain style file) to cite papers, and that constitutes the de facto 'style'. Different journals and conferences often prescribe their own citation format via a different BibTeX style. So the short answer to your question is that there isn't a common style, and if you believe in using BibTex, then merely changing the style file can allow you to accommodate different styles. 

First of all, I'm not sure why you even need a kd-tree for 3D data. Any reasonable grid structure will be ok to find near neighbors (See the work by Franklin for example, on spiral search methods). Secondly, the triangle inequality is needed to prune the search space correctly, so without it things will run a lot slower. Finally, as a side note, the coefficients on the first and third square terms are of the form $\alpha, 5-\alpha$, where $2 \le \alpha < 3$. So what you really have is a space in which the distance is defined by a Mahalanobis distance (an affinely transformed Euclidean distance) which is almost fixed (since the coefficients vary between 2 and 3). So even if the triangle inequality is not satisfied (and it's very likely it doesn't), it will be "almost satisfied" in a way that will make near-neighbor searching entirely possible.