Twitter sentiment analysis as the name suggests is ideal for Twitter data. They are different because for sentiment analysis of Twitter data, we can handpick some features for example, can be tagged earlier with their sentiment and also can be understood. Whereas a generic sentiment analysis mostly doesn't include these. Also, for twitter dataset the data is hardly a of two that is 160 characters for which the model might be different from others. I don't know of any API specific for facebook data, but if your dataset is small you can give Twitter API a shot else if generic sentiment analysis might do a good option. 

A suggestion I give is to change the layers of CNN. You have 3 CNN layers, all sequentially added acting on the same shape and outputting the same output shape with same filter size. What you can try is to change the filter size and using multiple filter sizes to catch different features of different sizes. For this, try this model: 

A naive definition of Parameter convergence is when the weights or the values of the parameters reach a point asymptotically. What I mean is that, when your model training is not altering the parameter values(maybe less than epsilon-small values) it might be a good fit. For decision trees, I found this paper which explains rate of convergence and more. It might be a good read if you want to get more details. 

In the case of supervised learning you assume there is a function $f:\mathcal{X} \rightarrow \mathcal{Y}$, which means there is a connection somehow between inputs and outputs and you want to use it in order to predict. But how this function might look? Without considering the model itself, usually what happens is that there is some noise. And this noise can be in $Y$ and also can be in $X$. This noise gives use the probabilistic setup of the problem, because without it we would have only to solve some equations. Now is important to understand that the noise defines the distribution. So a random variable can be imagined as a function having something fixed and well defined and something not fixed, but taking values according with a distribution. If the variable part does not exist, that you would not have a random variable, right, it would be a simple formula. But its not. Now the $P(X)$ incorporates what happens in $X$ alone, and $P(Y)$ what is in $Y$ alone. When you predict the decision theory says that you are interested in saying which is the most probable value $y_i$ given some input value $x_i$. So you are interested in finding $P(Y|X)$. A joint probability is not always completely described by marginal probabilities. In fact, is completely described only when marginal probabilities are independent. Which means for r.v. $X, Y$ knowing $P(X)$ and $P(Y)$ does not put you in position to know the joint density $P(X, Y)$ (thought for independence you have $P(X,Y)=P(X)P(Y)$). From here you can go directly and try to estimate $P(Y|X)$. In fact if your only interest in only in prediction, this might be a fair bet. A lot of supervised learning algorithms tries to estimate directly this probability. They are called discriminant classifiers. The reason is because they are used to classify something to the class with maximal conditional probability, you discriminate (choose) the maximum value. Now arriving at your question. Notice the obvious $P(X,Y) = P(Y|X)P(X)$ than you see that by trying learning the joint probability, you also learn the conditional (what you need for prediction). This kind of approach is called generative, because knowing the joint probability you can not only predict, but you can generate new data for your problem. More than that, knowing the join probability can give you more insights related with how what are you model works. You can find such kind of information which is not contained only in marginal distributions. 

We cannot go directly from input layer to max pooling because of the in between. The reason for convolution is to extract features. Max pooling down-samples the features that have been extracted. If you think there are features which are missing because of the direct jump from a large matrix to a max pooling layer, you can add more layers of convolution in between till you seem satisfied with a size and then do max pooling onto it so that it is not an overkill. Max pooling, which is a form of down-sampling is used to identify the most important features. But average pooling and various other techniques can also be used. I normally work with text and not images. For me, the values are not normally all same. But if they are too, it wouldn't make much difference because it just picks the largest value. A very good understanding from wiki - 

If you look at the Keras documentation, you will observe that for Sequential model's first layers takes the required input. So for example, your first layer is Dense layer with input dimension as 400. Hence each input should be a numpy array of size 400. You can pass a 2D numpy array with size (x,400). (I assume that x is the number of input examples). Your output has 13 dimensions for each output document and hence your final Dense layer has to be instead of model.add(Dense(1)). Coming to how it knows, it takes the first dimension of X as number of training examples and second dimension as the size of each example. Similarly for output layer. 

My strong opinion regarding automated tasks like imputation (but, here I can include also scaling, centering, feature selection, etc) is to avoid in any way do such things without carefully inspecting your data. Of course, after deciding what kind of imputation to apply it can be automated (under the assumption that the new data has the same shape/problems). So, before anything, take a wise decision. I often wasted time trying to automate this things, destroying my data. I will give you some examples: - a marketplace encoded as N/A, which I missed and considered to be North/America - numbers like -999.0, because the data producer could not find a better replacement for missing data - number like 0 for blood pressure or body temperature, instead of missing data (it is hard to imagine a living human with 0 blood pressure) - multiple placeholders for missing data, due to the fact that the data was collected from various sources After that you need to understand what kind of imputation would resemble better the information from your data for a given task. This is often much harder to do it right than it seems. After all those things, my advice is to delay your imputation task to an upper layer where you have tools to reproduce on new data and to inspect if the assumptions for the new data are not violated (if it is possible). 

In a classification project, on the training sets, I ran a selection of classifiers. These give me about 20-30% accuracy at best. For each sample, I generate probabilities of each class. I want to make a new model which takes these probabilities and gives a weighted average of the probabilities which is accurate. For this, I tried averaging probabilities. I also tried taking the best probabilities from each model (For instance, assuming Class A has better precision/recall for model1 output, I take model1's outputs for class A and so on). This also hasn't improved the accuracy much. Can you suggest some ensembling techniques? 

Apart from this, I suggest you to use layer. It helps a lot according to my personal experience. Also, do use adam optimizer. Experiment with various pooling mechanisms. Remember that there is a lot of experimentation that has to go on, so try various other parameters and filter sizees too. 

I used Binary classification for sentiment analysis of texts. I converted sentences into vectors by taking appropriate vectorizer and classified using OneVsRest classifier. On another approach, my words were converted into vectors and there, I used a CNN based approach to classify. Both when tested on my datasets were giving comparable results as of now. If you have vectors, there are already really good approaches available for binary classification which you can try. On Binary Classification with Singleâ€“Layer Convolutional Neural Networks is a good read for you for classification using CNNs for starters. This is one of the first blogs I read to gain more knowledge about this and doesn't require much of pre-requisites to understand(I am assuming you know the basics about convolution and Neural Networks). 

Technically speaking the inverse of the distance should not pose any problem. Considering the case when you have to classify an observation (instance) whih is identical with one observation from the 'learned' data set, than the inverse distance is not defined by a direct formula since you have to divide by zero. But considering the math behind, inverse of the zero distance is positive infinity. Which means that, the weight for the identical point would dominate all other weights of the data instances. Thus the classification for that specific point could be taken as the class of the identical observation from the data set. On the other hand the inverse of the euclidean distance is only one type of distance. You can always apply a kernel function on that to weight non-uniformly the contributions. For example you can choose a normal density with mean equals zero and variance equals 1. Than, your weight would be the value of density function for the euclidean distance. 

As always clustering is about what is the meaning of distance, since that encodes at least some part of your question. So you question is which channels are similar, where similarity is defined on users. Usually you do not assume some nesting on your instances, but what you have is basically a nesting of users in channels. So you have to incorporate this kind of nesting into the distance / similarity function. I would start with some observation on similarity function. We denote similarity between instance $i$ and $j$ with $d(i,j)$. Usually this function obeys the following condition $d(i,j)\ge0$ for any $i$ and $j$. Note that we usually have equality only on identical data points. The main consequence of this observation is that we can have basically an identity at user level. What you want is an identity at channel level. One way would be to define the similarity function like: $$d_c(i,j) = I_{c_i\ne c_j}d(i,j)$$ where $I_{c_i\ne c_j}$ is $1$ when channel of instance $i$ is different that channel of instance $j$. The main effect is that now all the points from the same channel are considered identical since the distance between them is zero. When the identity function is $1$ the distance is given by your real business distance which should be constructed by you and answer your question. If you use such kind of function in a hierarchical clustering basically it will start to find first some clusters which are identically with your grouping on channels and later on it will join clusters which are similar. This kind of approach will work even with a kmeans algorithm and perhaps with most of the clustering approaches. A slight different approach would be to define your $I$ function to return $1-\lambda$ when cluster is different and $\lambda$ when channels are equal, and have $\lambda$ a positive value close to $0$. This will not guarantee that all the clients will go into the same cluster, but it gives you the benefit that you have a slider which you can use to fine tune the compromise between all user of the same channel goes into the same cluster and a distance measure which is more robust to outliers. A totally different approach from an implementation point of view would be to define a more complex function directly on channel samples. This would be similar with how hierarchical clustering works since in order to joins to clusters it should have a distance function which would measure inter clusters similarity. See more on linkage-criteria. For example average linkage clustering. Note that I said that the approach is different only algorithmic. I would bet that the results would be similar with the first approach. A totally different approach would be to use a more robust criteria. It is known that sample average is not robust since one point can blow away the estimation. Median instead is much stable. You can use a median or a trimmed mean to have a more robust aggregation values. This would have the advantage that the clustering would be much faster since you would work with channels instead of clients and the running time for computing the clustering would be reduced. And finally, another approach which comes to my mind would be to go further with comparing channels, but this time using a distance which would be based on statistical tests. I will give you a scenario to clarify. Suppose that your users have an attribute named Weight, which as expected would be a continuous variable. How could you define a distance between the Weight of users of one channel and Weight of users of other channel? If you can assume a distribution, like a Gaussian on that weight you can build a two sample t-test on the two samples which are the two clusters. If you can't assume a distribution you can employ a homogeneity / independent test like two sample KS test. KS test is valuable since does not assume any distribution and is sensitive to both changes is shape and location. If you have nominal attributes you can employ a chi-square independence test. Be careful with what you can use from that tests. In order to have equal contribution for each attribute used in distance function you have to you p-values. Also note that if the test is significant it will have a small p-value, since the null hypotheses for both tests is the independence assumption, which can be translated as same marginal for both samples. So, smaller p-values means bigger distance. You can use $1-\text{p_value}$ or even you can try $\frac{1}{\text{p_value}}$. 

To convert from string to float in pandas(assuming you want to convert Employees and you loaded the dataframe with df), you can use 

No, it isn't that way. From my personal experience, I have built custom word2vec's to some datasets and observed that at times, shorter vectors did better than longer ones. I think you can try shorters length vectors for smaller datasets. I cannot tell any metric in specific for comparison of dataset size and word2vec length though. 

I trained a word2vec from the gensim package. Even though I pass a word in the model.train() method, it doesnt appear in the model's vocab. 

I used word2vec on a customer review dataset provided by my company and ran it on my local machine - Mac OS Sierra - 10.12.1(Beta) with 8gb RAM. It is working fine. But I wanted to fasten up the process and henceforth tried to run it on an AWS Instance with 16gb RAM. But when I tried to run it there, the program keeps crashing with memory issues or segmentation faults. I tried to provide 16gb of swap memory more. Even that memory gets full and the program crashes. 

I am using CNN for Sentence Classification code by Yoonkim. This is used for text classification. I noticed that he uses softmax layer and negative log likelihood error. This is optimal for single label multiclass classification. I now have a dataset which is for multilabel-multiclass classification. 

Suppose that you need to classify something in K classes, where K > 2. In this case the most often setup I use is one hot encoding. You will have K output columns, and in the training set you will set all values to 0, except the one which has the category index, which could have value 1. Thus, for each training data set instance you will have all outputs with values 0 or 1, all outputs sum to 1 for each instance. This looks like a probability, which reminds me of a technique used often to connect some outputs which are modeled as probability. This is called softmax function, more details on Wikipedia. This will allow you to put some constraints on the output values (it is basically a logistic function generalization) so that the output values will be modeled as probabilities. Finally, with or without softmax you can use the output as a discriminant function to select the proper category. Another final thought would be to avoid to encode you variables in a connected way. For example you can have the binary representation of the category index. This would induce to the learner an artificial connection between some outputs which are arbitrary. The one hot encoding has the advantage that is neutral to how labels are indexed. 

Since you have trained your model on air-line tweets, the model will learn the characteristics of the air-line tweets. There could be words used in air-line tweets which are positive and contain great weightage but never even used in other tweets leading to negative results. I suggest you to try 

Hence, we observe that model1 is being reset by the model2 and hence the word, 'third' and 'sentence' are in it's vocabulary eventually giving its similarity. This is the basic use, you can also check reset_weights() to reset the weights to untrained/initial state. 

I have not checked the dataset but this is what I suggest. Have a look at this method - get to extract. Apart from this, I also suggest checking Matlab engine for python. 

I want to do a text classification problem for which I want to train use Adaboost Classifier from sklearn using a Keras estimator. I do know how to use the Keras wrappers for using sklearn functions. So that is not the problem. I have a 3D input data (number_sentences, number_words, features) and when I checked the source code of Adaboost, it shows that X can be only 1D or 2D. 

This answer actually lies on the dataset and the use case. It's hard to tell definitively which is better. 

If you take a look over the specifications of PMML which you can find here you can see on the left menu what options you have (like ModelTree, NaiveBayes, Neural Nets and so on). 

k-means finds only a local optima. Thus a wrong number of cluster or simply some random state of equilibrium in the attracting forces could lead to empty clusters. Technically k-means does not provide a procedure for that, but you can enrich the algorithm with no problem. There are two approaches which I found that are useful: 

The point of using fresh data (pruning data set) is to check if the split is useful or the noise from data covers the signal given by the split. To evaluate the performance of both models, before and after pruning, you need another different data set, named validation data. To see way you have to understand that pruning is actually also part of extended training. The only difference than usual learning is that in training with REM you learn how to extend using some data and you learn how to unlearn using some other data. If you use the same data from training to do pruning you simply do not progress (you can stop early). To use for pruning the data set for testing will invalidate you model selection criteria, since it will almost always be the case that the pruning tree performs better (original tree learns from training data set, pruned tree learn from both). I dare to say that if you would like to asset the prediction error, you will have to use a different data set, or envelope everything in a cross validation. 

So let us assume you have training out of which you are using 80% as training and rest 20% as validation data. We can train on the 80% and test on the remaining 20% but it is possible that the 20% we took is not in resemblance with the actual testing data and might perform bad latter. So, in order to prevent this we can use k-fold cross validation. So let us say you have different models and want to know which performs better with your dataset, k-fold cross validation works great. You can know the validation errors on the k-validation performances and choose the better model based on that. This is generally the purpose for k-fold cross validation. Coming to just one model and if you are checking with k-fold cross-validation, you can get an approximate of errors of test data, but .(Because it is assumed here that the whole data will together perform better than a part of it.It might not be the case sometimes, but this is the general assumption.) 

To add onto the above answer, a simple feedforward network doesn't learn the function itself. But recently, Neural Turing Machines claim to learn the algorithm. Worth a shot! 

The machine cannot return neutral sentiment if you do not feed it. If you just have positive or negative, the machine will not return neutral. Alternatively, maybe you can try other approaches. You can check the confidence with which it predicts positive or negative and if it's low, maybe you can assign it to neutral.