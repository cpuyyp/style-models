In some of these bounds, the $\varepsilon$ doesn't come from an approximation, but rather from the degree to which you recurse in a partition tree. Roughly speaking, the idea goes like this. Find a small sample of the input objects that form an arrangement so that the remaining objects are "spread out nicely". In particular, each cell of the arrangement is not intersected by too many of the remaining objects. Then recurse in each piece. Using things like $(1/r)$-cuttings, you get recurrences that look like $$ S(n) = r^d( 1 + S(n/r) ) $$ Solving such recurrences gives you bounds like $S(n) = n^{d+\varepsilon}$, where $\varepsilon$ is roughly $1/\log r$. 

Dick Lipton has a new post out on the non-monotonicity of mathematical knowledge, and in it he documents examples of claims made that turned out to be false, or at least needed fixing. 

A good reference for this body of work is the DCG paper by Aloupsis et al, and the references therein. 

If a player's gift is stolen, they now have the opportunity to do the same thing. A round is complete when a player chooses a wrapped gift. While there are many variations in the system, one point to note is that the player going last has an unfair advantage because they alone are guaranteed the ability to choose any unwrapped gift. THis falls under the class of fair-division methods pertaining to indivisible goods (unlike cake cutting). My questions is: 

You need McDiarmid's inequality, or in particular the Hoeffding inequality, which is a simplification: Given $n$ independent $X_i$ with the property that $X_i \in [a_i, b_i]$, and $X = (1/n)\sum X_i$, then $$ \text{Pr}(|X - EX| \ge t) \le 2 \exp(\frac{2t^2n^2}{\sum (b_i - a_i)^2}) $$ 

One recent paper by Daskalakis et al shows that for a poset of size $n$ and width $w$, minimal elements can be found in time $O(wn)$. What's interesting is that in their abstract, they say 

One thing we do know is that you can't recreate Koebe's theorem with rectangles. Contact graphs of rectangles can't capture $K_4$. 

The DP for vertex cover on trees breaks the cases into two disjoint settings, one where the root node of the subtree is picked for the cover, and one where it isn't. This suggests that it should be adaptable to the counting version: the disjointness means you can merely add the contributions from the subcases. 

In one sense, browsing this site will tell you the kinds of questions theoretical computer scientists think about (at a low level). At a very high level, theoretical computer scientist ask questions about the mathematical foundations of computation: 

Construct the trapezoidal decomposition of the arrangement of all line segments in the input (i.e ignoring which polygon each line segment comes from). There are standard algorithms for doing this. Run a connected components algorithm over a graph induced by the resulting subdivision: each vertex is a cell of the subdivision and two vertices are connected by an edge if the corresponding cells intersect at an edge of the trapezoidal decomposition that is NOT part of the input. For each component, write down the canonical answer you desire. Build a point location data structure over the subdivision. 

There's been recent work on doing clustering in the MRC model (a formal model for analyzing mapreduce computations). Specifically, you should look at the work by Bahmani et al in VLDB 2012 on k-means$||$ and earlier work by Ene et al in KDD 2011 on the same topic. These papers have some discussion of the general problem of parallelizing $k$-means, which might be a good jumping off point for what you're interested in. 

Ordinarily, I'd think this was out of scope. However, there's an interesting series of papers involving Madhu Sudan, Brendan Juba and Oded Goldreich on what they call universal semantic communication (or how to talk with aliens). The papers are here: one, two, three. 

SVMs contain an underlying optimization step that is solved heuristically, so for any actual algorithm that purports to solve SVMs, the answer is undefined. A number like $O(n^3)$ is generally bandied around for implementations like libsvm, which means something like time/iteration * #iterations (where #iterations is assumed to be constant) 

The book by Cesa-Bianchi and Lugosi has much of the modern aspects of generalization bounds and learning theory. 

It would be too much to expect that this procedure would also be efficient, but it would also be interesting to see (natural) examples where the reconstruction exists but must be inefficient. 

You should add Dexter Kozen's book on the theory of computation to your list. Covers the basics of complexity theory very effectively, and the short lecture format is great. In terms of mathematical background, in addition to what's mentioned above: 

Let us define a class of functions over a set of $n$ bits. Fix two distributions $p, q$ that are "reasonably" different from each other (if you like, their variational distance is at least $\epsilon$, or something similar). Now each function $f$ in this class is defined by a collection of $k$ indices $S$, and is evaluated as follows: If the parity of the selected bits is 0, return a random sample from $p$, else return a random sample from $q$. Problem: Suppose I'm given oracle access to some $f$ from this class, and while I know $\epsilon$ (or some other measure of distance), I don't know $p$ and $q$. 

A quick google search revealed Experiments on Data Reduction for Optimal Domination in Networks. If you look at their tables they cite a number of data sets for which the actual dominating set is given. They also use a number of random graph generators and publish the average dominating set size for these. There's another paper from 2002: Experimental Analysis of Heuristic Algorithms for the Dominating Set Problem. I couldn't find an online copy, but I'm sure this paper also documents data sets and actual DS sizes. 

My usual answer, which is not snappy but is guaranteed to stop conversation dead (bonus!) is "like quantum theory is the mathematical core of physics, TCS is the mathematical core of computer science". 

Let me provide an answer from the other side. I've had a few undergraduate student researchers work with me. The experience has been mixed: with some, I have published papers and have work in progress, and with others, we never really got off to any kind of start. It's great that you know what you want to do. As an undergraduate, here's what you should be focusing on: