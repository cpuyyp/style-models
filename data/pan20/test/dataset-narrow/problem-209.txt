Change the collation of the instance back to what it was i.e SQL_Latin1_General_CP1_CI_AI. This will fix the issue. You can go through the below article on how to change the instance collation, this will need a downtime so plan it properly and go ahead. $URL$ 

This is a very annoying thing happening to the reports I am developing. The top breadcrumb sometimes works fine while at times breaks into multiple lines. I have only IE 11 in my environment, Is it the browser issue? If not then please help me fix this. Below is the screenshot. 

The extension of a backup file is just to have a notation and easy identification. The script you are using is for Log backup and not for Differential backup. Differential backup backs up the extents that had changes since the last full backup. Below is the t-sql for that : 

This was a mere co-incidence. Both backup and restore are 2 different tasks which depend on various factors. 

It is always good to have backups done with individual files (With date stamp). This will not only help you save diskspace but also be easier to do the cleanup. The cleanup can be done as per your retention period agreement(5 days).One big disadvantage being any mishap where the bak files gets corrupted. In this scenario you will have no useable backup while in other case you will have at least one good backup (Keep checking the for validity of backups). Use Ola-Hellengren's scripts which are the best way to configure backups locally.This ha all the required parameters and also the time stamp issue will be taken care. Coming on to answering your questions : 4.What will be the disadvantage of my above plan . 

sys.dm_os_buffer_descripters gives you the Buffer pool usage and not the total memory usage.DO NOT assume the results from this DMV to be the total memory usage by the database engine. There are lots of other things that SQL Server does and uses memory for. Total will be a sum of Buffer pool and Non-buffer pool usage. 

No there is nothing wrong in linking two identical primary keys in different tables but the problem arises when the data type is declared as identity. When you declare some field as identity then the value is auto incremented and is decided by the seed if provided. Even though the value are in sync sql server will not allow you to go ahead with this as pointed out by Dan. Use INT and it will not be an issue. 

As per Microsoft, having same service account is the prerequisite : All server instances that host an availability replica for the availability group must use the same SQL Server service account. The domain administrator needs to manually register a Service Principal Name (SPN) with Active Directory on the SQL Server service account for the virtual network name (VNN) of the availability group listener. If the SPN is registered on an account other than the SQL Server service account, authentication will fail. $URL$ 

If you have already deleted the Imported Tables then you are fine. You will not be required to do anything else. This can happen and to get rid of this Use the Import/Export Wizard or Properly use the databasemame.Schema.Table. 

The first and the foremost thing that your database is already in Simple recovery mode so log will not grow much until it is held up by a single transaction. Once the checkpoint comes the log will be truncated itself. Now that you have limited the log size, SQL Server goes for a toss when the transaction is in the middle and there is no scope of log to grow. Simply kept, the transaction needs the log to grow. You cannot make the transactions complete without allowing the logs to grow when required. This is why the best setting is to keep the AUTOGROWTH enabled. SQL Server treats it as a crash and then goes for a recovery to rollback the incomplete commands as seen in the error log. 

Database is a vital part of any application and is active all the time.This is why databases are often hosted in dedicated servers where nothing else can hamper the resource utilization for the database. Now coming to your question, there is no problem at all and SQL Server is working as expected. SQL Server is smart enough and caches as much as possible to make operations faster. There is a setting called MAX MEMORY SETTING that can be set to a value at which you want SQL Server NOT to use any further RAM. This is done after keeping aside the memory usage for the OS processes. 

Take database backup Take note of Availability group listener and database involved.(For future if they want it back) Remove the Databases from the AG. Remove the AGs Uninstall the instance as a normal standalone instance.(For all nodes). 

I used your data and was able to do it without any issues. Seems your first row of data does not have {CR}{LF}. The snap of data looks like there is an auto new line rather than you providing it.And that is why the 3rd column has ' Is_Active""1, so the wizard is looking for a comma now. Below is my test with data of similar type : 

First of all find the session from which the backup is running. To find that use Sp_who2 and check for the command column to find "Backup...'. Now just confirm that it is the same one that you have initiated by taking a look at the query fired for corresponding spid.Check this by : DBCC Inputbuffer(SPID); /Use the spid here/ Once you are sure of the SPID just kill the session by using : KILL (SPID) And then wait till it disappears. There will be STOP button next to EXECUTE in SSMS. You can use it to cancel the query execution from ssms. 

Script out the replication. Take the database backup for Publisher. Backup one of the subscriber (right after no.1). Copy subscribers backup (which is much lighter) to the future subscriber and restore Configure the new subscription accordingly. 

You just need to change the Directory parameter in the Ola Hellengren script. Go to the job, click the Step --> Edit-->@Directory = 'C:\Backup', This will be enough. Adding to your particular requirement, I went into the Store proc and was able to figure out. This took me some time so apologies for that. Please mark this as answer once you are able to complete your task. Below are the steps to get your required directory : 

How does it effect the performance? ( Did not find any explanation or proof of concept) Also confused as some articles say dropping them can cause performance issues too. Is it for all cases or any specific? 

I am playing around with SQL Azure and in my learning curve. I Have migrated a database from my on premises instance to the Azure server. Now my question is, how do I move a database from Azure to on premises instance. Source : Azure , Database name : Azuretest Destination : Sql server 2016 on premises instance in my laptop. Is Azure to On Premise database migration possible? 

The above answer from @kin is perfect. If at all you are looking for a script that you can use to backup your desired databases then use the below query. I have been using it to configure backup for Express Editions. Here is the query,i have edited the script accordingly, 

The database has corruption which is reported by DBCC CHECKDB and it is the only safe way it could repair it which did not happen in this case. Using repair_allow_data_loss will guarantee you some loss of data and you should not be going for that unless you can afford to lose data. As any DBA, you will have point in recover backup set up so use it to recover the database which will be more consistent. For any further troubleshooting use : $URL$ $URL$ 

No..Never!! This is why we have development/Test/UAT servers. The data from Production can be copied onto the test environment and the developers can go ahead with their testing. Select queries can be very harmful as well in case of a Production environment. It increases the load and at peak time can bring down the entire performance. Any information they need should go via the DBA who can run what they want to ( Select) and send them the results. That's how we do in our environment. 

Yes you can do that. Before hand make the log reader job disabled. This will make sure the jobs are not looking for the database. For more clarity look at the below link : MSDN : Replication and Single user Mode 

Its a normal behavior as the IntelliSense does not recognize the objects that are not cached. I use the Refresh cache option in Edit menu each time I see this. Edit -> IntelliSense -> Refresh Local Cache 

No. Index rebuild will generate logs irrespective of the way you run it because internally the t-sql will be same. 

Yes, that is how we need to roll out patches in the sql cluster with minimum downtime. The downtime will be during the failover part. Process: 

The characted '\' will be identified as invalid character and it will error out if you try to create a SQL login using it. This is not possible. First of all please go through the two types of authentication and how that works. Windows Authentication is for AD users which goes by Domain\user_name and are added to the instance while SQL Authentication is for logins created inside the instance. Please go through the below link : Authentication 

7.How better I would have thought than what I am thinking now . Which is more recommended and better , Overwrite / Append / Individual Backup Files with Date-Time Stamp ? 

Scheduling log backups is the best way to control the logs (considering the Full recovery mode).Make sure it is scheduled properly and more frequently. 

The above will backup all databases in AG except UserDb. For more information please go through the Databases parameter in the below link Ola hellengren 

Before doing this do test it in a Test environment and prepare a full proof plan. Also refer to this link : $URL$ 

For getting all backups to just one location, you will need to remove few parameters from the store proc [dbo].[DatabaseBackup]. There will be this part where you set variebles( Look for the comments) -- Set variables Go ahead and look for the below part : 

I have a request to uninstall one of the test instance which is part of Always ON. I googled around for a clean decommission but did not find steps for Always ON Instance uninstallation. Would be great if Steps can be shared here or any link if you are aware of. As per me I would : 

All major points have been already covered. Below are my 2 cents : Detaching a database is never a permanent solution as it was intended to be used for moving the database files within the server or to another server. Removing a database permanently can be done by Delete option in SSMS or the DROP database command as mentioned above. Usually the databases which are intentionally kept offline and keep generating Alerts are the ones we detach and keep it till it can be permanently removed(Deleted). Pre detach task : Run to know the file locations. Cleanup tasks : 

What is the frequency of the log backups ? Use dbcc sqlperf(logspace) to check how much % is free space in the logs. There is something that is holding the logs up. Check the log_reuse_wait_desc for that database and take action accordingly. Follow the below article : $URL$ 

I was going through various articles on Missing indices and Unused Indices. Was wondering how having multiple indices which are un-used causes performance issues. 

Delete the mdf,ndf and ldf files of the database from the locations they reside. Old backup files for the database needs either to be Deleted or moved to another server considering your retention period. 

Other than Logins,Agent Jobs,Triggers and already mentioned points by Max, these 2 can also be looked upon. 

Check for the space issues and also the permission on the Temp folder. If the permission is read-only then change it to Full Control. $URL$ 

Yes, you just need to have a different publication created for a specific Subscriber with the required articles. This will help you in customizing your subscriber as per the required articles. 

Sql Server Management Studio is just an application used to manage the Sql server database. It is not to be confused with Database Engine. You will have to install database Engine which is actually the database and will come up in the configuration manager. 

You are appending the backup into same file so it is actually accumulating the backups. You need to give a new name each time you trigger backup from Tasks->Backup. Use maintenance plan as it has timestamp for each backup file or else go with Ola Hellengren's solution. $URL$ 

The remote query timeout option specifies how long, in seconds, a remote operation can take before SQL Server times out. The default value for this option is 600, which allows a 10-minute wait. This value applies to an outgoing connection initiated by the Database Engine as a remote query. This value has no effect on queries received by the Database Engine. What is the value set for Execution timeout in SSMS for you from where you are running the query: 

In this case only the db_owner and sysadmin can see the databases. db_owner can see only the one he owns while sysadmin can see all. Again giving db_owner privilege is not a good idea as at db level it is the highest privilege. Also one database can have only one owner. Hopefully MS will have a resolution for this soon. 

Check Event logs for any server resource utilization errors. Database response is one thing and server response is another. If you are facing issues even while doing simple tasks like expanding the database then it is mostly sluggish response . 

All you have mentioned should be fine. Just remember to script out the replication before doing this as a backup. 

Index rebuild (Online) is a resource intensive operation. During the rebuild there are locks being held which for a very short period of time can make a table unavailable (Very short time).Below link provides guidelines for online index operations : $URL$ A paragraph from the above link : 

We have multiple instances on single node of the AlwaysOn setup, all are configured as AOAG. Right now we are connecting using the Listener\InstanceName. The listeners for each AG is having a dedicated port designated. This is why we cannot connect just by using the Listener without the port or InstanceName. I wanted to configure in such a way that just giving the AG_Listener would be enough to connect without using the port explicitly. 

The timeout is a client side setting and below explanation from Microsoft will clear the air why you are still having the issue. Also this setting change does not require a restart. This value applies for an outgoing connection initiated by the database engine. 

Check for any memory issues.A memory pressure can cause this kind of response. Check if the database Auto_Close property is set to True as this also cause slowness while expanding the database objects. $URL$ Evaluate the Disk IO as suggested by Pinal. 

Google is there for helping you out in coding. Take this as a challenge start coding , it will help you out more than someone writing it for you. Have a nice day. 

This is the most frequently faced issue for almost all the DBAs where the logs grows and fills out the disk. 

Now you need to remove the parameters : @CurrentDatabaseNameFS and UPPER(@CurrentBackupType). This will create all your backups to a single location as below 

The backup that you are taking is log backup and in addition to letting you restore the backed-up transactions, a log backup truncates the log to remove the backed up log records from the log file. This is very important to have a point in time recovery and has to be scheduled to run frequently. Please go through below link to understand how backups work : $URL$ 

AFter days of scratching my head and google finally fixed with a vey simple setting. This was resolved by adding the url to the Compatibility View Settings in the IE browser settings. 

5.How is it different from appending backup files and daily individual backup files with Date Time Stamp appended to the filename daily basis . 

Log backups will help you control the log growth unless there is something that is holding up the logs from being reused. Configure the log backups to run every 15 minutes. (That's what we use) Coming back to our question : 

It depends on what your needs are. Re-organize is a light weighed operation ,generated lesser logs and always an Online operation. While rebuild is online only for the Enterprise edition of SQL Server and is more heavy operation. 

We had to re-enable TLS1.0 to make the connection successful. Is there something that can avoid enabling it as it was identified to be a vulnerability. What needs to be done to make it work with TLS 1.2?