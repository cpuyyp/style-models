If you want to compare volume to crop type then you should use a plain column chart, where it is easy to distinguish relative heights. This is analogous to why bar/column charts are preferable to pie charts - humans are better at comparing heights/lengths vs area/volume. If, on the other hand, the point of the graph is to compare crop type irrigation by area, you would want to display a column graph with regions next to each other, but grouped by crop type. As far as a superbly produced duck, my guess is that the Applied Irrigation Water is a duck, but a superb duck, at least when compared to the monstrous duck on the preceding page of the Visual Display of Quantitative Information 2nd Ed. 

For feature engineering, because of the input-to-response lag, you will want to initially include variable lags. For example, for variable and a corresponding change in output o, you would want to add as features a_left_1, a_left_2, ... a_left_n, where each feature refers to the input value at some lagged time prior to the observed change in behavior. 

Have you read Janssens' dissertation "Outlier Selection and One-Class Classification"? He has a chapter on evaluation which may be of use. Have you thought about artificial generation of negative instances? I had to deal with OCC evaluation awhile back and never came upon an entirely satisfactory solution. As I recall, the basic problem boils down to the fact that you don't really have labelled instances of the negative class. Without these instances there is no way of computing precision = TP/(TP+FP) which relies upon the count of incorrectly accepted negative instances (FP). This just leaves you with recall = TP/(TP + FN) as a computable metric - which by itself is next to useless in evaluating classifier performance i.e. it's trivial to write a classifier that get's 100% recall, just predict the positive class for every instance in your test data... you will correctly identify every true positive instance, but you will also incorrectly predict every negative instance as a positive instance. What is typically required is a balance or trade-off between precision and recall. You didn't mention a particular problem domain in which you are working, but perhaps you can leverage this domain knowledge as a work-around to the problem of OCC evaluation. This particular problem I will relate is technically cast as a PU Learning problem (learning from Positive and Unlabelled instances). I'll try to describe some solutions that I explored while tackling this challenge - I'll give you fair warning that they are not entirely satisfactory solution, but I believe they are logically sound and somewhat defensible. From a purely pragmatic perspective, they are definitely better than the alternative. Implicit Negative Class Scenario: imagine a membership-based incentive program, in which people can pay an annual fee to become a member of Foobar Inc. Members, but not non-members, are entitled buy widgets at Foobar's retail outlets. You are in charge of a marketing drive to increase Foobar's membership. You have limited funds and reach, so you need to carefully select people from the general population that you believe will most likely become members if you target them with a promotional campaign. In this scenario you have a database of known Foobar member's (the labelled, positive class data, which we label POS) and a database of known non-members from the general public. This is your unlabelled data that contains people who may or may not become members if targeted. All we have is POS instances and UL instances -- there are no labelled NEG instances. Depending upon your particular problem, context, and domain, you may be able to transform the OCC/PU problem into a more classical problem with standard performance metrics. In the situation I just described, there are at least three ways of creating an implicit negative class: 

Error Analysis The type of errors produced by this type of machine should be nonsense words - words that look like they should be English words but which aren't (e.g., ghjrtg would be correctly rejected (true negative) but barkle would incorrectly classified as an English word (false positive)). Interestingly, zyzzyvas would be incorrectly rejected (false negative), because zyzzyvas is a real English word (at least according to words.txt), but its gram sequences are extremely rare and thus not likely to contribute much discriminatory power. 

I faced almost exactly the same scenario a year and a half ago -- basically what you have is a variation of the one-class classification (OCC) problem, specifically PU-learning (learning from Positive and Unlabelled data). You have your known, labelled positive dataset (clients) and an un-labelled dataset of prospects (some of which are client-like and some of which are not client-like). Your task is to identify the most client like of the prospects and target them... this hinges on the assumption that prospects that look most like clients are more likely to convert than prospects that look less like clients. The approach we settled upon used a procedure called the Spy-technique. The basic idea is that you take a sample from your known positive class and inject them into your unlabelled set. You then train a classifier on this combined data and then run the unlabelled set back through the trained classifier assigning each instance a probability of being a positive class member. The intuition is that the injected positives (so-called spies) should behave similarly to the positive instances (as reflected by their posterior probabilities). By setting a threshold this allows you to extract reliable negative instances from the unlabelled set. Now, having both positive and negative labelled data, and you can build a classifier using any standard classification algorithm you choose. In essence, with the spy technique, you boot-strap your data to provide you with the needed negative instances for proper training. 

Using one column but different values for different classes will fall under regression. The objective would be to predict a value as close as possible to target value. Mean square error, mean absolute error etc. are used as loss functions. Using two columns with class indicators (1 for the target class and zero for rest) falls under classification. It can be seen as maximizing the probability for target class. Binary cross entropy, categorical cross entropy are used as loss function. It mostly depends on the application on hand. If your end goal is just to know the class labels then use the second approach. Cross entropy is very sensitive and error will be large if the predicted and target class do not match. 

So the input to the CNN will be an image patch of size 45 * 40 regardless of the length of the audio file, just the number of such inputs will depend on the length of audio file. Why force alignment? Now we are doing everything at frame level so for each frame we need the state labels. Usually this timing information is not available. Transcribed data usually looks like this, 1.wav -> I am a cat Now I don't know how many frames belong to I or to a. HMMs are trained on this data and force alignment is done to generate state level labels for each frame. Better modeling of input-output relation gives improvement over other methods. Filter bank features also contribute to improvements. A DNN trained with filter bank features gives better performance as compared a DNN trained with MFCC features. 

Data representation does matter because this is all the information that you pass to a learning algorithm. It is normal for static and delta (delta-delta) to have different range (I have worked with mfccs). They represent different information. Static features can be small but they may change rapidly making delta large or vice-versa. The blue regions in the first spectrogram (low magnitude) becomes red in second( high magnitude). As long as all the input are processed in the same manner (static followed by delta followed by delta-delta or any order), it won't be a problem. 

DNN/CNN prediction(training) is done for 1 frame at a time. The output can be any of the 183 outputs states. Length of the audio files is not a problem since the input to the DNN/CNN is of same dimension only the number of inputs change with audio length. e.g 1.wav has 500 features and each feature is 39 dimensional and 2.wav has 300 features, the trained model will take a 39 dimensional input and output will be 183 dimensional. So depending on length we'll get different number of outputs. Since all the frames in an utterances are tested against all the 183 possibilities so the output always remains 183 dimensional. There is no need to specify number of phonemes an utterance can have as everything is being done at frame level. Frame concatenation (9-15 frames) is done to leverage contextual properties of speech data. Phone changes are context dependent. For 15 frame context, we change the input of DNN to [7*39 (left_context) 39 7*39(right_context)], a 585 dimensional vector. So now DNN will take 585 dimensional data as input and will output a 183 dimensional vector. CNN input 

It has been studied under various names likes Domain Adaptation, Sample Selection Bias, Co-variate Shift. Please go trough this survey paper on Transfer learning. It covers all the possible combination like 1)same distribution for train and test data 2) Gradual change between train and test distribution 3) Different but related distribution for train and test It'll also give you all the necessary resources required to study further on this topic. 

I don't think we are creating a number of negative examples for every positive example. Negative sampling is done to efficiently compute the softmax. Word2vec tries to maximize the similarity for words in similar contexts. For e.g. given drink the NN should predict water with maximum probability. Suppose you have V (typically > 1 million) words in your training data, so the last layer of your model has V neurons. For every word in your training data, you would have to compute output from all the V neurons (to compute softmax). It is computationally very expensive. Negative sampling is one way to address this problem. Instead of computing the all the V outputs, we just sample few words and approximate the softmax. Negative sampling can be used to speed up neural networks where the number of output neurons is very high. Hierarchical softmax is another technique that's used for training word2vec. 

Converting the categorical data into numerical data isn't really meaningful. Different mappings will give you different solutions. There is an extension of K-means algorithm for categorical data called k-modes. You can read about K-modes in detail here. This article explains the difference between K-modes and converting data into numeric vectors and then running K-means. 

Its possible that because this is a "kohonen" class object that there are functions that get these data matrices for you. You'll need to read the documentation to figure this out. What I've shown above is digging in the structure to find the data you want. 

Let s1 to s4 be the signs (+1 or -1) of the regression coefficients of f1 to f4. Then the predicted value for a data item with columns s1*Inf to s4*Inf will be (as near as precision allows it) 1. You go to the extreme of the parameter space in the right direction (as told by the coefficients signs). Inverting the signs and doing the same should give you a data item that is predicted to be 0. Example, code in R Create sample data set. 

I don't know what your is, so I'll assume you've attached and its the column from there. PLEASE edit your question and show ALL your working so anyone else can get your results. Let's proceed: 

Additional to other ideas: reduce your data until you figure out what you can run on the Amazon instance. If it can't do 100k rows then something is very wrong, if it fails at 590k rows then its marginal. The c3.4xlarge instance has 30Gb of RAM, so yes it should be enough. 

No. Unless you have some other idea of the scale and spread and distribution of the feature values. You can construct data sets that have any given mean and median with no outliers or with one massive outlier. For example, f2 looks like a well-balanced set of numbers with a very close mean and median, maybe all those values are from a N(0.184,3) distribution. Scale that up by a linear transformation to an N(X,Y) and you'll get a mean and median much like f6. Exercise: find X and Y. 

that is, a file called in the current working directory at the time you ran . You can get the current working directory with in R. So for example if its something you can read with , you can do: 

Using , you'll see numbers left of the bar go up by one, so now you can get exact reconstruction of your input, since you input integers: 

I see nothing wrong with your Poisson model. In fact its a pretty good fit to the data. The data is noisy. There is nothing you can do about it. Perhaps the noise if due to whatever else is on TV at the time, or the weather, or the phase of the moon. Whatever it is, its not in your data. If you reasonably think the weather might be affecting your data, get the weather data and add it. If it decreases the log-likelihood enough for each degree of freedom then it's doing a good job and you leave it in. This is regression modelling 101. Of course there's a zillion other things you can do. Scale the data by any old transformation you want. Fit a quadratic. A quartic. A quintic. A spline. You could include the date and possible temporal correlation effects. But always bear in mind what Tukey was saying - if your data is noisy, you won't get anything much out of it. So it goes. 

I would look at the package for this, which can read in raw binary data and present it as NxM grids. It can even extract subsets of large binary grids without having to read in the whole file (the R raster object itself is just a proxy to the data, not the data itself). 

Construct a new data set with differences in the numeric variables and new factors in the categorical data containing the first and second levels to represent change. Then fit a linear model: 

The Normal distribution is the same as the Gaussian distribution. Its just two names for the same thing. Whatever you do - fit parameters, compute goodness-of-fit, etc - if the documentation says its for a Normal distribution then you can say "Gaussian" instead. Completely and totally identical. 

"Because its there". The data has a seasonal pattern. So you model it. The data has a trend. So you model it. Maybe the data is correlated with the number of sunspots. So you model that. Eventually you hope to get nothing left to model than uncorrelated random noise. But I think you've screwed up your STL computation here. Your residuals are clearly not serially uncorrelated. I rather suspect you've not told the function that your "seasonality" is a 24-hour cycle rather than an annual one. But hey you haven't given us any code or data so we don't really have a clue what you've done, do we? What do you think "seasonality" even means here? Do you have any idea? Your data seems the have three peaks every 24 hours. Really? Is this 'gas'='gasoline'='petrol' or gas in some heating/electric generating system? Either way if you know a priori there's an 8 hour cycle, or an 8 hour cycle on top of a 24 hour cycle on top of what looks like a very high frequency one or two hour cycle you put that in your model. Actually you don't even say what your x-axis is so maybe its days and then I'd fit a daily cycle, a weekly cycle, and then an annual cycle. But given how it all changes at time=85 or so I'd not expect a model to do well on both sides of that. With statistics (which is what this is, sorry to disappoint you but you're not a data scientist yet) you don't just robotically go "And.. Now.. I.. Fit.. An... S TL model....". You look at your data, try and get some understanding, then propose a model, fit it, test it, and use the parameters it make inferences about the data. Fitting cyclic seasonal patterns is part of that.