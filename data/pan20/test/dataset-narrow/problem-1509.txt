Though continuum regression methods appear to be significantly different from binary classification methods, the crux of the difference is usually tied up in a sigmoid function. $$S(t)=\frac{1}{1+e^-t}$$ Sigmoid literally means sigma like or "S" like, as becomes obvious in looking at its graphical representation: 

PCA - Principal Component Analysis: I think the best option here would be to perform some dimensionality reduction using principal component analysis (PCA) prior to training your model and prior to predicting once you have a model. PCA will allow you to preserve a percentage of the total variance in the input data while significantly reducing the total number of dimensions. In the toy example that you present, I would expect a high degree of linear correlation between the height per square foot features and a high degree of linear orthogonality between the other features. In such a case, PCA will naturally mash the correlated height terms together into 1 or 2 features while preserving 95-99% of the total original variance. The only downside here, is that you will loose the physical meaning of your input features. I suppose you could decompose the eigenvectors and try to understand them, but loosing the physical meaning is usually not particularly hindering to training a model. There are other methods, like lasso-ridge regression (you mention classification, but then provide a regression example e.g. the continuum variable of home price) or directly tracking the feature importance in a linear regression, but I prefer how PCA is able to project out linear dependence in input features, which seems particularly topical to your problem. Give it a try and let us know how it goes. Hope this helps! 

I try to stay away from bounty problems as they seem to lead to one upmanship, but I can't resist this one as its an interesting question... The upper classification method that your describe sounds spot on. Use that one! The bottom method has a number of issues: How would you know that the clusters you get out would correspond to the categories of A? The answer is that you wouldn't. There are other data features and the clusters would be a linear superposition of the maximum separation between the various variables. I guess you could standardize your data and then coax the clusters to conform to the separation along A by multiplying the A feature by a factor greater than 1 (~2-10), but this seems like more of a bastardization of clustering than a good process. The main issue that I see with your method is that you are essentially employing a semi-supervised learning methodology, but you don't need to because you have the data to solve this as a supervised learning problem. It would be a good methodology if you only had a handful of labeled A values, but since you have lots more data, you should harness the power of that data that you have. The supervised learning problem will fully harness the variance of your data and much of that will be thrown out in the semi-supervised learning methodology. Hope this helps and good luck! 

This is generally referred to as "stability". What's funny is that stability is actually a useful concept in regular clustering i.e. not online. The "stability" of the algorithm is often chosen as a selection criterion for whether the right number of clusters have been selected. More specifically, the online clustering stability issue that you have described is referred to as the . 

Great question... Here are some specific answers to your numbered questions: You should cross validate on B not B`. Otherwise, you won't know how well your class balancing is working. It couldn't hurt to cross validate on both B and B` and will be useful based on the answer to 4 below. You should test on both C and C` based on 4 below. I would stick with F1 and it could be useful to use ROC-AUC and this provides a good sanity check. Both tend to be useful with unbalanced classes. This gets really tricky. The problem with this is that the best method requires that you reinterpret what the learning curves should look like or use both the re-sampled and original data sets. The classic interpretation of learning curves is: 

Finally, I'll point out that the method you are proposing is not as good as other novelty and anomaly detection methods. You should consider doing novelty detection using a single (or possibly even multiple) class support vector machine (SVM) with a nonlinear kernel. The nonlinear kernel will allow you to recover multiple "clusters" while the SVM will do much better at predicting which points are inside the class. 

First, thanks for the edits to your original question since we now know that you are applying the same transformation to all of your data. Q: Why do perform so much better than for some problems? A: Because they are inherently nonlinear models, with a great deal of flexibility. The drawback is that the additional knobs require more data to correctly tune. Big picture: Less data can cause . can be overcome by more data. You have reduced your data from a 4800 feature dataset to a 38 feature dataset, so should expect to see increased bias. require more data than models without hidden layers. Linearity vs. Nonlinearity Your () is an inherently nonlinear model, yet you are deciding to remove features from your dataset with a linear model (). The presence of a single hidden layer explicitly creates second order terms in your data, and there are two additional nonlinear transformations (input==>hidden and hidden==>prediction), which add sigmoidal nonlinearity at each step. The fact that the training data and the target data are both multiplied by the same matrix to achieve the reduced dimensionality really just means that your needs to learn the matrix if it wants to reconstruct nonlinear aspects of the original data. This requires more data or for you to reduce less of the dimensionality. An Experiment: I suggest trying an experiment where you perform second order on the full dataset and then performing on that enhanced dataset. See how many features you end up with while retaining 99% of the variance of the enhanced dataset. If it is larger than the dimensionality of your initial dataset then stick with the un-enhanced and un-reduced dataset. If it is betweeen the dimensionality of the original data and 38 then try training the with that data. A better idea: Rather than using the (linear) variance to determine the feature reduction of your PCA projection, try training and cross validating your model with different amounts of PCA dimensionality reduction. You will likely find that there is a sweet spot for a given set of images. For instance, an SVM on the MNIST digit data performs best when the 784 pixel features are reduced down to 50 linearly independent features using PCA. Though this is not obvious from analyzing the variance of the principal components. Other options: There are nonlinear dimensionality reduction techniques, such as isomap. You could investigate the use of nonlinear feature reduction since you are clearly loosing information with the linear PCA that you have applied. You could also look into image specific techniques to add some nonlinearity before reducing the dimensionality. Hope this helps! 

You can just barely see that there are some after midnight times included with the before midnight green cluster. Now lets reduce the number of clusters and show that before and after midnight can be connected in a single cluster in more detail: 

I suggest you use another clustering method. The first option that comes to mind is DBSCAN. This allows one to set a threshold for noise and the cluster numbers are not set a-priori. DBSCAN is therefor much more likely to return normal distributions within a cluster. Here is a single DBSCAN cluster of the same data: 

This Data Matching book is a good resource and is free for seven days on Amazon. Nominally, this is an $n^2$ algorithm without exploiting some sorting efficiencies, so I would expect to have to use multiple cores on $2\times10^7$ rows. But this should run just fine on an 8 core AWS instance. It will eventually finish on a single core, but might take several hours. Hope this helps! 

Initial Training Data You will need to prime your solution with some deterministic data or data from another dataset that you have adapted to fit your model. I suggest priming the system with common sense values and then employing some sort of algorithm to forget those values as the system becomes well-trained. Moving Forward You will have to decide whether each user's model will use other user's data or not. This is sometimes solved via clustering methods like k-means or DBSCAN), where by users are clustered into groups each cluster has a different model associated with it. I would suggest retraining (fitting) the model on regular time frames initially, and then investigating the possibility of moving to an online learning system. I absolutely suggest using a well known library for your problem. Scikit-Learn is great for the reasons mentioned above and for prototyping, but doesn't scale particularly well. H20 and Mahout are high quality scalable libraries that I would recommend for a big data production system. Mahout in Action is a great book to learn from, also. Hope this helps! 

Question 1) Is this a good idea in general? Solving this problem as a supervised learning regression problem is a fantastic idea and is the type of solution that will greatly benefit your company since it will translate to other similar and dissimilar problems much more easily than deterministic methods. However using a neural network to solve this supervised learning regression problem is probably a very bad idea. Neural networks can add great value to certain problems, are among the very best algorithms for complex problems where computing power is not an issue, and have fascinating implications for the future of machine learning and artificial intelligence. But... they can be very tricky to train, require a lot of computing power, require a lot of data, and perform poorly in many cases. I suggest you scope the problem using linear regression and then try a support vector regressor (SVM, SVR) or naive Bayes regressor. The analogue methods in SVR work very well with limited data and provide surprisingly accurate results. Question 2) If yes, what type of NN would be most suited for such a problem? If you must use a neural network then try playing with the problem. Start with a feed forward neural network. Note that it will likely underperform an SVR. Then think about moving toward a convolution neural network. Again, this is probably a very bad way to go. Try linear regression followed by other methods first. Moving forward The documentation for either Scikit-Learn in python, H20 in Java, or Weka provide very shallow learning curves to jump on the merry-go-round and take a spin or two. Please make sure that you thoroughly understand cross validation and scoring metrics as this is essential to continued, adequate progress. Hope this helps! 

The full dissertation is also available online for download. There's actually an entire company that is using this methodology and trying to sell it to companies. They have some pretty cool promotional videos on youtube also, which won't teach you the methodology, but show how powerful it can be. Hope this helps! 

I don't really agree with the idea of "wanting" a point to be an outlier and then massaging the algorithm to make it so. You have 2 dimensions and its either an outlier or its not. If one standardizes the data and then deciphers the Mahalanobis distances, point 6 is only one of two points that sit outside of a certain threshold (point 0 being the other point). Beyond that theres not much you can do beyond some sort of nonlinear transformation which you know to be true due to some deterministic knowledge you have about a particular phenomenon. Anyways... here's the two outlier version in case you weren't standardizing your data first: 

But there is still symmetry to be exploited in the inversion operators. You can probably intuit that the 4x4 matrix only has 8 final states: inverted or not and rotated by (0,90,180,270). It turns out that you can arrive at any possible state involving an inversion using any of the other inversion operators followed by one of the rotations. So we only need to retain a single inversion operator! So the final set of 8 orthogonal operations are: