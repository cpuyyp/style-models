There are the obvious academic positions. Apart from that, many industrial research (or research-like) labs are very interested in hiring theoreticians who are comfortable applying theoretical foundations to solving problems as well as coming up with new theorems. Theoreticians also find positions (though may be not as many) in financial institutions as "quants". 

which yields 1991 as when this first started happening at the journal. However, the Handbook was first published in September 1990 ! 

Note that some flexibility will be needed in the definition of fair since the goods are indivisible and we are not introducing monetary compensation for players. 

If your error norm is the Frobenius distance (that is, $m(X, U) = \|X - U \|_F$, then there is a closed form for your problem. $A = V^+ U$, where $V^+$ is the Moore-Penrose pseudoinverse of $V$ (this is a generalized inverse for non-square matrices). 

Is there a natural parallel analog to red-black trees with similar or even not-terribly-worse properties for updates while being reasonably work-efficient ? More generally, what's the best we can do for parallel search with updates ? 

All of these tools show up in computational geometry nowadays on a regular basis, and their use will only get more sophisticated with time. p.s As an aside, if you have a computer engineering degree, it's possible that your background in core theoryCS is weaker than if you had a straight-up CS degree (because of the emphasis on the EE side of things). If that's the case, then the material in the links provided by Kaveh will be immensely useful, probably more so than this list. 

See Eric Allender's survey of "recent" breakthroughs (circa more than a decade ago) in the complexity of division. The bottom line is that 

Using persistent homology, we can analyze the (topological) shape of a cloud of points using the following three-step method: 

The variant of your problem where you DON'T square the distance is called the (weighted) Minimum Linear Arrangement problem. It's NP-hard and there are a number of different approximation methods for it. Here's one link that popped up that has code that you might find useful. But fortunately for you, you squared the distance, which changes things quite a bit. Let's expand the expression: $$ \begin{align*}  \sum_{a,b \in A} s(a,b) (i(a) - i(b))^2 &= \sum_{a,b \in A} s(a,b) (i^2(a) +i^2(b) - 2 i(a) i(b)) \\ &= \sum_a i^2(a) \sum_b s(a,b) + \sum_b i^2(b) s(a,b) - 2 \sum_{a,b} s(a,b)i(a)i(b) \\ &= 2 \sum_a S(a) i^2(a) - 2 \sum_{a,b} s(a,b)i(a)i(b) \\ &= 2 u^\top A u  \end{align*} $$ where $u$ is the vector $i(a)$ and $A_{a,b} = -s(a,b), A(a,a) = \sum_b s(a,b)$. All of this is a long-winded way of saying that you've essentially defined the Laplacian operator for the complete graph with weights given by your similarity function. At this point, minimizing the form is straightforward. You compute the decomposition of A in the form $A = V \Lambda V^T$, at which point your goal is to minimize the norm $\| u^T V \sqrt{\Lambda}\|$. But note that since $A$ is symmetric, $V$ is orthonormal, and is essentially a rotation matrix, and so what you really want to do is line up $u$ in reverse order of the eigenvalues of $A$ (the elements of $\Lambda$) 

I was editing a student manuscript. The student remarked that it would be nice to see examples of quality writing in published work, and I realized that I couldn't really come up with good examples off the top of my head 

The title is a little misleading: but hopefully the question isn't: Grønlund and Pettie's new result showing that 3SUM has only $O(n^{3/2})$ decision tree complexity got me wondering: 

The Minkowski sum of two sets of vectors $A, B \in R^d$ is given by $$ A \oplus B = \{ a + b \mid a \in A, b \in B \}$$ I just heard an interesting problem (attributed to Dan Halperin): Given a shape $B$, does there exist a shape $A$ such that $A \oplus A = B$ ? But that's not my question (it appears to be an open problem). Observe that in the above problem, if $B$ is a convex set, then there exists a solution $A = (1/2)B$ since convex sets are closed under the taking of Minkowski sums. Fix a class of shapes ${\cal S}$. We say that ${\cal S}$ is closed under Minkowski sums if for any $A, B \in {\cal S}, A \oplus B \in {\cal S}$. So my question is: 

Crossvalidation is used to test the difference in behaviour between training and test data. There's no equivalent notion for approximations. However, for many problems it's possible (often via an LP relaxation) to get a good lower bound for a problem. This can be used to test the quality of a given approximation algorithm in practice. For a good example of this, you should look at David Johnson's survey of experimental work on the travelling salesman problem. 

Here's an example: Computational complexity and informational asymmetry in financial products by Arora, Barak and Ge shows that it can be computationally intractable (ie NP-hard) to price derivatives correctly - they use densest subgraph as an embedded hard problem. Along the same lines and much earlier is the famous paper by Bartholdi, Tovey, and Trick on the hardness of manipulating an election. 

Thus, $c_S(\Delta)$ is the quantity you're looking for (where $\Delta$ is the maximum cardinality of a range). A related question is to determine $c_\tilde{S}(k)$, where $\tilde{S}$ is the dual range space (in effect, your original hypergraph). One example of the kind of results obtained is that: 

If you're classifying using SVMs, then the underlying metric space is always a Hilbert space. If your classifier is linear, then the running time of the classification is linear in the dimensionality of the data (or the number of features). If the classifier (in general) involves some kernel, then the classifier is expressed in terms of the number of support vectors, and the classification is linear in the number of such vectors. Processing each vector takes time proportional to the kernel computation time (which could be constant, or linear in the data dimension, or something else) 

Another avenue for interesting exploration is when you're trying to understand the proof of a theorem or lemma. If you dig really deep into the proof to understand exactly how it works (and I don't mean literally, in the $A\Rightarrow B$ sense, but intuitively), you'll often realize that the proof is more ungainly than it needs to be. Asking whether it can be simplified often leads to new explorations. 

While I'm not entirely sure what you're getting at, there are a number of frameworks that explicitly capture the cost changes that show up when we hit real resource limits. Three examples: 

The Bentley-Saxe trick allows us to go from a static decomposable problem to a problem admitting insertions, where the insertion time is off the optimal time by a factor of $\log n$. Is this tight ? Or alternately, is there a more restricted subclass for which you can get insertion time $P(n)/n$ (amortized or worst-case) even if $P(n) = O(n\log n)$ ? Here, $P(n)$ is the time to compute the result statically. 

Some of the results depend on the model under which the matrix is presented, but there's a large collection of results that use sampling or sketching techniques to estimate norms accurately. Among some of the key results are: 

Abstract booklets. Problem: proceedings are now mostly online (a good thing) and I don't feel like uncorking my laptop to see the topic of a paper. Solution: Supply abstract booklets (some conferences are already doing this). It's a small booklet that lists abstracts in order of presentation and is an easy reference to decide what a talk is about. Note: this is an interim solution till we all carry around IPads :), but even then having all abstracts summarized in one place is very convenient. 

I'm still trying to understand your modified question, especially what limits you place on the TM. So while this answer might not get at exactly what you want, maybe it will help narrow things a bit. We know that there is an unconditional impossibility result for approximating to with a subexponential factor the volume of a convex body deterministically (this is an old result by Bárány and Füredi). In contrast, we can get an FPRAS for this problem using sampling. Is this an example of the separation you are looking for ? 

One model that captures hierarchical models nicely (think local cores, shared on-chip memory, and global memory) is a STOC 87 paper by Aggarwal et al. I don't think it ever got any traction, but it makes an interesting read. The main idea is that access to memory location x takes time $\lceil \log x\rceil$.