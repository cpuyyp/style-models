One of those gets rebooted weekly, another works perfectly fine for most people, and another has the capability to take part in huge wireless mesh topologies that work together to cover entire campuses, allowing seamless handoff, load balancing, etc etc etc. 

I don't guarantee that this code works right (most especially the line that starts with "RUNNING=", but substitute some test of your own that can return a string if it's running and no string when it isn't. 

Assuming that this is a locally authenticated user (as it sounds like it is), you should be able to only change /etc/passwd, /etc/shadow, and /etc/group, then mv her old home directory name to the new directory name. That would ensure that she has the same UID as before so no file permission errors will crop up. The only remnants of her old login would exist in log files. 

Is /dev/sda local or on the SAN? If it's on the SAN, then there's hope. If it's local, and it's using all of the available space, you're going to have to free up some disk space by removing files. Update the question and let us know what the status is. Edit Ah, it's a VM! You're in luck (if there's free space on the host machine, anyway). Create another virtual disk in the VM manager that is as large as you want the freespace to be. Then present that disk image to the VM. Make sure the VM sees it (use dmesg to see if it shows up). Assuming it does, fdisk it, and create one partition. Issue 

So in order to use it, either type "csh" and issue it from the command line, or write your script so that it uses #!/bin/csh as the interpreter at the top. Here are some csh basics to get you started. 

IANADBA, but I'm writing a script that will take action as long as the oracle standby database hasn't been activated. What I'm looking for is two queries (or sets of queries, if necessary). 1 - Is the database mounted (as in, has someone done "alter database mount standby database") 2 - How can I tell if the database is activated (as in, "alter database activate standby database")? As I mentioned, I'm looking for queries, but if there's a way to tell in the system, I'm open to that, too. Thanks! Update I took the suggestion below (modified, slightly, because I'm dealing with Oracle 8i, and I get this: 

Are you absolutely certain that you don't want to reinstall that machine? If it were a personal desktop, that would be one thing, but I'm not sure I'd trust a machine without a fresh install. I may be paranoid here, though. 

OK, this is why users should be disabled, rather than removed. There are built-in options to passwd for that sort of thing (and settings exist in every centralized-user-management software that I know of for it). But you're not here to get yelled at. Do the user's files still exist, or were those removed with the account? Screen uses the lock files to reconnect, and if they're gone, I'm honestly not sure how to reclaim the sessions. Try Keiran's suggestion of recreating the UID if possible. Sometimes it isn't feasible, especially in a centralized authentication where the IDs are hashed from the other user information, but if you can, that would be swell. Assuming you can't, you need to find a way to take over the terminal. I'm going to guess that TTYSnoop isn't configured on your system already. In that case, it's going to get hairy. Assuming that your screen sessions issue you a pts/# terminal like mine do, there may (MAY) be a chance. pt stands for pseudoterminal, and it's sort of client/server based. There are the pt slaves, which is the terminal that you type with, and there is the ptmaster, that receives the input. There are many slaves (/dev/pts/*) to one pt master (/dev/ptmx). From the man page to ptmx: 

Is NOT a good thing to do. (Here's a hint: .* matches ..) [EDIT] Thanks to Dennis commenting on this, you can disregard my warning (though I'm leaving the comment to show that I'm doubly an idiot) ;-) 

You could probably do it with your network ID, AAAA:BBBB:CCCC:DDDD:: or whatever it is for you. That would guarantee that only IPv6 interfaces would pick it up. I think. I'm no IPv6 master. 

As Gleb said, using mysqldump is the easiest way (depending on the traffic on the database. Very heavy and mysqldump won't pick up some changes). With apache, just rsync the directories across before-hand, then do it again when you're ready to do the switch. That way you minimize the size of the sync when it's crunch time. You'll also want to do it before hand to make sure that your various configuration files were picked up and synced correctly (and that you got them all). This isn't too complex, but it is good practice for bigger live migrations. 

Use an unencrypted transfer method that doesn't do compression. I'd suggest FTP, given how simple it is to setup and the lack of chatty protocol, like Samba 

Switches need to be reverse mounted (ie, their ports should face the same way that the server ports do, toward the back of the rack). Also, maybe you can get some use from this: $URL$ 

NFS always causes fun things to happen like this whenever UID/GIDs aren't lined up just right. Assuming that your webserver is running as user "apache", make sure that the permissions on the file are such that they're world-readable. su to the apache user and cd to the directory, and try cat'ing the files. It's most likely a permission issue. If apache isn't writing to the directory, it doesn't care if the files it's reading are on NFS or anything else. 

Look in your home directory, under .ssh and see if you have id_dsa or id_rsa (and an accompanying .pub file). 

You can selectively allow certain commands with sudo, but you must be careful to not permit programs that allow shell access, writing to sym links, or one of a few dozen other problems. Here's a page on secure sudo scripting: $URL$ You could always hope that they don't know about the chattr command ;-) 

This sends 2 (stderr) into 1 (stdout), and sends stdout to file.log Notice that it's also possible to redirect standard in into a command that expects stdin 

How many NICs per blade do you have? A proper iSCSI solution will require 4. 2 should go to bond the interfaces (probably in mode 1) so you can lose a nic (or switch (or cable) ) without losing connectivity. The other 2 should be setup with multipath to the iscsi target, and they should be on their own network with their own switches. iSCSI /can/ be routed and treated like normal network traffic, but according to all the people I've talked to, it /shouldn't/ be. 

1) Data should be kept as long as necessary according to the legal limits imposed by your industry Meaning that if you are in the financial industry, you have different requirements than if you are in the private industry, or ISP industry, or what have you 2) legal implications are a function of #1 3) The easiest ways are to use known 3rd parties for archiving. I use Global Relay for communications archiving, because they have many, many services available in the event of an audit and so forth 4-... First you need to decide which industry you belong in, then which requirements and legal actions you fall under the jurisdiction o, then we may be able to help, but your corporate attorney would be a better person for advice. Once he gives you advice, ask a question related to what he tells you you have to archive. We can't decide that for you. 

I was looking for a good vCenter or even just ESX(i) check for Nagios a while back, and I didn't find anything. I ended up writing one-off scripts to cover the need, and I'm going to go back and improve them when I get a chance. There is this: $URL$ but I haven't tried it, as it's only compatible the commercial version of Nagios, Nagios XI. If you can script in perl, the VMware SDK is available here: $URL$ It should contain all of the functions necessary to do what you want (and then some). 

You are really asking two entirely different questions. Here are the answers I'd give: Will my VPS's performance be dependent on other VMs on the same physical server? To some extent, yes, although (unless the admins are incompetent) you will receive at least the performance you pay for. Many VPSes are setup so that you can draw more above your set resources, as long as no one else that has been promised the resources needs them. As an example, CPU usage is affected by this a lot. If there isn't a lot of load across all of the VPSes, you may be able to get better processor usage than you paid for. But the second that someone else starts to heavily use their processor, you're going to lose performance, because the VPS host has promised them a certain number of cycles, and you were using what was theirs. Your performance will decrease, but not below what you're paying for (again, in the right configuration). Will a VPS experience fewer downtimes? This is a harder question to answer. Statistically, I say no. A hosted account is one small piece of a complex puzzle, but (in most cases) the server (or servers, more likely) are administered by one group of people, and those people probably have a uniform set of procedures that is documented. One change affects many, many accounts, so they're (hopefully) very careful about what they do. There is a process, and administration is managed, as opposed to ad hoc. When you get a VPS, you introduce a new variable to the mix. Namely, you. Not only can physical issues and hypervisor issues cause problems, you yourself can introduce instability. If you've got years of experience administering servers, then this probably isn't an issue, but unless you're confident in your abilities and have the experience to back up that confidence, the end result is that your server is unavailable more than it would be in the hosted example. Generally speaking, more moving parts raises the likelihood of failure. $URL$ 

I'll just say that the additional convenience of switching to virtualization has outweighed any performance issues that might have otherwise impacted my opinion. My stance is that my machines that need raw performance don't get virtualized. Everything else is making the transition as it becomes possible for me to convert them. 

I do not. My argument is that they impede airflow, and that there are better 3rd party cable management solutions that accomplish the same thing. I can count the times I've wanted to leave a server powered on while I was adding or removing hardware on 0 fingers, and that's their only^H^H^H^Hmain purpose. Edit I admit, they make it faster to pull hardware out of the rack, but in my opinion, it's not worth the hassle and heat.