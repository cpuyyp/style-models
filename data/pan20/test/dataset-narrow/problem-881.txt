Xen best practice is to disaggregate (in other words - split up) the functions of the management domain (dom0) as much as possible for security, overall system reliability, and even performance. ( $URL$ ), See also Ian Pratt's comments on a XenReference architecture ( $URL$ and $URL$ If you put your NAT firewall in a domU there is also the added benefit of isolating the firewall rules to that system and so they won't conflict. 

Two things. I usually put the IP address on the bridge. Also, I don't use the xend config file for this scenario. See an example here: (you can modify it to use eth1) $URL$ 

You can use: 1) xentop (and drill down on I/O) 2) monitoring tools like xenmon ($URL$ 3) Standard I/O monitoring tools that you would normally use should give a good indication of the I/O usage of the guest as well. 

The Xen Cloud Platorm (XCP) is a good solution and provides a lot of functionality and management options. It compares well with Citrix XenServer see: $URL$ There are also many management options: $URL$ Although not yet ready for production, you may also want to start taking a look at Project Kronos: $URL$ Using Xen the hypervisor and xentools for a small deployment is OK, but if you plan to do management of a larger set of machines or cloud orchestration (CloudStack, OpenStack, OpenNebula, etc.) then you should consider XCP, Project Kronos, or similar solutions. In the past new deployments (such as Amazon, Linode, etc.) came out on Xen, but new production deployments are likely to want to make use of the features of XCP and Project Kronos. See also: $URL$ 

prints a relative timestamp upon entry to each system call in case it's needed later for extra performance analysis. traces child processes and it might not be needed. The output looks something like this: 

Make the file immutable with chattr i.e. . Then no one will be able to change it, not even root, unless the attribute is unset. Note that only root can set or unset this attribute. 

The manual for ST32000641AS (alternative link) says that the drive has 3,907,029,168 guaranteed sectors, while the specifications for WD2003FYYS (alternative link) say it has 3,907,029,168 sectors. Therefore the drives have the same capacity. 

This could be caused by hard links which means that the files you deleted still exist under other names. To find them, run . 

or . Restricted means that only the user and perhaps the web server can read it (e.g. modes / or some ACLs). Immutability can be done with or by changing the ownership to something like . An easy way to create that hierarchy on Ubuntu would be to edit and set to . 

Use the PAM module called to set enviroment variables on login. If you use this solution, the environment variables will be available to GUI programs launched directly from the "Window Manager" (e.g. GNOME, KDE) too, not only to programs launched from a terminal. Also, the user can use another shell, not only Bash. Maybe this article from O'Reilly can help you. 

I think it is better and simpler to append space (notice the >> operation) to the disk image For example, to add 1G to a disk image do: 

First thing to take a look at is xentop and drill down into network usage. Beyond that you can also take a look at iftop to see if you notice any unusual traffic. There are several common symptoms and problems outlined here: $URL$ And several approaches to optimize things: $URL$ Take a look at those and do so more troubleshooting and data collection. That guide ( $URL$ has other general suggestions as well that should apply to Xen in general. Some things to think about looking at include CPU utilization or other over constraint symptoms (low memory, swapping, etc.). The other thing that is useful to think about is what type of guest is it (PV or HVM)? What driver is being used? Is there a better set of NIC configuration options that could be used? 

A few things to try: 1) Boot the guests by using kernel and ramdisk instead of bootloader in the domU config file. You can use the dom0 kernel and ramdisk 2) Try PV-Grub ( $URL$ ) 3) Boot the guests HVM and use the PV on HVM drivers ( $URL$ + $URL$ ) 

What counts as maintenance staff? The above answers seem to imply that only janitors would fall into this category but staff electricians, HVAC engineers, etc are often part of the maintenance crew in lots of shops. Lots of larger enterprise DC's I've had contact with have actually specifically excluded the majority of IT staff - to include senior systems and network engineers, etc. The idea is that a very specific set of DC facilities/operations people should be sufficient to physically operate the infrastructure without particular non-facilities domain experts being allowed into a space that they're often not qualified to be in anyhow. It's actually only in the smaller facilities that I've seen sysadmins typically involved in standard rack-and-stack / cabling. Some network organizations keep their hands in longer, but even they end up splitting off day-to-day cabling (and even a lot of the layout/design) to dedicated facilities people. I've generally just chalked this up to the greater need for specialization. BTW - Dedicated DC facilities orgs will often have their own specially trained cleaning staff. There -is- need to keep these areas clean over time, albeit through different means than standard office space. 

Don't ever use a ping to a Cisco router interface for anything other than diagnosing general connectivity. Responding to ICMP messages is (and has always been) just about the lowest priority on the box. This is true from the smallest to the largest routers and is called out. Ping -through- the router to something else (i.e. some sort of host). This is incidentally true not only of Cisco routers but plenty of others. The only exception to this is using SLA monitoring, which is a set of features that run at a higher priority to measure the latency of a given link. This is a very different animal than pinging a router interface, though. 

Enable the modules. I suggest starting with the corresponding file underneath the directory (I used ). Prepare the kernel for modules: 

This will delete all the jpg files from , which haven't been modified in the last 7 days (168 hours). If you don't care about the case of the filenames use instead of . Here's a correspondence between the parameters of forfiles and the parameters of find: 

Short explanation of the AKW one-liner: is the PID, is the timestamp, is the remote port and is the remote address. The advantage of this solution is that root is not required if the server runs under the same user. The disadvantage is that all connections are counted, there's no filtering, so it won't work if the application listens on multiple ports. 

dislocate is a command that comes with Expect (at least on Fedora). Answer with when asked . You could also try using pexpect, a pure Python Expect-like module. Check out its hive.py and sshls.py examples. 

I recommend the Fedora Live CD on a USB stick. It's not very small (it has 700MB), but it's easy to use and since it's a bit bleeding edge you shouldn't have any issues with new hardware or filesystems (although ext3 is already a bit old). 

Actually, the IP network works over MPLS. The idea of MPLS is that a series of labels can be applied to a given packet (or frame) that can subsequently be used to switch it through a network. In the case of an L3VPN that means that rather than the traditional mechanism of looking at the destination IP address, routing devices look at one or more previously applied labels to make forwarding decisions. The key point in the above is that the actual -contents- of the packet aren't actually considered. Once a given packet is labeled the intervening devices simply forward it based on whatever LSP has been signaled. In the case of an L3VPN, the packet is a fully formed IP packet. In the case of an L2VPN a frame from a particular interface has a label added and is forwarded. This might be a full Ethernet frame (with or without an 802.1q header), an HDLC frame from a serial link, one or more cells from an ATM PVC, etc.. One of the contrasts between L2 and L3 VPN is the mechanism used to signal and set up the overlay network. L3VPN (RFC2547bis) extends the BGP protocol to allow PE's to signal which routes are available within which VPN's. There are more possible ways to put together a layer 2 network (i.e. straight point-to-point links, multipoint, translational, etc) and there are also more mechanisms in use in the industry used to signal these various topologies. As for links - google is obviously your friend, but if there's some specific aspect of L2VPN design, architecture or setup it would make things easier for us to direct you. 

Secondly, not even this will work because ssh doesn't read the answer from stdin, it reads it directly from the terminal, so you'll need something like Expect: 

I don't think you've generated the required certificates, therefore you're getting that error. So either generate the certificates or use another transport, for example tcp (unencrypted, good only for trusted networks) or ssh (e.g. ). 

How about a plain ? After all most devices are just a file located under . LE: You need to do the renaming every time you boot. 

CentOS doesn't have as many packages as other distributions and some of them are old, too. Add the EPEL repository and see if it has the necessarily packages. The python-reportlab and python-psycopg packages are available in the RPMforge repository. I would recommend installing packages from this repository only if they're not available in CentOS or EPEL. This can be done with yum-priorities. 

If the NS server don't have an A record how can BIND find out their IP address in order to query them? It can't, so it can't resolve domains served by them. On a side note I've run a test myself and I've noticed that I can resolve 10gb.co.il using the DNS servers of my ISP, but I can't by running (this uses the root name servers). Your issue is caused by an improperly configured domain. My guess would be that the DNS servers of my and your ISP have the address of 10gb.co.il in their cache, while the current setup of the domain is incorrect. 

There are some devices that use 24V (Ubiquiti AP's come to mind) but in general 48V is the most common. From personal experience, however, I strongly recommend you do not mix different voltages - even experimentally. The camera manufacturer might have a power injector - that is probably the best bet. 

Your diagram is slightly unclear - I'm assuming that the two Ubuntu hosts are each connecting to the "router" device, which provides L2 and L3 connectivity. Anyhow - if the various VM's are on the same subnet (likely) then multicast will work between the hosts. Apart from making sure that multicast support is enabled on the guest operating systems there isn't a whole lot to do. If these VM's were, instead, on different L3 subnets then you'd need to make sure that the router supported multicast routing. 

It's fine to have two default routes but, as Ron points out in the comments, you'll almost certainly want them to have different metrics such that normally one (presumably the ethernet interface) will normally be preferred. You'll want to edit the script you added for the interface and add to the bottom. When the tunnel interface is brought up this should result in two default routes, the existing one with a metric of 1 and the new one with a metric of 100. If the default route via the ethernet port goes away then v6 traffic will start to flow via the tunnel. Keep in mind that you may also want to add some additional v6 static routes via the tunnel interface based on what you'd like to reach via HE. EDIT: Redhat/CentOS doesn't consistently implement/honor the METRIC value across the various interface scripts. You could certainly modify these scripts to include this value but this might not be desirable from an operability point of view, as you'd need to track any changes made upstream to these stock scripts. So... you can do this from the CLI pretty easily: 

That's the whole idea, to use systemd (or upstart etc) inside a container. Docker is used most of the time to run a single service per container, so for a complex site you would need a container for the web server and another one for the database server. With a process manager you could run both in the same container. Which approach is better is opinion-based. Though from what I've seen running systemd inside Docker isn't easy as of June 2014, for example there's bug #3629 - "running systemd inside docker arch container hangs or segfaults". For more details also read "Running systemd within a Docker Container". 

Not exactly an answer, but I would recommend using SFTP instead of FTP. It's easier to setup given the fact that every box comes with the OpenSSH server and client. It's also more secure and SSHFS is nice bonus, although I don't know if RHEL has support for it. 

Install one system, boot it and check out the block layer statistics from e.g. . Quoting from the documentation: 

I want to manage the mounted partitions from puppet which includes both modifying and creating the directories used as mount points. The resource type updates just fine, but using for creating the mount points is a bit tricky. For example, by default the owner of the directory is root and if the root (/) of the mounted partition has another owner, puppet will try to change it and I don't want this. I know that I can set the owner of that directory, but why should I care what's on the mounted partition? All I want to do is mount it. Is there a way to make puppet not to care about the permissions of the directory used as the mount point? This is what I'm using right now: