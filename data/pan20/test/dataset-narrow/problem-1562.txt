I have a pipeline built which at the end outputs a bunch (thousands to tens of thousands or more) of named entities. I'd like to do aggregates on those named entities (to see, e.g. how many times a specific named entity is mentioned in my corpus). A problem that I am arriving at; however, is that the named entities often don't match up with each other even though they are the same entity. For example, one instance of the named entity might be "Dr. John Smith" while another instance is "John Smith" or one instance might be "Google" while another might be "Google Inc.". This makes aggregating quite hard to do. In order to deal with this issue and set "Dr. John Smith" to be the same entity as "John Smith", I was thinking of doing word matching between my named entities. I.e. I would check if named entity A has a word in common with named entity B and if they do set them to be the same entity. This approach is obviously seriously flawed. I will be equating "John Nguyen" and "John Smith" as the same entity even though they are obviously not. What's potentially even worse with this method though is I might run into similarity chains where I have "John Smith" linked with "Richard Smith" linked with "Richard Sporting Goods Inc." linked with "Google Inc." etc etc... While I may be willing to allow issues arising from former problem through, the latter problem appears to be catastrophic. Are there any accepted techniques in the NLP community for dealing with this issue? 

I was not 'happy' with the documentation as well. Then I switched to working with RNNs in Keras. Which can recently be directly used in R: $URL$ 

That's a popular question, since many use cases require to understand what's going on in the model. I suggest to do just some simulation with the resulting NN. Like in your example you could have a test-dataset and den modify the 'studying_hours' by +5%, +10%, +15%, ... and then plot the result over this modification factor. This way you can see by an simulation what the NN is thinking of the 'studying_hours' and how it influences the result. 

If you initialize all weights with zeros then every hidden unit will get zero independent of the input. So, when all the hidden neurons start with the zero weights, then all of them will follow the same gradient and for this reason "it affects only the scale of the weight vector, not the direction". Also, having zero ( or equal) weights to start with will prevent the network from learning. The errors backpropagated through the network is proportional to the value of the weights. If all the weights are the same, then the backpropagated errors will be the same, and consequently, all of the weights will be updated by the same amount. To avoid this symmetry problem, the initial weights to the network should be unequal. Look at these links in more detail: 1) $URL$ 2) $URL$ 3) $URL$ 4) $URL$ 

I'm only aware (and using) a RNN which gets multiple time-series in it's first layer and then mixes those in the following layers. Let me know if you have a question on this approach. 

My intuitions is that training each tree on a subset of all variables helps the less useful variable to be used at all. Since often there are some features highly correlated to the target and then all trees will just use these very good features and never ever use the weak ones. By working on random subsets the weak features are used sometimes and contribute to the result a little. 

In Keras there is a helpful way to define a model: using the functional API. With functional API you can define a directed acyclic graphs of layers, which lets you build completely arbitrary architectures. Considering your example: 

There are two well-known regularization methods for fighting overfitting: 1) Using L1/L2 Regularization(Weight Decay) 2) Adding Dropout The question is: when/where it could be useful to utilize these two methods into one single model of the deep neural network? For example, in Keras: is there any situation that a model like this (applying both methods) works better than using just one of them? 

Short Answer: It is very related to the dimensions of your data and the type of the application. Choosing the right number of layers can only be achievable with practice. There is no general answer to this question yet. By choosing a network architecture, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. In a DeepNN each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the problem at hand, this information can never be recovered by later layers. This is usually referred as "Information Bottleneck". Information Bottleneck is a double-edged sword: 1) If you use a few number of layers/neurones, then the model will just learn a few useful representations/features of your data and lose some important ones, because the capacity of middle layers are very limited (underfitting). 2) If you use a big number of layers/neurones, then the model will learn too much representations/features that are specific to the training data and donâ€™t generalize to data in the real-world and outside of your training set (overfitting). Useful links for examples and more finding: [1] $URL$ [2] $URL$ 

Many neural network examples you see in the literature are doing classification problems, E.g. LeNet, AlexNet, Inception, etc are all image classification problems. In this domain, it's useful for the neural network to give outputs between 0 and 1 because an output between 0 and 1 can be interpreted, in some sense, as a probability. The reason these networks output numbers between 0 and 1 is in the layer activations of the network. The last layer in these networks is usually a softmax layer (or, if you're doing just binary classification, a sigmoid layer). Softmax and sigmoid functions have the nice property that they give outputs between 0 and 1 (softmax has the added nice property that it gives outputs which sum to 1). If you want your neural net to be able to output numbers that aren't between 0 and 1, simply change your activation function. Instead of a softmax last layer, you could use a linear one. In this case, it also makes sense to change the loss function you are using to perhaps something like Mean Squared Error (binary cross entropy, for example, won't work too well on negative numbers). There's nothing stopping you from using a deep neural network to perform regression rather than classification. 

These 2 are pretty popular: $URL$ - Tensorflow-Course by Google $URL$ - Deep Learning by a Kaggle hero The first is easy to follow and nicely presented, the second takes quite some time. 

Excel and Data Science - sounds really strange to me. Maybe Excel and 'Data Analysis'. Anyways, I think a good compromise between Excel and R is: KNIME ($URL$ It's free on the desktop and much easier to get started. You can import / export to Excel but also use R, Python or Java if the ~ 1.000 nodes miss some functionality that you need. Since the workflows are visually created, it's also much easier to show them to someone who doesn't know any programming languages - which is quite an advantage in some companies. 

Say I'm building a neural network to fit some classifier of some sort. To make things concrete, let's take the example of predicting housing prices using features of houses. What should I do if one or two of my features consist of many more numbers than the other features, or even all other features combined? For example, say I have a few housing features: size in sqft, age, median income of location. These are 3 numbers. And then I have another feature, height of the roof for each square foot of the house (it's a bit contrived for this example of course) for which I would have actually "size in sqft"-numbers for this feature. So now my feature vector looks like this: X = [1500sqft, 34 years, $54,000, 10ft, 10.1ft, 10.3ft...1497 more numbers here...] It seems that if I just naively put this into a neural net that the first 3 features would essentially be ignored since they only account for 3/1503 features. But they might actually be important. One try might be to simply average the "height of roof" feature over all of its elements to get an "average height of the roof" feature. That makes sense for this example, but what if sometimes I don't want to take this average? Are there any industry practices on what I might try if I ran into a problem like this? 

I've implemented a toy example with Keras, here, and the results is like this (comparing training and validation loss): 

You should design a multi-task model (MTM). MTM has the ability to share learned representations from input between several tasks. More precisely, we try to simultaneously optimize a model with types of loss function, one for each task. Consequently, MTM will learn more generic features, which should be used for several tasks, at its earlier layers. Then, subsequent layers, which become progressively more specific to the details of the desired task, can be divided into multiple branches, each for a specific task. You need an architecture like the following: 

I don't think that this is defined in general. So you should be more specific on where you are using this. I could guess, maybe you mean the Time-Series Object in R? Then take a look at: $URL$ And here the definition is: frequency -> the number of observations per unit of time. 

I don't completely understand your example, but: if you want to put your model into operation/production environment at some day ... then you should not allow 'looking into the future' at training time. 

Same here - no idea, have not seen this before. I guess they tried different transformations and picked the one that worked best. Since in the report they say that some other transformations would be also fine.