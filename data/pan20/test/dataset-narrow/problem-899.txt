Add a CQWP and put a redirect string in it. < meta http-equiv="refresh" content="0;url=$URL$ I had to put an extra space in this so the page could show it without redirecting Do that three times for each of your groups Set the audience targeting to one group per webpart. The Board would be targeted to the one that redirects to their pages and so forth. 

Make your library read only. Create new permission level that only allows add but not delete and not edit. You could also make a custom workflow with SharePoint designer that will automatically move a document from the insecure "drop box" library to a more secure library on all items that are added. 

Locking down by IP will help reduce the risk, but if the service is critical and you must strongly authenticate the two servers to each other you could also look at setting up an SSL/TLS tunnel between the two machines, authenticated by a certificate. 

I can see one major flaw with this - all an attacker has to do is hijack one communication and the app will then only talk to their computer. Realistically, it wouldn't be too hard to watch the traffic to see what would be required. Of course you could use encryption using a public/private key pair to avoid this issue, then you are authenticating - which seems like a much better idea. 

The clean solution is to investigate writing or buying a redirection webpart. Bamboo seems to have one that would fit your bill. You could also hack a CQWP to do the same thing in a less elegant but still works manner. On your Kickoff page 

There are a number of ways to do this. Be sure you choose a supported method so you don't have a nightmare upgrading to 2010 when you get around to it $URL$ Look at the first option for creating your own site definition. I don't think you want to copy the whole server over, I think you want to package up your changes and deploy them as features on the production server. Please note this is not coming from an experienced voice, but this is our plan when we finally customize our install. 

I'd be tempted to time it- if it comes back quickly assume refused, otherwise timed out. You may get some edge cases where it is refused just before time out period, but at least you should get reasonable results. Using bash's time command will give you what you need. You'll need to figure out what the thresholds should be for a refused and a timeout, maybe through trial and error. 

Security angle: The best assumption is that attackers could spoof any address, so you should blanket filter all the ones that should never come in through your perimeter. This would include the ones in James and Steve's answers, plus any others you can guarantee should never hit your outside interface. Don't just assume they would require a valid address so they can receive response packets - they may not need to, depending on the type of attack. 

We are looking to create a knowledge base system in our regional office. We have tried to convince the national office to purchase add-ons to our help desk software, but so far it has fallen on deaf ears. If we begin our own KB, the last thing we want to do is spend time migrating data manually, so are there free KB systems with robust export tools? The nebulous nature of the question is because we don't know if national will ever choose a system, so we don't have a target import to. As we gather requirements on our team we feel being able to dump the data and massage it into another system might be a good idea. Right now we are gathering requirements, so anything goes 

It is a trade-off, as @MartinVejmelka says. The reason you use key based auth is that the key is so far above current or near future brute forcing that you need to be either coming from your own PC, or have the key on a USB stick or similar. A password has the following issues: 

From carrying out a fair amount of penetration testing on virtual environments I would add these two items to watch: Plan your virtual environment exactly as you would a real environment - as any structural or architectural vulnerabilities you introduce in the real world will translate well into the virtual world. Get your virtual configuration right - 99% of all successful penetration I have managed into VM's or LPAR's has been through misconfiguration or reuse of credentials. And on a less technical note, also think about segregation of duties. What may have been handled by network teams, server teams etc. may now be one team. Your auditor may find this important! 

The only answer I can come up with is to create a new site collection for the sites that require the security. Your new problem is who will be the admin for those sites, which at best is something that is granted and taken away as needed, and at worst is done by someone unqualified for the position. Besides site collection admins, how to you keep the farm admins out? Then you are back to the answers already provided about the level of trust you must put in your admins. 

Ensure our criteria for deleting a site are met (business rules like site age, inactivity, lack of purpose, expiration date reach, etc) Contact the site owner and get a response in writing that we will delete their site Backup any data Confirm any groups created in the site aren't being use by another site (learned this the hard way) Check the site usage report (if inactivity is the issue) Confirm any alerts on the site are invalid Confirm no sub sites are available or needed. 

As someone who has managed many hundreds of penetration tests for organisations in the Fortune and FTSE 100 as well as very small local companies I have the following for you: Broadly speaking you are doing the right thing with regards pen testing in terms of using well known external companies to see what they can get access to, however the most appropriate way to do this is: 

You could do something simple as a first check: go to one of the internet speed check websites - these let you determine upload and download speeds, and often the latency as well. You run it, and let the client run it - compare times.