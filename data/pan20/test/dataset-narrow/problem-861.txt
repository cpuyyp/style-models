Problem solved! Oddly enough, disabling UDMA in the BIOS fixed the problem. Thanks goes to this Ubuntu thread for having the answer. And thanks to everyone here for their suggestions as well! 

Use Shorewall instead of iptables directly. Shorewall is a lightweight config tool that makes it much easier to manage your iptables rules. You can get OpenVPN up and running with your firewall in about 10 minutes by following the OpenVPN HOWTO. 

is a bash builtin, not a standalone executable, so won't work. That's why you're getting that error. What are you trying to accomplish? 

When a new user signs up, use a service like Twilio to send a text message to their phone containing a short passcode. They have to correctly enter this code to verify their account. This is similar to how Craigslist (attempts) to keep spammers out and is also frequently used by online banking as a kind of a poor man's two-factor authentication system. 

If your database is MySQL, MS SQL, or Oracle, you're much better off using an RDS instance rather than trying to roll your own using EC2 and EBS. RDS functions like a cloud-based version of a DB server and greatly simplifies configuring and maintaining a scalable, fault-tolerant database. The underlying OS is completely managed for you so you can't SSH into it, but you can connect to it with your normal client tools to manage your database. 

Supervisor (written in Python) will do most of what you want out of the box. It can be extended with the other bits of functionality using the XML-RPC API. 

I have a Dell PowerEdge with a 8x backplane and a PERC 5i RAID controller. I'd like to configure two disks in a RAID1 array and four other disks in a RAID5+1 array. I'm pretty sure this is possible using a single controller, but I wanted to make sure first. Thanks! 

Having two ElasticCache nodes replicating across regions isn't going to work very well. The network latency caused by being separated by 2,500 miles will negate most/all of the performance gain. You shouldn't need to be sharing session data across regions though. Setup latency-based DNS routing in Route53, which will route each user to the closest datacenter automatically. Their sessions will live within their region. 

Security groups are applied to instances, not AMIs. So the customer would be responsible for setting this up. However, if your application requires a specific security group configuration, you may wish to distribute a CloudFormation template with your AMI that makes it easier for the customer to set everything up correctly. 

If you've placed your RDS instances in a pair of private subnets (as recommended), then you're going to have to spin up a bastion host to access RDS from outside of the VPC. Please take a look at this answer from the RDS FAQ. 

You're not going to find a standard, shared webhost that lets you run an application that uses raw sockets. It would be too easy to abuse that feature and screw up everyone else on that box. (Google App Engine, for instance, doesn't even permit raw sockets in their cloud). You'll have to look at getting a VPS, at a minimum. There are plenty to choose from; a Google search will tell you more than I ever could. As an example though. I have heard good things about Slicehost (though I've never used them myself). They have a basic $20/mo. VPS plan that should do everything you need. But you're not going to find something that's like $5/mo. PHP hosting. 

Yes, this is possible. Here is the design pattern that you'll want to implement. The short version: You'll create an IAM user account for each Alice and Bob. They get their own AWS access keys that will be used to access the S3 bucket. You'll then apply a S3 policy to the bucket in question. Where most people get confused is that "IAM policies" and "S3 bucket policies" are two, different things and both need to be set up. 

The line here is extremely blurry: What constitutes a reseller? For example, I rent a private rack in a local datacenter. The equipment is 100% owned and managed by my company, including our firewall and switches. I also have my own /26 (~64 IP addresses). Let's do a bit of research: 

The Right Way(tm) to do this is with GlusterFS, which is a scale out filesystem that runs in userspace atop an underlying filesystem like NFS or CIFS. All this sounds impressive and complicated, but it's actually really simple to set up. It honestly should take you less than 15 minutes. You'd have two servers, each with a "brick" (unit of storage) defined. A volume would then be defined within this server pool. Replication of the data between the servers happens automatically, as does failover on the client if one of the servers dies. 

Your config is correct. Your carrier is likely preventing you from setting an arbitrary CID number; check with them. 

Google Analytics doesn't include traffic from web spiders, which is the likely source of your additional traffic. I'd recommend that you checkout out your crawl statistics using Google's Webmaster Tools. It (obviously) will only show you traffic generated by GoogleBot, but might offer a bit of insight. 

What's wrong with using the built-in Windows Performance Monitor (perfmon.exe)? It's free and works well. You can use it to log some pretty granular stats on IIS. And if you don't want to store the logfiles on the actual monitored server, you can always run perfmon on another box and connect to it across the network. 

How well does it work if you eliminate PHP from the equation? Use the CLI mysql client to connect to the server. Try it from both the db server itself and from the app server: 

Someone installed the 32-bit version of MS SQL Server 2005 on a 64-bit OS with 16GB of RAM. Two instances of SQL Server are running, but each process is only using about 1.7GB of RAM. Combined, this equals about 3.2GB, or the hard limit of 32-bit applications. I'm trying to figure out why each instance isn't using it's own 3.2GB max address space? 

I'm planning on stacking Dell PowerConnect 6248 switches together. I know that I need one stacking module in each switch, but I'm uncertain about the cabling. Since I only have two switches, do I need to loop them using two cables or will one stacking cable suffice? Thanks! 

There used to be a mechanism for checking the validity of addresses by using the SMTP "VRFY" command. It was handy when you were trying to email someone but weren't quite sure as to the proper format or spelling of their email address. However no modern MTA responds to that command anymore because people like you started to use it to clean lists of a million email addresses. 

My company currently uses Google Apps for email. I can configure Google Apps to forward all outgoing email through a relay server. I can also point my MX records at this server and have it forward incoming mail onto Google. Therefore I can configure Postfix act as a proxy for all email that is both sent to and from our users. This part is done and works fine. However, I'm not sure how to retain a full copy of each message that passes through it. I'm interested in doing this so that all email to/from customers can be dumped into our CRM system so they're searchable by everyone in our company who might talk to a customer. The most common way of doing this is to have a "always BCC" setting specified in Postfix. However, this would only archive email sent to customers, not email received from them. Does anyone have any suggestions? Thank you! 

It would make it more difficult to map the architecture of the LAN from the inside. This helps security but hinders troubleshooting. It's up to the people running the show to determine the balance between those two needs. 

You can use round-robin DNS, as previously mentioned. And if you're looking for a quick and easy solution, that's probably what I'd recommend as well. However, EC2 instances aren't durable and you shouldn't treat them as such. Meaning, that they can up and disappear like a fart in the wind and your application has to be tolerant of that. RR DNS could be a source of pain for you because its not smart enough to deal with fault-tolerance/high-availability. If one of your web servers were to die, the user would be stuck trying to connect to the dead machine until the TTL lapsed on the record. The previously mentioned trick of setting a ridiculously low TTL won't work. Downstream caching DNS servers won't honor a TTL that low. You'd be lucky to get away with a TTL of one hour. Many users might be stuck trying to connect to the same dead web server for 24 hours or longer. I'd recommend that you look into HAProxy. Reddit.com uses HAproxy on EC2 to load-balance about 200 million pageviews/mo. across several dozen web server instances. I'm sure it'll handle your app as well. 

AWS publishes their public IP address space. You could probably try pinging some random IPs in those ranges and see if your packets get killed at the firewall. 

Edit: It's taking about 5 minutes to just get past the "GRUB loading stage 2..." message. I've come to the conclusion that the problem is likely either: bootloader, BIOS/firmware, or a hardware issue. I've played with all relevant BIOS settings without any luck and replaced my IDE cable in case I had a bad one. I have a new compact flash card (a different brand) on order, but it'll be a few days before it gets here. I'm also currently running Memtest86 to see if I have an odd issue with my RAM. Once that's done, I'll see if there's a firmware update available for my motherboard. 

No, instance states have to be consistent (need to be identical) to properly work behind a load-balancer. The proper solution to this problem is to upload your photos directly to S3. 

You never want to run a single EC2 instance in a production environment. They aren't durable and can permanently go offline at any time (taking all the data stored on them with it). Use a minimum of two application server instances behind a load balancer. Since you're new to this, I'd recommend that you read up on the documentation for Amazon Elastic Beanstalk. It's a PaaS-like configuration framework that makes it easier to get a properly designed AWS infrastructure online. 

Solved! I happened to notice this line in on my NFS server when I was attempting to mount an export from the remote client: 

As previously mentioned, the robots.txt spec is pretty simple. However, one thing that I've done is create a dynamic script (PHP, Python, whatever) that's simply named "robots.txt" and have it smartly generate the expected, simple structure using the more intelligent logic of the script. You can walk subdirectories, use regular expressions, etc. You might have to tweak your web server a bit so it executes "robots.txt" as a script rather than just serving up the file contents. Alternatively, you can have a script run via a cron job that regenerates your robots.txt once a night (or however often it needs updating) 

First things first, if you only have two instances (one web and one DB), you're doing it wrong. You should be setting up an elastic load balancer with a minimum of two application server instances behind it. Instances can (and do) fail from time-to-time. And you really should be using RDS for your persistent data store. 

No, there is no way to automatically do this. You could write a simple script in your language of choice to manually replicate keys to a different region. I'm not really sure why you'd want to do this though? The network latency introduced from traveling half-way around the world would negate any benefits of memcache. You're better off replicating your persistent datastore and then having a separate memcache cluster in from of that data in each region. 

Yes, it's the customers' responsibility. However, a CloudFormation template could help here as well. 

Yes, go into your security group settings and permit incoming ICMP traffic on the one that is assigned to your ELB. Security groups work the same whether they're assigned to an ELB or EC2 instance. I just tested and confirmed: 

The only way you're going to be able to do something like this without a massive investment of hardware and network planning is using a hosted streaming service (similar to web hosting services, but more niche). You can also look into using Amazon's CloudFront Streaming service. 

This allows traffic to/from behind the peer. However, I also have another subnet, behind the same peer that I would like to allow access. So I've had to modify my ACL to include : 

You can, but it will frequently break stuff (esp email) in unexpected ways and shouldn't be done. This how I would do things: 

You didn't mention which distro you're using. That's important to know because startup scripts vary quite a bit from distro to distro. Assuming you're working with Redhat/CentOS... Ideally you should track down an init.d script. This is the better option because it's also used to safely shutdown your daemon, reload it's config, restart it, etc. If you want the quick and easy solution though, just stick "" at the end of you file. Edit: I see by the path name in your example that you're using Debian. I'm not sure if Debian has an /etc/rc.local file. But I'll leave this answer up just in case it does, or in case this information is useful to someone else. 

Running everything on a single EC2 instance defeats the whole point of a cloud-based deployment: the ability to autoscale and self-heal. As I've written before, autoscaling is the heart and soul of AWS and if you're not using it, you'd be better off using a traditional co-lo server or VPS. It would be both cheaper and more durable. I just completed an AWS deployment that is very similar to your needs. The client runs three, fairly high-volume Wordpress sites (quite a bit more traffic than yours). The config looks like this: 

I'm using CentOS 5.4 on my dom0 with a stock Xen kernel. I'm attempting to use the pciback module to hide some of the Ethernet ports from the host and reserve them for a domU I intend to use for a firewall (process described here and here). However, when I launch the domU, I get the following error message: 

This design does not have any single point-of-failure*, will automatically self-heal if an instance dies, and is smart enough to scale up or down according to resource demand. Total cost: about $200/mo. if you opt to purchase 1-year reserved instances. I'm working on putting this configuration into a combination of a CloudFormation template and cloud-init/Python scripts that are automatically pulled from Github on boot. Basically it will allow anyone to pretty much push a button, wait for about an hour, and then come back and this whole environment will be waiting. I hope to have this completed by the end of the year. If you'd be interested in getting a copy of the template, send me an email to "jamie" at the website listed in my profile. * The NAT instance is a SPoF, but that's primarily a design limitation of VPC. And it's a non-critical component that could fail and not affect the application. 

As part of a pilot project, I am attempting to set-up a thin client environment for a team of developers using NoMachine. Each developer will login to the same Linux box and do development via an X session. Currently, each developer runs their own HTTP daemon on their local workstation that listens on 127.0.0.1:5000. However, if I move everyone onto the same machine this obviously creates a problem with port conflicts. Ideally, I'd like to keep their workflow the same. If I have to assign everyone a unique port, it's just going to create a lot of grief and confusion. Is there a way to do this? Can different processes bind to the same port on a per-user basis? I discovered a way to use iptables to do port redirection on a per-user basis, but this only solves part the problem: