Is there a "best" way to automatically restart SQL Server on a regular basis? I read that I could create a batch file to net start, net stop the agent and service in a batch file and schedule that, but I was curious if there was a better way of doing this. Please nevermind how this is indicative of a larger problem of operation, I've been arguing this for a month and we've ended up here. Thanks :). 

I was recently tasked with moving some databases to a new server. I created backups then restored the databases to the new server and it looks like the database users were retained, however there's no corresponding logins, prohibiting the users from accessing the new server. All of the users are domain users. Is there any way to iterate over the existing users in a database and create a login for each? 

In Active Directory, I can see that the service account for ServerB is trusted for delegation to MSSQLSvc, but I noticed that the service account for ServerA does not yet have "trust this user for delegation" enabled. Does the target server also need to have that option enabled? Is anything else necessary to be able to use the current Windows login to use a linked server? 

I'm trying to get a linked server to ServerA created on another server, ServerB using "Be made using the login's current security context" in a domain environment. I read that I'd need to have SPNs created for the service accounts that run SQL Server on each of the servers in order to enable Kerberos. I've done that and both now show the authentication scheme to be Kerberos, however, I'm still facing the error: 

In SQL Server, I have a user in a particular database and I've been asked to grant them access to all of the non-system views of the database only. I believe this can be done by editing securables of type view and granting select on each one, but there are many, many views. Is there a more efficient way to accomplish this? 

But the delete statement in itself is also too slow or as slow as the orginal UPDATE statement so without the delete statement brings me to this point. 

What other alternatives can I use rather than just the normal UPDATE statement? Currently what I'm trying to do is using CTAS or Insert into select to get the rows that I want to update and put on another table while using AS COLUMN_NAME so the values are already updated on the new/temporary table, which looks something like this: 

Let's say I have an input of 20 gigabytes of raw data (text file) which in turn I would into an external table and would then swap partition in order to place them on my database. The question now is given 20 gigabytes, what should I expect the size of my TEMP/UNDO and USERS (default) tablespace to be when this happens given the following assumptions: 

First of all what kind of database are you using? And are you referring to table partitioning? Anyways for Oracle, please see this link.. $URL$ I'm not too sure about how this would be done in PostgreSQL, but I think partitioning is much less relevant for PostgreSQL rather than for Oracle 

Given these considerations, I have come up with this sizing estimate and as to why I chose these: (Please correct me if I'm wrong since I am not an experienced DBA) USERS tablespace (40 gigabyte) ~ I might use a bigfile tablespace data file instead of a regular one. I am assuming that the size of the input text file would be of the same size when I transfer it to a table. The reason why I doubled it is because I am going to use CTAS which from what I understand would be similar to copying the table itself. TEMP/UNDO tablespaces (20 gigabyte for each) ~ From what I understand, temp is used for sorting data and UNDO would be used for rollback and updates (please correct me if I am wrong about this), and given that most of my queries and update statements would load most if not all of the 100 million records on my table, I am thinking that the exact size for sorting and undo should be the same as the data file size itself. (Again this might be a less educated guess and please correct me if I am mistaken) Summing that up I would have around 80 gigabytes of tablespaces on my database machine, exclusing the redo logs and the indexes which I think I'll put on another tablespace sizing around 10-20 gigabytes giving me a total of 100 gigabytes So right now I am looking at 100 gigabytes table space needed for an input of 20 gigabytes. Is this okay or am I doing something wrong here and blowing the tablespace sizing out of proportion? 

MySQL Cluster supports storing non-indexed columns on disk-only with an LRU cache of recently accessed data. However indexed columns are always held in-memory. MySQL Cluster preallocates all memory, according to the DataMemory and IndexMemory parameters. It will not ask the underlying OS for more memory dynamically. This means that you need to have configured enough memory across your cluster to hold all indexed columns in memory. If your dataset is large enough that the indexed columns are larger than the available cluster memory, then you cannot load that dataset into the cluster. At some point you will run out of space, and your inserting transactions will be aborted. When configuring DataMemory and IndexMemory, it is best to limit yourself to somewhat less than the physical memory in each system. Some physical memory should be reserved for the OS and other processes. Theoretically MySQL Cluster can be configured so that it uses virtual memory via a swap device (e.g. more than physical memory), but as the other answers state, this is not a designed-for use case. Having in-memory structures swapped to disk is usually sub-optimal as in-memory random access patterns result in random access to the disk, resulting in swap thrashing and slowdown across the system. With MySQL Cluster, the most likely outcome is heartbeat failure and cluster failure due to a swapping data node not responding to signals quickly enough. To efficiently support larger-than-aggregate-memory indexes, MySQL Cluster would need to support on-disk index formats (perhaps a B tree etc) with caching and access patterns aligned with disk access properties. 

The data node marked as 'Master' in the output from the ndb_mgm show command is the coordinator of some cluster internal management tasks. For example, it coordinates changes to the distributed dictionary due to DDL commands, and manages node join and leave transactions. The 'Master' role is dynamic, and is generally assigned to the longest running data node in a cluster, only moving when that node fails. The 'Master' role is completely unrelated to the 'Master/Slave' concept in MySQL replication, and also has no bearing on the replication of data between nodes within a cluster. Perhaps it would be better if it were called 'Coordinator' or 'Leader'. As you say, you do not need to worry about which data nodes are used by MySQLD, all are equivalent for most operations and it is handled automatically. 

So far everything seems to work faster than the normal update statement. The problem now is I would want to get the remaining data from the original table which I do not need to update but I do need to be included on my updated table/list. What I tried to do at first was use DELETE on the same original table using the same where clause so that in theory, everything that should be left on that table should be all the data that i do not need to update, leaving me now with the two tables: 

And this table has about 1 Billion rows. I would want to update the status column with a specific where clause, let's say 

20 gigabytes of data would be placed on one table (partitioned) approximately having 100 million records I am going to bulk process this data using CTAS (then drop the orignal and rename the new table) as well as do some mass/bulk updates and selects I am going to place multiple indexes on this table (uniques) I am not going to compress the table I am using a 32kb block size for my USERS(or default) tablespace 

What other alternatives can I use in order to get the data that's the opposite of my WHERE clause (take note that the where clause in this example has been simplified so I'm not looking for an answer of NOT EXISTS/NOT IN/NOT EQUALS plus those clauses are slower too compared to positive clauses) I have ruled out deletion by partition since the data I need to update and not update can exist in different partitions, as well as TRUNCATE since I'm not updating all of the data, just part of it. Is there some kind of JOIN statement I use with my TABLE1 and TABLE1_TEMPORARY in order to filter out the data that does not need to be updated? I would also like to achieve this using as less REDO/UNDO/LOGGING as possible. Thanks in advance. 

I'm currently using Oracle 11g and let's say I have a table with the following columns (more or less) Table1