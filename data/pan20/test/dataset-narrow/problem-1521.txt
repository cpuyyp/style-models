In Neural networks [3.8] : Conditional random fields - Markov network by Hugo Larochelle it seems to me that a Markov Random Field is a special case of a CRF. However, in the Wikipedia article Markov random field it says: 

The "reasonable" size part might be difficult. As a guidance: If you have only a single node, it is certainly too small for a 1000 class problem. It might be big enough for a 2 - 3 class problem. I would usually suggest to keep the size of the features per layer roughly constant or at most reduce it by 1/10 or triple it. But that is only gut feeling. The reason for my preference for simple models is Occam's razor, the fact that they are often faster, easier to analyze and to manually improve. For more information about topology learning and rules how to design neural networks, see: 

If you have a vector of $n$ numbers $(x_1, \dots, x_n)$ as input, you might decided that the pair-wise multiplication $x_3 \cdot x_{42}$ helps the classification process. Hence you add $x_{n+1} = x_3 \cdot x_{42}$. This is a hand generated feature. In contrast, neural networks learn non-linear combinations of the input. 

I have read that HMMs, Particle Filters and Kalman filters are special cases of dynamic Bayes networks. However, I only know HMMs and I don't see the difference to dynamic Bayes networks. Could somebody please explain? It would be nice if your answer could be similar to the following, but for bayes Networks: Hidden Markov Models A Hidden Markov Model (HMM) is a 5-tuple $\lambda = (S, O, A, B, \Pi)$: 

Source: Wikipedia The most common spaces are $\mathbb{R}$ (the number line), $\mathbb{R}^2$ (the plane) and $\mathbb{R}^3$ (the space we often think in, with $x,y,z$ axes). Lets assume our main space is $\mathbb{R}^3$. Our data are datapoints in this space. However, we have to write a report. This report has to be printed on paper. Hence we project the data points from a 3-dimensional space to a two-dimensional subspace. So a subspace is just a part of the original space. 

Data: It is possible that your data has the wrong type. For example, it might be necessary that your data is of type but actually is an integer. Model: It is also possible that you just created a model which cannot possibly predict what you want. This should be revealed when you try simpler models. Initialization / Optimization: Depending on the model, your initialization and your optimization algorithm might play a crucial role. For beginners who use standard stochastic gradient descent, I would say it is mainly important to initialize the weights randomly (each weight a different value). - see also: this question / answer 

My thoughts Basically, I want to make sure that I didn't forget anything with this question. If you have other sources which explain this kind of thought, please share them with me. (a) Training For training with ADAM, I will now assume that I have a Mini-batch size of $B \in \mathbb{N}$ and $w \in \mathbb{N}$ is the number of parameters of the CNN. Then the memory footprint (the maximum amount of memory I need at any point while training) for a single training pass is: 

While Kasra Manshaei gives a good general answer (+1), I would like to give an easy to understand example. Think of a very simple problem: Fitting a function $f:[0, 1] \rightarrow \mathbb{R}$. To do so, you take a model out of the polynomial class. For the sake of argument, let's say you take a polynomial of degree 0. This models capacity is very limited as it can only fit constants. It will basically guess the mean value (depends on the error function, of course, but keep it simple). So relatively quick you will have a pretty good estimate of what the best parameters for this kind of model are. Your test- and training error will be almost identical, no matter how many examples you add. The problem is not that you don't have enough data, the problem is that your model is not powerful enough: You underfit. So lets go the other way around: Say you have 1000 data points. Knowing a bit of math, you choose a polynomial of degree 999. Now you can fit the training data perfectly. However, your data might just fit the data too perfectly. For example, see (from my blog) 

What do they mean by interleaved? How does that compare to the inception layers? Especially, as the Zeiler&Fergus paper states 

Generalization means you find rules which apply to unseen situations. For example, let's say I have a function $f: \mathbb{R} \rightarrow \mathbb{R}$ and I give you the (input, output) pairs (0, 1), (1, 2), (3, 4), (3.141, 4.141). If you learn by table look-up, you know exactly those 4 tuples. But If I ask you what $f(5)$ is, you have a problem. Because you didn't find the general rule/pattern, but you simply memorized the data. Another example: Imagine you have $n$ data points $(x, y)$ and you decide to fit a polynomial to it. As you know, you can fit any $n$ points (with the x's pairwise different) to a polynomial of degree $n-1$. But if you do that, even the slightest noise or a different unterlying model causes your predictions to be awefully wrong because your polynomial bounces like crazy. 

(edited) answer of /u/mostly_reasonable on reddit The thing to note here is that $F(x)$ can refer to the functioning of more than one layer. The paper's authors use '$H(x)$' to mean something like 'the function we want to learn in some (possibly more than one) consecutive layers of a neural network', see their statement 

I've read a couple of papers about kernel initialization and many papers mention that they use L2 regularization of the kernel (often with $\lambda = 0.0001$). Does anybody do something different than initializing the bias with constant zero and not regularizing it? Kernel initialization papers 

When you want to use Auto-Encoders (AEs) for dimensionality reduction, you usally add a bottleneck layer. This means, for example, you have 1234-dimensional data. You feed this into your AE, and - as it is an AE - you have an output of dimension 1234. However, you might have many layers in that network and one of them has significantly less dimensions. Lets say you have the topology . You train it like this, but you only use the weights from the part. When you get new input, you just feed it into this network. You can see it as a kind of preprocessing. For the later stages, this is a black box. This is manly useful when you have a lot of unlabeled data. It is called Semi Supervised Learning (SSL). 

Adding more data does not always help. However, you can get an estimate if more data will help you by the following procedure: Make a plot. On the $x$-axis is the amount of training examples, starting at one example per class going to wherever you are currently. The $y$-axis shows the error. Now you should add two curves: Training and test error. For low $x$, the training error should be very low (almost 0) and the test error very high. With enough data, they should be "about the same". By plotting those curves you can make an educated guess how much more data will give you how much improvement. 

(Take this pseudo code with a grain of salt. This is just what came directly to my mind; it is not tested.) Then you can calculate the activations in order of . 

The paper Going deeper with convolutions describes GoogleNet which contains the original inception modules: 

Dropout (paper, explanation) sets the output of some neurons to zero. So for a MLP, you could have the following architecture for the Iris flower dataset: 

It seems to be about classification with 2 classes. To understand where the 14 comes from, just try all cases of 4 points being in one of two classes: 

What you are looking for is called Collaborative Filtering / Matrix completion. See my blog post for a short introduction. 

Convolutional Neural Networks (CNNs) use almost always the rectified linear activation function (ReLU): $$f(x) = max(0, x)$$ However, the derivative of this function is $$f'(x) = \begin{cases} 0 &\text{if } x \leq 0\\ 1&\text{otherwise}\end{cases}$$ (ignoring that is not differentiable at $0$, as I think it is done in practice). For inputs > 0 this is fine, but why doesn't it matter that the gradient is 0 at every point < 0? Or does it matter? (Are there publications about this problem?) If a neuron outputs 0 for every sample of the training data, it is basically lost, correct? Its weights will never be adjusted again? 

I am currently reading Boosting the Performance of RBF Networks with Dynamic Decay Adjustment by Michael R. Berthold and Jay Diamond (online) to understand how Dynamic Decay Adjustment (DDA; a constructive trainining algorithm for RBF networks). Doing so, I stumbled over the word prototype a couple of times: 

However, I don't understand the details of step 3: The input weights to hidden units are frozen (indicated by boxes in the paper). When exactly do they get frozen? Are they just initialized by random and never learned at all? I also don't understand this paragraph: 

Linear, binary classifiers can choose either class (but consistently) when the datapoint which is to classify is on the hyperplane. It just depends on how you programmed it. Also, it doesn't really matter. This is very unlikely to happen. In fact, if we had arbitrary precision computing and normal distributed features, there would be a probability of 0 (exactly, not rounded) that this would happen. We have IEEE 754 floats, so the probability is not 0, but still so small that there are much more important factors to worry about. 

SOMs Self-organizing maps (SOMs) are one specific type of neural networks. In contrast to multilayer perceptrons (MLPs; they are used much more often) the SOMs neurons have a position on a regular grid. Training of SOMs SOMs are usually trained in a stochastic way (source: [1]). This means one training example is used at a time. After every single training example the network learns something. In contrast, batch learning means the network is presented the complete batch of all training-examples (or a big part of usually 128 examples; then you call it mini-batch). The parameters are only updated after all of the examples were presented to the network. In contrast to MLPs, you don't apply gradient descent (as far as I know; I've never used them, but only read about SOMs). I wouldn't even know to which function you would apply the gradient descent algorithm, because there is no error function in SOMs. (At least no standard error function). For MLPs, I know that the advantage of batch training compared to stochastic training is that the weights are not jumping as much. So you rather go in the right direction. However, the advantage of stochastic training is that you make updates much more often. You want to get the best compromise, so you make mini-batch training. Online Learning "Online" has quite a lot of different meanings: 

See my masters thesis. Chapter 2.2 and 2.3 for a high-level overview over the building blocks. Appendix D describes a couple of really well known CNN architectures in detail: 

I don't know what you mean by "In multi label classification we have to share the subspace.", but the concept of subspaces is fairly easy to explain. 

A pooling function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a function which is very symmetric: $$f(x_1, x_2, \dots, x_n) = f(x_2, x_1, \dots, x_n) = f(x_n, x_1, \dots, x_2) = \dots$$ So the function does not care about the order of the arguments. This is the case for $\min$, $\max$ and average. In the context of CNNs, you apply the pooling function to each feature map independantly. The output of a pooling layer has exactly as many feature maps as the input. For each feature map, you take a $p \times p$ section and apply the pooling function $f$ to it. Usually, you apply it with a stride $s > 1$ so that the output is much smaller than the input (only $\frac{1}{s^2}$). 

Although I didn't implement it so far, I am pretty sure natural language text vs code snippets is easy: For each block, you make compare the distribution of characters to ground-truth natural language text vs. code. See my paper The WiLI benchmark dataset for written language identification, page 4 "Single-Character Frequency Analysis". 

I thought both, PReLU and Leaky ReLU are $$f(x) = \max(x, \alpha x) \qquad \text{ with } \alpha \in (0, 1)$$ Keras, however, has both functions in the docs. Leaky ReLU Source of LeakyReLU: 

Depends on what you want to achieve. If only getting a better classifier, then you can only add it to the training set. However, if you're doing this in a scientific setting this might be more difficult. (I assume that your test set is of reasonable size). You might want to have a look at cross-validation. 

When should I use machine learning? Machine learning should only be used if there is no reasonable other way: 

I just had a similar issue with a dataset which contains only 115 elements and only one single feature (international airline data). The solution was to scale the data. What I missed in answers so far was the usage of a Pipeline: 

I would expect a very similar (if not exactly the same) result for a decision tree: Given only two features, it finds the optimal feature (and value for that feature) to split the classes. Then, the decision tree does the same for each child considering only the data which arrives in the child. Of course, boosting considers all the data again, but at least in the given sample it leads to exactly the same decision boundary. Could you make an example where a decision tree would have a different decision boundary on the same training set than boosted decision stumps? I have the intuition that boosted decision stumps are less likely to overfit because the base classifier is so simple, but I couldn't exactly pin point why. 

It would work like this: $$softmax(W_3 \cdot \tanh(W_2 \cdot \text{mask}(D, \tanh(W_1 \cdot input\_vector)))$$ with $input\_vector \in \mathbb{R}^{4 \times 1}$, $W_1 \in \mathbb{R}^{50 \times 4}$, $D \in \{0, 1\}^{50 \times 1}$, $W_2 \in \mathbb{R}^{20 \times 50}$, $W_3 \in \mathbb{R}^{20 \times 3}$ (ignoring biases for the sake of simplictiy). With $D = (d)_{ij}$ and $$d_{ij} \sim B(1, p=0.5)$$ where the $\text{mask}(D, M)$ operation multiplies $D$ point-wise with $M$ (see Hadamard product). Hence we just sample the matrix $D$ each time and thus the dropout becomes a multiplication of a node with 0. But for CNNs, it is not clear to me what exactly is dropped out. I can see three possibilities: