When you add the MAC address source filtering to IP source guard using the command, the client MAC address is not learned until it has obtained a DHCP lease. Until then, all non-IP traffic except for DHCP will also be blocked. Since the switch won't learn the MAC address of the client until after it obtains a lease, this leads to a problem of how does the switch know where to send the return DHCP traffic from the server to the client? The information inserted by DHCP option 82 gives this information to the switch. However you do need to make sure your DHCP server then also supports option 82. 

You don't say which switch you are replacing nor specify the type of transceiver it has (you only specify the connector). However, the only 24 port Cisco switches I know of personally that have 100Mbps MT-RJ ports utilize 100base-FX, but make sure you double check. 

VLAN tagging is not required to logically divide a switch into different LANs. VLAN tagging is only necessary when sending data from more than one VLAN on the switch to another switch or device over a single link. 

This may not take up much bandwidth, and while normally a network device will drop traffic not addressed to itself, this is not always the case. On this network, Computer 15 has been compromised and it now watching all the traffic it receives and now is able to also "download" the XML file containing the patient list as well. To protect against this, you now require that all internal traffic be encrypted or protected by some means. This takes resources to develop, resources to verify everything is secure, and resources on the end points themselves to encrypt/decrypt everything. So, back to your original question, "Why doesn't it always do this if it can, since it would be faster than doing a look up in a table?" Most switches utilize specialized hardware that does these look ups very, very quickly. This isn't measured in microseconds, but rather in nanoseconds (and won't reach the double digits at that). It is also not tied to CPU or memory resources. Compare this to the amount of processing required by each station as it inspects and drops all the unnecessary traffic (also typically measured in nanoseconds, but it needs to be done by each device and not just the switch). Or the time it takes to time out and re-transmit a frame when the network is congested (which may be measured in seconds). Or the time it takes to establish some sort of encryption between each set of two devices that want to communicate in a secure fashion (which can easily introduce microseconds of delay). In addition, this may require additional traffic (such as a SSL/TLS exchange, etc). Flooding all traffic is simply unfeasible on a network with more than a handful of devices. 

That should be caught by the default dos-protection-group. No? Do you have, perhaps, 'unset ike dos-protection' ? What does 'show suspicious-control-flow-detection ike' show? 

Since you have control of the network, (you do have admin control of the network gear; otherwise your Q is off-topic here), just look at the ARP caches on the switch(es) and router(s). Any system that is actively using the network will be known to your network gear, even if it wanted to "hide" from discovery/probing from other workstations/systems/programs. 

Each ISP can control the end-to-end experience (the speed, latency) by selectively applying Quality of service (QOS) and routing decisions based on the traffic source/destinatoin. So an ISP could provide premium (higher QOS, over higher cost/faster/fewer-hops links) service to/from service providers that pay a premium. So if Netflix pays, and gets the premium, but (for example) Hulu didn't... customers would start thinking "Hulu sucks. Netflix if fast!" when in reality, the invisible-to-the-user ISP service is affecting performance. The big issue people have (I think) is that it means that (for example) the big players (e.g. Netflix) can spend big money for premium ISP through-carry and make it hard/impossible for small players (netflix startup competitor that might come some day) to get the same level of service to the end users. To date, the Internet has been one big level playing field "in the middle"; A "neutral network." (...and see Ryan Foley's answer which has more technical clarity than my attempt.) 

There is a very important distinction between the Internet, and your own LAN, (or even WAN circuits.) On the Internet, 1500 is the max MTU. (End of discussion.) On your own LAN, (or in some cases WAN too) you can do whatever you want. As technologies expanded, 1500 MTU was no longer needed. So things like jumbo frames came into use. 

From OSPF commands, (IOS 12 is linked) I'm suspicious that the default (in that doc) is 240 though, maybe this isn't the right knob... :/ 

Create a general NAT access list for inside traffic to internet and remote network (with permit and deny rules) Create a specific NAT access list to match router-originated traffic Add outside matcher to tunnel interface Create 2 nat rules, 1 for each access list in #1 and #2 above. 

Finally got it to work, after much playing around. The high level steps to complete were as follows: 

So I have modified the 3 occurrences of public IPs with the private IP (based on $URL$ but the tunnel does not come up: 

I initially thought that the error might be related to this bug: $URL$ but firmware is a fixed version: 

I can confirm that the preshared secret is configured correctly, I downloaded the configuration from AWS and copied it into the Cisco config. Is the setup correct? Any ideas as to what might be wrong? I am not sure if I am meant to be using the private IP in all 3 locations (, and ), or should 1 of them reference the public IP (I've tried using it in and but it didn't work either. Before providing a public / NAT / private setup, the ISP used to provide an L2TP tunnel where the public static IP was on the router, and that worked fine. However the L2TP tunnel is no longer available (but this router / software worked with AWS in that environment) 

I have a generic ADSL router that supports dual bridge and pppoe mode. I have an ADSL account configured on the ADSL router, and have the same ADSL account configured as a pppoe connection on a cisco via 1 of the ethernet ports - so I can change the default route of hosts on the network to compare connections. When using the connection via the generic router, I have stable transfers. However, internet usage on the Cisco is inconsistent and jitterish. For example, if I try upload a 2mb file via HTTP to a service (e.g. AWS S3), it seems to upload in bursts, and will sometimes fail. Is there anything obvious in my config below that may be slowing down the connection? 

to me, that implies that "discontinuity" is related to bit-size-integer-rollover. So if you see the sysTime change, you know that counters that are now showing smaller-than-last-time-checked values have, in fact, wrapped around, as if you had rebooted. 

You want a Faraday Cage. Connect your aluminum foil box to an electrical ground, and that will prevent cellular signals from passing in/out of the box. 

I think you're confused about the usual uses of static- and dynamic NAT. Static is used on the outside router/firewall to expose a server/service located inward which wouldn't normally be reachable from the outside. (static: nailed in place, permanently mapped inward.) Dynamic is used to enable inward transportation of return traffic for outgoing connections; so a web browser on a system with a private IP is dynamically mapped to a public address as the packets egress, and then the return traffic is mapped back for ingress. (dynamic: the inside end points change so the mappings are changing too.) 

Your question is VERY broad, nearly unanswerable. You need to begin by getting some metrics so you can figure out what's going on. Start by buying a solid, commercial grade switch so you can see metrics/usage/errors, etc information. You could have a loop (as suggested in the other answer), a broadcast storm, bad cables, wonky router, nearly anything... 

This is an insanely broad question, but an outline is possible. Here's what I'd do if I was thrust into this situation. 

TL;DR: If you're a consumer/SOHO nothing needs doing. If you're providing services, start planning now while there's no rush and no deadline. Basically, everything is dual stack these days, meaning it supports IPv4 and IPv6 out of the box. The Internet in general is still operating fully supporting IPv4 as much as possible. At this point, if you're providing services to the Internet -- running DNS, web, email, etc. services. You should be planning to provide those services as both IPv4 and IPv6. If you're consuming services from the Internet -- small office/home office, larger offices, residential, etc. then there is currently no urgency to convert yourself to IPv6. I suspect as the bigger cable companies start deploying IPv6 the "last mile." Then we'll see more IPv6 traffic in general. Today, there are a lot of really big things -- Amazon, some of the content delivery networks -- which are only IPv4. So the focus, as I see it, is for the providers to continue working to provide evertyhing in IPv4 and IPv6 simultaneously. The first problem the Internet is going to face is fracturing/islands appearing as the newest areas, where IPv4 addresses are already exhausted, come online as IPv6-only. Those newest areas will need the existing services and service providers to offer stuff over IPv6. 

Source guard will now validate both the source MAC address and the source IP address before allowing traffic to enter the interface. Naturally we don't want to configure every client statically, so how can a switch allow client devices dynamically to allow them to send traffic? Well, an exception to dropping all traffic is that the interface will allow DHCP traffic only from a newly connected client. Once the client gets a lease from the DHCP server, the DHCP snooping binding table is update and also updates the IP source binding table. Source guard then has both the MAC and IP adresses now to validate any other types of traffic. However, allowing even DHCP traffic to trigger the switch to learn the MAC address creates a small vector of attack that could be exploited. A compromised device could send out DHCP traffic for a number of MAC addresses or targeted MAC addresses to get the switch to update the MAC address table in error. So while there is an exception to allow DHCP traffic, the switch still will not learn the MAC address until there is a non-DHCP data frame/packet transmitted by the client and accepted by the switch. So this now brings us full circle to the first question and why the insertion of option 82 is necessary in the DHCP exchange. 

PC1 is connected to the port first, it is given access and all traffic (L2 & L3) is allowed because it has a static port security configuration. The MAC address of PC1 is learned by the switch and inserted into the MAC address table. We then disconnect PC1 and connect PC2. PC2 is given access and all traffic (L2 & L3) is allowed because port security on this interface allows two MAC address. One is statically configured for PC1, but no other static configurations exist, so PC2's MAC address is configured dynamically. The MAC address of PC2 is also learned by the switch and inserted into the MAC address table. We now disconnect PC2 and immediately connect PC3. Because we have an aging time of 60 minutes configured, the interface will retain the MAC address of PC2 until 60 minutes have passed. So PC3 is not given access and all traffic is dropped (L2 & L3) as the interface already has two configured MAC address (one static for PC1 and one dyncamic for PC2). The MAC address of PC3 is not learned by the switch nor is it inserted into the MAC address table because all of it's traffic is being dropped (so there is no traffic from which to learn a source MAC address). In the second example, let's use the same PCs, assume DHCP snooping is configured correctly, and only have IP source guard enabled on the interface as below: 

Ouch. But ok, I can think of two ways I'd go at this... Eyeball it: If the switches have port indicators, you should be able to eyeball which ports are the most active. Those are the ones to start looking at first. Hopefully the cables are labeled so you can search for the low hanging fruit of finding two busy ports, on two switches with the same cable. SNMP monitoring: If you've got SNMP (or similar) usage stats, look for the busiest switch and the busiest ports. Then go looking at cables. ...if you have unlabeled cables, start tracing and labeling as part of your checking out the busiest ports. 

(I want to point out that others have posted excellent answers about how the delays et al work and what causes them. But the OP asked about modeling; A basic model is simple and you just plug in example numbers. If you want to know why the delays are what they are, then see everyone else's answers :^) The network latency is simply the transit time from one end point across to the other end point, spanning N hops between. So you have N segments (hops) with N-1 intermediate nodes. Each node has a delay (the cumulative effect of several things on that node, like queue delay, processing delays, etc), and each segment has a transit delay. Overall that's 2N - 1 independent variables. So it's seg1 +node1 +seg2 ... +node(N-1) +segN One hop, is just =seg1 , two hopes is seg1 +node1 +seg2, etc. Next you have to define what all those pieces are. So you might construct a model network with a CATV network, a satellite link, a fiber optic link, an ethernet, etc. For each of those technologies you have to look up example information. The transit delays would be approximately the data size divided by the transmission speed of the segment. If you need a more accurate model, you have add the flight time lag -- approximately the length of the segment, divided by the speed of data flow (approximate the speed of light.) This DOES matter if you have a satellite link involved; The up-and-down to the the geosynchronous satellite is significant. The delays on each node you will have to estimate based on what equipment you are placing into your model. If you want the application latency, (for example the delay until the start of an FTP transfer's data flow,) then you build up by counting how many times your network latency comes into play. For example, a 3-way TCP handshake adds triple-the network latency, and so on building up to what the application sees.