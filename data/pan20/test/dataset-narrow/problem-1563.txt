So we see that in fact our algorithm is performing quite well. We can then plot our points to see how it is classifying them. I will plot the training points as small circles, and the testing points as larger ones. The dark points are those which are misclassified 

Many classifiers do not directly give you L or R. They will give you the option which has a higher decision metric. For Naive Bayes this would be the class with the higher probability between $p(L|x)$ and $p(R|x)$. This is true for most classifiers. What you can do is set a hyper-parameter (if you have some anomalous instances) to determine how well contained within a class a certain novel example must be in order to be classified as pertaining to that class. For example, if $p(L|x)=0.01$ and $p(R|x)=0.05$ then we can reject both hypotheses and say the novel example $x$ belongs to neither class. You should look further into anomaly detection algorithms, you can also use p-value through the GLRT to test if your prediction falls enough within the class such that it can be classified as such. For example, set you p-value, $p = 0.05$, if the p-value of a given example falls below this threshold then do not attribute a class to it and flag it as anomalous. There's A LOT of different ways this can be done, including using some clustering or NN techniques. 

You can also use more rudimentary anomaly detection techniques such as a generalized likelihood ratio test. But, this is kind of old-school. 

You can use a deep learning network to generate sentiment analysis. Pretrained models can be found which have been trained using vectorized word lists which are commonly associated with positivity and negativity. However, these may not generalize well to your specific use case. Due to the historic nature of stocks you can easily compile your own database from online sources. For example, you can construct instances in your dataset as follows. Tokenize all the words found in articles pertaining to Google for February 5th 2001, then the target label can be the stock value for February 7th 2001. You can choose the time lag as you please. If you do this for a large amount of instances, your dataset should be able to contain some notion of what words found in articles will cause a stock price to rise or lower. 

This results in an error of $0.7406$. Of course, this result is kind of meaningless because you should split your data into a training and testing set in order to accurately test your results. You should do the following 

Projecting data naively can lead to problems so instead you can use a feature embedding method. Here I will give an example for 4 different methods: Isomap, MDS, spectral embedding and TSNE (my favorite). This is continuous data that I have access to but you can easily do the same for clustered data. Just set the labels $y$ as your determined clusters. 

I have a solution however I use a densely connected layer at the output to simplify the reshaping. If you can manipulate the sizes of this model such that you have 4 output parameters this should work as well. 

MemoryError is exactly what it means, you have run out of memory in your RAM for your code to execute. When this error occurs it is likely because you have loaded the entire data into memory. For large datasets you will want to use batch processing. Instead of loading your entire dataset into memory you should keep your data in your hard drive and access it in batches. If you are using Keras there is a helper class with a very efficient implementation of batch processing. Take a look at this blog post. This is a good starting point for avoiding MemoryError. 

Data pre-processing and feature extraction This and feature extraction are the two most important parts of a machine learning technique. That's right, NOT THE MODEL. If you have good features then even a very simple model will get amazing results. Data pre-processing goes from your raw data and remolds it to be better suited to machine learning algorithms. This means pulling out important statistics from your data or converting your data into other formats in order to it being more representative. For example if you are using a technique which is sensitive to range, then you should normalize all your features. If you are using text data you should build word vectors. There are countless ways pre-processing and feature extraction can be implemented. Then, you want to use feature selection. From all the information you extracted from your data not all of it will be useful. There are machine learning algorithms such as: PCA, LDA, cross-correlation, etc. Which will select the features that are the most representative and ignore the rest. In your case First, let's consider the data pre-processing. You notice that type might not be an integer value. This may cause problems when using most machine learning algorithms. You will want to bin these different types and map them onto numbers. Feature selection: besides using the techniques I outlined above, you should also notice that productID is a useless feature. It should for sure NOT be included in your model. It will just confuse the model and sway it. As a general rule of thumb, the amount of data that is suggested to have for shallow machine learning models is $10 \times \#features$. So you are limited by the size of your dataset. Also, make sure the outputs are quite well distributed. If you have some skew in your dataset, like a lot of examples where the item was sold right away, then the model will learn this tendency. You do not want this. The Model Now that you have your feature-space, which might be entirely different from the original columns you posted, it is time to choose a model. You are trying to estimate the time of sale for an algorithm. Thus, this can be done in two different ways. Either as a classifier problem or as a regression problem. The classifier problem would separate the different times of sales into distinct bins. For example 

This creates an array between $0$ and $1$. This is the different probabilities we will assume to calculate our likelihood. For example if we use 5 we have $[0, 0.25, 0.5, 0.75, 1]$. So we will assume that $P(H)$ is equal to these values in turn and get the resulting likelihood for each value. 

A generative adversarial network is comprised of two parts which are necessary for the training, the generator and the discriminator. They are playing a game against each other, the discriminator tried to learn what is real and what is fake, and the generator tries to create fake instances which will fool the discriminator. At the input of the discriminator you will use real images, for example the MNIST numbers. At the input of the generator you will put just random noise vectors. These vectors will then be shaped by the layers of the generator to create some image, hopefully resembling the MNIST dataset. If it fools the discriminator, it wins that round. Otherwise, it will tune its weights using backpropagation in order to produce better quality results in the next round. Once training is complete, you will detach the discriminator and use only the generator. Then for every single random noise vector fed to the input of the generator you will receive an associated image. Here is a GaN implementation. You will need to put a path "./gan/images/" i the directory where you are running this GaN. 

From here you can do last record of datetime minus the first record. And get the time it took for it to get to the emergency. 

This is actually a pretty complex question that has to do with many facets of how information entropy is carried in a dataset and how your learning algorithms will make use of the information. Machine Learning is Optimization Machine learning at its core is just finding the ideal parameter values in order for a specific function to be represented. This means you will use the data in your dataset and some framework in order to tune these parameters. Learning is an iterative process, as you can see providing one instance to your algorithm will not result in very good results. However, there comes a point when you have provided sufficient data that the information contained in that subset is representative of the information contained in the entire dataset. You can see this in practice, when training your model you will notice that performance starts to converge to a specific value. If you had infinite data this value would not get any better. This is caused by limitations in your data and not your model (machine learning algorithm). This is the best the specific model you are using can do with that kind of data. How to get better performance? Assuming you still have more data but performance is no longer improving, you will want to consider a more complex model. This model will likely be able to extract information from your dataset that is too subtle for your simpler model. This is why deep learning outperforms most classical machine learning algorithms when big data is available. Deep learning makes use of very high complexity to approximate functions very accurately, however, the tradeoff is that a lot of data is needed. Actually my dataset is quite limited, can I improve my performance? You actually can! This goes back to the core of machine learning, the dataset you are using. The dataset you have constructed has a certain information entropy based on the features you selected. By increasing the information entropy in your dataset, the complexity of your model can be reduced and this will lead to better performance. Intuitively, if you are trying to classify dog/cat, but your dataset contains information about the weather the day the measurements were taken, this will reduce the entropy of the information for classifying dog/cat, thus obscuring your other features and reducing performance. Furthermore, sometimes you may overlook the importance of a certain feature, or overlooking the potential of performing transformations to your features to better suit your model. For example, if you are using linear regression to determine points on 2 circles (radius=1, radius=4). Performing linear regression on the Cartesian plane will lead to very poor results, essentially 50%. However, transforming your $x, y$ coordinates into polar coordinates will lead to 100% performance as they will be perfectly separable. This is a simple application of the kernel trick. 

This is quite standard for the training time. It depends on how much optimization you did on your code. The speed of your processing unit, it's often better to use a GPU as opposed to a CPU. GPUs do mathematical operations much faster. Also, you should use parallel computing when you can, in the case of NN you definitely can. Training a machine learning algorithm only needs to be done once. Let it run all night and then you will be ready to do some pretty good predictions. 

There is only one small difference between gradient descent and stochastic gradient descent. Gradient descent calculates the gradient based on the loss function calculated across all training instances, whereas stochastic gradient descent calculates the gradient based on the loss in batches. Both of these techniques are used to find optimal parameters for a model. Let us try to implement SGD on this 2D dataset. 

n-grams is a feature extraction technique for language based data. It segments the Strings such that roots of words can be found, ignoring verb endings, pluralities etc... The segmentation works as follows: The String: Hello World 2-gram: "He", "el", "ll", "lo", "o ", " W", "Wo", "or", "rl", "ld" 3-gram: "Hel", "ell", "llo", "lo ", "o W", " Wo", "Wor", "orl", "rld" 4-gram: "Hell", "ello", "llo ", "lo W", "o Wo", " Wor", "Worl", "orld" Thus in your example, if we use 4-grams, truncations of the word Hello would appear to be the same. And this similarity would be captured by your features. 

Then we initialize our weights, there are many strategies to do this. For simplicity I will set them all to 1 however setting the initial weights randomly is probably better in order to be able to use multiple restarts. 

In your code the Area Under the Curve (AUC) is used to calculate the area under the Cumulative Distribution Function (CDF). Let's go through the code to see how this is done. However, in the last section, I do not agree with the two lines used to calculate the area. If all the labels have a ground truth label of 1, and we have a perfect classifier where the probability of being Class 1 is always 100%, we should have an AUC of 1. However, we will get 50%. I will show this later. You should note that the way this code is written will only work for a binary classifier, with labels 0 and 1. We will first calculate the probability of each instance being in class 0 and class 1 by 

Ok, this is a very exciting problem. Quite different than the ones we usually see on this site. And, even more exciting is that I think that one of the oldest methods in the book is what you are truly looking for. So from what I gather, you have the time that a user is active per day as your data and you want to determine times when they log on that deviates away from their usual activity. Ex: User 1, active from 9:00am to 15:00pm. This is often called anomaly detection. You are trying to detect when a behavior from a specific user is anomalous (different) than their normal. So how is this done? Anomaly detection algorithms are capable of learning the distribution a single set of labels (normal activity) and then it will be able to flag when an anomaly occurs (anomalous activity). This is when an instance is sufficiently different than the learned distribution. To get the baseline distribution in your case it is very easy which makes it quite exciting! All you need to do is collect information about the user during a burn-in period. Here you need to make a choice. Do you want to track the users activity daily, or throughout the week, the month? etc... If we are talking about activity in a server that is used for work, weekly would likely be better. However, if it's a gaming server then maybe daily is better. You need to think about the regularity of the users schedule in regards to the kind of server you are hosting. Let's assume you are interested in their daily activity. Then you can have a burn-in period of a few weeks. So set up a probability graph which will be your probability distribution function. The $x$-axis is going to be the hours of the day, and the $y$-axis will be the frequency (you will need this to be normalized afterwards for the math to work out). So, each time the user logs on, you will add onto the plot a box unit 1 ($y$-axis) for that time slot. This is essentially a histogram!! At the end you will end up with a curve that shows the likelihood of the user being active for any given hour of the day. Now, if you see that a user logs on at a time $X$, you can use the probability density function to get the probability of the event having been drawn from that distribution. In other words, what is the likelihood that the user would be online at a time $X$. If this likelihood is below a threshold $\alpha$ then you are safe to assume that it is anomalous. This is more commonly known as the generalized likelihood ratio test (GLRT). The disadvantage of this technique is that you need to train the hyper-parameter $\alpha$, your threshold. Which is easy to do given you have labeled data. You know what behavior is normal/anomalous. If you don't have this information. You can just set an arbitrary threshold and fix it as you see fit based on the resulting false positive or false negative rates you will encounter. I am suggesting this because you can easily get the parametrization of your probability distribution. So if you do not use it you are missing out on a lot of information. Another non-parametric way to do anomaly detection would be to use a specialized K-NN method which makes use of the $p$-statistic test in a very clever way. I have found a lot of success on very limited datasets using their methods. The paper you should start with is: Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs $URL$ And then you can go onto their simplest implementation, which you can then expand on by reading their other papers (bagging, ranking, etc...) Anomaly detection with score functions based on nearest neighbor graphs $URL$