This is an old question, but I came across this and was appalled at one particular comment made, suggesting the original asked was "confused". The question is perfectly clear, just not strictly about database administration. It falls into the fields of server virtualization and storage provisioning in virtual environments more than anything. It may well be that this question had been better suited for a different StackExchange site, but arrogantly dismissing the question because one does not understand the issue is unacceptable. Let me attempt to explain the question better and give my best advice on the subject, in an attempt to leave something useful here for any others that may get dropped here by a search engine. When deploying a database server, it has been considered best practice to put the OS, data files and log files on separate sets of disks. Just as a random example, let's say we have a server consisting of three RAID arrays: 2 HDDs in RAID 1 for the OS 4 HDDs in RAID 10 for the database data files 4 HDDs in RAID 10 for the database log files This setup would separate the IO and create separate points of failure for the data files and logs, as well as keep it separate from the OS. This was done for performance, resilience and maintenance reasons. Simple enough, right? But what then, if we are deploying a database server in a virtual environment? There are no physical disks in a virtual environment. No there really aren't, unless you are mapping the virtual machine disk 1:1 to a physical disk, which is not what you'll see done in your typical virtual server farm. Even then, there would be a virtualization layer in between the physical disk and the virtual server, presenting the physical disk as attached to a virtual disk controller rather than the physical controller the disk is actually attached to. So what is the problem? Let's take a small virtual server deployment as an example. A bunch of rack servers running in a virtualization cluster and one SAN with a bunch of disk groups in RAID 5 and RAID 6 with automatic storage tiering. All virtual disks are stored in the SAN in this example. How do you go about assigning separate physical disk groups to your virtual database server now? Or maybe you don't? That was the question asked here. No useful answers were given, and understandably so, since it's a complex question where the answer depends a lot on your particular deployment. Some setups might give you very tight control over what kind of and what specific physical disks your virtual disks end up on, whereas others might be more of a black box that handles everything automagically. In the simple "one-SAN" example I outlined above, I'd say that you don't really need to. But you might still want to. Even if you can't actually control what physical disks the IO ends up at, there are other benefits to splitting the data up. What if you migrate to a new virtual environment in the future, where you do get separate LUNs with different and known performance characteristics? If the data are already split over several virtual disks, moving them to new LUNs with the appropriate IO capability is much easier. In many hypervisors you can also give different IO priority to virtual disks. Again, this gives you some extra control. More accurate control over snapshot behaviour also becomes possible. Each database gets it's own separate underlying file system, that it doesn't need to share with other databases on the same server or even the OS itself. Basically, this boils down to your performance requirements, size of the database, hypervisor and storage solutions at your disposal and many, many other factors. I hope I've pointed out a couple of useful tips. Plan ahead. While virtual environments with SAN/NAS solutions might seem like black boxes in many ways, many of them do have ways to achieve the same performance and redundancy goals as traditional direct storage. 

I thought I had this syntax nailed down, but I am getting an error of Incorrect syntax near ) when I attempt to execute. Why is my syntax not correct? 

My query returns roughly 590 rows and 8 columns. The issue that I have is that from start to finish the query takes 2 minutes 30 seconds to complete. The great group of people on here have taught me a lot on how to write more efficient queries, so here is another! I am using a variable and not a since my variable only contains a date - and I am also using storing my date(s) in the format as suggested by Aaron Bertrand - Bad Habits To Kick. Is there anything I can do to optimize this query and have the results return faster? 

I am in need of a way to return all records from both tables BUT have a where condition on I have tried both queries below, however Query 1 ignores any row from @TTBL that does not exist in ZT1, while Query 2 ignores the WHERE condition I want to put on the table ZT1 (bc it was changed to an or) How should my query be written in order to accomplish my desired result of all data from @TTBL AND only data from ZT1 that fall between the date criteria 

I am wanting to perform the below query that essentially shows where a calculation is >= .70 or <= -.70 However, I get an error of 

I am attempting to run a dynamic SQL statement and it works for any that does not have an apostrophe in it. However, when the has an apostrophe in it the syntax does not properly, however I am stuck using dynamic SQL in this instance. How should I ammend this syntax in order for to include an apastrophe? Here is DDL 

I am wanting to alter my table to have a column that is the SUM() of two other fields. I have this syntax 

I have a SQL Table that we are selling off some of the data, so I need a quick way of splitting the main table into sub tables every 50 rows. Table nomenclature does not matter, I am just stuck on a way of selecting 50 different rows for each table with the table not holding a primary key or numeric field. How would one go about this in SQL Server 2008? 

I know that typically you would have an autonumber field - but in this instance there is not, and it honestly does not matter which row get's updated as long as it is only 1. I have sample DDL like such one table that contains requested qty, and one table that contains avaliable qty. I want to update with the value from and join on but only update the 1 record in How is this achieved in SQL Server 2008 

This is sample DDL and the query using Row_Number() I attempted. What would be the proper way to get the result I am after? I am actually wanting to delete the duplicates, so that each storeID only has 1 entry for each Hambasa. 

Also, it is possible to set timestamp per session. is used to always get timestamp in UTC, no matter what MySQL server's timestamp is configured to. 

MySQL is not cutting it at 233. The problem is likely in your save method which cuts it to 233 before the data even reaches MySQL. Also, don't forget that 233 limit is not character limit, and as some character might need more han 1 byte to be stored, you might see less than 233 characters. Also please make sure that data in MySQL server is really stored as latin1, this can be accomplished with: 

MySQL queries are not case-sensitive by default. It is possible that you have created case sensitive tables when importing data. Check if you have collation, that makes it case-sensitive. Reimport your data then using . Also, if you have collation, it will make queries case sensitive. Your collations changed when re-importing data. 

What I would do, is compare both files with tool. can come in help here as it has ability to diff word by word and if you pass it option don't even have to place them in git repository. This command can help 

Otherwise you will have to drop the table (with the command above) and recreate it manually or from a backup. 

When you are copying these files without detaching the database first, you are risking to corrupt your backup in the event that will be synchronized during the copy procedure thus rendering the backup broken. 

For inserts, you can use . This lets you update certain fields if primary key is already used. The syntax would be something like: 

After starting query log, investigate the file (or table) for further information. MySQL Query Log Documentation: $URL$ 

This is likely because your User has status on your system. Users without administrator access should not be able to access these files. If you enable Guest User, you can login and try same thing with Guest. You should not get access to the files as guest or normal user without administrator rights. 

Yes this is normal. When RAM is no longer needed it is not freed at the same time. It is kept as cached in case the server would decide that it needs to access it again. This would save you extra time that you would otherwise need for data to appear in RAM. Cached memory is freed only when new applications request more RAM. 

Secondary indexes (non-primary keys) in MongoDB and MySQL are very similar. Secondary indexes declare fields or columns to be sorted separate from the rest of the data, and use row identifiers to reference the rest of the row for a query. 

I would use for this. - $URL$ - The script imports only a small part of the huge dump and restarts itself. The next session starts where the last was stopped. 

While you seem to have fixed the issue, I will quickly explain why it happened in case anyone finding this will want to understand where the problem was. When setting foreign keys, the Primary Keys Columns must be of exact same type and attributes. E.g. If you have unsigned attribute on one primary key, you must have it on another. If you have INT data type on one column, then another column must also be INT (NOT TINYINT, MEDIUMINT etc.). As you have only one ID set to unsigned, I would go and set it to all IDs. As it is usually good idea to have unsigned attribute on primary keys (if you do not use negative IDs), I would have changed all IDs to have unsigned attribute, as it will improve your query performance. Also, take a look at what values you can get with various integers (when they are unsigned). WHat you set your lenght to - does not matter: