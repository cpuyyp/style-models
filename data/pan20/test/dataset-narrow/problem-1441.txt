I think incorporating a physics engine such as JigLibX (link) would yield better results and serve you better in the long run. The problem with most applications like this isn't collision detection, but collision response, and a physics engine really makes life much easier in that department. 

Otherwise, add the origin translation to the camera class, so that you don't need to compensate outside. 

Otherwise your scene graph will only work for children one level deep. 2) Don't translate the Z component In your matrix you're doing . I'm not sure but I think using a different from might cause problems because the parameter called is the one responsible for controlling the depth of the sprite, and this might mess that up. 3) You could also add the origin into the transform matrix This one is optional ,but if you incorporate the into your matrix (by doing ) and leave it as on the call, it can make other tasks easier to implement because you don't have to account for the origins separately. For example, you could check if the mouse is intersecting the oriented bounding rectangle of the display object simply by multiplying it by the inverse and then seeing if the result is within the to range. 

The reason why your draw order is messed up is probably because your depth buffer is not being used. With the depth buffer enabled the order in which you draw your meshes makes no difference. And since you've just stated that you were using a SpriteBatch component, it pretty much confirms that this is the problem: $URL$ Quoting that article: 

I always move the camera and would recommend it - there are many reasons why I prefer this, but the most significant probably boils down to this: 

You can find a way to describe any of the subclasses in very concrete terms (e.g. an ellipse has a center and two radius, a polygon is a collection of vertices, the others are specializations of the previous cases). On the other hand, a shape is an abstract concept in the sense that it can refer to any of the other cases, but you can't find a concrete definition for it. In sum, ask yourself the question: 

Long Answer A class will typically manage a view and projection matrix. But you usually rely on helper methods from your API to create these matrices, such as or . While it might be possible, I don't think it would be very practical to deduce the view and projection matrices from the frustum. So I'd forget about building the camera from the frustum. As for how to build the , I like the way that XNA handles it, which is basically by providing a constructor that takes a combined view-projection matrix and deduces everything else from there. This way there's no direct dependency between the frustum and your camera implementation. All it cares about it receiving some view-projection matrix from somewhere. Finally, as for whether one class should contain the other, my opinion is that, if anything, your camera class could manage a frustum object and keep it in sync. I think doing it the other way around would be a bad design choice, because the frustum is a lower level concept than the camera, and the frustum shouldn't have to rely on a specific camera implementation. Or the simple argument that composition should be used to implement type of relationships, and it doesn't make sense to say that a . 

The Battle Mechanics FAQ also has a lot of useful information, in particular about time management in the battles. But unfortunately this system (aka ATB or Active Time Battle) is patented so you can't make anything similar. EDIT I also recently found this website which provides a lot of technical information about the implementation of FF7. Unfortunately the battle module sections does not seem to be completely written yet. 

This creates a new action group (an entire line in the image above) and adds it to the manager. All of the groups are executed in parallel, but actions within each group are chained together so that the second one only starts after the first one finishes. When the last action in a group finishes, the group is destroyed. Problem Now I need to replicate this information across a network, so that in a multiplayer session, all players see the same thing. Serializing the individual actions is not the problem. But I'm an absolute beginner when it comes to networking and I have a few questions. I think for the sake of simplicity in this discussion we can abstract the action manager component to being simply: 

You're doing a lot of and all over the place. The problem with this approach is that you'll be able to see the characters being drawn one by one, and even at a fast rate such as 16 updates per second, you will still be able to notice a lot of flickering because of this. In a graphics application this is what happens when you don't use double buffering - you see the image being rendered instead of just seeing the complete picture. The solution is simple, render everything to a buffer, and after everything is done, write the complete buffer to the console in one go at the end. In your other question I explained how to do that, so I'll copy from there with a few optimizations: 

You can implement this concept quite literally by tracing rays from your player tile and intersecting them with your scene. You break from each iteration once the ray hits an obstacle (or exceeds a certain distance threshold) since you're only interested on the tiles the player can see directly. I'll break up the process for you: 

Using this strategy you could have the turret be a child of the tank, and whenever you rotated and moved the tank, the turret would follow automatically. 

You should be setting your effect properties before applying the pass, not after! That could also be the source of the problem if you're rendering other objects before and after the board. And move all the assignments that you're currently doing outside the loop. There's no meaning in them being inside the loop if the values are the same in every pass. This one is optional. Since the only uses one pass you can also simplify your rendering code by replacing the loop with a simple: 

Optionally you could do this last calculation with a vector instead. For instance, if the angle is indeed 45 degrees, you could do this last step as something like: 

I'd actually like to relax the algorithm so that it returns true in all of these cases. In other words, it should return true for points that are strictly inside, for the vertices themselves, and for points on the edges of the polygon. If possible I'd also like to give it enough tolerance so that it always tend towards "true" in face of floating point fluctuations. For example, I have another method, that given a line segment and a point, returns the closest location on the line segment to the given point. Currently, given any point outside the polygon and one of its edges, there are cases where the result is categorized as being inside by the method above, while other points are considered outside. I'd like to give it enough tolerance so that it always returns true in this situation. The way I've currently solved the problem is an hack, which consists of using an external library to inflate the polygon by a few pixels, and performing the tests on the inflated polygon, but I'd really like to replace this with a proper solution. 

This is a good function because the X value varies from 0 to 1 and then back to 0 again (which we'll be mapping to our sun's start and end X values) and the Y value starts at 0 and moves up to 1 and back to 0 again (which would be our day portion) and then repeats the exact same thing on the negative side before coming back to the original position (which would be our night although the sun will not be drawn at this point). The first step is scaling the hours from the [0..24) range to the range of our function which is [0..2PI):