Inevitably both will get updated in contradictory ways in between updates, and all hell will break loose...Or at least you'll get unpredictable behaviour. Imagine a case where two people update the same record in different ways. Which one is right? In a Master<->Master environment, there is no right. Both records are equally correct. So it'll go through and apply them sequentially, causing a data inconsistency. If you're using mysql, you also tend to end up with problems from row-level locking...Those rows often don't replicate. And you have to do an ugly hack to get around the problem of duplicate autonumber fields (server1 auto increments odd 1,3,5 and server2 auto increments even 2,4,6). It's just not pretty. 

You'll be using pkgadd instead of apt? You're not going to run into any huge gotchas. If anything, you may have a better experience, seeing how much of what you're installing is Sun software (apparently, all of it). The problems you can have with proprietary Unix are most notable when you move outside the proprietary comfort zone. If you like non-mainstream OSS, you're going to have to learn to love to compile from source, because Solaris binaries may not be available. Otherwise, what? Different gui? Sun hardware is solid and elegant. Support is responsive like you can't even imagine if you're used to Dell, et al. Solaris is stable and exhaustively tested. Imho, it's not worth the price for anything but a critical application, but if you've got it, it's very enjoyable. 

The classic question! Power vs Redundancy! I tend to come down on the side of redundancy; with modern hardware clustering, you can take any number decent machines and cluster them into one very nice machine. Add a SAN, connect them all to it, and you've got the best of both worlds. Or just share the drives in your local machines, depending on your storage needs. 

Not in my experience; your network bandwidth is always going to be lower than your internal hard drive bandwidth, so you're going to take a significant performance hit. 

The 0.0.0.0 block is reserved for the "default route", for routing purposes. Inversely 1.1.1.1 and 255.255.255.255 is used to signify broadcasting on all routes. The 169.254.0.0 -169.254.255.255 block is reserved for Automatic Private IP Addressing There are a number of other addresses that are reserved for INCAN, ARIN, etc. A partial list can be found here. 

Hah. Came in here to tell you I'd struck out, unless you were (for some reason) using Microsoft VM instead of VMWare. Microsoft puts the name of the physical host in the registry of the virtual machine under: 

Sounds like a terrible jury rig. Firewalls are designed to block certain machines, and let certain others through. Just give your managers static IPs, and allow access from those IPs, and your problem is solved. Sonicwall has a CFS exclusion list for this exact purpose. Of course, your managers are probably also going to use Facebook, once they have free access, but that's life. You'd be better off allowing everyone and monitoring their usage. If it gets to be a problem, fire them. Blocking policies aren't much of a solution, and they tend to make people bitter. 

You need to be more specific about what you need it to do. ICT means very different things for different types of institutions (Cloud, however, means nothing in the way you've used it: "intranet Cloud" is like saying "internal Internet". Cloud services are, by definition, not hosted locally. If it's local it's a "cluster".) Just in a very general case, I'd suggest going to Sourceforge, and looking under "Education". There is an "Administration" section that includes a number of OSS projects that will probably do some of the things you need done. From personal experience, you're going to find that they're not really supported, and that they're pretty customized. Still, it's a lot better than starting from scratch, and you may get lucky and find that there is a package that is being actively maintained (some kind of yearly class project? Who knows.) 

Check out Webmin for a nice web-driven remote management interface. It has module add-ons for a number of monitoring utilities (like Heartbeat, for example, as well as other multi-server monitoring tools). You can also link any number of servers running webmin into one panel, to put all your server monitoring in one place. For users, you probably want to set up something like OpenLDAP or a Samba domain, rather than copying users from one machine to the next. OpenLDAP can be a pain, but nothing like trying to sync users across multiple machines. 

Change the default port number. Forbid root logons. Force protocol 2 (assuming that it's not done by default). Whitelist the servers that are allowed to SSH in. 

Well if they're really only hitting you once per ip address, for all different pages, then there really isn't anything you can do but trim down the 404 page...Or you could just redirect 'em to the front page and declare it as a page view...;) 

It's not a big deal. In terms of processing power it's less than a lot of standard URL rewrite rules. Unless all your pages are pure html, you'll see much higher process costs on the scripting engines. 

I wrote up a big post, and I swear I posted it, and yet...Hmmm. You're going to want to do some kind of archiving: it will make your life so much easier. I recommend 7zip; it's a free zip utility that has a powerful command line interface. Very easy to write a 1-line 7zip command to archive and compress a whole directory tree, and it also supports the "update" option for archives, so instead of wrapping up every file every time, it can just update the changed files...Should save a ton of time. Once you've got your archive, you'll want to send it somewhere. I recommend WinSCP; it's a free FTP/SFTP client that also has good command line/scripting support. The scripting interface there is quite powerful. Put those two together, and you can write a simple DOS batch file to run the necessary commands. Quick, simple, and effective. Try to avoid the temptation to reinvent the wheel. That's a great lesson to learn. There are people out there who have already spent a great deal of time putting together pieces that you can use. 

Way more trouble than it's worth. One problem that is worth thinking about is the effect that venting the heat is going to have on your air conditioning. Keeping the cabinet cool will mean running a lot of air into it, most of which is going straight up to the attic. You can lose a lot more cool air that way than would otherwise be warmed up by the servers. You're probably going to want to experiment with heat exchanging until you find something that's good enough for the servers, and still doesn't suck out all your AC. 

I agree with Chris S. You're too exploited. You need to wipe and restore from backup. And this time, before you go live, you need to be extremely careful with your write and execute permissions. Once an attacker has obtained system-level access, you can't trust your code anymore. Directory permissions are HUGE. This cannot be stressed enough. They uploaded code to your site via an exploit, but that can only be done if your code can write to the local directories. If it can't, or if the local directories that it CAN write to can't be interpreted or used to host executable code, then the damage that can be done is extremely limited. I recommend removing write permissions everywhere you can, ALL OF THEM, even the ones belonging to root. The only things that should be writable anyway are upload directories and whatever directory stores your session files. If you don't allow uploads, then only the session file directory, and that one should be as locked down as you can make it. You should also run regular file integrity scans. Unfortunately, that's not as easy if you don't have complete access to the server. Still you can download the site and compare it to your backup on a regular basis. Ideally, you should be able to overwrite the entire site from backup at any time, and have no one notice the difference. 

Makes perfect sense. Well, far as I can see it should work: I use similar directives all the time. I'd check all the directory permissions and ownership, and the file permissions, and if those are all right, I'd check for a rogue .htaccess file or some other piece of lock-downery that may be tripping you up. If Apache's document root is /var/www/html, then you shouldn't have any trouble pointing at something in /var/www/html/includes/ unless there is some specific directive (or permission issue) blocking it. 

Root are the top ones. We're talking ICANN, or some other agency at the top level of the internet. There are servers that are responsible for directing your queries based on the TLD (e.g. ".com" goes to x.x.x.x, ".org" goes to y.y.y.y), and then those point you to specific authoritative name servers (foo.com). Authoritative are the ones that answer for specific zones. If I have foo.com, and I run the foo.com nameserver for all the machines on foo.com, that is the authoritative nameserver for foo.com. The root servers will tell you to ask that server, if you're looking for server.foo.com. Local servers could be anything. Could be an authoritative server for your local domain, or it could simply be a caching name server that stores addresses that it's previously looked up. 

If you're worrying about your dns/registrar instead of your 4 redundant data centers, your 8 redundant ISPs (two different pipes at each datacenter), and your trans-continental clustering failover hardware/software solution, you're looking in the wrong place. Six nines is basically impossible. You can be down for no more than 30 seconds each year. 30 seconds! If it takes you 10 minutes to failover after a catastrophe, you've blown your 6 nines average for TWENTY YEARS. Chances are, you're not going to be spending enough to make 6 nines realistic. It costs more than it's worth. 

I'm more bitter about the vanity urls. I had a chance to buy my first name in 1994, and I thought to myself, "Eh, 40 bucks is a lot of money." By the college conversion of 40 bucks = 24 pack of crappy beer, it WAS a lot of money. Now it's running $25,000 and is nothing but a goddamn squatter site, just like every other 4-letter combination. Oh the bitterness. 

They both work fine for me. Might try adding Google's DNS servers (8.8.8.8) as secondary or tertiary DNS in your system. 

I usually only do new domains for geographically separated business units, for the first reason echobeach2 mentioned: you want local admins to be able to admin their local domains, but you don't want them to have admin rights to ALL domains in the forest. It's also a pain to have AD replication constantly running across a VPN tunnel, or whatever. Otherwise, the fewer the better. 

Setting up SSL on Apache Setting up SSL for IIS You'll probably also want to get a registered SSL certificate: most modern web browsers throw an error if you use a "self-signed" certificate. 

It used to be a lot more common, back in the early days of the internet, to have a large external block of IPs. My own company just finished migrating off a /16 block. I used to work at a university that had two /8 blocks. But these days, it's very uncommon for a company to not use a NAT. Big external IP blocks just aren't that useful. My current company, (the one that just moved off the /16), set up an internal WAN on 10.0.0.0/8, and that allows for every computer in the company to have a "unique" IP (though, admittedly, not an externally accessible one), at a much lower cost than we paid for our original block. 

if you want to make the route change permanent, add the -p flag. The metric doesn't really matter very much in this context, it's more of a statement of priority, so the system knows which route to choose, all other things being equal. 

The easiest way is to scan with : nmap will usually correctly determine if a machine is a printer or not based on the OS. 

Looks fine...You don't want to be tooling along flatlined: that means you've paid too much for your hardware. And if it's topping out at 50%, then you've got plenty of spare capacity. The only thing I'd worry about is that Tuesday spike...Looks like you handled it fine, but spiky traffic is the worst to plan for. What kind of site is it? 

Mod_proxy absolutely works...Most of the proxy tags can be easily nested inside a VirtualHost directive, which makes it very easy to separate out your different domains. If you've got a bunch of them though, I'd definitely recommend using Squid. Mod_proxy is nice, but its a proxy service bolted onto a webserver, and, as such, not as capable as a dedicated proxy like Squid. 

You can use cpulimit. I usually just stick with nice though: if it's running by itself, I don't really care if it's taking up 100% 

The first three can be done by modifying /etc/sshd_config The fourth depends on which firewall software you're using. 

Well, IMHO the best practice (for individual ports/protocols) is to filter EVERYTHING, and then selectively allow things. That will knock out a ton of stuff, and force your users to justify why they need X, Y, or Z. Web filtering will have to be done via some sort of content filter (like Websence or similar), and those can be configured to death to block pretty much anything you want to block. I personally hate content filters, but I can afford to be sanguine about them, since these days I'm always high enough on the tech totem pole to circumvent them. It's never a popular step, but it makes sense from both a security and a utilization standpoint. 

Apt usually uses good old port 80, but you should check apt.conf to see what it is set as, since it can use any of several. Also check /etc/apt/sources.list 

I tend to add the gateway to the /etc/sysconfig/network-scripts/ifcfg-ethX config file for the appropriate interface, eg: 

You can configure logrotate to rotate a file via copy-truncate, which preserves the original file and doesn't require you to re-create the log file. The syntax is: 

Shouldn't need another switch: that's a tiny network. I actually run internet for 500 people through a 4 port procurve switch. Your router is much more likely to be an issue, though, again, 12 people is pretty low. Since you have a 100mb router and a 1gb switch, I'd make sure that those ports are talking to each other correctly. If possible, I'd set the port on the router and the port on the switch to autonegotiate (in my experience procurves aren't happy unless they're set to auto). If you can't set them to auto, better set them both to 100mb. If they're NOT talking correctly you can lose 80% of your bandwidth easily. 

Eh. If your server is IP locked, and your user is restricted to SELECT on a set of tables where you don't care about the information, it's not a huge deal. On the other hand, I set my MySQL passwords by whacking on the keyboard for a minute, and copy pasting the resulting gibberish into a protected file, which I reference in my code whenever I need to log in. This is how it should work. Why make it easy? If the password is attached to a limited local account (as they all should be), then why are you typing it in? If it's not, it should have a password whose strength is relative to the value of the data you're protecting.