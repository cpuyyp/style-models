With luck you can work it out from these. It's hard to say more or give a definitive answer as it stands, because there are patterns where you haven't said what it should do. For example, trailing backslashes can be present sometimes, as this is valid and optional if the input happens to be a folder, and could cause the regex to be mis-evaluated for some folders. Ditto for UNC shares such as \\?\D:\share\myfile.txt or \\server\share\myfile.txt, and ditto for paths with just one slash (c:\myfile.text). Ditto for illegal characters (could the file contain anything that isn't a valid filename, such as semicolon, or will you test this separately if it could happen?). If you clarify more exactly what the regex should do, or if you need an explanation of how these work, I'll update this answer and add a regex or more details. 

I'm guessing it's objecting to the "ada6" and I should be providing some other identifier, or a slice/partition ID instead. But I don't have these; zfs creates them itself when it attaches the disk. What is the correct command to use here, or what am I missing? 

Didn't really believe it was DOA first time. Definitely don't believe it's a DOA now. But if not, then what is it, and what can I be missing? I've tested everything (AFAIK) in the component chain, the HBA just doesn't have much OROM interface that can go wrong, or any options to recognise SATA/SAS/both, or anything like that, and the main PC/server in both cases just leave detection to the HBA/RAID card. I've tested on two completely different platforms, with two different models of controller cards, with SAS vs SATA drives, and I'm utterly stumped. (Note: I'm slightly limited as I'm starting to transition from SATA to SAS, with the intent being to replace SATA by SAS as they wear out, so at the moment I don't have any other SAS disks or cables to test with, which I would otherwise have done too. But I think I've probably covered that by testing the cards+cables while varying SATA/SAS) Updated for more accurate title to help others, now more info obtained. See answer. 

I am trying to be clear about how zfs dedup handles the case where some (but not all) datasets in a pool are deduped, from a dedup table/RAM impact perspective. I found this quote from the FreeBSD mailing list in 2012: 

I'm virtualising a small setup of half a dozen mixed PCs and small servers. The aim is to consolidate hardware resources (one high spec server + thin clients rather than 8 medium-to-high spec individual devices which aren't all used at the same time), to enhance mobility and session start-stop-suspend-move, to allow resource sharing (inactive machines can be suspended and the resources used for other things, rather than having dedicated PCs left idle, also the occasional heavy workload can be "averaged out" across VMs and doesn't needed every machine able to handle it), and to allow session snapshots on all devices. I've been trialling on a small scale using VMware Workstation for a year or so to see if the approach benefits the setup and it very clearly does, enough to move to a VM server more fully. Everything is straightforward, but my confusion is about the video handling aspects and how they interact (VDI, SVGA capable video cards, PCoIP/RDP, and pros+cons between the more generalist ESXi vs the more specialist View/Horizon for the desktops). This area is completely confusing me and holding me back. USAGE AND REST OF NETWORK - The active VMs at any given time would be a mix of about 3-5 Windows desktops and a mix of 3-4 small internal *nix servers (shell tinkering, tiny radius server, etc). The desktops are mostly used for desktop and "productivity" work on Windows 8.1/10 (heavy duty multitasking on Office suite, browsing, coding/development, video viewing, small amount of Photoshop now and then), but the desktop "windowing" use can be intense and multitasked. Most modern software can also use 2D hardware rendering if available to offload desktop GUI compositing and controls in applications. The servers are all light load *nix. There's a separate robust file server + offsite replication in place with enough capacity/hardware spec to support 1 or more VM servers, and 10G LAN for the file server/VM server link. My focus for this question is the graphics handling aspect. I'm reluctant to rely purely on soft (CPU) desktop and graphics handling due to the excessive CPU load it imposes even for moderate use, so I'd like to plan and spec for a bit beyond that. I want some flexibility in video resource sharing, and the usage varies, so if CPU alone isn't enough, I'm really looking to a VSGA or similar style solution, not dedicated-card-per-VM passthrough solutions. My question is borne of ignorance, openly admitted. I don't know what options make sense to consider in relation to graphics/desktop use. Virtualization is usually discussed in terms of a single purpose and on a larger scale, rather than a heterogeneous mix like this is. My points of confusion are things like these - 

Apparently the correct answer was "talk to LSI, get told nothing will be done; talk to Intel, get told it can be replaced under warranty, send off 910 SSD, receive top-of-line P3700 in return" :) The answer above got the key point though which was to speak to Intel 

I'm planning a small ESXi standalone server to host a number of VMs I use here. I haven't decided what facilities to use from the wider vSphere system. The underlying VM storage is local HDD RAID using enterprise HDDs and LSI megaraid, with the LSI card's onboard battery backed ram +ssd caching systems enabled. My concern relates to data corruption and bit rot in the VM store over time - I don't really know what my options are, and I'd like to be sure that stored VMDKs and snapshots, and other VM files, don't get corrupted over time and can be set to be periodically scrubbed and any bit-level corruption (within reason) detected and repaired. As background, for casual desktop use, I've tended to use RAID 1 (mirroring) rather than higher levels (reasons: fast read speeds, complete portability of drives without tie-in to specific brands or cards, no disruption if a drive fails). For my file server I use ZFS on a mirrored volume. But ESXi and VMware's suite use their own data store design for local storage. So I don't know how resilient against silent corruption, a setup would be "out of the box", especially when it holds many TBs of large files that might sometimes only be accessed years later, and with a local store rather than a dedicated separate storage system. I also gather VMFS uses a journaled filing system but not one with the self correcting capability of ZFS. Are the inbuilt capabilities of ESXi (and if necessary other parts of their suite) sufficient to protect against routine data corruption concerns? If not, what are my options for peace of mind? Update @mzhaase - I didn't feel confident about passing through to a second server that would act as a file store, because then every file access and snapshot has to be done remotely across a LAN or a second device and even if 10G was used (which is still cost prohibitive for most home setups) the slowdown would be a major concern. Part of the whole reason for getting this specific card is to get true cache-on-write for speed, so that bulk writes or rollbacks are less likely to slow everything down by "chugging" the main HDDs, which should be helpful whatever the file store location. Issues with latency impact sound like they would also happen with any remote data store, whethet a server appliance or a home build such as a second FreeNAS box (although if I had to choose, I'd use a second FreeNAS). I'm perhaps overlooking using a dedicated NIC port and multiple parallel 1Gb ports as a way round this, but latency and traffic implications for snapshots and rollbacks are a big concern. I'm also possibly overlooking running a FreeNAS VM on a small dedicated disk, which services the main VM store array off the raid card as a passthrough device, which keeps it local. (Meaning that ESXi can boot and can load the FreeNAS VM off one disk, once that's running it can act as a ZFS based file server for any other VMS with - hopefully - low latency). But running the file server virtualized might increase latency more than keeping it local would reduce it, and latency and disk bottlenecks are already an issue I'm trying to overcome. I will however look up the LSI card info, and - can you install file integrity checking/repairing software on the underlying ESXi platform itself to check and repair VM files? I didn't know that. And would iSCSI be that much of a latency-killer to make a remote store usable? Once a VM is up and running, how much does access speed/latency to the VM store affect the running of ESXi or other VMs currently running on it?