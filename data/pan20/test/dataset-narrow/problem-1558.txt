Discrete is the way to go. The reason is simple if you visualize a decision tree it involves drawing a decision boundary based on a set of constraints which are in the form of features. It would be much easier to draw these decision boundaries based on discrete features compared to its continuous counterpart. If the values are continuous it becomes difficult for the classifier to effectively draw this boundary and might have some skew in its results. There is a useful video series in Udacity, Intro into Machine learning, please refer the decision tree section they show a really good visualization on how decision trees work. Link: $URL$ Please check that out it may help you understand better. 

Some of the features which are being used now can't be used later because they might not be known. Will it affect the model? If they can be used, what type of algorithm can be chosen? 

Can I say this about your dataset? Say a well is a point on your map, so if each well has an x-axis and y-axis coordinates. So effectively I've converted your dataset into an 11-dimensional problem. Now let's talk prediction. First, perform PCA on your dataset with the number of components = 1.Remember the dataset which you have to perform PCA is on all the data but the X-axis and Y-axis values Then plot your data set in a 3D scatter plot with the other two dimensions being X-axis and Y-axis and the third being the new Dimensionally reduced values done by PCA. Look at the scatter plot if the following data follows any type of regression trend. Then use that type of regression to predict the values. 

Which looks basically like a diagonal line (it is), and by some easy geometry, you can see that the of such a model would be (height and base are both 1). Similarly, if you predict a random assortment of 0's and 1's, let's say 90% 1's, you could get the point , which again falls along that diagonal line. Now comes the interesting part. What if we weren't only predicting 0's and 1's? What if instead we wanted to say that, theoretically we were going to set a cutoff, above which every result was a 1, and below which every result were a 0. This would mean that at the extremes you get the original situation where you have all 0's and all 1's (at a cutoff of 0 and 1 respectively), but also a series of intermediate states that fall within the graph that contains your . In practice you get something like this: 

To clarify, I feel like the original question references by OP probably isn't be best for a SO-type format, but I will certainly represent in this particular case. Let me just start by saying that regardless of your data size, shouldn't be your limiting factor. In fact, there are just a couple main issues that you're going to run into dealing with large datasets: 

You can try something like SMOTE and see how your newly generated data fits your requirements. If your data has a statistical model you can use an appropriate parametric model to generate data. You can even try methods like Non Parametric estimation such as Parzen windows etc. All of this depends on the statistical fit of your image data which you have processed so far. Other Methods similar to SMOTE: ADASYN, Boundary SMOTE etc (Look them up on the internet) Note: Apply SMOTE on the already processed image data such as the training data you already have not directly on the image itself. 

Neural Networks basically act as a high memory-based machine learning algorithm. So for a given dataset the chance of it perfectly aligned with all the data at a given instance is high, as it most likely just ends up remembering every data point you give. Overfitting occurs precisely because of this, when a new expansive data set is introduced there is no way it can adjust its fit to the new data, graphically it ends up missing more of the values than its supposed to fit. In conclusion, it does not work well in the case where the scoring population is significantly different compared to training sample. 

The results you're seeing aren't a byproduct of your training product, but rather that are not a great choice for this task. are effectively a means to create a high order non-linear function by composing a number of simpler functions. This is often a really good thing, because it allows neural nets to fit very complex patterns. However, in a stock exchange any complex pattern, when traded upon will quickly decay. Detecting a complicated pattern will generally not generate useful results, because it is typically complex patterns in the short term. Additionally, depending on the metric you choose, there are a number of ways of performing well that actually won't pay off in investing (such as just predicting the last value in your example). In addition the stock market is startlingly chaotic which can result in a overfitting. This means that the patterns it learns will generalize poorly. Something along the lines of just seeing a stock decrease over a day and uniformly deciding that the stock will always decrease just because it was seen on a relatively short term. Instead techniques like and , which will identify more general, less complex patterns, do better. The winner of a similar Kaggle competition used for this very reason. You are likely to see better results if you switch to a shallow learning model that will find functions of a lower polynomial order, over the deep complex functions of a neural net. 

The question is poorly phrased, I've tried to edit it to the best of my abilities. However here are the problems you've stated, 

First and foremost you need to know the difference between the type of data you are trying to predict. The two general categories are discrete and continuous. Most people tend to miss out that classification is at its core a discrete "regression". The values are predicted for discrete variables by considering a decision-based approach which ultimately leads to finding or predicting the variable in question. You can call this classification. For continuous variables, however, since the values can have a certain range it most closely mirrors a curve fit with the addition of an error boundary. I think this should clarify what you're essentially trying to ask. 

There is very little information in this question. I will try to answer this in the most generic sense. Let's start by defining Noise. Noise here as you probably know is unwanted data. Any data which you are not looking for while evaluating a problem or scenario can be considered as noise. Examples for amplifying noise: Amplifying noise might occur in cases and scenarios where there is a small data set and you are trying to supersample the dataset or another example could be while working with waveforms. In order to detect weaker signals. Disadvantages of Amplifying noise The biggest disadvantage of amplifying noise from a data science perspective is that the model used to perform various operations on the data such as Regression, Classification etc will be less efficient. For example having noise based on supersampling in Classification may affect the model. If we were to use decision trees for classification you might create a bias in the algorithm which just pertains to noise while training. So your accuracy for classification also decreases. Similarly, in regression when you train with noise you might choose a wrong model because the noise alters the goodness of the fit. 

Again, can't say what this means for the future, but these are just a couple of relevant points when it comes to evaluating in my opinion. 

So basically, what you're actually getting when you do an over accuracy is something that will strongly discourage people going for models that are representative, but not discriminative, as this will only actually select for models that achieve false positive and true positive rates that are significantly above random chance, which is not guaranteed for accuracy. 

I personally have used for a good number of professional projects, and while, as Dirk mentioned, this is purely conjecture, I can give some insights on where Julia really stands out. The question of whether or not these reasons will prove enough to have succeed as a language is anyone's guess. 

There's a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.) Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches. 

The following image shows a scatter plot of my data. The Y axis points are the labels, labeled from 1 to 6 and X-axis are dimensionally reduced values of all my features. I reduced them for better visualization. 

Let's start by answering your first question. Is it required to balance the dataset? Absolutely, the reason is simple in failing to do so you end up with algorithmic bias. This means that if you train your classifier without balancing the classifier has a high chance of favoring one of the classes with the most examples. This is especially the case with boosted trees. Even normal decision trees, in general, have the same effect. So it is always important to balance the dataset Now let's discuss the three different scenarios placed. Choice A): This would be what I explained all along. I'm not saying necessarily you will have a bias. It depends on the dataset itself. If the nature of the dataset has a very fine distinction with the boundaries then the chance of misclassification is reduced, you might get a decent result but it's still not recommended. Also if the data does not have good boundaries then the rate of misclassification rises a lot. Choice B): Since you are placing weights for each sample you are trying to overcome the bias with a penalty. This is also called as an Asymmetric method. Normally these methods increase the accuracy of a model by a slight margin but that mostly depends on the machine learning algorithm you are using. In examples like Adaboost such a model the effectivity of the model increases. This method is also called Asymmetric Adaboost. But this might not necessarily work with all algorithms. Choice C): Assuming you have weighted the samples accordingly it should do the same as either choice A or choice B. I'll leave this for you to extrapolate based on my previous explanations. 

Really great question, and one that I find that most people don't really understand on an intuitive level. is in fact often predicted over accuracy for binary classification for a number of different reasons. First though, let's talk about exactly what is. Honestly, for being one of the most widely used efficacy metrics, it's surprisingly obtuse to figure out exactly how works. stands for , which curve you ask? Well that would be the curve. stands for Receiver Operating Characteristic, which is actually slightly non-intuitive. The implicit goal of is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class. A great example is in spam detection. Generally spam data sets are STRONGLY biased towards ham, or not-spam. If your data set is 90% ham, you can get a pretty damn good accuracy by just saying that every single email is ham, which is obviously something that indicates a non-ideal classifier. Let's start with a couple of metrics that are a little more useful for us, specifically the true positive rate () and the false positive rate (): 

I would recommend training on more balanced subsets of your data. Training random forest on sets of randomly selected positive example with a similar number of negative samples. In particular if the discriminative features exhibit a lot of variance this will be fairly effective and avoid over-fitting. However in stratification it is important to find balance as over-fitting can become a problem regardless. I would suggest seeing how the model does with the whole data set then progressively increasing the ratio of positive to negative samples approaching an even ratio, and selecting for the one that maximizes your performance metric on some representative hold out data. This paper seems fairly relevant $URL$ it talks about a which more heavily penalizes misclassification of the minority class. 

Now if it is equal to the median there is no problem in choosing whichever value but in real time it's most likely to follow the other two cases. The reason the mean is greater or lesser than the median, in this case, is because it skews to either higher computational time or lower computational time. What this skew represents is you could say where the most likely frequency of computational times would be. This implies it gives a more normalized approach and it shows where you can expect the computational times of most of your execution runs expected to be. So choose mean over median. 

You should choose mean or average over median. Let me explain why, specifically in your case since you're checking for expected computational time. Mean or average could be one of the three cases. 

Should we use PCA in machine learning algorithms more often? Well, that strictly depends, using PCA reduces the accuracy of your data set so unless you need to save up some space caused due to a lot of features with bad correlation and the overall accuracy doesn't matter. If your machine learning model scenario is similar to this then it is ok to proceed. However, most use of PCA is as you asked before for visualizing higher dimensionality data to determine the data trend and to check which model fits best.