Exploitation - Well defined work that can be easily divided into well-defined stages, where each stage can be learned and mastered on its own and handover between stages does not require communication. Exploration - Undefined work, which requires learning and experimentation to accomplish each stage and handover between stages requires massive amounts of communication of all learning and status of the project. 

One of the people active in ChatOps community is Jason Hand from VictorOps, who wrote a book published by O'Reilly called Managing Operations in Group Chat. This book is available for free as part of their content marketing campaign. The previous ChatOps for Dummies is no longer available and is replaced by the above book. You should definitely take a look at Slack Apps, especially the Developer Tools. There is article by SlackStorm on how to integrate Ansible and Slack. You might also monitor the subreddit on chatops. There is a long sidebar of resources including the video from GitHub ChatOps presentation and many more. I would also recommend to look at Cog from operable.io and the Cog Book. 

This is a brilliant idea, also because of Daniel Kahneman who showed that if you split a single score into 5 weighed scores and add numerical criteria and bounds to those, you will significantly reduce bias. You could design not just the resume scoring, but the entire hiring process, with phone screens, onsite interviews, everything in this way. It would significantly reduce the inherent bias of the interviewers. We have actually started to do something similar for all hiring. Obviously, inside each area, your should add weight to what is important to the company for the position, but you are hiring a well-rounded engineer and you want someone who will propose major changes to how your organization operates, you are not simply hiring someone for specific skills to work in a limited area. Many people simply see this role as a higher paid Release and Build Engineer and if that is the case, that is what you should hire and advertise for. For a DevOps hire, I would suggest replacing the Lean with Learning. It is originally CAMS and even though some extend it to CALMS to include Lean, that is somewhat restricting as DevOps is based on so much more than just Lean. It is also Deming's ideas about Special and Common Cause Variation and System Thinking, Nash's Equilibrium (if each optimizes for themselves, the result could be suboptimal, compared to when everyone includes the interest of the group), Shewhart's Statistical Process Control, Goldratt's Theory of Constraints, Taleb's Anti-Fragility and so many more. This would also allow you to include participation in conferences in Learning and presentations at conferences or meetups as Sharing. In a position where you are not always a part of a team or your company might not be big enough to have your peers as your co-workers, it is even so much more important to establish and maintain out of workplace relationships and learning opportunities. We generally grouped those two under Culture. I would personally put under Culture the soft skills required to be effective in improving processes at your organization. CMMI, Kanban, Work in Progress limits, Agile practices, etc. JIRA seems more like Sharing tool and Git is more closely related to Automation. 

In Atlassian Bamboo, in script tasks you can use special Bamboo environment variables e.g. for current build number 

Two example integrations of Docker and Kubernetes are OpenShift and Rancher Labs AFAIK. Sadly but in fact we have not completely escaped us from the dependency hell. Question: is there an established source of information which distributions here package which versions (like Ubutu/Debian version chronicles on Wikipedia)? Background. Not-so-obvious facts for newbies (judged by my learnings so far) are (defining acceptable technical usability to a level that you do no need hours to debug even 101 tutorials): 

Sure you can say "how important is that" but somewhere it gets irrational. SLA offers rational approach. 

As this question is quite high-level, I give a high level answer. Technically, if there is an incident, you would normally enforce a shutdown of a running container and a replacement with a newer image with an implementation of the change request resulting from the incident. How much automation will be possible to get there, I think it really dependends on single cases or at best their classes. Best is therefore I think to assess different scenarios, group them into classes and derive solution workflows. Once a problem catalogue and a required set of organization policies are there you will be able to sort out which tools can help to realize them and which automations grades are possible. For example: 

In my case, the catch was that I imported the certificate via the context menu, and therefore it went to another folder where Docker could not access it. The certificate store behaves however very much like the normal Windows Explorer tree folder, so I had just to copy the certificate over to the supported category, and after Docker restart it worked. 

Source code version control system Build agents Automation and packaging scheduling system to encapsulate artefact composition logic A binary repository (different classes of binaries: libraries, distros, container images) A configuration repository for different environments Package and configuration distribution system to encapsulate deployment logic 

This would be then inconsistent versioning. Is there a better/established way than some custom scripting to control sync of this? 

UPD. Strange enough, the very first image layer which always gets stuck before it downloads anything stucks rarely at "downloading... 987B". UPD2. Same behaviour with Docker EE client (Docker-Client/17.03.0-ee-1) UPD3. Filed this as a bug to the Moby project. 

What is the best practice either reference value in terms of an SLA spec to expect the infrastructure to: 

In first place I look for default JIRA functionality e.g. without 3rd party plugins to simlify and improve the issue workflow. Examples: 

Correct me if I am wrong. Then, typically, my assumption/observation larger companies (much larger than a s startup, much smaller than Google) would provision you with some SDDC resources and there you can play with IaC. In such a case which I consider yet not very rare, does this setup actually make sense if you want high performance? An SDDC solution would have some sort of scheduler, and your IaC CPUs are virtual CPUs on real CPUs, so in most pessimistic scenarios your Spark shares resources with say corporate mail server or some backup solution. And there can be priorities set between them, so the sharing is not equal. And collision cases described below from random towards a notorious DOS (Denial of Service) bias. Collision case 1. Then your Docker Swarm, or Kubernetes scheduler would "see" a machine and say, well, the load there was not so much, so I take it - and then it finds what if was not expecting because the SDDC system scheduler has decided to take it already for something else. Collision case 2. Just in the moment the IaC scheduer decides to compute something, the SDDC scheduler decides to unschedule it, so what would happen is kind of paralyzing spikes setting IaC to a slow motion mode. 

TL;DR: Chaos Monkey was developed in 2010 at Netflix and released into wild in 2012 is part of the Simian Army, wildly popular among devoted followers. Built on principles of chaos engineering, the army increases resiliency to failure by injecting constant failure to the system. Concept Chaos Monkey was developed specifically for AWS where it will randomly kill instances within an Auto Scaling Group. It is meant to run during the business hours when engineers are alert and can quickly react to discovered failures. Simian Army Members of the army would sow chaos through other means: 

Of course I was notified at 2am and it took me until morning to get it up and running and everything configured and tuned up, but I'm afraid it is not going to be as good as before. It might take weeks before it is back to it's former glory. Now my uptime is gone, I don't have even measly three 9s and who knows what this will do to my reputation. Who is this Chaos Monkey and why did he do that to my server and why is he trying to ruin me? 

The tension between development and operations is often caused by misalignment of incentives and attempts to optimize within the team. Developers are often judged by the speed and quantity of issues they can get through and merged into code repository and their reward is often not tied to that code actually working or working correctly. Much less scaling, performance and other factors. Operations are often judged on the stability of the environment and how well the code works in production, but rarely on the quality of the process for bringing changes in quickly. This creates the issue where developers are incentivized to create a lot of code and throw it over the wall to the operations team and the operations team has incentive to accept as little change as possible to ensure the stability of the environment. DevOps is in a way the set of solutions to this problem: 

Solution to this problem is to create an overlay filesytem over a read-only mount, but if you try to do it directly, overlay will refuse to put upper and work directories on another overlay filesystem. The trick is to create a tmpfs for the upper and work directories like this: Create a script called run-in-c.sh 

Correct files or packages need to move to the server. Configuration and service states need to change. 

When developing code on a very large repository, I would like to run multiple builds/tests in parallel in containers. I need to get my repository or some other large directory into the container. If I mount it read/write, the parallel processes will fight each other and I will need to be able to clean after them, there is also issue with privilege escalations on those files. If I mount it read/only then I cannot run processes that write files inside the directory. Docker suggest to build a new container with a COPY of the repository, save it as new layer and then start several containers from this new image. Having to make a copy of megabytes of data every time you want to run tests is making it take too long though. How can I solve this efficiently? 

Docker daemon allows, according to documentatio, using memory storage driver for its images. Has anybody any daily business experiences with that? 

Every CI tool defines more or less "their" view on how integration and delivery can work, and every DevOps team (re)invents their specific way to support the business using these tools. But, how could you to define and execute high-level business logic? Here are two examples from daily work: 

Imagine you would run a bunch of services say based on the same Tomcat image. Would Docker optimize memory usage for identical memory segments and reuse these patterns? 

Strange enough node.js unlikely maven installs the packages the software needs inside the project directory. Is there a way to introduce a central repository which also caches different versions of components and would allow to pull them on build like we know it from other ecosystems? 

Today, you could more or less setup a CI/CD toolchain by youself, even locally. Indeed I have seen teams doing it in an ITSM environment because the communication with management did not work. Nevertherless we know CI/CD is a very important, if not most important, company production line - it produces value, even if it is not seen so by ITSM to give it operations status and priority (in short, SLA for systems which should survive project time instead of setting them up in every project and department over and over). Let's ignore for simplicity network downloads. Question: what should be the execution speed of CI/CD relative to a setup on a local machine (if possible): 

Using Consul, you could allow your teams decentrally deploy and run microservices with dynamic dependencies. It is also possible to list the services: Now, how to assess these dependencies automatically if the architecture grows dynamically to get a visual representation of service inter-dependencies? 

Now, to save space I could take the statement... for a GZ arhive. But does not support automatic extraction of ZIP files. So the will be dangling around somewhere in a below layer. Just a statement does not help. How do you solve this? 

I have configured Git to use a specific CA certificate, but how can opt-out for it other than using the switch ? (or enforce other certificate which seems better in terms of security). Okay, a workaround was to create a shell script for a shortcut and use it like 

Both Docker Registry and Docker Engine have API interfaces. I consider it should be possible to implement a Docker plugin for this integration to have images scanned for example as somebody pushes them. (side note - Docker registry is an image registry, not container registry). This scanner functionality is also possible through the commercial Docker Enterprise edition. $URL$ UPD here an example with clair/docker, looks quite simple $URL$ 

Recently the Amazon S3 had a major outage in the us-east-1 region. It looks like it was likely caused by a spelling error when running a maintenance playbook in Ansible or a similar tool. You can put a shell script wrapper around ansible-playbook to look like: 

You need help figuring out what are the changes your company needs to go through on your journey to DevOps. You know what you need and just look to hire people to implement this vision for you. 

The startup stage (under 150 engineers). Of course, the developers need to run their own software. Everyone does so in a startup. You might not even have operations team to begin with, but even if you do, it is small and the speed of progress is so fast that there is no way to pass the knowledge required to run it successfully on another team quickly enough for them to be successful. The medium sized business (over 150 engineers, but a single operations team). At this stage the churn in the company starts to be too high, the engineers who build software do not necessarily stick around also to run it. You don't know everyone anymore and it is hard to communicate effectively for everyone to be in operations. It would start to turn into chaos. At this stage you want to turn to the Google model, where every team has to operationalize their software, but not necessarily operate it. They will operate it at the start, but a big part of building software is to reduce the cost of operating it to a point, where the load is small enough that the operations team can sign on running it. Only then it is considered done. Large enterprise with multiple business units, (where each has its own operations team): At this stage you can turn back to the Amazon model, where every team essential develops and operates their own services. Each of the business units has to be small enough for everyone to know each others, so under about 150 engineers and you operate each essentially as a startup. Amazon has each AWS service operating more or less separately and it works for them. 

Environment variables are defined inside the container and some are passed in the task definition. So you can use describe-task-definition to see the extra variables. To get the default environment of the container image, you can register-task-definition for task with command 'env' and the specific container image for which you want to find out and then run-task this task. 

Adam Smith concerns himself wholly with exploitation and not at all with exploration. The work done in Research & Development departments of the industry is by its definition mostly exploration and so it is not covered in any way by Adam Smith. But we have seen that in later continuous improvement stages, which are partly exploitative work, the application of CI/CD can bring similar gains in productivity, which could in some way be probably traced down to Adam Smith by someone very imaginative.