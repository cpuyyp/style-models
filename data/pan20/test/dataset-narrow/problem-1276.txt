I think my graph even has too many points, since you only need so much that your entities don't run into walls when they try to go straight from one node to another. The better way to do this would be to use navigation meshes, meaning "surfaces as nodes" instead of just points (in this case a lot faster, since you'd need very few of them) would ). But if you understand how A* generally works they're not hard to implement at all. 

The creation of the framebuffer causes no OpenGL status errors. In terms of usage for depth and stencil testing, the texture/attachment seems to work correctly. If I disable depth testing, I get the usual artifacts of objects in the back being drawn over objects in the front if they are drawn in the wrong order. If I disable stencil testing, the effects I'm using related to that don't work anymore (I'm masking out areas that shouldn't be lit by the lighting pass). Also, if I simply don't attach the depth/stencil buffer to the framebuffer, the related tests stop working (as expected). However, reading from that texture in a shader always returns 1 if I read the depth part, and 0 if I read from the stencil part. I confirmed this via glReadBuffer(GL_DEPTH_STENCIL_ATTACHMENT) and glReadPixels. All integers read show up as 0xffffff00, directly after clearing them via glClear and a black clearcolor and also after the screen is drawn full of stuff. Drawing to and reading from the rest of the attached textures (four color buffers for now) works fine. I have to note that I'm not actually manually drawing anything to the depth-stencil attachment in the fragment shader when rendering - I'm assuming OpenGL automatically draws depth and stencil values to the correct buffer. What could be the cause of these incorrect values showing up when reading from the texture? 

where r is the radius of the circle. As you see, the angle is the only variable in those equations. That means that you somehow have to integrate your progress into the angle. The solution is simple: One "run" around the circle equals 2*Pi (or 360 degrees). You have to split that distance into 100 smaller parts, so that when the progress value is 100, the value inside cos() and sin() is 2*pi (or 360 degrees). You achieve that by simply dividing 2*pi (or 360 degrees) by 100 and multiplying it with the progress: 

You can run those inside a for-loop that runs from 0 to your current progress, for example, if it was a circle that consisted of a series of connected dots (in c++ syntax): 

The following is taken from my current understanding of spherical harmonics. There may be errors involved which solve my question if they're resolved, so please notify me if I misunderstood something. Evaluating reflected diffuse indirect illumination at a surface point is done by computing the integral (simplified rendering equation) in the last line of 

My question(s): In the equation, the function c(x) seem to be the SH coefficients at point (x). So the radiance gradient seems to be computed like a normal numerical derivate as the weighted difference of SH coefficients at points x - (n/2) and x + (n/2). However, what is c(x) in my context? Currently I'm assuming that c(x) refers to the trilinearly interpolated coefficients at surface location(x), but I'm not sure at all, since I don't know how that is supposed to give you more information about the directional distribution of the SH coefficients. And how is that gradient then used to change how the sampled lighting from the cell is applied to the surfaces, exactly? The author just writes "comparing the radiance directional derivative with the actual radiance direction", but this is pretty vague. He mentions using a "central differencing scheme" and references these slides for the central differencing of SH coefficients, and also references this paper which shows the derivations of the gradient, but right now I can't draw any useful conclusions from them. 

Is there a way to get the disassembly that your driver generates when compiling a shader? I noticed that you can get an accidental disassembly dump if you go over the maximum thread group size supported by the hardware in compute shaders, so naturally I figured there must be some way to do this for all shader types. Is there a reliable way to get the disassembly (perhaps specific to NVIDIA/AMD/Intel hardware), using tools or even just "hacks" like causing errors during compilation / linking? 

Problem When looking up the cells during the lighting stage, trilinear interpolation (using hardware texture filters) is used to smoothly interpolate data between the center of a cell, its neighboring cells, and the actual looked up texture coordinate. Essentially, this interpolation mimics the propagation of the lighting information at the center of a cell to the concrete pixels around the center where the information is looked up. This is required because otherwise the lighting would look very rough and ugly. However, since trilinear interpolation doesn't take into account the direction of light propagation of lighting information encoded in a cell (remember, it's in spherical harmonics), the light can be incorrectly propagated to the looked up pixel. For example, if the radiance encoded in the cell only propagates towards (1,0,0) ("the right"), everything that's to the left of the center of the cell should receive less light than what is stored at the center, and everything that's to the right should receive more, but trilinear interpolation does not take that into account. This causes light bleeding incorrectly trough walls when the cell sizes in the grid are big compared to the surfaces in the scene (this is necessary because you need big cells to propagate light far into the scene with as few propagation iterations as possible). This is what it looks like: 

Often in papers people use various radiometric quantities, mostly radiant flux, radiant intensity, radiance and irradiance. It seems to me that all of these quantities are dependent on time. For example, radiant flux is defined as the amount of radiant energy per span of time (emitted by a light source). The other quantities contain radiant flux in some measure so they seem to depend on time as well. Yet when reading papers, people don't go into time at all. For example, some papers say "the radiant flux diffusely reflected by this area of the surface is x" and then they go on to explain that x is just light_colored_intensity * surface_color or something (depending on the illumination model used, of course), not factoring in any kind of time at all. Is this because the time frames observed are assumed to be 1 second? Is the time factored out somewhere (perhaps when defining the light intensity, which may be defined as radiant flux)? What am I misunderstanding here? 

I have an existing depth texture and some other color textures, and want to process the information in them by rendering to a 3D texture (based on the depth contained in the depth texture, i.e. a point at (x/y) in the depth texture will be rendered to (x/y/texture(depth,uv)) in the 3D texture). Simply doing one manual draw call for each slice of the 3D texture (via glFramebufferTextureLayer) is terribly slow, since I don't know beforehand to what slice of the 3D texture a given texel from one of the color textures or the depth texture belongs. This means the entire process is effectively