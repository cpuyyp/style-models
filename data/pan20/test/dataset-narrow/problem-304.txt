This question regards the proper use of NULL and utilizing CHECK constraints for business logic vs stored procedures. I have the following tables setup. 

With this setup, devices are archived by setting field to true and entering an . I could query any device easily whether it is active or archived. (Please ignore the field, as this is used for an unrelated concept). Take notice of the Phone subtype table, where I had to propagate an index of DeviceID and IsArchived flag because phone numbers must be unique for active devices. I have to do this with other subtype tables as well. I don't know if this is a good or bad design. This part really confuses me... What are best practices for handling soft deletes where foreign key values can be marked as deleted. The only thing I can think of is create a routine that searches for all records that are related to deleted data, and create a report for users to resolve the discrepancies. For example, if a table of locations is related to the devices table, and some locations are soft deleted, then the devices refer to locations that no longer exist and must be moved. By the way, I'm using MS SQL Server 2008 R2, and I plan on using Entity Framework 4 in my application. I value database maintainability over performance. Thank you for reading. 

I normalized the tables to avoid using NULLs. The problem is that some of these tables depend on each other due to business processes. Some devices must be sanitized, and some are tracked in another system. All devices will eventually be disposed in the Disposal table. The issue is that I need to perform checks, such as if the boolean field is true, then the cannot be entered until the fields are entered. Also, if the boolean value is true, then the fields must be entered before the can be entered. If I merge all of these columns into the table then I will have NULL fields, but I will be able to manage all of the business rules using CHECK constraints. The alternative is to leave the tables as they are, and manage the business logic in the stored procedure by selecting from the tables to check if records exist and then throw appropriate errors. Is this a case where NULL can be used appropriately? The boolean fields and basically give meaning to the NULL fields. If is then the device is not tracked in the other system and and are NULL, and I know that they should be NULL becuase it is not tracked in the other system. Likewise, and I know will be aswell, and a can be entered at any time. If is , then and will be required, and if and are NULL, then I know they have not been officially removed from that system yet and thus cannot have a until they are entered. So it's a question between separate tables/no NULLs/enforce rules in stored procedures vs combined table/NULLs/enforce rules in CHECK constraints. I understand that querying with NULLs in the picture can be complex and have somewhat undefined behavior, so separate tables and stored procedures seem beneficial in that sense. Alternatively, being able to use CHECK constraints and have the rules built into the table seems equally beneficial. Any thoughts? Thanks for reading. Please ask for clarification where needed. Update Example table if they were merged and I allowed NULLs. 

For the database to be able to guarantee uniqueness of a row, it needs to search the table to make sure that the new incoming row has a unique key. This can only be practical and efficient, in large tables at least, when there is a structure called an index defined on that table that can be searched quicker than searching each row of the table in question. The same index could be used to locate rows by user queries. The index can be built on 1 or more columns of your choice, even on non-key columns to speed the search or to prevent duplicate rows. Sometimes designers would use artificial keys such as auto-incremented keys of data type int for example, instead of long composite keys containing long strings of data. This increases the search speed but still adds an extra index that could affect the insert/update slightly. The shorter the key, the more keys could be loaded and searched in memory, hence improving the overall response time. In your case, you could, use such an artificial key to enhance performance. To guarantee uniqueness, you will still need a unique index defined on your composite key. 

The process step is not meaningful in your case unless a process version is defined. So you could say that process 1 has its steps executed in this order (a, b , d, c) when the process was in version 1, but in version 2 the step execution order changed to be (a,b,c). So I think that a process version is important. The diagram below represents my suggestion. The silly thing about this is that if you change the order of a step, you have to insert all steps again in the new order, but in this case, it won't matter either in space or time. 

I'm trying to make a multi-statement table valued function and failing due to what I think is a limitation of functions and CTEs. The intended function defines an input parameter(@Param) and and output table(@ResultTable). It then executes a complex insert statement involving a CTE into that table variable which is (of necessity) terminated by a semicolon. I then attempt another complex update statement to that table variable involving a CTE and receive an error saying that "Must declare the scalar variable "@ResultTable". Apparently it has dropped out of scope somehow. I have used this sort of pattern in the past, so my only thought is that the CTEs seem to limit the scope in some way. Is this a known limitation? 

I have a cmdexec step in a sql agent job that includes a redirection into a file at the end. It works as expected from a CMD shell running in the context of SQL Agent service account and produces a file in the desired location. If I run it as an agent job, however, the step 'succeeds' but never produces the output file. In both cases, the service account obviously has filesystem permissions and system rights sufficient to perform this action. Is the cmdexec environment more restrictive in someway than just running cmd.exe? 

I'm trying to perform the same scenario as in the following link; Create a SSIS Script Component as a Data Source that uses a pre-existing HTTP Connection Manager to retreive a page with GET and emit rows into the Data Flow pipeline. $URL$ My target platform is SQL Server 2008 and therefore C#. The MSDN documentation gives examples of File and SQL Connection Managers but not HTTP ones. $URL$ The specific problem is that I can NOT figure out why there's no HttpClientConnection constructor in my current context. The MSDN documentation of that class does not seem to apply in the case of Script Components and translating this to something useful is apparently beyond me. $URL$ My non-working code looks like this - 

Due to this design's granularity, I could theoretically allow any mix of statuses for devices with this design, but I wanted to control it so I wrote some triggers to only insert the correct mix of statuses depending on whether the device is network capable. Triggers as follows: 

I put check constraints on the table as follows to ensure a network status and network information can only be provided if the device is capable of connecting to a network. 

This is an inventory database for IT assets. The models used are trimmed in order to focus on the problem at hand. Using SQL Server 2008. Thanks for taking the time to read and for any input you can provide. My design includes a table which holds the various devices that can be entered into inventory. Each device has a boolean flag, which states whether a device has network capability, e.g., for most computers , for hard drives ; some printers will be true, and others will be false. You get the idea. The field determines if network-related information is relevant when an inventory record is created. Design 1 My first design uses an index on and to use in a foreign key constraint with the table. 

I'm designing an asset management database that tracks IT hardware. I decided to use a supertype/subtype design. I'm at a point where I want to track history of changes for devices. I wanted to use a separate history table, but I can't decide how to track history for changes made to subtype tables. If I use separate history tables for each subtype table I can reconstruct records by joining them with the supertype history table, except in the case where subtype history tables change independently of the supertype history table. By independently, I mean there are x updates to data in the supertype table, creating x supertype history records, and y updates to a subtype table creating y subtype history records. If the changes are made on the same day, how would I reconstruct records? Is this a good use of supertype/subtype, or should I denormalize the tables? Otherwise, can anyone suggest any way to approach the history issue for this type of design? Using MS SQL Server 2008. Here is a very simplified ERD: 

First, I suggest you read about star schema first. Not understanding the concepts could lead to wrong results. You may use the transaction as your fact table as in the diagram below. Don't use DISTINCT, instead you want to use: SELECT ... FROM fact GROUP BY dimension data (time, transaction and bank) WHERE condition to restrict dimension data and join with fact. 

Typically you'd have an Invoice table that contains 1 or more sold line items (as a child of Invoice) and each line item's price (as well as possibly other details) would be looked up from a lookup table (that would act as a parent for the line item and would contain the price of each item). A line item in this case has many parents. The price table plays the role of a reference or lookup table. 

I would not use GetDate() for this fixed-period scenario as the end date for queries, because the reports are only meningful for specific periods. A better approach is to use a specific run-date for each reporting period. That run-date is not the current date. For example, let's say one of your KPIs is the sales amount for the first 6 months of 2013. Your system should use end of June as the end date not the current run-date regardless of when the report is run. If you do this, you'd not have to touch the data. 

I'm planning a project to upgrade internally-developed app from SQL Server 2005 to SQL Server 2012. I hope to use Service Broker as a bridge while in transition. Are there any issues to consider in a forward-compatibility scenario like this other than newer features like multicasting not being present? I can't find anything in BOL specifically about compatibility. 

I have a server with a working installation of the Sql Server 2012 SSIS Catalog. I need to set up an additional instance including the SSIS Package Store service as an interim step while the packages are being re-written. The Package Store is a per-server feature, not a per-instance feature. Can these two features operate side-by-side? 

This is almost exactly the example used for 'factless fact tables' in analysis services, which illustrates the point. The thing represented by the table is a 'booking', which has no natural primary key. The true key would be a composite key of location-timeslot. You need to create an artificial primary key if you don't want to do that or the DBMS does not support that. You need at least these tables- Location(room #, Capacity) Timeslot(day,starttime), module(code,name). It's not clear if the person is tied to the module or tied to the booking. It's not clear if the extension is tied to the person or the location. 

This query works in the simplest case, but does not allow adding attributes to the root element, which is required in my case. 

The table looks like this in this setup (Notice the addition of record with id #4, and the field which specifies that this status is for use with devices that can can connect to a network): 

The issue I have with this design is I'm not sure if the relationship with and / is a good or bad design decision. Is propagating a non-key field like to other tables a bad design? I don't have enough experience with database design to make an informed decision. 

This design eliminates the need to propagate across the tables. The issue I see with this design is that every device that has network capability will have records in paired with ids 1, 2 and 3, while devices that can't connect to a network will be paired only with id 4. It seems like a lot of extra records that all mean the same thing: devices that can be networked can only use statuses 1, 2 and 3, and devices that can't network only use 4. This design seems to be more "relationally correct", but also smells a bit. Update The following update proposes variations on Design 1. I come across situations like this often, where there are many ways to achieve the same end result. I never know how to tell if there are hidden problems with the designs, and I can't judge when to normalize or denormalize. Is one of these designs preferred over the other and why? Design 1.1 

I'm starting a few new database projects and I'm attempting to create them at Data Tier Applications. There are two items I'm not able to find documentation for. I would like to set the db owner to SA and set the initial filesize and growth rate. Even if those items are outside the scope of the app, I would expect that there would some way to specify that at publish time, either in SSDT or SSMS. I can find no documentation either way. Is this the case? 

There's no answer that applies in all cases. In general, however... If the lookup list is small and you can cache it(or use a cache data source), there's not much performance penalty to doing it in SSIS. If you want crossref a list of 50 location codes to names of cities, go for it. It's nice to see all the process on-screen in one place, rather than buried in sql statements. TSQL will be better-performing in most cases, since it knows the most about the data and the query optimizer is always going to be smarter than you. If all the data is in one DB, you can hide a lot of complexity in a sql query source. If the data is spread out across different systems, the middle ground is to to do an SSIS merge join from each system. Trying to do that at the RDBMS level is madness. Always do the sorting in the source query, though. SSIS Sorting is almost always a bad idea. 

This is a bit strange. Have you done your analysis right? If you answer is yes, then you need to tell us more so that we may be able to make accurate suggestions. Document-based database(s) allow both of the above requirements, but I would not recommend it without understanding the problem first. 

Do you know of an industry standard that is designed to describe the metadata contents of a relational database (or part of it, such as tables, columns, etc.)? Do you know if a tool exists that does export the metadata of any of the top 5 RDBMS tools in any format such as XML (without having to type in DDL in the syntax of particular database)? Thank you. 

"integers are cheaper to compare than characters, because character sets and collations (sorting rules) make character comparisons complicated. Integers are very good when using as an index column." - HottestGuide 

Since NULL value is not the same as empty string in SQL, then the result you obtained is correct and makes sense in SQL terms. Think of NULL as "Not Defined Value" and as such it is not same as an empty string (or any non-null value for that mater) which is a defined value. From an English standpoint, null is the same as nothing but this is not the same in SQL. Now what do you expect this to return? 

Having a table for each data type has nothing to do with normalization. Nomralization is about not repeating base information in tables. It is valid to have a key for a row and different columns each having its own data type. In fact this is the most common way. 

is returned. Is there any way to achieve this behavior? To clarify, the behavior I would expect to see is demonstrated in this example 

In my specific case, this was a result of cell security. The role did not have access to all the fields returned in the drillthrough action. I missed this because there are two very similarly-named fields and my eye did not notice. It's still odd that a security denial creates a query timeout, but at least this note is here for the next person. 

After investigating a production issue, I have found a number of references to SSIS packages getting stuck in 'validation' when run with DTEXEC or through as SQL Agent job steps. Specifics of my situation aside, is there a technique to make this more robust so that in a production environment, I am notified if a job starts, but does not complete? Is it possible to have a timeout period on validation so that the execution explicitly fails? Or have a way to specify number of retries? 

I have a drill-through problem apparently related to security. Users in one role are seeing timeout failures when trying to invoke a drill-through action. They have permission on the action through that role and seem to have all necessary dimension and cell security rights. Profiler is not showing me any obvious reason this is failing. What else can I use to debug this? If I modify the user's role membership, the action works as expected, so I'm confident this is related to security somehow.