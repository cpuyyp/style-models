yes it is possible. you have to specify the nic with connection to the faster switch as primary. here is a bonding how-to. 

Perhaps you can combine with rsync. Especially the parameter could by helpful. Taking a look at the docs, something like should work. Here you'll find the docs and software. Haven't tried this on my own. 

one possibility is using openldap. here is a how-to to get you started. the how-to is a bit outdated, but it should get you started. perhaps your distro offers some of the needed packages, this would make it much easier. another option is ad. when you already have a windows network with a working ad, you can add your linux servers to the ad. you could use unix services for windows or pam or ... some distributions already have bundles to connect to ad, for example likewise in fedora/rhel. 

just change your mod_jk configuration and remove one tomcat from the cluster. after doing a graceful restart of your apache, every request goes to the desired tomcat. when you are finished with testing you just put the second tomcat back in configuration and do again a graceful restart of your apache. you could also change of the tomcat which you want to disable and do the same graceful restart. 

why do you need the myexamples folder? Couldn't you just deploy tomcatapp1 and tomcatapp2? These would be folders. I think this could be what you are looking for: 

try solr, it also has a date filter when you configure it. here is a php client to solr and here is a tutorial from developer works for setting this up. when it does not fullfill all your needs, you can also extend it. 

the best solution depends much on your setup, so there is no general answer. but the docs should help you to decide what solution to take. one more point to consider: a waf is adding some more complexity to your system, so be sure whether you want to use it or not. 

in addiotion to the previous answers, you should also take PermGen into account. PermGen is not part of the heapspace. with your current configuration your java process could sum up to 1792mb which is the total amount of your machine. 

try to telnet to your ssh server: . When this does not work it is most likely that you have a firewall between the two networks. do you have a local firewall on the client or the server? is there a central firewall between the two networks? 

give the server-vip certificate to your clients as trusted certificate. server-0 must have the certificate of server-1 as trusted and vice versa. this way your clients won't recognize a failover and your servers are independent of the server-vip certificate. 

Have you verified the services are activated for your default runlevel? You can get your current runlevel with . You can verify the activated runlevels with . 

this will work , but works from the back to the front. here is a good how-to for chopping strings in bash. 

you can do it without an extra vlan, but to protect your production environment, you should use an extra vlan just for the case. 

make sure your jar is using the packed libs on your ubuntu box. Just move the libs in the filesystem temporarily to another location. When your jar runs, you can be sure the libs in the jar are used. How did you reference the external libs from your jar? Did you add them to the Class-Path of your manifest file? When you did so, did you use relative or absolute paths? 

check the files and and whether the client is entered correctly. when this does not help, please post your export file. 

For monitoring and deployments on a single tomcat, psi-probe is a possible solution. It is a fork of Lambda Probe. For monitoring the hole infrastructure I agree with the Nagios answer. An alternative to jVisualVM is jconsole, also part of the JDK. For managing the hole infrastructure, I think there is no way around some custom script, whether they are written in bash, perl, ant, puppet manifest or whatever. But you can hide their complexity behind such tools like already mentioned hudson, puppet, cfengine, chef, ... 

have you checked your new user has the same shell as your root user? Its seems that your root user has bash your something like it and your new user has a ksh or something similar. Take a look at the /etc/passwd file or the SHELL environment variable. 

you should definitely exclude the zip file from gzip compression. There is no need in compressing already compressed content again. This is not only valid for zip files but also for jpg and so on. 

oracle's bug database is only telling something about 64bit and 1.5 version systems: $URL$ but perhaps you have found the bug in another version? have you tried this parameter to reduce the size of the code cache? the bug database is mentioning some more parameters, perhaps you try them and test whether the error still occurs. 

How are you coping the files? The *nix command has the parameter to preserve the time stamp. Perhaps you have something similar on your system. 

You can use jVisualVM. It is part of the SUN/Oracle JDK, but can also be downloaded as separate program. It has a tab for the thread overview and also some other metrics (Heap, Overview, ...). You can even connect from remote to your JBoss when you've enabled JMX. I haven't used it for JBoss, but for Tomcat, should be nearly the same. 

you can set the variable in . but this is the timezone for all crontabs. another alternative is using fcron. the documentation shows an option to set the timezone in each crontab. 

it seems the post you are referencing tells you most of the solution. Just create an additional partition and install some type of linux distribution on it. When you are installing your desired linux distribution as your main system, change grub or lilo to have a reference to your rescue linux os. When you experience any problems change your bootoader to boot your rescue partition. Mount your main filesystem in the rescue system and do whatever is needed. When finished change your bootloader back to your main system. Another possibility is to get access to a remote administration card of the server. The main server producers ship their servers with remote administration cards. IBM calls it RSA, Dell calls it RAC (I think) and HP calls it ILO. With these cards you can remote connect to the console of the server. This way you could choose the booted system from the console or directly do things via remote console. 

yum has no option to do this. and i don't know about a plugin which could this. but when you have time and fun with scripting, it should be possible to do it with yum commands. 

the server with the ssl endpoint has to hold the certificate. When your server another.example.com is directly accessed, than you have to install the certificate on this server. When your webserver example.com acts as reverse proxy (all request go through this server) to another.example.com, than you have to install the certificate on this server. I hope this clarifies the issue. 

yes this a recommended approach. one thing is also possible, when your application is getting more complex and you have different parts which have to do different work on your database you could create more accounts, e.g. one with rw access and one with only read access. but be sure to not make it more complex as needed. this will revert all security considerations, because at some point in time you don't know which user to use what for and every user will get more permissions than he needs. 

have you checked the filesystem permission on both machines? sshd is a bit picky about permission on folder .ssh and the files in this folder. .ssh should have 700 and the files should have 644 or less. 

let insserv create the symlinks for you. when using insserv, you can also add dependencies to your script header so insserv knows when to start and stop your script in a special runlevel. this is necessary for sequential and parallel execution. try to be compatible to lsb. here is a short example: 

you can export part of your ldaptree with to an ldif file and add it too your other ldap server with or . there are also products which support automatic or semi automatic replication in one direction like the fedora directory server. jabber can be authenticated against ldap, but i can't tell you how. perhaps someone else can answer this part. 

you cand find different ssh clients on the net which connect to multiple clients at the same time, so you can execute commands at the same time. one of these clients is cluster ssh. you could give it a try or search for an other client on the net. 

This will give you alot of output, from apache and the system calls, but perhaps you'll find something interesting. With you can write the output to a file which makes post processing much easier. 

you could monitor your application via jmx from the outside. when you know some metrics which indicate an upcoming OutOfMemory, you could trigger a jmap run before the exception is thrown. 

as i understand your post, you are building your own kernel from source. do you have any special reason to not stay with the centos provided kernels? my guess for your error is, you have a different kernel configuration and are missing a module or more, e.g. lvm. when you really need to build the kernel by yourself, use the kernel config of an existing centos kernel. you'll find the old config in your directory or just use . but be aware that this can be dangerous when you jump up multiple kernel versions. when you only want to add a new driver to your kernel, just compile the driver against your installed centos kernel and add it to the modules. 

i don't think this is possible. when you connect via ssh to your machine and then wipe out everything, your ssh server is gone and with him your connection. when your server has no cd/dvd drive and does not boot from usb/network, how did you install your server the first time? one possibility are remote management cards (e.g. rsa, ilo, ...). with them you can attach iso images or local cd/dvd/usb drives to the server and install from these devices. 

just combine the output with some other commands, like this . This way you should get the output you need. 

have you successfully completed step one of the tutorial? it is talking about port 8888 and jetty and not port 8080 and glassfish. 

Don't know whether this is present in the service context, but you can test it. Define a timeperiod "non-working hours" and take ISVALIDTIME as a parameter to your eventhandler script. When the script is called during working hours, let it do nothing and exit. When this doesn't work, you could just verify the time of day in the eventhandler script. 

you were heading in the right direction. in tomcat 5.5 connectionTimeout is the same as keepalivetimeout in tomcat 6.0. but you should change the value from 2 to 2000, as the tomcat value is in milliseconds and the apache value is in seconds. you are right as you don't see the timeout value in the http headers send by your tomcat. but the timeout will take action on the server side. i asume it is not send as the tomcat connector is HTTP/1.1 and the is HTTP/1.0. as the tomcat documentation tells, you should synchronize the timeout values on both sides, loadbalancer and tomcat in your case. 

you have to add the self-signed certificate as trusted to your client certificate store. This way the client will accept it and establish a connection. 

here is the documentation for log4j and commons-logging with an example for context-specific logging and here is a short how-to for multiple instances. 

this will not work, because drbd only supports three nodes. for your automatic updates you should use something like puppet or a central software distribution tool (e.g. spacewalk), depending on the type of your updates. puppet would be best for configuration changes, but can also do software distribution. spacewalk would mainly be used to do software updates, but can also be used for configuration changes. 

are you sure the ftps is only on port 990? The configuration should activate ftps on port 21 if you don't explicitly set . Find out on what ports vsftpd is listening and try to connect via ftps on port 21. If it still doen't work try to set and take a look at your logs. Another possibility is, you have a debian vsftpd version, which does not support . What version of debian and vsftpd do you use? The option has been added by version 2.0.7. 

you can use mod_security or other web application firewalls (waf). this way the request still hit your webserver, but mod_security will filter out the request which are marked as suspicious. there are different possibilities to setup a waf: 

perhaps your default policy of iptables is allow for all input traffic. this would allow a connection from any ip. change this policy to drop: 

when you put this in , will check wether a user is in group yourgroup when the user executes . this way you don't have to add a new user manually to the script. 

you can use manual fencing for testing. search for fence_manual in this doc this will not disable fencing, but nothing will happen when you not want to. 

Another method to separate the output would be to run one tomcat instance for each webapp. This way you can not only separate the webapp logs, but also the other log files, the JVM parameters, start and stop of the instances and so on. You'll find a step by step guide here. 

just add the the parameters to your mount command and all files and directories will belong to smbuser. 

i've a problem with our jmx monitoring. we monitor the perm gen of some jvms with jmx. but from time to time the name of the mbean changes from "PS Perm Gen" to "Perm Gen" or vice versa. it happens with a restart of the tomcat server, but not with every restart. the jvm version is 1.5.0_16. has anyone experienced this problem and perhaps has a solution?