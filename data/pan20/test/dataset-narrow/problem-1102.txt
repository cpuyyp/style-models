There is known connection between classical and modal logics and type theory (lambda calculus), but are there connections between nonmonotonic logics (e.g. defeasible logic) and type theory (lambda calculus)? Maybe HoTT provides some generatlization where such connections can be found? If such connections can not be established then widely available proof assistants (Isabelle, Coq) are not useful for nonmonotonic logics, aren't they? 

It is known that first order logic is too general to be decidable. Adding axioms with special meaning (e.g. expressing notions such as necessity/obligation, provability, etc.) leads us to modal logics but some of them (especially multidimensional fusions or products of them) are undecidable too. As I understand, then undecidability generally is created by quantifiers - it is quite hard to prove of refute formula, that should be valid in all worlds (if accessibility relation allow them). The undecidability can be handled by several approaches: 1) lowering the expressivity of the logic or considering the less expressive fragments; 2) following ideas of max-SAT problems; 3) moving to approximate reasoning (probabilistic, fuzzy-logic, etc.). All these approaches has this drawback: they tries to reformulate or make less relevant the original problem that was created as conscise and clear model of the real world problem. Maybe there is better way how to approach such undecidable situations - simply by adding additional axioms that reflect the real-world situation. E.g. axiom can be added that the number of variables is bounded in the logic (there can be different axioms depending on how we model the reason why there are only finite number of instances of some class in the world - e.g. bounded resources, restricion by law, etc.). So - the question is - is there some research trend that investigates the modal logics with aim - what axioms should be added to them the regain the decidability of logics (and according - the suitability for them for automated reasoning tasks, for use in autonomous systems)? 

Take any distribution $D$ on $\{0,1\}^n$. Sample $k(n)$ points $x_1, \cdots, x_{k(n)}$ independently from $D$ and let $\tilde D$ be the uniform distribution that gives a random $x_i$. Then, if $k(n)$ is super-polynomially large e.g. $k=n^{\log n}$, you cannot distinguish $D$ and $\tilde D$ using only $\mathrm{poly}(n)$ samples. Hence $D$ is computationally indistinguishable from a distribution $\tilde D$ with entropy $\mathrm{poly}\log(n)$. 

The obvious thing to try is setting $h_\pm(x,y)=f_\pm(x) \cdot g_\pm(y)$. Unfortunately, it doesn't seem to work out (essentially the signs and inequalities don't go the right way). I'm also interested in generalisations of this question, where $X$ is, say, small-biased, rather than $k$-wise independent. 

I have the following conjecture about bounded functions on the hypercube. Any help resolving it (proof, counterexample, some ideas) is much appreciated. 

Aside: We can also change the problem to get an efficient solution: If $x_1, \cdots, x_n$ have the property that strictly more than half of them lie in a ball $B(y,\varepsilon)$, then we can find a point $z$ that lies in $B(y,3\varepsilon)$ in time polynomial in $n$ and $d$. In particular, we can set $z=x_i$ for an arbitrary $i$ such that strictly more than half of the points are in $B(z,2\varepsilon)$. 

This problem is easy: We can use binary search to find the argmax with $O(\log n)$ queries. i.e. Build a complete binary tree with $n$ leaves corresponding to indices. Start at the root and walk down to a leaf as follows. At each node, query the maximum value in the right and left subtrees and then move to the child on the side with the larger answer. Upon reaching a leaf, output its index. The following noisy version of this problem has come up in my research. 

The following result is supposedly known. However, the proofs I am able to find all prove a weaker result with an extra log factor. Where can I find the proof of the tight bound? 

The answer to such question can have practical applications as well - e.g. it can help to define some kind of formal semantics of the "business rules" (declarative programming paradigm that is used more and more in business applications, e.g. IBM ILOG, JBoss Drools, Oracle Business Rules, PHP and Python also have their own rules engined) and going further - this can be useful for cognitive robotics as well. 

I am acquinted with the basics of such notions as logic programming, monotonic and non-monotonic reasoning, modal logic (especially dynamic logic) and now I am wondering - does logic programming provides anything new to any logic? As far as I understand, then (at least in dynamic logic) the logic programming refers to the formalization of state transitions (by actions, i.e. - logic programing can be understood just as logic about actions). But the same notion "logic programing" seems to be in use even in domains, where there is not state transition, just exploration of one state (or set of states - in case then the initial set of premises are vague enough to describe more than one state). It seems to me that some authors simply use the notions "logic programming" for describing the procedure how to evaluate the query (I am reading currently about defeasible logic programming). But such procedure (although practical indeed) does not add anything new to the underlying logic. E.g. there is notion of "rational closure" (e.g. used for adaptive logics; just the consequence set for some set of premises) which should contain all the possible knowledge about state and therefore all the possible results of "logic programing" (if it is indeed perceived just as state exploration, derivation). So - the question is - does logic programming provides anything new to the logic and does every logic (to be completely understood and readied for applications) need to have its own logic programming? Maybe I am just missing the point... Just for reference I find the following works interesting about this subject, if there are more along this line, then it would be great to hear! 

We're interested in additive approximations to #3SAT. i.e. given a 3CNF $\phi$ on $n$ variables count the number of satisfying assignments (call this $a$) up to additive error $k$. Here are some basic results for this: Case 1: $k=2^{n-1}-\mathrm{poly}(n)$ Here there is a deterministic poly-time algorithm: Let $m=2^n-2k = \mathrm{poly}(n)$. Now evaluate $\phi$ on $m$ arbitrary inputs (e.g. the lexicographically first $m$ inputs). Suppose $\ell$ of these inputs satisfy $\phi$. Then we know $a \geq \ell$ as there are at least $\ell$ satisfying assignments and $a \leq 2^n - (m-\ell)$ as there are at least $m-\ell$ unsatisfying assignments. The length of this interval is $2^n - (m-\ell) - \ell = 2k$. So if we output the midpoint $2^{n-1} -m/2 + \ell$ this is within $k$ of the correct answer, as required. Case 2: $k=2^n/\mathrm{poly}(n)$ Here we have a randomized poly-time algorithm: Evaluate $\phi$ at $m$ random points $X_1, \cdots, X_m \in \{0,1\}^n$. Let $\alpha = \frac{1}{m} \sum_{i=1}^m \phi(X_i)$ and $\varepsilon = k/2^n$. We output $2^n \cdot \alpha$. For this to have error at most $k$ we need $$k \geq |2^n \alpha - a| = 2^n |\alpha - a/2^n|,$$ which is equivalent to $|\alpha - a/2^n| \leq \varepsilon.$ By a Chernoff bound, $$\mathbb{P}[|\alpha - a/2^n| > \varepsilon] \leq 2^{-\Omega(m \varepsilon^2)},$$ as $\mathbb{E}[\phi(X_i)]=\mathbb{E}[\alpha]=a/2^n$. This implies that, if we choose $m=O(1/\varepsilon^2) = \mathrm{poly}(n)$ (and ensure $m$ is a power of $2$), then with probability at least $0.99$, the error is at most $k$. Case 3: $k=2^{cn + o(n)}$ for $c < 1$ In this case the problem is #P-hard: We will do a reduction from #3SAT. Take a 3CNF $\psi$ on $m$ variables. Pick $n \geq m$ such that $k < 2^{n-m-1}$ -- this requires $n = O(m/(1-c))$. Let $\phi=\psi$ except $\phi$ is now on $n$ variables, rather than $m$. If $\psi$ has $b$ satisfying assignments, then $\phi$ has $b \cdot 2^{n-m}$ satisfying assignments, as the $n-m$ "free" variables can take any value in a satisfying assignment. Now suppose we have $\hat{a}$ such that $|\hat{a}-a| \leq k$ -- that is $\hat{a}$ is an approximation to the number of satisfying assignments of $\phi$ with additive error $k$. Then $$|b-\hat{a}/2^{n-m}| = \left| \frac{a - \hat{a}}{2^{n-m}}\right| \leq \frac{k}{2^{n-m}} < 1/2.$$ Since $b$ is an integer, this means we can determine the exact value of $b$ from $\hat{a}$. Algorithmically determining the exact value of $b$ entails solving the #P-complete problem #3SAT. This means that it is #P-hard to compute $\hat{a}$. 

Suppose one has a set of $n$ points in $\mathbb{R}^d$, where the points are represented as $p_1$ through to $p_n$. Define the weight matrix $W$ as follows: let $W_{i,j}$ be $e^{-||p_i - p_j ||^2}$, also sometimes known as a Gaussian weight matrix. My question is, is there a relation of the rank of $W$ to the underlying dimensionality $d$? If not, is there a rank $d$ matrix approximation of $W$? The reason I say this is that in a loose "information theoretic" sense, all the weights can be reconstructed from the $d$ real coordinates of each point. 

Suppose we have a collection of linear subspaces $\mathbb{C}$ lying in $\mathbb{R}^d$, such that each $c \in \mathbb{C}$ is of dimension at most $k \leq d$ for a given fixed $k$ and $|\mathbb{C}| = n$. Now we want to find the subspace of dimension at most $k$ closest to each $c \in \mathbb{C}$ under the sin-theta distance. (Note we do not restrict this optimal subspace to be chosen from $\mathbb{C}$). How can compute this and how much time would this take? This seems simple enough that it might be standard and well-known, perhaps under the right geometric transformation? Even an exponential time algorithm would be very useful - I am more interested in computability rather than optimal efficiency. To be more precise, define the sin-theta distance for two linear subspaces $L$ and $M$ as follows: $\sin \theta(L,M) = \max_{x \in L, ||x|| = 1} \min_{y \in M, ||y|| = 1} || x - y||_2$. (This is symmetric if and only if $L$ and $M$ are of equal dimensionality.) Now if we let $S$ be the space of all subspaces of $\mathbb{R}^d$ of rank at most $k$ our problem reduces to finding $\text{argmin}_{s \in S} \sin \theta (\mathbb{C}, s)$. Note: Having though about this where all the subspaces are of dimension exactly $k$, the problem simply maps to having a collection $\bar{\mathbb{C}}$ of $n$ points on the surface of the sphere $\mathbb{S}^{d-1}$ and finding the point closest to them. It becomes more tricky where the subspaces in $\mathbb{C}$ are of varying dimensions, as you introduce more degrees of freedom. 

Unfortunately, this result is not very practical in the high-dimensional setting. A good question is whether we can compute $f$ more efficiently: 

Firstly it is well-known that the approximate degree of $\mathrm{AND}$ is $O(\sqrt{n})$: Theorem 1. For all $n$ and $\varepsilon>0$, there exists a multilinear polynomial $p : \{\pm 1\}^n \to \mathbb{R}$ of degree $O(\sqrt{n}\log(1/\varepsilon))$ such that $|p(x)-\mathrm{AND}(x)|\leq \varepsilon$ for all $x \in \{\pm 1\}^n$. Proof. We use the Chebyshev polynomials: Fact. For all $\ell$ and $d$ there exists $p_{\ell,d} : [-1,1] \to \mathbb{R}$ of degree at most $d$ such that $|p_{\ell,d}(z)-z^\ell| \leq 2e^{-d^2/2\ell}$ for all $z \in [-1,1]$. See Sachdeva-Vishnoi Theorem 3.3 for a proof. We also have $$\left|\mathrm{AND}(x) - \left(\sum_{i=1}^n \frac{1-x_i}{2n}\right)^\ell\right| \leq \left(1-\frac{1}{n}\right)^\ell \leq e^{-\ell/n}$$ for all $x \in \{\pm 1\}^n$. Thus $$\left|\mathrm{AND}(x) - p_{\ell,d}\left(\sum_{i=1}^n \frac{1-x_i}{2n}\right)\right| \leq 2e^{-d^2/2\ell} + e^{-\ell/n}$$ for all $x \in \{\pm 1\}^n$. Setting $\ell = n \log(3/\varepsilon)$, $d=\sqrt{2\ell \log(3/\varepsilon)}$, and $p(x) = p_{\ell,d}\left(\sum_{i=1}^n \frac{1-x_i}{2n}\right)$ proves the theorem. Q.E.D. Now we show that the approximate degree of $\mathrm{AND}$ becomes the exact degree of $\mathrm{AND}$ (which is maximal) for some value of $\varepsilon>0$: Theorem 2. For all $n$, there exists $\tilde\varepsilon>0$ such that no multilinear polynomial $p : \{\pm 1\}^n \to \mathbb{R}$ of degree strictly less than $n$ satisfies $|p(x) - \mathrm{AND}(x)|\leq \tilde\varepsilon$ for all $x \in \{\pm 1\}^n$. Proof. Let $\mathcal{P} \subset \mathbb{R}^{\{\pm 1\}^n}$ be the space of functions from $\{\pm 1\}^n$ to $\mathbb{R}$ that have degree strictly less than $n$. Claim. $\mathcal{P}$ is closed with respect to the $\ell_\infty$ norm. This follows from the fact that $\mathcal{P}$ is a linear subspace of $\mathbb{R}^{\{\pm 1\}^n}$. It is spanned by the multilinear monomials of degree strictly less than $n$. Claim. $\mathrm{AND} \notin \mathcal{P}$. The Fourier transform uniquely expresses $\mathrm{AND}$ in the multilinear monomial basis. We see that the degree is $n$: $$\mathrm{AND}(x) = \prod_{i=1}^n \frac{1-x_i}{2} = \sum_{s \subset [n]} \frac{(-1)^{|s|}}{2^n} \prod_{i \in s} x_i.$$ Claim. $\exists \tilde\varepsilon>0~\forall p \in \mathcal{P}~\|\mathrm{AND}-p\|_\infty>\tilde\varepsilon$. Since $\mathcal{P}$ is closed, its complement is open. $\mathrm{AND}$ is in the complement of $\mathcal{P}$ which means there exists a closed ball of positive radius $\tilde\varepsilon>0$ centered at $\mathrm{AND}$ that does not intersect $\mathcal{P}$. The final claim is exactly the theorem. Q.E.D. Thus we have shown that there exists $\tilde\varepsilon>0$ such that $\mathrm{deg}_{\tilde\varepsilon}(\mathrm{AND})>\mathrm{deg}_{1/3}(\mathrm{AND})$. 

We were eventually able to analyze hypercontractive properties of $R_{p_1, p_2}$ ($URL$ building off of the main Fourier analysis of $R_{p,0}$ by Ahlberg, Broman, Griffiths and Morris ($URL$ To summarize, the effect of a biased operator $R_{p,0}$ on a function $f$ can be analyzed as a symmetric noise operator in a biased measure space. This gives a weak form of hypercontractivity, which depends on how the $\ell_2$ norm of $f$ varies when switching to a choice of biased measure $\mu$ dependent on $p$. 

Informally, I want to ask if two functions $f$ and $g$ on the Hamming cube have the same Fourier spectra but w.r.t different measure and basis, then is $||f||_p$ related to $||g||_p$? (Where each $p$-norm is measured according to the appropriate measure.) This question requires some notation to set up more precisely. First, for the Hamming cube $\{0,1 \}^d$, given a bias parameter $0 \leq b \leq 1$, we may construct a biased measure $\mu_b$ that assigns a weight $b$ to $0$ bits and $1-b$ to $1$ bits as follows: \begin{equation} \mu_b(x) = b^{\sum_i x_i} (1-b)^{d - \sum_i x_i}. \end{equation} We continue that expectations and $l_p$-norms can also be defined under this measure. So for example, $ E_b[f] = \sum_{x \in \{0,1\}^d} \mu_b(x) f(x)$ and $||f||_p = \left( \sum_{x \in \{0,1 \}^d} \mu_b(x) (f(x))^p \right)^{\frac{1}{p}} $. We note now an Fourier parity basis for the space $F_b$ of all functions $f: \{0,1\}^d \to \mathbb{R}$ when $\{0,1\}^d$ is associated with measure $\mu_b$. First for each $x \in \{0,1\}^d$ and $i \in [d]$ we define: \begin{equation}\chi_i^b = \begin{cases} \sqrt{\frac{b}{1-b}} & x_i = 0 \\ -\sqrt{\frac{1-b}{b}} & x_i = 1 \\ \end{cases} \end{equation} This set of $\chi_i^b$ essentially correspond to a bit wise parity basis, which we can extent to arbitrary $S \subset [n]$ as $\chi_S^b(x) = \prod_{i \in S} \chi_i^b(x)$. The collection $\{\chi_S^b \}$ over all $S \subset [n]$ is now our desired orthonormal basis. All this notation reduces to the standard Fourier basis and notions for $b = \frac{1}{2}$. Now after all that lengthy build-in, here is my question. Suppose $f$ is a function over $\{0,1 \}^d$ with measure $\mu_{1/2}$ and $g$ is a function over $\{0,1 \}^d$ with measure $\mu_b$ (for $b \neq \frac{1}{2}$) and suppose $f$ and $g$ have the same Fourier cofficients w.r.t their corresponding bases elements $\chi_S^{1/2}$ and $\chi_S^b$ . Parseval's immediately implies $||f||_2 = ||g||_2$, where each norm is calculated according to the measure of the space the function lies in ($\mu_{1/2}$ and $\mu_b$ respectively). Is this true for all the $l_p$ norms? Is there a general relation between $||f||_p$ and $||g||_p$?