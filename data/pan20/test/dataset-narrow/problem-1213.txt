This is not an answer but a method which I believe would leave to an improved lower bound. Let us cut the problem after $a$ letters are read. Denote the family of $a$ element sets of $[n]$ by $\mathcal A$ and the family of $b=k-a$ element sets of $[n]$ by $\mathcal B$. Denote the states that can be reach after reading the elements of $A$ (in any order) by $S_A$ and the states from which an accepting state can be reached after reading the elements of $B$ (in any order) by $T_B$. We need that $S_A\cap T_B\ne \emptyset$ if and only if $A\cap B=\emptyset$. This already gives a lower bound for the required number of states and I think it could give something non-trivial. This problem essentially asks for a lower bound on the number of the vertices of a hypergraph whose line graph is (partially) known. Similar problems were studied e.g., by Bollobas and there are several known proof methods that can be useful. Update 2014.03.24: In fact if the above hypergraph can be realized on $s$ vertices, then we also get a non-deterministic communication complexity protocol of length $\log s$ for set disjointness with inputs sets of size $a$ and $b$ (in fact the two problems are equivalent). The bottleneck is of course when $a=b=k/2$, for this I could only find the following in Eyal and Noam's book: $N^1(DISJ_a)\le \log \big(2^k \log_e {n\choose a}\big)$ proved by the standard probabilistic argument. Unfortunately I could not (yet) find good enough lower bounds on this problem but assuming the above is sharp, it would give a lower bound $\Omega(2^k\log n)$ unifying the two lower bounds you have mentioned. 

Suppose that instead of the usual linear work tape the input is given in a binary tree structure with n leaves and log n depth, the initial position being the root. At every node, we can step to its parent, left child or right child. I am interested in what "tree-languages" can be accepted by what machines. More specifically, take the language where in every non-leaf we have the gate of a boolean circuit (ie. and or not), in the leaves either a constant or a variable (different for each leaf) and such a "tree-word" is in the language if and only if it has a satisfying assignment. Can this language be decided by a turing machine that has an o(log n) space worktape? Note that it is accepted by a non-deterministic finite state machine and also by a deterministic turing machine with O(log n) space. Similarly, if all the variables are constant, then it can be decided by a deterministic finite state machine. (If there are no constants, then every formula is satisfiable.) 

Consider the deterministic (resp. non-deterministic) one-way finite automaton that is defined in the usual way except that it has k heads and in each step can decide which head to move. (It is allowed to run until all heads reach the end-marker of the input.) These automata are denoted by k-DFA (resp. k-FA) and it was shown in several papers that k+1 heads are better than k, i.e., their is a language that can be recognized only with more heads. Probably the simplest of these arguments is by Yao and Rivest ($URL$ However, notice that if we allow the k-headed automata to read the input k+1 times, then it can also recognize the language given as a counterexample. (Here define reading t times as you would like to - when the first reading is finished, start the second one etc. OR run the machines in parallel t times from t different starting states and then take some boolean function of their final states.) So my question: Is there a language that can be recognized by a k+1-headed automaton but by no k-headed automaton that is allowed to read the input t times? (Here t can depend on the language but not on the input.) Note: Please do not link me to papers asking if I have seen it! I have read many related things... 

If I understand well, you are interested in the $k$-set problem: $URL$ In general their number can be $O(n^{\lfloor d/2\rfloor}k^{\lceil d/2\rceil})$ and though I could not find references now, I think you can also compute them in about this much time. 

The answer to this question is no because for any $g$ and $r$ there are $r$-uniform hypergraphs of girth $g$ with arbitrarily high chromatic number [Kostochka-Nesetril]. The girth $g$ of an $r$-uniform hypergraph is the smallest $g$ for which there are $g-1$ edges whose union has size at most $g(k-1)$. Thus, if a hypergraph has girth $g$, then any $g-1$ edges have a $1$-shallow hitting set by induction because their union looks like a tree. This means that locally it satisfies your 1-in-3 satisfiability condition, but it is not $2$-colorable, so there is no NAE-satisfying assignment for the whole hypergraph. 

The problem is NP-complete. The reduction is from the Set cover problem. Let one user has all the files $F$. Then our only hope is to achieve $\mathcal{F}(G_2)= \mathcal{F}(G_1)$. So the question is, from given $2k-1$ sets, are there $k$ that cover the ground set? This easily reduces to the decision version of the Set cover problem after adding a few dummy users/files. My old answer, which gives a polytime algorithm if we do not allow $\mathcal{F}(G_2)= \mathcal{F}(G_1)$: Yes, you can check this in P. Your condition is equivalent to that there is an $f_i$ and $f_j$ such that $f_i\in \mathcal{F}(G_1)\setminus \mathcal{F}(G_2)$ and $f_j\in \mathcal{F}(G_2)\setminus \mathcal{F}(G_1)$. So all you have to do is for each pair, $f_i, f_j$, you check whether you can divide the users appropriately. This can be done if no user has both files and at most $k$ users have any of the files. The total running time is $O(n^2k)$ (if the info is stored appropriately) which can be easily improved. 

A good old trick is that it's sufficient to give an algorithm that achieves this with $O(\log n)$ expected number of coin flips and $99\%$ probability, as if we are exceeding the expected running time a lot, we can just output anything, using Markov's inequality this won't ruin our error much. And this we can do as follows. Always keep an active interval that is supposed to contain an approximately unbiased coin (but might not). Then repeat the following. 1, Halve the size of the active interval by flipping its middle coin a couple of times. Update the active interval (if the middle coin showed no bias, then arbitrarily). 2, Flip both ends of the active interval a couple of times to check that we made no errors. If an error is detected, then go back to the previous active interval. This way, with a constant number of coin tosses we halve the size of our active interval with high probability and double it with small probability. Thus, in $O(\log n)$ expected time, we reach size $1$, which means one coin. We can check this coin with $\log n$ further coin tosses (though I'm not sure if this is necessary). 

If $$f=(\Sigma_{i=1}^n x_i)^{2^n}$$ then it has ${2^n+n-1\choose n-1} \approx 2^{n^2}$ monomials and $L(f)=O(n)$. By a counting argument, there are $2^{O(n\log n)}$ straight-line programs of length $O(n)$. As $f$ has more monomials, for some we need a longer program. In fact this argument gives a monomial $m$ for which $L(m)=\widetilde\Omega(L^2(f))$. 

With m eggs and k measurements the most floors that can be checked is exactly $$n(m,k)={k \choose 0} + {k \choose 1} + \ldots + {k \choose m},$$ (maybe $\pm 1$ depending on the exact def). Proof is trivial by induction. This expression has no closed form inverse but gives good asymptotic. 

Most puzzles that you can buy are in P, NP-complete (like Sudoku) or PSPACE-complete (like Sokoban), at least if you scale them up. Are there any natural puzzles that are PPAD-complete? What about other similar classes, like PPA, PPADS etc? If we don't insist on natural, then of course practically any PPAD-complete problem would be a puzzle, just during the puzzle you would need to run the some Turing machine a couple of times. What I have in mind would be for example finding another Hamiltonian cycle in a cubic graph, which is known to be in PPA, just now known to be complete. 

The function $\Psi$ can be computed in polynomial time from some secret random parameter $r$. $\Psi(\mathcal C)$ has a solution if and only if $\mathcal C$ has a solution. Any solution $x$ of $\Psi(\mathcal C)$ can be efficiently converted into a solution of $\mathcal C$ using $r$. Without $r$, the solution $x$ (or any other property of $\Psi(\mathcal C)$) does not give any help in solving $\mathcal C$. 

Consider full information two-player combinatorial games that end after a polynomial number of moves, and in an alternating way, the players picks from a finite number of allowed moves. The usual question is, how difficult it is to tell from a given position the winner. Another would be, how difficult it is to pick a winning move from a winning position. (Here I call a move winning, if the position remains winning after playing it.) To differentiate, I will call the former POSITION-COMPLEXITY and the latter MOVE-COMPLEXITY. It is easy to see that if the MOVE-COMPLEXITY is in $P$ or $PSPACE$, then so is the POSITION-COMPLEXITY - we can calculate the optimal moves and check who wins at the end. (I have not really thought through what happens if the MOVE-COMPLEXITY is in $NP$, probably the POSITION-COMPLEXITY is in something like $P^{NP}$.) However, there are dummy examples when the MOVE-COMPLEXITY is trivial and the POSITION-COMPLEXITY is arbitrary hard - like the (not very interesting) game of checking what is the output of an algorithm, with the players making the next steps, being allowed only one move. I have digressed a bit, my main question is the following. 

I think this is NP-complete, here I try to give a reduction from 1-in-3-SAT, where we want every clause to contain exactly one true literal. Suppose we are given a 3-CNF with $n$ variables and $m$ clauses, from this we construct a DAG. The first level consists of the source $s$. From here you have an edge of capacity $m$ to $v_i$ for each variable $x_i$. The third level has vertices of the form $x_i$ and $\bar x_i$, with an edge of capacity $0$ or at least $m$ from the corresponding $v_i$. On the fourth level, there is a vertex $c_j$ for every clause with an edge of capacity $0$ or at least $1$ from its literals. We also have an extra vertex, $w$ with edges of capacity $m$ from every literal vertex. Finally, on the fifth level we have the target, $t$ with an edge of capacity $1$ from each $c_j$ and an edge of capacity $mn-m$ from $w$. In this graph you have a flow of size $mn$ if and only if there is a 1-in-3-SAT assignment. 

Strictly speaking, PSPACE contains decision problems, while the others are search problems, so they live in different zoos, but FPSPACE would of course contain all the other problems. Then PPAD is contained in both PPA and PPP, whose relationship is unknown, but were separated by an oracle in Beame et al: Relative complexity of NP search problems. Here the PLS class was not considered, that was separated by an oracle later in Morioka: Relative complexity of local search heuristics and the iteration principle from all the other three classes. (For me this latter paper was too hard to read.) Without oracles, nothing is known. 

Suppose I have $n$ variables $x_1,\ldots,x_n$ that satisfy some inequalities that are highly symmetric, e.g., for all $S\subset [n], |S|=k$ we have $\sum_{i\in S} f(x_i,k)\le \sum_{i\in [n]} g(x_i,k)$, where $f$ and $g$ are some simple functions, e.g., composed of addition, minimum etc. Is there a polynomial algorithm to decide whether this system has an integer solution? What if the situation is a bit more complex, like we have several inequalities of the above form and/or additional individual inequalities, e.g., $x_i\le b_i$? Motivation: Many classic problems can be brought to the above form, e.g., $URL$ and $URL$ For example in Erdos-Gallai we have $f=x_i+\min(x_i,k)-(k-1)$ and $g=\min(x_i,k)$. Seems related but I don't see a direct connection: $URL$ 

I think that SUBSET SUM reduces to this problem. Take an instance of subset sum, $a_i$, $b$, and for simplicity suppose every number has $n$ digits (possibly starting with zeros). Now we will make all the numbers much longer. We modify $b$ so that it starts with "n"00001000010000100001... where "n" is the base two representation of n, then it is followed by n zeros, a 1, n zeros, a 1 and so on n times. We also modify each $a_i$ so that it starts with 10000000000000010000.... where the second 1 is at the $in$-th position. We also add the extra numbers, $a_i$' whose beginning is the same as the new $a_i$'s, but then it ends with all zeros (so it contains only two 1's). Now I claim that if $b$ has a integer combination with these new $a_i$, then the coefficient of each of them has to be 0 or 1. This follows from the fact that the sum of their coefficients is $n$, plus the $i$-th 1 in $b$ can only be obtained from $a_i$ or $a_i$', exactly one of them must be used. Also, from this combination we can get back the original (same coeffs) and from the original we can get this (if $a_i$ has coeff 0, then take $a_i$' with coeff 1). 

Define LOGLOG as the class of languages which can be computed in space O(loglog n) by a deterministic Turing machine (with two-way access to the input). Similarly define NLOGLOG as the class of languages which can be computed in space O(log log n) by a non-deterministic Turing machine (with two-way access to the input). Is it really not known that these classes differ? I could only find some older surveys and a theorem that if they equal then L=NL (which is not just a trivial padding argument!), but somehow I feel that separating these classes cannot be that hard. Of course I might be completely wrong, but if every second bit of the input is the numbers from 1 to n in increasing order in binary, separated by some symbols, then the machines can already learn loglog n and with every other second bit we can input a problem that can fool a deterministic machine but not a non-deterministic one. I don't see yet exactly how this could be done but feels like a possible approach, as with this trick we can basically input a depth log n binary tree along with its structure instead of the usual linear tape. 

It is still NP-complete. Here is a very sketchy reduction from subset sum. The goal of the whole reduction will be to make $T$ unimportant. If the inputs for subset sum are $a_i$, then add $a_i$ and $2^Na_i$ to our inputs, where $N$ is large, but still $poly(n)$. Set $S$ to be $2^N$ times the original subset sum plus $2^N-1$. Set $T$ to be the product of the $a_i$'s multiplied by a sufficiently big power of $2$, to be determined later. The idea is that if the number with index $i$ was not picked, then we can pick $a_i$, while if it was picked, we can pick $2^Na_i$. Finally, add a lot of each small power of $2$ to our input to be able to make the desired sum and product. Notice that this gives us enough liberty to get $S$ and $T$.