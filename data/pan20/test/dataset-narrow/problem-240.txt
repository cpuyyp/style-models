If you are rebuilding a clustered index, you are essentially rebuilding the table, but a non-clustered index is just a subset of the table (usually). If you use the SORT_IN_TEMPDB option, you will offload some of the logging to the tempdb transaction log. Version will also matter, in 2005 online rebuilding is minimally logged but was changed back to fully logged in 2008. You might also want to take a look at this article which might be useful in deciding whether to rebuild or reorganize with the log size in mind. ----Update---- Using Shanky's test setup I performed the same steps, but I added a few extra size checks. Before the rebuild, I perform a log backup and check the log space and utilization as well as the records in the transaction log for my index. Then I do the rebuild test and recheck the log size and the number of log records: 

Why are you trying to copy the secondary? Why can't you just use the backup from the primary to restore to your third location, that would make the most sense since this is obviously a copy of another database since you are restoring logs. I find it odd you'd have log backups and not access to a full backup. Having said that, if this is the only database on the instance or you have a maintenance window where there are no users, you could just stop the services and copy the mdf(s)/ldf(s) to your target destination and then start the services again. You should then be able to attach those files to another server or to the same server under a different name and obviously different file location. I tried both methods and it seemed to work fine. 

It seems I rarely ever see this recommendation, but Log Shipping is a great method to migrate large databases. If you combine that with turning compression on for the instance (it sounds like you have 2012 since you mentioned Always On), it should be a piece of cake. I've used log shipping to migrate a 2 TB database and several smaller ones. A few caveats: your database must be in full recovery in order to take the log backups and you'll need to create a share on the host machine or another system that both systems can access to copy the backup files. It might be a good idea to test this with a small test database first to make sure you know how it works, but it is pretty straight forward. 

I tried this about a dozen times and each time I get the same results, the DMV is only telling you for the transaction, but there are several nested transactions that are spawned as a result of the log_test transaction that don't show up including the allocation of extents and pages and the insertion of the data. If you look at the actual contents of the log file, you can see how the offline rebuild is very efficient. It just allocates the pages/extents, formats them and sets them and then deallocates the old ones. An online rebuild is doing a lot more work since it has to keep the index available while it rebuilds. This is probably more evident in the locking: offline it locks the entire object but online it has to go page by page, key by key. You can take a look for yourself to compare: 

In order to "downgrade" you'd have to uninstall enterprise and install standard. There is no in-place downgrade in the versions I've installed (2005+). As mentioned, you'd want to look at the sys.dm_db_persisted_sku_features for each database, it is not an instance-level dmv. You might want to look at this a write-up about two methods of downgrading. You'll also probably want to use that dmv in combination with Aaron Bertrand's sp_foreachdb because sp_msforeachdb can miss some databases. 

There are a few causes of this, the top ones are: storage subsystem, bad t-sql, missing/bad indexes. Paul Randall talks about this on his blog and at the sqlperformance blog. I like the scripts from Glenn Berry to find missing indexes and also SP_BlitzIndex. How do you know if you can trust the recommendations? Well I examine the existing indexes first to see what I have and if they can be altered to accommodate the missing columns. I also like to look in the plan cache to get an idea of what some of the top queries are on that table to see if it would actually make a difference. If you have a test/dev instance, you can implement there first which will give you an idea of the size any possibly usage pattern if you can generate a workload. If you are going right into prod, monitor sys.dm_db_index_usage_stats (check the index reads/writes query in Glenn Berry's DMV queries)and check the reads/writes to ensure it is used. You could also use SQL Sentry's Plan Explorer, but I think you need the pro version for the indexes. I'm sure someone has a better way to do it, but this has been working for me so far. 

You can use this to look in the plan cache. Like @Kris Gruttemeyer mentioned, the execution plan just shows you how the optimizer interprets your query. The plan itself doesn't cause bad data, but it can help you find where things went wrong. For instance, an implicit conversion can occasionally cause inaccurate results due to loss of precision or bad date conversions. You might also want to take a look at this article from Aaron Bertrand about why things in Management Studio don't always work the same in an app. It sounds like that is a component of your problem based on what you described. The article talks about the execution plan and performance, but does show you that query execution in management studio isn't always the same as in your app due to environmental settings and parameter sniffing. There is also an article about setting environment variables and getting different results from Microsoft. 

It sounds like you need to create a credential and then use that with a proxy account. I had that issue when I was running a remote script via the agent and the credential/proxy resolved the problem for me. You can refer to this post where someone had a similar issue/solution. Note the caveats about permissions, you could use your account for the credential, but if this is meant to run in the event you leave you would be well served to use a service account and grant it the needed permissions. 

Refer to the link Aaron Bertrand provided. There are a limited set of operations that are minimally logged, but reorg/rebuild aka defragmenting are not included. If you wish to minimize the effect, you'd have to do incremental reorgs as mentioned by Cibi or just allocate the needed space if this is expected to occur regularly. I've had to do this before in production with incremental reorgs (start reorg, stop, backup log, start reorg, stop, backup log, rinse/repeat). This might be a good time to check if you need all the indexes that are giving you grief. Glenn Berry's diagnostic queries can help you find unused indexes which are being unnecessarily updated (read as fragmented). You can also try sp_blitzindex from Kendra Little which gives you loads of details on your indexes.