Note that this is not a case against managed services. A hand of them are able to combine ease of use together with flexibility and functionality. But many of these service trade ease of use for flexibility and functionality: we can use them as an expedient to quickly build a simple functionality but they will prevent us to walk beyond that point. We need to be aware of this trade-off and consider. 

The word container refers to a lightweight virtualisation technology available on modern Linux kernels, this technology is very similar to FreeBSD jails. An older, non-container-able, Linux kernel is able to run processes concurrently. Some attributes of the system are private to process, like the process environment or the process memory: only the process owning these attributes and the operating system itself are able to access to this data. (There is a lot of loopholes, like some ps implementations, but that's essentially true!) Some other attributes are shared among the processes, like the filesystem and network interfaces for instance. A modern, container-able, Linux kernel is able to handle more attributes of the system as private data associated to a process or a group of processes. The resulting context is a container and instead of running a program in the “initial containers” using the filesystem and the network interfaces initialised by the operating system, it is possible to run processes in other containers, so that they see a different filesystem and a different list of network interfaces. Therefore, two processes running in distinct containers only really share the kernel. You are maybe familiar with the chroot command which can run a process in a distinct file-hierarchy, containers take the idea a few step further. Of course, this is just a very coarse explanation, but I hope it helps to clarify the idea of what containers are. Now, what are they good for? A popular interface to the container capabilities of Linux kernels is implemented by docker, a command-line utility that can be used to produce artefacts representing file-systems (docker images) and run processes in containers where these file-systems are accessible. This software suite is also able to build ad-hoc virtual networking systems to let several containers communicate on a private network. Container-based technologies are convenient to: 

When I see this, I happen to really be in my comfort zone. This kind of simple meta-programming for declarative languages is exactly the same theoretical paradigm as BSD Makefiles! Which I happen to have programmed extensively This excerpt shows us that the promise of working with YAML file is broken (so I cannot run my playbooks through a YAML parser, e.g.). It also shows us that Ansible must discuss the subtle art of evaluation order: we have to decide if variables are expanded at the “declarative part” of the language or at the “imperative” meta-part of the language. Here shell programming is simpler, there is no meta-programming, aside from explicit or external-script sourcing. The hypothetical equivalent shell excerpt would be 

This shift is concomitant of an increasing importance of continuous integration and deliveries methods. Please note that these ideas are much older than the formalisation of “agile” methodologies, some of them are already present in the NATO Scientific Committee report about software engineering published in … 1968! While the wording can be overlooked, the short description of the DevOps proposal (this site) mentions “software engineers”: 

and so on. The advice of binding several commands with has only a limited scope. It is much easier to write with scripts, where you can use functions, etc. to avoid redundancy or for documentation purposes. People interested by pre-processors and willing to avoid the small overhead caused by the steps and are actually generating on-the-fly a Dockerfile where the 

These two problems are essentially orthogonal, which means that given the deployment problem to solve, one can use one of them, both of them, or maybe even none of them, as part of the solution. When using both of them, the Debian package is used in the production of the Docker image, and your Dockerfile (the recipes used to prepare the Docker image describing the “virtualised system” ran in a container) would essentially register your Debian repository in the sources of the Debian packaging system and install your package. With this in mind, it seems to me that what you are really looking for is to implement the immutable server pattern. The recent development in cloud technologies made possible to upgrade software not by using the classical software upgrade system from a software package system (such as the Debian packaging system) but rather by simply replacing the full server at once. (Some persons did this before this development by having three OS-s on a server, two used in alternance to run the appliance and a mini-OS dedicated to performing the appliance replacement. While not overly complex, this seems to have always remained a niche.) This technique can be of interest for you because if you are used to upgrade software on your server using the package manager, the final state of the server depends of the “upgrade history” of the server – especially if errors occur in the upgrade process. This heterogeneity is bad, because it makes production problems hard to reproduce and diagnose, and your mixed experience 

The current organisation of the code and configuration you describe is structured by the technical solutions involved. This is a bad design that will add a lot of overhead in our maintenance activities and will add a lot of traps in our way as well. Instead, that organisation should be structured around the artefacts we are deploying. The reason for this is that we want to consider artefacts (e.g. a docker image or a software package) as the objects of the following verbs: 

Aside from a handful of standard situations readily implemented by available modules, I will have to feed the bits implementing the test myself, which will quite probably involve some shell commands. Checking for the conformity of installations might not be very relevant in the context where the immutable server pattern is implemented: where all systems running are typically spawned from a master image (instance image or docker image for instance) and never updated – they are replaced by new instead. 

Now if you are looking for a software engineer that can help your team getting started with these methodologies, engage them on this topic. Even if you are not familiar with this topic, following the “You build it! You run it!” slogan you can start from a programming problem and lead the interview towards question about deployment and maintenance of the application. 

(A more realistic example would build some software with the compiler instead of merely asserting its presence with the flag.) The Dockerfile snippet creates three layers, the first one contains the full gcc suite so that even if it is not present in the final filesystem the corresponding data is still part of the image in same manner and need to be downloaded, uploaded and unpacked whenever the final image is. The -idiom is a common form in functional programming to isolate resource ownership and resource releasing from the logic using it. It is easy to transpose this idiom to shell-scripting, and we can rephrase the previous commands as the following script, to be used with as in Advice #2. 

The difference is still probably not very meaningful. On that page we also discover that Ansible has a template meta-Programming language 

Now what are the strength of Debian packages over Docker images as a package system? The tight control over dependencies at installation. (The possibility to upgrade and downgrade also exists but has no practical importance if we are implementing the immutable-server pattern.) This leads to the Conclusion If you only have a single product deployed in a single version (which is typical for SaaS), your version management needs are very simple and using Docker as a ad hoc package manager should not have any hard drawbacks. As soon as you work with several versions of a single product or several products, the complexity of the version constraints problem you need to solve increases and you need an appropriate tool for this, which might be Debian packages or some configuration management system if you are mixing software from different origins. 

This has several consequences for how images should be built. The first and most important advice I can give is: 

could relate to this. The immutable server pattern wipes this source of errors by essentially destroying the notion of “upgrade history” from the problem. Now there are various options to implement the immutable server pattern, two popular choices are to use Docker images, images or to use “master instance images” from your cloud provider (these are called AMIs in AWS and just Custom Images in Google Compute Engine). Your use-case forbids the use of cloud based techniques, I will therefore assume Docker images as the only eligible choice. (For the sake of completion, it is certainly possible to use other approaches, for instance using Virtual Box or similar virtualisation solution, as an alternative to Docker.) When using the immutable server pattern technique, you introduce a new artefact (the Docker image) representing your server and this artefact can be tested as well, and it is very easy to obtain a setup replicating truthfully your production settings – aside from service load. Now to consider the concrete problem you described, let's assume implementing the immutable server pattern with Docker is actually what you want. Since the Docker system and the Debian packaging system are complementary rather than mutually exclusive (cf. intro) we still have to address the question if you should you prepare a Debian package for your software. The pertinence of using a Debian package to install your software (in the Docker image or on a host) lies in the complexity of the versioning problem you have to solve. If you run at the same time several versions of your software, occasionally need to downgrade, and have complex version requirements that you need to carefully document, having a Debian package is a must-be. Otherwise, this step can be skipped – but since you already put an effort to produce and deploy these packages, there is no real value into ditching your work. I would therefore suggest continue to produce your Debian packages.