(Adding an answer so the information is here, too) keeps some state over and above the underlying system. This results in information set in one that the other doesn't know about. Years ago when I still used , I found myself doing update checks in both because I was getting confusing results. Using exclusively would hide this problem, but as soon as you something else, it becomes apparatnt. Solution: either use only or never use . 

Load-balancing a MySQL database environment can be done, but it is not done by putting a LB device between your app servers and your database servers. This is exactly the wrong place to do it. The problem with doing it at the MySQL connection level is one of knowledge. It is the very nature of SQL to target a single piece of storage, the 'database'. The raw SQL does not have enough information about the relationship of different types of data, nor the required validity of data. Instead, it has to make guesses about why the unknown data is being accessed by seeing how it is being accessed. This is how MySQL Proxy works. And it still requires additional configuration of the database instances themselves. True cluster-aware load balancing requires the database instances to be able to talk to each other to sort out serialization of access and knowledge about comparative loads on the database engine and on the storage level. I haven't worked on them, but as far as I know, this is the type of thing DB2 and Oracle do in a clustered setup. MySQL does not - it doesn't have the architecture designed to do it. So you have to load balancing higher up: in the application, before the SQL is sent out. This is a much better place to do it because that is where all the knowledge about data relevancy actually is. This is where the code can know it needs to look in one of the databases over there for this table, and one of the databases over here for this other table. It can know that it can read this database for this table, because it's a replicant, but it must write to that one over there. It can even make decisions about how old the data can be before it's not useful anymore, permitting intelligent caching strategies. It also means many many database instances. This is what LiveJournal figured out over several years. The downside to this approach is what you lose. No (or few) stored procedures, constraints are difficult, reporting across disparate databases is difficult. It makes your sophisticated SQL engines simple storage. But they're good at that! It also means either your DBA has to become a DB programmer or your DB programmer has to become a DBA. Been there. Done that. I did this scalability in the object layer. It worked really really well. It was just hugely unconventional. 

This asks foo.bar 100 times for URI /test with fixed rate 10 connection per second. If you want to ask different URI's, run many httperf instances in parallel mode with different --uri parameters. 

I've 2 remote HP Proliant DL180 G6 servers with HP Integrated Lights Out enabled on both and I have to install RHEL5 on these servers. I try to do it with ILO - mounted ISO file from local hard drive as Virtual CD-ROM, and performed installation from ILO console. OS installation on the first server was successful. But I have to make 3 tries to achive this. The channel between me and the servers is too slow - they are in datacenter. The installation takes a long time - 5-6 hours. So sometimes Virtual CD accidently unmounts and installation hangs. How can I perform the installation on the other server in more convinient way? 

I have a 500 GB hard drive with one NTFS-partition on it. I can mount it with Ubuntu and view the contents. But when I try to copy something, I get an I/O error. Ok, I tried to make its image with . I/O error as soon as it starts. I have installed , but its manual page says not to use it with drives, failing on I/O. Can I manage to get some information from this drive and how to do this? 

Just to be complete, Courier fits all those requirements. All (well, most) of it's bits'n'pieces need to be explicitly turned on, so you can install it, and only enable the MTA. Courier also supports MSA mode, which means is an MTA listening on a different port and with some relaxed rules for accepting mail. That's almost enough for you right out-of-the-box. 

This gives me the current user, current host current directory (without replacing $HOME with ~), current window and last error return. Since I normally have 16 or more screen windows open, knowing the current one is useful. 

You can do this with Debian. I believe it supports what is called a chroot install. Beyond that, I can't help as I've only heard about it from a friend. 

Presuming you have a dual-boot arrangement, then, yes, you can have MySQL using the same database files, as the on-disk structure is identical. You will need to read the configuration documentation very carefully, though, as it is easy to think you've pointed MySQL completely to a non-default directory, but something's been left behind. You also need to be careful of the case of tables and files, as Windows can't handle two filenames in the same directory with different case of names. Your best bet is to understand the case-folding options for the Windows version, and to always always always use lowercase tablenames. 

Maybe some program is locking this file? It can be another copy of your program. Does show anything? 

I think you have some kind of MTU problem. It happens when you send packet larger then minimum MTU on the network path (PMTU) with bit set, and ICMP error message is somewhere blocked. You should check local and remote firewalls to allow ICMP first. Then trace path to see what the PMTU is and where package drop can occure. Turn on bit! You should do this on the same port you use for client-server communications. Use hping2, for example. If nothing helped, turn off PMTU discovery on both machines. 

I want all the files in directory be owned by nginx.devel. I have performed once and update these files using . But if I create a new file and then it, it will be owned by user.user and I need to run with privileges on it. 

I want to figure out some problems I faced performing the task. First, I had to mount .iso from Settings menu and put the tic "Connect on power on" - it was unchecked by default, and VM tried to load from net. Then, ESXi 4.0 has no clone fascility. You need to manually copy all the files from folder VM1 to VM2, cd VM2, right mouse click on .vmx file - 'Add to invertory'. Change the hostname and IP of the second VM - its all! 

It sounds like restarting the master wipes out the binary logs, for some reason. MySQL won't do that by itself; it is either receiving the command at some point, or something in the server's startup is clearing out . 

If you have trouble re-configuring your shell again, you could use GNU screen. If your SSH session times out, just login again and reconnect and your shell will be just as it was. 

As @zecrazytux indicated, did you install the appropriate apache-dev packages? PHP could well have built without them, even though you told it where they were. You need to look at the ./configure output. 

In theory it should be the same, but in practice you may encounter applications that assume there is always a drive letter. These sort of apps make it difficult to use a UNC. Older apps written with once popular toolkits (e.g. Borland's widget set) are a good example. Without access or knowledge to the Windows source code, this is difficult to answer. 

Other ways to tackle this problem involve modifying slaves and failing the application over. This is more closely tied up with how your databases replicate. I also haven't done this one myself, so I can't describe exact steps. Finally, there are a dozen server settings you can twiddle and several more that are much harder to change that will affect how long it takes to copy a table. The sort buffer is one, but also how much memory MySQL is allowed to use is another. (Remember that you can set a lot of those per connection, too, rather than setting some of them high globally.) When dealing with a lot of data, MySQL has a 'tipping point' effect where things are fairly linear up to a certain size, and then go to hell suddenly. It often comes up with complex queries working with a lot of data and is related to internal temporary table sizes and how much memory it is allowed to use, but it can come up with table alterations because they involve re-indexing the data. That is one reason why giving a database more memory is almost always a good thing. 

I need to monitor a single process (e.g. be warned when there are more than 3000 connections established) and collect statistics on it (e.g. to determine how many connections were established today 01:20 AM, when the server worked too slow, as client said). What tools should I use? 

I ssh on remote host but terminal performance is poor. Symbols I am typing are not shown immediately, but with some delay. Sometimes two symbols are shown at one time after delay. 

You can login to EC2 instance with the same access key you use for default user. Suppose, you are running Ubuntu and default username is . Then your public key is located in Create new user as described in AWS documentation and add public key to its . Now you should be able to login to you instance with original key pair. 

I think you should start of monitoring your memory usage/cpu usage and then monitor your server network activity. Try this tutorial. It describes the main linux monitoring tools that will help you. 

Account will expire at the end of 2009-07-03. Logged in user won't be kicked out. He won't be able to log in after he logout. 

I want to connect to VPN every time goes up. I have already a script to do connect to VPN. How can I run it on interface up event on Fedora Core 12?