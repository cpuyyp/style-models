In policy iteration, you define a starting policy and iterate towards the best one, by estimating the state value associated with the policy, and making changes to action choices. So the policy is explicitly stored and tracked on each major step. After each iteration of the policy, you re-calculate the value function for that policy to within a certain precision. That means you also work with value functions that measure actual policies. If you halted the iteration just after the value estimate, you would have a non-optimal policy and the value function for that policy. In value iteration, you implicitly solve for the state values under an ideal policy. There is no need to define an actual policy during the iterations, you can derive it at the end from the values that you calculate. You could if you wish, after any iteration, use the state values to determine what "current" policy is predicted. The values will likely not approximate the value function for that predicted policy, although towards the end they will probably be close. 

or some permutation. This is the same number of dimensions as the input, no additional dimensions are added by the convolutional layers. Each feature map channel in the output of a CNN layer is a "flattened" 2D array created by adding the results of multiple 2D kernels (one for each channel in the input layer). Usually even greyscale input images are expected to be represented as so that they fit the same pattern and the same layer model can be used. It is entirely feasible to build a layer design which converts a standard 2D+channels input layer into a 3D+channels layer. It is not something I have seen done before, but you can never rule out that it could be useful in a specific problem. You may also see 3D+channels convolutions in CNNs applied to video, but in that case, the structure will be some variation of 

In a neural network model, you can use autoencoders. The basic idea of an autoencoder is to learn a hidden layer of features by creating a network that simply copies the input vector at the output. So the training features and training "labels" are initially identical, no supervised labels are required. This can work using a classic triangular network architecture with progressively smaller layers that capture a compressed and hopefully useful set of derived features. The network's hidden layers learn representations based on the larger unsupervised data set. These layers can then be used to initialise a regular supervised learning network to be trained using the actual labels. A similar idea is pre-training layers using a Restricted Boltzmann Machine, which can be used in a very similar way, although based on different principles. 

Actually that is a trivial number of calculations for a modern CPU, and would be done in less than a millisecond. But multiply this out by a few factors: 

Your hypothesis about missing colours in your samples affecting results in production could be correct. However, it is trivial to convert images to greyscale as you load them from storage. So keep them in colour, and convert them as you load them if you need black and white. That way you can try both with and without colour as input and you will have your answer. This is very little effort to do in practice, and allows you to do the "science" part of data science by comparing two approaches and measuring the difference. This is standard practice, even if you are reasonably certain one approach or another is "the best", it is normal to explore a few variations. When you are not sure, then it is even more important to try it and see. To test your hypothesis, you could put all the t-shirts of a particular colour in your test set. If that reduces the accuracy of your results with the colour model, it would back up your concern. One fix for that might be to remove colour information from the model, if it is not relevant to the task. An alternative is to collect more data so you have enough samples of different colour shirts. However, you might find if you are fine-tuning a neural network trained on many more images (e.g. Inception v5) that the impact of colour is less even though your samples do not cover all possible T-shirt colours. 

The update is assuming a static distribution and estimating the average value. As each estimate is made available, it is weighted less of the total each time. The formula means that the first sample is weighted $1$, second $\frac{1}{2}$, third $\frac{1}{3}$ which is what you need to get the mean value when you apply the changes due to the samples serially whilst maintaining the best estimate of the mean at each step. This is a little odd in my experience of RL, because it assumes the bootstrap values (the max over next step) come from a final distribution to weight everything equally like this. But I think it is OK due to working back from final step, hence each bootstrap value should be fully estimated before going backwards to previous time step. 

In summary, it is possible your idea would work quite well in an online system with near-immediate feedback. In that case you could think of a set of weights as being "rules to update a belief state from data", and the output of hidden layer neurons as being "a current belief state". When errors occur, it does appear to make sense to update both the rules that led to the error and the current belief that resulted from earlier faulty rules. It is perhaps worth an experiment or two. The main caveat is that the two update processes (for weights and LSTM layer state) would interact and/or adapt to each other, so it may not lead to measurably different performance than just adding more LSTM cells to the layer. 

The subject areas Artifical Intelligence and Machine Learning (plus Data Science) are loosely defined, such that it is hard to make strict statements about how they relate. In the general case, it seems that there are parts that overlap, but that they are quite far from being "the same subject with two different names" as suggested in the question. The term Artifical Intelligence has many possible meanings and interpretations - which version to refer varies by time and by the source using it. Textbooks on artificial intelligence will often cover topics such as search algorithms, logical deduction and other things which are clearly not machine learning as it is practised today. For instance, we could take it to refer to Artificial General Intelligence (or "hard AI"), and it should be clear in this case that at least some form of learning algorithm(s) would be required to meet the goals of AGI. However, it is far less clear how much of AGI can be solved by combining machine learning into complex structures. The term Machine Learning has a few different working definitions, but this is a popular one: 

Sorry about ad-hoc/wrong notation. I understand now that $\nabla a_j^{(k+1)}$ is more properly written $\nabla_{a_j^{(k+1)}}E$ thanks to Ehsan's answer. What I really wanted there is a short reference variable to substitute into the equations, as opposed to the verbose partial derivatives. 

* Technically off-policy learning is learning the cost function for any policy $\pi$ from a different behaviour policy $b$, but the common case is for control systems attempting to find an optimal policy from an exploratory policy. 

If your classes arre not mutually exlcusive, then you just have multiple sigmoid outputs (instead of softmax function as seen in example MNIST classifiers). Each output will be a separate probability that the network assigns to membership in that class. For a matching loss function, in TensorFlow, you could use the built-in tf.nn.sigmoid_cross_entropy_with_logits - note that it works on the logits - the inputs to the sigmoid function - for efficiency. The link explains the maths involved. You will still want a sigmoid function on the output layer too, for when you read off the predictions, but you apply the loss function above to the input of the sigmoid function. Note this is not a requirement of your problem, you can easily write a loss function that works from the sigmoid outputs, just the TensorFlow built-in has been written differently to get a small speed boost. 

In practice, I have not found Keras' design difficult to use for deep networks. I typically write a separate build function, and parametrise a few things, such as input dimensions. However, I typically don't loop through a list of different layer sizes as the main param of the build function. Instead, I find the more verbose approach just fine, even when trying variations of network size/shape (I guess that might change if I wanted to grid search including layer sizes). I think that is because I find the function names very easy to read - even with a screen full of functions, I can see quite quickly what the NN structure is. 

Your weights have diverged during training, and the network as a result is essentially broken. As it consists of ReLUs, I expect the huge loss in the first epoch caused an update which has zeroed out most of the ReLU activations. This is known as the dying ReLU problem, although the issue here is not necessarily the choice of ReLU, you will probably get similar problems with other activations in the hidden layers. You need to tone down some of the numbers that might be causing such a large initial loss, and maybe also make the weight updates smaller: 

This makes no difference to the performance of your network, but I assume you have done this in the rest of the answer. Scaling Input to Normalised Values One clue to performance problems was in your first version which included the code: 

The equation that you show calculates (the negative of) a gradient to the objective function. The value $\delta_j$ is the rate of change of the objective function for an isolated change of output of a neuron before applying the activation function for the neuron. The factor of $fâ€²(net_i)$ is required because it is correct - it is due to the chain rule applied twice - once due to the linear relationship linking weights between layers, and again to get the gradient pre-activation function in the next layer down. An interpretation of $\delta_i$ is "the (negative) gradient of objective function with respect to a pre-activation value of a neuron the network". If, somehow, the pre-activation value could be changed - ignoring how it could be changed - then $\delta_i$ tells you the linear scale of the impact on the objective function, at least for very small changes. This value is not normally used directly in weight updates, but is fed deeper backwards into the networks layers if there are yet lower layers to update, because it can be used for weight updates in those layers using the same rule. The property of low gradients associated with strong activations is because you previously chose as an activation function, and is not because of the way you calculate the gradient. Once you chose , you were not really free to choose a different backpropagation gradient calculation. Or rather, there is not much to gain - and potentially a lot to lose - by doing so. 

If you have to predict $y$ only given $a,b,c$, then as a first look just discard $d,e$ etc from other data sets*. Any kind of regression might be useful, you will have to explore that - you might as well start with linear regression as it is simplest. In general, if you are not sure about which algorithm to apply for best results, you should set up an experiment to try multiple models: 

The formula you have quoted is a bit unwieldy, precisely because the R function as defined needs to "look ahead" to all possible outcomes and their probabilities, but how to do that is not included explicitly. There are actually a few variants of the Bellman equation that express more or less detail. A good place to start for a truly generic version is Sutton & Barto (2nd Edition): $$v^*(s) = \text{max}_{a \in \mathcal{A}}[\sum_{r,s'}p(r,s'|s,a)(r + v^*(s')]$$ Where $\sum_{r,s'}$ is over all possible reward and next state pairs. The above equation changes your transition function that only handles next state, to a similar function that handles successor state and reward: $$p(r,s'|s,a) = Pr\{ R_{t+1} = r, S_{t+1} = s' | S_{t} = s, A_{t} = a \}$$ Usually this does not increase the number of items to sum over, or add much complexity, because reward will most often associated with the transition. The benefit is that this approach removes the need for a reward function that works with expected reward, and just works with specific rewards. Other variants are possible too, such as an expected reward function based on $(s,a)$ or $(s,a,s')$ - the difference is just a little bit of juggling with the expression so that it remains effectively the same given the subtle differences in definition for $r$. 

One more piece of advice about your estimator: You are expecting a large amount of variance in your data, and will not have a lot of samples. So to avoid over-fitting you will want to have a relatively simple ML model. For a neural network for example, you will probably want only one hidden layer with very few neurons in it (e.g. 4 or 5 might be enough). Finding a model sophisticated enough to predict a curve, but simple enough that it doesn't overfit given very noisy target outputs might take a few tries - this is the main reason why I suggest performing trial runs with simulated data. 

When considering performance of these architectures in general, you have to allow that some problems will make use of these strengths better, or it may be a wash. For instance, in a problem where forwarding the layer output between time steps is already a good state representation and feature representation, then there is little need for the additional internal state of the LSTM. In effect the choice between LSTM and GRU is yet another hyperparameter to consider when searching for a good solution, and like most other hyperparameters, there is no strong theory to guide an a priori selection. 

As MountainCar is often solved with $\gamma = 1$ and a negative reward per timestep, you would immediately hit a problem with your ability to calculate a maximum action value in that case. However, I don't think that is your problem here, the discounted return with positive reward at the end should still encourage desired behaviour for the problem. It is likely that you are experiencing known problems with RL, and the "deadly triad":