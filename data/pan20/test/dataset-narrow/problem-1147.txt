If you do not abide by this principle, then you have to suggest an alternative mathematical criterion of correctness. I do not know of one. Example: representation of natural numbers For natural numbers the relevant structure is described by Peano axioms, and the crucial axiom that has to be implemented is induction (but also $0$, successor, $+$ and $\times$). We can compute, using realizability, what the implementation of induction does. It turns out to be a map (where is the yet unknown datatype which represents natural numbers) 

In general semantics is a mapping $[\![ {-} ]\!]$ of syntax to mathematical objects of some sort. The objects may be syntactic in nature, in which case it is perhaps better to speak of a translation. Some kinds of semantics are clearly not syntax. For example, if you interpret the simply typed $\lambda$-calculus into set theory, then it is not reasonable to claim that this is just a translation of one kind of syntax into another. For instance, in the set-theoretic semantics any set may act as a type, even a non-definable one, and even some definable types will have uncountable cardinality (consider $\mathtt{nat} \to \mathtt{nat}$) –– it would be absurd to claim that these are just syntax. One should not confuse semantics with how we express the semantic function. Of course, anything you express in mathematics is "just a bunch of expressions". But it's been a long time since we learned the difference between a word and its meaning. If you are going to defend the position that "it's all just formalism all the way down" then we have a different issue to discuss, namely: are you a formalist? 

For all we know, we are just a rather large finite automaton, capable of contemplating fictional super-heroes called "Turing machines" that are able to calculate with unbounded numbers (gasp!). If this is the case, Gödel was just a very good story-teller. How his stories translate to effectivity is then a matter of some (necessarily inaccurate) application of imagination to reality. Because incompleteness phenomena arise naturally in many contexts, and certainly in all reasonable notions of computability, we conclude that the same has to be the case for effectivity. For example, suppose we could send Turing machines into black holes to compute a la Joel Hamkin's infinite-time Turing machines. This gives us immense computational power in which the halting oracle is a kindergarten toy. But still, the model satisfies the basic conditions that allow us to show the existence of inserparable sets. And therefore once again, computation is not all-powerful and incompleteness is a fact of life. 

It looks like you want a confirmation of your opinion that classical paradoxes have something to do with the Halting Problem. This is the case on some vague level as most well-known paradoxes rely on a form of self-reference, and so do the usual proofs of the Halting Problem. But to be more precise, we would have to understand what it means "to use a paradox in solution of the Halting Problem". Have you looked at the article on Self-reference at the Stanford Encyclopedia of Phylosophy? In particular, section 2.4 seems to address your question quite well. The article also describes the mathematical ideas that can be used to treat the paradoxes. I recommend it. 

The theorem tells us that the Univalence Axiom gives us a sort of cateory theorist's dream: equivalent categories are equal. You ask whether you can reduce the Univalence axiom to a statement about categories. Attempts using skeletons won't work because there isn't a good way to say "skeletal". We could ask whether Theorem 9.4.16 implies the Univalence axiom. This is not going to be the case, as far as I can see, because a category has a $1$-type (groupoid) of objects and a $0$-type (set) of morphisms, so theorem 9.4.16 amounts to something like the Univalence axiom for 1-types, only. 

You are working on your PhD. Saying "I am not well versed in $X$" is not an excuse. And if you're good, then saying "my advisor does not know $X$" is not an excuse either. You are using monoids where you should be using categories. Your monoid operations presupposes that you can combine any $\delta$'s together. But does this really make sense, for example, how would you compose "add plastic casing" and "add metal casing"? I suppose some of your $\delta$'s result in empty relations because they make no sense. You should be suspicious of that sort of thing. As an interested observer it seems that the monoid should be a category, so we can compose two $\delta$'s only if it makes sense for them to be composed. Then your semantic evaluation is just a functor into the category of sets and relations. And then you see that there are lots of other categories that you could use. Functional deltas will correspond to a functor which maps into the category of sets and functions, the natural numbers deltoid is a functor into the monoid of polynomials on natural numbers (seen as a category), etc. I am not sure you want to formalize LaTeX and Nokia mobile phones too seriously in the general theory. But of course your theory should be applicable to such examples (just don't get hung up when you discover that mobile phones do not actually have a well-defined semantics). You are really shortchanging yourself by insisting on a predetermined technology (by your advisor?), by the looks of it. 

Yes, the initial algebra is by definition one of the members of the class for which it is initial. You may however be interested in the category-theoretic concept of a limit. Given a diagram (a collection of objects and morphisms between them), we can ask for an object which "best approximates" the diagram, but needs not be an element of the diagram. There are two such "best approximations", namely limit and colimit. The limit is like an initial algebra. 

Sometimes it pays to go back to the source. There are several ways to set up computability on general sets, of which one of the most general ones is ralizability theory. The idea of realizability theory goes back to Kleene's paper On the Interpretation of Intuitionistic Number Theory from 1945, but has since been generalized and developed into a mini-branch of computability, with a good mix of category theory, see for instance Jaap van Oosten's book "Realizability: an introduction to its categorical side" (Studies in Logic and the Foundations of Mathematics, vol. 152, Elsevier, 2008). Let me describe the idea of realizability very briefly, and discuss your "coordinate free" requirement later. Start with a model of computation, such as Turing machines, the $\lambda$-calculus, a programming language, or any other partial combinatory algebra (you can even take certain topological spacess to be "models of computation", this stuff is general). For concreteness, let us consider Turing machines. We code Turing machines by natural numbers, but note that I could have taken some other model of computation, so you should not assume that the use of $\mathbb{N}$ is in any way essential here. (Other possibilities include: the powerset of natural numbers, infinite sequences of natural numbers, the syntax of the untyped $\lambda$-calculus, certain categories of games, etc.) A computability structure on a set $X$ is given by a relation $\Vdash_X$ between $\mathbb{N}$ and $X$, called the realizability relation, such that for every $x \in X$ there $n \in \mathbb{N}$ such that $n \Vdash_X x$. We call such structures assemblies. This definition directly corresponds to the intuitive idea that some piece of data $n$ respresents, or realizes, an element $x \in X$. (For instance, certain sequences of bits represent finite lists of pairs of strings of characters.) Given two assemblies $(X, {\Vdash_X})$ and $(Y, {\Vdash_Y})$, a map $f : X \to Y$ is realized (or "computable") if there is a Turing machine $T$, such that, whenever $n \Vdash_X x$ then $T(n)$ terminates and $T(n) \Vdash_Y f(x)$. Again, this is a direct transliteration of what it means informally to "program" an abstract function $f$: the corresponding Turing machine does to representing data whatever $f$ does to the corresponding elements. Assemblies may be extended to a realizability topos. A topos is a model of higher-order intuitionistic mathematics. This tells us that every realizability topos (there is one for each model of computation) contains lots of interesting objects. For instance, it contains an object of real numbers, which thus gives us computability on reals. But it also contains many other objects, such as Hilbert spaces, Banach spaces, spaces of smooth maps, etc. You asked for some other computable structure, but you got something much better: entire mathematical worlds of computability. Since category theory and toposes can be scary and require some amount of technical proficiency in computability theory, category theory, and logic, we could also work in just one concrete topos, but we express everything in concrete non-abstract ways. A particularly good world of computation arises from Kleene's function realizability, and goes under the name of computable analysis. Let me comment on the "coordinate free" requirement: