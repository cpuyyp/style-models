Read uncommitted allows dirty reads. An lock will be taken on the row or higher level (in the data page or index) before it is made dirty. Rows accessed directly by the query when locating a row to update will take a lock and be blocked. However it is still possible for an Update to be affected by the isolation level. Connection 1 

The top branch gets all rows where the are the same and at least 2 out of the three other columns are the same. The join on is likely to be pretty selective in itself. That just leaves one other possible three column combination left which is dealt with by the second branch. Both branches of the have an equi join so it should perform better than a join with some complicated condition. 

No it isn't currently implemented. See the following connect item request. OVER clause enhancement request - DISTINCT clause for aggregate functions Another possible variant would be 

The three additional pages not shown in the output from are the database boot page and , . The write of the database boot page is correlated with the itself as the field gets updated. It would be expected that the insert would cause the other two pages to be updated (The and columns respectively) but this does not appear to happen immediately and I see no entries in the log file either so I assume this must happen periodically rather than after every modification. I'm not sure how to account for 9 writes being reported but only 8 occurring. Perhaps one page was double counted. 

I agree that there is no point in creating an column clustered index here but I would just create the composite PK as rather than . There is no point having 2 copies of the data, One in the heap that never gets used and one in the NCI. In fact it is downright counter productive as shown below. Your NCI will still get page splits anyway if you are not inserting into the table in order of primary key so you have not avoided the problem. You might also consider a constraint or index on the reversed key order as well depending upon what queries you run against that table. In general NCIs can be slightly be more compact than the equivalent NCI as they do not have the status bits B or (if no nullable columns) but in this case this is more than outweighed by the necessity to store the 8 byte RID pointer to the row in the heap as can be seen from the below. Script 

I've come across another case where / do not short circuit. The following TVF will raise a PK violation if passed as a parameter. 

then the predicate is pushed into the scan on and performance is much better. It is guaranteed by the join condition on that the two are equal so this does not change the semantics. With the example data I added to your question and an hint the original version had 

Yes. One potential benefit is that the slot array is ordered by index key order so matching row(s) on the page can be found by performing a binary search (or sometimes linear interpolation) rather than reading all rows on the page. SQL Server, Seeks, and Binary Search contains much more details about this. 

TSQL is not unusual in this respect. In C# attempting would give an error "String cannot be of zero length." This is not surprising as in any string of text you could argue that there are infinitely many empty strings. There could be 20 between the "T" and "h" at the start of this sentence. As they are zero length how could we tell? For your actual use case you can use 

As it incorrectly loads the results for L2 into a common sub expression spool then replays the result of that for the L1 result. I was curious as to why the difference in behaviour between the two queries. Trace Flag 8675 shows that the one that works enters and the one that fails enters . So I assume that the availability of additional transformation rules is behind the difference in behaviour (disabling either BuildGbApply or GenGbApplySimple seems to fix it for example). But why do the two plans for these very similar queries encounter different optimization phases? From what I've read requires at least three tables and that condition certainly isn't met in the first example. 

An insert is always within a transaction. If you don't have an explicit or then the statement runs as a self contained auto commit transaction. The trigger is always part of the transaction for the action that fires the trigger. If an error occurs in the trigger that causes transaction rollback then the firing action will be rolled back too. Triggers implicitly have on. An error with this setting on will automatically lead to transaction rollback (except for errors raised in the code with the statement). 

The table scan on emits only the column. The next compute scalar along multiplies that by 5 and outputs a column called despite the fact that this column is not even referenced in the query. The final compute scalar along takes the value of and outputs that as a column called . Using the trace flags explained in Query Optimizer Deep Dive â€“ Part 2 (disclaimer: these trace flags are undocumented/unsupported) and looking at the query 

This 2007 White Paper compares the performance for individual select/insert/delete/update and range select statements on a table organized as a clustered index vs that on a table organized as a heap with a non clustered index on the same key columns as the CI table. Generally the clustered index option performed better in the tests as there is only one structure to maintain and because there is no need for bookmark lookups. One potentially interesting case not covered by the paper would have been a comparison between a non clustered index on a heap vs a non clustered index on a clustered index. In that instance I would have expected the heap might even perform better as once at the NCI leaf level SQL Server has a RID to follow directly rather than needing to traverse the clustered index. Is anyone aware of similar formal testing that has been carried out in this area and if so what were the results? 

And loaded EmployeeBenefitData with integers from 1 to 4,000,000 (6,456 pages) And FilteredEmployee with integers from 2,000,000 AND 2,010,000 (19 pages) And then ran 6 queries of the following form 

Computed Columns can be stored in the data page in one of two ways. Either by creating them as or by including them in the clustered index definition. If they are included in the clustered index definition then even if the columns are not marked as then the values will still be stored in each row. These index key values will additionally be stored in the upper level pages. If the computed column is imprecise (e.g. ) or not verifiable as deterministic (e.g. CLR functions) then it is a requirement for the column to be marked as in order to be made part of an index key. So to give an example 

... happens to give a plan where the operators in the shaded area for the part don't get executed if the simpler predicate returns a row but this plan isn't guaranteed. 

So the question is... Why does adding additional rows with somehow affect the estimates for rows joined when ? It certainly seems counter intuitive that adding rows to a table should end up reducing the estimated number of rows from the query as a whole. 

If it is adding a row at the end of the index it will just allocate a new page for the row rather than split the current end page. Experimental evidence for this is below (uses the function which requires SQL Server 2008). See also the discussion here. 

If I explicitly try and reduce the sample size of that index's statistics to that used by the others with 

Disabling the optimizer transformation rule prevents the previously successful plan hint from succeeding too. Experimenting with greater quantities of data shows that SQL Server is certainly capable of transforming to naturally as well but I didn't see any evidence that the reverse is true. A very contrived case where the first query performs better than the second is 

This can no longer use that index and has different semantics in that it will return any rows where irrespective of what the values in the other columns are. 

and returns on input If you can live with instead of then you could just use . If you must have then you could use the following pattern 

If it isn't possible to change the mask formats to use or instead of it can obviously easily be done at run time with a single of to the desired character. 

Following the they instantly revert to the initial count but after the they remain as in the second table showing that garbage is now present. The single bucket that apparently all the PK values hash to now has 16 linked rows rather than the previous 8. And now the hash index on the other column has two buckets in use with 8 linked rows in each (for both the "before" and "after" string values of and ) Rows Expired After Commit 

The names are ordered by first. The secondary column is only used for ordering within groups of people sharing the same last name. 

Tables in SQL Server can be either organised with a clustered index or have no CI in which case they are a heap. You need to look at . Despite the name this also does analysis of heaps too (though logical fragmentation does not apply to these, pages cannot be out of logical order as there is no "correct" ordering in a heap). For tables with a CI the clustered index has an of . The leaf level of the CI is the table. For a heap this is given an of . 

is not some type of regular expression defined to just match digits. Any range in a pattern matches characters between the start and end character according to collation sort order. 

If you are on SQL Server 2008+ you can also use extended events with a histogram target. Possibly this would be more light weight than a trace. AFAIK you would need to create a different session for each database of interest though as I couldn't see any indication that bucketizing on multiple columns was possible. The quick example below filters on 

To determine whether it is a good solution we need to know the problem it is intended to solve. It turns out there is no problem to solve so no this is certainly not a good solution. If it ain't broke don't fix it. Identity columns have never guaranteed a gapless sequence. If you in fact need one then you would need to roll your own incrementing solution that blocks other transactions until each allocated range has been successfully committed. The code you posted will reseed all identity columns to 0 and then go through and reseed them all using the maximum value in the identity column? What happens if someone tries to insert rows into a table processed by the first step but not yet processed by the second? They will likely get a duplicate key violation as the insert tries to reuse identity values that have already been allocated to other rows in the table. 

but the data there is not persisted across service restarts and might not be accurate for your requirements (e.g. running will update the time even though no rows were actually deleted) 

In order to apply this change the process will need to acquire a lock and once it's got that it should be pretty instant (a metadata only change.) The only risk is that if you have a long running transaction that is using the table whilst the statement waits to acquire the lock any other subsequent requests coming in that require a lock on the table will be queued up behind it to prevent your alter request from being starved. So you could end up blocking production queries for a while until the lock is granted and the change made. 

The negative object id is the id of the object fleetingly created. Polling in a loop whilst generating the estimated plan and comparing with the TableID subsequently output can show this. The estimated plan you see for the statements referencing the temp table is of limited use however as even the addition of a single row to the temp table will trigger a statement level recompile and potentially a different plan when you actually execute the SQL. This behaviour does not apply to global temporary tables or permanent tables. I presume this is also the same reason as 

You need to rebuild the clustered index after making the columns sparse. The dropped columns still exist in the data page until you do this as can be seen with a query against or using 

These require finding all matching rows and sorting them. Because is not declared as a unique index the is ignored (as explained in More About Nonclustered Index Keys) and gets added as an index key column instead. This means that SQL Server can seek on and the matching rows will be ordered by . The plan in your question therefore avoids a sort operation but performance is entirely reliant upon how many rows in practice need to be evaluated before the first one matching the predicate is found and the range seek can stop. Of course this can vary wildly depending on how common the values contained in are and whether there is any skew in the distribution of these values with respect to (You say that both around 350k rows match the seek predicate and that around 350k end up being seeked so for it sounds like these are all at the end or perhaps no rows match at all - which would be the worse case for this plan). It would be interesting to know what the estimated number of rows coming out of the seek is. Presumably this is much less than 350,000 and thus SQL Server chooses the plan you see based on this estimated cost. If the table variable will always just have few rows you might find this rewrite works better for you. 

The answer to this question is "no". I just finished reading SQL Server 2008 Internals and the last chapter in that book written by Paul Randal on DBCC internals covers the huge variety of possible corruptions that can happen to database pages that can be detected by . You can use checksum protection on your data pages, backups, and regular checks for corruption with Your plan of writing and re-reading data seems somewhat pointless as unless you are continually running to ensure all dirty pages are written to disc and then you will likely just get handed the page straight from memory anyway. 

and use the histogram to estimate the number of rows with values in that range. This applies only to the cardinality estimation however. is preferable as it can use a range seek at runtime. or can't. 

The 6 permutations were made up by inverting the order of the two tables and trying all three join types , , . Results were as follows 

This is certainly a bug. The fact that the values happened to be the result of an expression involving random numbers clearly doesn't change what the the correct value for is supposed to be. returns an error if this is run against a permanent table. 

It can be seen that SQL Server reckons it will only need to scan around 100,000 rows before it finds 10 matching the predicate and the can stop requesting rows. Again this makes sense with the independence assumption as Finally lets try and force an index intersection plan 

First create an auxiliary numbers table larger than the maximum range you will ever be interested in. 

No lock is taken on the non clustered index () until after the update on the clustered index key (on page ) so yes this would be possible. And the above is with a narrow (per row) update plan. With a wide (per index) plan each index is updated in turn. Possibly with intervening sort operations as in the plan below. 

The lowest number of reads I've managed to get so far is 1020 but the query seems far too convoluted. Is there a simpler way of equal or better efficiency? Perhaps one that manages to do it all in one range seek? 

For a query that is doing an equality seek on both columns it doesn't matter. You'd need to look at your other queries to see how they would benefit from either possibility. An index on (A,B) could also serve for an equality seek on A but not an equality seek on B for example. Also if you had ORDER BY A,B on a query then an index on A,B could avoid a sort. You say you have no such queries and all are strict equality using both columns. As a tiebreaker you might consider whichever leading column would make the most useful histogram but for the query pattern you describe where neither A or B are ever queried except together to look up a key in an unique index neither will be useful. So in your case it doesn't matter.