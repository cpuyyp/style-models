This algorithm uses graph minimum cuts to classify large amount of unlabeled samples using only small amount of labelled samples. Its undergrad friendly. I have explained this to a few randomly chosen undergrads and they understood it. Ref: Blum, A., & Chawla, S. (2001). Learning from labeled and unlabeled data using graph mincuts. Self promotion Visualization of the algorithm on youtube. 

Lagrange Multipliers (Entropy example) Relation between crossing numbers of complete and complete bipartite graphs Simpler version of Stirling’s approximation It is impossible to write computer game without calculus. Many computer games use calculus. 

I was given task similar to yours on my job. I used two things given above. Both the things work and should give you general idea. In the end you will have to do some tweaks that are specific to your graphs. 

Say there's a public encryption scheme whose public key is $p_k$ and secret key is $s_k$. Prover $P$ wants to convince verifier $V$ that he knows $s_k$. The protocol is: 

A one-line proposition $A$ can generate a 100-page proof $p(A)$. Since the proof is very long, it's highly suspectable that there is a mistake in it, which cannot be found out even after careful proofread by many people. So we should write a proof $p(p(A))$ to make sure $p(A)$ is correct. By a similar argument, we should have $p(p(p(A))), p(p(p(p(A)))), \dots$. This seems ridiculous because $1+1=2$ is commonly known as correct, which doesn't need a complicated proof. So (from my own point of view) it's better to regard a proof as a decomposition of a proposition into a series of axioms. But there are two problems: 

In discrete landscapes things are more complicated since we need to specify which states neighbour each other, essentially giving us a graph underlying the set of solutions. What you end up with is not as clean as you'd like. It's a triple $(X,N, f)$, where $X$ is the set of solutions (i.e., possible genotypes), $N:X\to 2^X$ is a neighbourhood function, and $f:X\to\mathbb{R}$ is the fitness function. For a treatment of these discrete fitness landscapes that should satisfy you with regard to mathematical rigour I would direct you to this survey paper by Reidys and Stadler. 

"someone" is right. Timothy Chan's paper "Dynamic Planar Convex Hull Operations in Near-Logarithmic Amortized Time" appears to solve the problem with insertions/deletions taking $O(\log ^{1+\epsilon}n)$ amortized time, and queries taking $O(\log n)$ time. He solves your problem which is dual to the convex hull problem. 

How do we prove this? Does this also mean a size-optimality decider can't exist? How does this generalize to other kinds of optimization (speed-optimization, for example)? 

As can be seen in the comments, Jukka provides a $2 \times 4$ counter example, which is further generalized by Peter to $4 \times 4$ to meet the requirements. Both analysis and program verification (use dynamic programming to calculate $\chi'$) can be performed to make sure $\chi \neq \chi'$ is possible when $n = 4$. Actually Peter's generalization can be used to show there is an $n \times n$ matrix s.t. $\chi \neq \chi'$ for any $n$. The counter example is shown below: $\begin{bmatrix} 1 1 0 0 0 \cdots 0 \\ 0 1 1 0 0 \cdots 0 \\ 0 0 0 0 0 \cdots 0 \\ 0 0 0 0 0 \cdots 0 \\ \vdots \\ 0 0 0 0 0 \cdots 0 \end{bmatrix}$ There's a unique column with two 1's. At least one continuous rectangle is needed to cover the 0's in this column. And this continuous rectangle doesn't cover any elements in the two rows corresponding to the two 1's. Remove this continuous rectangle and the rest will be a $2 \times n$ matrix: $\begin{bmatrix} 1 1 0 0 0 \cdots 0 \\ 0 1 1 0 0 \cdots 0 \end{bmatrix}$ which needs at least 5 continuous rectangles to cover. So $\chi' \geq 6$. However, it is easy to see $\chi \leq 5$. So $\chi \neq \chi'$. 

If you're overwhelmed by all of the work that's out there, why don't you start out pretending that nobody's worked on the problem before? If your goal is to eventually build a competitive SAT solver, it's going to be a fairly long journey. By starting out just playing around without 'checking the solutions', so to speak, you have more to gain than to lose. First build the simplest solver you can and make sure it works. This will probably be a brute force algorithm whose running time depends more or less only on the $n$, the number of variables, and $m$, the number of clauses. Then implement something a little bit smarter like branch-and-bound. Write (or find) a generator that will give you random instances for given values of $n$ and $m$. Do some benchmarking tests for your branch-and-bound solver. See how it does for varying values of $n$ and $m$. Then improve your solver to make it faster. See how far you can get without reading about other work. When you run out of ideas, do some of the reading suggested in the other answers. 

$X = \{\{0,5\}, \{1,4\}, \{1,1,3\}, \{1,1,1,2\}, \{1,1,1,1,1\}, \{2,3\}, \{2,2,1\} \}$ $X$ is partition of 5 where only single digit numbers are allowed. Observation: Any number with digit sum 5 can be represented by permutation of one of the above sets stuffed with 0s. Example: $\begin{eqnarray*} 14 \rightarrow 10^040^0 \rightarrow 14:0,0\\ 104 \rightarrow 10^{1}40^0 \rightarrow 14:1,0\\ 10000000000004 \rightarrow 10^{12}40^0 \rightarrow 14:12,0\\ 100040000000000000000 \rightarrow 10^{3}40^{16} \rightarrow 14:3,16   \end{eqnarray*}$ All these four numbers can be stored in a dictionary with 14 as key and list of number of zeros to stuff between each non-zero digits. 

Space savings: $log_{10}$ roughly computes length of a number in base 10. If you store a number N, as a string then $L \approx \log_{10}N$ bytes are required (assuming 1 byte per char). But if we use representation of numbers given above then approximate number of bytes needed are $\approx \log_{10}L$. Reason for this is lots of zeros in the number. $$space \ savings \approx 1 - \frac{log_{10} \ L}{L} = 1-\frac{log_{10}\ log_{10} \ N}{log_{10}\ N}$$ Python Code: 

As you asked it, the question about the expected length (given the height) does not make sense without a prior distribution on the length of the string. You should instead consider the number of times you get tails before you get $h$ heads in a row, since this will give you the number of descendants of a node of height $h$ in a skip list. Let's represent this value with the random variable $X=X(h)$. When we start, or right after we get tails, the probability of starting and finishing a run of $h$ or more heads before getting tails again is $2^{-h}$. If we hit tails before getting $h$ heads in a row, we're back to square one. Thus $X$ is actually distributed as $\mathrm{Geometric}(2^{-h})$ and we have $\mathrm{E}(X) = (1-2^{-h})2^h$. Edit: Sorry, this gives you the expected number of towers in the left subtree. The number of nodes in the left subtree will be of the same order of magnitude though, since towers in between the first and last will have geometrically distributed height with expected value 2. Also, if you want to consider the right subtree as well, which probably makes more sense, you simply go until you get $h+1$ heads in a row instead of just $h$. In this case you get the number of descendants as defined in Devroye's paper that you linked to. 

Completeness and soundness are obvious. Intuitively decryption shouldn't leak information about the secret key $s_k$ if this scheme is CCA-secure. But I just haven't come up with a proper simulator to argue this. If $V$ cheats, it's hard to get the correct plaintext without knowledge of $s_k$. If the simulator just guess, the probability is so low that exponentially many rounds are required. So the question is: Is this protocol really zero knowledge? If so, how to construct the simulator? 

The uniform version (the version which we normally see) of deciding whether a CFG (Context Free Grammar) is ambiguous is undecidable. But here I'd like to know something about the non-uniform version of this problem. That means, we just pick one CFG instead of considering all CFGs. It's true (and trivial) that there is a Turing Machine which can decide whether a fixed CFG is ambiguous or not. just needs to blindly output or according to the fact, i.e., whether is actually ambiguous or not. However, this is a non-constructive solution. It tells you the answer but doesn't provide with a proof. This is something like God's knowledge. There is always a proof of ambiguity for any ambiguous CFG. This problem is recursively enumerable. But it's negation (i.e. proof of unambiguity) isn't. However, we can still find ways to prove unambiguity (for example, for very simple grammars like ). The relationship between the uniform and non-uniform version of this problem is that: 

To complete the search process for an element $x$ that is not in the tree, you have to go all the way to a leaf node to confirm that $x$ is not in the tree. If you repeatedly search for such an element $x$, you have to splay on an unsuccessful search, otherwise the query time for $x$ could indefinitely remain as bad as $\Theta(n)$, even in an amortized sense. 

The question should not be if there is a measure, but rather which measure is most appropriate. That depends on the problem you're trying to solve. 

Knowing the exact value of the chromatic number $\chi$ cannot help by more than a factor of $n$. Since there are only $n$ possible values of $\chi$, you can 'guess' its value, i.e., run processes $P_1,\dots,P_n$, where $P_i$ runs an algorithm assuming $\chi=i$. This whole scheme can find an optimum colouring in time at most $n$ times the time that it takes $P_\chi$ to find an optimum colouring. On the other hand, if you're talking about parameterizing the running time by $\chi$, then it's a much more interesting question. It's in FPT if you parameterize by $n-\chi$ S. Khot and V. Raman, ‘Parameterized Complexity of Finding Subgraphs with Hereditary properties’. If you parameterize by $\chi$ I would assume it's W[1]-hard. 

$V$ uniformly generates $m$ and sends $c = Enc_{p_k}(m)$ to $P$ $P$, receiving $c$, sends $m' = Dec_{s_k}(c)$ to $V$ $V$ checks whether $m = m'$. If so, accept. Else, reject. 

The uniform version tells you it's impossible to solve the problem for all instances. The non-uniform version tries to tackle each instance based on it's unique characheristics. 

If we let the whole problem space (here it's the space of all instances of valid CFGs) be , and the space of solvable instances (those come with a proof) w.r.t. ambiguity be , what's the relationship between the size of and ? (Note that both and have an infinite cardinality.) 

It seems the first problem has something to do with Gödel's Incompleteness Theorem. I'd sincerely thank those who recommend me very clear and illustrative materials about it. I'd also like to know what background will be needed to understand it. I tried to read the original paper but didn't quite understand what are the more fundamental facts that we can rely on to prove this theorem. (Maybe Peano Arithmetic and ZFC?) 

What you're talking about is, I believe, called the Capacitated Facility Location Problem. Searching for that should give you plenty of papers to read. It appears the problem is NP-hard, but in Euclidean space a simple search procedure gives you a 6-approximation [PDF]. 

This is the well-studied problem of Nearest Neighbour Search. There is not one 'best solution' to the problem---you'll want to choose trade offs based on the input and requirements. Do you need the creation of the data structure to be parallel? Or just the queries? Depending on how many thousands of points you're talking about, if your set of points is static it might not be impractical to do something naive like: