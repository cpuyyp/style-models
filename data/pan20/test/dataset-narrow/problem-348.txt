I would advice you to upgrade the package by following these steps If after your upgrade you still run in to problems, it could be that your package is using components/drivers that are not available on 64bit. In that case the "easiest" work around would be to have your package use a 32bit runtime. Open de Job properties and then edit the job step that is calling the SSIS package. Go to the Execution Options tab and check "use 32bit runtime" 

To prevent the scenario: that you can overdraw because two debit transactions run at the same time you would have to do the following: (with this table design) If your business logic mandates that you first do a select(as per your example) 

Note that when you force encryption on the server side, the connection will be encrypted by using ssl. You can use any certficate for this. Your connection will be encrypted, However the client will not check if the identity of the SQL Server is in fact valid. Without this check you could be open for a man in the middle attack. If you choose force encryption on the client side, a full validity check will be performed. That way not only is the connection encrypted, also the client knows that the server it is connection to, is in fact the server mentioned in the certificate because it can trust that the certificate has not been tampered with. For this to work, the certificate used on the SQL Server must be issued by a certificate authority that the client trust. This can be any well known certificate authority or for example when you have a windows domain and a CA in that domain, you can have that CA issue one. You then have to import the root certificate of that CA into all clients and that will work too. to check if your connection is encrypted use the following: 

Start with a fill factor of 90% (we have to start somewhere) In your index maintenance procedures, build in that you keep track of how fast your indexes build up fragmentation. (say 30% in a week) Now on your next rebuild routine, lower the fill factor slightly. say 80%. See if in the next 7 days, your fragmentation is less then 30%. Continue like this, untill either fragmentation doesn't go down, or the fragmentation has become very low. 

OUTPUT: File activation failure. The physical file name "e:\SQLServer\Log\RepSource_log.ldf" may be incorrect. The log cannot be rebuilt because there were open transactions/users when the database was shutdown, no checkpoint occurred to the database, or the database was read-only. This error could occur if the transaction log file was manually deleted or lost due to a hardware or environment failure. Warning: The log for database 'RepSource' has been rebuilt. Transactional consistency has been lost. The RESTORE chain was broken, and the server no longer has context on the previous log files, so you will need to know what they were. You should run DBCC CHECKDB to validate physical consistency. The database has been put in dbo-only mode. When you are ready to make the database available for use, you will need to reset database options and delete any extra log files. Now, if you look at the output you can see why transactional replication would no longer work: 

So on this page we have the records with ID 3 and ID 4. If I try and select the rows AFTER I have corrupted page 296 I get a checksum error: 

So every sunday, asuming you had changes every week, the diff backup will be bigger then the previous one. Untill one sunday, it's just as big as a full backup would be. That would imply that all data pages had changed. At that moment you want to do a new full backup. Since that will reset the differential bitmap and your next diff backup will be small again. (depending on the amount of changes) You could automate the process of making diff backups untill a certain percentage of the data has changed. Have a look at this blog post: $URL$ It explains a way on how to programatically look up the percentage of database changes. That way you could say.. make diff backups if percentage changed < 75% otherwise make a full backup. Now could you tell me why you want this? Because this isn't a setup I would advise a starter (with exceptions). So please elaborate a bit and we can see if this is the best solution for your setup. How big is your database? how much changes do you have? is batch based changes? Which version and Edition of SQL do you have do you have? 

Please be aware that this applies to ALL statements that contain passwords. So also when you create the keys. 

The above code will generate 500k records in a global temp table, you can export these with the following commands. If you run this from SSMS, make sure you are in SQLCMD mode: 

EDIT Okay, some extra info is needed after your edits and comments. First of all. I feel that the "Initial size" label that you see when you look at the file properties in SSMS is a misnomer. Basically, your intial size is just a concept. It's the first size that is used during creation of the database. You can either explicitly specify this in the statement, or you can have SQL Server implicitly copy it from the model database by ommiting that information during creation. However, once the database is created, from a DBA perpective there is no such thing as a "initial size" there is only one property visible for a DBA and that is: the actual size. Even the "Initial size" property in SSMS just shows actual size, not the initial size. Well how come that or "know" the initial size then? Even after you have changed the size. Interesting question. The first page of a dattabase file is the file header page. In there you have, amongst others, 2 properties: size and minsize. At creation of the file, both file header properties get filled with the inital value: 

To answer the question: How to find the offending query: Since the spikes in the graph you posted last for several minutes you have plenty of time to use the following method: Download sysinternals process explorer 

The 42MB unit (Hitachi calls this a page too, how convinient). is the basic allocation unit at which space is allocated to a LUN. So with Dynamic provisioning, a LUN will always grow in 42MB steps. These, Hitachi 42MB pages are taken from different Array groups with in a pool. So whenever more space is needed for the LUN a new 42MB Hitachi page is allocated. When you write your first 8KB SQL Data page to a file, a 42MB Hitachi page will be allocated. Subsequent writes of SQL data pages go to this 42MB area, until it's full. Now, what is important to understand is that a 42MB page goes to one parity raid group. If you have small RAID groups, for example : RAID-1+0: 2D+2P (2+2) (4 disks) and your data access is mostly sequential, this could be a problem when you have one file. Lets say you have 4 RAID10 groups each having 4 disks. You just happen to have only 1 important table with 1 clustered index that is 168MB in size. When you start inserting into this table, the first 42MB alloc page will be placed on one of the RAID groups, once this 42MB is full, the second alloc page will be allocated on another RAID group, most likely all 4 42MB alloc page will be devided across the 4 RAID GROUPS. (we are excluding tiering for simplicity at this moment). What's important to understand is that anything you do sequentially in the order of the clustered key, will only have 4 disks at any given moment to handle the IO. (2 disks if you are writing..) even though the whole table is spread across 16 disks.. If you would want to be using all 16 disks. you would have to create 4 files (on 4 seperate LUNS) in a filegroup and create the table on that. At least you now know that SQL is "striping the data across 4 42 MB alloc pages" This can be most problematic with the transaction log file. It's only 1 file and sequential. The only "tuning" you can do here is make the RAID groups used in that pool as big as possible. Contradictionary a concatinated array group RAID5 14+2 OR RAID5 28+4 might be the fastest even though it's RAID5 as opposed to RAID 10, since as far as I know the biggest RAID group in RAID 10 that can be created is 4+4 is 8 disks. Please be aware this becomes far less important if you are running a server with multiple databases with multiple LUNS all sharing the same pool, or even more generic multiple servers all having LUNS in the same pool. SInce obviously the general load would then be evenly distributed across all the raid groups. Regarding tiering, I wouldn't be to worried about it from a performance objective. (if you are not allowed to pin..) Heavy 42MB alloc pages are moved to a high performance tier, so you could have the situation where one table is spread across 3 different tiers. Apperently you have parts of your table that are IO intensive and parts that are not... worry about the combination small raid groups AND single files (actually single LUNs) AND (high incremental inserts OR heavy table localized updates/deletes OR heavy seqeuential reads) hope this helps a bit.. further reading: Hitachi Tiering design guide Hitachi SQL Server best practise Blog post about 42 alloc unit