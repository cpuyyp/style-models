Your data is already binned. You should do a Chi-Squared test to see if the hypothesis that your data is normally distributed is correct. You can refer to this question to see how you can go about this. You will set your baseline as the Normal distribution with the mean and variance from your dataset. Then you will test the difference in expected value between the Normal distribution and yours. 

A Euclidean vector is any vector with a magnitude and direction. A unit vector, is a vector with a magnitude 1. So it is fair to say that all unit vectors are Euclidean vectors. However, not all Euclidean vectors are unit vectors. Given the statement above we can conclude that any Euclidean vector in $\mathbb{R}^3$ can be described by the unit vectors $e_1, e_2, e_3$. These span the space. 

How to do this Load the data to memory Depending on the means by which your data is stored this step will be different. However, the goal is to go from the raw file source to either Python Numpy array or a Pandas DataFrame. I will assume your data is structured as follows 

This type of dataset is commonly called a skewed dataset. This is when a certain class is over-represented in contrast to the other labels in the dataset. There are many algorithms that are well suited to these types of problems. Specifically, anomaly detection algorithms are capable of learning the distribution a single set of labels (event not occurring) and then it will be able to flag when an anomaly occurs (event occurs). This is when an instance is sufficiently beyond the learned distribution. You can use this to get the probability of an event occurring based on a p-statistic test using the feature-space you have set up in contrast with those from your learned distribution. The simplest method would be doing a generalized likelihood ratio test (GLRT). But, I think you will most likely find more luck using a K-NN based method for skewed datasets. Since you have a lot of data you can use ranking algorithms to generate your p-statistics. Look into rankSVM. 

The greater the number of output nodes the higher complexity you will add to your model. This means that given a fixed amount of data, a greater number of output nodes will lead to poorer results. I would use a ABCD vs. others strategy. Instead of conditioning your model to learn the distributions of the class A, B, C and D separately you will combine them. This means that is A and B are different in some way, but this difference is irrespective of the classification with "others" then there is no need to learn that distinction. For example: if you want to detect dog, cat, human with features such as weight, height and number of legs. The number of legs feature will have relatively low importance, because cats and dogs will likely all have 4 legs. However if I want to classify cat/dog vs humans, then the number of legs will be the most important feature. It might be the only feature you need. One caveat may be severe class imbalance. By combining your classes in this way you may end up with an over representation of ABCD. You can use techniques such as anomaly detection to train a model on your ABCD data and then detect whether a novel instance falls within this distribution, or is an outlier in which case you would label it as "others." 

In short, you can wrap your classifier with an anomaly detection algorithm to catch hypotheses that are below an acceptable bound. 

Validating on hand drawn numbers More realistically would be to use hand drawn numbers since that is what we used to train. I just drew this one using Paint. 

This idea of matching a distribution can be applied to wide range of applications where you want to draw from a distribution. Some for demonstration purposes of the techniques without any real useful application and some very useful applications which are currently being used for a number of tasks. Here is a short lists of some examples: 

Yes! The accuracy of your algorithm should be tested on images that you expect to receive. This would not include data that has been rotated, shifted, blurred, etc. However, augmenting your data in such ways can and usually will lead to better results. Split your data first. Then augment only the training data while staying away from the validation data. Only use your testing data when you are confident that your solution leads to good results on your validation set without overfitting it. 

This example is as above. However, I add a metadata input of size 8 to the model after the LSTM layers. This will contribute in the output as it will embed the information from those features in the last few layers of the model. 

Look at the dimensions of our weights vector. It only has 2 values. One value associated with $x$, the first column of our $X$ matrix, and a bias, associated with the 1's column that we added. The equation of this line is described as $y = 0.01174825 x_1 + 2.30530303$ 

Our initial line looks like this Now we will iteratively update the weights of the model if it mistakenly classifies an example. 

It is hard to determine if this is in fact overfitting because your validation accuracy has yet to start decreasing. Overfitting is characterized by a set of model parameters which perform very well on the training set but do not generalize to different subsets of the data. I assume you are using Keras based on these plots. To avoid overfitting when training you can save the weights only when the model gets better on your validation set and ignore those which do no better or worse. This will ensure that at the end of the training session you have the weights which generalized the best to the validation set saved to file which can then be loaded into the model. Saving only the best set of weights in Keras Use a callback which will run between epochs 

We see that the accuracy measured by oob is very similar to that obtained with the testing set. It thus follows through the theory that the oob accuracy is a better metric by which to evaluate the performance of your model rather than just the score. This is a consequence of bagging models and cannot be done with other types of classifiers. Calculating oob using different metrics Yes, you can do this! However, it depends how exactly your code is structured. I am not sure how you can include the oob and AUC all together with the function. However, if you are doing the cross validation folds manually you can do the following, the random forests algorithm in sklearn provides you the decision function of the oob as 

I tried this experiment and was able to get some positive results. I will describe what I tried then perhaps you can specify where the differences may lie and we can further explore them. From what I tried I would assume that you are simply not training long enough. Creating the data 

What's the point? First, it is good to understand what we are doing that leads us to need these tools. When we are trying to apply machine learning we want to infer some meaning from data. This means that given an instance we want to put it through our model and then we can have some output that tell us something about this data. Let's look at the example where we try to label whether an image contains a cat or a dog. If we have a perfect model, we can give the model a picture and it will tell us if it is a cat or a dog. However, no model is perfect and it will make mistakes. When we train our model to be able to infer meaning from input data we want to minimize the amount of mistakes it makes. So we use a training set, this data contains a lot of pictures of dogs and cats and we have the ground truth label associated with that image. Each time we run a training iteration of the model we calculate the cost (the amount of mistakes) of the model. We will want to minimize this cost. Many cost functions exist each serving their own purpose. A common cost function that is used is the quadratic cost which is defined as $C = \frac{1}{N} \sum_{i=0}^{N}(\hat{y} - y)^2$. This is the square of the difference between the predicted label and the ground truth label for the $N$ images that we trained over. We will want to minimize this in some way. The cost is also commnly called the loss function. So we have a minimization problem on our hands! Indeed most of machine learning is simply a family of frameworks which are capable of determining a distribution by minimizing some cost function. The question we can ask is "how can we minimize a function"? Let's minimize the following function $y = x^2-4x+6$. If we plot this we can see that there is a minimum at $x = 2$. To do this analytically we can take the derivative of this function as $\frac{dy}{dx} = 2x - 4 = 0$ $x = 2$. However, often times finding a global minimum analytically is not feasible. So instead we use some optimization techniques. Here as well many different ways exist such as : Newton-Raphson, grid search, etc. Among these is gradient descent. How does gradient descent work? Let's use a famously used analogy to understand this. Imagine a 2D minimization problem. This is equivalent of being on a mountainous hike in the wilderness. You want to get back down to the village which you know is at the lowest point. Even if you do not know the cardinal directions of the village. All you need to do is continuously take the steepest way down, and you will eventually get to the village. So we will descend down the surface based on the steepness of the slope. Let's take our function $y = x^2-4x+6$ we will determine the $x$ for which $y$ is minimized. Gradient descent algorithm first says we will pick a random value for $x$. Let us initialize at $x=8$. Then the algorithm will do the following iteratively until we reach convergence. $x^{new} = x^{old} - \nu \frac{dy}{dx}$ where $\nu$ is the learning rate, we can set this to whatever value we will like. However there is a smart way to choose this. Too big and we will never reach our minimum value, and too large we will waste soooo much time before we get there. It is analogous to the size of the steps you want to take down the steep slope. Small steps and you will die on the mountain, you'll never get down. Too large of a step and you risk over shooting the village and ending up the other side of the mountain. $\frac{dy}{dx} = 2x - 4$ $\nu = 0.1$ Iteration 1: $x^{new} = 8 - 0.1(2 * 8 - 4) = 6.8 $ $x^{new} = 6.8 - 0.1(2 * 6.8 - 4) = 5.84 $ $x^{new} = 5.84 - 0.1(2 * 5.84 - 4) = 5.07 $ $x^{new} = 5.07 - 0.1(2 * 5.07 - 4) = 4.45 $ $x^{new} = 4.45 - 0.1(2 * 4.45 - 4) = 3.96 $ $x^{new} = 3.96 - 0.1(2 * 3.96 - 4) = 3.57 $ $x^{new} = 3.57 - 0.1(2 * 3.57 - 4) = 3.25 $ $x^{new} = 3.25 - 0.1(2 * 3.25 - 4) = 3.00 $ $x^{new} = 3.00 - 0.1(2 * 3.00 - 4) = 2.80 $ $x^{new} = 2.80 - 0.1(2 * 2.80 - 4) = 2.64 $ $x^{new} = 2.64 - 0.1(2 * 2.64 - 4) = 2.51 $ $x^{new} = 2.51 - 0.1(2 * 2.51 - 4) = 2.41 $ $x^{new} = 2.41 - 0.1(2 * 2.41 - 4) = 2.32 $ $x^{new} = 2.32 - 0.1(2 * 2.32 - 4) = 2.26 $ $x^{new} = 2.26 - 0.1(2 * 2.26 - 4) = 2.21 $ $x^{new} = 2.21 - 0.1(2 * 2.21 - 4) = 2.16 $ $x^{new} = 2.16 - 0.1(2 * 2.16 - 4) = 2.13 $ $x^{new} = 2.13 - 0.1(2 * 2.13 - 4) = 2.10 $ $x^{new} = 2.10 - 0.1(2 * 2.10 - 4) = 2.08 $ $x^{new} = 2.08 - 0.1(2 * 2.08 - 4) = 2.06 $ $x^{new} = 2.06 - 0.1(2 * 2.06 - 4) = 2.05 $ $x^{new} = 2.05 - 0.1(2 * 2.05 - 4) = 2.04 $ $x^{new} = 2.04 - 0.1(2 * 2.04 - 4) = 2.03 $ $x^{new} = 2.03 - 0.1(2 * 2.03 - 4) = 2.02 $ $x^{new} = 2.02 - 0.1(2 * 2.02 - 4) = 2.02 $ $x^{new} = 2.02 - 0.1(2 * 2.02 - 4) = 2.01 $ $x^{new} = 2.01 - 0.1(2 * 2.01 - 4) = 2.01 $ $x^{new} = 2.01 - 0.1(2 * 2.01 - 4) = 2.01 $ $x^{new} = 2.01 - 0.1(2 * 2.01 - 4) = 2.00 $ $x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $ $x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $ $x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $ $x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $ And we see that the algorithm converges at $x = 2$! Where do partial derivatives come into this? We just minimized $y$ for $x$. But if we had a multi-dimensional $x$ then we will need to identify where is the steepest slope in each dimension. To do this we will take the derivative across each dimension. In 2 dimensions gradient descent is performed using the gradient of the function $\nabla y$. For the function $y = 2x_1^2 + 5x_1 - 4x_2^2$ we will take the gradient in both dimensions defined as $x^{new} = x^{old} - \nu \nabla y$. The operator $\nabla$ is called the gradient it is defined as $\nabla y = \frac{\partial y}{\partial x_1} \hat{\textbf{x}_1} + \frac{\partial y}{\partial x_2} \hat{\textbf{x}_2}$. $\hat{\text{x}_1}$ and $\hat{\text{x}_2}$ are the two dimensions that we are optimizing over. So in our analogy we will go down this 2D surface a bit in the $x_1$ dimension and the $x_2$ dimension. Kind of like going down the mountain in the North-West direction. A bit in front and a bit to our left. That's a 2D direction! The total size of the step that we take across all dimensions is then described by $\Delta y = \frac{\partial y}{\partial x_1} \Delta x_1 + \frac{\partial y}{\partial x_2} \Delta x_2$. Relating all this to your question Change our function $y(x_1, x_2)$ by $C(v_1, v_2)$. The size of the step we are taking is defined by $\Delta C = \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2$. At each gradient descent iteration we will update $v$ as $v^{new} = v^{old} - \nu \nabla C$ where $\nabla C = \frac{\partial C}{\partial v_1} \hat{\textbf{v}_1} + \frac{\partial C}{\partial v_2} \hat{\textbf{v}_2} = (\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T$