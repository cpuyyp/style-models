In the world of streaming, there are many problems that are easy to compute with linear space and impossible (not just hard) for sublinear space. For example, computing the most frequent element of a stream exactly is impossible without linear space via communication complexity lower bounds. 

Are there any other methods (maybe with different "strengths of belief") that have been used ? For any answer, an example of where it has actually been used is required: obviously there are many ways one might try to show this, but examples make the argument more convincing. 

The Walsh-Hadamard transform (WHT) is a generalization of the Fourier transform, and is an orthogonal transformation on a vector of real or complex numbers of dimension $d = 2^m$. The transform is popular in quantum computing, but it's been studied recently as a kind of preconditioner for random projections of high-dimensional vectors for use in the proof of the Johnson-Lindenstrauss Lemma. Its main feature is that although it's a square $d\times d$ matrix, it can be applied to a vector in time $O(d \log d)$ (rather than $d^2$) by an FFT-like method. 

If you want to find an induced subgraph of "sufficient size" i.e not of constant size, then this is not possible. Consider the star $K_{1, n}$, which has average degree $2-1/n$. Take any subset of the vertices. 

The premise of the question is a little flawed: there are many who would argue that quadratics are the real "boundary" for tractability and modelling, since least-squares problems are almost as 'easy' as linear problems. There are others who'd argue that convexity (or even submodularity in certain cases) is the boundary for tractability. Perhaps what is more relevant is "why do linear systems admit tractable solutions ?" which is not exactly what you asked, but is related. One perspective on this is composability. Since the defining property of a linear system is that $f(x + y) = f(x) + f(y)$, this imparts a kind of "memorylessness" to the system. To build up a solution to a problem I can focus on individual pieces and combine them with no penalty. Indeed, the premise of most algorithms for flow is precisely that. This memorylessness imparts efficiency: I can break things into pieces, or work iteratively, and I don't lose by virtue of doing so. I can still make bad decisions (c.f. greedy algorithms) but the act of splitting things up itself doesn't hurt me. This is one reason why linearity has such power. There are probably many others. 

The study of real-world situations from a computational perspective is quite hard due to the continuous-discrete "jump". While all events in the real world (supposedly) are run in continuous time, the models we usually use are implemented in discrete time. Therefore, it is very tricky to define how small or large a step should be, what should be the size of the problem, etc. I have written a summary on an Aaronson's paper on the subject, however it is not in English. See the original paper. Personally, I have heard of another example of a real world problem modeled into computation. The paper is about control-systems models based on bird flocking.It turns out although it takes a short time in real life for birds, it's intractable ("a tower of 2s") when analyzed as a computational problem. See the paper by Bernard Chazelle for details. [Edit: Clarified the part about the Chazelle paper. Thanks for providing precise information.] 

Is there a quick reference for the definition of a sparse s-t flow? In the general case, having the max-flow it is quite easy to determine the min-cut, via the max-flow , min-cut theorem. The edges that are fully saturated form a cut set, so by selecting one vertex for each such edge, one can form a min-cut. Trivially, this is O(m) in the worst case, and also if one makes the running time output-sensitive, then the number of edges in the flow or even better, the number of saturated edges in the flow, always is an upper bound on the running time of the algorithm for finding the min-cut from the max-flow. So if you have a modification that finds those sparse s-t flows in linear time in the size of the flow, finding the min-cut won't change the algorithm's runtime asymptotically. 

First of all, I cannot help but point out a practice I consider bad in the way you defined your question. You are comparing a class of algorithms with a class of problems. Although closely related, they are not the same. As an example, consider the halting problem. It is a very well defined problem. However, we know that no Turing Machine can diagnose it. Given the Church-Turing thesis, this is the same as saying that there is no algorithm for this problem. However, one can rephrase the question such that the items compared are of the same nature. Consider constraint programming (CP) as the computational model A and boolean formulas as the computational model B. I chose boolean formulas because SAT is probably the most known CSP and furthermore boolean formulas have a nice property, which I will use below. Although I have programmed in Prolog, I cannot say I have a good knowledge of constraint programming. However, I am almost certain that they are Turing-complete. Since they use constraints, one could use them to "simulate" head moves,e.g. if [constraint] then move to this cell . Furthermore, one could write contents into that tape. Therefore, constraint programming is as powerful as Turing Machines. Since all programs in CP are algorithms, they can be simulated by a universal TM. Thus CP and TM have the same expressibility. One might think at this point that CP is a stronger model that boolean formulas. However, in Arora and Barak's book "Computational Complexity:A Modern Approach", in chapter 2, a proof is given that any boolean function has an equivalent boolean formula ( the theorem is labeled "Universality" of AND,OR and NOT). On a given input, the formula is satisfied if and only if the boolean function is true for this input. As an informal proof, consider that those 3 operators can express any piece of hardware. Furthermore, every other input can be translated into a boolean one. Therefore, every TM can be simulated as a boolean formula. Thus boolean formulas are Turing-complete. Furthermore, there is a TM that can diagnose the satisfiability of a boolean formula. Therefore, boolean formulas and TMs are equivalent. It follows that the two models have the same expressibility. 

While this might be too daunting to jump into before you do as Dave suggests, there's a nice collection of open problems in computational geometry maintained by Joe O'Rourke, Erik Demaine and Joe Mitchell. These provide a good snapshot of core questions in the theoretical realm. 

I'm particular interested in methods that can be implemented. On the face of it, both of the above approaches seem relatively easy to implement, but I'm curious if there are any others out there. 

This is not a direct answer to your question, but it's related. The probability you want is precisely what is provided by min-wise hashing schemes. In particular, a min-wise hash takes a set and produces a single element, with the property that the probability of the two elements being identical is precisely the Jaccard similarity between the sets (intersection/union). 

On the one hand, what you're asking sounds like binary search. On the other hand, the adaptivity makes it feel more like active learning: think of the "thing you're trying to learn" as a hypothesis and the "queries" as being labels of objects. Since this question is primarily about finding the right terminology, do look at the active learning literature and see if it captures the spirit of what you want. 

The most recent issue of the CACM has an article by Faliszewski, Hemaspaandra and Hemaspaandra on the use of complexity theory in the realm of social choice theory and election design in particular. One example of such a result is that while Arrow's theorem guarantees that any election system is 'hackable', it might be NP-hard to do so. 

Another good example of rigor (and new techniques) being needed to prove statements that were believed to be true: smoothed analysis. Two cases in point: 

For the purpose of this question, you can assume that $q$'s Voronoi cell is always bounded (for example $q$ always lies in the convex hull of $P$). Is there anything known about this problem ? Some constraints: 

In one form, your question is actually a clustering question that is addressed by the information bottleneck method. Roughly, $Y$ represents the rows of a joint distribution and $X$ represents the columns. The random variable $Z$ then represents a distribution over the rows ($Y$) that is highly compressed (because $I(Z;Y)$ is small) but represents the joint distribution well (because $I(Z;X)$ is large). It's not clear to me that this can be solved using LPs. the underlying problem is not convex. But there's a simple EM-style alternating optimization scheme that can be used to solve it. 

The complexity zoo reports that E does not equal PSPACE, citing the paper Comparing complexity classes by Ronald V. Book. The following sentences can be easily derived: SPACE(n) is a proper subset of PSPACE. (1) PSPACE union E is non-empty. (2) IF instead of E we had EXPTIME, it would be easy to deduce that SPACE(n) is a proper subset of EXPTIME, due to (1) and that PSPACE is a subset of EXPTIME. For E, the relationship between PSPACE and E is unclear to me : 1) Is E contained in PSPACE? If not, then it follows that SPACE(n) is a proper subset of E. To verify this, one must create a problem that uses more than linear space and less than O(2n) time. 2) Is PSPACE contained in E? This I believe, is even harder to answer than the previous question. 

In a previous question about time hierarchy, I've learned that equalities between two classes can be propagated to more complex classes and inequalities can be propagated to less complex classes, with arguments using padding. Therefore, a question comes to mind. Why do we study a question about different types of computation (or resources) in the smallest (closed) class possible? Most researchers believe that $P \neq NP$. This distinction of classes wouldn't be between classes that use the same type of resource. Therefore, one might think of this inequality as a universal rule: Nondeterminism is a more powerful resource. Therefore, although an inequality, it could be propagated upwards via exploiting the different nature of the two resources.So, one could expect that $EXP \neq NEXP$ too. If one proved this relation or any other similar inequality, it would translate to $P \neq NP$. My argument could maybe become clear in terms of physics. Newton would have a hard time understanding universal gravity by examining rocks (apples?) instead of celestial bodies. The larger object offers more details in its study, giving a more precise model of its behavior and allowing to ignore small-scale phenomena that might be irrelevant. Of course, there is the risk that in larger objects there is a different behavior, in our case that the extra power of non-determinism wouldn't be enough in larger classes. What if after all, $P \neq NP$ is proven? Should we start working on $EXP \neq NEXP$ the next day? Do you consider this approach problematic? Do you know of research that uses larger classes than polynomial to distinguish the two types of computation? 

I am aware of the "famous" DIMACS graph format (which frankly looks a little clunky to me - "c" for a comment line ?) and the METIS file format. While it's not particularly hard to invent my own graph format, it's nice to follow something accepted so that reuse of code for generating examples and testing algorithms is easier. 

One example of what you might call non-linear clustering is kernel-based clustering, where the similarity function is described by a (non-linear) kernel function. This is a weak form of non-linearity, since there's a lifting map that makes everything linear (albeit in a Hilbert space) 

Yyou have to be careful, since there's at least one result that improves the Christofides heuristic to 4/3 for cubic-3-edge connected graphs. A good further reference is $URL$ which just showed a 7/5 approximation (and lower bound) for subcubic graphs, and this result that improves Christofides for metrics induced by unweighted graphs. The Boyd et al paper discusses this matter in its introduction 

I'm not sure what kind of properties you're looking for but the spectral radius of planar graphs is one such quantity (the max absolute value of an eigenvalue of the adjaceny matrix). See for example this paper. 

The basic method works like this: Assume your inequalities are of the form $$ \sum_{i \le d} a_i x^i \le 0$$ Then you construct a lifting map to a space of higher dimension in which each monomial corresponds to one dimension. Now the polynomial can be expressed as a linear combination of the new dimensions and you can invoke the usual result for half spaces in the resulting space. I'm not sure where you get your bound from: the correct expression for the VC dimension of polynomials in d variables of degree D is $\binom{d + D}{d}$, which is the number of monomials of degree at most D formed from d variables. 

Space filling curves turn out to be useful when building quad trees for search. Sariel's book has more on this. 

Sublinear time algorithms (as well as sublinear space algorithms) are indeed an active area of research. The field of property testing covers much of the sublinear time regime, and I'll point you to the property testing blog to learn about recent work in the area. The question about complexity theory relating to this is very interesting, and is also an active area of research. P vs NP might not exactly be the right analogy here, but you're right that the boundary between computation and verification is something where sublinearity changes things. In particular, you can look at a PCP as "kind of" doing something sublinear, in that it only inspects a few bits of a long proof in order to check the prover's claim. More generally, there's been recent work prover-verifier systems where the verifier runs in sublinear time. Some references that are worth perusing: 

I am currently an undergraduate student, bound to graduate this year. After graduation, I am considering to work towards a TCS master/PhD. I have begun wondering what fields of mathematics are considered helpful for TCS, especially (classical) complexity theory. What fields do you consider essential for someone that wants to study complexity theory? Do you know of any good textbooks covering these fields and if yes, please include their difficulty level (introductory,graduate etc.). If you consider a field that is not heavily used in complexity theory but you consider it critical for TCS, please also refer it. 

It's good to see a fellow undergrad in pursue of this great problem, with such an enthusiasm. Allow me to offer you a piece of advice from my own experiences. $ P \neq NP $ is a very interesting problem. The implications of the answer are immense, especially in the case that the two classes are equal. The reward is great in many levels, from the altruistic scientific one to the materialistic money award. That leads many young people that encounter the problem in trying to solve it, with no or limited knowledge about it. Perhaps most theory students go through that phase. You will have an idea and think it is right, but it is almost certain that you are wrong. Some people never get through that phase and embarrass themselves by being too stubborn to admit their errors. In FOCS 2010, Rahul Santhanam compared the $ P \neq NP $ question to a mythical monster. It would take many sacrifices and courage to even try to defeat this monster. After all, it may be the most difficult problem ever. To have a fighting chance, you will have to study a lot about this problem and complexity in general. You'll never know what the "monster's weakness" will be. So my advice is this: Take your time in knowing the problem. Every time you figure out a solution, assume you are wrong somehow and try to find the problem with it. That way you'll learn much. As for references, I would recommend Sipser's book as well. After finishing it, I would recommend "Computational Complexity:A modern approach" by Arora and Barak, a more complexity-oriented book, that requires a good understanding of the concept of computation.