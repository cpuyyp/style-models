Looks like has a time part that makes the query not count the values on with time other than . Try a non inclusive comparison instead. 

the version would count 2 rows and the query would count 1 row. But with the index definitions as they are specified here using the directive SQL Server will prevent you from adding a row with multiple elements. That does however not let us use the function without specifying to guarantee the compiler that we will only get a single value. That is the reason we have a Top N Sort in the plan. Looks like I am closing in on an answer here... 

If you want to do this dynamically where you have the table name as parameter and you decide that empty string is the way to go regardless of data type you can do something like this. 

Database in full recovery mode Take a full backup Make some changes Detach database Make a filecopy of the LDF-file Restore from full backup in step 2 Detach database Make a filecopy of the MDF-file Throw away the backup (important step) Delete the database Somehow make the MDF and LDF play together with the change made in step 3 still in place. 

When wondering about things like this you should compare the execution plans for your queries. The shape of the execution plan for your query will of course differ depending on how many rows you have in your tables and what indexes is defined. One scenario that shows there is no difference in performance is when there are substantially more rows in than there are in . The optimizer will then choose as the driving table in a nested loop join against . In order to get a correct result back it has to use a Stream Aggregate on table in both queries to get only the distinct rows from . So in this case the distinct keyword has no impact on performance. 

There are a couple of connect items that might be relevant here. An INSERT statement using XML.nodes() is very very very slow in SQL2008 SP1 Poor xml performance with table variable Bad performance when inserting data from element-centric documents INSERT from .nodes with element-based XML has poor performance on SP2 with x64 I have not been able to reproduce what you see with your query so I can not say for sure what is happening for you but one issue was that there was an eager spool step in the query plan that was not needed. That eager spool can be removed by using a trace flag in the first fixed versions of SQL Server. I am not sure if that is needed in later versions. The eager spool was in there "due to a general halloween protection logic". That might explain why you see the bad performance when you insert to an already existing table. If you have a look at the query plans for both your queries you should see the difference if that is the case. There are also some workarounds suggested that you can try and I think you should try to specify the text node when you fetch . 

Looks to me like a bug when producing the query plan when you have batch mode execution. Here is a repro that shows the issue both in SQL Server 2016 (13.0.4001.0) and SQL Server 2017 (14.0.3015.40). 

A selective index will not be used when using to retrieve the data. From Selective XML Indexes (SXI) - Supported XML Features 

A query like that could make you think that SQL Server will do some time consuming looping operations but if you have a look at the query plan you will see that is not the case. The for loop is transformed to a plan that gets all the values from a single call to a Table-valued function, converts them to an integer and then using an aggregate operator to calculate the sum. 

Code to fill a memory optimized table variable that is used as a parameter to the natively compiled stored procedure and call the procedure. 

I would use the Object Explorer in SQL Server Management Studio and go from the top, one procedure at a time. 

The implicit conversion also works if the value is enclosed in curly brackets . If you add those in the query the implicit conversion will fail if the original value is too long because the last ends up in the wrong place. 

This is a bug in SQL Server and here is a Connect item to vote for if you want a change. dm_sql_referenced_entities does not shows columnes when temporary tables are used in statement Current status: 

The XML you have is invalid in UTF-8 encoding. The accented characters needs to be encoded. For instance should be encoded as . Here is a shorter version that also fails. 

A workaround would be to add an unlikely value to your empty nodes using insert (XML DML) and then replace the value on your string representation. 

Another way is to extract the XML to an untyped XML variable, modify the variable and put it back to the table. 

The synonym is owned by the database so you can have the same synonym name in different databases pointing to tables in different archive databases. The stored procedure can then be the same in all databases using the synonym(s). You do of course have to create one synonym for each table in the archive db that you want to use from the SP. 

Every row in the table has to be looked at but it will be way better if you need to scan almost the entire table anyway. 

You can change your column to a and update the value with a calculation using the values from the posts you want to end up between. Example: 

If you want the index to be used for the function you should match the data type in the index with the data type used in the query. You have in the query and in the index so and is not used in your query. It is instead parsing the XML data using the table valued functions. To make your query use only the indexes (no xml parsing functions) you can change the query and the index to this. 

is not valid SQL Server syntax in a regular SQL statment, it is used when you create a cursor. But you are probably using ADO with CursorLocation and then your query actually works because ADO will use sp_cursoropen which accepts the syntax used for cursors. The default behavior in SQL Server is that cursors can be updated so specifying does nothing for you unless you also specify a column list. Specifying the query hint on a cursor will only do things for you if you are running in a transaction. With the locks is placed when you do and without the lock is placed when you do , still only if you are in a transaction. So in your case, using will place locks when you fetch data if you are in a transaction. If you don't use , in a transaction, you will place the locks when you update the data. If no transaction is present there is no difference between the two and you could as well not use any of them. 

In my rather limited test case I saw an improvement from more time than I care to wait down to 4 seconds*. The query plans I got for the two queries. 

Add a column to that will hold the original value of . Add all rows from and then use to update with the value of for the parent row. Code with some comments: 

The version is for and the version is for . You might think that an index like that would give you a plan with a nice seek but selective XML indexes are implemented as a system table with the primary key of as the lead key of the clustered key of the system table. The paths specified are sparse columns in that table. If you want an index of the actual values of the defined paths you need to create a secondary selective indexes, one for each path expression. 

SQL Server assumes there are 10000 elements in the XML and keeps on guessing from there. Using the function assumes that 200 of those will be returned. Before that there is a Filter operator that checks if limiting the number of estimated rows to 66. Pure guesswork and not influenced at all by what data you actually have in your XML. To know if a query is good enough you should have a look at things like duration, number of reads and allocated memory. Don't use the estimated cost and please don't use the percentages of individual queries to compare performance. Your XML query could be improved as Mister Magoo suggests in a comment. 

You can use the third parameter of that is used to specify where in the string the search will start. 

Will give you one row returned but the estimated rows returned is 200. It will be 200 regardless of what XML or how much XML you stuff into the XML column for that one row. This is the query plan with the estimated row count displayed. A way to improve, or at least change, the estimates is to give the query optimizer some more information about the XML. In this case, because I know that really is a root node in the XML, I can rewrite the query like this. 

When comparing values of different datatypes SQL Server follow the Data Type Precedence rules. Since nvarchar has higher precedence than varchar SQL Server has to convert the column data to nvarchar before comparing values. That means applying a function on the column and that would make the query non-sargable. SQL Server does however do it's best to protect you from your mistakes so it uses a technique described by Paul White in the blog post Dynamic Seeks and Hidden Implicit Conversions to do a seek for a range of values and then doing the final comparison, with the conversion of the column value to nvarchar, in a residual predicate to filter out any false positives. As you have noted, this does however not work when the collation of the column is a SQL collation. The reason for that, I believe, can be found in the article Comparing SQL collations to Windows collations Basically, a Windows collation uses the same algorithm for varchar and nvarchar where a SQL collation uses a different algorithm for varchar data and the same algorithm as a Windows collation for nvarchar data. So going from varchar to nvarchar under a Windows collation will use the same algorithm and SQL Server can produce a range of values from, in your case, a nvarchar literal to get rows from the varchar SQL collation column index. However, when the collation of the varchar column is a SQL Collation that is not possible because of the different algorithm used. 

Not seeing the data you have makes this a bit harder but I managed to reproduce what you see if you see with this: 

Update: A demonstration of the different sort orders for varchar columns using windows and sql collation. SQL Fiddle MS SQL Server 2014 Schema Setup: 

Use SET STATISTICS IO (Transact-SQL) and SET STATISTICS TIME (Transact-SQL) and execute your queries in SQL Server Management Studio. 

*I actually started the original query and forgot about it. Scratched my head trying to figure out why my computer was sluggish and found that the query had been running for 40 minutes. 

You can generate the extra row(s) by doing an unpivot using a cross apply and the table value constructor. 

The flwor in the value function walks through the nodes and returns the first value that is an integer. 

Update 2: If you know for sure that your value never contains a period and that it is always a four part name you can use parsename. 

First add the new record after the existing one and the delete the first occurrence where Guid is a match. Without in the delete you will delete all occurrences, not only the first one. 

There sure is a lot going on here so we will just have to see where this leads. First off, the difference in timing between SQL Server 2012 and SQL Server 2014 is due to the new cardinality estimator in SQL Server 2014. You can use a trace flag in SQL Server 2014 to force the old estimator and then you will see the same timing characteristics in SQL Server 2014 as in SQL Server 2012. Comparing vs is not fair since they will not return the same result if there are more than one matched element in the XML for one row. will return one row from the base table regardless, whereas can potentially give you more than one row returned for each row in the base table. We know the data but SQL Server does not and has to build a query plan that takes that into consideration. To make the query equivalent to the query, you could do something like this. 

I see no benefit of having two indexes when one is enough to get the job done. Removing the extra index will save you disk space and IO cost when updating the table. 

Some parts could do with some more information. This extracts the last number in . So will give you a . And this part returns all but the last number. For it will return . I shamelessly stole the test data generated by wBob (thanks and much appreciated) and found out that this version is faster on my machine. I use a really ancient 2 core laptop for the tests so the result could be different on a real server. Any way, for me it took 3 minutes to execute the code by wBob and my solution was about 20 seconds. 

The best workaround in your case is probably to get the values you need into a temp table and then query the temp table with a cross apply to the UDF. That way you are sure that the UDF will not be executed more than necessary. 

You are using a linked server to access and . The first query is sent as is to the other server and executed there returning only the rows you want. The second query is doing a join between a local table and a remote table . To do that it fetches all rows from and all rows from to your local server and does the join operations locally. The drop in performance is there because it takes time to move the entire contents of and from the remote server to the local server.