Housing prices is a popular regression data set. Here is one example. Car prices is an other popular choice. Here is one example. Remember there is "no free lunch" in machine learning. Models tend to not generalize across domains. 

A GitHub repo with ~1,000 pdfs is here. Another GitHub repo has a corpus of pdf examples, including edge-cases, is here. 

It might not be your clustering that is the problem. But the visual representation of your clustering. It is unclear why you are plotting elements on a hardcoded grid. You should just plot the actual values of the raw data. Something like: 

Given scikit-learn's API, you create a separate instance for each optimizer and compare the results to see which optimizer makes better predictions. It would looks something like: 

Assuming your file can be converted to "plain text" from pdf format, you could write a regular expression rule to find the start of sections. It would require finding all lines that start with a digit followed by a literal period, ignoring tabs. The regular expression is something like 

Given that only have labels for a small subset of data, you should use unsupervised methods. You can cluster all the sensors data. Then see if there is a pattern to where the 100 bad sensors are and can generalize to the other sensors. If a majority of the 100 bad sensors are in the same region, you can label that cluster as bad and make a threshold based on that. There is also the field of semi-supervised learning that might be appropriate. 

You do not need to use Word Embedding to solve this problem. You are performing set membership check. In this problem, each book title and author combination is an element. You can create a hash for each element and perform constant time lookup. This assumes you have consistent normalization for each data source. 

"Source credibility" of Internet articles is best calculated through the Page Rank algorithm. Algorithmically determining writing quality might be intractable. However Page Rank could be a proxy. If an article is a hub then it is the authority on the topic and can be assumed well written (or at least very useful). 

You could reframe the problem as a regression - prediction of a single real-valued dependent variable based on several independent variables. If you choose to model with regression, then it technically a "calibration" problem. A calibration problem takes a known observation of the dependent variables is used to predict corresponding explanatory variables. 

Alternatively, you can compute the Jaccard Similarity between hashed documents and choose a similarity threshold. The algorithm is covered in detail in Chapter 3/"Finding Similar Items" of Mining of Massive Datasets book. 

It depends on how the cluster (i.e., compute resources) are being managed. For example, Apache Mesos is an open-source cluster management tool. One established option is Horovod. It is designed to manage workloads for TensorFlow and Keras across multiple cores in a single machine or across multiple machines. 

Conjugate gradient descent is a variation on gradient descent. Gradient descent is a method of finding the minimum of a function by taking steps that reduce the error between the data and the model parameters. Conjugate gradient descent extends gradient descent by searching a plane, instead of a line. The plane is defined as a linear combination of the gradient vector and the previous descent step vector. Conjugate gradient descent is very good at finding the solutions to a set of sparse linear equations. Gradient descent and variations are general methods for finding the best parameters. The best practice is to define your specific model and then call on a separate gradient descent package to search for optimal values (i.e., hyperparameters) for the model. 

One method to compare the topics across two corpora and measuring their similarity is with Kullback-Leibler divergence, aka relative entropy. Kullback-Leibler divergence is a measure of how one probability distribution diverges from a second probability distribution. Another, more scalable algorithm can be found in Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment 

The simplest method to test if a new document is like previous documents is to hash them and look for collisions. Hashing items put them into "buckets". If two items are in the same "bucket" (aka, collide), they are similar. Generally, people use locality-sensitive hashing (LSH) for document similarity. The process is: 

You are describing a variation on optimization, maximize a real function by systematically choosing input values from within an allowed set and computing the value of the function. Specifically, a variation called linear programming where you to maximize the similarity (similarity is the inverse of distance) subject to a couple of constraints: 

You should not use which is for plotting all-by-all comparisons. You should use which can specify particular comparisons to make. Then use subplots can control the tiling. 

Bayesian models specify priors to inform and constrain the models and get uncertainty estimation in form of a posterior distribution. Non-Bayesian Deep Learning computes a scalar value for weights and biases at each layer. Bayesian Deep Learning calculates a posterior distribution of weights and biases at each layer which better estimates uncertainty but increases computational cost. Edward is a Python package for Bayesian inference, including Deep Learning. 

You are describing the "Language Modeling" problem in NLP. Language modeling finds the probability distribution over sequences of words - given the words I have seen so far, what are the most likely words that will come next or are missing. word2vec can find reasonable collocations/cooccurrence for individual words. However, word2vec will do a poor job with sentence completion because it does not model grammatical dependencies. You can find a word2vec nearest individual word demo here. 

The better question might be - Why you do want to mix deep neural networks (DNN) and a finite state machine (FSM)? There is research showing that a DNN can simulate any FSM. Since there are more frameworks for DNN and DNN can perform more tasks than FSM, it appears to be more useful just to forgo FSM altogether. More specifically, "Neural network for synthesizing deterministic finite automata" shows how a relatively simple neural network (NN) can quickly and automatically learn the correct deterministic finite automaton (DFA). There is a strong trend towards end-to-end DNN. 

TensorFlow is a general purpose library for numerical computation using data flow graphs. It is primarily used for neural networks but can be used for any mathematical operations on multidimensional data arrays (tensors). Thus, TensorFlow can be used to estimate binary logistic regression with explanatory categorical variables. An example can be found in the TensorFlow tutorials here. 

Scikit-learn package has a limited selection of optimizers. The scipy package has many more optimitizers, including trust-region-reflective algorithm. You would have to use another third party package for Firefly algorithm. 

One solution to this problem can be found in Grammar as a Foreign Language. The paper outlines a system that "translates" English sentences into syntactic constituency parse trees using a sequence-to-sequence LSTM (Long Short-Term Memory model). A TensorFlow implementation can be found here. 

Since search result lists vary in length, normalized DCG is used. It is normalized by the ideal ranking, sorting all relevant documents in the corpus by their relative relevance and producing the maximum possible DCG: 

Source: "From Word Embeddings To Document Distances" Paper The gensim package has a WMD implementation. 

This is an open issue in Reinforcement Learning (and all of machine learning). Google has published a paper titled "Machine Learning: The High Interest Credit Card of Technical Debt" which addresses the many ways these systems can degrade over time. One way is to follow fundamental production engineering techniques (e.g., test coverage and graceful exit). Another way is to monitor the data, then retrain the agent if new data is statistically out-of-sample. 

One result for MovieLens 20M using Factorization Machine can be found here. They got MAE: 0.60 and RMSE: 0.80. Another result for MovieLens 20M using Autoencoders can be found here. They got RMSE: 0.81. 

The problem you are describing is called "query rewriting", taking a user's string literal and processing it to find the best items in the search index. You can always start with rule-based logic to parse queries, either hard-coded or regular expressions. Then move to statistical learning methods to improve query rewriting. A user's query should be tokenized based on the collocations, frequently occurring n-grams. Tokenizing on collocations would group together meaningful words. There is also session-based context. Create an overall session state (e.g., "phones" or "fruit"). Each token would contribute changes in session, some tokens would contribute a little and others would contribute a lot. For example, the presence of "case" would automatically void the "phone" state. The session state would then change term weightings. Session state works well because users often make several queries in a role, called query refinement. 

Graph embedding learns a mapping from a network to a vector space, while preserving relevant network properties. Vector spaces are more amenable to data science than graphs. Graphs contain edges and nodes, those network relationships can only use a specific subset of mathematics, statistics, and machine learning. Vector spaces have a richer toolset from those domains. Additionally, vector operations are often simpler and faster than the equivalent graph operations. One example is finding nearest neighbors. You can perform "hops" from node to another node in a graph. In many real-world graphs after a couple of hops, there is little meaningful information (e.g., recommendations from friends of friends of friends). However, in vector spaces, you can use distance metrics to get qualitative results (e.g., Euclidian distance or Cosine Similarity). If you have qualitative distance metrics in a meaningful vector space, finding nearest neighbors is straightforward. "Graph Embedding Techniques, Applications, and Performance: A Survey" is an overview article that goes into greater detail.