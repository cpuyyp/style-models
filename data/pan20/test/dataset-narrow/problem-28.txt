It was getting a little big to fit in the comments so posting it as an answer instead. Might not be a solution to your problem but the concept is related. People usually forget that whenever you define a transformation matrix by placing respective basis vectors in respective columns, you are specifying that with respect to another basis usually the right-handed or left handed world coordinate space. For example your above matrix \begin{bmatrix} r_x & u_x & -l_x & 0 \\ r_y & u_y & -l_y & 0 \\ r_z & u_z & -l_z & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} Reason why you put sign in the third column is you are working with a right handed coordinate system and the camera is looking in the negative direction (the -Z axis). So all these basis vectors are also with respect to another basis vector. Let's make this matrix simple so we can understand what happens when we multiply a matrix with a vector. Let the matrix $M$ be \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} This is your camera initially without any rotation/translation etc. This means your camera space $+Z$ axis maps to world space $-Z$ axis. Now consider a camera space coordinate $[2,3,6,0]^T$. Multiplying this vector by the matrix gives you $[2,3,-6,0]^T$. This is the world space representation of $[2,3,6,0]^T$. So as Nathan pointed your matrix transforms from camera space to world space. To do the inverse we just take the inverse of the matrix which is the transpose if the basis is ortho-normal. One of the reasons the transformation matrix is built this way is because it's much easier. It's easier to think about the transformed basis vectors of camera or any other space with respect to the world, write the matrix then invert it to go from the world space to that target space. 

Now we have to rotate around Z as this is the last in the order. But notice that the rotation around is similar to the previous rotation we performed around . The plane will roll either towards the left or right. This means we lost 1 degree of freedom. I think this is what joojaa was trying to explain to me. So everywhere where people are saying the axis coincide its the local, moving frame coinciding with the original frame. Initially both the local and global frames are coinciding. In this case when we rotated around by 90 the local moving X-axis collapsed on to the fixed original - axis causing this gimbal lock 

Unfortunately this won't work so well if you want say a dark red glow because the output pixel will either be the emissive colour (dark red) or the lit texture colour, which could be much brighter. In effect our emissive areas could end up darker than the surrounding illuminated areas. We could add the emissive colour to the illuminated texture but that limits how much control over final colour we have. e.g. 

From looking at your example images this looks like a case of Alpha blending being applied to the RGB channel of the font-atlas texture and then again applied via the A channel when rendering. The tell-tale sign of this is usually a feint dark halo surrounding your glyphs which is caused by the unwanted mid-greys present in the texture being multiplied by alpha. For font rendering it is typical to have anti-aliasing in the A channel and have your RGB channels a constant colour. 

GLSL would not know were to place anotherVar, because it would need arraySize to be set with a value before it can compute the offset of 'anotherVar'. I suppose the compiler could be clever and re-arrange the order of uniforms to get around this, but this will fail again if you have 2 or more variable sized arrays.. 

The top Zuckerberg image looks like you first convert your background Zuckerberg image to grayscale. Then you can add a second layer with the rainbow texture but use the 'Screen' blending mode, this will cause it to colourise the layers beneath it, but in such a way the whites are preserved. You could also try 'Multiply' mode too if the results are too bright. The second one with the facebook logo looks like alpha blending. This can be done using a default layer in photoshop but setting the layer opacity to some midway value, i'd guess around 75% opacity. In this version the rainbow texture is the background, and the layer contains the 'F' logo. 

I would be tempted to use a solution which involves decoding a single rectangular video stream so you don't need any special video decoder. As you have said there will be performance penalties for decoding multiple seperate streams. Something like octohedran cube maps might be a good offline way to convert 6 video streams into a single video stream while minimizing distortion. A relatively cheap pixel shader can be used to project the 'cube' map back to camera space. Further reading here > $URL$ 

In the book Computer Graphics Principles and Practice, they use the term specular reflection when they want to imagine things resembling a mirror and glossy reflection when things like a polished door knob or an orange skin. The charts shows you exactly that. When a material has more specular color, it should have less diffuse color due to the conservation of energy. That is, the sum of the light reflected specularly and light absorbed and emitted in random directions must be less than equal to 100% (the amount of light incident on the surface). Hence when you increase the specular color the material tends to go white or have a slight tint of the color like in metals. Where as glossy surfaces can have more diffuse color but show a specular highlight like the surface of an orange skin. So assuming the CGPP's point of view, we can say in pure specular reflection, the diffuse part is much less than the glossy part. Where as in glossy reflection the diffuse part is usually greater. 

So thanks to joojaa I finally got a hint, and I searched on the net further and found this link which cleared all the doubts and has my answer. Though I am still posting it here as a summary. So anyone reading this and who has similar problem to mine here is what I understood. Suppose we are considering the tait-bryan angle order X-Y-Z that is rotate first along X then Y and finally Z. Also to make it clear we are rotating around the "fixed" axes since a rotation matrix always rotates around the fixed axes. The matrix doesn't know anything about the axes moving or anything. 

If you are talking about an oblique perspective projection in which the line joining the eye and center of the projection plane is not perpendicular to the plane like here (upper left), 

I can't really answer this without the context so post some links where this is done. What I can tell you is that the general formula for viewport transformation as done by OpenGL is given as (taken from wiki) 

So i searched a lot after this and I think it was my confusion on FrameBuffer Objects. I thought you could use FBO's just like a default FrameBuffer and display the texture image attached to it but you can't. It's only used for offscreen rendering. So while you can use rendering commands to draw something to a "texture image" attached to it, you can't "display the image" by making it default framebuffer or something like that. 

I think (speculate) the likely reason for this is that it would be impossible to determine the location of a uniform if there are variable sized arrays in your uniform list which depend on the value of another uniform.. e.g. 

One technique you could use is break the image into blocks and measure each blocks variance - this way you can apply more samples to blocks with higher variance. The variance can be estimated by using 2 accumulation buffers instead of 1. You render each pass into an alternate buffer. The absolute difference between these buffers (with respect to each block) is proportional to variance. Upon presentation to screen you can add the two buffers together to get your full accumulation buffer back. 

AI_MATKEY_COLOR_TRANSPARENT is used to define the transparent colour and AI_MATKEY_REFRACTI is the refractive index. 

This projection is a 2d projection, with the Z axis pointing into your screen. When both vertices with the same X&Y are projected to screen space they will occupy the same screen coordinate so your line will either be 1 pixel or invisible. If you are expecting a difference in screen position from those 2 coordinates then you need a perspective projection. 

Before GPUs were popular we rendered without using Z-Buffers and instead used polygon sorting to make sure things were drawn back-to-front correctly. By having a BSP with only convex geometry we guarantee that each leaf node will not have any ordering issues with the polygons contained in them, the BSP traversal algorithm itself can then produce a depth-sorted node list for you. These days we have z-buffers so the need for convex leaf nodes isn't really that important, in some ways its better to render front-to-back also to take advantage of early-z optimisations on hardware. 

A more correct method would involve using 2 textures, one for diffuse illumination and another for purely emissive. These would be added together in the pixel shader. e.g. 

AssImp library may be a good choice for you, assuming you are using C++ ?. It supports a decent number of model formats. 

But we want certain parts of our texture to emit colours instead of being illuminated by only our light source. One method would be to use an extra channel in the input texture, say the Alpha channel, to control which pixels are emissive 

I don't think uniform arrays can be dynamically sized. In your case you should define the array as the maximum number of lights you will process and then use a uniform to control the number of iterations you do on this array. On the CPU side you can set a subset of the lights[] array according to the 'size' variable. e.g. 

To be honest, terms like these are very confusing as they aren't clear cut and on one side of the border. They are more grayish. I'm gonna tell you how I convinced myself, as I too had this confusion as soon as I read your question. But I managed to convince myself through this argument. First of all we are gonna clear up 4 terms, Radiance, Irradiance, Differential radiance and Differential Irradiance. "Radiance" is what you say associated with a certain direction. To be more formal and according to wikipedia, 

First of all we need to understand why do we need 4x4 matrices in the first place. With 3x3, we couldn't represent translation as it wasn't a linear transformation (it displaces the origin). So in order to avoid extra work, homogeneous coordinates and affine transformation was introduced. Now instead of doing $v' = Lv + t$ where is a linear transform and is the translation, we can do $v' = Av$ Where is the affine matrix. This makes it cleaner. So 4x4 matrices are a real necessity, we just can't work without them. In order to distinguish between vectors and points we use for points and for vectors. So you are suggesting to make this 4th dimension implicit and don't store it as it'll actually use space/memory. 

That's what I want to hear about more. Why "non-orthogonal" ? I can't see any problems with making the axes orthogonal? What does making the axes non-orthogonal give us? 2) After the above one has been answered. How is this all tied up to rotational matrices? How can we achieve gimbal lock through the matrices when they only rotate a given point/vector around global axis. For example If i multiply a column vector with a Rotation matrix around X-axis and then with It will rotate around the global X-axis first, then global Y-axis second. So how can I achieve the lock situation using matrices? EDIT:- To make it more clear I've also heard about rotation orders like in the order when Y axis rotates then and rotate with it but when rotates only rotates with it. thus making the axes non-orthogonal. I am assuming that's what it means by non-orthogonal mentioned in the wiki article. Here is a picture. 

If we rotate around the X-axis we can see that the plane will either roll left or right. Suppose we rotate a little around the X-axis and then rotate around so that it is pointing straight up like this