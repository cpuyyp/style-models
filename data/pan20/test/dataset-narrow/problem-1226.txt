The short answer to your question 1 is no, but for perhaps subtle reasons. First of all, System $F$ and $F_\omega$ cannot express the first-order theory of arithmetic, and even less the consistency of $\mathrm{PA}$. Secondly, and this is really surprising, $\mathrm{PA}$ can actually prove consistency of both those systems! This is done using the so-called proof-irrelevant model, which interprets types as sets $\in\{\varnothing,\{\bullet\}\}$, where $\bullet$ is some dummy element representing an inhabitant of a non-empty type. Then one can write down simple rules for the operation of $\rightarrow$ and $\forall$ on such types rather easily to get a model for system $F$, in which the type $\forall X.X$ is interpreted by $\varnothing$. One can do a similar thing for $F_\omega$, using a bit more care to interpret higher kinds as finite functions spaces. There's an apparent paradox here, where $\mathrm{PA}$ can prove consistency of these seemingly powerful systems, but not (as I'll show in a moment) normalization. The missing ingredient here is realizability. Realizability is a way to make certain programs correspond to certain propositions, typically in arithmetic. I won't go into the details here, but if a program $p$ realizes a proposition $\phi$, written $p\Vdash \phi$, then we have a certain piece of evidence for $\phi$, in particular if $p$ is normalizing, then $p\not\Vdash\bot$. We have: 

Another possible answer different from Andrej's is the given by the example of the $\omega$-set model of polymorphism. Since every function in the polymorphic calculus is computable, it's natural to interpret a type by a set of numbers which represent the computable functions of that type. Furthermore, it's tempting to identify functions with the same extensional behavior, thus leading to an equivalence relation. The relation is partial if we exclude the "undefined" functions, that is the functions which "loop" for some well-formed input. The PER models are a generalization of this. Another way to see these models are as a (very) special case of the simplicial set models of Homotopy Type Theory. In that framework, types are interpreted as (a generalization of), sets with relations, and relations between those relations, etc. At the lowest level, we simply have the PER models. Finally, the field of constructive mathematics has seen the appearance of related notions, in particular the Set Theory of Bishop involves describing a set by giving both elements and an explicit equality relation, which must be an equivalence. It's natural to expect some principles of constructive mathematics make their way into type theory. 

for 1., you proceed by induction on the term structure of a hypothetical closed term $t$ of type $\mathrm{False}$. Actually terms in normal form must be in the form: $$ \lambda x_1:T_1\ldots\lambda x_n:T_n.y\ t_1\ldots t_m$$ where $n$ and $m$ may be $0$. If $n$ is zero, then $t=y\ t_1\ldots t_m$, which is not possible since $t$ is closed (typed in the empty context). Otherwise, we may apply inversion and conclude that $T_1=*$ (and $x_1=X$), and we get $$ X:*\vdash\lambda x_2\ldots\lambda x_n.y\ t_1\ldots t_m\ :\ X$$ to be derivable in the CoC. Now since $X$ is not a $\Pi$, we can apply inversion again to conclude that $n=1$ and in fact the above term is simply $$ X:*\vdash y\ t_1\ldots t_m\ :\ X$$ Inversion yet again shows that $$X:*\vdash y\ :\ \Pi y_1:U_1\ldots\Pi y_m:U_m.X $$ but $y$ must be $X$, since it is the only variable around! Therefore $m=0$, and $X=*$, which is impossible, contradiction. Now all this reasoning is intuitionistic, as I've proven a negative (there can be no proof of...) and proofs of negations are always constructive. I do rely heavily on inversion, and you'll just have to take it on faith that this also can be proven in arithmetic, without the excluded middle, which is non-trivial. Now for 2. we define consistency to mean "does not prove $\mathrm{False}$"! Again a negative statement. Now if the CoC is normalizing, one can take any normal form of a proof of $\mathrm{False}$ and use the above argument to get a contradiction. Again a constructive argument! Finally to tie it all together. Now suppose you had enough arithmetic to carry out the above arguments in CoC. Note that this is almost possible: you actually need to add the axiom $0\neq 1$ to get anything off the ground. You can then prove that normalization implies consistency within the CoC, and you also have enough arithmetic for the second incompleteness theorem to apply. Therefore you cannot (if CoC is consistent!) prove normalization, as then you would have a full proof of consistency within CoC. 

This is a hard question to answer, in part because it's unclear what it means to get something "by accident". Regularly, though, people run into the error of Coq, as some quick googling will show (e.g. here). This certainly sometimes happens by accident, sometimes in the attempt at showing inconsistencies or testing the limits of the prover. One of the big sources of universe inconsistencies unsurprisingly comes from trying to formalize category theory in Coq, as is mentioned in this article by Gross, Chlipala and Spivak. In most cases, it's hard to tell whether the inconsistency comes from an actually inconsistent statement, or just a limitation of the proof checker, which sometimes has counter-intuitive restrictions on universes, particularly in the allowed parameters for inductive types. This is actually a somewhat tricky subject (see the official definition and Chlipala's more accessible discussion). 

The distinction is this: if STLC is taken as a primitive language at the type-level adding constructors and a small number of axioms is sufficient to give you the full expressive power of HOL. Taking $\iota$ as the base type of numbers ans $\omicron$ as the base type of propositions, you can add the constants $$ \forall_\tau:(\tau\rightarrow \omicron)\rightarrow \omicron\quad \supset:\omicron\rightarrow\omicron\rightarrow \omicron$$ where $\tau$ is an arbitrary type (so one $\forall$ constant for each type). One possible set of axioms: $$ \frac{\phi(x)}{\forall_\tau(\lambda x.\phi(x))}\mbox{$x:\tau$ not free in the hypotheses}$$ $$ \frac{\Large{\substack{[\psi]\\ \\ .\\ .\\ .\\ \\ \phi}}}{\psi\supset \phi}$$ where $[\psi]$ means that the hypothesis $\psi$ is discharged. Interesting fact: the other connectives $\exists_\tau, \vee$... can be derived from just those 2. The subtlety is distinguishing between $\lambda$-terms as a way to represent proofs, as predicated by the Curry-Howard-de Bruijn (Martin-LÃ¶f) correspondence, or as a way to represent the terms you reason upon. The two views are not incompatible, of course. In particular there is a typed $\lambda$-calculus that faithfully represents HOL (minus various axioms of course). This happens to be a sub-system of the Calculus of Constructions, and is described in detail by Geuvers in The Calculus of Constructions and Higher Order Logic. He also details the differences between the two (the CoC is not a conservative extension of HOL). 

I am really not qualified to give an informed answer to your question, but there are many reasons for which a theory-oriented researcher might be a good fit for a more applied environment. The main reason that comes to mind is this: good theoretic foundations are invaluable for a thorough understanding of an application which in turn increases trust, efficiency and decreases mental effort. In general it allows work to proceed much faster, as the foundations provide a mental reference that can be easily extended or made explicit. I'm sure there are many examples in complexity theory, where an abstract approach has led to fantastic practical improvements in concrete implementations, but the field I am the most familiar with is "theory B", in which programing language design has greatly benefited from advances in pure computing theory research. Haskell is a particularly good example. On the theory side, we have simple core languages like system $\mathrm{F}_\omega$ which have proven to be a very useful base for the core language of the GHC compiler. The research in type inference/reconstruction and more recently, on dependent types has guided the foundations of the Haskell design, including the Hindley-Milner architecture and GADTs. Monads, of course, were introduced in the from the most abstract domain of mathematics to the computing world by Moggi, a theorist. More pragmatically, knowledge of the theory of operational semantics turns out to be an invaluable tool for language extensions: the paper which introduces Software Transactional Memory in Haskell has explicit operational semantics for the small sub-language containing the STM constructs. This creates a reference for the implementation, and the ability to prove properties about a correct implementation, creating trust and understanding. All this point to the fact that having a strong theory background is an important asset of every research team, including those with a strong application theme. 

This question has been considered several times in the academic community, from the practical: Yakushev & Jeuring, Enumerating Well-Typed Terms Generically Fetsher & al, Making Random Judgments: Automatically Generating Well-Typed Terms from the Definition of a Type-System to the more theoretical Grygiel & Lescanne, Counting and generating lambda terms Implementations can pretty easily be found online. I didn't find any that addressed a system with iterators per se, but it shouldn't be a stretch from what already exists. 

My first advice is to take a look at chapter 15 of Girard, Lafont & Taylor Proofs & Types. In it, a weaker theorem is proved, but the basic ideas are quite similar, and the exposition should be simpler. Second: Provably type correct is a bit of a misnomer. The property looks more like definability or accessibility. In a nutshell it expresses $$ \forall t_1,\ldots t_n,\ \mathrm{Ind}(t_1) \wedge \ldots\wedge\mathrm{Ind}(t_n) \Rightarrow \mathrm{Ind}(f(t_1,\ldots, t_n))$$ where $\mathrm{Ind}(t)$ means the term $t$ is inductive, that is, built up from constructors. Being inductive is essentially being defined as an element of the inductive type, which makes the above property a statement about definability. Third: You are correct, if $B$ proves that $f$ is type correct, then $[B]$ is the definition of $f$. Work through an example first: If $\mathrm{add}: \mathbb{N}\rightarrow\mathbb{N}\rightarrow\mathbb{N}$, then the statement of type correctness is: $$ \forall x\ y,\ \mathrm{Ind}_{\mathbb{N}}(x)\rightarrow\mathrm{Ind}_{\mathbb{N}}\rightarrow \mathrm{Ind}_{\mathbb{N}}(\mathrm{add}(x, y))$$ in $\lambda\mathrm{PRED}_2$, which, erased, gives $$ \mathrm{N}\rightarrow\mathrm{N}\rightarrow\mathrm{N}$$ in $\lambda_2$ (with $\mathrm{N}=\Pi X:*.X\rightarrow(X\rightarrow X)\rightarrow X$) which is exactly the right type for $\mathrm{add}$! So at least the types add up. Checking that the proof of type correctness of $\mathrm{add}$ gives you a good definition is more tedious, but Leivant gives the details. Fourth: you are correct that (1 way of seeing) the converse requires examining the proofs of strong normalization. For each function definition in $\lambda_2$, you can "decorate" it into a proof of type correctness in $\lambda\mathrm{PRED}_2$, essentially using the "theorem for free" that comes with the type. A really nice exposition is Wadler, The Girard-Reynolds isomorphism, which is a highly suggested read for anyone in the field. Fifth: I think you are correct in your correction of 5.4.39. 

It is clear that can safely be applied to . However, for this to be statically verified, we need to prove the statement: $$ x < 2 \Rightarrow x \leq 1+1$$ So this imposes a restriction on the expressivity of the refinement language: either the statements that need to be proven to verify type-safety are decidable (linear arithmetic and the theory of equality is a common choice in this case), or there need to be dynamic casts that are verified at runtime. 

As means of clarification, I should note that Foetus is developed by Andreas Abel, who also developed the original termination checker for Agda, and worked on more advanced termination techniques since. The answer to your question might be a bit disappointing: the class of functions from $\mathbb{N}$ to $\mathbb{N}$ is exactly the functions that can be defined in system $\mathrm{F}$. The reason for this: the aforementioned class is equal to the provably terminating functions in Second Order Arithmetic ($\mathrm{PA}^2$) which in turn is equal to the functions definable in system $\mathrm F$ (see e.g. Proofs and Types, chapter 11). Furthermore, if you remove the polymorphism, then you fall down to functions definable in $\mathrm{PA}$, which happens to coincide with those definable in system $\mathrm{T}$. Again, the reson for this is that the decrease captured by the "call matrices" is provably well-founded, and that proof can be carried out entirely in $\mathrm{PA}$. However, this doesn't mean that Foetus is not more useful than system $\mathrm{T}$! In practice, more complex termination analyses are required to be able to accept certain presentations of computable functions. You don't want to have to do a complicated proof in Peano Arithmetic every time you write a unification function, for example. So in this respect, Foetus is very powerful, and allows you to define functions in a way which wouldn't be accepted by either Coq, Agda or any other common proof system. 

There can be no proof of $\mathrm{False}:=\forall X:*.X$ in the CoC in head normal form. Furthermore, this fact can be proven in a weak theory, say Peano Arithmetic (though the excluded middle is not required. This fact implies that if the CoC is normalizing, then it is consistent, and furthermore this implication does not use classical logic. 

I found signs that such a decision procedure was implemented in the (general purpose) theorem prover SPASS. In particular see the thesis of Ann-Christin Knoll, On Resolution Decision Procedures for the Monadic Fragment and Guarded Negation Fragment. This implements what you want, though I couldn't find the implementation online. 

A good rule of thumb with locally nameless is "pattern matching on a binder is opening". In particular, take a naive call-by-name normalization algorithm on terms with named binders that ignores variable capture (in OCaml syntax): 

Another possible answer to Neel's perfectly correct one: Suppose that there is a combinator $E$, well-typed in system F such that the above condition holds. The type of $E$ is: $$ E : \forall \alpha.\alpha\rightarrow \alpha\rightarrow {\bf bool}$$ It turns out that there is a theorem for free that expresses that such a term is necessarily constant: $$ \forall T,\ t,u,t',u':T,\ E\ T\ t\ u = E\ T\ t'\ u'$$ In particular, $E$ is the constantly true function or the constantly false function, and can not possibly be an "equality decider". 

If You take as a starting point $\beta+\eta$ equality at non-functional types and $(1)$ at functional types, what you get is extensional equality of terms, and it holds in most "natural" models. See e.g. Berardi et al for models of system $F$ and the classic Barendregt chapters 5 and 10. 

The lemma is proven by induction using an application of the conversion rule at the variable case. The theorem is then proven using the above lemma to discharge the conversion case.