1)R. Lader, N. Lynch, and A. Selman. A comparison of polynomial time reducibilities. Theoretical Computer Science, 1(2):103-124, 1975. 2)L.G. Valiant “The complexity of computing the permanent”, Theoretical Computer Science, 8 (1979), pp. 181-201. 3)A. Blass & Y. Gurevich “On the unique satisfiability problem.” Information and Control, 55(1-3) pages 80-88, 1982. 4)J. Balcazar, R. Book & U. Schoning. “The Polynomial-Time Hierarchy & Sparse Oracles” Journal of the Associate for Computing Machinery, Vol 33, No3. July1986. pages 603-617. 5)L.G. Valiant & V. Vazirani “NP is as easy as detecting Unique Solutions” Theoretical Computer Science 47 (1986) pages 85-93. 6)E. Allender. The complexity of sparse sets in P. In proceedings of the 1st Structure in Complexity Theory Conference, pages 1-11. Springer-Verlag Lecture Notes in Computer Science #223, June 1986. 6)R. Beigel. On the relativized power of additional accepting paths. In proceedings of the 4th Structure in Complexity Theory Conference, pages 216-224. IEEE Computer Society Press, June 1989. 7)R.Beigel & J. Gill “Counting Classes: Thresholds, parity, Mods, and Fewness” Theoretical Computer Science Volume 103 Pages 3-23. 1992. 8)S. Fenner, L. Fortnow & S. Kurtz “Gap-Definable Counting Classes” Journale of Computer And System Sciences Volume 48 Pages 116-148 1994. 9)R. Beigel, H. Buhrman, and L. Fortnow. NP might not be as easy as detecting unique solutions. In Proceedings of the 30th ACM Symposium on Theory of Computing, pages 203-208. ACM Press, May 1998. 10)B. Borchert, L. Hemaspaandra & J. Rothe “Restrictive Acceptance Suffices for Equivalance Problems” LMS J Comput. Math 3 Pages 86-95 2000. 

If ${\bf NP} = {\bf PSPACE}$ 1) Polynomial Hierarchy would collapse to ${\bf NP }$. 2) We will now have that ${\bf NP } \not ={\bf NL}$ since we know that ${\bf PSPACE} \not = {\bf NL}$ ---UPDATE--- 3) It is known that ${\bf NL} \subseteq {\bf C_=L} \subseteq {\bf PL}$, where they are the logarithmic space bounded versions of ${\bf NP}$, ${\bf C_=P}$ and ${\bf PP}$ respectively. Then by definition none of these complexity classes could be equal ${\bf NP}$ under the assumption that ${\bf NP} = {\bf PSPACE}$. 

It is known that some (non-relativized) syntactic complexity classes between ${\bf P}$ and ${\bf PSPACE}$ have the following property, ${\bf P} \subseteq {\bf CoNP} \subseteq {\bf US} \subseteq {\bf C_=P} \subseteq {\bf PP} \subseteq {\bf PSPACE}$. I am wondering if there exists a (non-relativized) syntactic complexity class ${\bf X}$ such that ${\bf PP} \subseteq {\bf X} \subseteq {\bf PSPACE}$? What are the implications of existence or non-existence of complexity class ${\bf X}$ ? 

There are two oracle sets defined in T88 such that ${\bf NP^A \not \subseteq \oplus P ^A}$ and ${\bf \oplus P^B \not \subseteq NP^B}$. Therefore, it seems unlikely that you can show that inclusion with the currently known methods. 

${\bf E} \not = {\bf NP}$ does not imply ${\bf E} \subset {\bf NP}$ nor ${\bf NP} \subset {\bf E}$. Similarly, ${\bf E} \not = {\bf PSPACE}$ does not imply ${\bf E} \subset {\bf PSPACE}$ nor ${\bf PSPACE} \subset {\bf E}$. You would need to show one of these containment's to be able to get a proper separation result out of those inequalities. 

I don't have much to add to the other answers, except that for termination behavior, it is sufficient to consider only the $\lambda_I$ version: see e.g. Strong Normalization from Weak Normalization by Translation into the Lambda-I-Calculus by Gortz, Reuss and Sorensen. 

At least the problem of whether 2 terms are equal modulo the theory of Cartesian Closed Categories (or $\beta\eta$ conversion) is decidable, because (in part) of the normalization property. Another, more categorical way to see this is by extracting a conversion algorithm through normalization by evaluation which gives decision procedures for equality in categories which can naturally be embedded in the presheaf or sheaf category over sets. See e.g. Altenkirch, Dybjer, Hoffmann & Scott, Normalization by Evaluation for Typed Lambda Calculus with Coproducts for the version with products and coproducts (the version for just CCCs without coproducts can be found in the references). An account which gives examples of implementations can be found here. Solving more complex questions than conversion, or solving conversion questions in the presence of datatypes like the natural numbers becomes undecidable rather quickly. I'm not aware of much work on algorithms for such systems, which I would be interested in as well. One could try simply encoding the equational theory into a first-order logic prover like Vampire, but I don't know how well that would work. It would be an interesting experiment! 

You may know this, but SMT solvers very often handle "quantifier-free" (implicitly universally quantified) linear arithmetic on integers, and sometimes even formulas with quantifier alternations (though with more limited success). You might want to try Z3 which performs quite well in practice, using variants of the Omega method referred to above, but with many twists and heuristics. In particular, the simplify command (explained here) seems to do what you want, i.e. simplify formulas to an equivalent form. 

As far as I know, showing that this direction is the hard part of Wells proof! At least this is what Pawel (Urzyczyn) explained to me a few years back. Apparently it is not too hard to show that type checking is undecidable; the hard part is showing that this implies undecidability of type reconstruction! Indeed there are some cases in which the first is undecidable and the second decidable: see e.g. Dowek 1993. 

You can try the Coq user manual, in particular this section is pretty nice (the server seems to be down at the moment though). For meta-theory you can try some recent work of Benjamin Werner et al., see Proof-irrelevant model of CC with predicative induction and judgmental equality and On the strength of proof-irrelevant type theories for the most salient work. 

I think there's a larger discussion to be had about how to distinguish syntactic notions from semantic notions in the field of formal logic, but I'm not sure I feel qualified to have it. 

Consider the following simple monotone circuit model: each gate is just a binary OR. What is the complexity of a function $f(x)=Ax$ where $A$ is a Boolean $n \times n$ matrix with $O(n)$ 0's? Can it be computed by linear size OR-circuits? More formally, $f$ is a function from $n$ to $n$ bits. The $i$-th output of $f$ is $\bigvee_{j=1}^{n}(A_{ij} \land x_j)$ (i.e., an OR of the subset of input bits given by the $i$-th row of $A$). Note that $O(n)$ 0's split the rows of $A$ into $O(n)$ ranges (subsets consisting of consecutive elements of $[n]$). This makes it possible to employ known range query data structures. E.g., a sparse table data structure can be turned into an OR-circuit of size $O(n\log n)$. Yao's algorithm for range semigroup operator queries can be turned into an almost linear circuit (of size $O(\alpha(n) \cdot n)$ where $\alpha(n)$ is inverse Ackermann) In particular, I don't even know how to construct a linear size circuit for a special case where each row of $A$ contains exactly two zeros. While the case of exactly one zero in each row is easy. (Each output function can be computed by an OR of a prefix $[1..k-1]$ and a suffix $[k+1..n]$, which can be precomputed by $2n$ OR-gates.) 

(I tried to post this as a comment to Stasys' answer above, but this text is too long for a comment, so posting it as an answer.) Ivan Mihajlin (@ivmihajlin) came up with the following construction. Similarly to Stasys' proof, it works for the case when the maximum (rather than average) number of 0’s in each row is bounded. First, consider the case when every row contains exactly two zeros. Consider the following undirected graph: the set of vertices is $[n]$; two nodes $i$ and $j$ are joined by an edge, if there is a row having zeros in columns $i$ and $j$. The graph has $n$ edges and hence it contains a cut $(L,R)$ of size at least $n/2$. This cut splits the columns of the matrix into two parts ($L$ and $R$). Let now also split the rows into two parts: the top part $T$ contains all columns that have exactly one zero in both $L$ and $R$; the bottom part $B$ contains all the remaining rows. What is nice about the top part of the matrix ($T \times (L \cup R)$) is that it can be computed by $O(n)$ gates. For the bottom part, let’s cut all-1 columns out of it and make a recursive call. The corresponding recurrence relation is $C(n) \le an + C(n/2)$ implying $C(n)=O(n)$. Now, generalize it to the case of at most $d$ zeros in every row. Let $C_d(n)$ be the complexity of an $n \times (\le dn)$ matrix with at most $d$ zeros per row (if there are more than $dn$ columns, then some of them are all-1). Partition the columns into two parts $L$ and $R$ such that at least $n(1-2^{-d})$ rows (call them $T$) satisfy the following property: if there are exactly $d$ zeroes in a row, then not all of them belong to the same part (denote the remaining rows by $B$). Then make three recursive calls: $T \times L$, $T \times R$, and $B \times (L \cup R)$. This gives a recurrence relation $C_d(n) \le an + 2\cdot C_{d-1}(n(1-2^{-d}))+C_d(2^{-d}n)$. This, in turn, implies that $C_d(n) \le f(d)\cdot n$. The function $f(d)$ is exponential, but still.