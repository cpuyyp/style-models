First compute the edit distance between all pairs of strings. See $URL$ and $URL$ Then exclude any outliers strings based on some distance threshold. With remaining strings, you can use the distance matrix to identify the most central string. Depending on the method you use, you might get ambiguous results for some data. No method is perfect for all possibilities. For your purposes, all you need is some heuristic rules to resolve ambiguities -- i.e. pick two or more candidates. Maybe you don't want to pick "most central" from your list of strings, but instead want to generate a regular expression that captures the pattern common to all the non-outlier strings. One way to do this is to synthesize a string that is equidistant from all the non-outlier strings. You can work out the required edit distance from the matrix, and then you'd randomly generate regular using those distances as constraints. Then you'd test candidate regular expressions and accept the first one that fits the constraints and also accepts all the strings in your non-outlier list. (Start building regular expressions from longest common substring lists, because those are non-wildcard characters.) 

I have hourly temperature and power consumption data of several days of a month. The pattern is almost similar across days like this: 

Using above approach, for 24 hours of a specific (test) day and using past 10 days consumption, I have obtained results as: Figure interpretation: Black line represents usage of current hour of current day; red and blue lines represent mean and median of past 10 days for same time interval 

Using above formula I think that I am placing knots at 9 AM and at 6 PM. Right? I don't know how should I enforce knots in temperature feature exactly at these specific times, so that knots of and will sync. 

I need to show this 10-day usage with a single baseline/representative curve. I can calculate baseline curve simply by taking mean/median of these 10 days data, but before that I need to answer following questions: 

I created a scoring system ("Thomas Scoring System") to deal with this problem. If you treat "distance" as a similarity score, this system should work for you. $URL$ 

This is how to do it mathematically, but I suggest that you consider whether it is wise, from a data analysis point of view, to normalize by school size. This depends on the purpose of your analysis and specifically how this ratio is being analyzed in relation to other data. 

So far the answers have focused on learning particular methods. They are fine, but they won't make you a Data Scientist. Being a Data Scientist is not solely or even primarily about having mastery of particular data analysis methods (ML or others). Most fundamental is problem solving and decision support. What ever data you collect, what ever analysis methods you apply, and however you improve those methods over time, these must support the over-arching goals of solving problems or making better decisions. You need to start getting first-hand experience with data in your field. I don't mean Kaggle data (i.e. already cleaned). I mean raw data or nearly raw. A good 50% of a data scientist's time is spent wrangling raw data and cleaning it to the point where it's usable in analysis. You need to learn how to deal with missing data, erroneous data, ambiguous data, misformatted data, and so on. You should also get some experience with decisions that do not map neatly on to the data. Recommender systems are easy in this regard. For example, you might take on the challenge of evaluating software vulnerabilities to guide vulnerability management decisions. 

First model for data from Midnight to 10 AM as usage remains almost constant during this time, and temperature does not vary too much Second model for data from 11AM till 6 PM. This portion follows a sharp increase and then almost constant usage Third model for data from 7 PM till midnight. This portion shows constant decrease in power 

I want to find outliers in power consumption in real-time, at hourly rate, i.e., at the end of the hour, I should say whether power consumption in current hour was outlier/anomalous or not. Approach: Till now, I am done with following steps 

I have time-series data of 10-days for the same time interval as shown in below figure. Here it shows one-hour power consumption for 10 days. Data is sampled at 10 minutes rate. 

Is your Masters in Computer Science? Statistics? Is 'data science' going to be at the center of your thesis? Or a side topic? I'll assume your in Statistics and that you want to focus your thesis on a 'data science' problem. If so, then I'm going to go against the grain and suggest that you should not start with a data set or an ML method. Instead, you should seek an interesting research problem that's poorly understood or where ML methods have not yet been proven successful, or where there are many competing ML methods but none seem better than others. Consider this data source: Stanford Large Network Dataset Collection. While you could pick one of these data sets, make up a problem statement, and then run some list of ML methods, that approach really doesn't tell you very much about what data science is all about, and in my opinion doesn't lead to a very good Masters thesis. Instead, you might do this: look for all the research papers that use ML on some specific category -- e.g. Collaboration networks (a.k.a. co-authorship). As you read each paper, try to find out what they were able to accomplish with each ML method and what they weren't able to address. Especially look for their suggestions for "future research". Maybe they all use the same method, but never tried competing ML methods. Or maybe they don't adequately validate their results, or maybe there data sets are small, or maybe their research questions and hypothesis were simplistic or limited. Most important: try to find out where this line of research is going. Why are they even bothering to do this? What is significant about it? Where and why are they encountering difficulties? 

To follow this intuition, I used three models accordingly and later combined predictions from these to output sequence of 24 numbers for coming day. Formula for each of these models is: , but each model is trained with the data corresponding to specific time duration of the day. Instead of dividing the data manually and using three separate models, is there any other existing technique which will take care of our intuition and does not need manual division of data or creation of separate models. During my search, I found that I can use GAM (Generalized Additive Models) and I came up with following formula 

Using this data I want to predict the usage of a coming day. I have features : 1) hour of the day 2) temperature; and the response variable, power. Looking at the data, I believe I should fit three separate models and not a single model 

the say you're storing it is fine in general you can further transform/store your data depending on your use case 

Data Scientists code every day. However, just because you don't have background doesn't mean you can't pick it up! The level of programming you need to know to start doing Data Science isn't very high, but you will at least need: 

Allowing me to, for example, look at the relationship between the Distance_speed1_Control vs Energy_speed1_Control columns. Basically subset/aggregate your data and then use the dcast to get the rows and columns the computer needs. 

the logical mindset to phrase the solution to your problem in procedural code to know the programming language, functions, and libraries needed in this field. 

Personally I would recommend Python first. To me the language places more emphasis on readability and cleanliness, making it a great first language. It's also a general purpose language so it's good to know. I did start with R though and it's also good, but is more function-over-form IMO. Try both out and see which feels best first, since you'll likely have to pick up both if you delve into this field anyways. 

Instead of "recursive neural nets with back propagation" you might consider the approach used by Frantzi, et. al. at National Centre for Text Mining (NaCTeM) at University of Manchester for Termine (see: $URL$ and $URL$ Instead of deep neural nets, they "combine linguistic and statistical information". 

Of the three terms, I suggest that MacKenzie's "performativity" is the best fit to your situation. He claims, among other things, that the validity of the economic models (e.g. Black-Scholes option pricing) has been improved by its very use by market participants, and therefore how it reflects in options pricing and trading patterns. 

Reinforcement learning does not depend on a grid world. It can be applied to any space of possibilities where there is a "fitness function" that maps between points in the space to a fitness metric. Topological spaces have a formally-defined "neighborhoods" but do not necessarily conform to a grid or any dimensional representation. In a topological space, the only way to get from "here" to "there" is via some "paths" which are sets of contiguous neighborhoods. Continuous fitness functions can be defined over topological spaces. For what it is worth, reinforcement learning is not the be-all-end-all (family of) learning algorithms in fitness landscapes. In a sufficiently rugged fitness landscape, other learning algorithms can perform better. Also, if there are regions of the space where there are no well-defined fitness function at a given point in time, it may be indeterminate as to what learning algorithms are optimal, if any. 

Say I want to find whether power usage between 9 AM to 10 AM was anomalous? For this, I first find the usage of past n days during the same time interval, then I find the mean/median of all the previous usages Now, I have usage of the current day and the mean/median usage of previous n days. Which statistical measure should I use to declare whether current day usage was anomalous or not? 

From the visual inspection, I can say that the usage between 07:10 - 08:00 and between 22:10 - 23:00 is anomalous as there is big difference between actual and previous mean/median usage. I don't know which statistical measure should I use to point out such anomalous instances automatically, using the discussed approach.