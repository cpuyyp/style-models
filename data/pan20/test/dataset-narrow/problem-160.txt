You will often see this scenario on point to point links. eg. (172.16.0.0/31 and 172.16.0.1/31). RFC 3021 describes this situation for conserving address space. Packetlife has a good blog post about this as well. 

You could use a tool like iperf to measure throughput and latency on each node with different inspection/filtering features turned on the firewall. This will show how the firewall features affect throughput/latency. See more at iperf's website Keep in mind that there will be other factors like the cabling and each node's resources that can effect these numbers. You might also look into logging/debug features of the firewall to see if there is any information from there you can look into. 

MX, CNAME, PTR, and NS records point to hostnames. A records point to IP addresses. Putting an IP address in an NS record is a pretty common DNS misconfiguration. One reason is that you might have NS records pointing to servers outside of your domain for someone doing secondary DNS for you. That remote domain might change the IP address of the server (but leave the hostname alone). 

Since you say you see no capabilities, there is no "route refresh" capability being advertised so there is no choice but to reset the BGP session in order to effect the change in routing policy. References: 

I am sure this is a common question but would like to know the best practices. I have a pair of Juniper routers connecting to a IRFed HP 10k core switch. My HP core runs stp and we are tyring to extend the L2 of HP core to router as well, for that we would need to enable the rstp on the Juniper mx routers. Does RSTP and STP on core works well or do we need to move to rstp on the core as well? 

We have 3 ISP connections (BGP based) coming to our redundant pair of MX routers. These routers connect to a WAN Switch and goes to Firewall and goes to core switch cluster. Now I would like to know what is the best practice to handle ISPs in terms of VLANs in the lan side. Is it better to have 3 different Vlans or trunk all ISPs in one VLAN in the wan switch and core switch? How do generally companies handle different routed ISP connections in Lan side? Do they separate them using L2 vlan tags or never mind about it? 

The router/switch you are plugging into should negotiate with the card. If you are plugging into a managed switch, you will likely be able to hard code the port's speed/duplex. In a Cisco device this would look like so: 

Your site-to-site tunnel will be transparent to the client devices. When the traffic hits each router from the 'inside' clients, it will be tagged as 'interesting traffic' against an access list and sent through the tunnel. There is no software or configuration requirements on the hosts. This is a great use case for two locations that do not have a direct leased line but need to share resources between each other. For more information, see Cisco's configuration guide here 

Now, I have a 2 MX240 routers terminating diverse path circuits from 2 ISPs. Now, I would like to configure multipath BGP confiugration. If One ISP goes down totally on both routers, traffic will go out through other ISP connection. Is it possible to have such configuration in Juniper routers and any pointers on it would be helpful. 

Most of the DDOS appliance can get input of sampling rate and multiply the values and should be able to detect volume attacks easily. So you don't need to worry about sampling. Most of the ISPs/DDOS protection providers uses sampled netflow for DDOS detection purpose. 1:100 is ok to have on DDOS perspective. It depends on what kind of collector you use. 

Brocade with their VDX line of ethernet switching gear uses it. But the thing to keep in mind is that different vendors implement TRILL in different ways and they don't necessarily interoperate. The standard is so loose that two vendor versions can claim to be standards compliant but be completely incompatible. 

All data bits flow at exactly the same speed down the wire (the speed of light in the medium of travel). So a bit of data travelling 1000 miles down a 10 meg circuit arrives at the other end at exactly the same time as a bit travelling down a 100Gig connection. The difference is the RATE at which the signal can transition state between 1 and 0. The faster you can transition the state of the signal, the faster you can send the next bit. So -- basically there is no such thing as "speed" because it all operates at the same speed (the speed of light), what differs is the RATE of data bits you can send. That is the "bandwidth". The faster I am able to flip the state of the signal from 1 to 0 and back again, the more data I can send in a given amount of time. Imagine two signalmen using old time semaphore flags. One signals quickly, one signals slowly. The actual light reaching your eye is arriving at the same speed in both cases but one can send a lot more data in a given time than the other. That signalman has more bandwidth and can send more data in a given period of time. The data doesn't travel faster, but that one can pack more data into a given slice of time. 

We are planning to put a Juniper QFX 5200 series switch which has 40G QSFP+ ports in front of the firewall which has QSFP+ 40G ports. They should work well with the 3800 Firewall's QSFP+ 40G port right? Actually my question is general. Irrespective device types and vendor, 40G QSFP+ will work across the devices right? So this will work without breakout cables and give 40G bandwidth on each link? 

We are planning to connect a pair of QFX 5200 (QFX is a WAN device where ISP connections terminates) and a pair Fortinet 3800. Fortinet further connects to my LAN core switch. For redundancy purpose we are planning to connect QFX and Fortinet in criss-cross so the each QFX will connect to each Fortinet. Now, Fortinet that runs in HA mode (active-active), should be able to talk to the same gateway IP in QFX. Each QFX has L3 separation no L2 link between them. So I cannot run VRRP on them. In this case, how do I provide redundant out going gateway in QFX. In each QFX I can have an aggregated interface and assign a IP to it. But I cannot have two WAN outgoing gateway IPs in the Fortinet without vrrp. In Fortinet HA mode, it is just one single config for both units. It internally syncs the config to other firewall. Any thoughts? 

In short, no, those VLANs would be considered primary VLANs, not private VLANs or PVLANs. PVLANs are a segmentation technique that can isolate hosts within the same 'primary' VLAN into 'secondary' or private VLANs. These PVLAN hosts belong to the same IP subnet. In order for the hosts to communicate, they will have to traverse a router. PVLANs have to be configured as such and are different from "regularly" configured VLANs. See if these links at Packetlife and INE can help explain more. PVLANs can get quite complicated. 

I think I understand these concepts but I'm a little rusty. Can someone give a concise, easy-to-understand explanation of these concepts? The planes are logical concepts, correct? Is this a cisco only thing? 

As long as you are fully meshed and the you can backhaul your own traffic arriving at one site but destined to the other, sure, it will work. So if your peering session drops with either ISP, your more specific will be withdrawn (you HOPE!) and the traffic will flow to the aggregate route on the other ISP and you can then backhaul the traffic to the other site. The most frustrating problem I see is network providers that advertise unreachable routes. Either through some screwup internally or whatever, they announce a route to their peers that they can not actually reach inside their network. There isn't much of a workaround for those cases. As for the F5 handing reply traffic back via the interface on which it was received, most load balancers can do that. I don't specifically know if the F5 will but I know Citrix Netscaler will and A10 will so it seems to be a rather standard feature. 

When we have multiple ISPs terminating on the router with full BGP routing tables, is there a guarantee that an incoming packet takes the same ISP for out going? How the BGP route selection process works when we have multiple ISPs? When we have multiple ISPs, it is still 1 routing and 1 forwarding tables? How the routes aer maintained in routing and forwarding table separately when we have multiple ISPs? Any pointers to BGP route selection process with multihoming would be helpful... Thanks 

I have a requirement like this. Please refer the attached image. We have 2 parallel firewall receives traffic from the internet and sends into LAN by NAtting into pvt ips. Both firewall clusters are independent clusters (never shares session) can receive anycast from internet traffic to LAN (I mean it can handle the same IPs from NATTing perspective/policies are synced). However, it connects to same core switch for the LAN. So now when the traffic goes out, is there any specific ways we can send the response traffic from LAN to the same firewall where it get NATTed, some kind of tags or any ideas we can use to send the traffic to the same firewall for sending it back to internet ? One way is PBR based, but then we have to configure a dedicated IP space for each firewall on the LAN side, is there any easier way to do this like tag based routing or something. 

I am in the progress of doing ISSU upgrades on some nexus 7710 chassis' with dual supervisor modules. I set a ping test that to traverse across the switch shortly before the upgrade and experienced about 15-20 seconds of lost pings. Is this test scenario a good one and does this amount of downtime seem acceptable? Is cisco's marketing on this feature just bs? I see the same behavior for a supervisor switchover as well. 

Can someone explain the differences between a RIB and FIB a little further? I didn't see any similar questions on the site so I thought this would be a good addition. The RIB routes and the FIB forwards? Is that all there is to it?