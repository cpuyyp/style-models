You can do this. You can create a single report that has multiple data sources (one per environment). Add the data sets required. Recreate the graphs (you may be able to leverage some cut-and-paste). In order to format them nicely, you will need to place the graphs into the cells of a matrix. You could also build a single report that links to other reports. I realize this is a high-level description of how to do it - if you need further details, let me know. 

It's a work around, but you could generate the CREATE PROCEDURE scripts for the database (right click database -> tasks -> generate scripts), find and replace CREATE PROCEDURE with ALTER PROCEDURE and then parse. I hope you get a better answer here - I'm interested too! :) 

In my experience, these two functions are identical in performance. But I would suggest two quick tests in your environment: 

That would be really fancy, but I don't think it's possible. You may be able to constrain the width of your bars of the chart and the width of your columns in your table as well as the placement of both to "fake it" though. 

I would like to know if it is possible to verify that an SSRS subscription that was set up to send an email sent it to a real address. That is, if I put in an email address that is not valid, can I find out that an undeliverable message was sent back from the destination server? I tested this with an email address that does return an undeliverable message when I emailed to it from Outlook. After the subscription ran, its status simply said . OUR SOLUTION: We set up a shared Outlook account with a generic email address. Those of us who have access to create subscriptions have access to this email account. We then modified the file on the ReportServer to send emails from this new generic address. Now, any time a subscription is set up to send to an incorrect email address, we can see the delivery failure in the shared Outlook inbox. Woo hoo! 

I am creating a few dozen tables as CREATE TABLE AS SELECT (CTAS). The only difference in the SELECT statements is specifying a different value in the WHERE clause for TEAM_ID. I'm an Oracle newbie and thought I could use bind variables to speed up the process, but those are not allowed in DDL statements. My data is quite large - 500+ million rows. What is my best option for speed? Currently we are using on the CREATE TABLE statement. Would it be better to explicitly create the table and INSERT with the hint and bind variables? Or do the gains of outweigh the gains of bind variables? (I am sure this is an "it depends" scenario, but I don't even know what it depends on...) 

I would like to set up an SSRS email subscription that attaches the report in PDF format to the email with a custom file name that changes daily. I was hopeful that I could to this via a data-driven subscription, but that does not seem possible. Does anyone have a handy work-around for this limitation? UPDATE: @swasheck provided a decent work-around, and one that we currently use. But I am hoping to remove the step of saving to a file-share. 

We have application database (APP_DB) that creates a "live copy" of itself via transactional replication (push) to a database (COPY_DB) on a separate server. This COPY_DB is used for reporting and troubleshooting. There is now a need for a small subset of the tables (4 tables out of 150) to be placed ("live") in a second database (NEW_DB). What would be the implications of setting up COPY_DB as a publisher of these four tables to subscriber NEW_DB? Good idea? Horrible idea? It seems like a silly idea to me, but I find myself in a situation where I have more control over the server where COPY_DB and NEW_DB live and could more easily implement such a solution rather than setting up a second publication on APP_DB. 

If you can reformat your CSV to , it will work. SQL Server doesn't like the colon between date and time. Otherwise, I agree with @HLGEM - you need a staging table. 

In an effort to scale some existing code, we are removing logic from CASE statements and putting it in dimension/lookup tables. Thus instead of CASE WHEN 'F' THEN 'Female' WHEN 'M' THEN 'Male' is now handled by a gender dimension. (A very simplistic example) But some of our CASE statements use LIKE operators, including comparison strings like and . I am looking for suggestions on the best way to handle this. Our fact tables have over a billion rows, and growing, so efficiency is key. We are Oracle 11gr2, if that is applicable. 

I have a table with two partitions and I would like to create two tables, one from each partition. Is there a quick way to do this (a la partition exchanges) instead of a create table as select (CTAS) statement? For what it's worth, this need arose because I want to exchange these two partitions into another table; from what I've read, in order to do a partition exchange the data needs to be in a non-partitioned table. 

The solution to this lies in the table creation itself. If I set on the index on my person ID column, the duplicate values are simply ignored and the rest are inserted appropriately. 

Try to take the MIN() of each of your CASE statements. If you can work this into a PIVOT, even better. 

You should format your label so that there is a background color of white (instead of default none). Right-click on one of the labels and select Series Label Properties. Select Fill. By default this is set to No Color. Change it to White. Click OK. That should take care of it! ADDITION: It may be possible to write an expression to change the distance between the label and the bar. When you select one of the labels, check out the properties window. You would need to play around with SmartLabels - specifically MaxMovingDistance and MinMovingDistance. You would need to have chart lines at specific intervals and then build the expression around the value being within one of those intervals. I know that is short on details, but hopefully the idea sketch gets you started in the right direction! 

When I look to at my query plan, I see plenty of clustered index seeks (yay!) and scans (not quite so yay!) but in some of them ordered = False. I will mention that we have parallelism involved, though it does not appear to have any relation to these. The indexes were set up with ordering. What does this mean? That the query plan is ignoring this order? Or that the query is choosing not to order? Or... heck, I have no idea what I'm looking at...! 

I just thought of an issue. A user could take the column that contains the password and use it as the subject of an email. Silly me! I am not sure why I didn't think of this before. Oh, well - documented here for others' future reference! 

Is there a way to write a check constraint such that values in a column are limited to table names currently in the schema? It appears subqueries are not allowed (ORA-02251), but logically I am looking to do: 

I am digging into and have found a single, persistent X lock of that looks odd. The database which on which the lock exists is rarely used. The tells me it's db_owner and the naming convention looks to be related to our replication processes. I would like to know if this is normal behavior and nothing to worry about or if I need to dig deeper to remove the lock. (We are experiencing a variety of intermittent performance issues on this server and have yet to determine why - I have no reason to think this is related, but I did want to mention it.) EDIT - I would also like to know what a lock on APPLICATION is - the MSDN descriptions are vague to me... 

I am fairly certain that it just looks at the third decimal digit... and that's 4 so it rounds down to 1.28. Clarify - it looks to the digit one greater than what you are rounding to... so it looks to the third because you are rounding to the second. 

We have been receiving complaints from a particular SSRS user that her reports run slow. I investigated in the table in the database and I observed something strange. Fairly consistently, the is much longer than other users. Both and are near the averages of other users. I am puzzled. The reports all use the same shared data source that runs as a service account. I would think if it was a crappy user computer issue, I would see differences in the rendering time. Same if there was a network issue. I don't know where else to look - any ideas? 

Now here's the "tricky" part... if you edit the report in the Source Reports folder in Report Builder, you can set up the sub-report to point to a specific folder structure on the ReportServer. This enables you to use sub-reports anywhere on your ReportServer, not just in the same local directory. My assumption is you can also type in the full path of the deployed report you wish to use as a sub-report into BIDS, but with Report Builder you can browse and click. So, in this case, I have made all of the site-specific reports point to the Master report in the Master Reports folder. This is a great solution for us. Now we can grant user access at the folder level without creating excessive burden on the report deployment process.