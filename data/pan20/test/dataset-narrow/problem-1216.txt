Exact Exponential Algorithms is a nice recent book about such algorithms. Algorithm X for the exact cover problem is also good to know. 

CTL properties can be checked in linear time (see Clarke et al). Long time ago I used to work in a company where many colleagues used Rulebase to verify integrated circuit designs. The property language is PSL, it is standardized by IEEE, and is a kind of CTL on steroids. 

I wonder if there are known cases where there is a significant gap between (1) and (2). By this I mean that either (a) the experimental data suggests a tighter asymptotic or (b) there are algorithms X and Y such that the theoretical analysis suggests that X is much better than Y and the experimental data suggests that Y is much better than X. Since experiments usually reveal average-case behavior, I expect most interesting answers to refer to average-case upper bounds. However, I don't want to rule out possibly interesting answers that talk about different bounds, such as Noam's answer about Simplex. Include data structures. Please put one algo/ds per answer. 

This is not an answer. It is a simple but long observation. I hope it will be useful. The decision version of your problem is: Does $\cal X$ contain a subset of $A$? This problem is related to the problem of evaluating monotone boolean functions of $n$ variables. A subset of $\{1,\ldots,n\}$ is equivalent to an $n$-bitstring, so the family $\cal X$ is equivalent to a boolean function $f$ of $n$ variables. Given a function $f$, one can define the least monotone function that is not bigger than $f$, namely $g(y)=(\exists x\subseteq y,\,f(x))$. The original problem is then reduced to evaluating $g(A)$. Conversely, the problem of evaluating a monotone boolean function can be reduced to the original problem, either naively by taking $f=g$ or by choosing an $f$ that makes $\cal X$ smaller. In practice BDDs tend to work well. So one possible approach is to build the BDD for $f$, derive from it the BDD for $g$, and then evaluate $g$. The average size of the BDD for $g$ must be $\Omega\left(\binom{n}{n/2}\right)$, because there are many monotone boolean functions. Hence, in theory this is a bad solution. But (1) a better analysis might be possible and (2) there might be tweaks to this approach that make it better. For example, I didn't use in any way the correlation between the size of $\cal X$ and the size of $g$'s BDD. (There must be a correlation, but I don't know if it is simple or usable here.) For completeness, a simple algorithm for computing the BDD for $g$ from the BDD for $f$ is the following. $$m(x?f_1:f_0)=x?(m(f_0)\lor m(f_1)):m(f_0)$$ Here $\lor$ is the standard or-operation on BDDs. 

When you refer to the Crescenzi-Kann compendium, I'm not sure if you're referring to the book or the website. The book is out of date but the authors try to keep the website continuously updated. It would seem that the logical starting point is to approach Crescenzi and Kann with your proposal. 

Let me respond to your suggestion with a counter-suggestion: Why don't you try setting up a business, acting as a middleman between amateurs and experts? Amateurs pay to have their proofs evaluated. You find an expert and pay the expert to evaluate the proof, taking a cut of the money for your middleman role. Trying to run such a business is the most reliable way of finding out whether your idea is a feasible one. 

I corresponded with Joel Friedman about 3 years ago on this topic. At the time he said that his approach had not led to any significant new insights into complexity theory, though he still thought it was a promising tack. Basically, Friedman tries to rephrase the problems of circuit complexity in the language of sheaves on a Grothendieck topology. The hope is that this process will allow geometric intuition to be applied to the problem of finding circuit lower bounds. While it's certainly worth checking to see if this path leads anywhere, there are heuristic reasons to be skeptical. Geometric intuition works best in the context of smooth varieties, or things that are sufficiently similar to smooth varieties that the intuition doesn't totally break down. In other words, you need some structure in order for geometric intuition to gain a foothold. But circuit lower bounds by their very nature must confront arbitrary computations, which are difficult to analyze precisely because they seem to be so structureless. Friedman admits right up front that the Grothendieck topologies he considers are highly combinatorial, and far removed from the usual objects of study in algebraic geometry. As a side comment, I'd say that it's important not to get too excited about an idea just because it uses unfamiliar, high-powered machinery. The machinery might be very effective at solving the problems that it was designed for, but for it to be useful for attacking a known hard problem in another domain, there needs to be some compelling argument why the foreign machinery is well adapted to address the fundamental obstacle in the problem of interest. 

You may want to know that there's also a notion of ambiguity for regular expressions. Also, you can decide ambiguity for NFAs in $O(n^2)$. 

There is some preprocessing of the original graph too: Remove arcs between SCCs, then process all SCCs of size $>1$ as indicated above. (In fact, TradeMaximizer iterates over all optimal solutions, according to the two criteria above, in order to heuristically optimize other things, such as the length of the biggest cycle. Big cycles increase the chance of a "deal" not going thru because one person changes their mind.) PS: The author, Chris Okasaki, confirmed that this is what the code does, back at the blog post. 

C.A.R. Hoare, An Axiomatic Basis for Computer Programming. From the abstract: In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. It has six pages that are quite easy to follow. 

While an EE undergrad I attended some lectures that presented a nice characterization of boolean circuits in terms of how many nested loops they have. In complexity, boolean circuits are often thought of as dags, but in real hardware cycles are common. Now, modulo some technicalities regarding what a loop is and what constitutes a nested loop, the claim was basically that in order to implement in hardware an automaton one needs two nested loops, and in order to implement a processor one needs three nested loops. (I might be off-by-one with these counts.) Two things bother me: 

Scientific computing. Computer algebra systems that compute integrals and derivatives directly, either symbolically or numerically, are the most blatant examples here, but in addition, any software that simulates a physical system that is based on continuous differential equations (e.g., computational fluid dynamics) necessarily involves computing derivatives and integrals. Design and analysis of algorithms. The behavior of a combinatorial algorithm on very large instances is often most easily analyzed using calculus. This is especially true for randomized algorithms; modern probability theory is heavily analytic. In the other direction, sometimes one can design an algorithm for a discrete problem by considering a continuous analogue, using calculus to solve the continuous problem, and then discretizing to obtain an algorithm for the original problem. The simplest example of this might be finding an approximate root of a polynomial equation; using calculus, one can formulate Newton's method, and then discretize it. Asymptotic enumeration. Sometimes the only way to get a handle on an enumeration problem is to form a generating function and use analytic methods to estimate its asymptotic behavior. See the book Analytic Combinatorics by Flajolet and Sedgewick. 

This depends on what you mean by "applying calculus to computer science." In your comment to Quaternary's answer, you make a distinction between "direct" and "indirect" application, but it's not clear to me exactly what distinction you're making. Following are some areas of computer science where calculus/analysis is applicable. 

Kuperberg recently proved that knottedness (of a given knot diagram) is in NP ∩ coNP, assuming that the generalized Riemann hypothesis is true. A knot diagram is close enough to a graph that I think this counts as an answer to your question. 

This is not an answer, but too long for a comment. I'm trying to explain why the question, as posed, may be hard to understand. There are two ways to define computational complexity for a device X. The first and most natural way is intrinsic. One needs to say how the device X uses the input, so that we may later look at how the size n of the input affects the run time of the device. One also needs to say what counts as an operation (or step). Then we simply let the device run on the input and count operations. The second is extrinsic. We define computational complexity for another device Y and then we program Y to act as a simulator for X. Since there may be multiple ways for Y to simulate X, we need to add that we are supposed to use the best one. Let me say the same with other words: We say that X takes $O(f(n))$ time on an input of size n if there exists a simulator of X implemented on machine Y that takes $f(n)$ time. For example, an intrinsic definition for NFA says that it takes n steps to process a string of length n; an extrinsic definition that uses a RAM machine as device Y says that the best known upper bound is probably what David Eppstein answered. (Otherwise it would be strange that (1) the best practical implementation pointed in the other answer does not use the better alternative and (2) no one here indicated a better alternative.) Note also that strictly speaking your device X is the regular expression, but since the NFA has the same size it is safe to take it as being the device X you are looking at. Now, when you use the second kind of definition it makes little sense to ask how restricting the features of device X affects the running time. It does however make sense to ask how restricting the features of device Y affects the running time. Obviously, allowing more powerful machines Y might allow us to simulate X faster. So, if we assume one of the most powerful machines that could be implemented (this rules out nondeterministic machines, for example) and come up with a lower bound $\Omega(f(n))$, then we know that no less powerful machine could do better. So, in a sense, the best answer you could hope for is a proof in something like the cell probe model that simulating an NFA needs a certain amount of time. (Note that if you take into account the conversion NFA to DFA you need time to write down the big DFA, so memory isn't the only issue there.) 

Problem is in $\mathsf{P\text{}}$, if subgroup $K$ is a $S_n$ or just identity element. My question is what are the other cases, where problem is easy to solve i.e. is in $\mathsf{P\text{}}$. 

$\le$ is used for subgroup $G = \langle A \rangle $ means group $G$ is generated by set $A$ $P$ means polynomial time in input size. $\Omega = \{1,2,3,\cdots,n\}$ is a input domain Sym($\Omega$) means symmetric group on $\Omega$ 

Isomorphism : Two hyper graphs $G(V,E)$ and $H(V,E')$ are isomorphic if there is a permutation $g$ on $V$ such that, $\forall $ $e \in E$, $$e\in E \iff g(e) \in E'$$ Reference: $URL$ Let me define the degree of a vertex in a hyper-graph: $$D_v = |\{e_i \mid v\in e_i, e_i \in E\} |$$ Question : Is Isomorphism of bounded degree hyper-graphs in P ? 

Informally, hypergraph is a generalization of a graph in which an edge can join any number of vertices. A hyper graph G=(V,E) is a two tuple, where $V$ is the set of vertices and $E$ is a set contain subsets of the vertex set of $V$. An example of hyper-graph is given below and for example edge $e_3$ is a subset contain $v_3,v_5,v_6$ and similarly for other edges. 

Question : Is it in $P$? Give an polynomial time algorithm if answer is yes. I know that If we drop the normal condition from the above problem then problem will not be in $P$. Also note that computing normaliser of subgroup $H$ is in P. 

Anything I've missed? Again my focus is on finding time-efficient methods rather than on doing every conceivable thing to keep abreast. Edit: Thanks for all the responses; I would accept more than one answer if the software allowed it. My somewhat arbitrary choice is based on the fact that I now recall having heard of the ECCC and the CCC before, but I was completely unaware of the Blog Aggregator. 

As others have pointed out, there are certain technical difficulties with the statement of your question. To straighten them out, let's start by avoiding the use of the term "unprovable" without qualification, and be explicit about which set of axioms your statement T is supposed to be unprovable from. For instance, let's suppose that we're interested in statements T that are unprovable from PA, the axioms of first-order Peano arithmetic. The first annoyance is that "T is true" is not expressible in the first-order language of arithmetic, by Tarski's theorem. We could get around this by working in a metatheory that is powerful enough to define the truth of an arithmetical statement, but I think for your purposes this is an unnecessarily complicated route to take. I think you're not so interested in truth per se but in provability. That is, I suspect you'd be satisfied with defining T to be Godel_0 if T is true but unprovable in PA, and defining T to be Godel_1 if T is unprovable in PA but "T is unprovable in PA" is unprovable in PA, and defining T to be Godel_2 if T is unprovable in PA and "T is unprovable in PA" is unprovable in PA but "‘T is unprovable in PA’ is unprovable in PA" is unprovable in PA, etc. That way we don't have to fuss with truth predicates. This suffices to make your question precise, but unfortunately there is then a rather trivial solution. Take T = "PA is consistent." Then T is true because PA is consistent, and T is unprovable in PA by Goedel's 2nd incompleteness theorem. Furthermore, "T is unprovable in PA" is also unprovable in PA for a somewhat silly reason: any statement of the form "X is unprovable in PA" is unprovable in PA because "X is unprovable in PA" trivially implies "PA is consistent" (since inconsistent systems prove everything). So T is Godel_n for all n, but I don't this really gets at your intended question. We could try to "patch" your question to avoid such trivialities, but instead let me try to address what I think is your intended question. Tacitly, I believe you are conflating the logical strength needed to prove a theorem with the psychological difficulty of proving it. That is, you interpret a result of the form "T is unprovable in X" as saying that T is somehow beyond our ability to understand. There are these monstrous conjectures out there, and we puny humans crack PA-whips or ZFC-whips or what have you at those ferocious beasts, trying to tame them. But I don't think that "T is unprovable in X" should be interpreted as meaning "T is impossible to reason about." Rather, it's just measuring a particular technical property about T, namely its logical strength. So if you're trying to come up with the über-monster, I don't think that finding something that is not only unprovable, but whose unprovability is unprovable, etc., is the right direction to go. Finally, regarding your question about whether unprovability seems at all related to separability of complexity classes, there are some connections between computational intractability and unprovability in certain systems of bounded arithmetic. Some of this is mentioned in the paper by Aaronson that you cite; see also Cook and Nguyen's book Logical Foundations of Proof Complexity. 

Build a bipartite graph as follows: For each vertex $x$ in the original graph introduce a left vertex $x_L$, a right vertex $x_R$, and an arc $x_L\to x_R$ whose cost is huge (bigger than the sum of costs in the original graph). For each arc $x\to y$ in the original graph, introduce an arc $x_L\to y_R$ in the bipartite graph. Find a minimum cost perfect matching in the bipartite graph. 

A tool that checks whether a program is correct is sometimes called a program verifier. In this context, "correct" usually means two things: that the program never produces certain outputs (think segmentation fault, NullPointerException, etc.) and that the program agrees with a specification. The code and the specification may agree and still be perceived as wrong. In a sense, asking developers to write specifications is like asking two developers to solve the problem. If the two implementations agree then you have higher confidence that they are OK. In another sense, however, specifications are better than a second implementation. Because the specification needs not be efficient or even executable, it can be much more succinct and hence harder to get wrong. With these caveats in mind, I recommend you look at the Spec# program verifier. 

The article by Gardnera et al. seems to reduce from more standard NP-complete problems. I don't understand well enough either reduction to explain it here, so I'll just leave the pointers from above for you to explore if you wish. This could all be useless, unless somebody figures out how to reduce BINARY DIGITAL TOMOGRAPHY to the question being asked. 

In Impagliazzo's imaginary world Heuristica, P ≠ NP but all NP problems are easy on average for any samplable probability distribution. In Impagliazzo's paper, he implies that if you do manage to find a hard instance in Heuristica, it won't take much more effort to solve the instance than it took to find the instance in the first place. 

I don't know about the terms "efficient" and "feasible." Since these terms even today have no precise technical meaning, I suspect that the history of their usage will turn out to be murky, just as the history of most words in most languages is murky. "Computational complexity" is a more interesting term. With the help of MathSciNet, I find that Juris Hartmanis seems to have been the first to popularize it. The famous 1965 paper by Hartmanis and Stearns uses the term in the title, but even before that, Hartmanis's Mathematical Review of Michael Rabin's paper "Real time computation" (Israel J. Math. 1 (1963), 203–211) says: 

To phrase the question another way, what are some exceptions to the heuristic that if can't figure out contradictory relativizations then it is easy to resolve the equality question outright? 

This is not a complete answer, but I can point you to some relevant papers and also partially explain why it's not so easy to extract an answer to your specific question from the literature. Let me start by asking, why do you want to know the answer to this question? Typically, the people who find themselves caring about this sort of issue are those faced with actually implementing a high-performance FFT for a practical application. Such people care less about asymptotic complexity in some idealized computational model than about maximizing performance under their particular hardware and software constraints. For example, the developers of the Fastest Fourier Transform in the West write in their paper: