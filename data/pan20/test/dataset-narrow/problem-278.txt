We need to add a column to a table that gets hit about 250 times a second, approx 170 selects, 125 inserts and 60 updates. The column will be a simple . The does not matter for the inserts or updates i.e. not part of the primary key, which I'll enforce separately. We basically don't want to have to do an over a range scan 170 times a second, as the number executed will drop massively. Does an index organised table guarantee that will always come before when running the following query: 

I have created two identical procedures in two schemas. I have then granted execute on each procedure to a third schema. I call each procedure using the qualified object name. When calling the procedure in one schema there is no issue, when calling the second I get PLS-00302: component must be declared. Why? Here's the code I'm running, actual usernames/passwords have been obscured. Firstly the procedures are created in and and execute is granted to 

The following are examples used to convey the point, there are multiple instances of each of these examples within your ERD. Naming Consistency is key. Some table names are pluralised () some are not (). Your table contains data about many tickets so I normally prefer pluralised table names. You're prefixing every column with the name of the table. This makes for column names like , which isn't instantly understandable to readers. If I named the primary key of then you store and you might access , which makes everything more understandable. Relations There's a couple of relations that don't exist, for example you've got in (makes sense if you have preassigned seating) but no relation to ? Why not? If your cinema/theatre doesn't have preassigned seating then you don't need this data. There are some duplicates within tables. Why does contain both and ? Data duplication There are some unnecessary (unenforced) relations that are allowing for data inconsistencies. If a must happen within a an a ticket must be at a then there's no need for to contain any information about the hall in which the movie is going to be shown. It's unclear what the difference between the and is. Maybe one of them is the number of minutes the screening runs for? This information is already within and so is another duplication. Data There's no need to persist the age as you've already got the date of birth. I've written about this on Stack Overflow a few times. It's quite an assumption that all staff and movie names will be 45 characters or less. I find the inclusion of in the staff table a little off-putting. What's the purpose of storing this information? Are the chromosomes of the person selling you a movie ticket that important in this cinema/theatre. I also don't understand why it's 10 characters long. How to start Think about exactly what information is required to go within each table. If you're not sure it's required then don't add it in. If you need to add something later then so be it, but you haven't included a lot of unnecessary data. I'd set-up some test tables with dummy data to see if what you're creating will actually work. Every relation to another table should be enforced. If you've got some unenforced relations then either add a foreign key constraint or remove the column. Your cinema probably has tickets, so removing the data will prevent you selling tickets. 

If this gets automatic login working, then you'll have to decide how you want to solve the issue. You can specify a list of sites for the trusted intranet zone via Group Policy, but this prevents users from being able to add or remove anything on that particular list. If there's a cleaner way to do it, I'd be interested in knowing too, as we run into this a lot with intranet sites on various DNS domains. 

I have a number of triggers that do this, and I find that is generally the best way to do it. Caution: the output is limited to 4000 characters. Very long queries will be truncated. 

Since all your matches have to match the LIKE pattern in order to be included, the simple thing to do is assume that shorter values for the column you're matching are "better" matches, as they're closer to the exact value of the pattern. 

You can't have nulls in a primary key column. You can, however, create a unique index/constraint on a column, and that column will then allow only a single null for the whole table (in addition to requiring unique values). 

If you're concerned about write queries blocking read queries, I would recommend either using isolation, or setting the database option, which effectively uses snapshot isolation for all queries. The net result is that instead of being blocked, read queries will see data as it existed prior to any currently running write transactions. Be advised that this option increases tempdb usage, as that's where the snapshot data is stored (and it has to be stored for all write transactions in databases where it's enabled). If this overhead is unacceptable, then I would only use after evaluating queries that need it on a case-by-case basis. I know you aren't the one making the decision to use it, but you can use these points as ammo if you need to push back on it. :) 

an additional (covering) index with a different column order may work for you. As with anything index-related, more (and wider) indexes means slower write/update performance. This is particularly true if you're effectively doubling the size of the table. If you just index without including the other columns, you'll get a smaller index but your query may end up either just using the clustered index with expensive Scan or Sort operators, or using Key Lookups from the non-clustered index, which may be great if you're querying just a few rows at a time but disastrous if you're scanning the entire table. This is general advice. For specifics, edit your question to include query, table definitions and a query plan. 

I'm pretty sure that sp_BlitzCache will show you the cost of the actual plan used for your queries, whereas the server will considers a parallel plan when the initially estimated cost exceeds the threshold value. A noticable difference between estimated and actual query plan costs could happen if you have stale/bad statistics on your tables. If you've identified a specific query that you want to run serially, you can add the following at the end of the statement - this will ensure a serial plan: 

This query might return zero rows, a single row or millions of rows, depending on the value of . The first time you run this query (without ), SQL Server generates a query plan based on the value of the variable for that execution. Suppose that value of returns a single row, the query can afford an execution plan that takes 0.1 seconds for each row it returns, completing your query in 0.1 seconds total. Now, if you apply that same plan to another value of where the query returns 1000 rows, that query is going to run for minutes and you would probably be better off with another plan, perhaps with different joins, aggregates, etc. When you use , SQL Server will evaluate the query (and the variable values) every time it runs and recreate the execution plan every time. This will add a few milliseconds to each execution (to generate the plan) but the resulting plan will probably be better suited for your particular value of . If you're fine with an execution time of two seconds, this won't be a problem. However, if your query needs to complete in milliseconds, I would start tuning indexes, updating statistics, etc. In summary, yes, this sounds a lot like a parameter sniffing problem, and appears to be a good solution in your scenario. I can't make any more qualified guesses without looking at query plans, tables and indexes, statistics and the different parameter values. Hope that helps. 

I won't go into detail about why it's not a great idea, since you're already aware of all that. For anybody else stumbling across this answer: Try not to do this if possible! For the occasions when I need to schedule an after-hours restart (patching, configuration changes, etc), I normally just run this in a batch file scheduled via Windows Task Scheduler: 

SQL Server doesn't have any direct analog of MySQL's query cache, i.e. an in-memory cache of actual query results. Data and compiled query plans will be cached in memory, however. If you've got a particularly expensive query with a lot of joins, you may want to look into creating an indexed view. 

I've got an Analysis Services database that's used for reporting on email activity. It mainly counts incoming and outgoing messages, who they're to and from, etc. Fact table: 11,367,910 rows Address dimension table: 386,015 rows To-address fact table: 21,303,290 rows (used for many-to-many intermediate measure group) Date dimension table: 9,132 rows It's a simple structure, but there's a lot of data in it. The two measure groups have 6 yearly partitions with varying numbers of rows in them. The whole thing takes about 30 minutes to process fully. But that's not my problem (at least I don't think it is). Seemingly randomly, the scheduled processing task will fail with the error The line number can vary, but the error is the same. I've tried hunting around, but I can't find any info on what this indicates. Anybody else run into this error? 

My hunch is that it has something to do with your loop opening and closing the connection 75,000 times. Maybe try something like this and see what happens: 

If you don't want to use this function though, why not just remove it from your code? The only time I can think of there being a valid use-case for this sort of structure is when you're running identical code in two different schemas/on two different databases but this function is useless in one of them. If that's the case the function is still useful in the other and so you will want to do something with the result anyway. 

The pk constraint, for the IOT, would become with a separate constraint on the pk solely for structure. is null in approximately 99% of the table so this isn't very selective anyway. Main index used is, I think, , we tested about 20 combinations and this came up top by a long way. I am completely unwilling to add any time at all to these queries, 0.01s a day added by select is 41 minutes a day. We'd much rather settle for "good enough" than perfection. 

As far as I can tell this doesn't appear to be a permissions issue (though it obviously is). The system privileges of the 2 users are identical, the following queries return no results: 

Earlier today we had a problem logging into a database that had just been created automatically. When on the actual box (Linux) , i.e. a local connection, was fine, though, it was occasionally impossible to do anything once connected. Otherwise, whether using a session on the box or not did not work at all. and DNS testing revealed nothing and there was no problem in . The Listener was accepting the connection but not handing it off to the database. This was confirmed by using on the listener log and watching it accept the connection. There is a logon trigger on the database that inserts the user details / time etc into a table, but this doesn't have any indexes and there cannot have been any locks. We finally traced the issue. There was a conflict in the file that creates the temporary tablespaces for the database. It resulted in there only being 5MB of tempspace instead of the normal 25GB. Disabling the logon trigger and then adding the additional temporary tablespaces, sorted everything out. There were some sessions already running that would have been using some of this tablespace. Apparently Oracle requires some temporary tablespace in order to accept an incoming connection. The specific version used on this DB was 9i, though I think this applies to all. Why and what is it used for? 

The restore GUI tool in Management Studio supports both of these options. To specify the name of the database you want to restore as, go to the "General" page, and enter the correct name in the "Database" field (under the "Destination" subhead). To specify where you want the database files to be restored to, go to the "Files" page, and specify the new paths in the "Restore As" column. If you're overwriting an existing database, you'll need to go to the "Options" tab and check the "Overwrite the existing database" option. If this is a procedure you plan to repeat regularly, you can generate a script for the RESTORE command by clicking the "Script" button on the Restore Database window toolbar. 

It sounds like a situation where you could potentially fall prey to - or benefit from - parameter sniffing. In a nutshell, SQL Server will use parameter/variable values when compiling an execution plan in order to determine optimal index usage (based on column statistics). Depending on the value that's passed in, you could get very different execution plans, and very poor performance if the "wrong" value is used later. (A nice article about it.). I'd personally calculate the date within the procedure using a scalar function, unless there's a need for whatever is calling this procedure to specify the date itself. Then you could always use the scalar function in the procedure to fall back on a default value. As long as parameter sniffing isn't giving you surprising performance changes, it's mostly just a matter of application design. 

Is MySQL Workbench doing something horribly undocumented with schema/object privileges? Just for kicks, I granted some table privileges to one of the specific user@host combinations, then updated mysql.tables_priv to change the host to '%'. After running FLUSH PRIVILEGES, it worked perfectly. Weird. 

Creating a SQL Server Agent job (owned by a sysadmin member) will do the trick, although I realize that this is not a very pretty solution. The user can start the job (ansynchronously) using msdb.dbo.sp_start_job. Running an Agent job synchronously, however, requires a few more lines of code if this is a requirement. Also, obviously, you need to have the SQL Server Agent service up and running. 

If your table's clustered index has as the first column, this may be the most optimal query. You could simplify it by breaking it into two parts, like so: 

In my mind, the query plan for the above should be the same as if you had an . The query plan should now have a Segment, Sequence Project and finally a Filter operator, the rest should look just like your good plan. 

In all probability, your query takes half a second to run, but it takes a lot more time to display all the results. The latter depends on the speed of your network and that of your computer and is not really relevant from a performance tuning perspective. To answer your question, the query execution time (0.5 seconds) is the execution time. 

You can rewrite the to a subquery, but it may still compile as an outer join query - I have no knowledge of those aspects of the H2 platform. 

If you query this view, the server will still need to check that there is a matching record in for each row in and vice-versa. The more tables you add, the more joins. You would work around this by using , but I doubt that this will have the desired effect once you add a lot of tables. In my opinion, designing your queries to use just the needed tables (as they do currently) will be your best bet. If you need to improve query performance, I would look at creating specifically targeted indexes on frequently used tables/columns. Basically, it comes down to performance vs simplicity of use, but the performance penalty for your view solution may be severe. Building a stored procedure to actually update the view is a fairly simple matter. 

The Tail call is where it finds the latest date beneath the currently selected Post Date member (typically the current year). That works fine. But if that returns Feb. 29, meaning the last sale for a particular combination of dimension members occurred on Feb. 29, then it passes Feb. 29 into the ParallelPeriod function, which subsequently returns null. And then the previous-year YTD measure also returns null. So, in a nutshell: Based on this particular schema, is there a simple way to have ParallelPeriod behave nicely for Feb. 29 inputs? If it just returns Feb. 28 of the previous year, that's fine. EDIT: A few things I've tried: 

Make sure the application connects to the server using a login that has been given only read permissions (give it the db_datareader role in the database to allow reading all tables), and you should be in good shape. The easiest way to prevent changing data is to ensure the user doesn't have permission to change anything. Be careful about granting execute permissions, as you may inadvertently allow users to change data depending on what the procedures do. 

Source: $URL$ I don't recommend Single User mode in these cases, as the application update process may need to open multiple connections simultaneously. 

This isn't a complete answer, but I do not recommend having live reports passing user credentials to SQL Server. Suppose a malicious report developer builds some kind of report that does Nasty Things that would require sysadmin privileges (vandalism, granting privileges, whatever). Just deploy the report, and trick someone with sysadmin to run it (CSRF might be sufficient to run the "report", though I've never tried it), and presto, your code is running with sysadmin rights. The approach we use is to select Windows authentication on the data sources within Visual Studio. Developers can thus build reports against whatever they have access to. All of the data sources are deployed to a single location ("/Data Sources", naturally), and these copies of the data sources are configured with static credentials. Since Visual Studio doesn't overwrite data sources by default when deploying report projects, everything works nicely, assuming we don't forget to grant permissions to the account accessing the data, like I always do. Now, back to the problem of excessive permissions. By default, any object (folder, report, etc.) that you create in Reporting Services will be set to inherit permissions from its parent. It's similar to Windows file permissions, though a bit simpler overall - an object's permissions have to be either all inherited or all explicitly set. Out of the box, Reporting Services has a few standard roles you can grant to objects. If you want to get more granular and customize the roles, use Management Studio: from Object Explorer, connect to Reporting Services, and specify the report server web service URL as the server name. Drill down to Security, Roles, and you can customize or add roles as needed. 

Joining tables is a fundamental principle of relational databases. In your case, A and B are related with the column, which means that you can use a syntax similar to this one: 

Your join has to describe a condition where rows in are matched to rows in . In your example, you're just filtering rows using variables; the join is effectively a (a cartesian product), which is why you get on every row. The following query should solve your problem, with the assumption that rows in cannot span multiple weeks in . 

Your solution, although a loop, is essentially still a cursor in the sense that you loop over a number of records and execute one or more statements once for each record. It's not the loop in itself, but rather the coding pattern where you loop over rows in a table and execute a statement for each row. Typically, "traditional" programming languages are row-based - this is the main difference between SQL and, for instance, C#, VBA, etc. Why your query performs better with table variables may depend on a number of potential factors that we can't deduct from your question. What you want to do when you're eliminating cursor-based solutions is to turn them into set-based solutions. Here's a very primitive pseudo-example: 

Database backups give you point-in-time restore capability (provided you have recovery model). Even if your IT people take backups every few minutes, which is extremely unlikely, you'll still have a gap. Server backups do not replace database backups, they complement them by "archiving" the database backup files long-term (i.e. more than just today). In the end you and your management have to decide your RPO (the recovery point objective - how much you need to be able to recover in a crash). With only daily server backups and no database backups, you stand to lose a full day's work in the worst case. Edit: @Sting has a valid point in that shadow copies (the mechanism most likely used to make server backups) are not very likely to take an exactly simultaneous copy of all your database files (including log files), which may lead to inconsistencies when you restore the backup. For instance, if the shadow copy reads the transaction log a few milliseconds before it reads the database file, the database file could contain an uncommitted transaction, but as the transaction was committed a millisecond later, the log will not have any record of it.