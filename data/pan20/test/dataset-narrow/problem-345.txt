We have a purchased reporting application that executes a query that just recently is taking an extremely long time (>2 hours). A week ago, this same query completed in less than a minute. I've restored the DB from last week to confirm the end user's story, and sure enough, even with similar row counts with last weeks and yesterday's backup, the difference in query run time is there. Table1 contains 1.4 million rows. Table2 contains 16k rows. In this case, none of the records from Table2 exist in Table1, so when the query is actually allowed to complete, no records are returned. The table schemas are identical, with the exception that Table1 has a non-unique, non-clustered index on Field10. There are no other indexes that exist on either table. All fields in the join are (NVARCHAR(40), null) types. The query looks like this: 

My company is getting some new VM (VMWare) servers provisioned for purposes of installing SQL 2014 or 2016 (version to be determined) with AlwaysOn Availability Groups. The server and storage provisioning is being done by managed services within the data center. As a DBA with the company, I recommended our standard drive layout (OS, App, SqlData, SqlLog, TempDB, and SQLBackup), so six drives per server. The Virtualization Lead with the data center is recommending just two drives per server. One for OS/Apps, and the other for SQL. His exact response was "With an aag [AlwaysOn Availability Groups] and modern storage, we can typically flatten it out to an OS and a SQL disk. The era of separating disk groups to alter performance is behind us with object based arrays on the back end". To me, this configuration seems counter-intuitive to everything I've been taught and experienced. My question is this: Are modern storage arrays (I have no insight into what the actual infrastructure is, other than it's supposed to be "state of the art") getting that good that we don't need to manage for splitting up IO for data, logs, and TempDB? I won't be managing the servers, nor even doing the SQL configuration on them, so I don't have much skin in the game. I'm more interested in hearing if this is others experience on server drive configurations going forward as we move to better virtualized environments. I should note that the data going on these servers will not likely have a high amount of IO, so I don't think performance will be a real factor. The two main priorities are that the system has very high up-time and is very secure. 

If you are using the 11g duplicate from active database option, the only port that needs to be open is the listener port (usually 1521). If you are using any other options to copy your files to the standby host, the required ports for the protocol that you use (scp/ftp/nfs ...) Apart from the 11g 'from active database' option, RMAN does only read backupfiles that are present on the node on which the restore/clone is to be taking place. Present in this setting means that they thould be readable. The can be on [n]fs or tape, as long as your host can read them it is ok. I hope this helps 

The fiddling with controlfile is only needed when you want to rename the database, what not is what you want. In you case ASM does not change anything for you. 

With dbvisit you can maintain a standby database, in a very similar way as Data Guard does in the Enterprise Edition. You might want to check this out. The setup is very easy, support is good and the pricing is very reasonable, if not, cheap. I blogged about it a few times, see how to configure dbvisit to manage existing standby database running SE and Cost reduction using Oracle SE and dbvisit standby I hope this helps, Ronald. 

Yes, there is. transfer the files to a new server, if you have one, using the same file structures. next create a init.ora file with the dbname and control_file parameters. Start the instance using that init.ora in nomount. If you happen to have snapshot controlfiles in $ORACLE_HOME/dbs/ of you database, start rman: 

If you happen to be using the same platform (cpu/OS) it is fairly simple to get this up and running, if you know from which version of Oracle this backup is AND the backup is taken in the correct way. In your case, since all you have is ctl and dbf files, it should have been a COLD backup. If all of the above is not in place: forget it. If all of the above is in place: 

Using a job status from a scheduler is not reliable at all, often the backup scheduling has problems so it is important to also notice that backups are not started at all. For this you can find a zabbix Oracle module at github that does the backup monitoring based on v$rman_status. Zabbix does a real good job for this kind of monitoring. 

I've searched what I could search on Google, SO, etc and haven't found an answer that would seem to fit what I'm looking for. I'm hesitant to make a post about this as I'm sure the answer is out there somewhere, I just haven't been able to find it :-\ I have setup a table in MySQL that I really could use some insight on. I have an existing index and it works pretty well in some cases, and others it takes 20+ seconds to run. Let me give you some background. It's a MyISAM table with fixed rows (every column is an INT(11) ) and this table currently has 150,000,000 rows on it (10.2 GB total). I use this to track analytics run on the web software we use as other open source alternatives (like Piwik) were simply overkill and we needed direct access to the "stuff". That aside here's the basic structure of the table. (again all INT(11) as I read and understood that indexes with the same type and length are most effective). 

I have a multi-column index on location, region, action, ts and this query can take 20 to 30 seconds to execute. I also have one other index that is simple visitor_id, ts, and that one doesn't show any issues I assume because visitor_id has such a high cardinality. EXPLAIN SELECT shows I'm hitting the index and it seems to do as well as it could do. 

A few days ago that index used to be region, location, ts (no action) and I did some experimenting on my own. I filled up a table with dummy data up to 55,000,000 rows just to see what kind of indexes would give me an improvement, although the index I just barely tried worked great at 55,000,000 rows, it doesn't really improve much at 150,000,000 rows. Another thing I've tried is that this query is actually pulling the days for a report, as in "Get all the visitors between day x and y, or the last 90 days". I tried making another column some time ago that just stored the unix time stamp equivalent to just represent the day (so the cardinality for the index would be substantially smaller). Oddly enough, the only thing that really seemed to do was increase the table size by adding a column, not really helping out with anything else (but this was also months ago probably around the 50,000,000 row mark, maybe the difference wasn't a big deal back then) I know that you generally are supposed to have the index go from high-cardinality to a lower cardinality. I also know that MySQL blows up on ranged queries, and the rest of the index is useless if I put the ts at the beginning of the index. The question that I turn to you for help for is what can I do so that I can improve the time it takes for this query to run? Am I really to the point where I need to break this table apart every 50M rows or so? Is there no index that can save that table or am I really bad at understanding how to setup an index? I'm open to other alternatives as well that may seem unorthodox, but at least will get the query time down. Update Since the posting of this question, the table now has nearly 159M rows. It looks like currently it's growing at about 10 million a month which will of course exponentially grow more. At this point with this crazy growth, am I better off splitting up the table into months or something equivalent? Updated Update 10/5/17 Table is now 800 million and still going strong with really fast queries. Table is about 80GB with the data bing 30GB and the indexes being 50GB 

If going for oracle, take a look at dbfs and Secure Files. Secure Files says it all, keep ALL your data safe in the database. It is organized in lobs. Secure Files is a modernized version of lobs, that should be activated. dbfs is a filesystem in the database. You can mount it similar like a network filesystem, on a Linux host. It is real powerful. See blog It also has a lot of options to tune to your specific needs. Being a dba, given a filesystem (based in the database, mounted on Linux), I created an Oracle Database on it without any problems. (a database, stored in a ... database). Not that this would be very useful but it does show the power. More advantages are: availability, backup, recovery, all read consistent with the other relational data. Sometimes size is given as a reason not to store documents in the database. That data probably has to be backed up any way so that's not a good reason not to store in the database. Especially in a situation where old documents are to be considered read only, it is easy to make big parts of the database read only. In that case, those parts of the database no longer have a need for a high frequent backup. A reference in a table to something outside the database is unsafe. It can be manipulated, is hard to check and can easily get lost. How about transactions? The database offers solutions for all these issues. With Oracle DBFS you can give your docs to non database applications and they wouldn't even know they are poking in a database. A last, big surprise, the performance of a dbfs filesystem is often better than a regular filesystem. This is especially true if the files are larger than a few blocks. 

What needs to be done depends on where the connection initiative is done. If the initiative is done from the Oracle side, you could use dg4odbc and configure it with the odbc driver and connection details for the Postgress database. dg4odbc sets up a special kind of listener process that you refer to using a regular tns alias. This listener process has to be on the same machine is the ODBC driver but does not necessarily have to be on the database server. Ofcourse, the datbase server has a listener process but nothing -well maybe licensing- does prevent you from setting up a dedicated listener for dg4odbc on a separate macine. In your Oracle database you setup a database link that uses your tns alias that points to the special listener.