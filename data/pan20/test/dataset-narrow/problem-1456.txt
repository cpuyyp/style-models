Generally I feel one element should be core when it comes to this and that is readability. From my own experience when I was just starting off with programming I'd always explicitly define my scopes. That is to say every if opens with a condition, followed by an encapsulated (series of) statement(s). The point for me back then was to get rid of ambiguity when you aren't experienced in recognizing scope and the flow of code. So I feel that at first you should try and teach your students a very clear and obvious way of coding. Ternary operations (i.e. the ? operator) tend to be hard to grasp when inexperienced, so try and stick to always start from the more explicit code, and then as you come across your N'th if statement in your example you show them alternatives. That way, you'll let them know you can make the code more interesting, without pushing them to do so and get confused in all the options there are. Then when they get more comfortable and experienced I feel you should slowly make your code more and more implicit and add some more sugar to it. That way you can be sure that once you explain it, everyone will get it fairly quickly, and it gives them the freedom appropriate for their skill level. The point here being that you want to let your students know you can do these things but advice them to get the basics down first. Then after repetition, and you yourself steering more to actual proper code, I reckon many students will pick and choose what they themselves are most comfortable with, which you can use to actually understand and support them better as well. So, think of it as an inverse funnel, you start them off straight and reliable, and as they learn more and more, you give them more options and freedom. 

Disclaimer: I'm not a teacher myself so take this with a grain of salt if you must. Nothing motivates a person more than to feel that they, and their work matters. If your students feel unmotivated I feel you should try and take a look at your own behavior that is unrelated to what you are actually teaching. See, if anyone wants to learn how to do anything on the computer, technically anyone can go to the internet and try and learn it themselves. So I personally feel that the teacher within this all is responsible far more so to motivate the students in learning what they probably already know they should. This means that drawing diagrams and showing them how its done is an important part of this all, as it helps them understand and learn more easily, but showing genuine interest and concern for the needs and wishes of the students is far more important than simply explaining things in a structural and orderly fashion. So I'd say to try and ask your students what THEY want to focus on in class. If they struggle with X, but you teach them Y, a lot of people will just get frustrated with the things you didn't teach them (to their feeling) sufficiently. I personally always felt very motivated to, and know that almost all my former classmates did too, when the teacher would tell us what we'd be doing first, giving some examples, then converse with the class about what they understand and don't about what you just explained to them. Given time constraints it'd be best to focus on what the majority wants and needs, and then give more personal attention and support to the students that have more or specific needs. Also remember that positive reinforcement (especially nowadays) is far more powerful than negative. In other words, compliments, even if small, will have a far greater impact than to tell them that they'll fail if they don't keep up. If you can show a positive and supportive side of you, then your students will want to work for their own sake and for the sake of doing their good teacher proud. 

I describe programming as a two-step process, of algorithm + coding. I.e. deciding how to solve a problem, and then implementing that approach in a particular language on a particular system. Can you have one without the other? Yes. There are occasions, particularly in the realm of unplugged approaches to computational thinking, when we'll think about algorithm-like solutions without any intention of automating them (e.g. the jam / PBJ / chocolate sandwich thing, or an effective way to play a board or card game). There are other occasions when we'll write in a code that a computer understands without their being any actual algorithm to implement - markdown, HTML, XML, json etc, etc. I think HTML is a great way to get students thinking of a formally specified language, and seeing some immediate effect from their coding, without the additional cognitive load of implementing an algorithmic solution. 

Any programming is a two step process: deciding how to solve the problem, then implementing that as code on a particular system: choosing or designing an algorithm is the first step. There are great ways to illustrate how the choice of algorithm matters. An introductory one might be search - comparing random, linear and binary algorithms to, for example, find a missing number, or a word in a (printed) dictionary, or a book in a (physical) library. Another might be exploring different sorting algorithms, for example bubble sort and quicksort using this CS Unplugged activity. Mathematics provides a rich source of contexts, for example asking students to think of an algorithm for finding the greatest common divisor (i.e. highest common factor) for a couple of numbers. Have them try their algorithms out on paper before coding them and then testing with some big test numbers. 

Lots of things you can do via role-play, e.g. simulations of how a processor works, how the internet work, how e-mail works etc, but also playing through some scenarios or dilemmas in online safety. Plenty of scope for debating broader moral and ethical issues around CS, including AI: What should Audi's programmers and managers have done? Should end-to-end encryption be available? What rules should a self-driving car be programmed to follow? 20 questions, or something simpler such as guess my number? Lots of fun with hand-drawn graphs, e.g. minimal spanning trees, shortest paths or the travelling salesman problem. 

Some interesting projects for those working with block-based languages (such as Scratch, Snap! and Blockly): For Scratch, check out Dr Scratch, which takes a rubric approach to evaluating how much 'computational thinking' is evidenced by a project. Whilst the analysis might seem a bit reductive, it can be used independently by learners and includes some useful guidance on how to progress. The developers describe their approach in this paper. Dr Scratch is built on Hairball, a Python module which does static analysis of Scratch projects. A more conventional autograder, lambda, is being developed by Michael Ball for Snap! It's already integrated into UCB's Beauty and Joy of Computing MOOC, and I think there are plans to make this more widely available. Michael wrote about this for Hello World #3. Chris Roffey has developed an autograder for Blockly used in the initial round of the TCS Oxford Computing Challenge programming challenge, although I don't think the code for this is shared publicly. 

A variant on the usual random drill and practice test would be to pre-populate with the questions and answers, then remove question and answer from each as they get answered correctly, allowing players to get more practice on the questions they get wrong. Here's an example for times tables. You could try something for an adventure game, building up an inventory of items collected in a list. Another possibility would be an adaptive 20 questions style game, adding additional questions into a database (of sorts) as the player gets to the end of a branch of the tree. 

Have you looked up some recent work in the area and reached out to the authors? For instance: $URL$ As many academic articles are, that's behind a paywall, but it's a paywall that many in the community have a key to, so I figured it'd be worth mentioning here. It might also be worth checking the authors' home pages, etc. as they are allowed to post it there as per ACM publishing agreements. The abstract, however, is allowed to be shared, along with the title and authors, so here's that info: Andreas M. Stefik, Christopher Hundhausen, and Derrick Smith. 2011. On the design of an educational infrastructure for the blind and visually impaired in computer science. In Proceedings of the 42nd ACM technical symposium on Computer science education (SIGCSE '11). ACM, New York, NY, USA, 571-576. DOI=$URL$ Abstract: The blind and visually impaired community is significantly underrepresented in computer science. Students who wish to enter the discipline must overcome significant technological and educational barriers to succeed. In an attempt to help this population, we are engaged in a three-year research project to build an educational infrastructure for blind and visually impaired middle and high school students. Our primary research goal is to begin forging a multi-sensory educational infrastructure for the blind across the United States. We present here two preliminary results from this research: 1) a new auditory programming environment called Sodbeans, a programming language called Hop, and a multi-sensory (sound and touch) curriculum, and 2) an empirical study of our first summer workshop with the blind students. Results show that students reported a significant increase in programming self-efficacy after participating in our camp. 

Writing code is essential, and a part of that is debugging (your own) code. If you can't debug your own code, you won't get very far, unless you write perfect code the first time, every time. Debugging someone else's code introduces students to things that debugging their own can't - different styles, and different approaches to problem solving, that perhaps they would not come up with on their own. So debugging someone else's code helps you write better code for yourself. You learn what is good, what works, and what is bad, and doesn't work. Just as for learning another natural language (written), you can get fairly far writing and correcting your own words, but at some point, you'll benefit from reading (and possibly correcting) someone else's words. So to answer your question, yes, I see no problem in having students debug code (that isn't theirs). In fact, I would almost go as far as to say you should. But I wouldn't say that debugging code is more important than writing code. I would say it is part of writing code. 

I noticed that most of the answers focus on the things (topics, lectures, etc.) I have found that it is helpful to focus on the students. I normally ask them for a one-on-one chat and ask them why are they here? What do they like / not like about CS? This normally opens an opportunity for making their experience more engaging for them. It also identifies those that really want to succeed, and those that don't. In my experience the students that end up dropping out are the ones that actually want to (or don't really care). It's worth knowing who really does want to succeed, and who doesn't. 

Simply - yes, but not always. Paper-based programming exams make students think differently than they do in front of their editor. What's wrong with that? Nothing (as long as it isn't always that way). What's the benefit? Thinking differently. 

We're introducing some aspects of parallel processing quite early on in Scratch. Each sprite has its own script which appears to execute in parallel with those of the others. Scratch has a broadcast and receive message protocol, and support for shared as well as private variables and lists. Children might encounter this in a maze game, perhaps programming a number of similar ghosts to chase the player's avatar. It's also useful for agent-based modelling, e.g. the spread of an epidemic through a population. Of course, it's not true multi-threading, as all of Scratch runs inside Flash, inside the browser, on just the one core, but I doubt those using Scratch will be aware of, or care about, the distinction. This does, though, lead to potential difficulties in 'graduating' from Scratch to a text-based language such as Python - young Scratchers who've been used to programming in parallel in Scratch can find it hard to adjust to doing just one thing at a time in introductory Python programming. 

I'm sure there are plenty of others. Not sure I'd use them in every lesson, but they might be a useful incentive towards more purposeful use of the devices they bring. I know one school where in free time pupils are only allowed to play games they've coded themselves... 

There seems little value in students copying code off a display, but much in watching the teacher model how they think about the task, talking through the problem solving process of coding, as well as debugging (in the case of, ahem, deliberate, mistakes), iterative development and refactoring. I think this works if it's editing a longer program or writing short examples. I'd say good practice would include sharing the code produced, via Github or elsewhere, as well as screencasting the talking through of the development process itself. 

Harvard's grading policy for CS50 is worth looking into. There are four components for the grade on problem sets (each of which involves submitting code). The overall grade is calculated as scope * (3 * correctness + 2 * design + 1 * style) Scope: to what extent does the code implement the features required by the specification? Correctness: to what extent is the code consistent with the specifications and free of bugs? This is done by the check50 autograder, and it's essentially unit testing. Style: to what extent is your code readable (i.e., commented and indented with variables aptly named)? there's a component for formatting: I think in Harvard's case these marks are awarded by teaching assistants, but basic static analysis or linting might suffice. Design: essentially, is this good code in terms of clarity, efficiency, logic, elegance - again, Harvard use TAs to award these grades, and it's hard to see a machine (or an inexperienced grader) being able to award these marks accurately any time soon. If you were determined to use automatic grading, I guess you could do something with run times for test data or the more sophisticated forms of static analysis. A compromise might be the use of peer-assessment and a detailed, criteria based rubric: peer assessment might have other benefits. 

When we were drafting the English national curriculum, we found it easier to think in terms of the foundations, applications and implications of computing, all three of which really should be included in any broad and balanced approach to the subject. You can map these to computer science, IT and digital literacy if you wish, although you would need to accept a rather broader definition of digital literacy than that used by the Royal Society Foundations would be about the underpinning principles of computer science (logic, algorithms, data representation, abstraction), as well as their practical expression through programming and more generally in computational thinking. Applications is about skills in using digital technology to get useful work done, including collecting, managing, analysing and communicating data and information and creative work in a range of digital media. Implications is about a critical understanding of the impact of digital technology on individuals and society as well safe, responsible and ethical use. I'd include intellectual property, privacy and security here too. I've an illustration of all three in response to the question 'How does Google work?' Foundations: big data, Page Rank, Big Table / the Google File System (GFS) etc Applications: type your query, click the button (well, these days it starts searching as you type), but also filtering results, advanced queries etc Implications: profile, filter bubbles, advertising, smart creatives, separating costs and revenues for accounting purposes etc