Less of a race condition and more of an issue with order of operations and dependencies. The way you stated your question, in steps, is how you want to think of the SSIS package. First it has to do this, then it has to do that. Since the line items are dependent on the latest information from the header, that should always come second. Letting them go simultaneously, you don't really know what happens first. 

Here's a good article on the subject; Given the overhead involved in re-opening a database after it's been closed, I would say, no. Do not enable it. The more databases you have, the more this problem is exacerbated. That being said, if there are some other circumstances that makes you think it should be enabled, please include those details. As sp_BlitzErik asked, what are you trying to achieve? 

This is especially useful where you need to use that variable in multiple places in your script. This example uses setting a variable to a literal value. A literal can also be used in the WHERE clause directly. 

I have the print statement in there to help you get the @sqltext exactly the way you want it. Once you have it as you want, use the EXEC sp_executesql line to actually run the output of the @sqltext. 

The CTE is the way to go on this one. You should post your attempted code so we can critique. Otherwise all I can do is get you started. Here's an example of what you are doing, almost exactly since this is one of the most common purposes for a cte.Pinal Dave Recursive CTE First step in the hierarchical cte is to create a query to select the top/terminal level of your data AKA the anchor. The example had, IMO, better source data because all you had to do to determine the top level was look for a record with a NULL manager. To me that makes more sense. With your data it will be something like; 

A related note, not necessarily an answer, but it could be a bit clearer for your code, .Net has a class for SQLConnectionStringBuilder which might come in handy here. Instead of just concatenating your string together, you might want to look into passing the values to that object and use the output connection string property. .Net SQL Connection String Builder 

100% agree with Marcin Gminski. The SA account is definitely not supposed to be used this way. Windows domain credentials certainly are preferable although not necessary. If you are going to use a single login account for all users, at least create a new login that is not a part of the System Admin role and make sure it has a strong password. As for your performance issue, we would need to know more. First off, what is your method used for data access in the MVC app. Are you using Entity Framework, Linq or something else? Next, have you debugged the app to see if it is making excessive, unnecessary calls to the database. Next, have you attempted to run a SQL Profiler trace to capture the activity on the server? Those steps should at least provide a little more insight in to where the problem exists. 

One problem I see with your sub-query is that you are trying to join using the personID but you aren't providing that value in the sub-query. Also, I would assume the purpose of the subquery is to reduce the number of results from the table to only the latest entry for each PersonID and eligibility. That being the case, you should really add that to the join as well. 

Also, you may want to wrap the concatenated values with ISNULL() if any of those values might actually return NULL. A NULL value concatenated with anything else will always return NULL. A trigger will actually be written very similarly except would need to be updated each time a value in the table is updated. Unless there's some performance or data structure related need, I would avoid the trigger approach. Either way for a "correct" answer we would need to know your objectives. How do you intend to use the generated value. BTW, this answer assumes you are using MS SQL. 

I'm seeing a few misses. First, the role table. If one user can belong to more than one role, you're going to need another table between role and user. 1 table to store the roll(like you have already) and another to store a junction table which will store the relation between a role and a user. That table would probably only have two columns, role_id and user_id. The user table would not need to identify the role unless it was a primary role or default role. That could also be accomplished in the junction table. Similar situation with Company - Member Type. Since you only have a single column in the company table, you will only be able to store one member type per company. If the requirement is to have companies be able to be assigned to more than one member type, you will need a junction table to store company|member_type relationships. One last thing I noticed, is there is currently no relation between Employee and Company other than the address. That will definitely get confusing. Since you want contacts to be associated with multiple companies, you will need another junction table to store the contact_id | company_id relationship. There might be a clever solution using your employee table. Since that essentially is the only direct connection between employee and company, maybe just change that to the junction table, company_employee. Then add a boolean/bit column (IsEmployee) to indicate whether the contact is actually an employee of the company. 

I've worked at places that never expire service account passwords and other places that do it as a requirement. I personally don't feel it's necessary in most environments because the service account passwords are generally not known to anyone except admins and can be made to be very strong. Also, if you are managing the service accounts security, they should each have limited access to do exactly what it needs and no more. If that is the case, the risk associated with such accounts is very low. At one point, I actually designed an application to handle the security aspects of sharing passwords within the IT organization. With hundreds of environments and potentially thousands of service accounts, managing it all with a spreadsheet was getting out of hand, not to mention the lax security associated with Excel. I'm hoping to add some new features like PW expiration warnings to the app at some point but its not really a priority now. My current employer never expires service account passwords. 

Assuming you are looking for the Top 50 distinct combinations of C1-C4, Steve is close. However, having the Top 50 inside of, or before, the DISTINCT may cause fewer than 50 returned results if there are any duplicates found. Instead, you would want to return a full list of all distinct value combinations, then limit it down to only the top 50. When doing things like this, I generally avoid using the DISTINCT command. Instead, use GROUP BY. It will do essentially the same thing and will add more control if you want to expand results later. As for your query, I'm not seeing a closing parenthesis where there should be one so I don't know where the WHERE and ORDER BY clause are actually supposed to be. I would suggest putting them in your inner query to limit the results returned for the TOP 50. 

@Matthew, Given that you do not want to revert to parent security on your sub-folders and children reports, I think your only option will be to write some custom code which accesses the SSRS Web Services API. I've done something similar using powershell for one project and a C# script task from SSIS for a different project and should work well although it won't be accessible from the SSRS management portal. I'm 99% certain that there is no functionality within SSRS that will directly handle your request out of the box. 

This cannot be done without a subselect as you state. There are two steps involved. 1. For each name, find the record with the MIN(qty). 2. Return the ID for that record. There are other approaches but the two steps remain. It appears the query you posted has some missing info. This should be the whole thing. 

This is largely opinion based as I'm sure a lot of design questions are. As you stated, it can be made to work in both scenarios. From a growth perspective, I definitely agree with your choice to supply the values in a vertical table with identifiers for each value type. It's a more normalized approach and won't require much dev work if/when more fields are added later. That's really the main benefit. As business needs change(they always do), how much work is it going to be to change or add. A normalized approach is almost always better for changing business requirements. The other option is simple to be sure. Writing reports against data like that would be very simple. That's really the only advantage that comes to mind. Unfortunately it's not a good one. This is most definitely an OLTP application we are talking about. So we should be focused on user input, not reporting. As long as the data makes sense, reporting shouldn't be that hard anyways. One other point. We have an application onsite that was designed by a business user with no actual developer input. It was designed as a single table with no primary key and about 100 columns. Everything is stored on that one table. Normalization WHO CARES!!! Now that the business is tired of the shortcomings of the old system and wants something better, there is almost nothing we can do. The old data can not be directly translated to the new system because the requirements are sooooo different. All of that data has become almost entirely useless. Anyways, the point of my story is, don't take these decisions lightly. They tend to come back and bite you. To add a bit of ammo to your point, try asking the business the following questions, Will we ever want to record who filled in values or when they were updated? Will we ever want to record multiple users answers for each field? If the answer of either of these is yes, even if it's a remote possibility, the normalized approach is better. 

Really, a literal value is just a value that is set in code and not directly updated through programming or by an input parameter. Also, while I used a varchar as an example, this can be any data type. 

Scott Hodgin is correct. You should award him the answer. In addition, if you do not want to grant the SQL Agent Login rights on your network, you can alternately run using a Proxy account with stored credentials. Then you can apply that proxy to the Job step and have it run with those credentials instead of the SQL Agent login account. Of course you will still have to grant permissions to the folder to that account. 

From Microsoft docs. Guess I just had to find the right doc... $URL$ Grouping Tables for CDC Processing Database projects range in size from several tables to many thousands of tables. When designing initial load and CDC packages, it is beneficial to group tables in much smaller groups for easier management and efficiency. This section lists various considerations that impact the sorting of tables into small groups, where the tables in each are initially loaded and then updated as a group. The CDC patterns supported by the CDC components assume that this grouping is already determined. Each group defines a separate CDC context that is maintained separately from other groups. For each group, initial-load and trickle-feed update packages are created. Trickle-feed updates are scheduled for periodic runs based on the rate of change processing constraints (for example, CPU and IO consumption, impact on other systems) and the desired latency. Tables are grouped based on the following considerations: According to the target database. All tables that are written to different target databases or undergo different processing should be assigned to different CDC groups. Tables that are related with referential integrity constraints should be assigned to the same group to avoid referential integrity problems at the target. Tables for which higher latency can be tolerated can be grouped so they can be processed less frequently and reduce overall system load. Tables for which there is a higher rate of change should be in smaller groups, and tables with a low rate of change can be grouped in larger groups. The following two packages are created for each CDC group: An Initial Load package, which reads the entire range of data from the source tables and applies it to the target tables. A trickle-feed update package that reads changes made to the source tables and applies the changes to the target tables. This package should be executed on a regularly scheduled basis. CDC State Each CDC group has a state associated with it, which is represented by a string with a specific format. For more information, see CDC Control Task. The following table shows the possible CDC state values. StateDescription0-(INITIAL)The state that exists before any packages are run on the current CDC group. This is also the state when the CDC state is empty. For more information about CDC Control task operations, see CDC Control Task.1-ILSTART (Initial-Load-Started)This is the state that exists when the initial load package starts. This occurs after the MarkInitialLoadStartoperation call to the CDC Control task. For more information about CDC Control task operations, see CDC Control Task.2- ILEND (Initial-Load-Ended)This is the state that exists when the initial load package ends successfully. This occurs after the MarkInitialLoadEnd operation call to the CDC Control task. For more information about CDC Control task operations, see CDC Control Task.3-ILUPDATE (Initial Load Update)This is the state that exists after the first run of the Update package after the initial load while still processing the initial processing range. This occurs after the GetProcessingRangeoperation call to the CDC control task. If using the _$reprocessing column, it is set to 1 to indicate that the package may be reprocessing rows already at the target. For more information about CDC Control task operations, see CDC Control Task.4-TFEND (Trickle-Feed-Update-Ended)This is the state expected for regular CDC runs. It indicates that the previous run completed successfully and that a new run with a new processing range can be started.5-TFSTART (Trickle-Feed-Update-Started)This is the state that exists on subsequent runs of the Update package after the GetProcessingRangeoperation call to the CDC control task. This indicates that a regular CDC run is started, but is not finished or has not yet finished, cleanly (MarkProcessedRange). For more information about CDC Control task operations, see CDC Control Task.6-TFREDO (Reprocessing-Trickle-Feed-Updates)This is the state on a GetProcessingRangethat occurs after TFSTART. This indicates that the previous run did not complete successfully. If using the __$reprocessing column, it is set to 1 to indicate that the package may be reprocessing rows already at the target.7-ERRORThe CDC group is in an ERROR state. Here is the state diagram for the CDC components. An ERROR state is reached when a state is reached that is not expected. The expected states are illustrated in the following diagram. However the diagram does not show the ERROR state. For example, at the end of an initial load package, when trying to set the state to ILEND, if the state is TFSTART then the CDC group is in an error state and the Trickle-Feed Update package does not run (the Initial Load package does run). Once the Initial Load package runs successfully, the Trickle-Feed Update package runs repeatedly under a predetermined schedule to process changes to the source tables. Each run of the Trickle-Feed Update package is a CDC run.