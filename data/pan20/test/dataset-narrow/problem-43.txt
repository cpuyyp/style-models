With compression we are after the best visual representation per bit and, as the eye is non-linear in approximately this way, it makes sense to use non-linear representations. 

You might find the 1983 paper that introduced this**, i.e. Lance Williams' "Pyramidal Parametrics" informative. You can ignore the scheme, in Figure 1, he used for the layout of the MIP maps as I doubt any hardware, at least in the last 20+ years, used that approach. ** (Actually, Williams *may* have described the technique at an earlier SIGGRAPH (1981) as part of tutorial/course but I've never been able to get hold of a copy) 

The area around the beak also illustrates the horizontal and vertical partitioning of the 4x4 blocks. Global+Local There are some texture compression systems that are a cross between global and local schemes, such as that of the distributed palettes of Ivanov and Kuzmin or the method of PVRTC. PVRTC: 4 & 2 bpp RGBA PVRTC assumes that an (in practice, bilinearly) upscaled image is a good approximation to the full-resolution target and that the difference between the approximation and the target, i.e. the delta image, is locally monochromatic, i.e. has a dominant principal axis. Further, it assumes the local principal axis can be interpolated across the image. (to do: Add images showing breakdown) The example texture, compressed with PVRTC1 4bpp produces: 

Apologies in advance for the poor quality of this answer, but this sounds a little like what an ex-colleague was doing as part of his PhD. The "Free-Viewpoint video" papers listed at the bottom of $URL$ might be useful, or at least a starting point to find related work. 

It could be that you have to overcome a different bottleneck first. Have you ever read Jim Blinn's "The Truth About Texture Mapping"? (I had a quick search to see if I could find a non-paywalled version but you may have better luck than me. Alternatively you might find a dead tree version of "Jim Blinn's Corner" in a library). Though this article is old and describes paging of texture data, it is still very relevant today. Essentially, if your textures are large (i.e. too large to fit in the cache), in scan order, and they have been rotated when displayed on the polygons, you are very likely to be thrashing your cache and, as memory is an order or two of magnitude slower than the CPU, this will hurt performance. To avoid the cache thrashing, textures are often stored in twiddled-order (that's what we called it in the early 90s but, more correctly, it'll be some variant of Morton order) or in a block order, which is what Blinn describes. This then makes texel/memory accesses far more coherent and the cache more effective. 

As you can see in the Y-Z-X order, the Y axis remains there as in 3rd picture causing the axes to coincide... 

so that these are centered around the Z-axis. In order to generate an oblique projection you can use the above mentioned method which takes input the coordinates for left, right, top, bottom, near and far planes. The other way around is to build your own custom projection matrix, and load it directly. The matrix should be stored as an array 

That's what I want to hear about more. Why "non-orthogonal" ? I can't see any problems with making the axes orthogonal? What does making the axes non-orthogonal give us? 2) After the above one has been answered. How is this all tied up to rotational matrices? How can we achieve gimbal lock through the matrices when they only rotate a given point/vector around global axis. For example If i multiply a column vector with a Rotation matrix around X-axis and then with It will rotate around the global X-axis first, then global Y-axis second. So how can I achieve the lock situation using matrices? EDIT:- To make it more clear I've also heard about rotation orders like in the order when Y axis rotates then and rotate with it but when rotates only rotates with it. thus making the axes non-orthogonal. I am assuming that's what it means by non-orthogonal mentioned in the wiki article. Here is a picture. 

So i searched a lot after this and I think it was my confusion on FrameBuffer Objects. I thought you could use FBO's just like a default FrameBuffer and display the texture image attached to it but you can't. It's only used for offscreen rendering. So while you can use rendering commands to draw something to a "texture image" attached to it, you can't "display the image" by making it default framebuffer or something like that. 

"How (hardware) texture compression works" is a large topic. Hopefully I can provide some insights without duplicating the content of Nathan's answer. Requirements Texture compression typically differs from 'standard' image compression techniques e.g. JPEG/PNG in four main ways, as outlined in Beers et al's Rendering from Compressed Textures: 

run principal component analysis on your image data to generate "the" main axis Project your pixels onto that axis (i.e. compute the dot product of each colour value against the axis) Using the dot prods, sort the the colour values from smallest to largest and Find the "partitioning" point, i.e. that splits smaller values in one set, larger in the other, that minimises, say, the mean squared error. i.e. each partition is represented by the average of its members and you compute the total error. 

I'm fairly certain your problem lies in not handling so-called degenerate Bezier patches correctly, in particular (as Joojaa noted), the computation of the surface normal. I say "so-called degenerate" because, geometrically, the surfaces are often perfectly well behaved. It's just that some assumptions people frequently make regarding the parametric equations may not hold. Books such as Gerald Farin's, snappily titled, "Curves and Surfaces for CAGD. A Practical Guide" will give more details, but I'll try to summarise two simple cases. Now assuming your Bezier is defined as $\bar{B}(u,v)$ the usual two causes of problems are: Zero derivatives: To compute the normal at $(a,b)$ one normally (pardon the pun) computes the two derivatives, $\frac{\partial }{\partial v}\bar{B}(a,b)$ and $\frac{\partial }{\partial u}\bar{B}(a,b)$ or scaled versions thereof, to obtain two tangents and then take the cross product. (Implementation note: Since we can use a scaled version of the tangent in the calculation, we really don't have to calculate the actual derivative. For example, especially at the corners, differences of control points can yield a scaled tangent. However for brevity in this discussion we will assume the actual derivatives). A common occurrence, at least at the corners, is that the 1st partial derivatives at the location can be zero, which leads to an incorrect normal, i.e. a zero vector. In the case of the tops and bottoms of the teapot, one whole edge of a number of the (bicubic) Bezier patches has been collapsed to a point, i.e. all 4 control points are the same, and thus, say $\bar{B}(0,v)==\bar{B}(1,v)$ and $\frac{\partial }{\partial u}\bar{B}(0,0)=\bar{0}$. The surface, however, is completely well behaved so you can simply choose another derivative that starts at that collapsed point. In this case, say, choosing $\frac{\partial }{\partial v}\bar{B}(1,0)$ for the second tangent. Having said this, you still have to check that your first derivatives are not zero for another reason (e.g 2 or 3 coincident control points), in which case, you can fall back (thanks to L'Hopital's rule) on the second (or if that's zero, even the third!) derivative(s) to obtain valid tangents. Parallel tangents: Another similar problem can arise if your two tangents, are parallel. - Farin has a good example in his book. In this case, I think you may need to look at using something like $\frac{\partial^2 }{\partial u \partial v}\bar{B}$ or, possibly, just fall back to using a small UV offset to approximate a vector. 

To be honest, terms like these are very confusing as they aren't clear cut and on one side of the border. They are more grayish. I'm gonna tell you how I convinced myself, as I too had this confusion as soon as I read your question. But I managed to convince myself through this argument. First of all we are gonna clear up 4 terms, Radiance, Irradiance, Differential radiance and Differential Irradiance. "Radiance" is what you say associated with a certain direction. To be more formal and according to wikipedia, 

I can't really answer this without the context so post some links where this is done. What I can tell you is that the general formula for viewport transformation as done by OpenGL is given as (taken from wiki) 

I think you are confusing all the binding targets thingy. From what I see your vertex data is coming from compute shader after some processing and now you want to pass it to the vertex shader. You can create a buffer object once, use it as an SSBO for use in a compute shader then use it as a VBO for use in rendering. i.e 

So thanks to joojaa I finally got a hint, and I searched on the net further and found this link which cleared all the doubts and has my answer. Though I am still posting it here as a summary. So anyone reading this and who has similar problem to mine here is what I understood. Suppose we are considering the tait-bryan angle order X-Y-Z that is rotate first along X then Y and finally Z. Also to make it clear we are rotating around the "fixed" axes since a rotation matrix always rotates around the fixed axes. The matrix doesn't know anything about the axes moving or anything. 

Sample gaps will occur if you don't do things "properly". In OpenGL or D3D, assuming a consistent winding order, if you have triangles ABC and CBD, then when a sample point - that is a test during scan conversion to determine if a point (read pixel) is inside a triangle - lies exactly on the shared edge BC, then that sample will belong to exactly one of those two triangles. This avoids both gaps and double filling. The latter would be problematic with, say, translucency or stencil operations. 

To align vectors, you are applying a rotation. If you consider a simple rotation matrix, e.g. rotation of angle $\theta$ around Z axis, then you will trivially see that its inverse matrix, a rotation of $-\theta$, is the transpose, since $sin(-\theta)=-sin(\theta)$ and $cos(-\theta)=cos(\theta)$. An arbitrary rotation matrix, R, can be constructed by multiplication of other rotations, e.g $R = A \cdot B$. If the inverse of A and B exist, then $R^{-1} = B^{-1} \cdot A^{-1}$. Similarly we know $(A \cdot B)^T = (B^T \cdot A^T)$. Put these together and you have your answer. 

Case 2 If the case 1 test fails, you could then check for the "trivial" existence of an intersection. Now there are probably better ways to do this, but the following, relatively cheap, approach occurred to me: Consider just curve A: 

(since the SBox function is quite non-linear) Having said that, (and please forgive me if I've got some of the details wrong) in a past life I implemented Perlin noise using a relatively simple RNG/Hash function but found the correlation in X/Y/Z due to my simple mapping of 2 or 3 dimensions to a scalar value was problematic. I found that a very simple fix was just to use a CRC, eg. something like 

I Recently posted this question on SO but didn't got any response so i thought to post it here since it's somewhat related to Raytracing. I am making a real time ray tracer in OpenGL using Compute Shaders for my project and was following this link as a reference. The link tells to first draw a full screen quad, then store all the individual pixel colors gotten through intersections in a texture and render the texture to the quad. However i was thinking can't we use Frame Buffer Objects to display the texture image instead of rendering the quad and save the over head? Like I save all the colors using ImageStore and GlBindImageTexture in a texture, then attach it to a FBO to display it. And since I won't be using any rendering commands I won't be causing a Feedback loop as in writing and reading the same texture? Here is the snippet 

Now we have to rotate around Z as this is the last in the order. But notice that the rotation around is similar to the previous rotation we performed around . The plane will roll either towards the left or right. This means we lost 1 degree of freedom. I think this is what joojaa was trying to explain to me. So everywhere where people are saying the axis coincide its the local, moving frame coinciding with the original frame. Initially both the local and global frames are coinciding. In this case when we rotated around by 90 the local moving X-axis collapsed on to the fixed original - axis causing this gimbal lock 

Next is differential radiance. We can think of it as an infinitesimal quantity of radiance emitted or recieved in a very small solid angle $d\omega$. Next is Irradiance. Irradiance isn't normally associated with a direction. According to Wikipedia it's 

Originally, it wouldn't have been perspective correct, but on (hardware) systems these days it will be. FWIW Dreamcast had perspective correct Gouraud shading because, once you are doing perspective correct texturing, it is relatively little additional cost to do Gouraud "correctly". 

Decoding Speed: You don't want texture compression to be slower (at least not noticeably so) than using uncompressed textures. It should also be relatively simple to decompress since that can help achieve fast decompression without excessive hardware and power costs. Random Access: You can't easily predict which texels will be required during a given render. If some subset, M, of the accessed texels come from, say, the middle of the image, it's essential that you don't have to decode all of the 'previous' lines of the texture in order to determine M; with JPEG and PNG this is necessary as pixel decoding depends on the previously decoded data. Note, having said this, just because you have "random" access, doesn't mean you should try to sample completely arbitrarily Compression Rate and Visual Quality: Beers et al argue (convincingly) that losing some quality in the compressed result in order to improve compression rate is a worthwhile trade-off. In 3D rendering, the data is probably going to be manipulated (e.g. filtered & shaded etc) and so some loss of quality may well be masked. Asymmetric encoding/decoding: Though perhaps slightly more contentious, they argue that it is acceptable to have the encoding process much slower than the decoding. Given that the decoding needs to be at HW fill rates, this is generally acceptable. (I will admit that compression of PVRTC, ETC2 and some others at maximum quality could be faster) 

That completely depends on how you are computing the shading. If you are then just linearly interpolating the shaded colours across the triangle, i.e. Gouraud shading then, clearly, the answer is "no". However, if you are doing per-pixel shading by, say, interpolating normals and light directions, then you can easily get a brighter area away from the vertices. 

First of all we need to understand why do we need 4x4 matrices in the first place. With 3x3, we couldn't represent translation as it wasn't a linear transformation (it displaces the origin). So in order to avoid extra work, homogeneous coordinates and affine transformation was introduced. Now instead of doing $v' = Lv + t$ where is a linear transform and is the translation, we can do $v' = Av$ Where is the affine matrix. This makes it cleaner. So 4x4 matrices are a real necessity, we just can't work without them. In order to distinguish between vectors and points we use for points and for vectors. So you are suggesting to make this 4th dimension implicit and don't store it as it'll actually use space/memory. 

In the book Computer Graphics Principles and Practice, they use the term specular reflection when they want to imagine things resembling a mirror and glossy reflection when things like a polished door knob or an orange skin. The charts shows you exactly that. When a material has more specular color, it should have less diffuse color due to the conservation of energy. That is, the sum of the light reflected specularly and light absorbed and emitted in random directions must be less than equal to 100% (the amount of light incident on the surface). Hence when you increase the specular color the material tends to go white or have a slight tint of the color like in metals. Where as glossy surfaces can have more diffuse color but show a specular highlight like the surface of an orange skin. So assuming the CGPP's point of view, we can say in pure specular reflection, the diffuse part is much less than the glossy part. Where as in glossy reflection the diffuse part is usually greater.