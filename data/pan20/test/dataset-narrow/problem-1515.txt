The filter weights absolutely must be updated in backpropagation, since this is how they learn to recognize features of the input. If you read the section titled "Visualizing Neural Networks" here you will see how layers of a CNN learn more and more complex features of the input image as you got deeper in the network. These are all learned by adjusting the filter weights through backpropagation. 

I agree with your analysis that this problem presents different challenges than single-image super-resolution. In the single image case, we know exactly where everything should be in the output image, because it is just a higher resolution version of the input. So why not try and extend this idea to your case? Here is how you might do that: 

What you have are predicted class probabilities. Since you are doing binary classification, each output is the probability of the first class for that test example. To convert these to class labels you can take a threshold: 

Back-propagation technically refers to computing the gradient of the loss function with respect to the parameters. According to Section 6.5 of the Deep Learning book: 

Just use an output layer with 12 neurons instead of 1. Qualitatively there is no difference. For regression the output activations should be linear, and you have a few choices for cost: RMSE, MAE, or Huber Loss to name a few. 

Assuming your "practice" data is drawn from the same distribution as your "real" data, you are on the right track by thinking about measuring the relationship between training accuracy and test accuracy in the first set to model the relationship in the second set. However you should split your practice data into training and test only and use k-fold cross-validation on the training set. Then you should train a model on the real data with the same cross validation scheme. For example you might get results like: 

Normalize the document (convert to lowercase, remove punctuation, diacritics, non-alphanums, etc) Remove stopwords Convert the document to tf-idf vector over our genre keyword vocabulary: Each document gets an n-length vector where each entry is the frequency of the ith genre keyword in the document. Normalize this vector to magnitude 1. Convert each genre keyword list to a tf-idf vector in the same way (again over the keyword vocabulary for all genres). Compute the cosine similarity between the document vector and each genre vector. 

If I understand this correctly you want to convert this feature to a single number like an integer from to to reduce memory usage. This will not affect the results you get from a tree model, but most other models will now interpret the labels as having a natural ordering and an importance relative to the size of the number representing them. There are a few possible tricks for encoding high-cardinality categorical features as a single number (or a lower-dimensional vector). 

Let's say I am trying to predict whether a cat will be adopted, and I have found the ratio aka to have predictive power. However, for some cats I have much more data than others. For example: 

If you are trying to predict future values then it doesn't make sense to treat them as categorical features. There is nothing you will learn that can predict future data, since you won't see those times again. The same holds even if you are trying to predict unseen past data but each time appears only once or a small number of times. Instead the time values should determine the order of your training data. This way you can avoid leaking future data, and models with state can learn features from the sequential nature of the data. However, in some cases you may want to extract additional features from the time values. Here are some examples: 

What is the expected distribution of my target population? What is the cost of misclassifying each type of example? 

Update: After looking at your data I found the problem: Your csv file has spaces after the commas, so the rows containing have a leading space. If you use to read your data then either of the above methods will work. 

Encoding categorical variables as integers is generally bad for linear regression, because the model will interpret that to mean that category 2 is twice as significant as category 1, and so on, which is not necessarily true. It isn't surprising that you got bad results. A better approach is to encode your categories with dummy variables. Let's say your categorical variables are C1, C2, and C3, each taking values from 1 to 4. Then we can have twelve 0/1 dummy variables corresponding to each possible category for each categorical variable. For any input exactly four dummy variables will be 1 and the rest will be zero. Your linear regression now looks like: $$ \hat{y}=a_1*d_{C11}+a_2*d_{C12}+a_3*d_{C13}+a_4*d_{C14} + a_5*d_{C21}+a_6*d_{C22}+a_7*d_{C23}+a_8*d_{C24} + a_9*d_{C31}+a_{10}*d_{C32}+a_{11}*d_{C33}+a_{12}*d_{C34} + a_{13}*x_1+a_{14}*x_2+a_{15}*x_3+a_{16}*x_4+a_{17}*x_5 $$ where $x_1$ through $x_5$ are your numerical inputs. If a given input has C1=1, C2=4, and C3=3, for example, then this would reduce to: $\hat{y}=a_1*1+a_8*1+a_{11}*1$ It's also possible to do the same thing with 64 dummy variables for each possible combination of the categorical variables as you were doing, but in a single linear regression as above. See the Wikipedia entry on dummy variables for more info. If you are still not getting good results with linear regression, then consider using Gradient Boosting Regression Trees. 

LSTMs do not require a sliding window of inputs. They can remember what they have seen in the past, and if you feed in training examples one at a time they will choose the right size window of inputs to remember on their own. LSTM's are already prone to overfitting, and if you feed in lots of redundant data with a sliding window then yes, they are likely to overfit. On the other hand, a sliding window is necessary for time series forecasting with Feedforward Neural Networks, because FNNs require a fixed size input and do not have memory, so this is the most natural way to feed them time series data. Whether or not the FNN will overfit depends on its architecture and your data, but all standard regularization techniques will apply if it does. For example you can try choosing a smaller network, L2 regularization, Dropout, etc. 

Here I used the rule to generate the data. A maximum-margin classifier would get all of the test examples right, but a linear regression model fit on the training data would get all of the test examples wrong. 

Unless there are many training examples per user, using as a feature will most likely result in overfitting. This should be fairly easy to verify using a tree model that can handle categorical features of high cardinality, as you suggested. One approach I have seen in the past is to only label the top most frequent users and label everyone else , so you only have possible labels for the category. Another is to cluster users into groups based on some useful similarity measure. You will probably have to apply some domain knowledge to come up with a good similarity metric. If you expect the top users to be present in a significant percentage of future examples then this might be useful, otherwise it would be better to cluster users based on other features. In the latter case you compute the user's cluster membership at prediction time and use that cluster label as an input feature. You should compare all models to one that doesn't use at all to gauge its usefulness. 

Some packages provide separate methods for getting probabilities and labels, so there is no need to do this manually, but it looks like you are using Keras which only gives you probabilities. As a sidenote, this is not called "normalization" for neural networks. Normalization typically describes scaling your input data to fit in a nice range like [-1,1]. 

The weights are updated right after back-propagation in each iteration of stochastic gradient descent. From Section 8.3.1: 

During image preprocessing in Keras, you may run out of memory when doing , which involves taking the dot product of an image with itself. This depends on the size of individual images in your dataset, not on the total size of your dataset. The memory required for will exceed 16GB for all but very small images, see here for an explanation. To solve this you can set in ImageDataGenerator. 

I would like my neural network to consider that there is more data for Cat B and possible consider that it is more likely to be adopted. How should I tell it about both the ratio and the sample size? Here are some ideas I have so far: 

If you choose then it becomes a simple average, which can be computed by multiplying each indicator by and summing, with no need for additional normalization. 

Design a cost function $e(s)$ that measures how good a seating arrangement is. Lower cost means better seating. Design an acceptance probability function $P(e, e', T)$ that takes the costs of seating arrangements $s$ and $s'$, and a temperature $T$, and returns a probability $P$ with the properties (a) $P>0$ even if $s'$ is worse than $s$ (b) the better $s'$ is relative to $s$, the higher $P$ is and (b) the lower the temperature, $T$, the close $P$ is to $0$ when $s'$ is worse than $s$. Set some large initial $T$ and design an annealing schedule for decreasing $T$ with the number of iterations. Start with some seating arrangement $S$. This can be random or chosen according to a greedy strategy that tries to satisfy as many constraints as possible. Repeat for $i$ in $k$ steps (for some sufficiently large $k$): 

In the AlphaZero paper AlphaZero Go wins 60 games in a 100-game match against the 20-block single resnet AlphaGo Zero, but it is not explicitly stated which architecture AlphaZero is using. It is either the 40-block single resnet since this was the best, or it is the 20-block single resnet in order to make a fair comparison with AlphaGo Zero. 

If we are trying to predict when a certain user might visit next, then these features might help us a lot. For example we might decide that Alice is more likely to visit in the afternoon, or Bob is unlikely to visit two days in a row. 

Use ratio as input feature and don't worry about sample size Use ratio and sample size as separate input features Use $p$ and $n$ as separate input features, and let the NN do what it wants with them. Use an average of the cat statistic and the average population statistic over all $M$ cats weighted by number of samples: $$ x_i = \beta * (\frac{p_i}{n_i}) + (1-\beta) * (\frac{1}{M}\sum_{j=1}^{M}\frac{p_j}{n_j}) $$ where $\beta \in [0,1]$ is an increasing function of sample size $n_i$. Use a Binomial proportion confidence interval 

So if you have 3-channel input images in each training example, your input shape should be This should give the network enough information to learn the correct transformation. 

This question has been answered in detail on CrossValidated: How to choose the number of hidden layers and nodes in a feedforward neural network? However, let me add my own two cents: There is no magic rule for choosing the best neural network architecture, but if you can find an architecture someone has used to solve a similar problem this is often an excellent starting point. The best places to look are official or unofficial examples using popular neural network libraries such as Keras, PyTorch, or Tensorflow, and architectures described in academic literature. keras/examples on github is a great resource. These architectures were likely chosen after lots of trial and error, so most of the work will have been done for you. 

Classification is the more direct approach and it will likely give better results. This is because the model's goal is exactly the same as your goal - i.e. predicting whether the price is above or below the threshold - and it will maximize this accuracy. A regression model that is trying to minimize MSE, for example, could give you a model that doesn't accurately put predictions on the correct side of the threshold - which it doesn't know or care about. The model does not have the same goal as you do. Let's construct a very simple example to see why this might be the case: 

Here are a few ways you might use neural networks to solve this problem: With a plain Feedforward Neural Network: 

Here you can see that the parameters are updated by multiplying the gradient by the learning rate and subtracting. The SGD algorithm described here applies to CNNs as well as other architectures. 

What you describe has been explored in Deep Residual Neural Networks. A residual block will combine two or more blocks from a standard architecture like a CNN with a skip connection that adds the input to the first block to the output of the last block. 

Reinforcement learning is more about interacting with an environment, and while this could be posed as an RL problem, I think using Global Optimization would be a more direct approach. Essentially you want to design a cost function that describes how good a particular seating is and then use it to search the space of possible seatings. For example to solve the problem with Simulated Annealing: 

Time since last interesting event Number of interesting events in last time window of size n Time of day (morning / afternoon / etc) Day of week Holiday Season 

And then make an informed decision about how to proceed, which may involve either sampling negative examples differently, or adjusting your performance metric to something like AUC (which can guide early-stopping, for example). Additional material: A systematic study of the class imbalance problem in convolutional neural networks 

If you are working with only enough data for training and validation, consider using K-Fold Cross Validation: 

So every input example will be a vector of length 30, where the last 3*(10-$k$) values are zero when there are $k$ points present in the set, and the output is a vector of length 10 summing to 1, whether the largest value corresponds to the predicted point (whose position is corresponds to that position in the input). With a Convolutional Neural Network: 

This is essentially what t-SNE does. It supports transforming data from any input dimension to any output dimension, and it tries to preserve distances between all pairs of points. From the author's website: 

If we were doing single label classification we could normalize each row to add up to 1 and we might have a working model. However there is no such trick for multilabel classification here. We don't have a good way to calibrate these values into probability estimates. At this point the only solution I see is to build a small training set so we can fit our model to actual data. After gathering some training examples, we can run a multilabel regression with sigmoid activation and binary crossentropy loss with the cosine similarities as input features to get a probability estimate for each class. Using this method our list of genre keywords will at least save us having to build a large training set to solve the problem directly with bag-of-words or similar approaches. 

For scikit-learn can set , for example, and as long as nothing in your script is modifying the seed nondeterministically then you should get reproducible results. This is described in the scikit-learn FAQ under How do I set a for an entire execution? However, I don't believe it is possible to do the same thing for pandas. See here for discussion. 

The main reason to use statistics computed on only the training set is to avoid leaking information from the test set. If this is not a concern, then it is perfectly OK to use statistics from the entire data set. See here for further discussion. 

The intuition is that deep networks have a harder and harder time learning the identity function between layers, which has been proven to be useful especially in image recognition tasks. Residual connections also mitigate the problem of vanishing gradients. Residual connections help solve the "degradation" problem, where deeper architectures lead to reduced accuracy. For example GoogLeNet won ILSVRC in 2014 with a 22-layer CNN, but in 2015 Microsoft ResNet won with a 152-layer Res Net. 

If you are not worried about leaking information from the test set then choose (1), otherwise choose (2). If you standardize the test set with different values than the training set you might end up with train and test data from different distributions, which will lead to failure to make accurate predictions. 

And for multilabel classification where you can have multiple output classes per example you can use thresholding again: 

There are various ways to handle string inputs to neural networks, but since you are trying to predict spelling difficulty, I suggests representing your words as a sequence of characters. This will preserve information about the particular spelling of the words including the order of the letters. To represent a sequence of characters you can one-hot encode each character, so each word will be represented as a sequence of one-hot length 26 vectors. To handle this kind of input I suggest either a 1D Convolutional Neural Network or some flavor of a Recurrent Neural Network. If you choose a 1D CNN you will have to feed it fixed sized inputs. To do this choose a max word length, , and either cut off or pad with zeros each input word to fit into a input matrix. Here is an example of a CNN architecture you might use in Keras: