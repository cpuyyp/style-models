In general, I would suggest to use a regularization technique for reducing the dimensionality ofa data set in linear regression cases. Please refer to L1 regularization. If you want to decrease the number variables using PCA, you should look at the lambda values that describe the variations in the principle components, then, select the a few components with the largest corresponding lambda values (eg the first four). Note: 

Any classifier that naturally provides feature importance can be applied here. For example, take a look at Decision Trees and Random Forrest. For other algorithms that do not provide such information (eg KNN and Neural Nets), you may take a (forward/backward/bidirectional) feature selection. For example, chose a subset of features, train a model and evaluate it with any preferred metrics (better to use bootstrapping or cross-validation). Choose another subset if features and try again. After several trials, you can find the subset with the best prediction power. This is the set of the mist contributing features. Note that different techniques/measures may give you different results. 

Do a scaling if necessary. Sometimes the very first component is not very relevant and can be eliminated. 

If I'm not mistaken, you are looking for some metrics to investigate if two columns of values represent similar information. Right? When the order of numbers is fixed for both columns (such that the ith elements of both columns relate to the same record) and hence both columns have the same length, then you can use a wide range of techniques to measure the similarity/distance between the two vectors such as cosine similarity, Pearson correlation coefficient, mutual information, etc. If the order of numbers in the columns are different or they basically refer to different concepts, then the above techniques are meaningless. In this case, you can look at these columns as two populations of samples. In this case, you may compare their statistical distributions using their distribution parameters (eg mean and std for a normal distribution) or statistical tests (eg ANOVA). In some cases, the number of items (population size) can be different. 

Assuming you already have the raw text, you can do the followings. Create train data: You need to create set words and bigrams labeled as skills. There might be some available lists to help you out. Otherwise, generate your list using resources such as wordnet and thesaurus. You can also start with a short list based on the data you have and then expand it using word2vec or similar word embedding techniques. For example, we start with a list that contains coding as one of the skills. Then, we query the word2vec pretrained model for closest words/bigrams. There is a fair chance you will end up with programming, software coding, and computer programming. Another approach would be to cluster the words and bigrams in your dataset using word embedding techniques. Then, look into your clusters to see which ones contain the skill set. Note that any word/expression not on your list will not be considered as required skills. Therefore, you may need to expand your list after a few trials. Detecting the Skills 

As shown in these docs: $URL$ at the section "Classification". You can export your tree using graphviz (it states that you have to install the graphviz package, too). And this way you're able to visualize the tree built by the algorithm. About the problem of the input features being transformed from the original ones it's a problem the algorithm can't help you with but you should be able to manage that by yourself if you've made the transformations yourself. Any further doubt, comment. 

You can do the following: use all the other features as input and the missing data as the label. Train using all the rows that have the column filled with data and classify the others that don't. Use the values predicted by the Random Forest as the value of that field on the subsequent models and transformations. 

You can do the following: A Pipeline can be made of other pipelines. Isn't that great? A Pipeline ($URL$ inherit from Estimator class and by definition a PipelineStage ($URL$ can be either an Estimator or a Transformer. This way, you can build smaller pipelines, save them seperated and on the other software/class, join then again as a single one and call transform on the DataFrame. 

I'll throw at you some standard names that the are you're building a project on has, every term that you don't understand, research it, it'll improve your knowledge a lot. First, there are great resources out there on NLP (Natural Language Processing)regarding classification problems such as this one: $URL$ Which algorithm to use? It depends on your data, there are two topics that you have to pay attention: the over or underfitting of your model and the distribution of your classes, those two things if managed correctly can make your model way better than those that don't look at it. Try to explore your data, plot the distribution of the classes based on a single word, two words, a phrase, a hashtag, or something like that, try to see if they can describe one or more classes, your model will probably improving by doing that. Here are two articles about feature extraction on text data: $URL$ $URL$ 

Dbscan seems a great choice for you, look at scikit-learn implementation for further $URL$ discovery. About being discrete or continuous, it actually doesn't matter, what you have to look at it is if the scale is the best suited for the algorithm in hand (and scikit-learn has algorithms to handle that). Other tip is to actually see if the attributes fit on a distribution, some of them might and parametric methods of detecting outliers are better suited for the task. 

As explained here in the docs of the function returning the ROC Curve they return a array because when you interpret a ROC curve what you're looking the performance of the predictor given a threshold. So, it means that, if I'm looking for only classifications that the model is above 80% sure, the ROC curve will have a different TPR/FPR than a threshold of 90%. If you have further questions, a great link explaining it: $URL$ 

I want to plot large heatmaps (say a matrix $500 \times 500$). I can do it in Python/matplotlib.pyplot with pcolor, but it is not interactive (and I need an interactive heatmap). I have tried with D3.js but what I found is aiming at displaying small heatmaps: $URL$ Naively extending this example with a bigger matrix (e.g. $500 \times 500$) can crash the web-browser. So, can anyone point me toward a good way of displaying and interacting with large heatmaps with a web-based technology: I want to be able to interact on a web-page or a ipython notebook. 

I am interested in parsing semi-structured text. Assume that I have a text with labels of the kind: year_field, year_value, identity_field, identity_value, ..., address_field, address_value, and so on. These fields and their associated values can be everywhere in the text, but usually they are near to each other, and more generally the text in organized in a (very) rough matrix, but rather often the value is just after the associated field with eventually some non-interesting information in between. The number of different format can be up to several dozens, and is not that rigid (do not count on spacing, moreover some information can be added and removed). I am looking toward machine learning techniques to extract all those (field,value) of interest. I think metric learning and/or conditional random fields (CRF) could be of a great help, but I have not practical experience with them. Does anyone have already encounter a similar problem? Any suggestion or literature on this topic? 

I had similar problem for my business: parsing tons of mails with different formatting and some of them slightly changing over the time. I did not solve it, but I use the service from $URL$ which is apparently using machine learning tools to parse these sort of semi-structured messages. 

I want to generate documents based on a grammar to build a custom training database. What are the tools and techniques to generate random texts based on a given grammar. More specifically, I would like to build a collection of texts with random 'sentences' and each one is a possible derivation (chosen randomly) from the grammar. I would like also to draw randomly the terminal symbols (the final words). I have looked into (python) nltk and have achieved some document generation, but the derivations chosen are not selected randomly. 

Then, the desired sampling is $v \sqrt{1-w^2} + w \mu$, where $w$ is the result from the rejection sampling scheme, and $v$ is uniformly sampled over the hypersphere. 

I found the following article on "Hierarchical Clustering With Prototypes via Minimax Linkage". It is stated in Property 6 that 

Thanks to your help. I finally got my code working, plus some bibliography. I put my hands on Directional Statistics (Mardia and Jupp, 1999) and on the Ulrich-Wood's algorithm for sampling. I post here what I understood from it, i.e. my code (in Python), with a 'movMF' flavour. The rejection sampling scheme: