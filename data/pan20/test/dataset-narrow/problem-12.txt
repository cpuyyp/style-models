DevOps fundamentally is an arbitrage play: it exploits the idea that if the things you are developing and operating are known to be stateless and immutable and ephemeral you can move very quickly, because there are a bunch of things you just don't need to do. And this is true, you can, and you don't. But "traditional" IT is all about managing statefulness and controlling the transitions between states (or "mutation" if you prefer). A database server, a file server. A database server for example: once you have made changes to a schema and data has gone in, you can no longer "roll back" because you probably have nowhere you can put that data that satisfies the constraints of all the stakeholders. If the database server goes down you can't just spin up a new one, it has to be exactly the same to the greatest extent possible, and you need to have a plan for resolving the deltas. DevOps doesn't really address this and even worse, the typical DevOps pitch includes some jabs at "traditional" IT for being slow and obsolete, without any acknowledgement that maybe there were reasons for things to be done the "old fashioned" way other than the practitioners being dinosaurs. What that sounds like to a stakeholder is that DevOps people are willing to take risks with the company's most precious assets - customer records, accounts, inventory control, all the other things that typically reside in the stateful parts of the technology infrastructure. Therefore to work with and bring on board skeptics the very first thing you must do is convince them that you have mitigated all the risks of operating the stateful parts of the infrastructure in a DevOps fashion. I will wager that some variation on this theme is the root cause of TSB's woes at the time of writing. They failed to exercise sufficient care with the stateful parts of their system. 

This answer is more about scalability considerations - if the number of workers can be high and/or multiple of them can produce logs at high rate at the same time. Yes, using multiple logfiles simultaneously is a good practice. Attempting to combine into a single logfile logs from multiple workers in real time will raise problems: 

In general scaling out is preferred, primarily because ability to scale up is typically limited by the types/sizes of the available physical resources. In particular for augmenting build power I'd recommend an analysis of your actual build to determine how it uses the machine resources, which/where its bottlenecks are and what scalability limitations it raises to reveal if scaling up even helps. For example I encountered cases in which the build time for 2 parallel builds on the same machine was longer than the combined build times of the same 2 builds executed sequentially (non-overlapping) on the same machine. In such case I wouldn't even consider scaling up as it would actually decrease the overall building capacity. 

Personally I don't see any reason for which an Artefact Repository - the recommeneded DevOps tool of managing artefacts - wouldn't be applicable to trained neural nets or other artefacts. The artefact size might have some upper limit for a particular artefact repository, but in such case it would be a technical or policy limitation one, not a fundamental/principial one. As for applying DevOps methodologies for the process producing these artefacts, I think most if not all of them can be applied equally well, as long as the artefacts: 

The above answer covers part of it but misses one of the important elements: convergent design. I wrote some words a while ago about this in the context of Chef at $URL$ but the short version is that a bash script is a set of instructions, while an Ansible playbook (or Chef recipe, Salt state, etc) is a description of desired state. By documenting the state you want rather than the steps you want to take to get there, you can cope with a lot more starting states. This was the heart of Promise Theory as outlined in CFEngine long ago, and a design which we (the config management tools) have all just been copying since. tl;dr Ansible code says what you want, bash code says how to do a thing. 

For a purely EC2-based environment, the easiest solution is to use AWS IAM roles and S3 bucket policies. Some formulations of this pattern also include KMS for encryption and DynamoDB instead of S3 for storage but I've not found a hugely compelling reason to use either. Check out $URL$ it is specifically aimed at Chef users but the underlying pattern works for anything, you just might have to make your own helper for doing the actual download from S3. 

Application deployment is a hard thing to pin down because it has a lot of sub-problems. Config management systems are excellent at modeling tasks which are convergent and work with "what is the desired state of the system". In the context of app deployment, this is great for things like deploying bits to a machine, managing configuration files, and setting up system services. What it is extremely bad for is things that are inherently procedural, notably database migrations and service restarts. I usually try to put the convergent logic in Chef and let an external procedural tool (usually Fabric in my case) handle the few remaining bits as well as sequencing the actual converges. So, basically, you should use both for the pieces they are best at. 

Let us say I do and within the section I edit the JSON to add . I save this and verify that it has taken effect by issuing , and I see it there. How would I then use this value from within a recipe? I would have expected to just be able to say when running the recipe on but that just returns nothing. The usual Ohai attributes such as obviously work as expected. 

The tool you need is Packer using Docker as the "builder" and Chef as the "provisioner". Then you can add the resulting image to your repo and reuse it without having to pack again, until your recipes change. 

One way to do this would be to write a recipe that stored the stage it was on on each node and just keep running it on both nodes repeatedly until eventually it had a chance to do all operations on all nodes. But this is clumsy and won't scale if there's node C, D, etc. I obv know about notify to sequence dependencies within a single node, but that won't work across multiple nodes. I can't be the only person who needs this, so is there a mechanism or a design pattern/best practice/TTP for this style of activity? 

Use a Git post-receive hook to issue a in the directory you want to keep updated. You might want to mount the directory you want to keep updated locally to the Git server, or execute that command remotely with into Git Bash on Windows, or Powershell. Alternatively have the hook call a webhook and have the webhook execute the . The webhook method will be easiest with BitBucket, I don't know about the others. 

And I'd add a wrapper script tying them together for one-shot end-to-end execution, either by CI/CD systems not interested in the individual steps or even for manual invocation. 

For organisations building software delivered for deployment and operation by 3rd party customers (like IT infrastructure equipment or consumer products, for example) the QA role could bring their expertise (ideally alongside Development) in customer-facing interactions (maybe just in an assisting role) to improve the understanding of how the customer is using the product and the challenges they face and bring that feedback into the aspects of the product lifecycle under their influence. For organisations which also deploy and operate the software being produced the QA role may extend to/blend into the Security/Operations field while closing the loop back towards Development and itself, for example: 

The jenkins user driving the deployments, deployment info hacked to a string for this example - 'artifact' : 

IMHO it's rather redundant to write TDD tests for items entirely covered by the IaaC state specification. Doing so implies that the effectiveness of the IaaC is questionable - why would you use it if so? Looking at it from a different prospective IaaC itself (if/when done properly) incorporates capabilities already tested and considered to be functioning reliably. Which is what makes it attractive and which makes writing TDD matching tests redundant. For example, an IaaC configuration specifying a system with SSH being installed already incorporates reliable checking for SSH being correctly installed and, if not, mechanisms for properly installing it. Which makes a TDD test for checking if SSH is installed redundant. If your IaaC config also specifies sshd to be started and listening on a specific port then TDD tests for sshd running and listening to the respective port would also be redundant. Note that my answer is not targeting TDD or any other type of testing that checks if your IaaC configuration as a whole fits a certain purpose. That remains valid and can be used in TDD, CI or similar checks during development of that IaaC specification - I believe @AnoE's answer is applicable in such case. 

Talking specifically about the image packaging piece of Docker, not the container runtime there are a few minor bits. The biggest is that a Docker image is more like a chroot, which means you are prevented from accidentally depending on shared system state since every file in use must be explicitly included in the image while a system package might pick up dynamic links you didn't expect or otherwise get more intertwined with other packages. This can come up with complex C dependencies getting loaded without your knowledge, for example OpenSSL. Additionally using deb packages don't de-duplicate shared bits in the say Docker's storage system does. For some this might be a good thing, better I/O performance and fewer moving pieces, but for others it might be a problem. 

Travis CI doesn't offer running your own workers, you use their infrastructure unless you mean the on-prem Travis Enterprise. You might be thinking of GitLab CI? 

Docker is a specific implementation of Linux containers, or if you want to be more precise Docker is a distribution of tools that includes which is an implementation of Linux containers. Other implementations include rkt, LXC, LXD, and (I think) Snappy from Ubuntu. 

True continuous integration tools (as opposed to just continuous testing) like Reitveld and Zuul can help, though they are only as good as the tests you write and code reviews you do. 

The author of that article has misappropriated the term "systems thinking" which has its origins in biology and sociology with Ludwig von Bertalanffy and his General Systems Theory (1968), and was later applied to cybernetics (in its original meaning of feedback loops in living and nonliving contexts - cells, machines, organisations, before "cybersecurity" and similar buzzwords hijacked it). In short you cannot learn about it because it is not a thing that exists in the way he uses it, just something he made up to sound clever - but if you are interested in real systems thinking, I'd say start from GST and work your way forwards. (Source: I have a Master's in Systems Analysis, before that term too was purloined to mean "IT support") 

The answer is: a bit of both. To satisfy the constraints of "use git" and "manage a vast codebase" Microsoft developed a new filesystem (previously they were using a variant of Perforce called SourceDepot). It's open source but I have no personal experience of using it. Why would you want a monorepo? The most obvious reason is that you can modify an API and all the callers of that API in an atomic commit. Also there are advantages to being able to do a search across the entire codebase... 

I don't think the image you mentioned is really in contradiction with the corresponding text. From the Preprod section further down on the same page (emphasis mine): 

But generally the DevOps transformations tend to not happen fast enough to allow for the above choices :) Now onto scaling up/out. Ideally the DevOps transformation should have greatly increased the predictability of your sw development and delivery process. You should be able to tell with a great degree of precision you existing team's capacity: how much sw/services of a certain quality can your current team get done in a certain amount of time. When you need more stuff done, or done better/faster than the current team's capacity you need to scale the team up/out. The exact details come from whatever process/methodology you use for your day-to-day activities, probably some agile variant (see also Does my organization need adopt Agile Soft. Dev. before adopting DevOps?) How - apparently easy: hire more people to handle the overflowing activities. If you're worried about how fast you can get them onboard and actually handling that overflow work at the expected rates: start hiring earlier, based on projections. The reality can, however, a bit more difficult than that. Scaling a software product/service or the development process and tools supporting it is not necessarily straight-forward, even for a DevOps-empowered organisation. One of my favourite examples is the weakest DevOps link when it comes to scalability: Continuous Integration. If the CI build fails there can be no Delivery/Deployment (kinda undermining the "continuous" term in the CD context). Just because a certain team is happily using CI with decent results it doesn't mean the same will happen if the team size doubles. Nope, the results could be devastating - the delivery process can grind down to a halt (in extreme cases). I'm trying to explain this in Congestion in Traditional CI Systems (disclaimer: I'm the founder of the company owning the referenced page and offering a solution to this problem). Another example could be, for example, the version control system (VCS) being used: as the software being stored into a repository evolves so does its change history, pushing the limits of the VCS system, sometimes beyond acceptable performance levels. Switching to a more scalable VCS system or splitting the codebase into multiple repositories could be an approach to address the problem. But evolving the development processes and tooling supporting them alongside the evolution of the software products/services being delivered are fundamentally just what a mature DevOps team does. They just need to also be taken into consideration when in the team's scaling up/out strategy. With that in mind, the how can be, indeed, reduced to just increasing the team size to cover the additional amount of work necessary for execution of the above-mentioned strategy. 

"Serverless" mostly just means you've got relatively simple microservices, generally just a little webapp or a single function that is automatically connected to a REST frontend. The same concepts apply as you would use for a more traditional web services: usually some mix of remote syslog and ElasticSearch writers. Networked or remote syslog has been around for a long time and has a fairly robust set of tools around it. You would have to run the central syslog server(s) but the protocol is very simple and there are pure client libraries in every language that you can use for sending logs. One common problem with remote syslog is that it has traditionally been based around UDP. This means that under heavy load, some log messages may be lost. This could be a good thing, helping avoid a cascade overload, but it is something to be aware of. Some newer syslog daemons also support a TCP-based protocol, but client support is less unified so just do your research. More recent but very popular is logging to ElasticSearch. This is mostly useful because of the Kibana dashboard and Logstash tooklit (often called ELK, ElasticSearch+Logstash+Kibana). Amazon even offers a hosted ElasticSearch option, making it somewhat easier to get started. ES uses a relatively simple REST API, so any language with an HTTP client (read: all of them) should be okay with logging to ES but make sure you are careful with blocking network operations in cases of partial system outages (i.e. make sure your app won't get stuck in a logging call that will never succeed and stop servicing user requests). More complex logging topologies are bounded only by your imagination, though these days you'll see a lot of use of the Kafka database/queue/whatever-you-want-to-call-it as a nexus point in very complex log distribution systems. On the "serverless" side, you'll generally want to integrate with these systems directly at the network level, so sending log data directly to syslog or ES from your service/function, rather than writing to local files (though maybe echo to those too for local debugging and development). 

You'd have all the project dependencies (IDEs, libraries, tools, etc) installed inside the virtualized environment, not on your system itself. You'd need to create/bring the project's corresponding image on your machine when you start working on it and you can delete or save it for later re-use on some external storage when done. A shared artifact respository would be recommended as storage if more than one developer can work on these projects. A huge advantage of this approach is that such virtualized environments play really well with automated CI/CD pipelines, in true DevOps spirit, helping to minimize/eliminate the differences between the development and staging/production environments. Depending on the programming language and/or the supporting development tools being used on a particular project you may have some more specific approaches/alternatives. For example or Virtualenv/virtualenv can be used to obtain such isolation for Python projects. But these solutions are typically not usable in other contextes. 

GIT hooks could work, but the trouble is that they're not automatically installed when the developers pulls a workspace. Which means they might end up being installed/used correctly or not. Unreliable. CI scripts could work as well, but the're reactive, simply indicating that something went wrong, in most cases humans still need to identify the culprit (which might be caused by an improper DTO subtree or by something else) and fix the problem. If you want your developers to properly pull their subtrees - offer them a scripted wrapper to do so - put the effort into that script so that it is simple to use and robust/working well such that the developer's workflow is effectively simplified. If you manage to do this (most of) the developers will see the benefit and they will use it. For the push side - you need to take into account that mistakes in push will happen (even with the above-mentioned script, if it's not used by all developers, for example). Everyone makes mistakes. Personally I favour preventing developers from directly pushing their changes into the integration branch and thus blocking possible mistakes from affecting the integration branch. Instead I'd have all candidate changes (i.e. either pushed in separate branches or just as diffs) funnelled through a mandatory centralized pre-commit verification system. Automated, of course, with the proper git subtrees management made reliable through that automation. The changes would only be committed/merged into the integration branch automatically by this system, if they meet the quality checks. This approach is applicable to any potential regression-causing mistakes, not just improper DTO subtrees. An example of such approach would be the gerrit-based development on OpenStack.