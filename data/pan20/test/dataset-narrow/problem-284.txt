Needless to say, it is impossible to get fluent in the naming of our database objects. Also, because we are very agile with releasing new updates (uptime is not much of a concern so fast is better than correct at first attempt), we keep adding more objects with wrong naming. Postgres will help me out in some cases, but it will not fix references to a table inside a stored procedure and the same goes for any references from our any application, or configuration script. What I am after is that if I write a stored procedure then the stored procedure will use the following while running: 

Now I would like to utilize the test view or any other view in a GUI which simply presents an editable table. This is possible with the recently introduced automatically updatable views. I want the table to contain drop-downs containing all possible values every time there is a column with constrains on it, like the column with a foreign key in this case. How can this be achieved? I know that I could utilize enums and I do that today, but on Amazon RDS they are unreasonably hard to modify. I had an idea of (ab)using the "EXPLAIN SELECT" query for each column and check which table and column is being queried. 

If I share a Google Spreadsheet with someone, I will share a link like this: $URL$ This means that I am free to edit the name of the file (for example to "Book - absolute final version really7"), the Google Drive folder it is located in etc. etc. while the link will still work. Would it be possible to simulate something similar in a database? The problem I am trying to solve is the intense pain and risk I have to go through every time I need to rename a column, table, view or stored procedure in the database, or if I want to move it to a different schema. Because of this pain, I currently have all sorts of naming of all these things so you will see a primary key lookup looking like this in my database: 

I am running log shipping with SQL server 2008 R2. I have a situation where the secondary database drive ran out of space and was not applying log shipping transaction logs. The way I want to fix this is delete the databases at the secondary and configure log shipping from scratch. The problem I have now is that my secondary databases are in the restoring state, and I cannot delete them. How can I proceed? For instance if I try to take them offline I get the error, 

If more records are added to Table2 I would want the contents of Table1 to be changed accordingly when the sync process next occurs. I found out that this is not working. Is it because when I tried it Table2 was not an article in my merge replication publication, or is it because merge replication doesn't support subqueries like this and recognising when a filter has changed. What it would mean is that the sync process would have to scan each filter, and work out what tables the subquery relates to and figure out whether these have changed. Does it do something like this? UPDATE: I have tried a number of things, but they don't work. 

I must put out a warning. We have now moved away from merge replication, and I would have to suggest that the scheme above could be a major performance issue. The exception to that would require you to have a small publication with either small amounts of filtering and/or filters that are not multiple levels deep. For instance 10 - 100 articles might work fine. If you push the above scheme too much you could have performance / locking issues. Every time you insert a record into the top filtering table the merge replication trigger has to process all the child tables. It also adds records to MSMerge_contents and MSMerge_genhistory for all the child tables. The more child tables you have, and if they have a big amount of records in them then the more processing power will be required. We had a problem with sp_MSsetupbelongs being too slow and timing out. In the end we came to the conclusion we were pushing merge replication too much and that this technology would not work for us. This leads me to suggest that if the filtering scheme in merge replication out of the box is not flexible enough for your situation then either don't filter, or don't use merge replication. Test Test Test though of course, every situation is different. 

I just spent one hour debugging a query which had previously worked but suddenly created strange results. The cause of this bug was that I have a table in one schema which I reference in the query. Someone had, however, added a view in the current schema which has the same name as the table in the other schema. This caused the new view in the current schema to "take over" the reference in the query causing it to reference the view instead of the table. Obviously this caused me to start looking in the wrong place for this bug and getting completely confused. My question is therefore, how can I prevent this from happening? For example by enforcing that all views an tables in the database needs unique names. By enforcing this, I mean that it will cause and error if one attempts to create two objects with identical names. Because I am using some external tools, I have set up postgres to search for tables in multiple schemas (I have forgot how I did this), which is probably partly to blame for this mistake, but I cannot change this. 

I am considering a model where I use PostgreSQL COPY to copy data from a file into a table. I was wondering what kind of performance to expect on high-end hardware. An interval in MB/s would be nice so I can start estimating. If you just know the answer on other databases, I would be interested to compare. 

What this achieves is that I could now modify the new "matches" table and for example add a column or rows, which do not need to get presented in the "reconciliations" view (by adding a where clause to filter them out). I am on Postgres 9.5 so the view is automatically updatable. Initial tests shows that there are no immediate problems with this, so I am asking this question to know what kind of problems I should be looking for. Performance is not a big issue. 

The answer at this point appears to be quite simple. I was using a different domain account to run the subscription agent. I had to log into my machine with that account and install the security certificate. 

So it uses the indexes, but does an index scan across the whole index, so 50000 records it scans 50000 records in the index. 

When they sync with the server won't it repeat the default constraint and set the value of 'NumberOfSides' back to 4? Am I missing something here? I don't want to disable the default constraints being replicated to the client either because there are valid default values that need to be set. The client side user needs to have these values set to use the software. UPDATE: The explanation about the default constraint being applied only once makes sense. So my only remaining question is if I have default contraints which use sequences I am in trouble because sequences cannot be replicated. Are these my only two options? 

So the thing to be aware of, because I was using parameterised filters the snapshot folder above doesn't contain any data, only schema. Also I was not filtering correct using SUSER_SNAME() so the filtering was not using the snapshot at all causing me performance issues. Once that was sorted I could see in the snapshot folder there subfolders with a snapshot for each user of the system. This data is the data filtered for that particular user. 

So it is definitely the case statement. And the issue does not resolve if I use ELSE 0 in the case statement. On my actual query I have a where clause (the where clause on the very first query above) which selects only 3 records as a proof of concept, and they all have 8022 as the value. The ELSE 0 never gets used if I add it and I still get the same error. 

Another solution could be to add aliases, so after renaming a_table to b_table then querying a_table would still work, but all queries to the a_table would get logged and I could search for those queries that get run towards the old table. This would mean that there would be no risk involved in renaming a table. The same goes for columns, stored procedures etc. Does such a solution exist? I realize that the first solution requires major work in the IDE but having such an IDE would mean that I could safely rename tables and columns at will. Maybe a quick-fix solution to this would be a Mac/Windows hack where if any text editor presents the following text to me: 1UL0Aii1aaErbU then that will always get auto-replaced with a_column no matter where it is presented. I believe AutoHotKey.com could achieve this on Windows. 

We are creating SAAS where we will at most have 50.000 customers. We are considering creating a user in the Postgres database for each customer. We will map each user that logs into our service to a user in the database in order to be very sure that they only have access to their own data. We also want to implement an audit trail directly in the database by this solutions, which utilizes triggers. If each customer has its own database user, then it would be very easy to see who did what, even if two customers would share the same data. Will we be running into some unexpected problems because we have 50.000 users in our database? Performance-wise or administration-wise. Maybe connection pooling would be more difficult, but I do not really know whether we would need it. 

I think it is incredibly tedious to administer user rights in Postgres. If I for example want to give editing access to an updatable view, then I need to ensure that the user has the right accesses to, the database, the schema, the underlying table, the sequences used (for inserts), all the columns. So often something goes wrong, also due to my lack of understanding. However, I don't really see any need to have a deep understanding of this. I can imagine a tool where I could just grant access to the view and the tool would somehow cascade (is that the right expression) the user rights to all needed objects. I understand that this could get difficult to do if giving access to, say, a pl/pgsql function, which touches 100s of objects under various conditions, but for basic objects like views, tables, etc. this should really save me a lot of time. Please tell me if this question should have been in the softwarerecs StackExchange, I felt that this was the right place, since it is so specific to postgres and, I believe, of general interest. 

I am using Merge Replication with SQL 2012. I look in the snapshot directory, but the largest file in there is a prc file which is 646 KB. I know for sure that the biggest of my replicated tables is 25 MB in the database after replicating, so I am not sure I understand why there aren't larger files in the snapshot directory? Also is there a place I can look for the snapshot files as they are downloaded to the subscriber? For instance the merge agent outputs messages such as, 

I am running merge replication in SQL2012 using web sync. Has anybody done any experiments to compare the performance of web sync to connecting to replication using a straight TCP/IP connection? Just wondering if it might work to open up a port for a TCP/IP connection over the web and not to use web sync. If it could work is it faster? 

I am running merge replication with SQL 2012. When I first create my publication the MSMerge_contents table is populated with a large number of records with the colv1 being set to 0xFF. When I run the stored procedure like this, 

I'm just wondering what happens if you do some kind of bulk insert for instance which inserted 5000 new records into this table. What would happen to the identity column then? 

I am using merge replication in SQL 2012. Why can't you mark default constraints as NOT FOR REPLICATION? You can disable all default constraints for a merge article, but it is all or nothing so it doesn't seem to offer enough control. How about this scenario, 

The problem is after step 2. We can force a software upgrade to the clients so that data is written into the new field. But some data may have been modified in the old field at one of the subscribers before they do the sync which makes the schema change. This would also be before they were forced to do the update. The migration could be done more than once, but by the time you get to step 4 its hard to know which modifications have been made to the old field, and which have been made to the new field. 

I am wondering whether anyone has experience with using this function? Does it have similar functionality as Greenplum, Aster Data or Stado/GridSQL? These all offer the ability to utilize many computers to process any SQL query in parallel, which is a huge benefit if one wants to undertake data-mining on a large dataset. I was unable to find any documentation on this on the Pgpool-II pages. 

But that would throw error messages in all the applications, and potentially stopping new customers from registering. Therefore, I am looking for this auto-converting rule/trigger which will handle any updates or inserts into this table. Is it possible to do this at all, so it won't come back and bite me later? 

I have about 26 applications interacting with our database, some of these will store email addresses in mixed case. Since I made the mistake of using varchar as the column type and not a case insensitive one, I would like to make some kind of trigger or rule, which will convert all attempts to save email addresses as mixed case, into lower case. I could achieve the same using the following query: 

I have a very central table in our database that is used by a range of applications, it has rules attached to it, triggers and all the dependencies that you can imagine. Now I would like to modify the table without causing any problems with the dependencies. I have previously been successful with doing the following but in a much less complex case: 

This fails with the error: ERROR: syntax error at or near "RETURNING" What I am trying to achieve is to be able to run an update statement on table t with a returning clause. I know that I can achieve this if the statement run in the rule was an INSERT or an UPDATE (by adding a RETURNING clause), but I do not know how to achieve this when it is a SELECT statement. If I remove the RETURNING part of the query above, then I get the following message: cannot perform UPDATE RETURNING on relation "t" query: UPDATE t SET c = true WHERE id IN (1000460) RETURNING id hint: You need an unconditional ON UPDATE DO INSTEAD rule with a RETURNING clause. It was this error message that lead me to attempt the syntax in the beginning of this post.