I'm not suggesting that you do it this way, but I've gotten around this problem by using Active Directory to manage my centralized authentication and implemented Likewise Open to authenticate my Linux machines. LWO gives a consistent UID and GID across all machines because they're based on a hash. This makes things like NFS and rsync very easy to deal with. It also solves nested groups nicely. Are you using any sort of centralized authentication like NIS or LDAP? 

Why is everyone giving the wrong answer? It's called a scissor lift: $URL$ They make servers that are hundreds of pounds. Lots of storage arrays are far more than that. There comes a time when you don't want to rely on hands that were, most likely, just reaching for greasy potato chips. Use the actual tool if server lifting is an issue. 

Have you sniffed for arp traffic to see what's being sent out, and from where? That would be my first step. Maybe it's screwing up because the other machines are sending advertisements. Whatever you find there would lead to your next step. If you have evidence that the other machines are sending ads, login and check the network config with a fine tooth comb. Virus scans all around. Look for more strange traffic from them. If there are no advertisements, but it still changes the arp table...well, come back and let us know, because I've got no idea. 

I have used GLPI ($URL$ which I liked, spreadsheets (which are cumbersome but effective), and I have been looking at IP Plan ($URL$ which seems nearly abandoned. Since I'm going to be getting another admin soon (YAY), I'm going to go back to GLPI. Also, I label each of the cables going to the switch with a Brother p-touch labeler. It's pretty cheap, but it's better than nothing. 

In a "normal" install, there's typically /etc/httpd/ that contains configuration. Specifically, there's /etc/httpd/conf/httpd.conf which is the "main" configuration file, and it specifies that all .conf files in /etc/httpd/conf.d/ are then included. In my configs, I have /etc/httpd/conf.d/ssl.conf which does all of the https-specific configuration, but there's a wide degree of flexibility for the administrator to decide on. 

No. Unless the user's group is set to _example (in the password file) or the user is explicitly added to the _example group in /etc/group, the user is not a member of that group. 

If you're doing an internally facing site, or a site with a closed userbase, then openLDAP would be perfectly fine. I do something similar with my own intranet sites (though I use Apache authenticated against AD). For public sites, I agree with Chopper3. Allow a local authentication if you want, but definitely allow OpenID (or even use the Twitter/Facebook/whatever centralized accounting). 

I am also having this problem. The cause is that upon installing 'rabbitmq-server' package on Ubuntu, an instance of rabbitmq starts. This is by design. Sadly. As for a solution, I haven't found one yet. Edit I don't know if this is the case for you, but in my case, I was changing the node name in puppet from 'rabbit' to something else. The brief explanation is that, as I mentioned, installing rabbitmq-server causes the rabbitmq-server service to run. By default, it uses a node name of 'rabbit'. In my case, puppet came along, configured rabbitmq, and then, before trying to start the service, ran '/etc/init.d/rabbitmq-service status', to check and see if it was already running. In an ideal world, the answer would have been "yes", since it was, in fact, running, but in this case, the '/etc/init.d/rabbitmq-service' script uses the configured node name to check and see if the instance is running - and when puppet changed the node name in /etc/rabbitmq/rabbitmq-env.conf, that completely broke the service script's ability to determine if it was running, so of course, the script returns 0, because it can't find a running instance. Puppet then tries to start the new instance with the new node name, but that fails because only one service can own a port at a time, and the running instance had it. To fix it, I set the RABBITMQ_NODENAME back to 'rabbit', and everything works. I wrote about it here: $URL$ 

These set up the actual service description and the "host_name" assigns it to a server. Look at the documentation here when creating your service. The stuff in red is necessary, the others are used to refine how your service check behaves. I hope this helped in some way. If not, please reply and let me know. You should also know that the default layout for Nagios configs is simple but unproductive. I wrote some documentation on how I lay mine out, and it's saved me a lot of time trying to find exactly the definition I was looking for. Depending on your installation, it may be overkill. $URL$ Good luck! 

I would recommend going to Crucial ($URL$ entering the make and model of your machine, and finding out how to max your machine out. I use that as my primary memory-upgrading resource :-) 

Note that I have not tried this, and I am not an apache guru, but assuming DaveRandom is right, my next attempt would be to use a .htaccess file in directory /A/, then use URL rewriting to point things to /A/child, with the logic that /A/child would never cause a directory traversal to /A/ to read the .htaccess file, since the directory interpretation occurs later in the process. Of course, if that works, it's probably a bug and will be fixed. 

I have heard great things about Netapp, but I haven't had the cash to pick one up. I have had two SNAP! appliances, and the performance was horrible on both. One has been junked, the other relegated to near-offline of probably-useless data. 

I suggest this might be cheating (since I gave you the answer offline after you asked this), but I'm claiming my points anyway ;-) Check $URL$ 

This is relatively recent news, but Drupal Mavens have released Open Atrium: $URL$ It looks pretty decently features, and sounds extensible. 

No, there's no record keeping of who modified which file. To give you an idea, you might check the logs to see who was logged in at that time, and under ideal circumstances, history files can examined. 

Assuming it's a Windows server, there's a knowledge base article that talks about how to get it out of Win2k: KB224544. I don't know if LDP.exe still operates like that, but it may. 

We have lots in common. One of the things we don't have in common is that you've got a 5Mb/s link, where as I'm using a 1.544Mb/s T1 between my various offices. Here's how I've coped. First, Robert Moir's suggestions are all spot on and his observations are correct. If I can flesh out one of his points, if you aren't monitoring the traffic on the various network links, do it. MRTG is not the best solution, but it's the simplest, and it makes it easy to see when your peak hours are. It may be (and in my case certainly is) that your slow traffic speeds are caused by your users. My wires run hot pretty much the whole time my users are doing anything. They know that when people are doing large DB queries, transferring files, and downloading content, then the network will be slow. Since it sounds like you might be using leased lines and not internet-pointed T1s or DSL links like I am, you can take advantage of a device known as a WAN accelerator which gets installed on both ends of a line (one each at your home office and your branch office, for example) and compresses the traffic, which yields an effective higher throughput than the bandwidth is capable of. Next, make any services local that you possibly can. The less you have to send over the link, the better. If the users have roaming profiles or home directories, keep those at the branch office, or at least sync them nightly so they can use the local copy. Decentralize your network security. I don't know how your network traffic works, but lots of places make their branch office only have one network connection, to the central office, and so internet bound traffic goes through the same pipe as internal traffic, in order to hit the corporate firewall. That setup is more secure, but it kills your bandwidth. Decentralizing that by getting internet access lines at the offices and installing smaller firewalls on those sounds like the opposite of what every security professional recommends (and it is), but business operations come before security in the IT version of Maslow's hierarchy.