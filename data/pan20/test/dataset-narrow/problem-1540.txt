I guess, a nice place to start is seminal work of John Kleinberg: $URL$ Yes, this paper is outdated, but anyway it's still very influential and simple. Also, digging articles, citing this paper, could be very helpful. By the way, these guys solve very similar task ( and they even use Weibo logs as well!): $URL$ 

The point is to learn useful variations of data instead of just splitting by large categorial variable. Each new row after encoding becomes immediately related with the output, while original categorial variable may be related only in indirect, latent manner. Plus, the interactions between output and the original variable are represented too, by definition. Think of it as of adding this interaction explicitly to add more intuitive justification to this method. So I think this method will be perfect not only for divide-and-conquer RF-style methods but also for plain LR as well. 

So, after you have instantiated it, just .transform each your upcoming mail_id and use results in upstream applications ( like online learning, for instance ). Obviously n_features is some knob to tune. But this has its flip side: the cardinality of mail ids is apriori high, so unless you have very limited amount of users you will need enormous n_features to minimize collisions. The better would be to take logs, where your ids coappear, and learn item2vec style model ( $URL$ ). This will deliver much denser ( and meaningful ) representation of mail_ids than FeatureHasher would do. Also take a look at this: $URL$ 

That's okay, as soon as you know how this should be related to the output. 0.5 in this example is just a rough approximation of input, but given that you have learned model even this approximation is still meaningful, as it instructs model -- which path to output to choose. If your model of choice is RF then 0.5 will land at about same set of leaves as more granular distribution ( if you were able to know it ) Meanwhile much stronger approximations are possible, for instance, one can use KDE to estimate response variable distribution and then draw samples from it at test time. Adding uniform random noise is just a hint in this direction. Maybe you want to follow it? Knowing only particular level of categorial variable is way less informative, than this 0.5, isn't it? 

I see many facets of your question and in what follows will present my top 2 :-) Facet 1 -- assessing the uncertainty in estimated coefficients In logistic regression, assessing the uncertainty in the estimated coefficients is virtually the same as for least-squares regression . In both logistic regression and least-squares regression, the regression coefficient table will include a column for the regression coefficients followed by a column of standard errors, then by a column of test statistics, and finally a column of p-values. The table below shows the example coefficient table output for the some regression problem (where the probability of buying some magazine for kids is estimated). 

I'd say there are pros and cons of using FeatureHasher for this purpose. If you really striving to use it, then just instantiate it like this: 

Note that the test statistics are labeled “z value” and the p-values are labeled “P(>|t|)” in the table above. The standard errors can be used to construct confidence intervals for the regression coefficients. Roughly speaking, going plus or minus 2 times the standard error from the regression coefficient gives approximately a 95% confidence interval for the coefficient. For example, for Residence Length, the regression coefficient is 0.024680. The next column gives the standard error of the regression coefficient which is 0.013800. Thus an approximate 95% confidence interval for the Residence Length regression coefficient is: $$ 0.024680 \pm 2 \times 0.013800 = 0.024680 \pm 0.0276\ $$ This means that the regression coefficient for Residence Length could be anywhere from -0.00292 to 0.05228 (with 95% confidence). As is well known, we often use the odds-ratio, which is the exponential of the regression coefficient (i.e., $\exp(\beta)$ ), to help to interpret the meaning of the regression coefficient. The odds-ratio for the Residence Length coefficient, as shown in the coefficient table, is 1.0250. This means that there is a 2.5% increase in the odds of buying the magazine associated with each additional year of residence. We can also compute the odds-ratios corresponding to the ends of the confidence interval. These odds-ratios will give us an equivalent confidence intervals for the odds. So continuing the example using Residence Length, the odds ratios corresponding to the ends of the confidence interval are $$ \exp(-0.00292)=0.99708 $$ and $$ \exp(0.05228)=1.05367. $$ Thus, the interval [0.99708,1.05367] is an approximate 95% confidence interval for the odds ratio. This means that there could be anywhere from a 0.292% decrease to a 5.367% increase in the odds of buying the magazine associated with each additional year of residence. Facet 2 -- modeling data with measurements uncertainty Here again many options are known. Take a look at this to get started: $URL$ I will just cite the source: 

I assume you want to build a Recommendation Engine which will recommend mutual funds to the clients based on their profile. Recommender System is a special class of Machine Learning and make use of Supervised/Unsupervised Machine Learning techniques. According to Wikipedia, 

Content Based Approach - In Content Based Approach, features of both the item as well as the user/client is used to offer personalized recommendation. According to Wikipedia, 

Like Word2vec is not a single algorithm but combination of two, namely, CBOW and Skip-Gram model; is Doc2Vec also a combination of any such algorithms? Or is it an algorithm in itself? 

So, let's say if one of the client with features UF1, UF2 purchased a mutual fund X with a set of features F1, F2 and F3 in the past. New mutual fund products, lets say Y and Z will only be recommended to this client if their features matches with that of F1, F2 and F3. It is also called Classification Based Approach because you are feeding in list of features of products and user into a classifier to predict what item should be recommended. To build a Content Based Approach model, you can use Machine Learning models like Naive Bayes Classifier or a Decision Tree. 

Popularity Based Approach - Popularity Based Approach will recommend items, in your case, mutual funds, based on the total count of purchases. It will rank the mutual fund with maximum purchase in the past at the top. The item at the top will be recommended to every user/client. 

I am trying to implement Doc2Vec model to convert a corpus into vectors using a pre-trained model (). I want to return the resultant vectors and save it in a text file. This is my code: 

If it is a categorization problem then you should look for a classification algorithm, not a regression technique. The simplest classification algorithm is Logistic Regression. But by the looks of it, seems like you do not have a labelled data-set and if that's the case you should look for Clustering techniques. Clustering is a part of Unsupervised Learning Technique in ML which create clusters or groups of similar data points. 

I have a data set which is distributed across various web services. The problem statement given to me is 1) Is it possible to determine which pregnant women are prone to an abortion? 2) Is it possible to determine which pregnant women are least likely to go to a hospital for delivery? 3) Is is possible to determine which pregnant women are most likely to give birth to an under weight child? I have made a comprehensive list of all the web services and the attributes within and it is given below: 

Popularity Based Approach Content Based Approach Collaborative Filtering Based Approach which composes of Nearest Neighbor and Matrix Factorization Hybrid Approach 

Abbreviations used: 1) H.B. - Health Block 2) H.F. - Health Facility 3) H.S.F - Health Sub-Facility 4) P.H.C. - Primary Healthcare "In terms of a political entity, a state is any politically organized community living under a single system of government." "A district is a type of administrative division that, in some countries, is managed by local government." As per the problem statement I get an intuition that it is a classification problem but the data set lacks output variable. What other kinds of ML algorithms could be applied to this data? Does the data makes any sense? Do I need additional attributes? What kind of data analysis could be performed and the intuition behind it? 

So essentially, the item here in your problem statement is the mutual fund of which you need to predict the preference a client would give. Higher rating/preference of a Mutual Fund, let's call it X for a client A will mean X should be recommended to A. Now, how do you give rating/preference to a list of items? Recommender Systems uses a number of approaches listed below: 

I'm building an NLP question-answering application using Doc2Vec technique in package of Python. My training questions is very small, only 20 documents and I am getting very inaccurate and different similarities even for same document while running at multiple instances. Almost all the sources which I referred trained data set containing thousands of documents. So I infer the reason behind my model's inaccuracy is the size of my data set. Is there any way to improve the similarity between documents, maybe by changing parameters or feature engineering? If yes, what are those parameters and by what ratio should I change them? If no, what are other ways or perhaps other neural network models to tackle the problem? 

Indeed is a parameter for Tree Booster. There are 4 choices, namely, , , and . The default is set to which heuristically chooses a faster algorithm based on the size of your dataset. Approximate algorithm () is chosen in your case since your dataset is huge. To change the tree construction algorithm, you have to pass in the function as follows: 

C45 made by Quinlan is able to produce rule for prediction. Check this Wikipedia page. I know that in Weka its name is J48. I have no idea which are implementations in R or Python. Anyway, from this kind of decision tree you should be able to infer rules for prediction. Later edit Also you might be interested in algorithms for directly inferring rules for classification. RIPPER is one, which again in Weka it received a different name JRip. See the original paper for RIPPER: Fast Effective Rule Induction, W.W. Cohen 1995 

If you take a look over the specifications of PMML which you can find here you can see on the left menu what options you have (like ModelTree, NaiveBayes, Neural Nets and so on). 

Suppose that you need to classify something in K classes, where K > 2. In this case the most often setup I use is one hot encoding. You will have K output columns, and in the training set you will set all values to 0, except the one which has the category index, which could have value 1. Thus, for each training data set instance you will have all outputs with values 0 or 1, all outputs sum to 1 for each instance. This looks like a probability, which reminds me of a technique used often to connect some outputs which are modeled as probability. This is called softmax function, more details on Wikipedia. This will allow you to put some constraints on the output values (it is basically a logistic function generalization) so that the output values will be modeled as probabilities. Finally, with or without softmax you can use the output as a discriminant function to select the proper category. Another final thought would be to avoid to encode you variables in a connected way. For example you can have the binary representation of the category index. This would induce to the learner an artificial connection between some outputs which are arbitrary. The one hot encoding has the advantage that is neutral to how labels are indexed. 

As always clustering is about what is the meaning of distance, since that encodes at least some part of your question. So you question is which channels are similar, where similarity is defined on users. Usually you do not assume some nesting on your instances, but what you have is basically a nesting of users in channels. So you have to incorporate this kind of nesting into the distance / similarity function. I would start with some observation on similarity function. We denote similarity between instance $i$ and $j$ with $d(i,j)$. Usually this function obeys the following condition $d(i,j)\ge0$ for any $i$ and $j$. Note that we usually have equality only on identical data points. The main consequence of this observation is that we can have basically an identity at user level. What you want is an identity at channel level. One way would be to define the similarity function like: $$d_c(i,j) = I_{c_i\ne c_j}d(i,j)$$ where $I_{c_i\ne c_j}$ is $1$ when channel of instance $i$ is different that channel of instance $j$. The main effect is that now all the points from the same channel are considered identical since the distance between them is zero. When the identity function is $1$ the distance is given by your real business distance which should be constructed by you and answer your question. If you use such kind of function in a hierarchical clustering basically it will start to find first some clusters which are identically with your grouping on channels and later on it will join clusters which are similar. This kind of approach will work even with a kmeans algorithm and perhaps with most of the clustering approaches. A slight different approach would be to define your $I$ function to return $1-\lambda$ when cluster is different and $\lambda$ when channels are equal, and have $\lambda$ a positive value close to $0$. This will not guarantee that all the clients will go into the same cluster, but it gives you the benefit that you have a slider which you can use to fine tune the compromise between all user of the same channel goes into the same cluster and a distance measure which is more robust to outliers. A totally different approach from an implementation point of view would be to define a more complex function directly on channel samples. This would be similar with how hierarchical clustering works since in order to joins to clusters it should have a distance function which would measure inter clusters similarity. See more on linkage-criteria. For example average linkage clustering. Note that I said that the approach is different only algorithmic. I would bet that the results would be similar with the first approach. A totally different approach would be to use a more robust criteria. It is known that sample average is not robust since one point can blow away the estimation. Median instead is much stable. You can use a median or a trimmed mean to have a more robust aggregation values. This would have the advantage that the clustering would be much faster since you would work with channels instead of clients and the running time for computing the clustering would be reduced. And finally, another approach which comes to my mind would be to go further with comparing channels, but this time using a distance which would be based on statistical tests. I will give you a scenario to clarify. Suppose that your users have an attribute named Weight, which as expected would be a continuous variable. How could you define a distance between the Weight of users of one channel and Weight of users of other channel? If you can assume a distribution, like a Gaussian on that weight you can build a two sample t-test on the two samples which are the two clusters. If you can't assume a distribution you can employ a homogeneity / independent test like two sample KS test. KS test is valuable since does not assume any distribution and is sensitive to both changes is shape and location. If you have nominal attributes you can employ a chi-square independence test. Be careful with what you can use from that tests. In order to have equal contribution for each attribute used in distance function you have to you p-values. Also note that if the test is significant it will have a small p-value, since the null hypotheses for both tests is the independence assumption, which can be translated as same marginal for both samples. So, smaller p-values means bigger distance. You can use $1-\text{p_value}$ or even you can try $\frac{1}{\text{p_value}}$.