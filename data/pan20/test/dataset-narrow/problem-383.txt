A better option would be to configure either a trace or extended event session to record rollbacks, though this would only help you going forward. 

Are you copying the mdf/ldf files with SQL Server running? That will almost never give you a usable copy, unless there are no writes to either file during the entire copy process. Either stop SQL Server or detatch the database before copying the files. 

You should be able to get away with this by creating views in SQL Server that are effectively wrappers for tables in the linked server. You will of course have to deal with the extra layer of security configuration for the linked server, thus ensuring that the Access users are connecting to the external data with the proper credentials (whether it be something per-user, or just a global set of credentials that's used for everybody accessing the FoxPro data). Keep a close eye on performance, since Access is going to be generating queries meant for SQL Server, and it tends to be a bit chatty to begin with. SQL Server is then going to generate queries for fetching the data from FoxPro. With that many layers of query generation, performance could be a crapshoot for large tables, and it'll be pretty difficult to diagnose or improve it. If the FoxPro data is mostly static historic data, then I'd recommend bringing it into SQL Server. If there's still read/write activity happening, then you'll have to either settle for the linked server approach, or periodically refreshing the data in SQL Server via SSIS or some other ETL. 

As far as I can tell, when SQL Server sees one of the two item or serial number parameters is null, it generates an execution plan that ignores that filter completely (since the query uses it for a LIKE, it's assuming it won't match any rows, but the procedure turns it into '%'). If either of those parameters is an empty string, it considers that filter (which will end up matching every row, since it turns into '%'), thus hash-matching the small number of rows with a matching serial number to a totally unfiltered dw_invtrans (4.7 million rows). What would be the best trick to have the execution plan ignore either of the two filters if it's null or a blank string? Bear in mind that I'm going to bind this to Reporting Services - normally I'd just have it dynamically select which query to run inside the procedure, but that often screws up Reporting Services' output column enumeration since it runs the procedure with FMTONLY on. 

A many-to-many design would account for the possibility that you could have multiple usable groupings for a set of rooms; perhaps you can book ABC, AB, or BC. Now you insert records in that table to define which rooms make up which virtual rooms, and you can easily take these into account when anti-joining to the reservations to see what's available: 

In the equality conditions, I like to list the tables in the same order they appear in the query. That's just personal preference, and from a technical standpoint, it makes no difference if you swap the sides of the operator. Obviously, for more complex (in)equality statements, or conditional expressions, you may be forced to improvise a bit. For outer joins, the vs. designation is simply to allow telling the DBMS which 'side' of the join should have all records returned regardless of no matches existing. A full outer join includes all records from both sides. The table listed first is considered 'left', and the table listed second is considered 'right'. Potential outer join pitfall: 

Welcome to hell. I've found that the most reliable way to get data from SQL Server to Excel is to use the querying capability from within Excel itself. The location of this function has moved around in recent releases, but right now, it's under Data, Get Data, From Database. This usually gets the best results for various data types, long data, etc. 

It's not totally seamless, since you'll have to modify the trigger logic, but it's a start at least. 

To make a long story short, I have a view called vwRelatives which uses CTE recursion to build family trees. It's meant to be queried for a single person at a time. This runs in about a quarter second: 

If any of those is NULL, that row's result will be NULL. So then you're summing up those terms, potentially with NULLs along for the ride, meaning if any of those columns happened to be null, that whole row would be left out of the sum (you may see a warning about an aggregate ignoring NULL values at this point): 

It's not included with Express Edition, but the other (non-free) editions have it, including Developer Edition, which is about $60 (USD) list. You may run into some sticky licensing situations, however, which I can't address with authority. 

I rather like SQL Pretty Printer for SSMS. $URL$ It doesn't format things 100% identically to how I would, but it certainly makes the awful 3rd-party code I have to debug a hell of a lot more readable. 

The first three items are stored in the master database, and can all be backed up. The fourth is stored (encrypted by the certificate from #3) in the header of the encrypted database. So in a failure scenario, you would have to restore enough of the encryption hierarchy to allow you to read the TDE key. SQL Server creates the service master key at installation; thus while restoring the master database to a different instance will also restore items 2 and 3, the necessary key(s) to decrypt them will not be present. Result: unreadable data. The two best options are to either restore the certificate (#3) from a backup (a good option if master cannot be restored for whatever reason), or restore your master database and its master key (#2) from a backup. Restoring the master key may be a better option if you have a lot of certificates/keys protected by this key, and need to make them all accessible at once. This comes with the same precautions normally associated with restoring the master database (collations, logins, database names and file paths, etc.) Generally, I'd only recommend restoring master in a recovery scenario. For a migration/scale-out scenario (such as using Availability Groups/mirroring with a TDE-encrypted database), it's better to backup/restore the certificate (#3) so that it's encrypted using the master keys unique to each instance it's moving to. You will need to include the private key with the certificate backup. In any case, you're going to have to make key/certificate backups, so guard them well, and store them in redundant, secure locations. Simply having a backup of master will not get you out of a TDE disaster; you're going to need a backup of at least one key or certificate. 

They are both 100% required. However, mssqlsystemresource is read-only, and specific to your version/patchlevel, so it's easy to restore, in theory. 

I have a need to allow certain users the ability to create folders and linked reports within specified folders, but I don't want them to be able to put actual reports in those folders. Is there a magic combination of permissions I can put on a role that will allow this? I'm assuming the "Create linked reports" permission applies to the source report that's being linked, and I'm not sure if linked reports fall under "Manage reports" or "Manage resources". (For the curious, I want to be able to differentiate between the people developing and deploying reports to a designated location, and people linking those reports to directories that certain end users will have access to.) Edit: Just to make it clearer what I'm trying to do... Say I've got two directories on the report server: Base Reports, and Sales Department. I want the developers to be able to create reports in Base Reports (no problem there). I also want to allow another group to be able to create linked reports in Sales Department which are based on reports within Base Reports. I do not want this group to be able to create actual reports within Sales Department. In order to allow the second group to create linked reports based on what's in Base Reports, I set Create Linked Reports permission on the Base Reports directory. What do I have to set on Sales Department to allow them to put the linked reports in there, without allowing them to create actual reports? 

The best out-of-the-box solutions I've found are to use a combination of the slow query log (which sucks compared to Profiler), and just running Wireshark on port 3306 (which really sucks compared to Profiler, and won't work if you're encrypting connections). There's also SHOW FULL PROCESSLIST, which is like a reduced combination of sys.dm_exec_sessions and sys.dm_exec_requests (with a little sys.dm_exec_sql_text thrown in). 

We've got an SQL Server instance that's used for email archiving (courtesy of a 3rd party archiving package). Every so often, the software is rolled over to a new empty database. We've done this quarterly in the past, but we're looking to do it monthly now. The amount of data being archived is about 15 - 20 GB per month, and the bulk of the data resides in only a handful of tables (usually 2 - 4). Once we roll over to a new database, the old one becomes used on a strictly read-only basis. What I'd like to do is optimize it into a nice, tight data file, with all the tables/indexes contiguous and having a very high fill factor, and not much empty space at the end of the data file. Also, we're using Standard Edition on this server, with all the limitations that implies (otherwise I'd be using data compression already). A few possibilities I can think of: 

Suppose I want to log all severity 16 (and higher) user error messages to a database table in order to allow some retrospective application troubleshooting (since the answer to "Can you send me the error message?" is quite often "No, I closed it.") Is a server-side trace going to be the most effective option, or would there be an even better approach? 

If Page reads/sec skyrockets, and the other two drop significantly, the troublesome query is probably trashing your buffer pool and causing a ton of disk I/O. You could then either work on tuning the query, or throw more hardware at it (more RAM, faster disks). 

We just migrated our web site database from a server running MySQL 5.5.11 to one running MariaDB 5.5.41. Everything is working mostly fine, except there's at least one query from our storefront that's generating pretty terrible execution plans on the new server, causing scans of a table that's around 6 GB. It's a query with two InnoDB tables and a relatively simple join between them, on two columns that are indexed (PK in catalog_category_flat_store_1, but not a covering index on core_url_rewrite): 

For a few reasons, I'd like to have all permission denied errors (number 229) emailed to me. Checking master.sys.messages shows that is_event_logged for this error is 0. Thus that rules out using SQL Server Agent error alerts, which rely on the SQL Server error log (I've tested this - I get no alert). I figured I'd peek in mssqlsystemresource and look at the view definition for sys.messages, thinking maybe I could update is_event_logged for the message in question. But this view gets the system error message from , so that's a no-go. Is there a reasonably simple way I can get all 229 errors emailed to me immediately (or within maybe 30 seconds) without harming server performance? A 60-second cool-down between emails would probably be a good idea too.