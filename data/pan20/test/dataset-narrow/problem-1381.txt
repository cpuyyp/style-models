Production management or technical management skills, or both, depending -- and experience. Some studios don't clearly differentiate between production (in this context I mean the scheduling and pipelining of tasks across each internal team, assisting in bug triage and balancing, ensuring all team dependencies are satisfied, that your schedule is accurate and that you are trending towards ship) management and technical management (which I mean to be more about the architecture of the in-code systems that implement the tools), and so you'll probably need both, or at least to be focusing on whichever one of those interests you more. That kind of role is almost always sourced internally, though (i.e., by promoting existing programmers or producers) so your best bet is to get an entry level position in the approximate area you're interested in and work your way up. Studios with dedicated tools teams generally have those teams because they require tools that are unique to their product's production goals, and so they're usually not building the same thing except in very general terms -- usually tools revolve around asset/content creation, management, and tracking. Especially with large, AAA titles tracking/reporting/metrics concerning content and content production can be extremely important due to the shear volume of data. 

Implementations are generally reasonably deeply involved and you probably won't see much talk of them here. The StackExchange software isn't well-suited to that kind of involved discussion, not to mention it would involve a significant investment of somebody's time. 

No, Max is not used to design all game content. Some studios may use it to design some content, but there are many options for building content. Some studios use Max, some use Maya, some use proprietary tools. Max and Maya are both 3D modelling programs, so some studios that do primarily 2D artwork probably use something like Photoshop (or again, something custom-written). 

You can combine the above two approaches, in fact: use one texture layer for the parts of the tile which would always remain their fixed, original color and another layer (or layers) containing the parts of the tile that can be color-shifted by some amount. This gives you quite a bit of flexibility and is very similar (if not exactly the same as) a common technique used to support "dye channels" in character's clothes or armor. 

You must define a tile size (constants and ). Then when you load your map, you know how many tiles the map has via some simple division ( for example). Once you have compute the tile counts in both directions, you can create an array to store all the tiles: 

Randomly. But not entirely randomly, as pure noise isn't very pleasing and isn't really even realistic in this case. Instead, generate some kind of one dimension smooth, repeatable noise (based on Perlin noise, although that alone might be too smooth, so perhaps some FBM noise). Use that to adjust the light's attenuation over time, and perhaps to jitter the light's effective position within a very small radius over time, and you should get a reasonable approximation of the candle effect. As for exactly how much or how often to do this... that's mostly up to you and your art style and what "looks good" in practice. You'll likely need to tweak both the frequency of updates and the parameters to the noise functions to get an effect you really like. 

Yes, it's feasible: if the player hits the trigger area, create the new form and position it appropriately and then close the old form (the order is relatively important here, because the default Windows Forms application driver, the call, will terminate the application when the last form is closed so you always want to make sure at least one exists, or you want to change that call). For example, assuming you call a method of the current form called when the player hits the exit area, you could: 

Per the documentation, you can acquire a reference to the depth-stencil surface for the device (if one exists for that device) by calling . If the method returns successfully, you can lock the bits of the resulting surface using the method of the surface interface. This gives you a locked rectangle with the bits of the surface accessible via the member. You will need to use the as well as surface format information obtained from to interpret the bits correctly. I don't know why you are trying to use the surface-copying routines as you are, but I don't believe they are necessary. 

It probably was not done by hand, but as the WAD file format is pretty well documented it was probably relatively easy to write a one-off tool to perform the conversion. It's possible one of the WAD editors floating around have the functionality buried away, but I suspect that the author used a tool he wrote himself (and has not released) in this case. The JSON structures and arrays used in the linked JSON file don't seem to correspond directly to the lump types in the WAD file, which suggests a custom tool to me. It looks like some of his maps aren't direct ports of DOOM maps either (at least none I recognize) so it's possible he's using an internal map building tool instead of just an internal JSON conversion tool. The author's email is at the bottom of his homepage. You'd probably get a better response if you ask him directly. 

Fewer meshes equate to fewer draw calls, so generally you should try to go for as few as possible. Split them when: 

There isn't a standard way to do this; each game or engine will (and should) elect to use the coordinate model that makes the most sense for it. That said, it's generally standard practice to normalize the coordinate model, so everything works the same where viable (it makes it easier to think about). That means it's generally better to have your logical world origin and rendering origins align (in the lower left or upper left, for example). Most 3D APIs provide mechanisms to support this through the configuration of the projection matrices or similar. As you speculated, if you are using orthographic projection in OpenGL, you can just flip the parameters to accordingly to adjust how your coordinate system appears. 

This article ("Advanced Character Physics" by Thomas Jakobsen, with a PDF mirror here that preserves images) discusses solving fixed distance constraints (which sound to me like your fixed joints) between particles by relaxation -- specifically you want the section "Solving several concurrent constraints by relaxation" on page 2, I think -- treating the constraints as infinitely stiff springs. I found this article approachable enough years ago when I was implementing something similar, so hopefully it will have what you need. A relevant passage: 

The controller object generally knows about (and thus contains some reference to) the view and model objects so as to facilitate the communication between them. The controller is usually the really vaguely-defined aspect of any particular MVC scenario, especially in games, where most of the controlling comes from a singular input layer that feeds its updates directly into the game logic code. You rarely actually need a distinct controller type for every logic or render type (for example, you should not really need ). The bits of your code responsible for turning user input into commands probably lives somewhere in your game loop and those commands translate to operations on the set of objects currently in play. Your card render objects can contain a reference to the corresponding object (or be given one immediately to render) such that they always draw the most up-to-date rendition of the card's logical state. You will probably want to simply create an instance of your overall view (which presumably owns all the s and other GUI elements and such, in your game class. You probably will have one instance of your game class created somewhere in your Main function or entry point. 

The problem you are referring to has to do with the camera's view volume clipping through the water plane; typical "underwater effects" are done via full-screen post-process effects, and wouldn't look correct if the view is clipped like this (it would just be the opposite problem). Even if you do bother to detect the clipping case (potentially difficult) and scissor or clip the otherwise full-screen underwater effect so it only applied to the underwater scene (which is a potential solution you could explore), you'd still have the problem of the actual water plane itself. The transition there would still be a jarring visual artifact. You could detect that plane and blur it heavily; this is probably the closest you can get to a real-world analogy of what the view may be through a partially-submerged camera lens. Alternatively, prevent this from happening entirely: don't let the camera go below the water plane if the player is "above" the water or above the water plane when the player is "below" the water. This will only really work if you have discrete player operation modes for above/below water, and works best if there is a physical button or other clear and obvious action the player can take to transition between "swimming on the surface" and "swimming below the surface." Instead of an explicit keystroke to dive, you could interpret forward motion while the camera is pitched down sufficiently to mean "dive." You can then use a graphical effect (splashes or whatever) to hide the camera's transition through the water plane, which it would normally not be able to cross. We used a technique like this in Guild Wars 2. 

No, you can't, there is no such concept in the graphics pipeline. If you are using instancing, you can control the rate at which the instanced attributes advance for a given attribute index. This can be done independently for each attribute, and is probably the closest you're going to get to what you are asking for using the actual functionality of the pipeline. It may not be flexible enough for your needs, however, and it does require instanced rendering. You can, of course, always build this abstraction into your CPU-side rendering layer by having a buffer object that understands what kind of primitive it stores (triangles, quads, lines, et cetera) and provides a way to manipulate per-primitive attribute buffers which, under the hood, it will replicate out into the real vertex buffer on a per-vertex basis. This won't give you the space-saving advantages, but it would present the simpler client-side interface that you are probably interested in.