My goal is to create a Model which I can help me understand which Music may generate a lot of revenue (Boolean). I created a field AverageRevenueGenerated which is the average of all Revenue generated for all artists. Im looking for a tool that can help me associate or generate insights based on input signals above. This cold be automatically or a specific guide that allows me to say for example if: 

I need to do pre-processing, My prediction data contains as hence it ends up with 0, instead of 10. How can I pass data to my predictions that needs to be transformed to categorical values. If I tried to convert it to categorical I end up with a different class value. User-agent is just an example, but could be IP address or called number which corresponds to very large finite set hence a dictionary does not scale. I think this is a common problem, but not sure how can I solve it. I tried using dummies Approach#3 and I end up generating additional columns which doesn't match my prediction dataset. Issue reported here. Complete code here. 

So far the answers have focused on learning particular methods. They are fine, but they won't make you a Data Scientist. Being a Data Scientist is not solely or even primarily about having mastery of particular data analysis methods (ML or others). Most fundamental is problem solving and decision support. What ever data you collect, what ever analysis methods you apply, and however you improve those methods over time, these must support the over-arching goals of solving problems or making better decisions. You need to start getting first-hand experience with data in your field. I don't mean Kaggle data (i.e. already cleaned). I mean raw data or nearly raw. A good 50% of a data scientist's time is spent wrangling raw data and cleaning it to the point where it's usable in analysis. You need to learn how to deal with missing data, erroneous data, ambiguous data, misformatted data, and so on. You should also get some experience with decisions that do not map neatly on to the data. Recommender systems are easy in this regard. For example, you might take on the challenge of evaluating software vulnerabilities to guide vulnerability management decisions. 

Hi I'm currently trying to predict if an item will be successful in my store, this means (How much is going to sale in USD) My training dataset contains many features: 

I provide an offline library of Music to my users. My goal is to understand what my users are looking for, which means translate raw user searches to: Music Artists, Songs, Albums and then add music to the company library. What are the suggested clustering algorithms to group common short sentences into a single entity. Example: 

Most of the examples I have found online for LSTMs refer to "random" text generation. One of the problems that I'm trying to solve is to generate a "summary" of many docs into 1 doc. For example: 

I'm using Cosine similarity to find common documents and group them. I have realized that similar docs: 

I tried this example and works fine for a specific number of clusters (K) where K < N | K <= N. But since searches are unpredictable need to find a way to automate the number of clusters: my goal is to cluster 2 or more similar items and let alone single searches in independent clusters, example: Cluster 1: 

Start with a specification or definition of what "quality" means for your analysis, and therefore for your data. Use definition/specification in 1) to enumerate the types of errors, omissions, missteps, modifications, etc. that might arise in the course of data collection and recording. This enumeration will always be provisional because there could be ways that quality could be degraded that you didn't think of originally. Use 2) to define inspection and test methods that might reveal the existence of any of these types of errors, omissions, etc. The default method is "eyeballing" -- having a knowledgeable, experienced person looking at the data to see if it looks right. When you uncover errors, omissions, etc. try to determine the root causes and generating processes. E.g. erroneous data in dozens of columns resulted from an "off-by-one" bug in the ETL job that generated/translated the data from source files. Using 2), 3) and 4), define processes that might either correct erroneous data or mitigate the effects of data quality problems. E.g. Soundex transformation of names can mitigate effects of misspelling, but can't mitigate people misusing the "name" field to enter notes: e.g. "Nelson - DO NOT CALL". Be aware that any process you might use to correct or mitigate errors or problems might create new errors or problems (e.g. truncation of numerical data). After you perform your analysis, look back at the data and ask "Could these results be the result of data quality problems rather than true/accurate/appropriate data?" In other words, double check. 

Not sure if this is the right forum, but currently i have a dataset which contains a list of TV shows. Each record contains pricing between competitors (price in provider 1. Example: Itunes) TV show cover image, synopsis, country of origin, language, etc. Looking for ideas what project is suggested that i can prototype it, i want to learn ML and this may be a useful dataset. 

I'm new to RL, does this scenario makes sense for live systems? Can I use offline data to train an RL model? Offline data includes Fraud cases detected historically. Reference. 

Sundar Pichai is correctly identified as a person, but also the word users. How can I differentiate real names vs words which refer to persons? I have seen that for popular people there is metadata like Knowledge Graph mid or Wikipedia articles, but for others there is no reference, (Example: from "Susan Fowler" recent Uber scandal) Any ideas/pointers will be greatly appreciated. 

I suggest you use ANTLR, which is a very powerful parser generator. It has a good GUI for entering your BNF. It has a Python target capability. 

The most direct and obvious transformation is from time domain to frequency domain. Possible methods include Fourier transform and wavelet transform. After the transform the signal is represented by a function of frequency-domain elements which can be operated on using ordinary algebra. It's also possible to model a time series as a trajectory of a dynamical system in a state space (see: $URL$ and $URL$ Dynamical systems can be modeled symbolically at a course-grain level (see: $URL$ and $URL$ Symbolic dynamics draws on linear algebra. 

Is your Masters in Computer Science? Statistics? Is 'data science' going to be at the center of your thesis? Or a side topic? I'll assume your in Statistics and that you want to focus your thesis on a 'data science' problem. If so, then I'm going to go against the grain and suggest that you should not start with a data set or an ML method. Instead, you should seek an interesting research problem that's poorly understood or where ML methods have not yet been proven successful, or where there are many competing ML methods but none seem better than others. Consider this data source: Stanford Large Network Dataset Collection. While you could pick one of these data sets, make up a problem statement, and then run some list of ML methods, that approach really doesn't tell you very much about what data science is all about, and in my opinion doesn't lead to a very good Masters thesis. Instead, you might do this: look for all the research papers that use ML on some specific category -- e.g. Collaboration networks (a.k.a. co-authorship). As you read each paper, try to find out what they were able to accomplish with each ML method and what they weren't able to address. Especially look for their suggestions for "future research". Maybe they all use the same method, but never tried competing ML methods. Or maybe they don't adequately validate their results, or maybe there data sets are small, or maybe their research questions and hypothesis were simplistic or limited. Most important: try to find out where this line of research is going. Why are they even bothering to do this? What is significant about it? Where and why are they encountering difficulties? 

In case of Toll fraud and client being insecure, the attacker can send calls via original agent hence the Service Provider won't reject calls immediately at least based from IP information. (Toll fraud). I'm exploring which approach is the best to implement an ML model, instead of static rules to be able to detect Toll fraud in a live system: 

Not all the features present in training data/test data will be present when I will be making predictions. Is this normal in ML ? What is the rule of thumb when doing feature engineering for this type of cases. 

I have a dataset which contains various columns: numerical and categorical. Dataset here: I was able to process the categorical data using and features in Pandas dataframe as explained here in Approach #2. 

Currently I have a list of Books which I need to compare to a different list. For example in my local list I have: