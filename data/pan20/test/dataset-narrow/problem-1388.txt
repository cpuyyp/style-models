I can think of a few approaches, here's a few in order of work required (but probably increased quality) First: alternate edge direction; this should be easy to implement 

If you use completely random numbers, there's no way to be sure. However, you can keep statistics of the payouts and adjust the probability of a payout at runtime using a feedback loop. If there hasn't been a payout for some time, make it more probable to win, and vice versa. You can run simulations of the runs (a couple billion iterations should do) to make sure your feedback loop works as intended. 

If this is for hack prevention, it's a losing battle. Things like hashes and sending random bits of data back to the server for validation come to mind, but nothing stops the hacker from using a separate, non-compromised copy of the original data as source for answering such checks. Same goes for sending checksums of in-memory resources. One way might be to send screenshots or checksums thereof, but then you hit the problem of different video cards/drivers rendering subtly different frames... All that said, I'd consider outsourcing the problem to someone who has been doing it for a while, like $URL$ (punkbuster). 

Sorting objects purely by depth has a huge value, especially on binning or tiled rendering architectures. Sort solids front to back, transparencies from back to front. When rendering, the stuff that's closest to the camera and solid gets rendered first, and anything else that occupies the same space will be rejected by an early z-buffer check. If the graphics architecture has some kind of hierarchial or low resolution z-buffer, the geometry may get culled even before anything gets rendered. 

Don't know if "everywhere" is true these days, but in good old days (on C64, PC and consoles like NES) the games ran in relatively low resolution (around 300x200 pixels), possibly with special hardware for tiled graphics and sprites. Back then, it made perfect sense to measure distances and sizes in pixels, as everything was "solid" and there were relatively small amount of pixels available. Today, even if you're doing a 2d game, you'll probably render it through OpenGL or similar, and can keep your camera dynamic, which in turn decouples pixels from your actual game "physics" entirely. Of course it may still make sense to think of your visible world in some pixel resolution and do the math that way, if it makes it easier to think about, but there's no longer any actual need. 

I won't refer to any specific formats, but here's points for your "yea or nay" question: Reasons to use custom formats: 

Pre-baked fractures are typically simple object swaps. Consider a vase getting hit by a bullet. Instead of having one vase, you have two: an intact one and one in pieces. When the bullet hits, you remove the intact vase from the scene and place the broken one in. 

I recall reading of a technique where the game engine tracked changes to a C source file and parsed the numeric values - that let the developers tweak the constants without recompile. I can't remember what they called this technique so I can't easily find the article. However, this sounds to me like too much of a hassle when you could, for example, move the constants to a LUA file and use that instead. 

Like postgoodism's answer states, it's Khronos api that basically does the same thing as WGL, GLX, and other platform-specific "gl bootstrapping" libraries do. It also provides some cross-api functionality, like using OpenCL with OpenGL while sharing some resources. In theory, at least. =) 

You're referring to two different kinds of avatar editors. Wii's avatar editor is mostly textures with glyph bombing - moving, scaling and rotating different facial features like nose, eyes, mouth etc. The only other features there are how tall and/or (iirc) fat your character is, and these are trivial scaling operations. More complex 3d avatar editors, like in mass effect or sims, are probably(tm) based on morph targets. The artist creates a "generic" face mesh, and lots of variants of it for each morph dimension set (like small nose, big nose, etc), and by combining all these morph dimensions, you get your final face. At least some of the commercial solutions are based on huge databases of actual 3d-scanned faces which they have analyzed to generate a morph database, resulting in interesting dimensions like male/female or caucasian/asian, etc. 

Parallel port is nice, but hard to find on a modern pc, and windows has made its programming rather difficult since XP or so. 

My guess is that they are forward-looking and see that a 64-bit build will be needed eventually (esp. related to stuff like unreal engine), and don't want the hassle of supporting both 32bit and 64bit versions. 

Couple things come to mind. First, it's possible each of your int uniforms actually takes a whole vector in uniform space, and thus you may be running out of space - you can try to combine them into vectors to see if it helps. Second, it's possible that the shader compiler happily discards the uniforms in case they're not actually in use. If this is the case, then they naturally won't get a location. 

You could just use plain text files and consider each character in the text file a small tower, with the character saying how tall the tower should be, and pick the material randomly or something.. That's the easiest route I can think of. Apart of procedural generation, which is probably easier. If you want to use minecraft maps, the format is documented here on the minecraftwiki.net, and a bunch of editors and similar programs are also available, most with source code too, so if the level format is hard to decipher, you have plenty of code as a reference. 

The .love files are actually renamed .zip files. These zip files contain directory structures, so if your code refers to, say, "foo/bar.dat", your zip file has to have a "foo" directory with the "bar.dat" contained within. See $URL$ for details. 

The quake 3 "shader" is basically just a description of the rendering parameters, and has little or nothing to do with opengl shaders. Here's an example from your tree package: 

Nick already gave a more specific answer, but I get the sense from your question that you'd benefit from a more generic answer. Different platforms have various ways of getting pixels to the screen. Software is written in layers. You can implement OpenGL on top of D3D (like Microsoft has done), or even on top of GDI as software rendering (like Microsoft did in older versions of Windows). It's also common to implement different versions of graphics APIs on top of each other (like OpenGL ES 1.0 on top of OpenGL ES 2.0). Microsoft also has an old graphics API called WinG, which can work even on the latest Windows versions; the output just goes through GDI instead of through specialized drivers. Conversely, you can also implement DirectX on top of OpenGL (and there are some such open source projects out there, with WINE including one implementation). And since modern windows uses hardware acceleration for GDI, I wouldn't be surprised if the modern GDI was actually "kind of" implemented on top of DirectX. Now, we have some cross-platform systems such as Java or WebGL. These generally offer the application developer one (in case of WebGL) or multiple (in case of Java) graphics APIs. What those APIs use to get the pixels on screen depends on the implementation at hand. For WebGL, Firefox (for instance) actually uses Direct3d as the back-end on Windows. On Linux, I'm pretty sure it uses OpenGL instead. On mobile devices the same will undoubtedly use OpenGL ES (although I'm not sure if mobile browsers support WebGL). 

Next, instead of drawing the fonts as line segments, render them as splines. Estimate the velocity of the pen, and over/undershoot by random (or periodic) amounts. Try to correct mistakes, like you would when doing actual writing (so the errors in the text don't accumulate into random mess). If you try this out, I'd love to see the results =) 

As others have noted, the matrix stack is on its way out, which is a good reason to investigate alternatives. On one of my projects, I used the OpenGL's matrix ops to calculate my matrices, but ended up having to use glGet functions to fetch the matrix back so I could do some operations on the matrix that OpenGL's own functions didn't support. The performance was abysmal, and I traced the loss to the single glGet function. It's possible that the matrix ops are actually performed on the GPU somehow, and using glGet caused pipeline flush, or something. Whatever the reason, I replaced the matrix operations with CML, the configurable math library, and the performance improved dramatically. Not only was I no longer bound to the performance metrics of the OpenGL matrix operations, CML also included tons of operations OpenGL doesn't support. And implementing a stack of matrices is pretty easy using STL vector or some such. 

Short answer: no. Slightly longer answer: you can use irrlicht to display video (for instance by updating a texture you're displaying), but you have to handle the decoding in some other way. Based on a quick googling, ffmpeg seems a popular answer. Here is a thread at irrlicht forums asking exactly the same thing you asked here. Personally, I'd suggest using some other format than mp4 due to licensing issues, such as xiph theora (cool guys, horrible project names). 

There were limitations for NPOT textures on older hardware. As mentioned on this OpenGL wiki, some older hardware requires NPOTs not to have mipmaps, compressed textures require alignment of 4x4 pixels, but new hardware should handle it perfectly. In my experience, some even relatively new hardware experiences major performance hit if you use NPOT textures instead of POTs. I don't know what the issue is; it's possible that in some combination of render states, the rendering is actually done in software. So, unless you have good reasons, I'd recommend still trying to use POTs as much as possible. As to why use NPOTs instead of POTs - if you have images that are of NPOT dimensions, say for example 1600x1200, using 2048x2048 pixel surface will waste a lot of video memory. 

If the data has no indices, you can simply feed linear list if your rendering API requires them. I.e, 

Your list of functions looks pretty exhaustive, possibly containing more than you actually need. Given the number of libs out there, my personal priority list when picking which one to use is: 

Another approach would be to generate a 2d mesh that has a hole in the middle, and render that on top of everything else. Generating this mesh may seem complicated at first, especially when the hole is partially bigger than the screen, but remember that the mesh may be way bigger than the screen. I'd go with something like a huge "o" circle shape. Alternatively, you can use stencil buffers to first render the parts that you want to keep visible to the stencil buffer and then render a huge quad over everything using said stencil buffer. 

What you probably want is a stencil buffer. Render your circle with stencil write, and then change opengl state so that only the bits that have the stencil bits on should be updated, and then render your shadow thingy. 

As to what price point to choose for your game, and whether you want to anchor to one currency or define unique price for each, depends on the market you're entering. The iDevice market, for instance, is a completely different beast from Steam. In any case I'd look at what the other games in your target market are doing, and start from there. It does seem, for instance, that Europeans always get higher prices than the folks in the US. Maybe we're just stupid enough to pay higher prices. 

Yes, doing several draw calls is slower than one. This is why state changes are expensive. There are techniques (like instancing) that fight this, but they're still more expensive than simply doing a single draw call. To combine several VBOs into one, they must have the same state (i.e, same shader, same textures, etc), and they must be static (i.e, not moving independently). Then simply transform the vertex data into the same space (for instance world space, or possibly the object space of a reference object) and handle the result as a single object. The first question you have to ask, however, is whether all of this is worth it. First make it work, then make it fast. 

Depends what you want to use it for. If you just want to play with new features and learn stuff, go right ahead! Whatever you do will be useful in the future. If you're aiming to sell software, and want to have a large target market, it might not be the best move right now. According to steam hardware survey, only about 5% of users have dx11-capable systems, while over 50% have dx10 now. Keep in mind though that this is survey of steam users, who more likely have gaming-capable PCs, so the total market situation is probably worse. 

So first, we have the card (or whatever you want to draw), depicted here by (a). Next, we take a copy of it, fill it in black and run a gaussian blur on it (b). All of this happens in photoshop or whatever your favorite art tool is. Next, in-game, when we want to draw the card, first draw the blurred black image with multiplicative (or alpha) blend, offset in some direction a bit, and then draw the card on top of that. Voil√°. For further trickery, you can alternate between shadow and card render to get effect like (d), or first draw all shadows and then the cards, like in (e) (I colored the cards in the (e) case to show that they're still separate =) It's also best not to use pure black (or full alpha) for the shadow to create more subtle, transparent shadows. 

In my experience the games that use passwords only have one password per level, so you end up with a couple dozen passwords which can be random, or, in some cases, recognizable words. There are some exceptions though; populous, lotus and sentinel come to mind. All of those use procedurally generated levels, and the "password" basically just encodes the seed for the generation. From usability point of view, the passwords are a horrible solution. Automatically saved state on exit and "do you want to continue" when re-starting the game is probably the best choice for a simple 2d game. The only positive side that I can think of is being able to take your game state with you and continue playing on some other computer.. But let's say you ignore all of the above and want to store your game state in a password. Let's say you end up having, say, 40 bits of state (or 5 bytes worth). This is short enough so you could just give it as five numbers the user has to input - IP addresses are 4 numbers, and people seem to be able to do that just fine. But let's do some encoding. Let's say we have an alphanumeric space - 0 to 9, A to Z. That's 36 possibilities. Let's simplify that down to 32 by removing four possibilities that are the most probably confused with other letters - o, i, l and 1. The remaining 32 is 5 bits, so you can encode your 40 bits in 8 characters. Whether this is easier for the user than 5 byte-range numbers, that's up to you. =) 

If you need even more compression, you COULD go and use RLE compression, but that will make the data less efficient to access. The compression described above gives you random access. If you only ever access the data sequentially, RLE is a good match. A trade-off is to only RLE one scanline at a time, but the compression ratio gets worse; you'll have to benchmark and see whether the compression ratio is good enough AND whether you can keep framerate high enough. Again, you can slice your data in various ways, and some are better than others, depending on your exact use case. Finally, for even further compression, you could use ZLIB or some other general-purpose compressor. You could, for example, only keep the currently active level in memory in an uncompressed form. And here "uncompressed" may still mean any of the methods mentioned above. 

and then just draw a (maxpow*2)-1 square around the light center.. Something like that. Doesn't give exactly the results you mentioned, because this one generates a more round light (yours are rather square). 

Making levels is an artform into itself. What makes a good level? That completely depends on the game in question. Play the game. What's fun? Make more of it. What's not fun? Remove it. Can't remove it? How to make it less bad? And so on, and so forth. I made a simple game for mobile devices over a decade ago, and it had certain number of features. In the hands of great level designers (from AAA game world), they managed to use the set of features in ways I would never have imagined, and the game was much better for it. (This was a demo game for mobile game tech, and I don't think we ever released it to the public. Which is a pity). 

The answer is, it kinda depends. Basically there are two schools of thought; small, optimal shaders for everything and the uber-shader camp. The small shaders are just that; do one thing, and do it well. Uber-shaders either control their functionality at runtime through uniforms, or compile into a bunch of different shaders through preprocessor macros (and/or generated shader sources). The optimal solution is probably neither, but some kind of hybrid. Perhaps a few uber-shaders or uber-shaders combined with specialized shaders for some cases. Small shader Pros: