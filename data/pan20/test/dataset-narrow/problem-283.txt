Access the backups files is controlled on the Windows end, not necessarily by SQL Server. SQL Server only requires that the account the SQL Server service is run as is given read/write access to the backup directory. If you want another account to have access to the backup directory you will have to specifically add it to the ACL of that directory, or a group that might already have the permissions. 

No, there is no option to adjust the file size of database files during the restore. You would have to shrink the files in production before taking the backup. If this is a copy of the database on the development instance you can always tell the developers if they want it to give up space (aka start dropping the other databases on that instances they don't need). Outside of that I would suggest letting the developers tells you what tables they actually need "fresh" data from in production, then just pull that out or a sampling of it out. However, depending on the method they use to develop and test their code they could always come up with their own test data. 

The general premise being that you can use the first process to save your passwords to a file, or table. Then in your SQL Agent job read the password back in a semi-secure manner. It is not the most secure because if anyone finds the key, they can obviously read the password as well. As well, in the end you are likely going to be passing it as plain text to the destination, unless it accepts use of a PSCredential object. 

The way you speak of only needing the primary file group, it sounds like to me that you only want the database structure and a very small amount of data. I would suggest if you just want an update of the database structure (objects, tables, etc) to simply script the database out with all the objects. This can be done quickly and easily with PowerShell. Then the data you actually do need, that cannot be regenerated, simply export that out (a PowerShell script could do this too). I am sure the export file can then be compressed down to a small size and then transfered over the connection to your site. Of course just like this one, @gbn suggestion could be scripted so it would be up to you to determine which option takes the least amount of time. 

They don't require SQL Server full installation to view the execution plans. They just need the tools/client software, which they should realize this if they want to look at them. They can use the following to view a plan file: 

An actual backdoor into SQL Server does exist that does not require restarting and/or rebooting anything into single-user mode. I have done this on systems where I did not have access but needed to check stuff. Download PSexec tools from here. Place this on the server and then in a command prompt execute this command:, or sqlwb.exe 

In this example I am going to pull that query in from a file and then search for the value, replacing it with the database name I want to query for... 

You can also access this by using the search function of the start menu, by just typing you should get similar results: 

SSRS Planning a Deployment would be a good article to read through. You can read specifically on the SSRS databases here. As it states: 

Provisioning the SQL Server Agent service account is simply referring to "configuring it". Which you stated you changed the service account from Network Services to Local System, that is provisioning the service to run as "Local System". It is just terminology used in their documentation. Regarding the issue at hand, you need to review the system log within Event Viewer or the SQL Agent error log to determine what issue it is having. The link provides methods of viewing that log, you can also browser to your default log directory for the instance and find it there also. The name of the current error log file for SQL Agent is generally "SQLAGENT.out", any previous error log file is "SQLAGENT.x" (x = sequential number). 

The listener is a failover resource within the WSFC the AG is built upon. It will follow the primary replica as it fails over so there is some redundancy in that portion for it. As mentioned the DNS side is based on your AD infrastructure and DNS uptime. In your example of the secondary data center you will want to configure the WSFC as a multi-subnet configuration, allowing your listener to hold an IP in both subnets for data centers. If the AG fails over to the secondary DC then your listener will become active under that subnet. Your applicatoins will only see downtime for the IP to come online in that secondary DC, and then the replica switch over as well. 

Short of checking the default path for your database you can do a search through Windows Explorer for . Default path for SQL Server is usually something like but that can be changed during installation. Ensure you are searching for the files on the database server, whether it is a local instance or you are connecting to a remote server. [e.g. I can connect to MYSERVER1 default instance from DESKTOP1 but if I detached a database from MYSERVER1 I have to search MYSERVER1's local drives.] 

It appears you are trying to use the GUI interface to attach the database. In this instance you are going to have to write a T-SQL statement in order to attach it, and let SQL Server know where you put the filestream data as well. There is a write up here that provides a walk through on doing this, the command the author used in order to attach a database once he had moved the default location of the filestream data for the example database: 

Once that is finished you should get some output that shows it was successful or not. Overall should look something like this: 

Depending on how badly you need to get the password, there is a way to read it from memory if the application has an active connection to the instance. However it is more of a parlor trick and not advised to perform on a production server. I would suggest going the route of checking the application code to see if a configuration file may have the password stored, or the developers may have the password documented. It is best to document the password somewhere secure, whether it be on a piece of paper in a safe or an encrypted master file that has limited access. 

This KB is the closest thing I can find regarding your error, that would come from SQL Server. I would see if you could find any other errors that may give you more explanation as to the issue. Since this seems to be dealing with XML data I cannot help you that much, have not worked with XML much. Other folks on this forum might be able to chime in though... 

Fixed database roles provide permissions to the whole database. Custom roles come into play when you don't want to give users permissions to all of the database, just a portion of it. For example, db_datareader provides SELECT permission to every table, view, etc in that database. For compliance reasons there may be some tables or views that HR staff should only be able to SELECT from and other staff shouldn't. That is usually where custom roles come into play. 

With (1) you should be good giving them and . On (2) I would probably create a role and then grant that role execute permissions on all the stored procedures, in place of granting it to each individual user. Then just add those users to the role. If you have all the stored procedures under a schema name you should be able to grant execute to that schema. That will save time and not have to worry about new procedures being added to the database. 

You will not be able to capture the print statements in the manner you are trying. The handler is meant to output the statement to the console, not to actually capture it. This would cause the message to write the console, but you will never (at least my testing) be able to append this into your variable. Which I don't recall this working like you have it in PowerShell 2.0, but I don't have that version available anymore to try. 

This is just an opinion but could be a consensus... Based on search this is actually a EMC product. However it also states in the guide that it uses VDI devices to backup and restore. I, and others probably, would advise against doing this with production databases. Again this is just a preference for me to stick with native backups. Now since you are getting service through a provider it could be there prerogative to force the service to be used. So I would try to be as professional as you can about it. 

I would say that SQL Server does care when you change the IP address of your server. It is all based on the configuration of the TCP/IP settings for that given instance. 

Regarding no column header in your CSV file One note if you actually want a header row or labels for your data then you are going to have to specify the columns in your query. Using gives PowerShell no idea what property names to apply. So if I just do as you did the first row of your CSV file will be data. If you change it to be the columns in the table like this: 

The easiest way to setup an alert is to base it off the error number that occurs for failed backups. That would be error 3041. To setup the Alert 

You can utilize the trace catalog views in SQL Server to get information about a given trace. Just pass in the ID of the trace you are interested in, code below pulls trace ID of , which is the default trace. 

What OS? If you are using Window Server 2008 or R2 have you set the firewall to allow connections? Something I always try is go to a command prompt and type in "telnet 1433". It is a quick way to see if the port is open and listening for traffic to your instance. That is if you configured it to use this port. If you have it set to dynamic ports (meaning it is not set to static) you might check to see if Browser service is running. I would also check the SQL ERRORLOG to make sure you see the messages that state "listening on...". That way you know it is in fact trying to listen to incoming calls.