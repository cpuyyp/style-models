Depending on your renderer you could make the surfaces more "fuzzy" applying a miniscule offset in multisampling depth using a noise like algorithm. This should result in a sort of blending effect for surfaces in close depth proximity. For what I know setting per-sample depth in fragment shader is quite recent in OpenGL and then only as an extension. OpenGL has the PolygonOffset but requires knowing in advance you are about to render something coplanar. While not feasible for surfaces within a single model it could work nicely for example when rendering a road overlaid on a piece of land. 

While I have no idea how UnrealEngine handle depth I have thought of a technique which would result in that kind of depth output. The idea is to better manage Z-fighting by rendering multiple (stacked) frustums with shorter distance between ner and far planes. Each step in the cascade would represent one of those frustums. Using near/far plane clipping, stencil masking and optional vf-culling you can easily render the cascade front-to-back, or each frustum to each own framebuffer and consolidate afterwards, with only minor hit in performance. This should scale better than increasing depth buffer bit depth which has diminishing returns. Besides not all platforms support high bit depth. OpenGL ES require only 16 bits! It is just a theory though :-) 

I display text using OpenGL which is aligned with (2D) objects that can have any rotation on display, making the text also appear at any angle on display. So far I've used FreeType to render raster for each supported symbol to file and load the files into OpenGL as textures at application start. When the text is rotated (other than 90 degree increments) the hinting goes all bad. My plan so far to remedy the situation is to render a few rotated (transformed) variants of each symbol and linearly interpolate between the two closest variants. As an example I may render variants in 15 degree increments between 0-75 degrees inclusive, and if text is rotated by 5 degrees on display I'd take 1/3 from the 0 degree raster and 2/3 from the 15 degree raster. ...but this is only a theory of mine. Should work if having small enough increments, but if there are too many of them I'll run out of memory resources and making the shader awfully inefficient. I imagine the increment count depends on the pixel size of each symbol which could turn problematic... So I reach out to the expert community: How to render rotated text with proper font hinting in real-time? Using OpenGL ES 2.0. Font size ~16pix. If you need more info ask away. 

Now, I tried everything I could think of but I don't get: 1) why it is not working properly. What am I doing wrong? Could anyone shed some light? 2) also, if there is any obvious more performant way of achieving what I want, please do let me know too (the current solution is taking 12ms in a non HD resolution). Or if you prefer, I can come with a different question for that later, after the operational problem described above is solved. Thanks anyways for your time. 

If that was not a shader task but rather a standard CPU code, it would be just a matter of saving the th into an array-of-list or array-of-array (e.g. ) at position . However, in a shader, for most hardwares and most DirectX and OpenGL versions, it seems to me that 1) we can't have multi-dimensional arrays, 2) we can't have resizable containers like lists and 3) we also can't pass variables and containers from one pass to the next. As one can easily see, this situation is also not well suited for using a simple texture to store values from one pass to the next, since I would need multiple into each grid cell and I can have quite many floats to store. So, how could I proceed in such a situation? In other words, how could I store the pixel-screen coordinates of given fragments in a spatial-partitioning of the screen (in this case a simple fixed size grid of the screen), to be passed from one shader pass to the next? 

I'd like to invite readers to read this article about Quake 2 engine rasterization technology explained in details, if they have the time. If TLDR, please pay attention to this image: 

Gets eliminated because of C/C++ lazy operand evaluation rule which is standardized with left to right order. The compiler generates no code so the machine has no overhead. 

Is open to interpretation, that could mean the larger in dimensions, therefore finer. And honestly it feels that using is a very unfair convention for the , which can only be selected with exactly , poor thing. 

In shaders, when using intrinsic, if we pass in the mip level parameter, and the sampler is a , what mip level is selected ? According to this blog: $URL$ It seems as if the coarser level of is selected, so that would be a operation going on internally. I still ask because the wording in the blog: 

wil, you largely have enough scholar background, you must have done Fourier and Laplace transforms in second year, and maybe in your engineering school again as part of signal processing classes. If you read "stupid tricks" there is not much more you can do to find a condensed course at this point. The second most famous paper that goes with SH for graphics is by Robin Green called "the gritty details": $URL$ And the third most important is the one by Ramamoorthi (the original paper preceding "An Efficient Representation for Irradiance Environment Maps"), which was called On the relationship between radiance and irradiance: determining the illumination from images of a convex Lambertian object And I think they mention somewhere that SH were previously most used by another science field, forgot which one, physics maybe, and that most of their base material came from these papers. So if you want to dig in the roots, you've got to pull out these mid last century references. 

Include a benchmarking phase for of both solutions in your application and then select the winning solution at runtime. This is simple, portable, and future proof. I mean, you do have test for this, right? ;-) I know this is a pretty generic answer to "best practice" for high performance but if you think of it there are thousands of possible target configurations and many vendors to consider. If you need that little extra, go pay your vendor for a driver optimized for your application. 

To make an application, OpenGL or not, run for extended time safety-critical applications are mostly built to restart on error, be it a driver bug, your application is broken, or hardware failure. If function during the downtime is required you need redundnacy. In your case maybe start multiple sessions of the same program, or even having dual computers running the same program outputing to the same display. OpenGL is no different from other applications, BUT vendors optimize and test for certain kinds of customers. Your scenario is not common so be suspicious. 

When cylinder and frustum intersect the hole will be a view into the internals of the cylinder. The hole is where external of the cylinder occlude frustum and internal of the cylinder does not occlude the frustum. Knowing this you should simply do the following to find the surface (or rather volume) made from the hole. Render frustum without color and set depth accordingly. Set a stencil bit where the cylinder (external) occludes the frustum, without setting new depth. Clear the previously set bit where the internal of the cylinder occludes the frustum, without setting new depth. Alternate between internal and external using CullFace. The stencil mask you now have is the surface where the frustum hole is. Render the frustum (in color) masking it with the stencil. Make sure to use LEQUALS depth test to allow the frustum to render on top of itself. Voila. Note that the depth buffer holds the value of the unmasked frustum so If you need to render something within the hole you would have to render the whole scene again and mask it to only render in the hole unless depth test passes against the frustum. Left as student exercise. 

Can we cut bandwidth by (for ) if we access only one slice of a multisampled texture in a shader ? I'm thinking of doing a manual resolve in a shader, because some targets may make no sense to average. e.g. depth, encoded normals, IDs.. Unfortunately, the memory layout may be so that MSAA slices are close in memory which means that a fetch nearby would cache the whole line. A solution to get an answer would be to measure the performance difference of accessing all slices vs 1. But I suppose there are no guarantees of repetition across hardware ? 

What we see is the Albedo channel, that's what you want to encode in 16 bits if I understand your question correctly. I'm not going to say "if it could be encoded in 256 colors for games in the past, why would we need 281474976710656 (that's 281 trillion) in new games?" even if I want to but that sounds like pixar's grumpy guy from Up. More constructively, if you noted in this image, everything is at the same lighting level. More particularly, the maximum intensity possible that preserves the desired saturation. (meaning in HSV space, V is maxed out) The emphasis is key, we barely need any bits because albedo has no depth to encode anyway. The dynamic comes from the shading, it makes sense to work in per components within shaders, and output to render targets. But storing albedo textures in that's not only overkill, that's a severe unjustified performance hog for our precious bandwidth. 

That will depend if x is coming from a uniform value, in which case the drivers will compile an adjustment of the shader (and put it in a combinator cache). At some point before starting to render. (this can cause hiccups in framerate when new uniform values are encountered that cause massive shader rebuild) Now, if is dependant, which means it comes from a previous calculation that comes from varying values or system values, the compiler will cut your expression in 2 parts, the calculus and the call. It will move the sampling of the texture in the first instructions of the shader. (provided it's unconditional in the shader flow). So you're basically screwed when it's 0, you'll pay the sampling anyway. In your case (0-2 wrt frag pos), I would recommend using because this is a low frequency "step" function, which means very good branch elimination in 3 quarters of the screen. 

I have a post-effect camera-shader in which I want to implement a simple spatial partitioning of the screen between two passes of the fragment shader. The first pass should divide the screen into cells of size = 30x30 pixels, and then for each green fragment, store somehow which of those screen cells the coordinate of such green fragment is located. That information should then be available for the second pass to work with. Well, merely dividing the screen into cells is trivial, considering that it's easy to calculate the row and column indexes of such screen-grid: 

However, besides very inefficient, the solution above is also not working correctly. Obvioulsy, I was expecting to get, around each green point at the screen, something like a pink-ish circle that is of stronger intensity when close to the center and fades away at the border, i.e. away from the center. Instead, this is what I get (in actual and zoomed sizes): 

First of all, let me explain what I am really trying to achieve. In a post-effect shader acting on a camera-renderer, I want to change the color of each fragment depending on how many green (RGBA = 0,~1,0,1) fragments there are nearby. So, for the sake of simplicity, let's imagine my screen is all black, with some (~10k) green points spread around. In the fragment shader, for each fragment I have to find how many green fragments there are within a given distance and then change the color of the current fragment being evaluated depending on that number of surrounding green fragments. I began with the most naïve approach one could think of: for each fragment, I calculated the coordinates of the 10th neighbor fragments to each side: right, left, up and down (which gives a surrounding square, then I discarded the corners from the computation to get a surrounding circle of radius ~10pixels, centered at the current fragment). Then I tested which of the fragments within such "surrounding bounding circle" was green (i.e. color.g > 0.9f). In affirmative case, I increased a counter variable. In my way of thinking, that would be doing a brute force and in order to visualize the result, I then painted each fragment with the RGBA color (counter*0.1f, 0, counter*0.1f, 1). Here is the code snippet: 

In principle you avoid using scatter (casting) behavior with GPU. They have offered random output coordinate write out since only shader model 5 as a need for extreme situations. But you should as general rule write your GPU code in a "gather" fashion. The difference: the hardware threads are logically soft-locked to one output position in the render target. The scheduler decides to what rectangle (or cube) in the target buffers, the kicked thread group will output results. So you should work around the designated destination, and figure out the start; instead of working from some start and computing a dynamic destination. This way not only will you please the hardware by avoiding contention, and race conditions completely; but also you'll avoid holes. 

There is no relation between 's direction and the world axis. And that's fortunate otherwise it would mean your camera is not a free view, it's some kind of a axis bound camera, which has its usages but most likely nothing in your mind right now. The default camera matrix is looking at +z when everything is Identity, that's most surely where you got confused. Know that this is purely a convention, but sticking to a widely adopted convention allows for easy compatibility with libraries like glm. the direction vector goes outward, so its . The view matrix can be constructed directly with TBN vectors in rows 0 1 2 :) (that's your RUF) Point 2. There is no translating of the camera to the origin, that's just how you chose to view (pun) it. There is translation of world objects toward the origin. Because the rasterizer will work in device coordinates (NDC) which is not configurable, so having a roaming camera (that travels) makes it necessary to indicate its translation in world as part of the last row (row 3), and in reverse since it's not a world matrix representing the camera position, it's the matrix that will bring back points into the view space. (by the way, incidentally meaning the actual camera world matrix is the inverse of the view matrix).