That by itself is fine, but you need to make sure to multiply the rays by the Lambertian BRDF (i.e. dot(normal, ray)/Pi). Even better is to directly sample from a cosine hemisphere, in which case all factors cancel out. This blog post has all the important info in one place. 

It is in fact possible to augment a ray tracer to make it capable of simulating wave effects. The paper Rendering Wave Effects with Augmented Light Field describes one way to do it. In summary, they introduce a framework called Augmented Light Field that allows them to model wave effects with a ray-based representation. In this framework, rays can carry negative radiance in addition to positive radiance; intuitively, rays carrying negative radiance can "subtract" light from surfaces that it cannot reach because of interference effects, for example. I haven't implemented it personally, and I can't speak for its implementation complexity, but I believe it is a good starting point. 

Yes, such a thing is possible. There are a few different ways of doing it, but the basic idea is to split up the scene into chunks and assign these chunks to different machines. Each machine then traces rays locally within their assigned chunk. The tricky bit is of course when rays leave a chunk, in which case machines need to synchronize and exchange rays that cross the local chunk boundaries. A recent paper from this year's EGSR, Distributed Out-of-Core Stochastic Progressive Photon Mapping, describes how to do this efficiently for Progressive Photon Mapping, although other light transport methods might be feasible. Although such distributed ray tracing is possible, it is certainly not the most efficient form of parallelization, and non-uniform lighting directly translates into non-uniform distribution of the workload. It is really only beneficial when the size of the scene far exceeds the memory of a single machine. 

Let be the number of vertices of valence 4. A closed sphere has and we have the equation . Now and . As a first step let us count the edges. If we multiply each face by its valence and sum those up, we get twice the number of edges (as each edge is shared by two faces), so: 

We can draw a perpendicular bisector of the isoscles triangle . Then the anlge at is the desired , the angle at is and the angle at is just . By the triangle postulate the sum of the interior angles of a triangle is . By applying this to our new triangle we get , so we get α = 90° - β/4 

As I said in the comments, this is indeed called torus or toroidal space when it comes to the topology. Even if the images suggest something 3 dimensional, this is just a visualization of the embedding of such a space in $\mathbb R^3$. Regarding the distance between two points, I think you mean following: Just consider the coordinates $p=(x_1,y_1)$ and $q=(x_2,y_2)$ in the $[0,X)\times [0,Y) \simeq (\mathbb R / X\mathbb Z)\times (\mathbb R / Y\mathbb Z)$ square. Then define all the translations $q_i$ of $q$ : $$ \begin{align*} q_0 &= (x_2\hphantom{+X\,\,\,},y_2\hphantom{+X\,\,\,})\\ q_1 &= (x_2+X,y_2+Y)\\ q_2 &= (x_2\hphantom{+X\,\,\,},y_2+Y)\\ q_3 &= (x_2-X,y_2+Y)\\ q_4 &= (x_2-X,y_2\hphantom{+X\,\,\,})\\ q_5 &= (x_2-X,y_2-Y)\\ q_6 &= (x_2\hphantom{+X\,\,\,},y_2-Y)\\ q_7 &= (x_2+X,y_2-Y)\\ q_8 &= (x_2+X,y_2\hphantom{+X\,\,\,}) \end{align*}$$ Then I think the distance you're looking for is $$d(p,q) = \min_{i=0}^8 \Vert p-q_i \Vert$$ 

Immediately we can see that divides . Similarly if we multiply each vertex with its valence we get again twice the number of edges, as each edge is shared by two vertices, so: 

It is likely that your specific problem arises from a singularity in when goes to 0. Rather than try to debug that, I'll point out that you don't need to transform your camera position into an angle to then modify it: rather, just create a rotation from your and angles, in the form of a rotation matrix or quaternion, and then apply it to your vector, directly rotating it without doing the perilous round-trip to an angle. 

Modern hardware doesn't really have the concept of texture binding points as exposed by OpenGL. Rather, the shader unit uses a descriptor (which is just some kind of fat pointer) which can potentially address any texture as long as it's resident in video memory. This is what makes things like bindless textures possible. So the large amount of "texture units" available in current implementations is simply trying to handwave this now-irrelevant part of the API. That said, the recommended way of implementing what you're trying to do (avoiding re-binding of textures) is to use texture arrays, which allow you to dynamically index into a set of textures in the shader, as long as they all have the same format and size. This presentation contains more details about these and other techniques for reducing driver overhead when rendering with modern OpenGL: Approaching Zero Driver Overhead 

You don't need to rebind the attributes, so long as you ensure that their location stays the same in both shaders. (Usually using the syntax in GLSL, but can also be done with if former is not available.) Uniforms, however, are part of the Shader Object state, and so will need to be set at least once for every shader. One way to minimize the cost of this is to use a Uniform Buffer Object, which will contain all your uniform values and can then be bound to a shader with a single call. 

And again we see that and must have the same parity. Plugging the first of this equation (with the very first ones) into we get 

The general equation for a upright cone with the tip at $Z=0$ is $$ 0 = X^2 + Y^2 - Z^2$$ (Ignoring the points for which $Z>0$). If you consider slices for some constant $Z$ you get circles centered on the line $X=Y=0$. To make an oblique cone with circles along the line $X=aZ,Y=bZ$ we can just shift the coordinates accordingly and get the equation $$ 0 = (X-aZ)^2 + (Y-bZ)^2 - Z^2$$ (again for $Z<0$). You can make it narrower or wider by scaling the $Z$ coordinate. To position the tip at some other point $(x,y,z)$ you can just shift the whole thing using the transform $(X',Y',Z') := (X-x,Y-y,Z-z)$. With the same idea you can easily parametrize the (upright) cone by $$(u,v) \mapsto (u\cos(v),u\sin(v),u)$$ or the oblique cone by $$(u,v) \mapsto (u\cos(v)-au,u\sin(v)-bu,u)$$ 

It certainly isn't always about low pass filters (see for example here on WP on "Noise Reduction") but you have to keep in mind that in your case the noise will always have a high frequency because you can basically consider each pixel with a independent noise realization. So any way of removing noise in this situation will have a low pass effect. 

Now the system of equations has all the information encoded (excluding the parity information), so it appears you don't already have everything you need to know to determine these four unknowns. I made a small Haskell program that does some bruteforcing with these conditions: Try it online! The first solution it does get is 

"Physically based" is not a very well defined term, so it's difficult to answer this question exactly. In general, "physically based" refers to the fact that the algorithm in question is derived from physically based principles. It's not physically correct (because we can't afford that) and some approximations usually have to be made, but it's well known what those approximations are and ideally there is some intuition as to what kind of error it introduces. This is in contrast to ad hoc models, which are usually made by an artist who observes an effect in real life and tries to write a shader or similar that somehow mimics the look of it. Usually ad hoc models are simpler and cheaper and tend to be the first solutions to turn up for a given problem, but they don't offer any particular insight into what's actually happening. It's also practically impossible to say how accurately such a model is able to reproduce the effect that it attempts to simulate. In the context of rendering, a "physically based renderer" would therefore be simply something that renders an image using physically based principles, which is a very vague classification. A ray tracer is not inherently physically based, and most early ray tracers in fact used ad hoc models for lighting and similar. From my personal experience, "physically based rendering" used to usually refer to solving the rendering equation. However, it seems that in recent years, many game engines have claimed this term as well to mean "we do energy conservation" or "we no longer use phong". So really, there's no hard classification of what "physically based rendering" means, and using ray tracing by itself does not make a renderer physically based. In offline rendering, this term is still mostly used to refer to renderers that solve the rendering equation, whereas in real-time rendering, it more likely refers to the use of microfacet models or similar. But it's possible that the meaning of this term will change over the years. 

It's completely possible to do all pixel lights in the fragment shader (or, say, do 4 at a time) by looping over an array. However, this comes at a great performance cost: You're going to be calculating lighting for every light on every fragment of the scene, even if that geometry isn't actually affected by the light's influence. I suspect that is reason why Unity doesn't do so: it probably culls the objects and submits only geometry that might be influenced by the respective light on each pass, meaning that shading for a light is only evaluated in the fragments where it may have an effect. This is a trade off between processing less fragments but duplicating the geometry cost. 

If the level is water-tight. That is, the brushes are adjacent to each other and fit together to form a hull without gaps or holes. If the level is disjoint, identify the set of brushes in each section. If all entities are inside the water-tight level, and in which section (from (2)) they are in. 

Simple answer: because it costs. :) Some higher-end monitor models are indeed calibrated at the factory (and come with a calibration report sheet) but it would not be cost-effective to do so for all models in the product line. Color reproduction varies even between batches of the exact same panel model, so it isn't possible to just use a set calibration for a given monitor model, it need to be done on a unit-by-unit basis. So to save costs, they instead determine the color reproduction curves of the "average" panel and use that for an ok but inconsistent result.