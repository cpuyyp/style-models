Just set up a VirtualHost for my.drupal.com and configure its DocumentRoot to point to the directory in which you've installed Drupal. Then configure an Alias inside that DocumentRoot to point /vislab to the root location of the static content (which is in a separate folder from the Drupal files). Here's a brief example: 

This can be accomplished by using recipient_bcc_maps to BCC all emails to a local-only address that is configured to route to your script. Add the following line to /etc/postfix/recipient_bcc. Run "postmap /etc/postfix/recipient_bcc" after editing is done. This tells Postfix to BCC all emails where the domain matches "@yourdomain.tld" to the "robotscript@localhost" address. 

If you set Apache to log via syslog and also use rsyslog to handle logging, you could configure rsyslog to send your logs to a file that's named according to the current date. That would eliminate the need to restart Apache, and you could easily rotate the older log files away. Check the following sites for more info: $URL$ $URL$ 

Rather than group your hosts by function, you could group them by role and have Ansible run a given role on a host only if that host is found in the designated group for that role. 

Make sure to chmod the permissions to 0600 on that options file to prevent it from being viewable by anyone except the owner. Then your mysql command would be: 

First, make sure you have a line in your /etc/apache2/ports.conf file. Then try changing to in your config. 

This will pick up ONLY sites that actually do have classes that extend the MY_Output class, and it will also give you a count of files within that site that you can expect to change. You can also use the cut utility instead of awk: 

If you just want to know the number of messages sitting in the deferred queue, then the following should get you a quick answer: 

Add the following lines to /etc/postfix/main.cf so that Postfix will use the configurations entered above. 

You can get a dump of the PHP environment by setting up a page with the following code and pointing your browser to it: 

Restart Postfix, and all your emails should then be copied to your script as well as delivered to the original recipient address. 

I've found that specifying "-O maxssf=0" on the ldapsearch command line is necessary in order for GSSAPI AD searches to work properly. The following command works for me to search the AD global catalog via a SSL connection: 

Putting login credentials into a command line like this is risky, as anyone who can view the process list could view those credentials. A better option is to put those credentials into an option file such as ~/.my.cnf and then reference that file in your command. 

There was also some fairly major headaches getting single sign-on working with NIS/Kerberos and all in all, I think we spent more time in support than we saved on licenses from MS From a purely non-tech point of view, the differences between Open Office files and Microsoft files caused a significant headache. I know that OO saves as MS Office files, but it just did not work in the real world. To take eMail as an example - People also get comfortable with the Outlook instantaneous update you see with exchange - not so happy with a 5 minute poll on a local IMAP / POP server. 

I would advise against NFS. Simply put - we had a web server farm, with JBoss, Apache, Tomcat and Oracle all using NFS shares for common configuration files, and logging. When the NFS share disappeared (admittedly a rare-ish occurrence) the whole thing just collapsed (predictable really, and I advised the 'devlopers' against this config time shortcut). There seems to be an issue with the version of NFS we were using that, if the target disappeared during a write, the client would drop into a never ending wait loop, waiting for the NFS target to come back. Even if the NFS box reattached - the loop still did not end. We were using a mix of RHEL 3,4,5. Storage was on RHEL4, servers were on RHEL5, storage network was a separate lan, and not running on vlans. If there is a load balanced front end, checking single storage - would this not bottleneck your system? Have you considered a read-only iSCSI connection to your storage, with an event driven script to move the uploaded file to the storage via ftp/scp when a file is uploaded? The only time I have implemented a successful centralised storage for multiple read heads was on an EMC storage array... All other cost effective attempts had their drawbacks. 

I used to host the canteen menu on a finger server, so that everyone could see the days menu <suck eggs> I'd also suggest that you write your program with specific server/groups in mind and not rely on the default DNS server :O </suck eggs> 

Personally, I tend to use binary for every transfer. I was stung once with a file that had been placed from a windows machine onto a unix machine using binary transfer. As a result the txt file still had CR LF endings. ASCI mode looks at source and dest platforms and performs line-ending translation, so the resultant file I got had CR CR LF line endings (LF was translated by ASCI to CR LF as it was unix -> windows) Sounds petty - but it was a 20Gb log file, and I only had one time window in which to collect it. I use eol conversion utils on the local machine if the necessity arises. EDIT: I was getting the file back onto windows from the unix host 

Make sure that the "mydestination" parameter in /etc/postfix/main.cf includes the "localhost" value. Add the following line to /etc/aliases. Run "postalias /etc/aliases" after editing is done. This is what actually passes emails destined for "robotscript@localhost" to your PHP script. Your script should be set up to read the contents of the email from STDIN. 

Actually, your LDAP domain should probably just be , not . This is also known as the base name in format. You might want to do a to reconfigure your OpenLDAP installation accordingly. Then, use as the LDAP URI (don't include the domain on the end), and set Ldapadmin to use as the base. You may also need to specify a bind DN (which looks something like ) and a bind password in order to fully connect to your server. 

Use the TMPDIR environment variable to tell mysqldump where to put its temporary files, and point it to a writable location. 

Also, in order for Kerberos authentication to work with ldapsearch, DNS must be properly configured for reverse IP lookups. If not, you'll get a "cannot determine realm for numeric host address" error. If necessary, you can put the IP and hostname of your AD server in your hosts file to get it working. 

It sounds like you may need to configure the refint overlay, which helps to maintain a directory's referential integrity in situations such as that which you described. There is a page at $URL$ which may be helpful towards setting up this overlay. 

Instead of using a self-signed certificate, would you be open to getting a free SSL certificate from startssl.com? That's what I've been doing for my personal web sites. If that's not an option, you'll want to import either your self-signed certificate or the CA that you used to sign that certificate into your browser and set it to be trusted. The method for doing this is browser-specific. The page at $URL$ has a section called "To Import Self-Signed Certificates into a Web Browser" that explains how to import certificates into several browsers. 

There's no need to dump the password into an actual file. Just echo it with the -n flag to prevent the newline, then read it in from the STDIN file descriptor (/dev/fd/0) as follows: 

Add the following line to /etc/postfix/transport. Run "postmap /etc/postfix/transport" after editing is done. This tells Postfix that emails destined for the "robotscript@localhost" address are to be delivered directly on this server, and not relayed elsewhere. 

Your RewriteRule directive is looking for the URL to start with either a forward slash followed by "secure_secrets" or only by "secure_secrets" - you don't mention the "~user" part anywhere. 

If I ping a VPN address from the Smoothwall box, it works. However, when I try to ping a VPN address from any other host on the local 192.168.65.0/24 subnet, I get a "Destination Port Unreachable" result. If I set a route on the local host to send VPN traffic directly to the 192.168.65.160 gateway, that works. It's only when I try to route VPN traffic through the Smoothwall box that it doesn't get through. What's going on? 

I have a fairly fresh Debian Testing install, and I am trying to install PEAR / PECL but when I run the command nothing happens. I don't want to install the PEAR package in the Debian repositories because in my experience it is too old. I have installed php5-dev. Here is what I tried: 

Where is a symbolic link to and is a symbolic link to . If I transfer I get the entire directory, but if I transfer I only get the symbolic link. On the computer I have downloaded it to this symbolic link is broken. Is there a way to tell VSFTP to follow the symbolic link and download the file? 

I would like Squid to only accept connections from users connecting using 'proxy.example.com' as the proxy domain. In other words if someone were to connect using another a domain that points the my server on port 8080 I want them to be denied access to proxy, including accessing the IP address directly. Is this doable in Squid? If not how would I set this up in IPtables? 

There is no phar.ini in . -- Edit 2 I have installed PEAR from the Debian repositories now and run . Still interested to figure out why the go-pear.phar installation is not working. $ whereis php 

I'm not sure if this is a Squid subject or IPTables. In my Squid configuration I have something like this setup: 

When I received warnings that the hard drive was almost full. I panicked and Ctrl-Ced the import. Twice. Looking at my DB I can see that the import was canceled. However when I look in I can see is still too big - I guess it still has all the imported data in it. It's about twice the size it should be for the data available in the DB. I ran but if anything it made the file larger. How can I clear this orphaned data out? I have about 100MB left on the hard drive ... 

Installed in Debian 7.4 in a Vagrant. My run script is working, but the moment I create a service/pants/log/ directory I start getting the following error: . My service continues to run but nothing gets logged. I've tried two different services and both have the same issue. I've tried various different service/pants/log/run scripts (mostly using svlogd), I've tried changing permissions on everything (a+rwx), the directory to store logs in exists and has the same permissions. If I run svlogd straight off the commandline it works as expected. The bash log below shows what happens as I rename to and back again . 

After which the went smoothly. I am concerned that I may have broken other things, by changing the owner from super, but that remains to be seen. 

The only effective way to do is is with mod rewrite rules. If the referrer is not from your own domain, then rewrite the image url to be one that is non-existant, somewhere else, or a 1px by 1px transparent gif. A lot of people do this with an image that says 'No leeching, buddy' or similar 

I used a product by Castle Rock called SNMPc - its not the most polished of tools, but it does everything that you could want and wont break the bank. Its basically an SNMP statistics collation tool, that can baseline and warn if baselines are deviated from. It can be given thresholds for growth and decline warnings and works well with any SNMP capable device. Enabling SNMP in *nix is simple, as it is within Windows. Extensibility of SNMP is quite easy too (at least on *nix) SNMP is free - there are 3 levels; all to do with security. SNMP 1 is plain text and very 'insecure'. SNMP 2 is encrypted, but its trivial. SNMP 3 uses certificates. It can be a bit of a chore getting it to work the first time though. Because there are so many counters and statistics that you can pull, it can also take a while working out which ones are right for you - but once this is done, its very straight forward. You pay for front end collation and trigger on events to make SNMP useful. You can do it with open source software, but I wanted a modicum of commercial support. Data can be polled from the devices (normal) and on critical systems, you can get the individual system to send a trap event notifying the trap manager that something went wrong, and they need to know now, and not wait for the next poll period. Polling remote devices can be done by using a collection agent - same sort of thing as the console, but without all the reporting wizardry - that then pushes the stats at the central console periodically. Of all the monitoring systems I have used, SNMP kept supplying what I was asked for, and within the budget I was given. There is a product for Microsoft Servers called MOM 'Microsoft Operations Manager' where the 5 server workgroup version is (or at least was) free... but extending it to keep an eye on enterprise systems such as Exchange and SQL could cost a lot in licenses and connectors. Beyond that - My experience is limited to SNMP, MOM, and Spotlight (by Quest) which was awesome and a bit too far beyond our budgetary range for all but the most critical of Oracle Databases. 

It is not possible to snoop on your machine without extra software. Remote Desktop does not allow a 'view' or share - Remote Assistance does but requires you to initiate it. VNC/RAdmin etc require that you install software on your machine, and although it can be push installed from a remote location, it's not generally done, and will alert you that it is installed or that someone is using the connection. The same applies to LogMeIn and GoTomyPC Back Orifice will do exactly what you are afraid of, but is detected by even the lowliest of Antivirus products. Dameware allows surreptitious observations of a machine (most others tell you that you are being observed) and also allows the remote administrator to push the installation onto your machine as standard. The real question you should be asking is this ... If you are scared of being discovered - should you really be doing it in the first place? Is it breaking acceptable usage policies? Are your actions illegal / against your employers company interest? If so - do not be scared of doing things you could be disciplined for by SIMPLY NOT DOING IT! Of course it is also possible to identify the port your PC is connected on, set that as a monitor port and send all its traffic to a packet capture station (thats what most big corporates do in cases where monitored activity is required) There - they see everything. I have before now installed SSH on windows machines, so that I can run command line utilities such as PSLIST and PSKILL to see if people are doing things they shouldnt and stop them) So the short answer to your question is 'No, not easily' The long answer to your question is 'Yes they can monitor everything you do, the level of their monitoring will take different amounts of effort, and often the effort is too much for the end results'