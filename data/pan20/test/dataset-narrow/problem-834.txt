The mathematical formula you are referring to is actually the way to determine the most efficient transmit window size settings for TCP, not the actual bandwidth available. TCP uses a mechanism called sliding windows that allows for adjustment of transmit speeds based on network conditions. The idea is that a TCP transmitter will send more and more data without requiring an acknowledgement from the receiver. If there's a loss of data then the amount of data sent between acknowledgements decreases, thus also decreasing the effective bandwidth. The formula in question actually determines the ideal sizing of that TCP transmit window based on the latency and round-trip latency between a given pair of hosts. The idea is to have a window sized such that the amount of data 'in flight' corresponds to what's known as the bandwidth-delay product. For example, if you have 50 megabits per second (6.25 megaBYTES) and an average round-trip latency of 100ms then you'd have 6.25 * 0.1 = 625 kilobytes of data. This would be the value that TCP would negotiate (if configured correctly). As the latency and bandwidth characteristics of your links varies then so too does the window size. What you need is a bandwidth management tool like iperf (free) running on both the source and your various destinations. This should give you an idea of the actual amount of throughput possible (independent of other apps) while also providing some insight into latency. Running an extended ping between hosts will also provide a general idea of latency characteristics. When you have this data you'll have a better idea of what you should be seeing as far as throughput goes. BTW - The use of any kind of LAN optimizer will often incorporate data compression, TCP optimization, caching, etc.. While handy, it can obscure the nature of the underlying links. Once you have an idea of the raw bandwidth / delay (and packet loss, potentially) you can take a closer look to make sure your various hosts are set up to take proper advantage of available bandwidth. 

In general it sounds like you're looking for a transparent proxy. It's not entirely clear what router you're using, but I'll assume it's some kind of Linux box. If so, iptables can be configured to redirect certain traffic (i.e. web traffic) to a squid server without altering any client configuration. Here's an example that might be useful. Searching for "transparent proxy" will likely provide some additional clues. If you're using a proper hardware router you also may want to look into WCCP as a mechanism to redirect traffic to- and from- proxy servers. 

It's generally not recommended to run copper directly between buildings as individual structures tend to be separately grounded (or earthed, if you prefer). Even a relatively small difference in potential can destroy equipment and even create a hazardous condition for people working on the equipment. Perhaps more to the (immediate) point connection integrity can be significantly degraded - especially if the method/path used to run the cable between buildings is subject to excessive interference. This can manifest as poor performance even on in-spec runs. The right way to do this is with fiber. Distance limitations cease to be a practical issue, electrical isolation is assured and the network will perform as it should. Go for a simple media converter or a switch with native capability for fiber ports - either will be fine for your application. If you don't have need for additional fiber capacity, the media converter is probably the cheaper/less intrusive mechanism. PS - Don't be concerned with what "ought" to work. The distance certifications on network media exist for a reason, a reason which will become abundantly clear when a previously working over-spec installation ceases to function for no apparent reason. Saving a bit of money up front doesn't seem like much of a deal when whole buildings are cut off. 

Ideally your ISP generates a filter based on the RIR information such that only your specific /25 is permitted via your BGP peering to them. The various registries (ARIN, RIPE, APNIC, etc) have mechanisms by which routes can be certified as belonging to particular entities. That said, BGP (as widely deployed) has no means to confirming that a given prefix legitimately belongs to a particular ASN or, indeed, whether the prefix was actually originated by the right network. There are (...and have been over the years) efforts to secure BGP by allowing for the cryptographic signature of prefixes as they're originated and propagated (see BGPsec as an example). Keep in mind that whatever approach is adopted has to make it through the standards bodies, be adopted by various vendors, some kind of set of central authorities has to be able to manage it, the carriers themselves have to adapt it to their operational processes, etc. Overlay all of this against a wildly decentralized network that encompasses most the planet and it gets even more difficult. In the mean time proper filtering at the edge is very much in the realm of widely accepted best-practice for carriers and when bogus advertisements do occur the complaints and filtering follow pretty quickly. 

1.) Bandwidth != latency, and jitter matters. A dedicated leased line's latency is constant and is generally substantially lower than a DSL or FIOS connection. This can be a critical point for certain types of applications. The upstream bandwidth from a given POP is certainly a point of variability, but generally far less so than consumer-oriented concentrators which can suffer in non-intuitive ways under periods of congestion. 2.) Consistency of bandwidth - If you lease a DS-3 then you have that bandwidth available at all times and under all circumstances. Even business-grade DSL/cable/similar services are subject to variations in available bandwidth based on usage. There's more bandwidth available under most conditions, but potentially less under others. 3.) Availability - At least in the US, traditional leased lines can be had anywhere. If your facility happens to be in an industrial park or located somewhat remotely then cable/DSL/FIOS may simply not be available at all. This is especially true in many commercial settings where a SONET mux may already be present but other services aren't economically viable for SP's. 4.) Mixture of services - Many SP's will offer Internet transit bandwidth as well as private WAN service on the same physical pipe. While this can be accomplished on a shared medium, better results are often had on dedicated pipes. Also, as antiquated as it may sound there is still a lot of use of standard PRI's rather than SIP trunks for a lot of phone systems. These can be mapped into a timeslot along with other services. 5.) To your first point, ALL Internet service is multiplexed. That's kind of the point. Without oversubscription networks don't really make a lot of sense. Predictability of the behavior of oversubscription is the point - and this isn't something that has been a major design goal of transports dedicated to consumer transport. In practice I'd generally rather see a straight Ethernet connection run natively or mapped over a SONET circuit for sites with substantial bandwidth requirements but there are plenty of instances where traditional SONET framing is really the only practical solution. For edge sites the consumer edge services (inclusive of so-called business grade) are generally both sufficient and attractively priced. 

This is an area that could have a huge amount written but here are some quick points: 1.) There are a series of timers with any Netflow implementation that, when exceeded, cause a flow record to be exported and flushed from the cache. 2.) One of these timers it the max timer. The idea here is that even an ongoing flow will cause a record to be generated. A long-lived flow might actually generate dozens (or hundreds) of sequential records. A good example of this would be a TCP flow lasting, say, 60 minutes on an implementation with a max timer of 5 minutes. That flow might actually be represented by 12 (+/-) records. Put another way, records are being generated even without a FIN (or without the session ending at all). 3.) Another timer is the inactive/idle timer. In this case if traffic isn't seen for a given flow within x seconds then the cache entry is timed out and the record exported. This is how Netflow identifies the end of any non-TCP flow (hint: not all implementations handle TCP FIN as a close-out method correctly either). There's a key point to consider here: if this timer is very long then you may not have a very accurate view of exactly when a given flow actually ended. 4.) To make things more interesting - if there's enough traffic to be overrunning the cache on the Netflow collector then many implementations will actually age out flows early to make room. Ideally it's flows with longer idle timers that are selected but if the cache exhaustion gets bad enough we can actually reach a degenerate condition where literally each packet generates a new flow. As an aside, this is why full (i.e un-sampled) Netflow is pretty much a thing of the past in most modern DC switches, as the sheer size of the flow cache becomes completely untenable. So, as always, the Devil is in the details. Netflow implementations vary in terms of size of cache and sample rate, how low (or high) these timers (and a few others) can be set, whether or not they correctly track on TCP flags, etc. It gets a lot more involved when figuring in IPFIX/v9 where locally programmable aggregation adds another level of intermediation in flow collection, etc. 

The vrf statement is optional at least in the 7.x code on 5500's and 9K's I have access to at the moment as well on 6.2 on 7K's I've used recently. Indeed, the point of the default vrf is that it's taken as the vrf when something else isn't specified. One thing you might consider trying is the command: 

When you configure a VM it gives you the option of assigning a number of CPU's. These -are- virtual CPU's. Generally the configuration presents these CPU's as copies of what the physical host contains but you do have the option of selecting a different CPU type, assuming that type is compatible with the physical hardware in the box.