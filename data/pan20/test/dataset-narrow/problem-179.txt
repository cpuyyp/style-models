Layer 1 is responsible for transferring bits across a wire -- SSL/TLS does not fit in this category. Layer 2 is responsible for hop-to-hop delivery -- SSL/TLS does not fit into this category. Layer 3 is responsible for end-to-end delivery -- SSL/TLS does not fit into this category. Layer 4 is responsible for service-to-service delivery -- SSL/TLS does not fit into this category. Which leaves the ambiguous trifecta of the application -- SSL/TLS is an application layer protocol (L5-L7). Each application (FTP, TFTP, DNS, HTTP, Telnet, etc...) does L5-L7 differently -- so trying to distinguish between what exactly happens at L5 vs L6 vs L7 is less productive. Hence, often, Network Engineers simply refer to L5+ as the application and we leave it at that. 

Another way of looking at it is in 8.3+ code, the NAT happens before the ACL. In 8.2- code, the NAT happens after the ACL. 

And now, lets say packet #220 arrives. This advances the window, and now packets with sequence number #157-#220 are being tracked. 

The terms , , and exist to create abstractions from what one layer is responsible for verses the others. To explain that, and to answer your question thoroughly, I'm going to start a bit "earlier" than what you are really asking. 

Your router is participating in Proxy ARP. Proxy ARP occurs when someone answers an ARP Request on behalf of another entity. In your example, the Router is offering an ARP Response, on behalf of the hosts in the 10.15.208.0/24 network. The router is doing this because it knows how to get to the /24 network because of the static route (). Here is an illustration of how Proxy ARP works: 

There are many use cases for Gratuitous ARP, all typically having to do with some sort of need to update the ARP Mapping or Switchport MAC address tables. HSRP (and many other redundancy protocols) use Gratuitous ARP frequently. This animation will illustrate how: 

(from the command "show ipsec sa") I'm sorry, I don't know Openswan very well (or at all), so I'm unable to provide any suggestions to that regards. :( 

This question is in regards to Control Plane Protection, or CPPr -- not to be confused with Control Plane Policing (CoPP). The major difference between CoPP and CPPr is that CPPr allows for more granular restricting of control plane traffic by separating the control plane traffic into three categories(also called Sub-Interfaces): 

(personal note: 8.3 changed the NAT, but you may as well consider it the rough draft and 8.4.2 the final. I would highly, highly recommend not using 8.3 for any reason) 

You are probably getting a Link Local address. The Link Local address is typically formed using EUI-64, which means it is based upon the MAC address of the interface. If the interface changes (due to stack of switches, or redundant switches, or failover, or new SVIs), then the Link Local address is likely to change as well. The solution? Manually configure a static IPv6 link local address, so that all manors of redundancy use the same one. You could, theoretically, also disable IPv6 if you weren't looking to use it. But again, that wouldn't be an option if you actually wanted to reach this switch via IPv6. 

That seems weird, but is probably not your problem. As far as I can interpret it, your problem is as follows: It seems like the server itself naturally responds to your crafted SYN packet with the RST-ACK in packet#23 (of the inside cap, see above) -- probably because port 8000 is closed. This will prompt the Firewall to forward the RST to the Outside (Packet#23 in Outside cap), and purge this connection from its state table. But your crafted SYN-ACK packet fires off in #25 (of the inside cap), prompting a RST from the Firewall (#26) because there is no entry in the connection table related to this flow. 

The metric will not change unless the interface goes up or down, the interface speed changes, or the number of hops change. Since those are what goes into the calculation, those are the values that will affect a re-calculation. In a stable network, these typically will not change, and therefore the figure will not typically change. 

You're probably running into issues with your DNS host having a policy against it. And/or your web host thinking you mean to have two different IP addresses pointing to the same web server (which doesn't make sense, or accomplish anything). If you do happen to get your DNS host to advertise two IP addresses for two different Web Servers hosted at two different companies for your single domain, keep in mind the toll that will play on the users session. Approximately half your connections will go to one host, and the other half will go to the other. Notice I said connections, not users. It is very possible (and likely) that even a single user's session will end up sending packets to both web servers. Which means every few clicks your users will loose all their session information (shopping carts, log in cookies, etc) 

Longest Match will always be looked at first. A /25 RIP route will be preferred over a /24 EIGRP/OSPF/BGP/anything route. 

And the application data itself, of course, is going to differ with every application. HTTP for web requests, FTP for file transfers, and so on. The term Datagram itself is simply the construct of a header and bits. And the terms above are simply the unique names for the particular Datagram that operates at each layer. 

The difference between your two Manual NAT statements is one of them also looks for a match on the destination (or source on return or outside-originated traffic). If you convert your NAT statements to 'layman' explanation, it will make more sense. For example: 

It is possible to split a single 8-wire UTP cable into two 100Mbps cables that only require 4-wires. But then both sides of the two wires would require two router ports. Each of the 8-wires are grouped together into 4-pairs: 

It seems you have the SVI for VLANs 10, 20, and 30 configured on all three switches. This is going to cause an IP conflict. The only switch that actually needs the SVI is whatever switch that will be doing the routing. All the others just need to simply forward data at L2. From what I can gather, that would be your Switch 3. I would suggest reading through this article to help understand the different options for routing between VLANs, and specifically this section on using a Layer 3 Switch. 

In comparison, both of these models are best suited for the purposes that each provide. SSL/TLS is best suited for a world where the Client can be anyone, but the Server's identity must be validated. In the World Wide Web, this is ideal, because you want anyone to be able to access your Web Server, and you want that "anyone" to feel safe in knowing that they are indeed connecting to the Server they mean to. SSH is best suited for a world where the Server is known a priori, and the Client must be validated before providing access to the resource. From then on, all that matters to the Client is that they are always connecting to the same server they initially connected to. This article is written from the perspective of the typical deployments of SSH and SSL/TLS. There are extensions to either protocol which allow for additional features, but they are rarely implemented. Namely, there is a method of SSL/TLS that also requires Client Authentication, known as Mutual Authentication. There is also a method of SSH that provides Certificate Authority validation. 

If you were capturing at every single one of these points, you would get the same flow data, duplicated four times. Which is obviously not very useful. The simple solution is to just pick one of these to monitor, but then what of the return traffic? Imagine a packet going from Host Y to Host X, that packet would also cross four possible (what I'll call) Netflow capture points: 

What command do you use to view the ARP Timeout setting on a Cisco Router? Not the remaining time or ageing time on a particular ARP entry. But the actual timeout setting itself. I know it is by default 4 hours, I just need to know what show command shows this. Some searches online pointed me towards or but neither of these worked on the GNS3 Routers I tried it against. It may only be available on real Routers, in which case I would love to have that confirmed. (And for someone to post the output of the command from a real router). 

It will ask you for a username and expect the one from the local username database. Using the vty line password will not allow you access to the device. 

This line, however is incorrect. 8.3+ ASA ACL syntax expects you to use the real attributes in all cases. You are specifying , which is in fact the mapped attribute. It should look like this: 

Neither. "Overload", or Dynamic PAT, is unidirectional. The connection will only work if the Inside hosts initiate the connection. If you ping from the Internal hosts, an entry in the connection table will allow the response back through. If you ping from an external host, there will be no entry in the connection table, and no way for the NAT device to know what internal host was the intended recipient... so the result: the packet is dropped. 

I teach TCP, and I often run into people who were mis-taught that the ACK is only sent when the Window Size is reached. This is not true. (To be really transparent, I too taught this incorrectly before I knew better as well, so I completely understand the mistake). NOTE, I'll be using Receiver/Sender to describe it, but keep in mind TCP is bidirectional, and both parties maintain a Window Size. The Window Size (that the Receiver sets) is a hard limit on how many bytes the Sender can send without being forced to stop to wait for an acknowledgement. The Window Size does not determine how often the Receiver should be sending ACKnowledgements. Originally, the TCP protocol called for an acknowledgement to be sent after each segment was received. Later, TCP was optimized to allow the Receiver to skip ACKs and send an ACKnowledgment every other packet (or more). The goal of TCP then, is for the Sender to continually be sending packets, without delay or interruption, because it continually receives ACKnowledgements, such that the count of "bytes in transit" is always less than the Window Size. If at any time, the Sender has sent a count of bytes equal to the window size without receiving an ACK, it is forced to pause sending and wait. The important thing to consider in all this is the Round Trip Time. Often, when you are studying TCP in a wireshark, you are only seeing the perspective of one party in the TCP conversation, which makes it hard to infer, or truly "see", the effect of the RTT. To illustrate the effect of RTT, take a look at these two captures. They are both capturing the same conversation, a 2MB file download over HTTP, but one is from the perspective of the Client, and the other is from the perspective of the Server. Note: its easier to analyse TCP if you turn off the Wireshark feature "Allow subdissector to reassemble TCP streams" Notice from the Server side capture (who is the sender of the file), the Server sends 8 full sized packets in a row (packet#'s 6-13) before receiving the first ACK in packet# 14. If you drill down in that ACK, notice the Client's acknowledgement is for the segment sent in Packet#7. And the ACK the Client sent in packet 20 is from the segment sent in Packet#9. See how the Client is indeed acknowledging every other packet. But it almost seems like it is acknowledging them "late". But in fact, this is just the effect of Round Trip Time. The Sender is able to send 7~ segments in the time it takes for the first segment to reach client and for the client's ACK to reach the server. If you take a look at the capture from the Client's perspective, it looks very 'clean', which is to say that every second packet it receives, it sends out an ACK. Notice also what happens at Packet# 23. The Server has sent all it can, because the "bytes in transit" reaches the Window Size, so it is forced to stop sending. Until the next ACK arrives. Since the ACK's are coming in every other segment received. Each ACK allows the sender to again send two new segments, before the Window is full again, and the Server is again forced to pause. This happens up until Packet# 51, when the Client (Recever) increases the Window Size significantly, allowing the Server (sender) to start transmitting data uninhibited again... at least until Packet #175, when the new Window fills up. 

When this packet makes it to the target server, and that server generates a response, the Source and Destination will be flipped. That return traffic will therefore have the following attributes: 

We're still only using one TCP connection, and we're still only using 9 packets. However, we don't have to wait the Round Trip Time (RTT) it takes between the Client and the Server in between asking for and receiving each object. If you need an analogy, imagine you're at a Restaurant, and you need Salt, Pepper, and Ketchup. Is it more efficient to ask your waiter/waitress for all three items at once, or to ask for them one at a time and wait for them to come back before making the next request? (Pipelining isn't directly related to your question, but is often described in conjunction with Keepalives and other HTTP efficiency features, so I decided to include it in this answer for completeness) 

The OSI model is not 100% conformed to. It is a guide line for understanding and organizing the various functions required to create a network, or an inter-network, or even the Internet. So not everything falls perfectly within a single OSI layer. However, understanding the primary purpose of each layer will help categorize different protocols into different layers. Layer 1 is primarily responsible for providing a mechanism or medium to move bits (1s and 0s) from one device to another. Twisted Pair wiring, Serial cables, Fiber Optics are all considered Layer 1 technologies. Wifi, though it doesn't have a physical medium, is also considered a Layer 1 technology since it has the ability to move 1s and 0s from one device to the next. Anecdotally, you could consider the string between two cans a Layer 1 "technology", since it has the ability to move 1s and 0s from one device to another. Layer 2 is concerned with what I like to call Hop to Hop delivery. It is responsible for putting the 1s and 0s on whatever L1 technology is being used, as well as retrieving the 1s and 0s from whatever L1 technology is being used. The Network Interface Card (NIC) is an example of a Layer 2 technology. To facilitate "hop to hop" delivery, Layer 2 uses a MAC address -- which is effectively the identification of a particular device's NIC. Layer 3 is responsible for what I like to call End to End delivery. This is where protocols like IP exist. Between two end points on either side of the Internet, there might be 10s or 100s of NICs that a packet must pass through. Layer 2 will get it from one NIC to the next. But Layer 3 will determine where the final destination is. Layer 2 and Layer 3 work together to move Data from one end point to another through each individual "NIC to NIC" hops between the end points. Layer 4 is responsible for segregating Network Streams. All the 1s and 0s carried by L1 arrive on your PC on the same NIC. Something has to distinguish which 1s and 0s belong to your Internet Browser, or your Music Streaming application, or your Chat program, or Operating System updates, or a plethora of other applications that send or receive data from the Internet. Layer 5, 6, and 7 all blend together. I wrote a potential example of the initial intention of each of those layers on reddit, but understand that was mostly an example, not a description of how it actually works: 

I don't mean for this to be considered an answer, I just wanted to show the output of testing @OzNetNerd's answer. I configured three routers: 

You can read more about VLANs and how they work in this article, and how to configure them on Cisco switches in this article. 

But to be clear about one of your questions... the Phase 1 keys are not regenerated every Phase 2. They new keys for each new Phase 2 come from the same Derivative Key, but different random values generated each Phase 2 exchange. 

It is non-standard (and in fact, counter-productive) for a firewall to not allow an ACK back through the Firewall if the original SYN or Data packet that caused the ACK was permitted through. TCP Requires ACKs to even form a connection. Your Network Manager is misinformed or confused. ACK's do not pose a security risk. Your next step is to prove your Firewall is receiving the initial SYN, and returning the SYN ACK. If the packet capture in your picture is captured from your Firewall, then you have sufficient proof of this fact. Specially if this capture is from the outside interface of your Firewall (the one facing the Internet)