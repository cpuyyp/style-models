Databases will typically issue a checkpoint prior to dumping the data to ensure all commits are written to storage. The checkpoint can cause the database to truncate the transaction log, depending on database settings. According to Microsoft, the following conditions will automatically trigger log truncation: 

In ASE, a cluster is multiple physical Database Servers working as a single logical Database Server. There is no separate designation if a Database Server (single or cluster) is running multiple databases, since that is the expected operation. Database is the collection of related objects (Tables, views, triggers, users, etc). Schemas: In ASE a user can own objects, and set permissions on those objects independently, which satisfies some of the same use cases for schemas BUT a user can not be dropped from the database if they own any objects. So for practical purposes, most sites don't use user level objects. The lowest granularity storage object in ASE is a (other databases call this a ) the size of which is set when the server is installed, but can be 2k, 4k, 8k or 16k. This means that the database allocates pages in chunks equivalent to the page size setting. Referential integrity can cross database (schema) boundaries. I highly recommend checking out sypron.nl. Rob Verschoor is an SAP VP & Engineer and his site is packed with good information, tips, tricks and scripts. His quick reference books are also VERY good, and I would recommend them to anyone needing to work with ASE, Replication Server or IQ. 

By default, Teradata SQL Assistant will attempt to query the views , , to populate the Database Explorer. It is possible in your environment that those objects are not accessible to developers or end users via the user. Instead, you may need to modify your connection settings ODBC or .Net Provider to use the X-Views in DBC. These are a collection of views which restrict the rows returned based on the privileges your user account has been granted to access or which you have created. The ODBC DSN, .Net Provider, and JDBC drivers for Teradata have a means to use the X-Views by default to enable database tools such as SQL Assistant or Teradata Studio/Studio Express to populate the database explorer controls "transparently". Try this and see if it works. 

Firstly increase the default InnoDB Buffer Pool Size in my.cnf (I believe it default to 8MB) You should probably set this to 75% of your RAM size (in general) 

You can alter some values in your /etc/my.cnf file. Some important ones are innodb_buffer_pool_size (assuming you are using InnoDB) and also query_cache_size. You should try to set the innodb_buffer_pool_size to 10% larger than the size of your data if you can. The default of 8MB really is woeful for performance. 

which seems to relate to the authentication protocol. Do I need to completely restart the MySQL master server, with old_passwords=0 in the my.cnf file? 

Teradata's optimizer quite possibly will re-write Method 3 to the same query plan as Method 2. Method 1 will result in an INNER JOIN because the qualification on the table shouldn't be in the WHERE clause but the ON clause of the condition. If you were to place an aggregate step such as a DISTINCT or GROUP BY in the derived table of Method 3 you will find the optimizer will likely satisfy the derived table as an individual step without re-writing the plan. I would suggest that you run the EXPLAIN for each query and compare the output. 

If you execute this from and using the flag to supress headers, and flag to redirect output to a file, it will build a script that can then be executed directly. 

The first name is the table name and the second name is the column name. You can use '%' as a wild card if you don't know the exact name you are looking for. 

You should probably create a .sql script that can run and return the value for multiple commands. A script can be easily created using the following format: 

Sounds like you are describing dimensional data modeling where you have a fact table supported by multiple dimension tables. Your Line Item table is your "fact" table and then you have a product and price "dimension" tables. Your price table could very well be a fact table or a slowly changing dimension table as well because there is a time element associated with a product's price changing over time. 

The space consumption for statistics on Teradata is not significant enough to qualify as a disadvantage. For example, the statistics for a single column is retained in a 16KB VARBYTE column on DBC.TVFields The rule of thumb has always been 10% change in the data which you have statistics collected or if they are stale. Unfortunately, stale has never really been clearly defined. Teradata 14.10 will introduce a more automated mechanism for maintaining statistics to help reduce the cost (CPU and IO) associated with the collecting stats using a homegrown maintenance schedule. This enhancement will be supported through Viewpoint. Teradata 14 also introduced some changes with statistics that have to be taken into consideration from previous releases. Carrie Ballinger has done a good job of capturing these changes in her articles on the Teradata Developer Exchange found here and here. Your stats maintenance schedule will be driven by the size of your environment, your ETL schedule, and manner in which your ETL maintains the target tables. We have multiple streams that maintain the same set of large target tables. As such we have moved the stats maintenance for these target tables to an external process instead of within each ETL stream. 

Yes, I believe this is the cause of your errors. Your method for introducing the new slave seems to be correct. It is quite strange in my opinion to define a table with a DATETIME field as the Primary Key. As you've quite rightly pointed out, the slave gets the replicated queries from the master and they will use the now() keyword in the queries, which will grab the timestamp from the local server. Really, the table should be defined with some other data type for the PK (such as INT or BIGINT) which can be guaranteed to be unique, unlike a timestamp inserted with now(). 

Most data models are lacking in a good DATE Dimension and thus force developers and report developers to rely on date arithmetic to find date boundaries that are relevant to the business model. (Fiscal Year, Fiscal Quarter, Fiscal Period, Calendar Quarter, etc.) A good CALENDAR table would go a long way to making your life easier. A simple EventDate BETWEEN SYSDATE - 458 and SYSDATE risks truncating dates out of your oldest quarter. Take TODAY as an example: SYSDATE - 458 yields 2010-09-28. If my math is correct the 3rd Quarter of 2010 started on July 1, 2010. You need roughly 548 days to make sure you are covering the entire range of current quarter plus the previous four full quarters. Trouble is that when you this will cause some overlap as your current quarter is partially complete. So you are faced with some additional logic to truncate out the fifth oldest quarter that you don't wish to include. My PL/SQL isn't the sharpest right now to write that logic but I hope the explanation helps shed some light on the approach you will need to take. 

Then any query you enter won't be sent to the binary log, and thus won't be sent across to the slaves. Once you're finished, re-activate binary logging with 

If you're just going to delete this extra data manually on the odd occasion, you can simply turn off the binary logging for the current session. On the MySQL prompt run the following: 

You could quite easily write a simply Perl or PHP script, running on a cron, to periodically check the MySQL server and alert you to any problems. For example, I have the following Perl code in my setup: 

There are a multitude of factors that go into determining which Teradata platform and the configuration of the platform that will suite your needs. Teradata has spent untold amounts of money on intellectual property and decades of experience working with potential customers to help them properly size a configuration that not only meets the immediate needs of a customer but provides them capacity for which the environment can adequately grow and evolve. I would strongly suggest you reach out to Teradata directly and engage them in a pre-sales capacity if your company is considering their technology to meet the needs of your data warehouse environment. For a sandbox environment, you could may be able to get away with using the one terabyte version of Teradata Express on an adequately sized server or consider using Amazon EC2 to stand up a instance of Teradata to complete a proof of concept. It should be noted that either of these options should not be used to gauge the performance of a production environment for service level agreements but whether or not the technology will accomplish what you are trying to do. 

From my Googling on the subject matter, I'm led to believe this is a network/firewall problem. I've checked that my RDS has a security group allowing all network traffic with the source machine (both private IP and Elastic IP) and the source machine has a security group allowing all network traffic with the RDS instance IP address. The master server has a user set up with replication privileges. I'm sure there must be something easy I'm missing here. I've tried running on the master server. How do I get the replication to proceed successfully? Some additional information: