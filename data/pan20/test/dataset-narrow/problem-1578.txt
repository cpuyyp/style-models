If I correctly understood your problem: you have a set of routes between the same two start and destination locations, and each route is described as a set of (lat, lon, time) tuples. In order to compute the distance between two routes, one possibility would be to apply a variant of the edit distance between strings. This algorithm computes the distance between two strings as the amount of string operations (edit, remove, add letters) required to transform the first string into the second one. You just have to come up with a set of "route operations", and the cost of each of these operations (add point routes, remove point routes, replace one point route by another one, and so on). 

The ten times rule seems like a rule of thumb to me, but it is true that the performance of your machine learning algorithm may decrease if you do not feed it with enough training data. A practical and data-driven way of determining whether you have enough training data is by plotting a learning curve, like the one in the example below: 

find the direction of greatest variance in your data write down the direction of the vector pointing in that direction, and 'divide' the data along that direction by its variance in that direction, so the resulting variance in that direction is 1. This provides you with an eigenvector (direction) and associated eigenvalue (scale). repeat steps 1-2, potentially as many times as you have dimensions, but with the constraint that the next vector must be orthogonal (aka at a right angle) to all previous. 

The result will be an ordered list of orthogonal vectors (eigenvectors), and scales (eigenvalues). This set of vectors/values can be viewed as a summary of your data, particularly if all you care about is your data's variance. I think there is an implicit assumption that the orthogonality implies independence of the resulting vectors, and from what I understand that's true if the data is Gaussian but not necessarily true in general. So I suppose whether your data can be modeled as Gaussian may or may not matter, depending on your use case. 

I'm trying to build a neural network with an unconventional architecture and a having trouble figuring out how. Usually we have connections like so, where $X=$ input, $H=$ hidden layer, $Y=$ output layer: $X_t \rightarrow H_t \rightarrow Y_t$ and $H_t \rightarrow H_{t+1}$ Normal Tensorflow or Keras nodes build above, where we end up with weight matrices connecting each component, i.e. $W_{xh}, W_{hh}, W_{hy}$. I would also like to introduce a connection: $Y_t \rightarrow H_{t+1}$, defined by a new weight matrix $W_{yh}$. I have looked at the Tensorflow and Keras implementations and there is a ($W_{xh}$) and ($W_{hh}$), where the second is defined in terms of the previous hidden node's output , defined here. Does anyone have a sense of how to access the output of the output layer in the subsequent step? Is it ? How are defined, and where is that documented? Thanks for any insight you can give! Edit: Before anyone asks yes, I recognize that such a connection is not strictly necessary. I am trying to understand information flow through network nodes. 

As you can see, creating the best set of features is not an automatic process, and domain expertise is very important. This is a very common thing in many data science related tasks, as you will see in the next point. 

Let's assume that you are training a model whose performance depends on a set of hyperparameters. In the case of a neural network, these parameters may be for instance the learning rate or the number of training iterations. Given a choice of hyperparameter values, you use the training set to train the model. But, how do you set the values for the hyperparameters? That's what the validation set is for. You can use it to evaluate the performance of your model for different combinations of hyperparameter values (e.g. by means of a grid search process) and keep the best trained model. But, how does your selected model compares to other different models? Is your neural network performing better than, let's say, a random forest trained with the same combination of training/test data? You cannot compare based on the validation set, because that validation set was part of the fitting of your model. You used it to select the hyperparameter values! The test set allows you to compare different models in an unbiased way, by basing your comparisons in data that were not use in any part of your training/hyperparameter selection process. 

You are describing a classification problem, where each 'class' is a person. You are trying to learn the hand->person mapping. Having two inputs map to the same output if they are similar enough is exactly what it means to 'classify' an input. The way you're framing your question matches it perfectly with a basic task of machine learning, multi-class classification. I have no idea if hand recognition software actually works this way - it would need to learn your hand from a single example or a very few, so I expect there is some 1-shot learning sorcery in the mix when used 'in the wild'. Edit: The adversarial part is particularly interesting. There are techniques for training neural networks to be robust to attacks, for example you might read about Generative Adversarial Networks, where a second network feeds inputs into the first to try to trick it. 

Clustering is difficult to do in high dimensions because the distance between most pairs of points is similar. Using an autoencoder lets you re-represent high dimensional points in a lower-dimensional space. It doesn't do clustering per se - but it is a useful preprocessing step for a secondary clustering step. You would map each input vector $x_i$ to a vector $z_i$ (not a matrix...) with a smaller dimensionality, say 2 or 3. You'd then use some other clustering algorithm on all the $z_i$ values. Maybe someone else can chime in on using auto-encoders for time series, because I have never done that. I would suspect that you would want one of the layers to be a 1D convolutional layer, but I am not sure. Some people use autoencoders as a data pre-processing step for classification too. In this case, you would first use an autoencoder to calculate the $x$-to-$z$ mapping, and then throw away the $z$-to-$\hat{x}$ part and use the $x$-to-$z$ mapping as the first layer in the MLP. 

As you can see on the rightmost part of the plot, the two lines in the plot tend to reach and asymptote. Therefore, you eventually will reach a point in which increasing the size of your dataset will not have an impact on your trained model. The distance between the test error and training error asymptotes is a representation of your model's overfitting. But more importantly, this plot is saying whether you need more data. Basically, if you represent test and training error for increasing larger subsets of your training data, and the lines do not seem to be reaching an asymptote, you should keep collecting more data. 

There are ranking algorithms based on machine learning that are aimed to build ranking models. Training data for these models is given in the form of partial ordering between each pair of elements in a sample. A brief description, together with a list of useful references, is given in the corresponding Wikipedia page. 

The fact that a feature is redundant in the presence of another one, or is not informative enough to describe the target variable, is not necessarily a sign of that feature not being useful. Indeed, it may be the case that such feature may be extremely informative when combined with another one, in spite of not being very useful when considered in isolation. Therefore, when applying feature selection methods, you should also consider combinations of features. However, and as pointed out by another answer to this question, finding the best combination of features is a NP-complete problem. Therefore, applying feature selection to individual features may be a good approximation. However, I'd rather apply a greedy approach (see for instance $URL$ for more information about the topic.) EDIT to answer OP's comment: a) The table below displays an extreme example of a feature that by itself is very informative, but in combination with others is totally redundant (feature_2). This is a regression problem in which we are trying to build a model to predict the "output" variable from "feature_1" and "feature_2". 

I think your image is mislabeled. I think each blue box is an LSTM layer, composed of multiple cells/units, each of which accepts a vector input x_t. With that, the answers to your questions are: 1) Yes 2) Yes, they are independent (at a single time step - they share information with each other between time steps) 3) Yes, each unit cell will take an input of size 5. I think the output size is always 1, similar to neural network nodes (like sigmoidal units) that combine and then activate. 

You are describing every binary classifier. However, you are missing a key point. If your classes are separable by the value of just ONE feature, you can do what you're saying and find e.g. F1 > 1.23 as a threshold. If classification involves a combination of features, you will need to describe some combination of thresholds for each feature, or (equivalently) some relationship between the features that tells you about the class label. It's the job of every binary classifier to do exactly this - they just do it in different ways. See for example this post. Your desire to have a combination of fixed sets of thresholds will only work if you can have a set of thresholds that will encompass/describe/classify every combination of feature values. If you want a set of easy-to-read thresholds like you mention, you should read about decision tree classifiers. They'll do something like what you want - but will also ensure that you provide a class label for every possible combination fo features values. The nice about about decision trees is that they'll let you leave out your current feature selection step - they just do it for you by (1) picking the feature that best discriminates the two classes overall, (2) picking the threshold value of that feature that gives the most information about the class label (usually), and (3) repeating (1-2) several times. 

One option is to use a simple approach, like just choosing random attributes, or taking one at a time. Another option is to use a metric to decide, at each step, the attribute that best splits the data. Different tree algorithms apply different metrics, like for instance Gini impurity (CART algorithm), or information gain (ID3 and C4.5 algorithms). You can read a brief description of these methods in the "Decision tree learning" entry on the Wikipedia. 

There are several techniques that you could apply in order to cluster data if your input is a matrix of pairwise distances between elements. As usual, the best option depends on your specific data, so it is hard to answer to the question of what is the best one, but you could try any of the following ones: 

Dynamic Time Warping (DTW) has quadratic complexity. There are several other versions of the algorithm, like FastDTW (linear complexity) that decrease the complexity by computing approximations. FastDTW is implemented, for instance, in this Python module. 

There is not such thing as "the best way to impute data". The best method will always depend on your specific application and model. Just remember the No Free Lunch Theorem. There are many ways in which you can impute values. You can ignore the rows containing missing values, you can impute values based on the other rows (mean, using classification, etc.), or you can even replace the missing values by a constant. Which method to use will depend on how much effort you want to put into the imputation algorithm and the final results. As usual, I'd start by using a simple method (the average, as you suggested), and increase the complexity of the imputation only if the results are not satisfactory enough. 

I would try using Google's CausalImpact package. Your use-case isn't causal inference exactly, but CausalImpact relies on bayesian structural time series models (using the bsts package) and has some good defaults that keep you from needing to dive into bsts immediately. Basically, you fit a model to the first part of your data, then forecast the rest. You see where your model deviates from the forecast. Using bayesian models means you can get error bounds - so you can have a degree of confidence in the deviation. In your case, you'd set your 'intervention' point to wherever timestamp you want to separate your modeling data from your forecasting data. Then compare the forecast to the actual data (look up 'nowcasting'). Here is a tutorial to get you started, and here's an introductory video. 

A loss function and cost function are the same thing. As you intuit, classical regression treats loss/cost as symmetric, which is not always what you want. In classification tasks, you can make an asymmetric loss matrix. You can do a similar thing with regression if you solve it with gradient descent, but the ordinary least squares has symmetric loss baked in. So I would consider either (1) using a numeric optimization library like sklearn or tensorflow to explicitly define the regression parameters you want to estimate, write your own custom loss function, and then do parameter estimation via gradient descent, or (2) finding a software package that allows for asymmetric loss, for example see this discussion. 

Feature selection is a very well established field in Machine Learning. The objective of feature selection algorithms is to select a subset of your feature set in order to maximize your system's prediction performance. There are two kind of feature selection approaches: Filter methods: filter methods select features without taking into consideration any specific prediction algorithm. They are based in applying different kinds of measures (like mutual information or correlation) in order to evaluate the relative information that a feature or a set of features can provide about the output variable. Filter methods are faster than wrapping methods, but do not perform as well as wrapping methods. Wrapper methods: these methods evaluate features or sets of features by using a specific classification/regression algorithm. I'd suggest to read one of the seminal papers in the feature selection field as a starting point in order to get to know the basics: Guyon, Isabelle; Elisseeff, André (2003). "An Introduction to Variable and Feature Selection". JMLR 

As usual in these cases, there is no magic wand to determine which splitting method to use. It all depends on your specific data. Is your collected data redundant enough so that k-fold cross validation is not necessary? Does your data capture most of the input space? Now, taking a look at your numbers, I'd say that the number of observations you have (145) is not likely to be large enough to capture all the potential variability in the input space, giving the fact that you have a high number of features (22). This conclusion, of course, depends on the type of the features (are they binary? categorical? numerical?), and whether all these features are actually necessary to make a prediction (are there redundant/correlated features? features that do not give any information about the output variables?). In your case, and not knowing more about the data, I'd go for a cross-validation splitting scheme. 

As you said, stationary just means the model's statistics don't change over time ('locally' stationary). ARIMA models are essentially regression models where you use the past N values as input to linear regression to prediction the N+1st value. (At least, that's what the AR part does). When you learn the model you're learning the regression coefficients. If you have a time series where you learn the relationship between the past N points and the next point, and then you apply that to a different set of N points to predict the next value, you are implicitly assuming that the same relationship holds between the N predictor points and the following N+1st point you're trying to predict. That's stationarity. If you separated your training set into two intervals and trained on them separately, and got two very different models - what would you conclude from that? Do you think you would feel confident applying those models to predict new data? Which one would you use? These issues arise if the data is 'non-stationary'. My take on RNNs is this - you are still learning a pattern one segment of a time series, and you still want to apply it to another part of the time series to get predictions. The model learns a simplified representation of the time series - and if that representation applies on the training set but not in the test set, it won't perform well. However, unlike ARIMA, RNNs are capable of learning nonlinearities, and specialized nodes like LSTM nodes are even better at this. In particular, LSTMs and GRUs are very good at learning long-term dependencies. See for example this blog post. Effectively this means that what is meant by 'stationarity' is less brittle with RNNs, so it's somewhat less of a concern. To be able to learn long term dependencies, however, you need LOTS of data to train on. 

The fact is that you could use any of the algorithms you mentioned, and in general any algorithm that requires to set the number of clusters as a parameter (or any other parameter that indirectly sets the final number of clusters, like the threshold in a hierarchical clustering algorithm.) The solution to your problem is model selection. Model selection methods evaluate different clustering solutions, and select the one that optimizes a given criterion. For instance, in the case of K-means, you could find a solution for a range of k values, and keep the one that maximizes any cluster validation measure (see the Wikipedia entry for cluster analysis to read about some examples of cluster validation measures). There are automatic and more complex approaches (one specific example is "Automatic Cluster Number Selection Using a Split and Merge K-Means Approach" by Muhr, M. and Granitzer, M., but this is just an example). These methods use cluster validation measures to automatically split or merge clusters, but the idea is basically the same. 

I'd suggest to use ARIMA (autoregressive iterate moving average) as a way to detect the regularities in your time series. Take a look to the following link, in which you will be introduced to ARIMA by means of series of blog posts: $URL$