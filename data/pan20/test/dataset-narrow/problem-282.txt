Evaluating The First Respondent Table Given the assumed functional dependencies, the first table is indeed in 1NF, and thus an R-Table, as it meets the rules laid out in the background. However, realize that you really must look at the data to really determine if those rules are being met. For example, we may assume that each column contains a single value, but upon inspection of the data find that some column has been populated with multiple values separated by commas or pipes. An even more insidious violation of normalization may be using the order of rows or columns to encode additional meaning, for example the arrival order of survey responses. For this example let us assume none of this to be the case and the table is normalized and thus an R-Table. The R-Table is also in 2NF as there are no partial key dependencies. The T-Table is clearly not in 3NF, as there are transitive dependencies. Each of the 4 DESC columns functionally dependent on the CODE columns, which in turn functionally depend on the RESPONDENT_ID column. Evaluating The Second Respondent Table Given the assume functional dependencies, the second table is not in 3NF. It is not even normalized (in 1NF) and thus not an R-Table. The RESPONDENT_ID is claimed to be the PK but there is also a composite candidate key on the RESPONDENT_ID plus the RESPONDENT_TYPE. This is because there must now be multiple rows per respondent, each with a different type and that type's corresponding code. The 3NF violation with a transitive dependency between the RESPONDENT_ID and RESPONDENT_TYPE functionally determining RESPONDENT_CODE and then RESPONDENT_TYPE and RESPONDENT_CODE functionally determining RESPONDENT_DESC is still present. Conclusion Many designers often assume that taking a specific data element, such as a categorical domain for age, and breaking it into two data elements which are a name value pair is performing "normalization." It is not. It is instead choosing a more generalized vs. more specific design. Normalization is a scientific process with clear rules. An R-Table in BCNF clearly has less redundancy that one on 2NF. The BCNF R-Table is clearly better with respect to avoiding redundancy. Deciding between specialization vs. generalization however is not a scientific process. So how to know which is better? Well it depends on the situation. The advantage to generalization is that it is more flexible for future considerations only in that the data structure doesn't have to change when more data points are added, but comes at a cost of more complexity. The advantage to specialization is it is simpler and enables the DBMS to constraint the domain of each specific column, but it does require additional columns to be added when new data points are added. Personally I prefer to do things like Einstein said "as simple as possible but no simpler." With respect to the two tables you've presented, I prefer the first design (over a corrected second design) but again not because it is more or less normalized. It is simply more clear what is being represented. The only change from a normalization perspective I would make would be to place each CODE --> DESC combination into its on R-Table so as to achieve full normalization. If you want to be sure to avoid the anomalies associated with R-Tables that are less than fully normalized you must ensure full normalization. References A good primer on normalization (and on data management fundamentals in general) is Fabian Pascal's Practical Database Foundation Series. Paper number one describes business modeling (where dependencies are determined) and paper number two which describes normalization. Chris Date's Database Design and Relational Theory provides an in-depth look at normalization along with orthogonality with respect to reducing redundancy in logical database designs. 

The user account that is running the SQL Server agent might not have permissions to use the database mail profile you can check that with EXECUTE msdb.dbo.sysmail_help_profileaccount_sp; But you are having some issues with the agent account security, please try the following steps. 

You can ship the logs to as many sources as you want. Just add a new secondary server on the primary one. The impact will be on the Korean server as it will have to copy the logs from the backup directory and replay them. 

Short of restarting the SQL Server service or the box this is enough. If you want to be absolutely fresh you might want to drop user created statistics from your last run. 

sp_depends will give you list of dependent objects in all current versions of SQL server but is listed as deprecated. Here is a MSDN article with a way to find objects referencing a function and with small modifications you can get the list of object referenced by a function: 

For most parts I'm referencing Paul Randall's Inside the storage engine blog series. The only way to reclaim unused space from database files in SQLServer is using the DBCC SHRINK command which reallocates data within the database files freeing pages and after removing them from the Global Allcation map removes them from the database file. This operation is slow, creates fragmentation within the database and is even slower when dealing with LOB pages as those are stored as linked lists within the database files. Since you are dropping the NTEXT column you will have to wait for the ghost cleanup process to drop the data before shrinking. Now having lots of free space in the database files will actually do you no harm, if you have the disk space, backup compression will take care of the free space within the files. If you absolutely want to make the files smaller you can create a new filegroup with the database and make it the default and then move the tables into the new filegroup but this can take time and cause downtime. I have used the technique explained here by Bob Pusateri with good results. 

Each employee may be assigned to one or more departments. Each department may be composed of one or more employees. Each department may be operating in one or more locations since some point in time. Each location may be an address of one or more departments since some point in time. Each employee must be found in one and only one location which is by definition a location that is an address of a department the employee is assigned to. Each location may be an address of one or more employees. 

These rules imply that each employee has a role that requires they work either only for a bureau, an office, or a division, or requires they work for a given bureau and many offices, or an office and many divisions, or a division, and that only employees who work for a division but are not division staff run projects. The schema shown however asserts only that employees work for a division in any given role and that any of them can run a project. Changes are needed to bring the schema into line with rules. The simplest way to address the business rules is to add tables to associate employees to offices and bureaus in addition to divisions. Each employee would then be associated to each organization level in which they work. Employees working only at a bureau, or office, or division would only be associated to that level. Employees working at a bureau who also work at many offices would be associated to the bureau in that associative table and then also associated to the many offices by that associative table. Note this approach only addresses where each employee actually works and not the rules as to which roles can work at which organizational levels. To address the rules more discovery and modeling is required and is beyond the scope of this answer. Secondly, the business rules imply that an employee has a single role - say a CIO or office manager or division chief - and not a role per associated location. The schema can be brought into agreement with this rule, if correct, by making the role table a parent of the employee table such that each employee is assigned a single role. Now this implication may or may not be true. Perhaps an employee can have only a single role at a moment in time, but many over time. In this case temporal columns are also required. Perhaps an employee really can have many roles at the same time at different organization levels, in which case the role would not be determined by the employee. Ultimately, more clarity is needed on the full scope of the business rules by having the subject matter expert review the model and determine if it represents accurately the rules by which they operate. It is vital to fully understand and document the functional dependencies and natural keys in order to arrive at the correct logical design. Right now the diagram does not show any natural keys so even if the functional dependencies were presented it would not be possible to determine if the logical design is in agreement with them. Finally, the diagram shows that each parent entity is optional and I doubt that is really the case. A business SME can determine under what circumstances the parent occurrence is optional, and for each of those circumstances more work is needed to determine why the parent occurrence is optional and resolve it so it is no longer optional. I hope these points help in improving the design to meet all the informational objectives. 

The rollback time is highly dependent on how busy the server is but will take at least the same time as the elapsed time, safe bet is elapsed time x 1.5 

By default the Full-Text Engine uses automatic population on a full-text index and changes are tracked as data is modified in the base table, and the tracked changes are propagated automatically. The full-text index is updated in the background. For incremental population your table must have a column of the timestamp data type. At the end of a population, the Full-Text Engine records the largest timestamp value and will use that for next incremental population Table-Update will use change tracking to maintain a full-text index after its initial full population. The SQL Server will then maintain a record of the rows in the base table or indexed view that have been modified by updates, deletes, or insert and will use this information to populate the Full Text Index. This comes with an overhead. In most cases you are better off using Automatic updates, but if you have a table that's frequently updated and you have a timestamp column a manual Table-Incremental can help. ($URL$ 

Enabling filestream by it self will not have any impact on the databases not using filestream. Queries on filestream data will be done using system cache not buffer cache so you might need to make sure that the SQL server service is not using all availible resources, see $URL$ 

Operating system error 5 is a simple you can always check those errors in the command prompt . Now Volume mount points are sometimes a bother. In Windows server 2000 the calcs.exe had an error and could not set the permissions on the underlying drive and the only way to do it was to mount the volume using a drive letter, set permissions on the root folder and then create a mountpoint. Later versions of windows should be able set the permissions on the mountpoint by using the /M switch so that 

In this example, the PK to Provider Product includes the Product Number provided, and the PK to Feature includes the Feature Number supported for that Product. The Product Number in Provider_Feature is a FK back to both the Provider Product and the Feature. The FK constraints thus prevent inserting a Provider Feature for a Product Feature combination not already instantiated as a Product Feature possibility. This solution assumes that a feature is identified by a Product - that is a feature cannot exist outside the context of a product. That assumption is likely valid as features, in and of them selves, have no meaning with the context of a product. However, if you do want to instantiate features that can included on multiple products, then you simply add a Feature table and make the current Feature table a Product_Feature table. Often database designers assume each table must have its own "identifier." Nothing could be further from the truth. Please see Fabian Pascal's excellent blog post on this topic. Adding a surrogate Key to each table means the natural business rule you are trying to enforce can no longer be enforced declaratively via PK and FK constraints, just as you have found. If you must have a SK for some reason your only option is to enforce the constraint procedurally using a trigger. This option is much more problematic. First, you have to write, debug, test and maintain the trigger. The book Applied Mathematics for Database Professionals provides excellent detail on how difficult it can turn out to be to express constraints using procedural logic in a way that performs adequately. Secondly, triggers in and of them selves can sometimes be performance inhibitors as they execute serially. My recommendation is to use the natural declarative approach. 

Yes, if you are using Peer2Peer replication between the primary and secondary servers which is an enterprise only feature I would personally use Mirroring/AlwaysOn for redundancy between primary and secondary sites as this includes ways to failover without changing the DNS records. Either by adding ';Failover Partner=PartnerServerName;' to the connection string or by using Availability groups with multisite failover Mirroring or AlwaysOn, depending on your SQL Server version - or Peer2Peer replication. Using Mirroring or Availability groups this will in most cases simply by achieved by bringing the server online and the service will automatically send all the changes over and then you can manually failover. 

Will grant the domain\account full permissions on the root of the drive mounted at j:\data, I still haven't leard to do this with and a comment how to do so will be well appriciated. As you have found out creating a subfolder on the mount point makes it possible for you to create an accesslist for that folder but there still seem to be issues with setting permissions on the root folder of the drive using the GUI. Usually I dont bother with the calcs command and if I need to set permissions on the root I'll add a driveletter, set the permissions using the GUI and then remove the drive letter. 

Disabling the service will not impact the cluster configuration. I would disable the node in possible owners as well to make sure that the server will not try to failover. 

It is not stated if this is for the log file or the database files but lets start by trying the log files and if not then try setting the database files to fixed growth: 

Department - Number, Name Employee - Name, SSN, Address, Salary, Sex, DOB Location - ??? Project - Number, Name Dependent - First Name, Sex, DOB, Relation 

A functional dependency is exactly what the term implies - the output of the function is always determined by the input. If for example we have a function f(), and provide variable x, and we always receive output y, then y is functionally dependent on x. You can think of this like a simple graphing function 

and then do your insert or delete. The only other way I can think of would be to design the schema to store the next process id on the row. Something like: 

This design is preferable to a single table with a recursive association as it cleanly separates what are really two entity types - nodes and links. In our case, the products are the nodes, and the product components are the links. While the network design is a common structure, querying it is problematic as when completely filled it is a recursive structure of varying depth. Industrial strength DBMS' such as Oracle and SQL Server have special language elements (Oracle's CONNECT BY and SQL Server's recursive CTE) to aid in making the query declarative. Given you are using File Maker Pro, which I know little about, you may not have such language constructs to help and may have to write procedure code to traverse the network. This issue can be eased however if the network turns out to be of fixed depth - say every product has either no components, or one level of components. Here are some references with regard to network structures in database design: 

I would prefer having a history schema or a second historical database over a linked server any day. It saves license costs is easier to manage and query. You can then also use simpler schema and drop some of the indexes making the database smaller But since you have the enterprise edition you have the third option which is to partition your tables which, when put in place makes it easier to archive the data and querying the old data is transparent to your users and you will not need to make application changes. 

You will have to setup a linked server forwarding the user credentials between the instances. There are step by step instructions here: It boils down to running this on DB_INSTANCE1\DB1 

If you can setup a linked server to the source database server you can simply do a restore from the last backup device containing a full database backup by finding the last backup file like this: 

You are stuck there. There is no option to re-size the files during restore. The backup is made by dumping the data pages directly to the backup device. There is the option of exporting/importing the data using SSIS but that takes time and it would probably be cheaper to buy a new larger hard drive and take the test server down and exchange it's drive. 

And then you can export that certificate from your personal store and import that to the store for the SQL Server account. When you have a certificate installed you can enable encryption in the SQL Server Configuration manager. It's recommended to only use TLS 1.2 but then you need make sure you have updated the SQL Server native client and your SQL Server to the latest update. This is all well explained by SentryOne and in the release announcement