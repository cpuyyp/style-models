In the book Computer Graphics Principles and Practice, they use the term specular reflection when they want to imagine things resembling a mirror and glossy reflection when things like a polished door knob or an orange skin. The charts shows you exactly that. When a material has more specular color, it should have less diffuse color due to the conservation of energy. That is, the sum of the light reflected specularly and light absorbed and emitted in random directions must be less than equal to 100% (the amount of light incident on the surface). Hence when you increase the specular color the material tends to go white or have a slight tint of the color like in metals. Where as glossy surfaces can have more diffuse color but show a specular highlight like the surface of an orange skin. So assuming the CGPP's point of view, we can say in pure specular reflection, the diffuse part is much less than the glossy part. Where as in glossy reflection the diffuse part is usually greater. 

I Recently posted this question on SO but didn't got any response so i thought to post it here since it's somewhat related to Raytracing. I am making a real time ray tracer in OpenGL using Compute Shaders for my project and was following this link as a reference. The link tells to first draw a full screen quad, then store all the individual pixel colors gotten through intersections in a texture and render the texture to the quad. However i was thinking can't we use Frame Buffer Objects to display the texture image instead of rendering the quad and save the over head? Like I save all the colors using ImageStore and GlBindImageTexture in a texture, then attach it to a FBO to display it. And since I won't be using any rendering commands I won't be causing a Feedback loop as in writing and reading the same texture? Here is the snippet 

The missing link between sample locations and the greyscale noise textures is "ordered dithering". Ordered dithering is a list of pixel locations with a "rank" (order) for each pixel. If you have a white background and want to add two black dots, you add them at the locations for the two pixels rank 0 and rank 1. Choosing how to rank the order of the pixels to turn on can vary dramatically with different results. For instance, a bayer matrix is a specific ordering of the points, and blue noise sample points are as well. White noise is just shuffling the points so that they have a random ordering. How we get from this "ordered dithering" (stippling) to the greyscale color noise images is that we divide each point's rank by the number of points to get a value from 0 to 1, and use that as the points greyscale color. That gives you the greyscale noise textures. The problem with converting the concept of "uniform sampling" to this is coming up with a definition of what uniform samples in 2d would look like. Would it just be in scanline order of the pixels? Would vertical stripes be any more or less uniform sampled? Maybe uniform sampling would consist of a course grid followed by a finer grid, and a finer grid, and a finer grid, until all pixels were filled in? It seems open to debate and interpretation. That question would need to be solved before you could assign ranks to the pixels. The problem with converting the concept of "low discrepancy sequences" to this is figuring out how to do a low discrepancy sequence in 2d on a discretized grid where samples are only chosen for places that do not yet already have a sample. I'm betting there are some LDS's that fit this or could be made to fit this but many don't. 

That's what I want to hear about more. Why "non-orthogonal" ? I can't see any problems with making the axes orthogonal? What does making the axes non-orthogonal give us? 2) After the above one has been answered. How is this all tied up to rotational matrices? How can we achieve gimbal lock through the matrices when they only rotate a given point/vector around global axis. For example If i multiply a column vector with a Rotation matrix around X-axis and then with It will rotate around the global X-axis first, then global Y-axis second. So how can I achieve the lock situation using matrices? EDIT:- To make it more clear I've also heard about rotation orders like in the order when Y axis rotates then and rotate with it but when rotates only rotates with it. thus making the axes non-orthogonal. I am assuming that's what it means by non-orthogonal mentioned in the wiki article. Here is a picture. 

Hence there is no need to generate 1 more buffer object for the texture, just pass the same buffer but change the binding points to However do note that you need to use appropriate Memory barrier calls as SSBO reads and writes are incoherent memory accesses. Don't know much about these but you can find more information here 

I can't really answer this without the context so post some links where this is done. What I can tell you is that the general formula for viewport transformation as done by OpenGL is given as (taken from wiki) 

The best I've personally seen is the stuff inigo quillez does, which is used in demoscene stuff. Ray March the terrain, taking larger steps the farther you get from the camera since (usually) detail matters less at a distance (exception = thin walls!). He uses penetration info and other easily gotten metrics to simulate ambient occlusion and other sophisticated lighting techniques. Here's a demo of the stuff in action: $URL$ And here is IQ's page on the terrain raymarching which is a pretty interesting read: $URL$ BTW, in modern games, the technique of "screen space reflection" often is just a ray march against the Z buffer of the rendered scene. The Z buffer is really just a heightfield. I saw some talks on this at siggraph 2014, and while some of the people were using similar techniques as IQ, a few were doing things not even as well as IQ, which was interesting to see :P 

Let's say we are path tracing and that there is an object which has some amount of diffuse reflection, and some amount of specular reflection. What is the best or correct way to handle that in the path tracer? Looking at smallpt ($URL$ it doesn't allow for objects to have both, but instead forces materials to be either diffuse or specular. I've seen other path tracing code which rolls a random number to choose which way to go, using the luminance of the diffuse and specular reflectance as weightings in the random roll. I've also seen code which follows both paths and diverges when this happens and sums the results, where each recursive ray was multiplied by it's corresponding reflectance. I've seen refraction dealt with similarly to all the above. Is there a physically correct way to handle these situations? 

so that these are centered around the Z-axis. In order to generate an oblique projection you can use the above mentioned method which takes input the coordinates for left, right, top, bottom, near and far planes. The other way around is to build your own custom projection matrix, and load it directly. The matrix should be stored as an array 

Next is differential radiance. We can think of it as an infinitesimal quantity of radiance emitted or recieved in a very small solid angle $d\omega$. Next is Irradiance. Irradiance isn't normally associated with a direction. According to Wikipedia it's 

To be honest, terms like these are very confusing as they aren't clear cut and on one side of the border. They are more grayish. I'm gonna tell you how I convinced myself, as I too had this confusion as soon as I read your question. But I managed to convince myself through this argument. First of all we are gonna clear up 4 terms, Radiance, Irradiance, Differential radiance and Differential Irradiance. "Radiance" is what you say associated with a certain direction. To be more formal and according to wikipedia, 

So i searched a lot after this and I think it was my confusion on FrameBuffer Objects. I thought you could use FBO's just like a default FrameBuffer and display the texture image attached to it but you can't. It's only used for offscreen rendering. So while you can use rendering commands to draw something to a "texture image" attached to it, you can't "display the image" by making it default framebuffer or something like that. 

I would say yes with a small asterisk. When generating a perlin noise texture, using multiple of octaves of noise like you are talking about, the point of adding higher octaves (higher frequency lower amplitude) is to add high frequency details to the noise. When making mipmaps of a texture, the point is to remove high frequency content that would cause aliasing at the resolution of that mip map level. So, when making a specific mip level of a multiple octave perlin noise texture, you could find out which frequency would cause aliasing, and just use all the octaves up to (but not including) that point. The result would be that you would have reasonable mips. But, there are some reasons for that small asterisk: 

Note that in your case, instead of clamping, you could just detect when the projection was out of bounds and return false at this point as well. Continuing on, you now have the distance from $A$ to $B$ that the closest point to $P$ is, as the value in $projection$. You calculate that actual point: 

If you are ok with specular being only white, you could put diffuse and specular into RGBA. It looks like that is what unity does for it's deferred rendering. Someone discovering this issue: $URL$ The docs mentioning it: $URL$ 

The intuition is that without the xy term, that equation describes a plane defined by the 3 points involved (all points except $P_{11}$). The xy term is there to correct it to be a bilinear surface, which is defined additionally by the point $P_{11}$. 

If you are talking about an oblique perspective projection in which the line joining the eye and center of the projection plane is not perpendicular to the plane like here (upper left), 

I think you are confusing all the binding targets thingy. From what I see your vertex data is coming from compute shader after some processing and now you want to pass it to the vertex shader. You can create a buffer object once, use it as an SSBO for use in a compute shader then use it as a VBO for use in rendering. i.e 

This is the simple case for just a projective transformation. Consider the vector going through 5 or 6 transformations and in the last comes the projective transformation. If we pre-multiply all these transformation to create a single matrix, you will notice that now when we multiply the vector with this combined transformation matrix the division factor isn't just a simple value. The 4th row of the matrix won't be as in the standard projection matrix. It might have changed due to multiplying all the transformations together. Now when you multiply that 4th row with the 4D vector, you will get your value by which you need to divide now. 

So thanks to joojaa I finally got a hint, and I searched on the net further and found this link which cleared all the doubts and has my answer. Though I am still posting it here as a summary. So anyone reading this and who has similar problem to mine here is what I understood. Suppose we are considering the tait-bryan angle order X-Y-Z that is rotate first along X then Y and finally Z. Also to make it clear we are rotating around the "fixed" axes since a rotation matrix always rotates around the fixed axes. The matrix doesn't know anything about the axes moving or anything.