Keep them on TrueCrypt encrypted disk. They will be stored secure while you don't need them, and you would mount it only once you really need to use them. TrueCrypt creates a file, which then is mounted as virtual disk. Edit: If you need to protect it in a code which is not interactive, you could obviously encrypt the private key and decrypt it you application code. There is a plenty of samples how to decrypt file with Java. 

One thing is for sure - if you use security groups, the filtered traffic never reaches your server, so it reduces the load the server needs to take to process firewall rules. This is important if you talk about DDoS. The security groups seems to have just simple filtering rules, while with iptables you can do really fancy things. But do you need them? Personally I would filter as much as possible with security groups and then make next stage on iptables if you need something more sophisticated than security groups allows. Nothing prevents you also from setting the same filters on both - you would be then double protected ;-) 

It is not what you ask for, but you can actually have everyday a full backup with rsync, while it will upload and take as much storage as normal incremental back up. It is done with hard links. I have done a script that automates that and I use it daily on multiple machines. You can grab it with description here: $URL$ Note - when backup is done it will update with backup directory. This is in case backup was interrupted. 

I have an ESXi host with the following virtual machines: 1 x Active Directory Server 1 x Remote Desktop Services Server 1 x SQL Database Server 1 x Accounting Software Application Server I have a second "blank/empty" ESXi host. In theory, what I wanted to do was simply schedule the VMs to be cloned the second host as a backup. If something happen to the first server, I could simply boot the VMs on the second machine and resume as if the first never went down. In practice, this seems a lot less practical having done quite a few searches here on SF. My main concern is the integrity and consistency of the SQL database... This backup strategy does not seem to be recommended for SQL servers due to unwritten data residing in memory. I suppose I could shutdown the server, clone it, then reboot, but in my perfect world, I'd like to duplicate these VMs at least nightly while still live. What would be the best backup strategy for replicating these particular types of servers to a second ESXi host nightly while they are still live? Consider separate options for a budget of $1,000 and a budget of $10,000. Is there a better possible backup strategy as a whole? 

I installed the SonicWALL Global VPN Client on a MS Server 2008 R2 File and Print server, connected the client to the local SonicWALL, and then installed the local printers on the remote print server and deployed them via Group Policy. This solution will work for legacy printers until I can replace them with "cloud enabled" printers. 

Try the PHP cookbook available here: $URL$ it contains a lot of extra modules as recipes and supports both rhel & debian platform families. FYI, if you use CentOS, the cookbook depends on $URL$ too for the Atomic PHP packages! 

The request conditional should contain an absolute location since you're using the "start of line" anchor (^)! ^traq will not match /traq, ^/traq will. Fix this wherever you use the ^ anchor and you're golden. 

The web server update will occur either asynchronously, whenever the Chef client runs or you can force the run by hand via manual or 

Finally, when testing your cookbooks via Vagrant you can use the local Chef's address to do it! As a helper you can define an function that can server as the tool for your local Chef. Just put in your user's .bashrc 

From there, the options to manage your logs are endless: graylog2/logstash, splunk, php-syslog-ng or a number of other solutions (google is your friend!) 

This will run the command as www-data, launch a login bash shell instantaneously (which means /etc/profile.d/rvm.sh gets loaded) which in turn runs rvm with your specified gemset and commands. Remember to host your code outside of the web server's docroot! 

This can happen almost surely if this is a micro instance you're running. In this case, add a swap file recipe before everything else (try this one: $URL$ and your issues will be over! 

The Atomic repository offers a nice, well thought out, compliant with the standard Redhat naming convention and up-to-date bunch of PHP rpms for both Centos 5 & 6. Their official site is this and installing the repository (not the packages) is as simple as 

I see the policy to force removal of the Downloads "link" in the Start Menu, but is there a way to force Downloads to be present by default? 

I'm running your standard Domain set up - AD with DNS and a RDS server. On the RDS server, I've mapped some drives to a local NAS using the DNS shortname... \\folders\sharedfolder \\folders is a static DNS entry on the AD server. "Randomly" it starts throwing the error "Windows cannot access \\folders" I can: ping folders nslookup folders (connects to local DNS server and returns the proper IP) connect by ip address \\192.168.1.111\sharefolder connect by FQDN \\folders.domain.com\sharedfolder but I cannot use \\folders\sharedfolders again until I restart the RDS server. net view \\folders = System error 53 has occured. Thing is, I restarted the server Saturday afternoon for the problem and now it's Monday morning and right back to it! 

The setup: Plantronics USB CS50 Headset > USB Port of "Thin Client" (Running Win Pro) > (Wireless) RDP to Remote Desktop Server > Eyebeam Softphone Client > VOIP Server > Caller (cell phone or another in-office extension) During a live phone call, the incoming voice (From the caller played through the USB Headset on the remote client computer) is all broken up and barely intelligible. The outgoing voice recorded from the remote client computer and sent to the caller is flawless. Any other audio playback is flawless as well. For example, the caller can call and leave a voicemail on the VOIP server and then the user can play the voicemail back through the headset on the remote client and it's clear as day. Is there anything I can do to improve the audio playback sound quality during a live call? Note: I am currently using "Play on this computer" versus "Play on remote computer" as the server does not have a sound card installed. 

The open file cache is a caching system for metadata operations (file mtime, file existence etc), not for file content, so it helps but not as much as you would expect. Some workarounds you could try are: 

to get your sqlite extension installed and automatically loaded. Note: Atomic offers a lot of updated packages and PHP there is on the 5.3 version. Bear that in mind if you are running 5.3-incompatible applications. 

You can use the command line option in order to define attributes you may want to override the default ones. If you want to directly define the options via text, you can use the input redirection feature of bash like: 

SNI requires both server AND client (web browser + OS) support. For OSes/web browsers not supporting SNI, such as Windows XP or IE6, the browser will be ignoring the SNI SSL header and still request the default SSL certificate of the server which will most probably mismatch with the actual domain you are visiting. To be able to support almost all popular platforms I would suggest obtaining an different IP for each one of the domains you need to supply with SSL support and configure nginx to listen on a separate address for each one of these domains. 

You can use a properly formatted method on the Chef recipes you run on the web server that return the node object for your DB server. Then you can get the aforementioned IPs via node["ipaddress"] Î¿r, if you need the public IP/hostname when running on EC2, node["cloud"]["public_hostname"]/node["cloud"]["public_ipv4"]. For example, after deploying the DB server use something like: 

You could first migrate it to VM Ware ESX with VM Ware Converter - here is how. Then you can move the server to Amazon with VM Import Connector. Disclaimer: I haven't try it. 

The hourly prices gives you flexibility if you need to add or remove capacity to adopt even hourly bursts (eg. peak hours). If you need something running instantly, then usually there are some sort of subscriptions or so. Amazon has reservations, others have monthly packages. But yes, it makes it all even more complicated to calculate. It may get a bit easier though with Cloudorado, which calculates prices for multiple providers. I think that good approach is to have dedicated servers for constant load and cloud for bursts. It is called hybrid hosting, but not many provide it (eg. Rackspase and GoGrid does). But also keep in mind that with some cloud providers, you may really adjust the instance size to your needs (separately define RAM, CPU, storage). This may also bring you some savings if you need resources in uncommon proportions. These are for instance CloudSigma, ElasticHosts or OpSource. 

If you focus on costs, you could try cloudorado, which will calculate price of a server at multiple providers. It won't calculate backups though, but server and transfer. 

You could also use standard ssh mechanisms. The best approach would be if user run on their machine to generate his/her key pair. Then they send you (or id_dsa.pub, depending on chosen algorithm) and you add its content to the on the destination host in the home directory of the user account they should be able to access. There can be more than one key in the file. One per line. And that is all! The same public key (id_rsa.pub) can be used on any number of hosts - it will always identify the user. You can also do it other way round - you run the ssh-keygen and post ~/.ssh/id_rsa (or id_dsa) to the user. And the user saves the file to ~/.ssh/id_rsa. Just need to remember to change permissions to 600 (-rw-------) of that file, otherwise ssh won't accept it. This is obviously less secure, since the private key is being distributed over email probably. It can also be done in PuTTY with PuTTYgen. 

The git action actually triggers notifications if the revision has changed. So add an execute block that runs only on notification, and you're done! 

If you really need to farm out to FastCGI servers instead of nginx + php-fpm boxes, you can try the fair module for nginx found here. The plugin assesses reposponse time from each of the backends and rotates respectively. Note that this will require you to recompile nginx. If you don't want that, make sure at least that you are not using the directive, since you will not get rotated when requesting over a benchmark (since the source IP is always the same) and try (found in nginx >= 1.2.2). More information here. Finally, adjust your criteria for nginx selecting the next server using 

Every action is logged in /var/log/chef/server.log. You can start by examining the file and exporting relevant information. You can also try santoku that supports event hooks on knife events and forward these events on a variety of services (hipchat, logstash etc). 

and see what happens. Also, could you please paste the relevant lines of the access & error log when you request the script? 

It depends on the user you are using to run Rails. By best practice for security, this should be a different user than www-data (which is the user debian-based systems use for apache/nginx owned files). www-data should only own what you export in your static directory. That said, if the files you are generating are static (such as thumbnails) and you are sourcing rvm from /etc/profile.d/rvm.sh you can try running the following: