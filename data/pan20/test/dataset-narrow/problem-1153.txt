Consider the following reconstruction procedure $P(y)$: given $y$, output $x$ such that $\Pr[X = x \mid Y = y]$ is maximized. The probability that this procedure succeeds is $\max_x \Pr[x \mid Y = y]$. This is also $2^{-H_\infty(X | Y = y)}$, where $H_\infty(X \mid Y = y)$ is the min-entropy of the random variable $X$ conditioned on $Y = y$. We know that $H_\infty(X) \leq H_1(X)$, where $H_1(X)$ is the standard Shannon entropy of the random variable $X$. Now we just have to upper bound $H_\infty(X|Y = y)$ in terms of the mutual information $I(X:Y)$. Write $I(X:Y) = H(X) - H(X|Y) = H(X) - \mathbb{E}_y[H(X \mid Y = y)]$. Using the inequality mentioned above, $I(X:Y) \leq H(X) - \mathbb{E}_y[H_\infty(X \mid Y = y)]$, or $\mathbb{E}_y[H_\infty(X \mid Y = y)] \leq H(X) - I(X:Y)$. The probability that the procedure succeeds where $X$ and $Y$ are chosen randomly is $\mathbb{E}_y [2^{-H_\infty(X\mid Y = y)}]$, which by concavity is at least $2^{-\mathbb{E}_y[H_\infty(X \mid Y = y)]}$. Thus the probability the procedure succeeds is at least $2^{I(X:Y) - H(X)}$. This procedure is optimal: given any randomness procedure $P$, the probability of success is $\mathbb{E}_y [ \sum_{x} \Pr(X = x \mid Y = y) \Pr(P(y) = x) ]$, which is maximized point-wise when $P(y)$ deterministically outputs the most likely $x$. 

Short answer: if you believe quantum mechanics is an accurate description of nature, then since QM is a linear theory, it isn't possible to physically realize nonlinear operations. As far as we know, the only nonlinear operation in quantum mechanics is that of measurement, but it's of a very restricted kind: measurement only happens at the end of your computation, and it doesn't allow you to amplify exponentially small amplitudes into non-trivial ones (except with negligible probability). On the other hand, the kinds of nonlinearities discussed by the Abrams and Lloyd paper (as well as others) are generally those that allow you to take a quantum state with exponentially small amplitude, and amplify it to something nontrivial. We don't believe that it is possible to physically realize these nonlinear operations. Why? If it were possible, then you would be able to solve #P problems efficiently! From a complexity point of view, this consequence effectively chucks out the assumption. Being able to solve NP-complete problems efficiently is laughable; to handle #P with ease is a bad joke taken a little too far. The Abrams and Lloyd paper can be understood as an argument against the possibility of nonlinear quantum mechanics. To try to answer the last part of your question: if there existed nonlinear quantum gates like described in the Abrams and Lloyd paper, then PostBQP could certainly be simulated by their nonlinear quantum computer (because PostBQP = PP by a result of Aaronson, and their nonlinear quantum computer can solve #P-complete problems efficiently). 

I remember struggling with this very same question! Ultimately I concluded that the $C$ is just a notational device (it doesn't represent any mathematical operation), just to indicate that, for a specific $\tau$, the Hamiltonian evolution operator $e^{iA\tau t_0/T}$ is performed on $\left|b\right>$, but only when conditioned on $\left|b\right>$ being tensored with $\left|\tau\right>$. If you ignore the $C$'s, I think a calculation will show that you just apply the operator $\sum_{\tau=0}^{T-1} \left|\tau\right>\left<\tau\right|\otimes e^{iA\tau t_{o}/T}$ "as-is". I hope this helps. 

The assumption is that the number of queries is at most logarithmic in the input size ($n$), so $2^q$ is still polynomial in $n$. 

It is known how to construct "lossless" unbalanced bipartite expanders with the following properties: the bipartite graph has $n$ left vertices, $m$ right vertices, left-degree $D$, and for all left-sets $S$ of size up to $\gamma n$, the neighbor set $\Gamma(S)$ has size at least $(1 - \epsilon)D|S|$, and this is achieved for any $m \leq n, \epsilon > 0$, and $D = \Theta(\log(n/m)/\epsilon)$, $\gamma = \Theta(\frac{\epsilon m}{Dn})$. The construction due to Capalbo, et al. achieves these parameters. It's clear that $\gamma \leq m/(Dn)$, but how large can $\gamma$ be? In particular, I'm wondering if it's possible to make $\gamma$ very close to $1/4$, say; this would essentially require one to make $m = n$ and $D = 4$ for any hope of accomplishing this -- $\epsilon$ hasn't even been considered yet! However, does anyone familiar with expander constructions have a sense of what hidden constants lay within the $\Theta$'s? 

I'm currently doing research in pseudorandomness, which involves a zoo of wonderful objects such as pseudorandom generators, randomness extractors, expander graphs, etc. I find it a fascinating topic, but one thing that drives me crazy is the glut of parameters that are involved. I understand that these objects are very complex, but I cannot help but break out into a sweat when I see "Let $G$ be a standard $(\alpha,V,\epsilon^2,k,\delta)$-pseudorandom widget...". Then I have to flip back in the paper or find another paper (which probably uses a different parameterization) and try to remember what all $\alpha,V,\epsilon,k$ and $\delta$ all meant. It takes me quite a while to acquire a feeling for "good" parameter settings versus "bad" parameter settings, versus "natural" settings versus "easy" settings. There's probably no magic bullet for this issue - but I was wondering if other people had some method of managing the "parameter explosion" so that it's easier to retain in memory for a longer period of time? Thanks! 

The title should be pretty self-explanatory, but to be more precise, consider the decision version of factoring, which is given input $(x,k)$, where $x$ and $k$ are binary encodings of integers, to determine whether $x$ has a prime factor less than $k$. Does this decision problem have a statistical zero knowledge proof? 

This is because there isn't really a gap between $\epsilon$-close to having a property $P$ and being $\epsilon$-far to having property $P$. As a crude example, suppose we're testing the property of being the all $0$ string. What's the difference between being $\epsilon$-far and $\epsilon$-close to the $0$ string? You could try to distinguish between being $\epsilon/2$-close vs $\epsilon$-far, because there's a gap. However, in many applications of property testing, we're guaranteed that either the object $x$ has property $P$ or is quite far from having property $P$. For example, in probabilistically checkable proofs, the object is the PCP proof, and it's either satisfying, or far from satisfying, and we want to determine which is the case. Hope this helps. So, short answer: there's an asymmetry because of how property testing is used "in practice". 

Under a hardness assumption, namely, that the complexity class $E = DTIME(2^{O(n)})$ requires circuits of exponential size, suffices to derandomize $MA$, so that $MA = NP$. In fact, the derandomization is to show that $BPP = P$ (see Impagliazzo-Wigderson or Sudan-Trevisan-Vadhan) . But since in $MA$ the verifier is a $BPP$ machine, we can replace it with a deterministic machine. Thus, modulo this hardness assumption, $MA$ should have the exact same PCP characterization as $NP$. The complexity community seems to strongly believe that the hardness assumption is true, as well. Edit: You might also want to take a look at Andy Drucker's Masters Thesis: "A PCP Characterization of $AM$": $URL$ Impagliazzo-Wigderson: $URL$ Sudan-Trevisan-Vadhan: $URL$ 

How does $\epsilon$ compare to $n$? If $\epsilon$ can be $O(1/\sqrt{n})$, then I think we can accomplish what you want. Let $B = \mbox{Supp}(X) - E$. Note that $B$ is given $\epsilon$ probability mass under $X$. Let $\lambda(i,\sigma)\epsilon$ denote the probability mass assigned to strings in $B$ such that the $i$th coordinate has symbol $\sigma$. Suppose $(i,\sigma)$ were a low probability coordinate for some strings in $E$. Let $\delta(i,\sigma)$ denote the probability mass assigned to those strings. Then, by definition, $\frac{\delta(i,\sigma)}{\delta(i,\sigma) + \lambda(i,\sigma)\epsilon} \leq \epsilon$, implying that $\delta(i,\sigma) \leq 2\lambda(i,\sigma)\epsilon^2$. We can discard these low probability strings while only suffering a $\delta(i,\sigma)$ loss in prob. mass to $E$. Continue doing this for all possible bad $(i,\sigma)$, and in the end we only discard at most $\sum_{i,\sigma} \delta(i,\sigma) \leq \sum_{i} \sum_{\sigma} 2\lambda(i,\sigma)\epsilon^2 \leq 2\sum_{i} \epsilon^2 = 2n\epsilon^2$. This uses the fact that for all $i$, $\sum_{\sigma} \lambda(i,\sigma) = 1$. If you wanted $E'$ to have probability mass $1 -\gamma$, then $\epsilon$ needs to be such that $\epsilon + 2n\epsilon^2 \leq \gamma$, or that $\epsilon = O(\gamma/\sqrt{2n})$ suffices. It's not clear to me at the moment whether this dependence on $n$ can be gotten rid of; I'll continue thinking about it. 

This is a great question, Suresh! Our randomness expansion result does not imply any complexity theoretic result. Here's one way to understand the result: we believe that quantum mechanics governs the world, and under this assumption, there are quantum devices that generate genuine, true, information-theoretic randomness. However, imagine that you're mistrustful of these boxes that claim to do this wonky quantum stuff and generate randomness (for some, this may not take too much imagining). You don't want to deal with qubits. All you understand are classical bit strings. Randomness expansion is a protocol where you, as a classical verifier, can interact with a bunch of black boxes (think of them as non-communicating provers), and after running a protocol with these black boxes, you've certified that their outputs contain very high entropy -- if the provers pass. Furthermore, the amount of randomness you started with is much less than the output entropy you certified. In other words, it's an interactive proof for randomness generation. So, the only "derandomization" aspect of it is to argue that the protocol itself requires small startup randomness. But the outcome is very un-derandomized: the output produced by the boxes is true randomness, not pseudorandomness (i.e. no computational assumptions). 

As far as I'm aware, most implementations of pseudorandom number generation in practice use methods such as linear shift feedback registers (LSFRs), or these "Mersenne Twister" algorithms. While they pass lots of (heuristic) statistical tests, there are no theoretical guarantees that they look pseudorandom to, say, all efficiently computable statistical tests. Yet these methods are used indiscriminately in all sorts of applications, ranging from cryptographic protocols to scientific computing to banking (probably). I find it somewhat worrisome that we have little to no guarantee about whether these applications work as intended (because any sort of analysis would likely have assumed true randomness as input). On the other hand, complexity theory and cryptography provide a very rich theory of pseudorandomness, and we even have candidate constructions of pseudorandom generators that would fool ANY efficient statistical test you could come up with, using candidate one way functions. My question is: has this theory made its way into practice? I would hope that for important uses of randomness, such as cryptography or scientific computing, theoretically sound PRGs are used. As an aside, I could find some limited analysis of how well popular algorithms such as quicksort work when using LSFRs as a source of randomness, and apparently they work well. See Karloff and Raghavan's "Randomized algorithms and pseudorandom numbers". 

Information complexity has been a very useful tool in communication complexity, mainly used to lower bound the communication complexity of distributed problems. Is there an analogue of information complexity for query complexity? There are many parallels between query complexity and communication complexity; oftentimes (but not always!) a lower bound in one model gets translated to a lower bound in the other model. Sometimes this translation is quite nontrivial. Is there a notion of information complexity that is useful for lower bounding the query complexity of problems? A first pass seems to indicate that information complexity is not very useful; for example, the query complexity of computing the OR of $N$ bits is $\Omega(N)$ for randomized algorithms and $\Omega(\sqrt{N})$ for quantum algorithms, whereas the most straightforward adaption of the notion of information complexity indicates that the information learned by any query algorithm is at most $O(\log N)$ (because the algorithm stops when it sees the first $1$ in the input). 

IP=PSPACE is listed as the canonical example of a non-relativizing result, and the proof for this is that there exists an oracle $O$ such that ${\sf coNP}^O \not\subseteq {\sf IP}^O$, while ${\sf coNP}^O \subseteq {\sf PSPACE}^O$ for all oracles $O$. However, I've seen only a few people give a "direct" explanation for why the ${\sf IP} = {\sf PSPACE}$ result does not relativize, and the usual answer is "arithmetization". Upon inspection of the proof of IP=PSPACE, that answer isn't false, but it isn't satisfactory to me. It seems that the "real" reason traces itself back to the proof that the problem TQBF - true quantified boolean formula - is complete for PSPACE; to prove that, you need to show that you can encode configurations of a PSPACE machine in a polynomial-sized format, and (this seems to be the non-relativizing part) you can encode "correct" transitions between configurations in a polynomial-sized boolean formula - this uses a Cook-Levin-style step. The intuition that I've developed is that non-relativizing results are ones that poke around with the nitty gritty of Turing Machines, and the step where TQBF is shown to be complete for PSPACE is where this poking around happens - and the arithmetization step could've only happened because you had an explicit boolean formula to arithmetize. This appears to me to be the fundamental reason that IP=PSPACE is non-relativizing; and the folklore mantra that arithmetization techniques don't relativize seems to be a byproduct of that: the only way to arithmetize something is if you have a boolean formula that encodes something about TMs in the first place! Is there something I'm missing? As a subquestion - does this mean all results that use TQBF in some way also do not relativize? 

A two player game $G = (I,O,V,p)$ is such that, if two non-communicating players Alice and Bob are given questions $(x,y)\in I^2$ drawn from the probability distribution $p$, they are supposed to produce two answers $(a,b) \in O^2$ such that $V(x,y,a,b) = 1$ - in which case they are said to win the game $G$. The maximum probability that Alice and Bob can win the game $G$, over the randomness of the inputs and their strategies, is called the value of the game. The value of the game can change depending on the resources allowed: if Alice and Bob are purely classical players, then the classical value is denoted $\omega(G)$. If Alice and Bob can share quantum entanglement and employ quantum strategies, they can achieve a quantum value $\omega^*(G)$. If Alice and Bob can share general no-signalling strategies, they can achieve a no-signalling value $\omega^{ns}(G)$. It is known that $\omega(G) \leq \omega^*(G) \leq \omega^{ns}(G) \leq 1$. Are there games for which all three inequalities are strict? For example, the famed CHSH game is an example of which two of the inequalities are strict: $\omega(G) < \omega^*(G) < \omega^{ns}(G)$, but $\omega^{ns}(G) = 1$.