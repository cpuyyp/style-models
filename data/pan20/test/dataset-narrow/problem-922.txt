Maybe you could define a new group, just for this sftp user, change group ownership of directory to that group and set setgid bit on the directory? This may or may not work, depending on how the application creates the directories. 

Quick-and-dirty solution: copy /etc/ssh/sshd_config and /etc/ssh/ssh_config from a server where it works. You could also strip all comments from the files on working and non-working server (e.g. run and compare results on both servers. Also look at the directory and file permissions, as Alex suggested. 

So no, you can't do JBOD for data (or the Wiki wasn't updated, Btrfs is under heavy development). I love Btrfs checksuming data and metadata, it will save you from a silent data corruption (well, it will inform you, that your file has been damaged but should prevent metadata corruption). Still, I wouldn't trust it with things I'd like to keep just yet. Personally I'd go with 3 disks and a software RAID 5. You can grow array later as you need. For a filesystem I'd choose something more mature, like ext4 or XFS. If you are more concerned about disks spinning up than about data persistence you could use LVM to create a logical volume out of both disks and make a Btrfs file system on top of that. LVM allows for a linear mode which may or may not cause just relevant drive to be spun up (if both data and metadata resides on the same spindle), but in case of a drive failure you loose all data. 

From what you describe it should work. If you can ping gateway's interface on the same network, but cannot ping any other interface, that's mightily strange. TCP/IP stack should respond correctly to ping to interface A even if received over interface B. As you are pinging gateway's local address (though on a different network), no packet forwarding is involved. I see three possible reasons: 1) The gateway didn't actually receive the packet (routing/filetering problem on the client). 2) The gateway didn't send the reply to the correct place. 3) The gateway chose not to reply (Some kind of firewall. I would double check that, just to be sure it's not the case). I would use tcpdump or wireshark to make sure what's going on on the wire. You should see ping requests leaving the client. Then on the gateway's interface you should see them arriving. Then I'd listen on all interfaces to see if any reply is sent anywhere. If you see a ping request coming in, then routing is OK on the client side. If you don't see ping reply going out of any interface, then it's either firewall or some weirdness happening to routing tables on the gateway. Finally if gateway sends the reply over correct interface and it doesn't register on the client, it's all client's fault. I would try a device on some other network (e.g. 10.1.3.0/24), preferably directly connected to server's NIC to be sure that nothing interferes in communication. It may just be a typo, that's devilishly hard to see if you know what you should see. Configuring another device (or reconfiguring the PC you used for first test) makes it less probable that you make the typo again. And last question -- did it ever work, or is it a new box set up and inserted into the network? Edit: As long as you didn't enable any routing daemon on the Linux box it will ignore router traffic from your switches. What you observe strongly indicates some outside influence (switches, firewalls, aliens or an intern locked in the NOC ;) ). Try to test the setup with two clients connected directly to the Linux box, like this: 

For me the primary benefit would be capturing of oops/crash logs. With the likes of ILO you are loosing whatever rolls out of the screen. Serial console allows you to collect the whole thing and record it without using a camera. 

Display the currently selected initial program load (IPL) type (and logical key mode on some system types). Display the currently selected IPL speed override for the next IPL. 

It depends on the time resolution you need. If the process started more than 24 hours ago you'll have only the date of its start given in ps -ef output. Try looking into the system's logs. There's a good chance that the starting server wrote a line or two into the log. 

A 1. & 2. In VMware you have 3 types of network: Management, VMkernel (IP Storage, i.e. NFS ans/or iSCSI and VMotion) and guests. In ideal world, you keep them separate and each with at least 2 physical interfaces to avoid a SPOF: - Management doesn't need much bandwidth, but you don't want VMs to mess with packets there. - If you keep datastores on NFS / iSCSI then VMkernel will eat bandidth. VMotion too. In ideal world you separate them to prevent VMotion affecting host's access to its datastores. If you don't have datastores on NFS (or they are rarely used, e.g. keep just templates and you don't deploy servers by dozens) it's one network - VMs' network -- kept separate for security reasons. This can have multiple VLANs defined, trunked over physical ports in the virtual switch. If you cannot separate the networks physically (because you don't have enough NICs), it's good practice to have them on different VLANs and IP subnets. A 3. For ESXi cluster to make any sense the datastores (i.e. disk space where your VMs live) should be kept on a shared storage. The file system for datastores (VMFS) is parallel and cluster-aware, so it's not only safe, it's recommended. If any of your physical machines dies, the surviving hosts will restart VMs. They won't be able to do this if VM disk images are on the dead host. A 4. You can define a VLAN, that comes from vSwitch in host A, through physical interfaces, physical switch(es) to host B. If it does not include anything else in the physical world, you now have a "private" network connecting VMs on different hosts. Actually, all VLANs used by any VM should be defined on all physical hosts. That way you enable both VMotion and HA features between hosts. A 5. Reading is good. I'd recommend official VMware documentation -- you won't get any misinformation. I went to some VMware trainings, but they are as good as the trainer. Some just run you by the script, others know a lot or know where to get the answers to your questions. Beside that caffeine, chocolate and pizza help ;) 

For monitoring you may try to use monit -- it should be able to restart a runaway server, if you put it under its control. As a fast-and-dirty solution you may put something like 

I would not mix (software RAID) and (LVM) RAID features. In the spirit of KISS, I'd go with pure with LVM on top for snapshots / resizes. With 4 disks going RAID6 is a Bad Idea (TM). It gives you exactly as much space as RAID 10, but with much, much worse performance (you have to calculate two parities and face read-modify-write penalty for writes smaller than stripe size). RAID 6 gives you marginally beter resilience (any 2 disks can fail, while in RAID 10 one disk from mirror-pair can fail) at a high cost. Not worth it. RAID 10 gives you best performance possible in this setup. 

You have better part of the solution figured out, the only thing left to do is correct permissions for things created by root. There are at least two approaches 

Keep current with bugfixes. Compile and package the upgraded software. Verify that it is stable, does not behave differently than the previous version and generally doesn't break anything in your system. Verify that the upgrade procedure itself works. Distribute to the field. Goto 1. 

Generally you want to give your users minimum privileges they need to do their job. Services do not need shell, so you generally wouldn't give shell access to accounts dedicated to running a daemon. Especially, if a service accessible from network runs as a particular user, it's good idea not to give that user shell access. The reasoning is, that if your service gets compromised, then the attacker won't get shell access to the system. If this user has a shell access, the attacker potentially has one obstacle less to overcome to take over your system. If this user has a equivalent privileges (via ), then if an attacker manages to trick the system to run some command, he can do it with authority. While having a web server run under a uid with shell access is something you can reason about, your setup is very close to running the service as . Bad idea IMO. How do you administer? Either "by hand" or find a tool that does not require you to compromise your systems security. 

Run iometer in your virtual machine. With just two 7.2k rpm drives random access is going to hurt you. You can get only so many iops from them. Try running two scenarios with iometer: 1) sequential read/write -- this should give nice, fat numbers. 2) random access to the drive -- here you should be in for a land of hurt. Setup a file for tests large enough to force it to be pushed out of cache of the virtual machine. 

I'd brute force it. First I'd try if it works with iptables stopped. If it does, then it's something in the iptables. Then I'd add rules one by one and watch which one causes connection failure. Then I'd play with that rule until it did what I wanted without totally disrupting the traffic. If it doesn't work with iptables stopped, then it would start to be really strange. 

There's no such a thing as a Grand Unified Database Layout. If there are custom questionaries, there, really, need to be custom tables. Otherwise you are on a quick path to a single-table-of-200-columns of VARCHAR(128)-with-no-primary-keys monstrosity out of thedailywtf.com, which is inefficient, unsupportable and will hurt you in the future. Sharding, as recommended by toppledwagon may be a thing to consider, but first, double check, that your database is rationally designed. If it is not normalized, then have a very good, preferably backed by testing, reason, why it is not. If it has hundreds of tables, it's probably wrong. If it has single table, it is definitely wrong. Look at the ways you can divide your problem into independent sets. You will spend more effort up front, but the system will be better for it. Million rows, with, let's say, 2k of data per row (which seems a lot of characters for a survey), is 2GB of memory. If you can throw a bit more hardware onto your problem, maybe you'll be able to keep your data set in RAM? Which leads to the next question: What's your load in absolute numbers? Customer requests per second, translated to I/Os per second, divided into reads and writes per second, how many gigabytes of data, with what growth rate? How does your load scale with number of requests? Linearly? Exponentially? You don't have to publish your data, just write it down and think about it. What is it today, how do you think it is going to look in a year or two. Wikipedia says a 15k rpm SAS drive will give you 175-210 IOps. How many do you need in RAID 10 to satisfy your current and projected load? How big is your data set? How many drives do you need to fit your dataset (probably a lot less than to meet the IOs requirement). Would buying a pair (or a dozen) of SSD be justifiable? Is local storage going to be just OK, or are you going to saturate two 8Gb fiber links to a high-end storage subsystem? If currently you need 1k IOps, but have three 10k rpm HDDs in RAID 5, then there's no way your hardware will be able to satisfy your requirements. OTOH if your app has a user request per second and brings a 32 core 256 GB of RAM beast, backed by an enterprise-class storage to its knees, then chances are the problem lies not within hardware capabilities. 

You can modify the root-side process to call after modifying a file. I do not think it is applicable to your case, because then you'd probably just call and and didn't set up sgid directory. You can modify for the root process. I think it would be better to modify umask just for this process, not for root environment, because sometimes root touches files other users shouldn't mess with, and it's usually better to remember to open access than to close gaps. Therefore, you could add in front of call to the binary that runs as root. 

I would recommend the newest one (6100-06-04-1112), which you can download from Fix Central web site ( $URL$ ). Take a look at e.g. $URL$ to see possible TL update strategies. If you want to determine what you have currently installed, then run . This will tell you for which TLs you have complete filesets, and for which not. The list of filesets missing to form a complete TL run Caveats: make sure your TL download is complete. I had problems with truncated files, which then wedged upgrade process. Also double check that you have no exotic software that wouldn't cooperate with a changed TL. Then see if your firmware, HMC, PowerHA/HACMP is compatible with the new TL. 

I think you do not have a good project definition here. For every (or almost every) product on Windows platform there will be functionally similar (sometimes better, sometimes worse, sometimes just different). You can replace what you have with new software and then the thing will come crashing down around you. Deployment of products providing functionalities you want is easy. It may be, depending on a scale of the thing, a huge, time consuming project and take months to complete, but it is not a big challenge. The much bigger problem will be integration of the whole thing, to make different parts cooperate and understand each other (e.g. single sign on). Then there will be even bigger issue: you are a Microsoft shop, and suddenly you'll need Linux administrators. If you want to do the infrastructure migration internally, then your ability to do business will perform of the ability of your new Linux administrators to do their work. Apart of that you will need to train your users. If you change anything end-user facing, the users will need to be trained. The fact that your new solution is functionally superior to Sharepoint will not prevent your users complaining, that the thing does not work. Faced with different menus they will have problems. If you plan to switch to Linux infrastructure as an offering for your clients, then again you need to bring the expertise internally. Otherwise you won't deliver quality products. Yes, you can replace Windows infrastructure with Linux one. You can support it and have your users use and like it, but it is a huge undertaking.