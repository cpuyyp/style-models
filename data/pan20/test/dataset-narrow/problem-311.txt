If you compare any two authorative references for a single subject they will employ different words even though they convey the same meaning. This is just the nature of English - it has a very rich and overlapping vocabulary, accumulated from many sources over centuries. Looking at the examples listed in the question they all contain the concepts of data, collection and organization. To me they seem to be describing the same thing, even if they describe it in different ways. As used specifically in digital technology the term "database" covers a great many things. An IMS hierarchical database looks very different to a relational database, which is different again to a sharded, clustered JSON document store. Yet all these products use the same word to describe one tier of their offering. Writing a set of words that covers the usage these technologies make of the word "database" would be impossible. Yet all of them allow the collection and organization of data, so meet the definitons listed. 

Of course the function can be replaced by as your examples show. The correlated query may cause performance issues. Is your set of probe values sufficiently small to pre-compute the function values for each? You may be able to use one of the fast-but-wrong queries to reduce the initial sets to a managably small superset of the correct answer(s), which can then be processed by a slow-but-correct algoithm. 

SQL Server stores all the columns for one row together in a single disk page. (It's more complex than this but for and etc. this is more-or-less true.) To retrieve any column's value the whole page is read into RAM. So once you have any column available for a given row, all of the columns for that row are available. There are several buffers / caches involved in responding to a query. One is the great big page bufferpool where pages read from disk are held in memory so execution plans can act on them. And yes, if one query causes a page to be loaded into the bufferpool all subsequent queries will use the same page from the bufferpool and not have to suffer the IO overhead. This is an incidental performance gain, however, and shouldn't be relied upon because SQL Server may choose to evict any page from the bufferpool at any time it needs the space. Another set of buffering occurs when the output from your query gets sent over your connection to your application. This is (mostly) on a per-connetion basis so duplicating the work duplicates the resources required (more or less). Another cost is the optimisations you miss out on when you return all columns. Covering indexes and "INCLUDES" columns are meaningless. Every read will have to retrieve the underlying clustered index (or heap) page. It's a fair bet that SQL Server will not be using specific indexes it might otherwise have chosen because a scan-the-cluster plan is cheaper than an index-and-cluster plan. 

Although Normalization and partitioning both produce a rearrangement of the columns between tables they have very different purposes. Normalization is first considered during logical datamodel design. It is a set of rules which ensure that each entity type has a well-defined primary key and each non-key attribute depends solely and fully upon that primary key. Partitioning comes in during physical database design, when we start to map logical attributes to physical columns and determine the operational characteristics required from the system. Sometimes it is an optimisation added after testing under load because performance was found to be inadequate. It can also play a role in implementing a data retention policy. In partitioning we recognise that a table is made from rows and columns. When we partition we separate some of those rows (or columns) from the others and hold them in a physically different location. Horizontal partitioning is when some rows are stored in one table, and some in another. There could be many sub-tables. A typical example is when currently-active transactional data is separated from old "archive" data. This keeps "hot" data compact, with associated performance improvements. We many be able to make the archive tables read-only, compressed and on cheaper disk, too. As the next step each partition may be moved onto separate hardware. This is commonly know as "sharding." Advantages include being able to use many cheaper boxes rather than one very large, very expensive server, and being able to position a user's data geographically close to her. The cost is increased application complexity. Some DBMS incorporate this ability natively. Vertical partitioning is when some columns are moved to a different table or tables. Similar to horizontal partitioning the motivation is to keep the "hot" table small so access is faster. Say you run an e-marketing company. 99% for the time you need a person's name and email address and nothing else. These will go in one table and all the other stuff which is useful but seldom-used - birthday, golf handicap, PA's phone number etc. - go in a different table. It can also help when the partitions have different update regimes or are owned by different sections of the business. The two tables can have the same primary key column, and corresponding rows could have the same key value. While it is possible to have multiple vertical partitions for a table, and to shard vertically, I've never come across it. Vertical and horizontal partitioning can be mixed. One may choose to keep all closed orders in a single table and open ones in a separate table i.e. two horizontal partitions. For the open orders, order data may be in one vertical partition and fulfilment data in a separate partition. The techniques I've talked about are ways to change the design to improve performance. Scaling is when you change the hardware. One can scale up by buying a bigger box with more RAM, CPU or faster disk, or scale out by moving some of the work onto a different box. Scale up is sometimes called scaling vertically whereas scale out can be called horizontal scaling. While horizontal scaling and sharding have an obvious relationship they are not synonymous. It would be possible to use replication technologies to copy an entire database to another location for use by the users there, thus achieving scale-out, without having to partition any tables. 

may help (or try it the other way around). Refactoring the query as an outer join will likely produce a different execution plan 

Drop all the constraints from one database, so there are no declared foreign keys. Update all the auto-increment values by, say, 100,000,000 or whatever will put the two DBs in distinct ranges. Copy the data into a single DB. Continue. 

First off, it's a bit of a rubbish framework that doesn't acknowledge multiple relationships between entity types! If you have this many foreign keys, chances are you'll have more (or fewer) in the future. The solution @Wil details will allow you achieve this without schema changes. One work-around which may fool your framework would be to define views in the database for each of your contact types and define framework relationship from transaction to the view. For example 

The order-invoice-payment pattern is one commonly seen in accounting. Those tables belong together in the "finance" part of your business model. It is well-established practice to be able to tie incoming money back to a customer request. Your financial auditors will expect to be able to follow that trail and reconcile the rows there with other records. belongs in the "sales" part of your model, along with any other channel you have for bringing in trade. Although a subscription may share a number of foreign keys with an invoice they are distinct and separate concepts with different lifecycles. For example, a subscription may be removed once it has expired but an invoice must be retained for, say, seven years for tax purposes. I would suggest you keep your tables just as you show them in the question. As a subscription is written, automatically generate the corresponding order and invoice to match. This can be done in the application or through a DBMS trigger. There will be a one-to-one relationship between and . The foreign key column should be added to so can remain agnostic to the source of the order. 

If Always On isn't available you could combine a custom ETL process with Change Tracking to ensure it does the minimum of work. This allows your secondary to have a different schema, not just different indexing. The ETL package can be scheduled as often as necessary. Using triggers to fire off Service Broker messages to the secondary can work. I've heard reports of it not scaling well at the very highest transaction loads, however. I implemented Service Broker-like functionality once by have a CLR SP fire a WCF message to an external C# program. It worked well for my use pattern. 

The DMV has column which is the computer at the client end of the connection. You can join that to your posted query through the . 

Each participant needs an account per product since there is no direct translation from e.g. sheet to circle, according to your description. Even from circle to rolled circle there may be wastage during handling so sending out 100 units does not mean 100 units are always returned. It's a lot like owning some dollars and some Yen - you can convert each to the other, but the point-in-time exchange rate fluctuates. Each must be held in its own bank account. You yourself will have an account for each of the five products you deal with. This represents the items sitting on your shelves at any point in time, waiting for further work or to be sold. 

Treat the various processes as sub-types. There will then be entity , which contains all the common attributes, , , etc. for the process-specific attributes. Implement these as a table each. A view which combines them all together may simplify usage 

Query plan generation is complicated. The number of possible plans grows exponentially with the complexity of the query. Each possible plan will be optimal within a small range of data counts and distributions. Change any one of these, however, and another plan becomes optimal. Say you add an index. This can be used for some range of counts and distributions. For a different range the optimiser may chose a plan which does not use the index, so the index may as well not exist. (Indeed, it's causing harm through slower INSERTs.) This is why it is always best to test code against realistic production data. In practice, however, most development occurs against "big enough" data sets with the expectation that the plans seen will be close enough to those produced by full-scale production data that the differences will not matter. This seems to hold in practice, by-and-large. There can be nasty surprises on launch day when it doesn't, however. Of course if your code change is in the procedural application rather than the invoked SQL then most likey this will scale linearly and 30% relative improvement will be seen however many rows are pushed through. The PICASSO tool has helped me visualise the stability of plans with respect to changing statistics. 

It should have the minimum required to do the job required of it. If its task is to run a set of stored procedures, then EXECUTE on those SPs and only those SPs. If its job is to perform management and administration within a database then it must have the necessary CREATE, DROP or whatever rights. If the login is for an architect to explore possible implementation strategies, and there is no risk of sensitive data leakage or unintended down time, then higher instance-level rights may be appropriate. There is no one correct answer. Have separate credentials for each use case and minimize each credential's access. 

So let's think forward a little. The application goes live, the tables are created and everything's lovely. Then, you have a new requirement and the tables must change. How do you code that? There's the CREATEs for environments where the code's never run, plus a bunch of ALTERs for existing environments. Now another requirement comes along. This time you have to migrate data for some reason or other. Now there are the CREATEs (for new environments), the ALTERs (for existing environments) and a whole bunch of DDL and migration logic. But wait - there's more. For really good business reasons you want the column holding the migrated data to have the same name as an existing column. How do you now tell if the database has been migrated or not? No longer do you have a metadata-only lookup. No, now you need a flag or version table to show what DDL has to run at each and every execution. Yeah! You're a great success. Zillions of people want to use your service. Venture capitalists wheeling barrows of money to your door. Third-party organisations are begging to partner with you. The dev team's doubled (nay, trebled!) in size to handle the business opportunities and customisations. Oops. These all have to be coded in the API start-up code. Each. And. Every. One. You no longer have an API. You have the mother of all migration scripts with a fragment of business code tacked on. :sad-face: 

Is Azure DB mature enough? As I understand it, it is SQL Server, but tailored to a cloud environment. Rumour has it it is actually vNext and new code is first deployed on Azure, then released as on-prem CUs. So the question becomes is Microsoft's cloud ready for production, but that will be a common factor in any of the options you're considering so is moot. Currently I work for a small start up. All our environments are on Azure and work well, though our current requirements are quite modest. Previously I've worked as DBA and developer for large multi-nationals. I would have no hesitation suggesting a heavy workload be deployed on an Azure DB. 

I happen to be using SQL Server 2008R2 but I am interested to learn how other dialects would handle this. 

Likely you'll want to hold the answers, too. That would be a suitable place to persist the ordering. 

Yes that is correct. Of course you no longer actually have a many-to-many relationship in your model. You are no longer "representing" the relationship, as you say in your question. You have, in fact, resolved the relationship into an association entity type with two one-to-many relationships. 

The returned and can be used to determine which values to choose from the normalised tables. These can be joined to each other in the normal fashion. Since there are four layers in the hierarchy there can be only four possible queries (service, service/ city, service/ city/ state, service/ city/ state/ country). It would be simple to call the appropriate one like this: 

Recognising that this count is denormalised, and assuming you need it for some purpose that can't be obtained on-the-fly at run time, such as a or , I would suggest you use a trigger on . More specifically, three triggers, one each for update, insert and delete since each will have a different effect on . Note, too, that you may need equivalent triggers on other bar-like tables which also have a foreign key pointing back to , depending on what your specific needs are. In this case a trigger has the advantage of separating "proper" business rules (which should be in a stored procedure) from system optimisation. The former can be maintained by developers without the latter cluttering the code. Secondly, there is less risk that the optimisation code is compromised during a code change. Third, future SPs cannot forget to implement this optimisation or choose not to implement it. The trigger is there for all usage of the table. If there is a case when the count should not be updated that can be made explicit in the trigger code and well commented there for all to see.