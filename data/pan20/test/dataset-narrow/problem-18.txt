You probably know that the BRDF is to calculate the reflected light, from a light source to a camera (In examples a light source and a camera is used, but it does not need to be just that). The property that you are talking about, basically says that when you swap the light source with the camera, it still gives the same result. Lets look at an example I quickly made. 

In short: You should not gamma correct your glow map. In fact, you should do everything in a linear color space. At the very end, when doing any color grading (which is the very last step), you convert the final image to the right color space. The color space include that gamma correction. In long: Gamma correction is a step in the encoding of images. It is usually done so that we use more bits to store the shadows and less bits to store the highlights, since we see less detail in the highlights anyway. It is also part of some color spaces. These color spaces make sure the image looks the way it is supposed to. It is the final step, often part of color grading. Just before it is send out to the display, or saved to a file. This means that everything before should be in a linear color space. Usually things would look like this: 

Load textures and convert them to a linear space, so you do not double gamma correct.. Render the image. Do the post effects. Convert the image to the final color space with color grading. Probably sRGB Gamma. Converting to the color space includes the gamma correcting. Color grading is not necessary, but optional. 

I get where the confusion comes from, but the explanation meant something else. It meant that when $p1$ and $p2$ are given, they are not always ordered so that $p1$ is always $pMin$ and $p2$ is always $pMax$. So we need to find out. However, it could be that when solely looking at the x-axis, $p1$ is $pMin$, but when looking at the y-axis, $p2$ is $pMin$. So, we need to find $pMin$ and $pMax$ for each of their components, seperately. If we look at a two dimensional example, we can see more easily how it works. We have $p1=(7, 4)$ and $p2=(2, 6)$. 

One thing that I see being done, is that when the ray is generated, the location is shifted on the normal. Basically the normal multiplied by a small amount is added to it. If you are shading the backside of the face, then the normal needs to be inverted. The reason why, is, for example, when a light is behind a face, and you cast the light ray. Without the offset the origin is exactly on the face, so t is 0. Algorithms see this as not a hit, so the offset along the normal, makes it so that t isn't 0 anymore. I hope this answers it. 

This looks normal and logical. You said that you should invert the direction vectors. If we do that, this is then our result. 

After a quick google of 'Inverted perspective' I found out that you have five different names for it; Reverse perspective, inverse perspective, inverted perspective, divergent perspective and Byzantine perspective. $URL$ I think that if you want to use it in a render engine using a projection matrix, in the perspective divide, you use multiply by w instead of divide. This is because in a perspective projection, the divide by w is what actually gives the perspective. In a orthographic projection w is always 1, meaning that there is no actual perspective. For a ray tracer you probably just need to scale the image plane up of course, but probably instead of having the origin of the ray to be the center of the camera, you have some kind of second image plane that is larger than the actual image plane. If both are the same size, the rays are straight and you have an orthographic projection, if the actual image plane is smaller then you have your Reverse perspective. Quick extra note: If you have proper lenses, you might be able to get this effect in real life. Yes, you would need to have a lens that is a few meters in diameter, but I do not think it is impossible! :P I hope this will help you and that I have not made any mistakes. Good luck with whatever you are doing! 

The problem is with how wavefront files and OpenGL work with indices. OpenGL OpenGL allows you to have different buffers, allowing your vertex to have multiple attributes. However, for every vertex in a face, it has only one index. Let us say that we have a quad. It consists out of a position buffer and a uv buffer. The positions would look like this 

Normally woth rotating objects in 3d you have a pivot. With rotation matrices it is assumed that the pivot is at (0;0;0), the origin. This means that if the pivot is not at (0;0;0), you have to move all the points on that object around so it is. This can easily be done by subtracting the location of the pivot and after the transformation adding it back in. So with translating space they mean that you move all the points so that the pivot point is at the origin. All the points then would not be (0;0;0), just moved. Hope that helped you! 

$pMin=(2, 4)$ and $pMax=(7, 6)$ We can see that for $pMin$, we took the lowest value from $p2$ for the x component and the lowest value from $p1$ for the y component. For $pMax$, we took the highest value from $p1$ for the x component and the highest value from $p2$ for the y component. If we would only look at the x component and determined $pMin$ and $pMax$ from that, we would get $pMin=(2, 6)$ and $pMax=(7, 4)$. This works for the x component, but it makes no sense on the y component, as $pMin$ has the highest y value. The bounding boxes are still the same. We still store the same rectangle, we just used different points to store it. When going to the third dimension, it is still exactly the same. You just do the same for the z component as you would do for the x and y components. 

You can see that it now looks like the surface is illuminated from the other side, and since the normal is not inverted, it will end up looking black. So that is where you made your mistakes. The property says that you can swap the incoming and outgoing vectors and the result will still be the same. This is called the Helmholtz reciprocity. And where you might got confused was this part "how a ray of light and its reverse ray encounter matched optical adventures" (from the wikipedia). But it is not talking about the inverse (multipled by -1) but about the outgoing ray of the event in something like a reflection or refraction. I recommend reading up on the Helmholtz reciprocity. For example on the wikipedia: $URL$ I hope I explained this properly, and if I made any mistakes, I am happy to learn and fix this answer. Good luck with your adventures through the computer graphics universe! 

When having everything at linear and doing the gamma correction at the very end, you will need a high precision for the color. Using floating point framebuffers is recommended. When using the default 8-bit framebuffer the whole way, you end up getting banding effects when gamma correcting. This setup is not always done for games, to my knowledge, since most people will barely notice it or even care about it. In the film industry, VFX and animation, using the proper color space is more important. When not doing it the 'right' way, the textures are not converted to linear and at the end, there is no gamma correction. Lights are supposed to have a quadratic falloff to be physically correct, but when gamma correcting it with a gamma of 2.0, it becomes a linear falloff. Lights can therefore get a linear falloff and look normal. As Noah Witherspoon pointed out, when blending and blurring you should use a linear color space, however this is not always done either. The reasons for not doing it the 'right' way, is probably that it still looks fine. You do not need to do it this way, to end up with good graphics. It is always a matter of what the artist likes the most. 

Ok, so here you see the basic setup. You have a light source, a camera and a point on a surface. Wi is the direction towards the light, the incoming direction. Wo is the direction towards the camera, the outgoing direction. N is the normal of the surface. The property says that if we switch Wi and Wo with each other the result is the same. The example would then look like this. 

What do I mean with how I defined those angles? First off all, that both angles are between a common direction and the vector. In this case it means that the angles should be between the vector and the red vector where the red vector is the common direction. Which is the case for the angles you provided. It is difficult for me to properly explain why they need to share a common direction, but it should be self explanatory. What do I mean with that they should be either both clock-wise or both counter clock-wise? When having an angle between two directions, it can either be clock-wise or counter clock-wise. The angle between red and green is 60° and if you start at the red vector (which will be the common direction) you go counter clock-wise with an angle of 60°. The other angle should then also be counter clock-wise. This is the case between red and blue with an angle of -170°. If you look at the red vector, then you go clock-wise. However, the angle is negative, so you go backwards. This means that you actually go counter clock-wise. And lastly, the angles should be in the range from $[0, 360)$. This makes the formula work a bit better. It is not needed, but then you would have to do some stuff with $\angle \phi$ instead. Basically, if the angle is larger than 360° you just keep subtracting 360° until it is within the range. If the angle is smaller than 0° you keep adding 360° until it is within the range. Let us try this method with your example. We have three vectors. $\vec Red$, $\vec Blue$ and $\vec Green$. We now the angles between $\vec Red$ and $\vec Blue$ and between $\vec Red$ and $\vec Green$. $$\angle RB = -170$$ $$\angle RG = 60$$ $\angle RG$ is within our range, but $\angle RB$ is not. So we add 360° to $\angle RB$ and we get. $$\angle RB = 190$$ They also both have a common direction, $\vec Red$ and are both counter clock-wise. When adding them into the formula, we get; $$\angle \phi = |190-60| = 130$$ And there we have it. The answer. I do not know if this is a formula with a name or something like that. This method is something I just came up with. Good luck! 

When OpenGL does it's rendering, it looks at the index for the location of the data. If we have the very first vertex of the very first face, with the index of 0. It then picks the position with the index of 0: . Then it picks the uv with also the index of zero: . Using the same index for each attribute means that all data can be sequentially stored in one long buffer. If OpenGL needs the data for a vertex, it only has to load one long strip of data. This is better for the cache than having to go to different parts of the memory to find all the information for just one vertex. OpenGL uses the same index for all vertex attributes. You cannot tell OpenGL to take position number 3 with uv number 1. Wavefront Wavefront works differently. When doing the indexing, it indexes each attribute separately. Here is a declaration of a face