I have noticed differences in executions between data pump imports lately, and I am trying to determine what is causing the differences. I am using impdp to move data between databases, and remap the schemas. It seems like I have seen two different things happen when I run the import in parallel, and I am not sure what is causing the differences. Sometimes I see the job running and it is importing a bunch of tables side by side, other times the import process is only importing a few tables at a time, but using parallel inserts for each table. Using Parallel inserts on just a few tables at a time seems to cut my overall run time substantially, but I can't figure out why it only runs like that sometimes and not others. 

I have a powershell script that I am trying to add in to a sql agent job, but the job step is failing with the following error 

if you are running the deletes in batches, do you have an index on column that you are using to chunk up the process? IF there is no index and the query is resorting to full table scans to try and find the data that should be deleted, you could be adding a lot of time to the process. Unfortunately backing up the good data then dropping/truncating the table is probably your best bet. You could always rename the existing table and create a new table to do this rather then trying to extract just the data you need to keep. This way if you found your initial load of data was lacking, you still have everything to go back to for a second look. 

Adding the new field Migrating the data from the old field to the new field Wait for a while, perhaps a month Delete the old field 

I am in the process of implementing merge replication in SQL 2012 with web sync. I am wondering two things, 

I am using Merge Replication with SQL 2012. I look in the snapshot directory, but the largest file in there is a prc file which is 646 KB. I know for sure that the biggest of my replicated tables is 25 MB in the database after replicating, so I am not sure I understand why there aren't larger files in the snapshot directory? Also is there a place I can look for the snapshot files as they are downloaded to the subscriber? For instance the merge agent outputs messages such as, 

If more records are added to Table2 I would want the contents of Table1 to be changed accordingly when the sync process next occurs. I found out that this is not working. Is it because when I tried it Table2 was not an article in my merge replication publication, or is it because merge replication doesn't support subqueries like this and recognising when a filter has changed. What it would mean is that the sync process would have to scan each filter, and work out what tables the subquery relates to and figure out whether these have changed. Does it do something like this? UPDATE: I have tried a number of things, but they don't work. 

I currently have a impdp job running for a fairly large schema (2TB) and ran into an error. The source database has one column that is larger then the destination database. This is causing ORA-02374 and ORA-12899 errors. This has caused the import job to come to a crawl on this one table. I am trying to get past the errors, and the best solutions I can come up with are to either fix the table definition, or tell the import job to skip the current table. I can not currently fix the table definition because the data pump job has the table locked. Is there a way to pause the job, make the column modification, and then resume the import? Alternatively is there a way to tell the job to skip the current table, and move on? This would allow to to come back once the job is finished, fix the table definition, and then re-import just the one table. ETA: This is one of the last few tables in the import, I would rather not kill the whole thing and start over. 

Well, it looks like when the first sql server install ran, it also configured the file server services, which were still trying to hold the disk and the network name. Odd thing is, the disks where all showing 0 dependencies. This probably could have been fixed if we opened a case with microsoft, maybe cleaned up some registry entries, etc, but the guys from out windows team wanted to just rebuild from bare metal, as it would be quicker and easier. I was on board with that, so that is the solution I am implementing 

I'd go for option 1. With this approach there are no "hacks" in the app for anonymous users. The standard security model continues to apply with the only difference being that if you don't know who the user is (ie, they haven't authenticated) that you pretend they are the anonymous user, which in effect they are. Option 1 is also the approach used by most (all?) web servers and file servers: they use an "anonymous" or "nobody" user. 

Be careful. Your two designs are not the same. Case 1 implements a unique constraint on the name of the book but includes case, so "The Lord of the Flies" and "the lord of the flies" would be different. Case 1 then creates a second index to support efficient searching of queries of the form 

If using Oracle -- and if you're able to run arbitrary statements against it -- then you could first sanitize the input using (at least, it's my understanding that this makes sure that the literal is properly quoted -- even allowing for quotes embedded in strings). See $URL$ for more details. 

Oracle installs some handy "$ORACLE_HOME setting scripts" in /usr/local/bin (or possibly in another location such as /opt/bin depending on your flavour of Unix/Linux). These are called and . They can be "sourced" (ie its commands executed) in your current shell and will then prompt for $ORACLE_SID and set $ORACLE_HOME based on this value. If there are any Oracle instances this script should default the values so you just need to press enter to each prompt. If you're running bash on a Linux box you would use to do this. See $URL$ 

I am trying to track down a cause for a difference in oracle impdp processing, and I am not finding anything, so I am wondering if anyone here can explain the cause of the differences. At times I have seen times where using the parallel=x parameter in oracle datapump will cause either multiple tables to be inserted at once, up to the value of 'x', or other times will use parallel threads to import a single table. I have not been able to track down what might be causing the difference in performance, and I am wondering if anyone has an explanation, or even just a direction to point me to. It would be helpful to determine why it runs in a given matter. For instance right now I am monitoring a single table, data_only import that was started with parallel=8 in the command, but the import job is only using a single thread to do the import, as shown by the following hint the insert query /+ PARALLEL("XXXXX",1)+/. If the process would use the maximum 8 threads specified to should run much faster. 

I am trying to setup a new sql server 2008 cluster, on windows 2012R2, and the installer is failing on the cluster shaed disk availability check. I have verified that there are 5 disks assigned to "Available Storage" when viewed in the fail over cluster manager. Some background, this is my second attempt to install sql server on this cluster. The first time, the cluster object was unable to create the new computer object during the cluster installation. This caused the installer to fail do to lack of permissions. I have since resolved this, and have run "Remove node from cluster" to uninstall sql server from the node. I am now trying again to run the installer. About the environment, OS: windows 2012R2 SQL Version/Edition: 2008/Enterprise I am running the installer from the current cluster host(node1), and all storage is owned by node1. This includes the quorum, as well as the 5 disks assigned to the available storage group. Both cluster nodes are up and available, and accessible either through the node names, or through the windows cluster name. There are no cluster validation warnings that I know of, but I have asked the windows admin to rerun the validation tool to confirm that, that is still the case. SQL 2008 is required by the front end application (I pushed for at least 2012, but was told it was a no go)