according to Microsoft own announcement So in other words, it is kind of equivalent to encryption and firewall that are seen in communication but this time applied to running programs in a Trusted Execution Environment (TEE), which the term for the area which is protected when server runs a program. 

I consider myself as a fullstack developer on the basis of the following combination of responsibilities: Front end and back end programming I can do UI changes until some extent: write html, css (as a web developer) and in other hand in some extend provide data to the UI from database, provide it in a service etc. I leave testing, architecture and those aside, meeting customers may be added to the working description. Opposite The opposite for my point of view would be strict roles of UI guys and back end guys. Conclusions I don't see full stack really that full as you mentioned, more like a fancy expression like agile or cloud that in certain conditions just have to be mentioned to attract people's attention and the real implementation may vary vastly. At least about scrum and agile I have seen so many variations that the terms have run out of meaning. 

The article you are referencing mentions encrypted RAM but that was only a reference to possibilities of enhancing the technology more in the future, not about the current state of confidential computing. Confidential computing means, that when data is in use these things are prevented: 

You did not tell how big data amounts you tested with, but if you test ok with little data block successfully and larger real situation fails, there may occur same as occurred here: 

I am new to DevOps and I am trying to catch the techniques and their names and meanings. I looked wikipedia explanations on Puppet (a Configuration Management System example) and that of Docker. Docker page says it is categorized as a virtualization platform. Docker manages many virtual containers on same Linux machine. Is there some similar way as with Puppet and other CMS to tell what configuration is set up on container ad-hoc creation as there is in a CMS or is there anything similar on these two (a CMS and Docker and like)? Could they be used only separately or also together? At least there exists a dockerfile to do some kind of similar sounding thing than system description language makes in CMS. That makes me think they perform similar tasks. So in short I would like to how to categorize CMS vs Docker. 

There are tools around the web for saving desktops. Still best solution I found out to be running a set of virtual machines on your computer, having one setup in one vm. That takes some space, but you can save the state of vm upon vm shut down and come back to exactly same state when you come back. Maybe sessions have died but the windows certainly are as you left them. Solution is stated on $URL$ and it is for windows but I suppose the same will apply for Linux vm too. 

This feature is not supported as the issue in the link says. They promised it for late 2016 but now at 2017 the issue is still open. One comment was to look for gitlab CI, which might be my best guess of workaround for now, waiting the Docker Hub version to come. 

In an environment of Secured Virtual Private Clouds, in a Cluster of Docker Containers, we have to setup some routes. We can do that with the Ambassador pattern, which is simple and easy to maintain. (Docker does have some usage of IPTables under the hood - but from what I can see the Ambassador pattern uses socat, and not IPTables to achieve its forwarding. We can setup NAT rules with IPTables to achieve a similar goal. My question is: When would I choose IPTables over an Ambassador pattern for port forwarding? 

I'm trying to wrap my head around the Azure confidential computing offering. It appears that that AWS does not offer encryption at the application level (see diagram for what I mean by this:) 

The benefit of unikernels is managing a large number of applications in a protected space. (Some might say all the benefits of Docker without all the overhead). (Please don't get me wrong - I'm a huge fan of docker and use it 20 times a day in my work - I'm using this question as a a way to explore an idea). The following commentator writes: 

In our work environment we have a standard Corporate Intranet with Active Directory. We've been granted limited access to an AWS VPC. Our connection allows outbound (from the Intranet to the VPC) but not inbound. That is - if we run a webserver in the AWS VPC, then a client in the Corporate Intranet can connect and browse to it. But a client in the AWS VPC cannot connect to a webserver in the Corporate intranet. Note that this 'outbound connections only principle' applies to all ports, not just http port 80. An associate has suggested we need to replicate our Active Directory down to the AWS VPC AD. I think it is not possible to do a one-way replication. My question is: Can you replicate Active Directory from a Corporate Intranet to an AWS VPC where there is an outbound-only link? 

I've been evaluating Netflix Ice - as a billing tool - but it appears to only work at the Machine instance level - not at the container level. I'm looking for a tool to help generate billing reports for using docker containers across a cluster of docker servers on EC2. (Non ECS) My question is: How to track (non ECS) container costs on EC2? EDIT: I have different groups each running their container on the same host. So I want a way to split instance costs by container usage. 

There is a great discussion of the Cattle vs Pets distinction from Randy Bias here. Martin Fowler talks about a SnowFlakeServer. In a series of talks, Adrian Cockcroft talks about how they moved toward Cattle Servers for solving a scalability problem at Netflix. The challenge with this distinction is always managing persistent state. Does it make sense to treat your database servers as Cattle? It does if you (a) manage the state outside of your cattle model (external volumes for your docker containers), or (b) use a distributed database like Cassandra that allows for individual nodes to fail, but still maintain state in the cluster. I get that you can get very close to the 'disposability with persistent state' of Docker containers mounting a shared volume, with launching AMIs to machine instances mounting a shared drive. You can get closer this this idea of scheduled cluster management by having an autoscaling group that recreates machines that you've blown away. To me - the machine instances lack the granularity of a docker container. They gravitate more towards the 'pets' end of the spectrum. My question is: Does the "cattle not pets" distinction apply as equally to machine instances as to containers? 

It depends a lot on what your infrastructure situation is. If you're doing auto-scaling, the health of individual instances is mostly irrelevant. The important metrics are total cost, and cost per unit of work (e.g. per request). Personally I don't like to monitor individual instance state if I can possibly avoid it - I try to focus more on broader service-level and application-level metrics: 

A Chef admin can create, modify and delete other users and clients. Basically an admin can manage access to Chef. 

I don't think there are any popular tools for doing this, because it's not a popular way of monitoring. Anything running completely inside the browser will only execute checks as long as it's open in a browser window, so it's not going to be a popular method of monitoring, full stop. It's just not reliable. That said, it would be pretty easy to hand-roll something like this in a couple hours with simple HTML/CSS/JS. It might not be pretty, but it would meet the stated requirements. 

I'm not 100% sure I understand from the question, but it sounds like the Docker solution would be to go from having an (physical?) appliance with an OS and your app installed on it, to having an appliance with an OS and Docker on it, running a single container with your app in it. That doesn't obviate the need to update the OS in the host, and it adds a layer of complexity (and more updates to contend with, as you'll now have to keep Docker and the OS patched) with no readily-apparent benefit as far as the specific areas mentioned in the question are concerned. However, if you're talking about going from a virtual appliance to a Docker container, that could potentially smooth things out for you, but it also adds Docker as a dependency for your product; you're shutting out anyone who isn't using Docker and doesn't want to add it to their stack just to use your product. You could continue to support those that don't/won't use Docker by continuing to ship the (now "legacy") virtual appliance as before, but now you've just doubled your workload because you have two distributions to support instead of one. 

It really depends on your situation, but I'll go with the generic/naive approach and say do exactly what your developers do in the same situation: I pulled an hour ago, I made some changes, now I want to push my changes, but other changes have been pushed in the meantime. I commit, pull/rebase, then push. Assuming that whatever Jenkins is committing isn't tied to the commit it built from, this is the same process it should follow to get the same result. 

Some of your listed metrics like user registration over time, to me, don't belong in an infrastructure monitoring system like Zabbix. What is anyone watching Zabbing going to do about a 10% drop in registrations? Nothing. This is business reporting data that should be exposed to whoever wants it via a reporting DB, possibly rendered in a nice dashboard because pointy-haired bosses love dashboards. 

One of the things I have the hardest time getting across to people is that the man-hours saved doing repetitive tasks is often only a small part of the value of automation. The bigger part is often hard to measure, and nearly impossible to estimate when automating something for the first time: how it changes the way you work. Talking with developers, this is a lot easier, because they know test automation (unit tests) and it's an easy comparison to make. It's things like: