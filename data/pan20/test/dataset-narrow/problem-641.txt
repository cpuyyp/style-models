Why are your Python lists instead of NumPy arrays? If you're already using NumPy, you might as well use it wherever you can. You probably don't need the loop, do you? Can't you use NumPy array slicing and the matrix capabilities of to replace this loop? If you are going to loop, you don't need to do and then reference . You can do and then reference in your loop code, for example. Write some docstrings for your functions please! 

From then on, every operation on your data should be an operation on a NumPy array instead of on a Python list. The way I wrote it, it assumes your data are integers, and also that 0 can never occur as a real data point. You can modify the and offset of the call to accordingly to meet the requirements of your particular data. This approach will only be good if each user has a number of data points that is not too different from the number for the other users. Other wise representing your data as a full matrix will be memory inefficient. Use s. If your data are non-negative integers, for example, then will be much faster than , for example. Actually, if your data are integers, then you could probably just use to make your histograms in native Python, which could also save time. 

Overall the shape of how I would approach it is very similar to your second attempt. Just a few critiques. Generally is the way to build lists. Secondly by mapping up from prior results you are keeping quite a bit extra of data on the stack. 

Looking through the examples, append is used mainly to keep the double-recursive call in the same order as input. Notice with yours that you flip the remaining to lst. when you hit the bottom of the tree. Using append on average will cause a O(N log(N)) delay. 

This evaluates right-fringes first, That way at most you have at most log(n) number of fringe-helpers on the stack waiting for a return. It's the same as the second try in your link, with some abstract procedures instead of directly using null? pair? etc. 

Using an ADT would help readibility. empty-tree? instead of null? - left instead of car, right instead of cdr. Don't recur twice at the same time call e.g. 

The best case as for time in any complete tree traversal is O(N log(N)) as is yours. The big problem is that it's memory overhead is O(N) 

In the loop, the first call you make to is superfluous and can be removed. You aren't even using the results of the transformation calculation. The next line, where you have is doing all the work. Removing this line halves the number of model fits and speeds things up by ~twofold. You don't need to assign outside/before the loop. If minimization of code lines is the goal, you can avoid assigning to temporary variables and just put the expression that you would have used to define the variable in the code line that uses it. I did that for in the while loop. It is more compact but probably harder to understand. You don't seem to need to full module, so that can be removed. I didn't change anything in my code below, but why are you squaring ? According to the docs this error is the Frobenius norm of the difference matrix (X - WH), so it will always be positive even without squaring. 

The bottom line conclusion is that whether in native Python or NumPy, -based approaches are about twice as fast. Of course: 

Why did you write your own functions for and ? It looks like you are doing fairly standard things with them, so I would use implementations from or instead. (And calling the median of a vector times a "weighted median" of vector makes some sense but it is really just the regular vanilla median of .) Your class seems pretty monolithic. I'd either remove it entirely and just use global-level functions and scripts, or make more classes. For example it looks like could be a class, and maybe even . I agree with all of Ethan Bierlein's comments too. You could make the binary logic statements in the big decison making loop a bit easier to read. For example, instead of duplicating and in every , you could pre-define a before the and then just use that variable instead of repeating the binary logic. You could do something similar with the other repeated logic. In a few spots you are mixing and native Python when you don't have to. For example, does really need or would the python built-in do the job? Or even better yet, if why not make the , , etc. variables numpy arrays? 

The last bit is the hard part to explain. When asked for the in-order permutation of lefts and rights, you can start forming the next permutation with either the car of lefts or the car of rights. In the nested s here the inner is evaluated first because the scheme interpreter does eager evaluation. The value returned by examining the permutations that involve picking the left side is going to be one or more permutations tacked onto the existing known permutation at that point in the calculation. This returned value is used as the accumulator when examining the permutations that involve picking the car of the right side instead. As far as effeciency, this loop executes once for every element in every permutation. Nothing fancy there, but it will do it in a memory stack no deeper than the sum of the length of the inputs. 

This works as any multiple of n still in the partial sieve must also be coprime with all primes less than N. (of which the partial sieve is a complete list) Now while not purely functional instead of keeping the result and reversing it in the end on your second iteration, you could implement it as a tail-modulo cons. While not stickly functional on the inside, it is functional from the outside. Another optimization would be to pass the maximum canidate to the remover of multiples and just return the xs when i is smaller than the last search. 

UPDATE: After a nice comment from the OP with new data, it seems that at larger systems my proposed improvement didn't help at all. With my method, the larger system would require creating a dense 5000-by-2000 matrix. However if I understand the problem then this matrix with 10,000,000 elements will have only 4000 nonzero values. Sparsification would seem to be in order. The question is which of the various sparse formats supported by SciPy would be best. I did some empirical testing and found that for matrices in which every column has two entries at random rows, the "CSR" and "COO" formats seemed to be best. 

I usually dislike when people are sticklers for PEP8 variable-naming conventions in mathematically oriented code, but I think your naming could use some work. For example I had to read multiple times to understand that was not an index but the data. So would have been more natural to me. Plus isn't the best name either. If I understand the code correctly, perhaps would be better? Possible bug: Related to the above, do you really want to create a dictionary keyed on the elements of ? Doing so means that the behavior when integers are repeated in the input is probably not what you want: 

And can thus handle a calculation like though it may take several minutes to do so. ;Value: 76915410 

Now I know I've messed up brackets or parenthesis somewhere, but I hope I've given you and idea on how much cleaner the code can be. And now as a bonus, because I've abstracted how the loop interacts with the data, you are now able to replace the list of lists data structure with a better data structure. Not that it would make a huge difference for a data set this small, but could give you order of magnitude savings on different sorts of problems. 

Results are timings (cpu-time garbage-collection-time real-time) Addendum: I also figure out that my output list was sharing internal list structure where possible. 

If you add a running sum to the formal parameters of , you'll have a truly iterative piece of code. As-is, every time through is called, you leave a "1+ ??" waiting on the stack, which will lead to a recursion depth exceeded error on larger inputs. 

The sub-definition of is what is being asked for in the 1.12 exercise. However it's entirely unsuited for calculating the entire triangle of X rows. Without a proper data structure (list of lists, vector, hash table...) poor has to do all the work (and absent memiozation must to it many more times than is really necessary) Not having the data structure also forces you to mix the printing logic with the step by step calculation of the triangle, which while clever makes it a bit hard to follow, and difficult to use either separately down the line. My advice is to keep moving along in the book, at which point most of this will be more clear. 

Using the built-in solution is definitely the way to go and is still way faster than even the improved "slow" method. 

This was noted in the comments by @flodel. Identifying slow steps The best way to figure out what lines of the code are slowest is to use a line profiling tool. I like profvis. In general, the slow steps in R are element-wise assignment to vectors/matrices/data.frames, and unvectorized functions. In your case, 

Defining default parameter values is often a good idea. Above, rather than set to 30 in I just set the default number of points in the for . I also renamed that variable because (at least to me) is more descriptive than , but admittedly I don't work very heavily with time series. The error structure you are using right now has no cross-correlations, i.e. the matrix is diagonal. If that will always be the case, it could be much faster to avoid in favor of doing separate, 1d calls to instead. Of course, if you will later use this code with non-diagonal matrices, keeping it the way it is is probably better. I agree with the other answers that you shouldn't keep the data you won't be returning, and also that you should pass in some kind of parameter to offer control over how many points to compute before starting to "keep" the data. 

Compare the sublists and which in are elements 2 and 3, and in are 4 and 6. is only true for lists if they are the same object in memory 

Within the for loop you should assign sum to zero and break whenever is zero as continuing through the loop is pointless (the product of zero and anything is zero). Bonus points if you can assign so the loop it's nested in doesn't waste time with a bunch of sums that will also turn out to be zero 

As noted above append gets expensive, with time and memory constraints liners to the size of the first argument. Actually recurring on each cdr of the list makes the overall function n^2 when the input is a list of lists. The other thing to keep in mind is that list? also takes time linear time to complete. (resulting in another n^2 time constraint.) Alternatively pair? runs in constant time but doesn't protect/guard againstimproper list. (however when you are worried about improper input you should just go ahead and check it at the beginning of a function. ) You can reverse a list by folding over it as well.