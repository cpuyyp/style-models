There is a nice software which generates small planar graphs with respect to isomorphism which might help. As I see one of the problem was to generate non-isomorphic planar graphs and most of those planar graphs (on less than 15 vertices) are of small treewidth. For checking whether their treewidth is smaller than given value $k$, one way is to use heuristic algorithms to speed up this calculation, just in the case that exact algorithms are not practical. e.g in a planar graph $G$ first we can find a diameter of $G$ and the corresponding path $P$ of length $d$ (which is a diameter). Then find a vertex $v\in P$ which has shortest longest distance ($l$) to any other vertex $u\in G\setminus P$ among all vertices $w\in P$. The treewidth of $G$ is at most $d+l$, if this is smaller than $k$ then we are done otherwise either apply some other heuristic algorithms or run the exact algorithm. For less than 3-connected graphs it's also possible to apply heuristics by finding cut vertices and then fixing those vertices and finding the tree width of a remaining graph. But as the number of nodes is small ($15$) if the graph is $4$-connected then the diameter is not big and I think the first heuristic should work there. (I don't know if there is any 5 connected planar graph on at most 15 vertices, but as we know there is no $t$-connected planar graph for $t>5$) As the size of largest obstruction for tree width $k$ is not known we cannot simply guess the upperbound value of treewidth of a given graph $G$. But it seems that at least for planar graphs it should not be so big (one should give a proof for this). 

I think graph minor theorem by Robertson and Seymour was most wonderful theories I ever seen (and partially read it). First of all it's quiet complicated, but base conjectures are not hard and may be everyone working in TCS can guess them. Their extreme effort to prove them was wonderful. In fact after I read some of the papers in that series I understand power of human mind. Also graph minor theorem has a great impact on different fields of TCS. Like graph theory, approximation algorithm, parametrized algorithms, logic, ... 

I think one of such examples is the sparsest cut problem. Uniform sparsest cut problem is solvable on graphs of bounded tree width but weighted sparsest cut problem is not even approximable (better than 17/16) in graphs of bounded treewidth. There are many different variants of sparsest cut problem but one of the well known one is as follows. Given a graph $G=(V,E)$ and a weight function $w:E(G)\rightarrow N$, find an edge cut $E(S,V\setminus S) \subseteq E(G)$ for $S \subset V$ such that $\frac{W(E(S,V\setminus S))}{|S||V\setminus S|}$ is minimized over all possible such cuts. (for $E'\subseteq E(G)$ we have $W(E')=\sum_{e\in E'}w(e)$, also we can simply change the problem definition to decision version). The main ingredient is made of two things: 

If we generalize the game condition then it's equal to Cop-Robber game of path-width. The only relaxation is that robber can move to any vertex $v$ he wants if there is a clean path (no cop along that path) from his current position to $v$. Then the minimum number of cops needed to catch the robber is path-width(G) - $1$. If cops are allowed to see a robber in similar game as I stated, then the minimum number of cops are needed to catch the robber is equal to the tree-width(G) - $1$. In both cases there is a polynomial algorithm to find the robber for a fixed $k$, also for a planar graphs it's possible to approximate the number of cops (and then obtaining corresponding decomposition) in polynomial time. May be you are interested to read more from this lecture notes. 

For undirected graph problem admits a FPT algorithm for any fixed $k$. Robertson and Seymour, Graph Minor XIII. Their algorithm runs in time $2^{2^{2^{2^{O(k)}}}} P(n)$ which means for $k = O(\log\log\log\log n)$ is polynomial. But it's not known whether there is better bound for $k$ or not. In special cases like undirected planar case and recently bounded genus case, running time improved. For directed graph it's NP-Complete even case of 2-disjoint path problem. But in planar graphs is FPT by result of Marx et al., in graphs of bounded genus is at least in XP (not known whether is FPT or not), in directed acyclic graphs is $W[1]$-hard by result of Slivkins, for tournaments is NP-complete (not known if is FPT or even XP, edge disjoint version admits XP algorithm by result of Seymour et al). 

If I understand corectly, it seems that the problem is NPC even if we want to devide it into two parts. Here is a reduction from partition problem. (I didn't check it carefully hope it works). Let say $w_1,\ldots,w_n $ form an instance of the partition problem. Take a graph with $2n$ nodes $v_1,\ldots,v_{2n}$. Assign weight $w_i $ to $v_i $ for $i <n+1$. Assign weight $0 $ to $v_i $ for $i >n$. Connect $v_i$ to $v_{i+1}$ For $i =1,...,n-1$. Connect $v_i$ to $v_{i+1}$ For $i =n+1,...,2n-1$. Connect $v_i$ to $v_{i+n}$ For $i =1,...,n$. Connect $v_i$ to $v_{i+n+1}$ For $i =1,...,n-1$. Connect $v_{i+n}$ to $v_{i+1}$ For $i =1,...,n-1$. If there is a solution to the partition problem then we can find two vertex disjoint paths in the graph corresponding to those partitions. On the other hand clearly if we can devide the graph into two connected sets of same size then we have a solution for partition. Edit1: As Sasho mentioned in comments if the input graph is complete graph then thete is a straightforward reduction from partition problem. I leave previous reduction as it uses sparse graph. Edit2: As asked in comments about other graphs except the complete graph I undeleted this answer, because even though it's easy, it's maybe non trivial. 

I don't think we can get any property close to dual's properties in planar graphs, e.g Babai show that every graph can be embedded in a book with three pages (see Archdeacon's survey Theorem 5.1), so if $p$ is a duality property for books with three pages then $p$ holds for general graphs. 

For example we have an edge $1100\rightarrow 1101$ and an edge $1100\rightarrow 1110$ but there is no edge between $1100$ and $1111$ or there is no edge between $1101$ and $1110$. Find corresponding vertices of $x_i$'s in this DAG ($x_i$ represents an integer $\ell$ and it corresponds to $v_\ell$) and colour those vertices. Call a coloured vertex $v$ a coloured source (or source), if there is no other coloured vertex which reaches $v$. With one BFS (from node 0), we can find all source vertices. Erase all colours (just a colour not vertices) except source colours. Find corresponding vertices of $y_i$'s and colour them with another colour. Start from one of the source vertices and run a BFS, once we find another coloured vertex we output that vertex together with the source, otherwise we mark the visited vertices of the graph and we continue BFS for other sources and over unvisited vertices. It is not hard to show that this algorithm finds a pair if it exists. Time complexity: There are $2^d$ vertices in the graph, every node has at most $d$ outgoing edges so total number of edges in the graph is at most $2^d d$. So the algorithm is $O(n\log n)$. If $d$ is arbitrary, the above algorithm is $O (n+d 2^d)$, and per lowerbound in Ryan Williams answer, it is essentially tight. 

As mentioned, different methods such as branch decomposition (or tree decomposition) give subexponential algorithm which can improve simply by Catalan structure in planar graph to $2^{O(\sqrt{n})}$. Furthermore we can extend those results to graphs of bounded genus, that means even in graphs of bounded genus (and even on excluded minors) the problem is subexponential time solvable as described in Dynamic Programming for Graphs on surfaces. 

Computing tree-width and tree decomposition in undirected graphs is FPT w.r.t width parameter. Many digraph width measures (or their corresponding game) are equivalent to tree-width in undirected graphs. Exact complexity class of many of them are not known but recently shown that DAG-Width is PSPACE-complete. 

And I'm looking for this unpublished paper (how they proved the conjecture for di-planar graphs), or related stuff in this case, actually how to use such a grid (I mean $J_k$). 

In fact $n$ refers to the order of the original graph (they wrote $G^n$) not the graph in the inductive steps, actually this is for the first part of the proof. The idea in the first part of the proof is to find a small set* of vertices of high degree and a small set of vertices of low degree and eliminate them from the original graph so that the resulting graph is almost $d$-regular. So when we eliminate that small set or $C_1$, we have to count the remaining edges w.r.t the original graph minus $C_1$ (not in $G^m$ but in $G^n$). But if the first part does not happen, then we are in the second case, they don't do recursive operation to obtain something which fits for the first part. In the second part of the proof they say we have a big subgraph so that each vertex has a big degree, they obtain that big subgraph with a recursive procedure (it has nothing to do with a first part). I think the paper is not very well written in fact in the part that they wrote: "Either we obtain 10A-regular graph by a) or a $G^{m_2}$ by b) from it." They don't want to transfer to the a) part during recursion if the b) part happened. They simply shrink the graph obtained recursively from the b) part to satisfy the required properties. *: Small in a sense that the remaining graph has some function of $n$ many vertices. 

Minimal cover automata is one of a related stuff. Given a finite language $L$, we can obtain a minimal DFA for $L$. But if we relax requirements of DFA we can find smaller ones. We know that longest word in a finite language $L$ has length $l$. Define DFCA as a DFA which accepts only words in $L$ or possibly words which are longer than $l$. Then this DFCA can has smaller size than DFA for $L$. In practice checking a length of a word is not matter. If we have a smaller DFCA which accepts original ones we simply can reject words with length larger than $l$. There has been some research on this class (introduced at 2001), and e.g there is an $O(n^2)$ algorithm for finding minimal DFCA. An optimal running time algorithm is not known yet. Also there are other aspects of DFA that we can consider them about DFCA. 

Your algorithm is correct and its running time is in $\Theta(n)$ which is exponential to the input size. But we can use simple greedy algorithm to achive $O(\log n)$. Find largest number $i$ such that $i! \le n$. To do so just simply iterate through numbers in $1,\cdots, \log n +1$. And do the same for $n -i!$. 

Output: Find $k$ edge disjoint paths $P_1,\ldots,P_k$ such that $P_i$ connects $s_i$ to $t_i$ and additionally sum of demands of all paths going through any vertex $v$ is at most $c(v)$. We can show the problem is NP-hard even if the vertex cover has size 2: Consider the following instance of a graph and demands. Vertex Sets: $s_1,\ldots,s_k,t_1,\ldots,t_k,u,v$. Edge sets: $(s_i,u),(s_i,v),(v,t_i),(u,t_i)$ for $i\in [k]$. Demands: arbitrary even integers $d_1,\ldots,d_k$. Capacity: $c(u)=c(v)=\Sigma {d_i}/2$. All other vertices have infinite capacity. We want to find paths between sources ($s_i$'s) and terminals ($t_i$'s). It is clear that $u,v$ cover all edges of the graph, also it is easy to see if there is a polynomial time algorithm for this then $2$-Partition problem is in P. 

I don't know if there is a machinery way to do this, but my little personal experience works as follows. I try to provide a polynomial time algorithm for a problem. In those tries usually I can see there are some restricted versions of problem which are polynomial time solvable. I also will understand what part of my algorithm was handwaving for original problem. I can compare this two cases (difference between restricted versions and the general one also the part of an algorithm which was hard to improve). By comparing those two cases usually it's possible to guess how does the problem bottleneck looks like. So we can find a related hard problem. Usually providing an algorithm for a problem is hard job, and needs good knowledge about a problem. After we get this knowledge about the problem we can have many different ideas to tackle problem in different scenarios (not just hardness results). P.S: If your proof relays on a particular problem, I think it's because that problem is very close to your work, so don't blame yourself. 

I think dmanet is one of a good sites for this, it also emails open positions for job and education in CS. But it's good for Europe. 

Very important property : tree-width duality. e.g look at : Tree-width of hyper-graphs and surface duality by Frederic Mazoit, The abstract is as follow: 

Let suppose $d\le \log n$. We can define a DAG $D$ on $2^d$ vertices $v_1,\ldots,v_{2^d}$, we add edge from $v_i$ to $v_j$ in $D$ if the following conditions hold: 

Maybe I misunderstood the question, but it seems it's NPC and this is trivial. Finding hamiltonian cycle in planar graphs of max degree $3$ is NPC. Therefore this problem is also NPC (input: A planar graph of max degree $3$ on $n$ vertices and a cycle on $n$ vertex. 

A graph $G=(V,E)$, with a capacity function on vertices $c:V\rightarrow N$. $k$ source and terminal pairs $(s_1,t_1),\ldots,(s_k,t_k)\subseteq V\times V$. $k$ integers $d_1,\ldots,d_k$ as demands. 

Consider this variation: we want to find number of paths which goes from $(1,1)->(n-2,n)$ in $(n-2)\times n$ solid grid, this can be done in almost $2^{n(n-2)}$ possible ways. Then is simple to convert it to your case. Just go up one step, then go left $n$ steps, again up one step and then right $n$ steps. That path covers all of that $k$ edges, and these are just restricted versions, that means independent to $k$, the upperbound is big : $2^{O(n^2)}$.