I am new to provable security and am working on cryptanalysis of a certificate free signature scheme. Unfortunately, I don't have much knowledge about finding attacks on schemes. It would be very helpful if someone can point me to a survey or other reference to study about cryptanalysis and basically develop some intuition to attack an scheme. 

Consider the following function $$f_s: k \rightarrow \lvert \psi_k \rangle$$ where $s,k$ are bit strings, and $\lvert \psi_k \rangle$ is a $n$-qubit state. Assume the function is a one-to-one mapping. Given only $\big(f, k, \lvert \psi_k \rangle \big) $ is there a way to produce a (zero knowledge) proof $P$ that only verifies that $\lvert \psi_k \rangle$ has been generated from $k$, but not learn any more information about $\lvert \psi_k \rangle$ or $s$? 

However this can only be true if there exists an efficient construction of a trilinear pairing... Is there a feasible construction available for a trilinear mapping or at least is it/has it been used in theory and in proofs? Is there any other way to examine the same? 

Here is a wrong answer: it outputs some vertices that are part of non-simple paths from $s$ to $t$ and that are not a part of any simple path from $s$ to $t$ of length $\le \ell$. The answer might still be relevant to the asker's application, so I'm leaving it here. Here is an algorithm that runs in time $O(|V|+|E|)$ (and actually is faster than this when $\ell$ is small). The algorithm runs a BFS search from $s$ that terminates at depth $\ell$. This BFS gives a set $V_s$ of all vertices reachable from $s$ with a path of length at most $\ell$, and it also computes the distances $dist(s,v)$ for each $v \in V_s$. Then I'd do the same from $t$ and get the set $V_t$ and distances from $t$. Finally, the vertices you're looking for are exactly $V_{solution}=\{ v : v \in V_s \cap V_t, dist(s,v)+dist(t,v) \le \ell \}$. The edges are exactly those edges in $E[V_{solution}]$ ($=(v,u) \in E : u,v \in V_{solution}$). The running time of this algorithm is surely $O(|V|+|E|)$ because it just does two BFSs. But the running time is actually $O(|V_s| + |V_t| + |E[V_s]|+|E[V_t]|)$ which will be much smaller than the size of the graph when the $\ell$-radius neighborhoods of $s$ and $t$ are small. Edit: there's probably a somewhat faster algorithm in practice that does a BFS from $s$ and $t$ of depth only $\ell/2$ rather than $\ell$. This discovers all the paths, and then with a bit of bookkeeping you can find all the vertices. This cuts the running time by a square root for the case of a large random-looking graph when $\ell$ is small. 

An excellent book is Rudiments of $\mu$-calculus. It's not cheap though. You'd be better of learning from survey articles such as 

I would just put the publication on my CV, though in the early stage of your career listing the presentations you've made isn't a bad idea. Cite as a conference paper if the 'book' is the proceedings of the conference. 

The search technique simulated annealing is inspired by the physical process of annealing in metallurgy. Annealing is a heat treatment where the strength and hardness of the substance being treated can change dramatically. Often this involves heating the substance to an extreme temperature and then allowing it to cool slowly. Simulated annealing avoids local minima/maxima in search spaces by incorporating a degree of randomness (the temperature) in the search process. As the search process proceeds, the temperature gradually cools, which means that the amount of randomness in the search decreases. Apparently it is quite an effective search technique. 

You might want to look at the SECD machine. A functional language (though it could be any language) is translated into a series of instructions that manage things such as putting arguments of stacks, "invoking" new functions and so forth, all managed by a simple loop. Recursive calls are never actually invoked. Instead, the instructions of the body of the function being called is placed on the stack to run. A related approach is the CEK machine. These have both been around for a long time, so there's plenty of work out there on them. And of course there are proofs that they work and the procedure for "compiling" a program into SECD instructions is linear in the size of the program (it doesn't have to think about the program). The point of my answer is that there is an automatic procedure for doing what you want. Unfortunately, the transformation won't necessarily be in terms of things that are immediately easy for a programmer to interpret. I think the key is that when you want to iterize a program, you need to store on the stack what the program needs to do when you return from an iterized function call (this is called a continuation). For some functions (such as tail-recursive functions) the continuation is trivial. For others the continuation maybe very complex, especially if you have to encode it yourself. 

Decisional Diffie Hellman It states: given $(g, g^a, g^b, g^c)$ where $g$ is some generator of a cyclic group $\mathbb{G}$, verify if $g^c = g^{ab}$ Under the standard assumptions of hardness of discrete log problem, this problem may also seem hard. However, with bilinear maps this problem is easy and can be verified as $$e(g,g^c) \stackrel{?}{=} e(g^a,g^b)$$ where $e: \mathbb{G} \times \mathbb{G} \rightarrow \mathbb{G}_T$ More about this can be read on The decisional diffie-hellman problem, Boneh'98 or a google lookup on Pairings 

Algorithm Design by Kleinberg Tardos This book helps develop a concrete understanding of how to design good algorithms and talk of their correctness and efficiency. (I studied this in my first year at college, very much readable) For an online copy/lecture notes/reference, (as suggested by Suresh Venkat) go with Jeff Erikson's lecture notes. They are really awesome! 

I only have a (very) introductory knowledge about the Hardness of Approximation and PCP theorem, and I am wondering if it has any specific implications (or can somehow be studied) with Zero Knowledge Proofs? 

I suspect you won't get a closed form solution for the distribution you're looking for. Think of the seemingly easier problem where $k$ is always chosen to be exactly $2$, and where you get the set $\{1\}$ "for free". This problem is just like asking for a closed-form distribution of the cardinality of the connected component in the Erdos-Renyi graph $G(n,s)$ that contains a specific vertex $v_1$. I'm pretty sure that there's no known closed-form description of this distribution, and that in fact the lower order terms are poorly understood, especially around the critical value $s \sim n \log n$. 

Depth-2 TC0 probably can't be PAC learned in subexponential time over the uniform distribution with a random oracle access. I don't know of a reference for this, but here's my reasoning: We know that parity is only barely learnable, in the sense that the class of parity functions is learnable in itself, but once you do just about anything to it (such as adding a bit of random noise), it ceases being learnable. But depth-2 TC0 is strong enough to represent all parity functions and strong enough to represent perturbed versions of parities, so I think it's safe to guess that depth-2 TC0 cannot be PAC learned. However, parities and noisy parities can be learned in polynomial time if we're given a membership oracle. So it might be interesting to check whether depth-2 TC0 can be learned using a membership oracle. I wouldn't be totally surprised if the answer is yes. On the other hand, I doubt that $O(1)$-depth TC0 can be learned with membership queries. It might be good to start with AC0[6] (or even AC0[2]) and go from there. 

This context of this question is Rutten's Universal Coalgebra, used for modelling systems. I'm interested in finding a description of a functor between different types of coalgebras corresponding to finding a certain subcoalgebra. An $F+1$-coalgebra $\langle S, \alpha: S\to F(S)+1\rangle$ can be thought of as a system whose transition shape is given by functor $F$ plus the possibility of an error/termination, given by the $+1$. Assume that $F$ is a so-called Kripke polynomial functor: $F::=Id ~|~ B ~|~ F+F ~|~ F\times F ~|~ F^ A ~|~ \mathcal{P}_\omega F$, thus it preserves pullbacks. A subcoalgebra of $\langle S, \alpha: S\to F(S)+1\rangle$ is coalgebra $\langle S', \alpha': S'\to F(S')+1\rangle$, where $S'\subseteq S$ and $\alpha'$ is $\alpha$ restricted to $S'$, such that its range falls withing $F(S')+1$. I want to find the maximal subcoalgebra of this coalgebra which corresponds to an $F$-coalgebra. In terms of my application, this means I'm looking for the subset of states $S'\subseteq S$ that do not lead to the error state. Clearly, I can take the pullback of the functions $\alpha: S\to F(S)+1$ and $\mathit{inl}:F(S)\to F(S)+1$ to get a set $S_0\subseteq S$ which do not lead to an error in the first step. Iterating this process for functors $F^i+1$ seems to lead to progressively smaller subsets $S_i$ of $S$ each avoiding the error state for $i$ steps. What I'm lacking is a coherent description of the process. Is whether there is a more universal description of this construction in terms of limits or colimits, or at least, some known approaches to the problem? 

I thought about this question once, and I convinced myself with an argument similar to this: Unlike the SVM (kernel), the features are not projected to a different space, instead they are weighted! The weight of the features make them acts like like they would under "gravity" and balances the "space", that way the features that have more weights associated to them actually has more "dominance" over the output (of the activation function). I like to think of it this way- consider a non linear data set with just 2 attributes, now, after the training phase, each attribute has some weight assigned to them. these weights inherently change their "role in the activation function" thus might actually make one of them to shrink in in size relative to the other, now as this happens if you then again look at the points, you might see that the new data does not look as non linear but instead you can actually make a lot of sense of it.. This was a very high level "reasoning" with a lot of "handwaving". But intuitively on a high level this is what I think is happening under the hood. 

One approach for using internet routers etc as a computer was published in a letter in Nature by Albert-Laszlo Barabasi, Vincent W. Freeh, Hawoong Jeong & Jay B. Brockman. They called the idea Parasitic Computing. Their idea is to use the checks performed in the TCP protocols to perform calculations and solve NP-complete problems off-line, albeit on-line. The work has been extended here to a fully functioning virtual machine. Plenty of documentation (in German) and even the source code is provided. The book Silence on the Wire provides a description of how this idea can be extended to use the packets floating around the internet as RAM by sending ill-formed packets containing data which subsequently get bounced back. The amount of RAM is equal to the number of packets sent out times the size of the packets. 

The paper referred to is Ramanujan graphs A. Lubotzky, R. Phillips and P. Sarnak, COMBINATORICA Volume 8, Number 3 (1988), 261-277, DOI: 10.1007/BF02126799. 

In the first of these papers, the structure of regular expressions is treated algebraically and the languages generated are dealt with coalgebraically. These two views are integrated in a bialgebraic setting. A bialgebra is an algebra-coalgebra pair with a suitable distributive law capturing the interplay between the syntactic terms (the regular expressions) and the computational behaviour (languages generated). The basis of this paper is algebra and coalgebra, as treated in computer science under the umbrellas of universal algebra and coalgebra, rather than what one sees in mathematics (groups etc). The second paper uses techniques that come from the more traditional mathematical treatment of algebra (modules etc) and coalgebra, but I'm afraid that I don't know the details. Neither treats Kleene star as an adjunction, as far as I can tell. More generally, there is a lot of work applying category theory to automata instead of regular expressions. A sample of this work includes: 

One approach to proving your conjecture would be to try to use the Szemer√©di regularity lemma, similar to the way the triangle removal lemma is proved (see e.g. here). I don't know if you'll get the right constants from this approach, though. 

Answer to question 1: $\left\lceil \log_2 \binom{M-1}{r-1} \right\rceil$ bits suffice to encode the variables. Proof: Count how many ways there are to choose $y_1,\ldots,y_r$ such that $y_i \ge 0$ and $\sum y_i = M-r$. There are exactly $\binom{M-1}{r-1}$ such ways (see e.g. here). Now, if there are only $k$ possible values for a variable, then $\lceil \log k \rceil$ bits suffice to encode that variable. Therefore, $opt=\left\lceil \log_2 \binom{M-1}{r-1} \right\rceil$ bits suffice to encode our input. Answer to question 2: This is a bit more tricky. The best approach is to check the literature on succinct rank-select or other succinct data structures: I suspect you can match the results for succinct rank-select, so to get something like $opt+o(M)$ space and $O(\log M)$ running times for all operations. If you're interested, tell me in the comments and I'll try to look it up and tell you my best guess on what's possible. You might also want to check out Dodis-Patrascu-Thorup for some ideas. 

Look at Bob Harper's book. It provides loads of declarative formalisms for specifying programming language semantics and so forth. Chapter 6 describes declaratively how to translate concrete into abstract syntax. You could use the same ideas to translate one language into another. Following Harper (and many others), you could write rules defined inductively on the structure of your source syntax, as follows: $\begin{array}{c} a \leadsto e \qquad b \leadsto f \qquad x=foo(a,b,e,f) \\ \hline \mathsf{K}~a~b \leadsto \mathsf{S}~e~f~x \end{array} $ Here $\mathsf{K}$ is a constructor in your source language and $\mathsf{S}$ is in your target language. $foo$ is some function which may compute other required information (it too could be defined as a set of rules). The translation function/relation essentially inductively decomposes your source syntax and produces a term in your target. You can also thread through additional information to help with the translation process. For example, your rule could look like: $\begin{array}{c} E'\vdash a \leadsto e \qquad E''\vdash b \leadsto f \qquad x=foo(E,E',E'',a,b,e,f) \qquad E' = \cdots \qquad E''=\cdots \\ \hline E\vdash \mathsf{K}~a~b \leadsto \mathsf{S}~e~f~x \end{array} $ Here $E$ can store any information that needs to be threaded through. Note that such specifications differ very little from a standard recursive function defined inductively in the program syntax. But they may be easier to read, and they may actually define a relation, rather than a function, by allowing a little nondeterminism.