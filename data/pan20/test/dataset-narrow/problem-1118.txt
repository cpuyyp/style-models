Counting the number of perfect matchings in a bipartite graph is immediately reducible to computing the permanent. Since finding a perfect matching in a non-bipartite graph is in NP, there exists some reduction from non-bipartite graphs to the permanent, but it may involve a nasty polynomial blowup by using Cook's reduction to SAT and then Valiant's theorem to reduce to the permanent. An efficient and natural reduction $f$ from a non-bipartite graph $G$ to a matrix $A = f(G)$ where $\operatorname{perm}(A) = \Phi(G)$ would be useful for an actual implementation to count perfect matchings by using existing, heavily-optimized libraries that compute the permanent. Updated: I added a bounty for an answer including an efficiently-computable function to take an arbitrary graph $G$ to a bipartite graph $H$ with the same number of perfect matchings and no more than $O(n^2)$ vertices. 

Write everything, all the time. In TeX, preferably. Whether you are considering a question or proving a lemma, put it in a digital format as soon as possible. Write the necessary background. Try to keep the thoughts organized in a narrative. Having all of these things in a digital form makes paper-writing much easier, but still a lot of work. In my experience, it helps a lot to start again from scratch. This allows for a clean start to find an improved organization. Also, proofs are always easier the second or third time around. 

This process will eventually terminate. If there is no loop, it will take $n$ steps. If there is a loop, the two-step pointer cannot pass the one-step pointer without a collision, and this occurs before the one-step pointer finishes the loop (which is under $n$ steps). 

All publishable research problems should have these three properties: 1) open. 2) interesting. 3) challenging. For recreational research, you can drop the third condition (or vary it based on your own abilities and energies). There are mountains of problems in combinatorics and graph theory that are wide open, but are not "core" or "fundamental" enough to have lots of people working on them. Frequently, these problems can have algorithmic interpretations. Also, some can be turned into communication complexity problems ("How many bits are required to determine if property X is true?") but these are usually either trivial or very difficult. 

To get things rolling, I have a potential game and would like feedback. Let $k \geq 2$ be an integer and $m$ be an integer at least $3k+1$ with $m \not\equiv 0 {\pmod {k+1}}$. The cycle-power game is the 2P1R game where the provers attempt to convince the verifier that the graph $C_{m}^k$ is $k+1$ colorable. Here, $C_m^k$ is the graph with vertices given by integers modulo $m$ with edges if the mod-$m$ distance is at most $k$. If there is a $k+1$-coloring of $C_m^{k}$, it must be given by choosing an ordering of $\{1,\dots,k\}$ and coloring the numbers $\{0,\dots,m-1\}$ in this order, since each set of sequential $k+1$ integers in $\{0,\dots,m-1\}$ form a clique. Since $m$ is not a multiple of $k+1$, there will be some point where this coloring fails. The verifier either asks for a single vertex from both players, to verify that the colors match, or asks for an edge to verify that the colors are different. I believe this is a good example for two reasons: 

Brendan McKay's nauty (No AUTomorphisms, Yes?) program solves the canonical labeling problem of graphs (simultaneously solving the Graph Isomorphism and Graph Automorphism problems) and has exponential worst-case performance (Miyazaki, 1996). However, it works very quickly for most graphs, especially those with a few automorphisms. Specifically, the algorithm begins by partitioning the vertices by degree, then by the degree between each part. When this process stabilizes, a choice must be made to distinguish a vertex in a non-trivial part, and this leads to the exponential behavior. In most graphs, the depth of this branching procedure is small. 

The solution is a log-space algorithm, using two $O(\log n)$-sized pointers to linked list nodes. Start both at the start of the linked list and perform the following iterative procedure: 

Log-space guarantees polynomial time, since there are at most $2^{O(\log n)} = \operatorname{poly}(n)$ configurations of a given log-space Turing machine. The complete problems of Undirected Reach and Directed Reach (for L and NL, respectively) are very "nice" to think about. Note that your definition of PolyL also gives PolyL = NPolyL, by Savitch's theorem, since $\text{NSPACE}[\log^k n] \subseteq \text{SPACE}[\log^{2k}n]$. When polylog space is concerned, work has been done to consider polylog-space with simultaneous polynomial time, giving the SC hierarchy: $\text{SC}^k = \text{TISP}[\operatorname{poly}(n), \log^k n]$. 

This answer is more of a toy problem than a real research problem. My typical example of a log-space algorithm to give to programmer friends is the following puzzle: 

We can extend this bit by bit, and thus arrive at what a single parse tree for a range concatenation grammar looks like, and then apply the same optimization as Earley grammars do to recover a shared parse forest. First, let's extend it to support concatenation. To do this, we'll move from ranges represented as $(start, end)$ pairs to lists of such ranges - the classic rope data structure. Next, let's extend it to support splitting. This is a single additional member in the node: a list of positions the input rope was split at. The third step is to support multiple arguments. Now, we will store an $n$-tuple of ropes, where $n$ is the predicate's arity. In addition, we need to store split information for each argument, making it a tuple of lists. At this point, a node in our parse tree looks like this: 

However, this does not mean they can be "anywhere freely" (introduced on the RHS), merely that they may "have come from anywhere". Consider the following rule: $A(xyz) → B(xy, yz)$ - here, $B$ takes two overlapping ranges as arguments. re: Question In a context-free grammar, a single parse tree is a recursive data structure, consisting of: 

The usual notation (as used in your original question) of $A(xay) → B(xy)$ can thus be desugared into: $$ A(text) :=\\ let\ (x, a, y) := split(text, 3)\ in\\ B(concatenate(x, y)) \wedge a = ``a" $$ re: Edit 3 

So, I'm actually going to reply to your edits first, then come back to your original question. re: Edit 1 In your first edit, you express some confusion over what a parse emitted by an Earley parser is; to explain that I'll jump into a quick refresher on the difference between parsers for deterministic context-free grammars (such as LR parsers) and an Earley parser (or other parsers, such as GLL or GLR) which can handle nondeterministic context-free grammars. Fundamentally, a parser for a deterministic context-free grammar is one that, for any input, will parse it in at most one way: It can either give a unique parse, or fail to parse. As a result, its output is either nothing, or a valid parse tree. A parser for a nondeterministic context-free grammar, on the other hand, may admit multiple valid parses. Grammars which can actually give rise to such situations are said to be ambiguous, though determining whether a given context-free grammar is ambiguous is undecidable. Because of this, its output is a potentially-empty set of parse trees. This is generally referred to as a "parse forest". The shared parse forest, then, is merely a compact representation of such a set. Because the grammar is still context-free, a rule $R$ matching a subrange of the input $x$ will always result in the same parse forest - regardless of what sequence of rules led to applying $R$ at $x$. As a result, those two parse trees can share the parse forest generated by $R$ at $x$, drastically reducing the space and time requirements. (Another phrasing might be that, as $R(x)$ is a pure function, it can be memoized.) re: Edit 2 As far as the interpretation of the predicate clause, I feel that the usual notation of range concatenation grammars somewhat obscures their nature. Fundamentally, a range concatenation grammar is: 

That's it - a single parse tree of a range concatenation grammar. Now, there are two members of the node that could differ between two parse trees matching the same input. The first is the same as for a CFG: the index of the matching clause. If two clauses can match, then we get two parse trees; one for each. The second is the list of split points. Let's go back to the example rule $A(xay) → B(xy)$ for a minute, and ask it to match "cbabcacb". There are two points it could split at: $(``cb", ``bcacb")$ or $(``cbabc", ``cb")$. Each of these, then, also generates a distinct parse tree - assuming $B$ succeeds. However, the predicates of a range concatenation grammar - like those of a context-free grammar - have bodies that refer only to the inputs to the predicate. They are thus context free (i.e. pure functions), and can be memoized. Thus, one can follow a relatively simple evaluation strategy when parsing: 

This is almost exactly the case. The "almost" comes from the concatenation behavior: If I have the example rule I used above, and I apply it at $A(``cbabc")$, I will arrive at $B(``cbbc")\wedge ``a" = ``a"$. If $B$ is defined as $B(xbby) → \epsilon$, then it will succeed - despite "bb" appearing nowhere in the original input. Thus, terminals match not against ranges, but against concatenations thereof. re: Edit 4 The ranges spanning "anywhere in the original string" is as a result of the concatenation, rather than them being free to span arbitrary parts of the original string. re: Edit 5 I presume you mean where Boullier says (found in the standalone paper (PS)) 

Indeed, while fully homomorphic encryption is very useful for executing code between multiple untrusting parties (see for example, this paper), you need some kind of interaction, when the party that computes the encrypted output sends it to the party that knows the secret key. The notion you're looking for does sound suspiciously close to software obfuscation, for which we proved an impossibility result mentioned above. I also wrote once an informal overview of that paper, that some people may find useful. Given this impossibility result, there are two (non disjoint) ways one can relax the definition: either by restricting the classes of programs/functions one is required to obfuscate, or giving a looser notion of security. The second approach is perhaps possible, and we remark on some weak obfuscation-like notions in our paper. However, note that our attack completely recovers the original source code of some program, no matter how it's obfuscated. So you'd have to somehow make a security definition which trivializes for the case of our counterexamples. The first approach was done for every restricted functionalities (e.g., point functions), but again one has to make sure the class doesn't contain our counterexample, which roughly means it shouldn't contain pseudorandom functions. 

I think the following should answer your questions, even though it's not exactly in the same order. The original formulation of the small set expansion conjecture states that, analogously to the Unique Games Conjecture, for every $\epsilon >0$ there exists $\delta>0$ so that it is NP-hard to determine whether in a graph $G$ it's the "YES" case where there exists a $\delta$-sized set with expansion less than $\epsilon$ or it's the "NO" case where every $\delta$-sized set has expansion at least $1-\epsilon$. The paper of Raghavendra, Steuerer, and Tulsiani $URL$ showed that this is equivalent to the case where $\epsilon = O(\log (1/\delta))$ and in fact the case where in the NO case, for every $\delta' \geq \delta$, sets of size $\delta'$ have at least the same expansion as they would in the "$\epsilon$-noisy Gaussian graph" (see the paper for the precise statement). The reason for the relation $\epsilon = O(\log (1/\delta))$ is because this is the relation between those parameters in the Gaussian noise graph. This result of Raghavendra et al can be thought of as the small-set expansion analog for the paper of Khot, Kindler, Mossell and O'Donnell who showed a similar result for unique games, giving a very precise relation between the parameters $1/\delta$ (which in the unique games setting is known as the alphabet size) and $\epsilon$. The result you mention discussed in my lecture notes is from Section 8 in my paper with Brandao, Harrow, Kelner, Steurer and Zhou ( $URL$ ). What we show there, roughly speaking, is that a graph is a small set expander if and only if the span of eigenvectors corresponding to low eigenvalues of its Laplacian does not contain an "analytically sparse" vector. The intuition is the following: consider the following two extremes: 1) A random vector $w$. In this case, the distribution of entries of $w$ is approximately the Gaussian distribution, and so this satisfies that $\mathbb{E}_i w_i^4 = O(\mathbb{E}_i w_i^2)^2$. 2) A vector $w$ that is the characteristic vector of a set of measure $\delta$ (i.e. it has $1$ in the coordinates belonging to the set, and $0$ in the others). In this case, $\mathbb{E}_i w_i^4 = \delta \gg \delta^2 = (\mathbb{E}_i w_i^2)^2$. Now, roughly speaking, the subspace $W$ corresponding to eigenvalues smaller than $\epsilon$ of the Laplacian corresponds to set which have expansion at most $\epsilon$ in the graph. So, if there exists a $\delta$-sized set with such expansion then there would be a vector $w$ (namely the projection of the characteristic vector of this set to $W$) with $\mathbb{E}_i w_i^4 \gg (\mathbb{E}_i w_i^2)^2$. The other direction (which is more challenging to prove but turns out to be true) is that if there is a vector $w$ with this property then we can also find a set with $o(1)$ measure with not too good expansion. 

A classical example was by Valiant (I don't know the reference but I think this is described in the book of Hoory, Linial and Wigderson on expander graphs). Valiant showed an explicit lower bound (I think that a certain explicit function $f:{0,1}^n\rightarrow {0,1}^n$ doesn't have a circuit of $O(n)$ size and $O(\log n)$ depth - something we're still far from proving) under the assumptions that certain types of graphs, called superconcentrators, don't exist. (This was an asymptotic question, and not about just one graph.) However he later showed that these do exist (and in fact have other uses) 

The issue may be a bit cleaner with $E=Dtime(2^{O(n)})$ and $NE=Ntime(2^{O(n)})$. The easiest way to think about these classes is that they are the same as $P$ and $NP$ but restricted to unary languages. That is, all inputs are of the form $1^k$. That is, the language $L$ is in $E$ if and only if the language $U_L = \{ 1^x : x\in L \}$ is in $P$ (identifying strings with numbers using binary representation), and similarly $NE$ is isomorphic to unary $NP$. So, trying to separate $NE$ from $E$ is just like trying not just to separate $P$ from $NP$, but actually do it using a unary language. No reason it should make your life even conceptually easier.