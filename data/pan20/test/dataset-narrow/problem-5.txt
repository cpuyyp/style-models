GIT_AUTHOR has a value in the freestyle job If I save GIT_AUTHOR to a properties file, I can view the file (with the author inside!) in the workspace of the freestyle job The pipeline job isn't receiving the value, just the actual string . If I check the parameters page for the triggered pipeline build commit and branch have values, author is the string . 

If you can't find a plugin to do what you want, there is a more manual solution. Create multiple upstream jobs that control the parameters and pass them to the downstream deploy job. Then you can restrict each group to only see the relevant upstream job. It will result in semi-duplicate upstream jobs, but will guarantee appropriate separation. 

We use Jenkins and have the Bitbucket plugin installed. We've got multiple repos and use webhooks for lots of jobs coming from our repo. No problems there, the jobs go off without a hitch. Now however we want to start running a couple jobs via webhook from our repo. The Problem The default behavior here is that as soon as we attach a webhook to , commits to either repo will trigger any job with polling. I know we can hardcode in logic in each job to either only build when commits match a certain pattern, or ignore any commits matching a pattern. Given the number of jobs we have that currently use polling this would be insanely unideal to have to code in to make it backwards compatible, not to mention potentially doing this again if we start running unit tests on even more repos in the future. The Ideal Solution I'm hoping I might be missing some way to differentiate between commits made from vs on the Bitbucket side, rather than needing to change anything in each Jenkins job. Ideally, there's some way to change the webhook so each repo gets its own url. Does this, or some other blanket solution, exist? (I will accept "no" as an answer if that's the case). 

Is there an easy way to get a count of how many agents are currently up? I know I can go to to see the agents, but counting them manually is tedious. Is there an API call or something I can run in the script console that just outputs the count for me? 

User types in Slack. This is calling an umbrella job that will parse the parameters to find out what to run. Command is somehow sent to the umbrella jenkins job. Heroku instance, direct integration, w/e. Umbrella job receives as a string. (Perhaps as the value of a single parameter, perhaps pre-split into multiple fields) Umbrella job splits the string if it hasn't already been done, and figures out that the user wants run, using parameters and . Umbrella job does a buildjob to run the downstream job as called, which will Slack back to the user their results. 

It's steps 2-3 that are tripping me up. How can I pass a string (with spaces!) from Slack to my Jenkins job? If my entire premise is wrong I'm also open to having my frame challenged. 

A while ago I had written about how commits were being merged (successfully) into our develop branch in Git, but would later go missing, with no evidence of the missing code other than all the Bitbucket logs proving the merge happened. We just figured out yesterday that what's happening is Jenkins clones our repo, then holds it for up to 40+ minutes while jobs run, then pushes back with . This is essentially overwriting any commits we've made in that 40+ minutes as Jenkins pushes back the branch exactly as it saw it on the clone, and minus our additions. We thought the solution would be to change to 

This is probably 100% user error, but I can't get a pipeline job to use a variable as the channel name in a slackSend step. This uses both the Slack plugin and the Build User Vars Plugin. I'm trying to use to identify which channel to Slack back to (ie, the one starting the build should be the one to get the message). Got halfway there with this article on how to use the build user plugin in a pipeline job, but substituting the variable is tripping me up. The relevant part of the job looks like this; 

Is there a way in Groovy directly or by integrating Jenkins plugins to have a downstream job complete only if it hasn't been run in the last X hours already? Let's say I have unit tests, when green, kick off a job to create an image. I want to put an intermediary step between the two that will check to see when the last iteration of the image creation job ran, and if it was less than say 3 hours ago, NOT run the image creation job. There isn't a way to make this work with cron that I can see, since the upstream job runs at random whenever commits are made, obfuscating any timing control. I can't just schedule the image creation because sometimes we get 10+ commits in an hour, and sometimes there's barely one a day. When that single one of the day does turn green I'd rather immediately create an image for use downstream, rather than waiting as much as 3 hours if I schedule it. I'd prefer not to make it a 1:1 ratio commit to image only because storing them gets tricky in large numbers, and downstream from the image you'd never need them as frequently as the aforementioned 10 per hour. 

There's plenty of questions asking how to return a list of all jobs on your Jenkins instance, or even to get a list of which jobs are currently running, but I have a more specific use-case. I need to see if a single job is already running. As in, return one value if there is 1 builds of the job running, and a different value if 2 or more builds are running. The use case here is that we've automated our image creation to build after successful unit tests. It's set to build a maximum of one image every seven hours, by checking the build time of the last image and cancelling the build if it's been less than 7hrs. This works perfectly for most cases. However, these images take ~20min to build. If it's been over 7 hours and two unit tests finish within less than 20min of each other, the second build doesn't see the first creating, and continues building. We're getting two functionally identical images made once every 7hrs, only minutes apart. I'd like to check the status of the controller job to know if it's already running, so that the second build can know to cancel. I've tried curl-ing the following API, but it turns out it doesn't register individual builds, just that the job is running in general, which is useless in this case as it self-triggers since it checking means it's already running; there's no difference between a single build and multiple builds. 

I'm missing something obvious here. Is there no way to trigger a Jenkins scripted pipeline job via Bitbucket hook? There's a bug out about it but it's been around for years with apparently no fix. Is the only way to make this work really to make an upstream trigger job that kicks off the pipeline? That seems insanely clunky. 

To clarify, Jenkins' "branch" has files on it that we do need merged into the repo. The ideal situation is to find the right way to do a fresh pull of develop into Jenkins' copy before pushing back. What's the correct git procedure to avoid this issue? 

Looking for help troubleshooting an intermittent issue; We run an Ubuntu version of Jenkins on AWS infrastructure using on-demand spot ECS instances. I've noticed that while most agents spin down and correctly terminate their ECS counterparts, occasionally an agent will quit without killing its instance, leaving orphaned instances that it could take us weeks to notice. This problem has been happening on and off for a year, through many different versions of Jenkins. I can't find a common thread between the orphaned instances, and would appreciate any insight on what I can troubleshoot to find the cause. Can you clarify how you're terminating the instances? Nodes are automatically terminated x minutes after the job they're running finishes, presumably taking the instance down with them. I'm not clear on the actual particulars; it's just a setting in Jenkins for "Idle termination time". 

You have two options for placement. Putting this inside a stage will exit only that stage as a success. Putting it outside a stage (but inside the node) will end the whole job as a success. I can't be more specific without knowing your code, but I'd expect something like "make commit, if full continue as normal, if empty build result is success". 

Took a while and it's a little on the hacky side since I'm no Groovy expert, but I got it working with this; 

I'm going to assume that you have a way to know if the commit is empty or not. You can use the following code to mark the job as a success instead of relying on error codes. 

As a bonus you can set it to take a commit message each time you change a job, for easier restoration if necessary. 

How do I use a generated variable's value from an upstream freestyle job when building a downstream pipeline job? 

I have a job that will create files, unless one of the values being fed to it matches an older value. What's the cleanest way in Jenkins to abort or exit the job, without it being ? It exiting is the correct behavior so I want the build marked . It'll end up in an if statement like so; 

We have Jenkins running unit tests when commits are made to our repo in Bitbucket. This is controlled by the Bitbucket Plugin, ie via a Bitbucket webhook. Currently, if a commit is made to Branch A a unit test kicks off. If while that job is running a second commit is made to Branch A a second unit test will kick off, so there are now two unit tests on the same branch but with slightly different code. Our preferred behavior is that the first commit's test would be aborted when the second test kicked off, so that only the most recent unit test is being run. Can this be achieved? To clarify; We have many branches so we cannot just prevent concurrent builds, cancel the last as soon as the next starts, etc. Whatever method is used must specifically check if the branch already has a job running for it, not if the job in general is already running. I've seen some trigger controls for Git, but not Bitbucket. I also found a script to check if the job's already running and cancel it if so, but as mentioned before that doesn't suit our use case. Am I missing something? 

In an ideal world our less tech-knowledgeable employees could just use a Slack command to run a job and get the results slacked back to them, negating the use of the Jenkins GUI. I found this on GitHub, but it's got a few limitations including that it seems to require a 1:1 ratio Heroku instance to Jenkins job, which isn't great for our purposes. My thought is to have people use a singular command like , and use parameters after it that will be parsed by one job, and used to kick off others downstream. That keeps us to one slack command / one instance, but still gives us all the flexibility we need. The issue falls in getting the parameters to Jenkins. I'm picturing something like the following workflow; 

Downstream, the pipeline job echos the and as expected. However, the just echos the variable name, not a value. Somehow, the git commit and branch are being passed along, but the generated variable git_author isn't. I tried piping the author to a file, archiving it in the freestyle job and using the additional step, but that still isn't passing to the pipeline job. I can confirm that 

Corey's answer ended up inspiring me to take another look at the plugins available, this time considering anything that could be applied to jobs as a post-build step. I found AWS CloudWatch Logs Publisher Plugin, which is a post-build step that pushes your logs to CloudWatch. Not quite the destination we originally had in mind, but it worked for us. Easy to configure too! Only downside is it doesn't currently have Pipeline functionality. 

We use the AWS CloudWatch Logs Publisher plugin on our Jenkins and most of the time the logs are sent within 2-5 seconds of the request being sent out. However, sometimes the step will hang for up to 5-10 minutes, with no errors in the console or in Jenkins' log. The job hangs with the following status message; 

I don't want to throw an error code unless that can somehow translate into it being marked a successful build. 

will stop the stage or node you're running on which is why running it outside of a stage is important, while setting the prevents it failing. 

If you're interested in only backing up configs, try installing the SCM Sync Configuration Plugin. This plugin syncs with a repo each time you change either a job or the Jenkins configs, but does not keep builds. 

Turns out you can hit the following URL to get an XML output of all currently running builds, including their build numbers. 

Is there a way to connect to a VPN for the duration of a Jenkins job? (freestyle or pipeline, I can work with either). I scoured the plugin list figuring someone might have developed something, but the only page found was this OpenConnect page with no actual code developed, so that seems to be out. We're running Jenkins on Ubuntu AWS boxes, if that changes anything. I'd prefer the ability to connect and disconnect in a single job, rather than making everything run on VPN. 

We use AWS's EC2 Plugin to spin up spot instances in AWS for our Jenkins master based on a single AMI, exactly as you describe, so Jenkins does support this behavior. From a quick bit of searching Google Cloud seems to offer a similar solution; 

Figured it out. Outside of any stages (otherwise this will just end the particular stage as a success) do the following; 

Recently Jenkins has been putting out a number of security warnings related to CSRF Protection. We tried enabling it on our version of Jenkins (ver. 2.89.2) only to find that with it enabled you can no longer add or edit parameters on either freestyle or pipeline jobs via the GUI. I did fairly extensive testing to narrow it down as the culprit, but I still don't understand why the option exists and is being recommended if it cripples Jenkins jobs. Is this a feature or a bug? If it is a feature, why? What's to gain?