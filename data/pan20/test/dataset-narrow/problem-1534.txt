I really like this article because it gives an overview of common approaches to churn prediction as well as a new state-of-the-art method. There is also code available. However, the state-of-the-art method uses a custom loss function built on top of recurrent neural nets, which is a complex place to dive in. 

Are you looking at the accuracy on your validation set, rather than your training set? (you should be). Are you making sure your gap between training and validation accuracy is low? (you should be!) Is there enough data to warrant 3-fold cross-validation, or should you do 10-fold (and use more data for training)? In general, random search (where you sample randomly from the parameter space) will get you a good result faster than grid search. It has the added benefit that you can specify how many models you want to build, as each model's parameters are sampled independently and not constrained to cover an entire space like grid search. 

As you said, stationary just means the model's statistics don't change over time ('locally' stationary). ARIMA models are essentially regression models where you use the past N values as input to linear regression to prediction the N+1st value. (At least, that's what the AR part does). When you learn the model you're learning the regression coefficients. If you have a time series where you learn the relationship between the past N points and the next point, and then you apply that to a different set of N points to predict the next value, you are implicitly assuming that the same relationship holds between the N predictor points and the following N+1st point you're trying to predict. That's stationarity. If you separated your training set into two intervals and trained on them separately, and got two very different models - what would you conclude from that? Do you think you would feel confident applying those models to predict new data? Which one would you use? These issues arise if the data is 'non-stationary'. My take on RNNs is this - you are still learning a pattern one segment of a time series, and you still want to apply it to another part of the time series to get predictions. The model learns a simplified representation of the time series - and if that representation applies on the training set but not in the test set, it won't perform well. However, unlike ARIMA, RNNs are capable of learning nonlinearities, and specialized nodes like LSTM nodes are even better at this. In particular, LSTMs and GRUs are very good at learning long-term dependencies. See for example this blog post. Effectively this means that what is meant by 'stationarity' is less brittle with RNNs, so it's somewhat less of a concern. To be able to learn long term dependencies, however, you need LOTS of data to train on. 

There is no single transformation that will be appropriate in all cases, whether the properties/values are known or unknown. Even with known properties, you'll likely need a unique transformation for each type: mean, median, mode, min, max, boolean, etc. 

Classes related to Artificial Intelligence are typically taught in Computer Science departments. Looking at the IT Project Subjects offered by your university, I suspect Data Analytics would indeed be more relevant to AI than Internetworking and Applications. Looking at the courses offered by your department, the following likely involve aspects of AI: 

Regarding your first question... Do you anticipate the majority category to be similarly over-represented in real-world data as it is in your training data? If so, perhaps you could perform two-step classification: 

Same answer for both: you can't do this for unknown properties, and for known properties it will depend on how the values were computed. As you alluded to: 

As you can see, OpenNLP does not provide training tools for the coreference component. However, it seems at one point it was possible to train new models for OpenNLP's coref component using the third-party WordFreak plugin... however, it hasn't been updated in over a decade, so your mileage may vary. 

People talk a lot about data imbalance, but in general I think you don't need to worry about it unless your data is really imbalanced (like <1% of one label). 50/200 is fine. If you build a logistic regression model on that dataset, the model will be biased towards the majority class - but if you gave me no information about an input to classify, the prior probability is that the new input is a member of the majority class anyway. The question you want to be able to answer is whether you are differentiating classes fine - so if you do have a minority class, do NOT use 'accuracy' as a metric. Use something like area under the ROC curve (commonly called AUC) instead. If your data is really super imbalanced, you can either over-sample the minority class or use something called 'SMOTE', for "Synthetic Minority Over-Sampling Technique", which is a more advanced version of the same thing. Some algorithms also let you set higher weights on minority classes, which essentially incentivizes the model to pay attention to the minority class by making minority-class errors cost more. To learn to differentiate between lots of classes, I think (a) you will need to have a ton of examples to learn from and (b) a model that's expressive enough to capture class differences (like deep neural network, or boosted decision tree), and (c) use softmax output. If those still don't work, you might try a 'model-free' approach like K-nearest-neighbors, which matches each input to the most similar labeled data. For kNN to work however, you need to have a very reasonable distance metric. 

I get asked this question all the time, so earlier this year I wrote an article (What is Data Science?) based on a presentation I've given a few times. Here's the gist... First, a few definitions of data science offered by others: Josh Wills from Cloudera says a data scientist is someone "who is better at statistics than any software engineer and better at software engineering than any statistician." A frequently-heard joke is that a "Data Scientist" is a Data Analyst who lives in California. According to Big Data Borat, Data Science is statistics on a Mac. In Drew Conway's famous Data Science Venn Diagram, it's the intersection of Hacking Skills, Math & Statistics Knowledge, and Substantive Expertise. Here's another good definition I found on the ITProPortal blog: 

Since it sounds like you're interested in abstractive summarization (which is much harder than traditional extractive summarization), I'd recommend the following academic papers: 

As you've described it, Step 4 is where you want to use TF-IDF. Essentially, TD-IDF will count each term in each document, and assign a score given the relative frequency across the collection of documents. There's one big step missing from your process, however: annotating a training set. Before you train your classifier, you'll need to manually annotate a sample of your data with the labels you want to be able to apply automatically using the classifier. To make all of this easier, you might want to consider using the Stanford Classifier. It will perform the feature extraction and build the classifier model (supporting several different machine learning algorithms), but you'll still need to annotate the training data by hand. 

Feature-extraction mechanisms like GIST, HOG, etc are built and optimized to improve performance on given datasets. Because of this, they don't perform as well across datasets. It's kind of like putting specialized fuel in a vehicle that isn't built to utilize it - it might even do harm. Hand-engineered features are, as a rule, brittle. I once heard it said that the dirty secret of machine learning is just knowing how to transform your domain-specific information into meaningful features - after that, you can use an extremely simple classifier and it may do surprisingly well. The drawback is that the rules you built are very specific to your domain. Deep neural networks, and convolutional neural networks in particular, are an advancement in that they learn what features are useful about raw data - for CNNs, these are the raw pixel or time-series values. Instead of hand-building feature extraction mechanisms, these architectures automatically build them. One benefit of this is that if you use a CNN to identify images in general, you can re-use the top few feature extraction layers of the CNN on a different image recognition dataset, and re-train the bottom few layers to make the network specific to recognizing e.g. dog breeds. You can transfer what you learned about the statistical structure of natural images in general to other, more specific questions (general -> specific). In your case, the 'top few layers' are analogous to your GIST/HOG methods - and they wouldn't be expected to perform well when the task changes, because they were constructed for a specific task (specific -> other specific). 

In terms of open source NLG components, I'm most familiar with Mumble and FUF/SURGE. They've got both similarities and differences, so it's hard to say which is better... Mumble: 

The bottom four rows are the table stakes -- the cost of admission just to play the game. These are foundational skills that all aspiring data scientists must obtain. Every data scientist must be a competent programmer. He or she must also have a solid grasp of math, statistics, and analytic methodology. Data science and "big data" go hand-in-hand, so all data scientists need to be familiar with frameworks for distributed computing. Finally, data scientists must have a basic understanding of the domains in which they operate, as well as excellent communications skills and the ability to tell a good story with data. With these basics covered, the next step is to develop deep expertise in one or more of the vertical areas. "Data Science" is really an umbrella term for a collection of interrelated techniques and approaches taken from a variety of disciplines, including mathematics, statistics, computer science, and software engineering. The goal of these diverse methods is to extract actionable intelligence from data of all kinds, enabling clients to make better data-driven decisions. No one person can ever possibly master all aspects of data science; doing so would require multiple lifetimes of training and experience. The best data scientists are therefore "T-shaped" individuals -- that is, they possess a breadth of knowledge across all areas of data science, along with deep expertise in at least one. Accordingly, the best data science teams bring together a set of individuals with complementary skillsets spanning the entire spectrum. 

I'm trying to build a neural network with an unconventional architecture and a having trouble figuring out how. Usually we have connections like so, where $X=$ input, $H=$ hidden layer, $Y=$ output layer: $X_t \rightarrow H_t \rightarrow Y_t$ and $H_t \rightarrow H_{t+1}$ Normal Tensorflow or Keras nodes build above, where we end up with weight matrices connecting each component, i.e. $W_{xh}, W_{hh}, W_{hy}$. I would also like to introduce a connection: $Y_t \rightarrow H_{t+1}$, defined by a new weight matrix $W_{yh}$. I have looked at the Tensorflow and Keras implementations and there is a ($W_{xh}$) and ($W_{hh}$), where the second is defined in terms of the previous hidden node's output , defined here. Does anyone have a sense of how to access the output of the output layer in the subsequent step? Is it ? How are defined, and where is that documented? Thanks for any insight you can give! Edit: Before anyone asks yes, I recognize that such a connection is not strictly necessary. I am trying to understand information flow through network nodes. 

Without that context, there's no way to know which is the correct category. I'd suggest looking into how named entity recognition tools like Stanford NER work -- that will help you better understand how to do something like this. You'll see that the input to an NER tool generally needs to be a sentence, in order to take advantage of the context to properly categorize the extracted entities. 

What you're describing is often achieved using a simple combination of TF-IDF and extractive summarization. In a nutshell, TF-IDF tells you the relative importance of each word in each document, in comparison to the rest of your corpus. At this point, you have a score for each word in each document approximating its "importance." Then you can use these individual word scores to compute a composite score for each sentence by summing the scores of each word in each sentence. Finally, simply take the top-N scoring sentences from each document as its summary. Earlier this year, I put together an iPython Notebook that culminates with an implementation of this in Python using NLTK and Scikit-learn: A Smattering of NLP in Python. 

Someone correct me if I'm wrong, but the PCA process itself doesn't assume anything about the distribution of your data. The PCA algorithm is simple - 

You are describing every binary classifier. However, you are missing a key point. If your classes are separable by the value of just ONE feature, you can do what you're saying and find e.g. F1 > 1.23 as a threshold. If classification involves a combination of features, you will need to describe some combination of thresholds for each feature, or (equivalently) some relationship between the features that tells you about the class label. It's the job of every binary classifier to do exactly this - they just do it in different ways. See for example this post. Your desire to have a combination of fixed sets of thresholds will only work if you can have a set of thresholds that will encompass/describe/classify every combination of feature values. If you want a set of easy-to-read thresholds like you mention, you should read about decision tree classifiers. They'll do something like what you want - but will also ensure that you provide a class label for every possible combination fo features values. The nice about about decision trees is that they'll let you leave out your current feature selection step - they just do it for you by (1) picking the feature that best discriminates the two classes overall, (2) picking the threshold value of that feature that gives the most information about the class label (usually), and (3) repeating (1-2) several times. 

Whenever possible, try to preserve the full granularity of the smallest possible step. Assuming you know how to transform the values, you can always roll-up the steps (e.g., day to month, month to year)... but you won't necessarily be able to reconstruct smaller steps from larger ones following a lossy conversion. 

There's also Richard Socher's recent PhD dissertation on intersection of NLP and deep learning: Recursive Deep Learning for Natural Language Processing and Computer Vision 

Train a binary classifier (on all your training data) to predict membership (yes/no) in the majority class. Train a multi-class classifier (on the rest of the training data) to predict membership in the remaining minority classes. 

I highly recommend this tutorial: Getting Started with Topic Modeling and MALLET Here are some additional links to help you get started... Good introductory materials (including links to research papers): $URL$ Software: 

I'm not sure I fully understand your question, but it seems to me that you're trying to determine the category of the string/entity "titanic" out of context. Your data tells you that "titanic" could be a book, a movie, or a product, and you want to figure out which one is correct -- is that what you're trying to do? If so, the problem is that you've dropped the context in which the string/entity "titanic" appears in your original text. For example... 

Though neither are well defined, as commonly used they are somewhat orthogonal concepts. In my opinion, AI has a fairly narrow definition - it is about optimization through actions. AI is about decision making, either in deterministic or probabilistic environments. Typically, this is operationalized as action selection to maximize some reward function, or equivalently to minimize some loss function. Supervised or unsupervised learning (i.e. machine learning) about your environment can be helpful in using your experience to aid in selecting optimal actions. As it's commonly used, data science has no rigorous definition - businesses use the term to refer to anything from creating charts Excel to deep reinforcement learning models that can win Go. From the point of view of a practitioner, these have absolutely nothing in common. From the point of view of a business owner, the common thread is extracting meaning, and therefore value, from raw data. A data scientist is a 'meaning extraction layer'. How this operation is performed, and the techniques used (again in my experience), make no difference to the employer of a data scientist. The job title may as well be 'data magician'. But the point is that data + data science = business value, whether that comes in the form of insights into customer trends, causal analysis of marketing campaigns, or an AI bot that is 'rewarded' when it recommends you a movie that you like. I suppose that means that AI is a subset of data science - but you could also say the same thing about clear communication, so it's a bit of a non-statement.