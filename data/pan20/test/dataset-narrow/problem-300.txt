How do you define the "tablespace size"? Are you interested in the total size of the data files on disk that comprise the tablespace? Or are you interested in the total size of all the segments that are part of the tablespace? Issuing a will not affect the size of the table's segment so it will have no impact on the size of the tablespace under either definition. Both the size of the table's segment and the size of the tablespace's data files will remain constant. Of course, there will now be additional free space in many of the table's blocks that can be used by subsequent and operations. Issuing a , on the other hand, will decrease the size of the table's segment. That won't affect the size of the tablespace's data files. But it will affect the total size of all the segments that are part of the tablespace. So there may be a difference depending on your definition of the size of a tablespace. A , being DDL, will not be transactional so it cannot be rolled back. Assuming that you are deleting a large fraction of the rows in the table, it will also tend to be much more efficient than issuing a because it generates much less and . If you are stating in your last paragraph that the size of the segment is increasing much faster than the rate at which new data is being added, assuming that the new rows are roughly the same size as the old rows and that the old rows are not growing over time due to updates, is it possible that the new rows are being added via direct-path inserts which will always go above the current high-water mark for the segment and will thus never reuse the space in blocks that is freed up by a ? If so, is that intentional? If the table is small, you might see similar differences because of the granularity of extent allocation-- you might insert 100 rows without requiring a new extent, the 101st insert requires Oracle to allocate a new extent, and that new extent might be sufficient for thousands of new rows to be added, but you'll only see the size of the segment change after the 101st insert. But that is less likely if this is a reasonably large table unless you've chosen a particularly large extent size. 

Oracle has no information about what value I might pass in for so it does a very generic estimate. If there are 20 distinct values, for example, it will probably guess that the query would need to access 5% of the rows in the table. If you actually execute this statement and pass in a value, on the other hand, Oracle has a lot more information-- it may know from a histogram that the value you passed in will actually require it to access 7% of the rows in the table. If the actual query plan remains unchanged, it's entirely plausible that the would increase by 40% since the expected amount of work grew by 40%. A complete list of everything that causes details from an estimated query plan to differ from details from an actual query plan and an explanation of how those things interact would be much too long for this format (particularly since a lot of the items get very complicated very quickly). There are situations where statistics on an object are missing, for example, where the optimizer has to do a random sample of the data to infer the statistics at compile time that will differ a bit every time the query is compiled. There are a number of situations where the optimizer has some sort of feedback mechanism that kicks in when a query is being run that it doesn't have when a query plan is being estimated-- it may choose a degree of parallelism based on available resources, it may change the cost of a sort depending on how much PGA space it can get, depending on the version it may be able to change course if an operation retrieves substantially more or less data than it expected. There are effects do to plans being cached or different technologies that try to ensure plan stability kicking in when queries are actually compiled. 

does not change the value of . It compares to + 300 and evaluates to a boolean. But it would only be valid in something like an statement or in a SQL statement, i.e. 

in a (from a user standpoint) idle database, you'll note that the is incrementing every few seconds. That is Oracle doing various sorts of housekeeping. That housekeeping will generate a small amount of redo each time. Over the course of a full day, it's pretty common that this would add up to enough for a couple of archived logs even if your system is otherwise pretty idle. 

The error you're getting indicates that the User1 user does not have access to the package. You'd need a DBA to 

Then, you'll need to go through each child table dropping the old foreign key constraint and creating the new constraint, i.e. 

If the table is in a single database, you don't want to replicate it. You don't want to create a second copy of the data. You simply want to give access to whatever schemas need access to the data. You may also want to create some synonyms so that you don't have to use fully qualified names. If owns and you want that data to be visible to 

Generally, though, I'd question the problem you're trying to solve. I can't think of many times that it would make sense to drop a user that has active sessions at the time you decide to issue the drop. In the vast majority of cases, the presence of active sessions strongly implies that the account should not be dropped. 

If you create the trigger and procedure to be owned by a user other than the one that owns the table, you'd need to include the schema name in your statement inside the procedure. 

You need to add aliases for the computed columns so that Oracle knows what names the columns of the table should have 

A hard drive's IOPS tells you how many I/O operations that particular drive can perform per second. Since random reads, random writes, sequential reads, and sequential writes have different performance characteristics, if you're looking at a single number, that implies that this is a weighted average of these four numbers that represents some workload. That workload may or may not be similar to the workload that a database would actually perform so your system may end up getting more or fewer operations per second. A SQL query would, assuming it does physical I/O (many queries do not need to do any I/O because they are simply reading from data already in the database's memory or in the file system cache or in the SAN cache), tend to perform many I/O operations. If your queries are actually doing physical I/O, a single query could easily do 100 I/O operations per second. You could potentially have 100 queries running simultaneously each only getting to do 1 physical I/O operation per second but that would likely mean that all 100 queries would take an unacceptable amount of time to return. Realistically, if you need to handle 3000 queries per second, you'll need to ensure that the vast majority of your queries do not need to do physical I/O by ensuring that the data they need is cached (preferrably by the database). And you'll need to spread the I/O over a relatively large number of drives (with an appropriate RAID configuration) to increase the total IOPS that the system can perform. This sort of capacity planning is going to require a reasonably good understanding of your application, your data, how much I/O each query is likely to really need, etc. And you'll likely need to balance cost vs. performance-- adding memory or adding drives or changing the RAID configuration will involve different tradeoffs between cost, performance, and available space that will have to be considered. You may also want to consider splitting the workload among multiple servers (which may require architectural changes to your application). 

Currently, the two are synonymous. VARCHAR is an ANSI standard data type but Oracle's implementation of the VARCHAR data type violates the ANSI standard by considering the empty string to be NULL (Oracle's implementation predates the ANSI standard). As Leigh points out, Oracle has stated that the semantics of the VARCHAR data type may change in the future with respect to how the empty string is treated. If and when that happens, the semantics of the VARCHAR2 data type will remain the same. Using the VARCHAR2 data type is safer because you don't have to worry that some future version of Oracle will break your code by causing empty strings to no longer be considered NULL. 

should be valid syntax. Whether it is actually appropriate to create a table for this sort of thing is a question you'd have to answer. My bias would be that it would be more appropriate to create a view or potentially a materialized view but I don't know the exact nature of the problem you're trying to solve. 

where is 11 and is 20, Oracle would fetch the first row, give it a of 1, then discard it because it didn't satisfy the predicate. It would then fetch the second row from the inner query, give it a of 1 (since no rows have been successfully returned yet), then discard it because it didn't satisfy the predicate. That would repeat until every row had been fetched, assigned a of 1, and discarded. So the query would return 0 rows. The extra layer of nesting ensures that the outer query with sees rows with values between 1 and and that the full query returns the expected set of rows. If you're using 12.1 or later, you can use the syntax as well