It should be noted that finding combinators with certain reduction properties is always difficult, and finding the smallest such combinator may easily be undecidable (for trivial reasons, as it may be undecidable to prove that a certain application of the combinator even halts). There are several simple open questions of a similar flavor, e.g. problems #4, #6 and #10 from the TLCA list of open problems. One thing to note is that your combinator certainly needs to have at least 2 bound variables, one of which is duplicated (as does any complete set of combinators) and one needs to be erased. This puts a lower bound of 4, I think (2 abstractions and 2 appearances of a variable), which is not so far from the upper bound of 11. Edit: Noam's comments and reference push the lower bound to 5! I wouldn't be surprised if the proof also requires the extra variable to appear as well, which would push us to 6. 

You could adopt the opposite point of view of keeping your more general conversion rule and weakening the context formation rule to $$ \frac{\Gamma\vdash \beta:s\quad \beta\equiv \alpha}{\Gamma,x:\alpha\ \ \mathrm{wf}}$$ then you get a system with possibly ill-formed types, but no additional typeable terms, though I think this is a very non-trivial result. 

If every "syntactic task" merely required pushdown automata and every "semantic task" something more, then the matter would be settled, but unfortunately there are many tasks that are associated with programming language syntax that cannot be handled by PDA alone, and conversely, some semantic tasks which seem to require only weak computation power. Still, the distinction provides some intuition. Pragmatically, compilers are designed in phases, which makes managing the task of generating executable code manageable. I particularly like this diagram from the Compcert website. 

It's hard to add anything to Andrej's or Neel's explanations, but I'll give it a shot. I'm going to try to address the syntactic point of view, rather than try to uncover underlying semantics, because the explanation is more elementary and my give a more straightforward answer to your question. I am going to work in the simply-typed $\lambda$-calculus rather than the more complex system underlying Haskell. I believe in particular that the presence of type variables may be confusing you to a certain extent. The crucial reference is the following: Mendler, N. (1991). Inductive types and type constraints in the second-order lambda calculus. I haven't found a reference online I'm afraid. The statements and proofs can however be found in Nax's PhD dissertation (a highly recommended read!). Mendler explains that positivity is a necessary and sufficient condition for termination in the presence of non-recursive case definitions (and structurally decreasing recursive ones). He states it using an equational formulation. I give a simple example, which is a simplification of your $\mathrm{Bad}$ type. $$ \mathrm{Bad} = \mathrm{Bad}\rightarrow A$$ Where $A$ is any type. We then have $$ \lambda x:\mathrm{Bad}.x\ x: \mathrm{Bad}\rightarrow A$$ and so $$ (\lambda x:\mathrm{Bad}.x\ x)\ (\lambda x:\mathrm{Bad}.x\ x): A $$ Mendler shows that this can be carried out for any type $$ \mathrm{Bad} = F(\mathrm{Bad})$$ where $F(X)$ is a type with at least one negative occurrence of $X$ (there may be positive occurrences as well). He gives an explicit term which fails to terminate for a given $F(X)$ (pages 39-40 of his thesis). Of course you are working not with equationally defined types but with constructors, i.e. you have 

so in other words a basic answer is that Lambda Calculus is in many ways the ultimate legacy system of TCS. its still around in much the same way that Cobol is even though not as much new development goes on in the language! it appears to be the earliest Turing Complete computation system introduced and even predates the fundamental idea of Turing Completeness. it was only later retrospective analysis that showed that Lambda Calculus, Turing machines, and the Post Correspondence Problem were equivalent and introduced the concept of Turing equivalence and the Church-Turing thesis. Lambda calculus is simply the way to study computation from a logic-centric pov more in terms of representing it as math theorems & logical formula derivations etcetera. it also shows the deep relationship between computing and recursion and the further tight coupling with mathematical induction. this is a somewhat remarkable factoid because it suggests that in many ways the (at least theoretical) origins of computing were fundamentally in logic/mathematics, a thesis advanced/expanded in detail by Davis in his book Engines of Logic/Mathematicians and the origins of the computer. (of course the origins & fundamental role of Boolean algebra also further reinforce that conceptual historical framework.) hence, dramatically, one might even say Lambda calculus is a bit like a pedagogical time machine for exploring the origins of computing! 

excellent/wide responses so far. I suggest some classes not mentioned so far. esp classes that lean toward application of the theory & require the student to write/debug code & visualize [graph results] as part of assignments. or build/debug working systems. etc. 

also as in his comments SJ covers this similar case in his book in the section exploring star complexity of graphs sec1.7.2. 

there is a lot of interest however it appears there are some early prototypes but overall not commercial chips so far. see eg Reversible computing is ‘the only way’ to survive Intel's heat mentioning michael frank at UF, his page here RevComp - The Reversible and Quantum Computing Research Group. from what I can tell it looks like the reversible designs probably require larger chip area to compute the same problems and current designers would rather pay for mainly minimizing chip area at the expense of "heat". and designers have gotten very adept at minimizing heat in nonreversible designs. one area of new innovation are energy capturing designs that can capture waste heat and convert it back into electricity. eg see Phononic Devices’s Chips Convert Waste Heat into Electricity the industry is moving toward capturing waste heat for useful purpose eg building heating esp with supercomputers. of course quantum computation is the "holy grail" for reversible designs but its proving very difficult to create systems due to the issue you mention in qm circuits-- noise or the so called "decoherence problem". the only commercial system so far seems to be dwave which is very expensive & niche right now. 

The approach used in RSA using pairs of primes can also be applied in a more general framework of cyclic groups, notably the Diffie-Helmann protocol that generalizes $\left(\mathbb{Z}/pq\mathbb{Z}\right)^{\times}$ to an arbitrary group, notably elliptic curves which are less susceptible to the attacks that work on integers. Other group structures have been considered which may be non-commutative but none are in widespread use AFAIK. There are other approaches to cryptography, notably lattice-based cryptography that rely on certain hard problems on lattices (finding points with small norm on the lattice, for example) to implement public-key cryptography. Interestingly, some of these systems are provably hard, i.e. can be broken if and only if the corresponding hard problem in lattice theory can be solved. This is in contrast with, say RSA which does not offer the same guarentee. Note that the lattice based approach is conjectured to not be NP-hard (but seems harder than integer factoring for now). There is a seperate concern to key sharing, namely secret revealing, which has very interesting complexity theory properties. I don't know the details, but the theory of zero-knowledge protocols allows Alice to reveal to Bob her knowledge of a secret which is NP hard to compute (Graph Hamiltonian) without revealing the secret itself (the path in this case). 

As far as I know, Principia Mathematica uses essentially a formalization of set theory using a typed first order logic. It would therefore be tempting to use a first-order automated theorem prover like Prover 9 or possibly ACL2 to formalize your statements. However, I am seeing several set-theoretic constructions (like $\in$, $\cap, \subset$) in there, which usually don't play very well with first order ATP. Any modern interactive proof assistant will surely have the expressiveness to formalize and prove your statements, as Andrej has suggested. In fact, since there seem to be some statements including arithmetic, it would be wise to use a system like Isabelle, Coq or HOL which already have extensive theories to treat statements of arithmetic. My emphasis on modern is not a coincidence: great strides in usability have been made since Automath, and I honestly think you would be doing yourself a disservice by using anything that hasn't been actively developed since the 90's (if you could even get one to work!) Finally, ITP and ATP have rather challenging learning curves, and you shouldn't expect to be able to enter these theorems into such a system as if you were writing a $\LaTeX$ proof. Expect severe frustration and lost time, especially in the first months (yes, months). You definitely need to work through some tutorials at first before getting to the main formalization. 

To my knowledge, no machine checked proof of a complex mathematical development has ever been retracted. As Andrej points out though, it occasionally happens that soundness-breaking bugs do crop up in these systems (though usually not silently, as Andrej suggests), and the fix to that bug involves some changes to existing proofs, or, more likely, of the standard library of the proof system involved. Some examples of such library breaking proofs in Coq: $URL$ $URL$ It's hard to say whether the established proofs depended on the inconsistency, since after the fix, they required minor tweaks to be accepted by the proof checker. But this happens at each non-trivial update! My personal opinion is that such mistakes are unlikely to happen, since the paper proof needs to be well polished before machine formalization can even be attempted. Inconsistencies in proof frameworks usually require the heavy use of strange combinations of esoteric features, and so very rarely crop up "by accident". 

there is some research angle here dating at least to Knuth's Art of Computer Programming and presumably earlier in finding optimal sorting networks for low $n$. its intractable to find optimal sorting networks for small $n$ but it has been done up to about $n=10$ eg as in this recent notable paper, also using SAT. details about how to reduce the problem to SAT are in the paper. basically a large SAT formula encoding is built that asserts "these boolean variables configure a circuit that sorts all inputs for size $n$". (the more nonresearch angle is to use existing sort algorithms or sorting network configurations as mentioned in the paper by Har-Peled you cite to generate the (nonoptimal) circuits, this is more like a CS/EE exercise.) Optimal Sorting Networks Daniel Bundala, Jakub Závodný 

[1] Recurrence of Simple Random Walk in the Plane Terence R. Shore and Douglas B. Tyler The American Mathematical Monthly Vol. 100, No. 2 (Feb., 1993), pp. 144-149 

(fineprint: and if all this actually worked, it would likely be a significant advance & publishable result.) 

however another angle, a close nearby area is Reaction Diffusion equations which are a special class of differential equations. these were initially studied by Turing. he proposed they were capable of a kind of computation. this was later borne out and they have somewhat recently been proven to be Turing Complete. see eg 

this very apropos NYT article Computer experts building 1830s Babbage analytical engine deconstructs the interconnecting historical threads of awareness more carefully. 

heres another angle on this. the question starts out with an a priori "hard sell" situation/case ie a dept that is mostly applied is already going to have an inherent bias against a more theoretical researcher, otherwise that researcher would already be in the dept. one possible direction "in" is the following. an often overlooked area of software development/design in industry and sometimes computer science departments is architecture. sometimes even TCS does not have large awareness of architecture either, but it is better positioned to recognize it than with applied focus. in a few words, software architecture is the study of complexity in the solution/software, and ways of minimizing it, which is a major issue esp. with large software projects, which these days are increasingly more common. software architecture involves in part the choice of algorithms (ie choice among equivalent algorithms, in terms of input/output relationship, based on different performance and scalability characteristics). sometimes applied workers are not even familiar with the basic theory that tells you stuff like an indexed column on a database is $O(log(n))$ access time instead of $O(n)$ and why this is in fact a "big deal". architecture is also tied with very practical topics like when refactoring of code is appropriate or not. also it seems like some complex commercial software has many dimensions of scalability. in robotics an analogous concept is "degrees of freedom". the software scales in many separate dimensions. for example, memory size, size of web pages, size of database results, etcetera. a good theoretical researcher will be able to more systematically/scientifically assist in identifying and quantifying all these dimensions, ie using the scientific method. more applied workers will tend to just resort to trial and error, sometimes over a period of years, never really understanding exactly why their software is underperforming, hitting internal ceilings, or failing. there are a few rare researchers who can cross between theory and applied realms without difficulty, or as the vernacular goes, "feet in both worlds". maybe studying them in particular (their career directions and writing, etc) will help to develop the mindset and philosophy to "cross over". in particular, Grady Booch is one eminent figure who comes to mind. he invented much of the concepts behind OOP programming and has much excellent research/writing on the topic of software architecture. another similar researcher is Phillip Kruchten. and yes [as in comments], Peter Shor. having worked with both, one possible distinction is that in a more applied setting the main deliverable is code and in a more theoretical setting the "deliverables" are analyses/paper(s). this can cause a cultural gap if not everyone is "on the same page" with what are the deliverables. in this case the theoretical scientist will be the one who will have to bend & be more flexible and also "be ok" with that. so the personality of this key person will be a key factor. another emerging/very prominent area these days that has strong interdisciplinary connection/overlap between theoretical and applied scientists is datamining, reaching larger prominence with official government/corporate initiatives into big data (and again architecture is a foremost element in this). 

I'm going to assume that by $\mathrm{Fix}$ you mean a new type constructor $$\frac{\Gamma\vdash F:*\rightarrow *}{\Gamma\vdash \mathrm{Fix}\ F:*} $$ Along with the conversion rule $\mathrm{Fix}\ F\simeq F\ (\mathrm{Fix}\ F)$. This is often referred to as equi-recursive types as opposed to iso-recursive types, where the conversion rule does not exist, and you need additional $\mathrm{fold}$ and $\mathrm{unfold}$ constructs to use $\mathrm{Fix}$. I'll ignore the fact that the rule for $\mathrm{Fix}$ is non-terminating on it's own, since this isn't that harmful. You question about terms $$\Gamma\vdash t:A$$ with $$\Gamma\vdash A:*$$ when are they sound proofs of the statement $A$? You suggest using a "totality checker", to sidestep the need for messy rules such as positivity and various other restrictions when dealing with iso-recursive types. The first remark is that simply checking termination of a term $t$ won't do: we can easily build a proof of $\forall n:\mathbb{N}. n = 0$ which has a normal form, but loops when applied to some natural number! So we really do need to check totality, which involves proving that $t$ will not lead to non-termination when placed in an arbitrary well-typed context. Sadly, we don't really know how to do this, other than building back in all those nasty restrictions you want to avoid! For positivity for example, taking $\Lambda=\lambda X:*.X\rightarrow X$, then you can type $$\vdash \lambda x:\mathrm{Fix}\ \Lambda.x\ x:\mathrm{Fix}\ \Lambda $$ which smells like trouble: your totality checker had better reject that as a theorem. The other restrictions will crop up pretty quickly. As a side note, if you can prove something using some "reasonable" totality checker (provably correct in HOL) then you can actually "reverse engineer" a proof in CoC + $\forall n. S\ n\neq 0$, by internalizing the totality check. This technique is similar to realizability, where there is a correspondance between provably total functions and $\Pi^0_1$ theorems. 

Here's a quick sketch to show that there is no Turing machine to decide whether an arbitrary class of problems is decidable. I should clarify what I mean by class of problems: a class of problems $T$ is a Turing machine which enumerates the elements (natural numbers, say) of a recursively enumerable set one after the other, such that each element in the set is eventually printed. The problem intuitively captured by $T(n)$ is: "is the number $n$ in this set?". This captures the usual problems in the field of computability, such as "is i the index of a Turing machine that halts on empty input?". Suppose there was machine $M$ which, given as input a class of problems $T$ answered $\mathit{true}$ if that class is decidable and $\mathit{false}$ otherwise. Now take an arbitrary Turing machine $T$. We build the following class of problems $T'$ in the following manner: