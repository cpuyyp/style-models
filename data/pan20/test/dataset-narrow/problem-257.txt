UNION The builds a hash table for a hash aggregate on the overall data set, meaning that the memory usage is proportional to the total number of distinct rows (5.4MM rows in this case; generally, at least the number of rows in the larger of the two tables). The memory usage is over 10x greater than the , and both CPU time and elapsed time are slower as well. If I were to scale this up to the point that hash aggregate can't fit within a single query's memory grant, the performance difference would become huge (as it was in our large production query). 

Lastly, a point of advice: You are typically going to be get the best help on this site if you post a specific query (or even better, an entire script) that describes your question more precisely and shows what you have tried so far. It was difficult for me to tell exactly what you were asking, and a script can help avoid misunderstanding. 

And now on to the details: Create rowstore data set Nothing too exciting here; we create 40MM rows of pseudo-random data. 

When run with an account that is not a , this same query takes about eight minutes! Running with actual plan on and using a p_queryProgress procedure that I wrote to help parse output more easily, we can see that almost all of the processing time is spent on the operator that is performing the check: 

Since it looks like you only want a data point every hour, one approach could be to use the durationSeconds column to figure out how many subsequent hour boundaries the data point should cover. The following query performs this logic and yields your desired results on the test data. 

Business context We are semi-frequently moving around millions of rows of data, and it's important to have these operations be as efficient as possible, both in terms of the execution time and the disk I/O load. We had initially been under the impression that creating a heap table and using was a good way to do this, but have now become less confident given that we observed the situation demonstrated above in an actual production scenario (albeit with more complex queries, not the simplified version here). 

In the future, please post the queries that you have tried so far so that we can help point out where they may have gone wrong. 

this is the best way as not all the data has to be transported over the network. i definitely recommend this replacement for pg_basebackup because it is the only RELIABLE way to get things back in order (nobody knows for sure what happens before the crash). sure, there might be other tricks out there but i would strongly vote for this method. 

it allows people to work on various files concurrently. it makes using git and so on a lot easier. also, consider looking at CREATE EXTENSION. it allows a pretty nice way to handle versions and all that. i strongly encourage to use that one as well. git is also a good thing to have in general. 

a sequence makes sure that values are ascending BUT it does not make sure that it does not contain gaps. it is also important to notice that a sequence cannot be rollbacked. you cannot have strictly ascending and gap-free at the same time as it would not work with a mix of long and short transactions. therefore a sequence should never be used for an invoice-id and so on. 

you can make use of the so called ctid (which is basically an internal thing but for a lousy hack it should do. here is an example: ' 

i would not see this as a major issue. of course things are a little larger but if we talk about a single integer column things will most likely be lost in noise anyway. we got 24 bytes of tuple header ... then comes the data. if things are 4 bytes larger? what difference does it make in a 10 column table. so, no worries ... make sure that you do what is good for your app. 

it is not possible to have dynamic column lists. at the time you issue a SELECT the target list has to be clear. so you cannot just add a column somewhere just because some value is added. 

one thing you got to keep in mind is: if you materialize the count, you might have some fun on the concurrency side. however, a materialized column is of course always a lot faster. 

the crux might be your loop. you can get rid around that one easy: there is a function called unnest, which can transform an array into a table which can then be joined or whatever (via a LATERAL join maybe). you are on the wrong path with your approach. if you really want this loop and so on, you need a set returning function and use it inside COPY. 

I think something like the following will work. You already have the database name () in your first query, so you can use this to join to . Beyond that, you are using an aggregate function in your second query. So you simply need to group by the columns you are selecting from the first query. 

REBUILD vs. REORGANIZE vs. DROP/CREATE SQL Server offers and options, but they are available only for a full-text catalog (which may contain any number of full-text indexes) in its entirety. For legacy reasons, we have a single full-text catalog that contains all of our full-text indexes. Therefore, we have opted to drop () and then re-create () on an individual full-text index level instead. It might be more ideal to break full-text indexes into separate catalogs in a logical way and perform a instead, but the drop/create solution will work for us in the meantime. 

I had a script that does a rudimentary version of foreign key traversal. I adapted it quickly (see below), and you might be able to use it as a starting point. Given a target table, the script attempts to print the join string for the shortest path (or one of them in the case of ties) for all possible source tables such that single-column foreign keys can be traversed to reach the target table. The script seems to be working well on the database with a couple thousand tables and many FK connections that I tried it on. As others mention in the comments, you'd need to make this more complex if you need to handle multi-column foreign keys. Also, please be aware that this is not by any means production-ready, fully-tested code. Hope it's a helpful starting point if you do decide to build out this functionality! 

Assumptions There are a few significant assumptions baked into this approach. I suppose it will be up to Paul to decide whether they are acceptable :) 

Using , this approach reports that no logical I/O has occurred! Wow, a perfect solution! (Actually, it seems that does not report I/O incurred within CLR. But from the code, it is easy to see that exactly one scan of the table is made and retrieves the data in order by the index Paul suggested. As reported by , the CPU time is now . So this is quite an improvement over the T-SQL approach. Unfortunately, the overall elapsed time of both approaches is very similar at about half a second each. However, the CLR based approach does have to output 113K rows to the console (vs. just 52K for the T-SQL approach that groups by product/date), so that's why I've focused on CPU time instead. Another big advantage of this approach is that it yields exactly the same results as the original loop/seek approach, including a row for every transaction even in cases where a product is sold multiple times on the same day. (On AdventureWorks, I specifically compared row-by-row results and confirmed that they tie out with Paul's original query.) A disadvantage of this approach, at least in its current form, is that it reads all data in memory. However, the algorithm that has been designed only strictly needs the current window frame in memory at any given time and could be updated to work for data sets that exceed memory. Paul has illustrated this point in his answer by producing an implementation of this algorithm that stores only the sliding window in memory. This comes at the expense of granting higher permissions to CLR assembly, but would definitely be worthwhile in scaling this solution up to arbitrarily large data sets. 

shared_buffers are statically taken at startup and are never resized. effective_cache_size is just a hint to the optimizer. it is never allocated. it merely gives a hint of what is going on. so shared_buffers is what you see as taken. 

as far as i know there is no such tool around. however, you got a couple of choices: a.) use oracle_fdw and just join the stuff or b.) export data to text files and use a simple UNIX diff. it works like a charm usually. 

from experience i can say that one large delete is usually better. however, there can be some corner cases with IN, which might invalidate my statement but basically this is true. Make sure your got enough work_mem around to allow PostgreSQL to nicely hash the IN. 

no, pgstrom cannot do that. it only able to scan relations - it is not able to join. what you an do is pack an operator into a GPU routine but = is not a good candidate for that. 

yes, this is possible. however, you have to be a little careful. DDLs in a stored procedure USUALLY work. in some nasty corner cases you might end up with "cache lookup" errors. The reason is that a procedure is basically a part of a statement and modifying those system objects on the fly can in rare corner cases cause mistakes (has to be). This cannot happen with CREATE TABLE, however. So, you should be safe. 

i think you are facing spinlock contention. you can verify this using "perf" (in case s_lock shows up on top my guess is right). in general try the following: 

in PostgreSQL you should use a tool called pgbench. it allows you to run custom scripts with an arbitrary number of concurrent requests and all that. it is also able to create random values for your script and all that - it is really powerful. for the test: make sure you got a proper setting for -j (number of threads used by pgbench) so that pgbench is able to create the load. also make sure the system is properly tuned (especially synchronous_commit set to identical values, checkpoint_segments, shared_buffers). make sure you got proper fillfactor settings on those tables and ensure that prepared plans are used (pgbench can do that for TPC-B). if you got it right, it expect MySQL to be beaten by a wide margin in a high-concurrency read / write test (transactional of course). 

Create columnstore data set Let's create the same data set as a , using the techniques described to load data for better segment elimination on Niko's blog. 

If SQL Server knew that so few rows would come out of this grouping, it would almost certainly use a (Hash Aggregate) rather than a (Sort-Stream) in order to implement your clause. It's possible that this may fix itself if you make some of the other changes above in order to improve cardinality estimates. However, if it doesn't you could try to use the query hint in order to force SQL Server to do so. This would let you evaluate the magnitude of the improvement and decide whether or not to use the query hint in production. I would generally be cautious about query hints, but specifying just is a much lighter touch than something like using a join hint, using , or otherwise taking too much of the control out of the query optimizer's hands. Optimization: memory grants One last potential problem was that SQL Server estimated that the query would want to use 72GB of memory, but your server was not able to provision this much memory to the query. While it's technically true that adding more memory to the server would help, I think there are at least a couple other ways to attack this problem: 

I think that the approach you came up with is a pretty good one. Here is a proposed simplification that avoids the self-join on by using the new SQL Server 2012 LAG function as well as CTEs to distill the logic into a single query (also in SQL Fiddle form). 

Documentation There is strong evidence in the documentation that partition numbers are in order by value, but I agree that the documentation itself is not conclusive. Here are a few pieces of evidence that I found: The CREATE PARTITION FUNCTION documentation states that the partitions will initially be in order: 

For SQL Server, a quick and dirty approach that we have used for a number of years without any problems is to use the existence of a temporary table that has a specific name (typically based on the trigger name) in order to provide an easy manual override that bypasses the trigger. The #temp table will be specific to this connection, but remains in scope in the trigger and can therefore be used as a way to pass information. If you are dealing with a very high concurrency OLTP type of environment you'll want to test this approach and measure any overhead before proceeding in production. Let's say you have a trigger called . Here is what the code might look like when you want to bypass the trigger: