Did you just imply that you're expecting to make something better than world-class modelling application developers? Not that I don't consider all this doable, it's just time-consuming and will most likely require an artistic touch in the end anyway (which procedural generation tools simply cannot provide from that point in the pipeline). Especially since you haven't considered texcoord generation here at all. So before I make my suggestion, let me point you to someone who struggled with a similar problem: Art Asset Overview #32 (from Wolfire Blog). They solved it by allowing to edit the procedurally generated terrain mesh. If you check their blog, they've done really impressive things like making crazy algorithms work (Delaunay triangulation is extremely crazy because of floating point number inexactness, for example). So we've come full circle back to my suggestion. It's quite simple, actually. 

It has different formats in different graphics card drivers, however it should be possible to detect common things and retrieve them. There are things that should be common between various drivers: 

Everything else (action music and funky camera effects) is just superficial fluff that will fade away rather soon. Player's knowledge is everything. (Source: developer commentary of some HL2 game, independent design research). 

The typical situation is that all sprites are rendered with the same view/projection matrices (same "virtual camera"). So the way I see it, all your sprites need just one draw call - just generate all the vertices on the CPU: 

Logic in these systems? There is almost none. :) After chatting for some time about this problem with other GD.SE users, we came to a conclusion that many of us weren't really convinced by these systems that everyone's an expert with. The original idea of components in programming was somewhere along these lines: 

The only way a game rule can be data is if it's an external script. But this method has its drawbacks like simplified cheating. P.S. It's the first time I hear that a data driven game engine has a solid definition. Anything that includes the word "driven" is usually just a bunch of buzzwords glued together. 

In an intersection, all of the surface planes involved form a new shape defined by these planes. To find the points, formally there's vertex enumeration. However, I find it easier to just skip all math theory and do 4 nested loops over the planes, intersection of the first two planes gives a straight line/ray, which is checked against the third to obtain a point, which then has to be checked if it's inside the shape (= it's behind every plane). All generated points that pass the test are the vertices of the polyhedron. In practice though, remember that there has to be some room for numerical errors (merge similar points and allow for some bias when projecting points on plane axes). There's also another implementation of vertex enumeration that I could find, perhaps it can help: $URL$ 

Using decals and even meshes of highly varying scale for additional detail is completely fine and has been done for quite some time already (I'd say at least 20 years or so, since the dawn of somewhat realistic 3D). Yes, it's definitely worth it. And no, there's nothing wrong about making something as long as it looks right and meets all performance requirements. For example, The Last of Us still did things like that: $URL$ - if you look closely, it can be seen that level of texture detail isn't exactly matching for all foliage. Though of course, before you make detail like that, you should probably make sure that someone will actually be able to see that detail. Not much point investing in polishing for rarely visited or distant areas. P.S. 

Both are true for simple games, false otherwise. BSP or any other spatial partitioning / bounding volume hierarchy (both are almost the same) system can significantly reduce the time it takes to process things. As for scene graphs, once you want a character that has all kinds of things attached to it or at least, relative object positions in world (light attached to a moving lamp, for example), you'll see that they become useful. Only as far as rendering is concerned, though - physics always has a somewhat flat "scene graph" (sometimes even cyclic, so the DAG (tree) structure of a scene graph isn't useful). 

First of all, this question doesn't really have anything to do with design patterns. This is closer to optimization, though it's lacking a performance problem. So, question #1: is your current entity system a performance bottleneck? If no, don't worry until it actually is. If it already is, there's a few things you can do: 

For meshes that support multiple materials, that should not be necessary. The detail can be added into the same mesh. Unless you need many meshes of the same kind together, in which case it might be easier to just export all variations separately, if you use a visual editor of any kind. 

Range picking is basically a set of intersection tests between objects and camera frustum (6 planes that together define a convex volume). If you have some frustum culling code, you can just fix it up to make it work. The easiest way is to generate point data from camera matrix (view*projection), adjust the point data by means of coordinate transforms and interpolation (interpolate point data on near/far planes depending on the screen-space selection rectangle corner values) and generate planes from those points. 

I don't see any special shaders there, just some vertex colors that are generated with an ambient occlusion (or similar) calculation algorithm and perhaps some differently calculated vertex normals. The results remind me of a demo I've seen some time ago which simulated global illumination by "bending" vertex normals and generating occlusion values for vertices. The only thing one might call a special shader could be a hemisphere/SH (spherical harmonics) light that appears to be used in there. 

Note: this assumes screen-space axis directions, not the mathematical (Y+ points down, not up). "ln" is the line normal - a normalized vector that points away from line and is perpendicular to it. The "perp" function returns the vector, turned 90 degrees counter-clockwise. Both "dot" functions return projections of points on an axis that is defined by the line normal. "distance" is a subtraction of those projections, which makes it a distance from an infinite version of the line defined by the two points "p1" and "p2". Now it wasn't clearly defined whether you need a finite or an infinite line (and if this, what kind of infinite). I initially assumed infinite so... this code works with all kinds of "infinite" - you really need one point and the line normal to use this method.