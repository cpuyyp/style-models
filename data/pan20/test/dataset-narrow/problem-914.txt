I have a PXE menu configuration that I'm working on. It incorporates the RHEL6 Rescue option in order to perform repairs after booting from the network. The Rescue boot environment asks three basic questions to set itself up: Language, Keymap, and source for the rescue image. I have the first two sorted by adding the "lang=" and "keymap=" options to the "append" line in the PXE menu: 

After a few minutes I wised up and thought to look at the isolinux.cfg file from the DBAN ISO. The only option after append that is common on all of them is 'nuke="dwipe"'. I put that option in place and the application loaded properly. 

I've verified via that I am able to connect to the slaves from the master via port 53. EDIT 2 One thing I've done is to ensure that all of the slaves have the same configuration as well as fixing the logging which was misconfigured. About three hours after I modified the zone file the transfer happened. I still can't sort out why it isn't happening immediately. Below is the master . There are several more zone entries, but they are all identical with the exception of the domain. 

I have a functioning PXE configuration that allows to boot to an installation by Kickstart or rescue, both via RHEL6 media. I'm trying to add DBAN to the mix to make for a more complete solution for our needs. I have it set up to boot and it will load, but it keeps failing with 

mxtoolbox has a pretty comprehensive blacklist checker. I know you already checked but this might be useful to others who come across this post as well. Your best bet - as mentioned by caelyx - is to implement DomainKeys and be done with all this nonsense. Yahoo and Gmail both green light dkim signed email till they have reason to do otherwise (users tagging it spam). If this is at all business related it's a pretty obvious choice ROI wise. An hour or so of configuration and testing equals much fewer issues with spam filters. 

I would like to have ephemeral ec2 instances push logs to a central flat-file store for archiving and manual perusing, as well has have that data pushed to elastic search. Is there a single agent that can tail local log files and both push them to a central flat-file store as well as push them to elastic search? 

Is there any way to enable/disable APC on a per virtual host basis? I'd rather not use precious apc shared memory on caching my PHPmyAdmin subdomain or lesser used sites. Setup is Nginx, php-fpm, and apc. From what I've read the filter setting of APC cannot match against full path so I am looking for some other method. 

Performance, that is why you should use a good VPS provider over something like a mac mini. Disk throughput is going to be utterly terrible on a mini. So any and all server requests that hit the platters are going to suffer. Too many concurrent requests will quickly result in disk bound limitations. Not even mentioning RAM with ECC, faster processors, dual power supplies, etc. If you want to own the hardware you can purchase a used server and find a colocation facility near you. However, this will likely cost more than $55 a month. Stick with VPS, who are you using? I use Linode and have a 512MB instance at $20 a month. There may be cheaper offerings than your current host. 

I just installed RHEL 7.1 on a server. I'm using it to study for the RHCSA/RHCE exams. One of the steps in the study guide is to install a VM through using installation media on an FTP server. The OS variant list only goes up to RHEL 7.0. I attempted to install 7.1 but it threw an error every time: 

Looking at the man page for either mail or sendmail has not clued me into which option I need to use. I did see in the sendmail manpage that there is the -O option which uses the option=value format. I tried that instead of the -S listed above. Still no dice. The manpage even says the -O option is ignored. Can someone help me out with this? 

I'm trying to figure out how to get some of our servers to send email through our relay when I use the mail command. If I simply type 

I've also tried without the root= or init= options with the same result. I'm not even entirely sure about those options. Not much that I've found online gives much detail or explanation. One thing I know for sure, is that right now I don't want to enable the autonuke option. I simply want to load up the interactive method. Has anyone been able to make this work? EDIT: Potentially pertinent information The server that I'm testing this on (and, in fact, 99% of our servers) are VMs running on VMware ESXi. The CD works when I attach it as an ISO to the VMware console and boot to it. 

With that many systems two factor authentication via LDAP would work well. With one factor being an RSA secureid. If a cracker were to discover a user password they would still need at least three RSA number generations in a row before being able to duplicate future generations. Strong password policies to boot. 

As warner mentioned (that's becoming familiar), zone transfers are othen denied for security reasons. If the name servers aren't something you have access to you can attempt to discover the most common subdomains of a given domain using one of the popular DNS bruteforce scripts. They work by performing DNS requests against a local nameserver using a user supplied dictionary list. Dictionary lists exist solely for this purpose. 

So I need to install the original /etc/pam.d/system-auth-ac file from the authconfig RPM. I attempted to do so by "reinstalling" the authconfig package/rpm like so: 

That will list which services are listening on which IP:port. You may need to modify apache's listen directive like so: Listen 0.0.0.0:80 And verify iptables is configured to accept traffic destined to port 80. If the server is setting on an internal network where you do not have access or permission to modify the router...then you should not attempt to make that server available publicly. It would most likely be against company policy if you could get it to work. Also, instead of giving out your home IP address to everyone (since it's going to change/expire at some point) you can instead give out a dyndns address. See dyndns.org If this wasn't what your looking for then please give additional detail in your question. 

When I created the kickstart file I had the directive uncommented. It failed but I did not think to look at the details so I don't know why. The Red Hat knowledgebase article omitted this directive so I commented it out. The issue persisted. That said, now that it is again uncommented and allowed to be executed as part of the kickstart it is working. So, since I didn't look at the details when I first encountered this error I don't know what the problem originally was. 

I posted it over at $URL$ as suggested by Sirex in chat. It was pointed out that I had renamed the file module manifest to users.pp, but it should have been init.pp as per the automatic loading of classes and subclasses that is built into Puppet. 

I'm disabling on a RHEL6 server and enabling . In the file I have commented out the line and replaced it with . Unfortunately, this leaves the message hanging on a command prompt instead displaying the message and then dropping to a new prompt. I've tried and added a newline at the end of the string, but that still won't present a new prompt after displaying the message. I've tried using to simply send the message to STDOUT, but that does not display anything at all. A simple solution would be to add text indicating that the user needs to press Enter to return to a prompt, but I'd prefer this happen on its own. How do I echo a message and then return to a prompt? 

I've been fortunate enough to make it four years as an SA without having to administer systems making use of Java. My luck has run up and I am starting a new job and they make extensive use of Java. What resources are available for a SA who needs to troubleshoot and administer java applications and environments? I didn't go to stackoverflow because I want fellow SA perspectives. 

It's a best practice to have the owner be whatever limited user account is used for uploading/managing the files on the server. The group is often the account that php is running under, so in this case apache would be correct. The other permissions should be set to nothing, as they are. You are close to perfect. If you have a situation where multiple accounts may be modifying/editing the files you can creat a cron script that chowns the dir recursively every hour or so to maintain correct ownership. The same technique works to keep the permissions correct as well. Also, you may want to modify the umask of the limited user account that has ownership to be inline with your permission scheme. 

Does the originating/source IP show up in that log output? If yes does that IP show any valid requests in the http logs? Perhaps a monitoring system of some kind is checking http on your server, since you said it was in consistent intervals. Just throwing stuff out there. 

We statically assign our routes using the /etc/sysconfig/network-scripts/route-ethx files. This makes managing them fairly easy since we add the routes during the kickstart process (by way of post-script). They rarely change and if they need to be updated we simply push out a change and update the build scripts to include the new route. Recently, we had a customer ask for a virtual interface (eth0:0) with a unique IP. It needs to connect exclusively to one network. The first assigned IP on eth0 will handle all other traffic. Everything I've seen states how to add a route using , however I can't sort out how to specify that anything going to a particular IP or network can have the source IP set. If I use the ip command will it update the route files or is it stored elsewhere for persistence across reboots? If I have to add the entry to the route file for eth0:0 what should the line look like? So basically: If it goes to 10.0.0.2 use IP 10.0.1.3 as the source. If it goes anywhere else use IP 10.0.1.2 as the source. Is this possible? How do I accomplish it? 

I've also explicitly set despite it being enabled by default. Additionally, I've removed the option on the master to ensure that all hosts are able to dynamically update. Replication still isn't occurring. I am, however, seeing that the master is sending the NOTIFY announcements: 

Run the above on the server and it should list which services are running and which ports they are "bound" to. If 80 is not listed then you need to troubleshoot the apache startup. 

I know some people are using keepalived and heartbeat for active/standby but what action is taken if the haproxy process were to die? What would be nice is if the virtual IP would switch servers if the haproxy process were to die and/or a networking issue were to occur. We are currently investigating heartbeat and corosync with pacemaker. Can anyone explain their solution to this issue in depth? UPDATE: Thanks Kyle, see answer and links therein. 

which upon search path resolution becomes the same fqdn. Anyone have a way to expand shortnames before calling ssh, or an alternative approach? 

In the iptables output make sure there are not any rules rejecting connections before the accept rule. Also, try verifying it's accessible from another computer on the same network before trying outside the network (through the router). 

I'm coming from munin and a CPU graph contains data for system, user, nice, etc ALL on one graph. I just installed ganglia and setup the basic monitoring. It appears that each type of cpu data is a separate graph! WTF is this and can I change the defaults to combine these into a single per host? That is my question, how do I combine cpu data into a single graph. Also, can I change the layout to something closer to munin's day-week side-by-side layout? I'm trying to be impartial and give ganglia a chance. ;) 

I work on a platform which serves several different projects. For the most part, all of the projects install their web server using Red Hat's RPM. We never created a policy for this which would require the projects' application developers to install from either source or in-house binary/tarball. This is causing issues now because as we patch servers with security updates, the Red Hat httpd package gets updated with configuration files that are either not originally part of the project's configuration or overwrite the configuration. We need evidence or supporting documentation which points to a safer, more stable option which can be written into policy. Is there any documentation which can provide this support? I'm looking through Google results, but my Google-fu is weak so it requires sifting out the cruft. If someone here has a source they can point me to directly I would appreciate it. 

We've recently gone through a security audit and among several valid and other, quite pointless findings is one that states that tcpwrappers is not disabled. I've never used tcpwrappers so I don't have a great deal of experience in how it is configured. Having said that I've always been under the impression that it isn't simply disabled or enabled like a daemon. Instead, rules are created which define services that use it and which servers are and are not allowed to access the service. By default, if nothing is defined then tcpwrappers is, effectively, disabled. Am I wrong?