You have now loaded the SQL Powershell Module and have been switched to the SQLSERVER drive. Let's see what we can retrieve from the system: 

Public database role There is also a database role, which is somehow linked to the server role. If you query the (view) in the master database, then you will see that there is a principal with the same id as the server role which is 0. I am assuming that this is the missing link, between the database_role public and the server_role public. Edit: Added some information regarding the database role "public" 

Corruption of the tablespaces is apparently a normal thing. It happens every now and then and can be fixed following these steps: Find the corrupt block (V$DATABASE_BLOCK_CORRUPTION) Run the following script to retrieve the blocks that are corrupt: 

Yes, I do realise that all paths are on C:; it is just an example The white paper by Paul Randall et al. goes on to explaing that... 

In your example the should be the IP address of the application server or if the server has DNS resolution it can be the hostname ( record or ) of the application server. E.g. Given the following values: 

You could add a variable to your stored procedures and then code around the important bits when you require relevant information. Your stored procedure would then look a bit like this: 

So depending on the size of the databases and the amount of changing data, you could end up needing quite a lot of disk space for your transcation log backups. Then again, having a short retention period in your maintenance plan could result in lower disk space requirements. Pssst, after reading the Microsoft article, have a look at some alternative backup solutions, like Ola's Maintenance script or other alternative solutions. 

I then queried the TEST database with the Windows login DOMAIN\USER. Because the DOMAIN\USER is also memmber of DOMAIN\GROUP the user is allowed to SELECT from all tables in the TEST database. You are writing that the DOMAIN\USER is unable to select any data. Correct? Do you have any orphaned users on the database level? Follow these steps to check. 

The SYSTEM and SYSAUX tablespaces are reuqired by the RDBMS to function correctly. The SYSAUX tablespace A list of objects that reside in the SYSAUX tablespace can be retrieved using the V$SYSAUX_OCCUPANTS view. This view displays the following information: 

When you create a user in SQL Server Management Studio (SSMS) with the graphical user interface, you are initially creating the SQL Server Login (Windows authentication or SQL Server authentication). Differences at this stage are: 

Enabling Mixed Mode (aka, SQL Server and Windows Authentication mode) will get rid of this error message. 

In General I would consider reading up on what information the master database contains and why you need to backup the system databases (master, msdb, ...). Reference: Back Up and Restore of System Databases (SQL Server) (Microsoft Docs) master Database The master database contains the following information for each SQL Server instance: 

No, database mirroring is not dependent on the SQL Server Agent running. There is however a job that is scheduled to run every 1 minute to update the monitoring status of the mirrored databases. This is all it does: 

You might want to read up on the concept of Transaction Log Backups in the following Microsoft Technet Article: SQL Server - Understanding SQL Server Backups If you set up a maintenance plan that creates a transaction log backup of all your user databases, then the backup job will create a *.TRN (default extension) file for each database and for each scheduled run. Depending on the retention period you set in your maintenance plan, you will/can have literally thousands of transaction log backup files (*.trn) in your backup location. E.g. Backing up a 10 user databases every 15 minutes and leaving the retention period for clean-up to 30 days, will leave you with 28'800 transaction log backup files lying around. 

It depends on what you need, the basic data for conenctions can all be found in table Main points of reference are 

The problem you have is that the where clause is executed before the Select clause of a query, so when it hits the where section it has no concept of what the muni_score is There are two simple workarounds (not taking any performance into account here) and I'll note down a slightly more complex one. The easiest is to simply replace the muni_score in the where clause with what makes is up, so: 

SQL Server is designed (along with most db engines) with security in mind the main areas you'll look at for viewing users are 

I'm not sure what your dynamic columns are so I cant 100% promise this will work, but if you get any column errors you should just be able to put ts. infront of it to declare its from that table Ste 

Your transactionlog backup doesn't clear out the transaction log while the full backup is taking place, this is down to how the backups are organised requiring a full backup to then have the transaction log to work from. When you start a full backup that is the new point in time that the tLog backups will work from, but they cant work until that backup is complete, so during this time the transaction log grows, once the full backup is complete you now have a new point in time and the tLog backups will now be able to actually release the data from the transaction log and it will return to its normal usage levels 

User 2 can see the entry created by user 1 Now user 1 disconnects or runs a forced rollback their entries to the table are removed so if user 2 re-runs their query the data is no longer there 

The only query this would really benefit would be counting the male / female ratio. examples of queries your trying to run would be useful to give examples of what would help. the issue you have is an index that is basically a bit is that you put half of your data in one half, and half in the other half, there isn't actually any organisation to that index, it is barely any better than a heap, Consider your table as a multiple deck of cards(distinguishable by different backs), and you're looking for the 6 of hearts, the fact that its red(female) only helps you find all the red cards in those decks, from there, you still need hearts, and the fact that its a six, and which colour back it is (address = colour of back, age = number) both those other factors will narrow down your search far more than is it red or black. On the note of speed increase, technically yes your speed for inserting / deleting will increase (update shouldn't as you should very very rarely change someone's gender) the increase in speed will be minimal however since there isn't really any sorting going on to the index. I imagine the index is quite small since if it is M/F then every record is one character and the lookup to the main data. 

Gives the list of all the IDs cursor through those values and simply run on each of them. After the process has finished the temp DB space will loosen up (Note it does not do it as you close them, it seems to do it in large chunks when you're no longer working on creating / ending them (I cant guarantee this is how it works, just what I observed after stopping mid process and tempDB getting some space back) 

I've been researching slow running queries in our database, and have concluded that this is a classic Ascending Key Problem. As new rows are inserted almost constantly, and a given piece of SQL to pull the latest data out of the DB runs every 30 minutes, the first option of updating stats every 30 minutes sounded like it could be a waste of resource. So, I looked into Trace Flag 2389 which in principle should help, however that requires the Leading column to be branded as Ascending, and when I used Trace Flag 2388 to check the (PK) index stats, I see that the leading column is actually branded as Stationary - as it is for several of the PK indexes on other tables updated at the same time. 

There doesn't seem to be much guidance on what results in a branding of Stationary, however I did find KB2952101 that says if less than 90% of inserts were greater than the old maximum value, it would be classed as Stationary. All our inserts are new submissions, and the leading column is a bigint IDENTITY column, so 100% of inserts should be greater than the maximum previous value. So my question is why would the column be branded as Stationary, when it is obviously Ascending? An earlier attempt to resolve this problem for some daily running SQL (which worked really well) resulted in a job being setup to update statistics for this table nightly. The update doesn't do a FULLSCAN, so could it be that the sampled scan is missing the new rows sometimes, so it's not always showing as ascending? The only other thing I can think of that might affect this, is that we have an archive job running behind the scenes deleting rows over a certain age. Could this have an affect on the branding? The server is SQL Server 2012 SP1. Update: Another day, another stats update - same stationary branding. There have been 28049 new inserts since the previous stats update. Each row has a timestamp of when it was inserted, so if I select max(id) from table where timestamp < '20161102' I get 23313455 Similarly, if I do that for when the stats were updated today, I get 23341504. The difference between these is the 28049 new inserts, so as you can see, all the new inserts were given new ascending keys (as expected), suggesting the leading column should be branded as ascending rather than stationary. During the same period, our archiving job deleted 213,629 rows (we're slowly clearing old data). Is there any chance that a reducing rowcount could contribute to the stationary branding? I've tested this before and it didn't look like it made any difference. Update 2: Another day, another stats update, and the column is now flagged as Ascending! As per the theory about deletes affecting this, I checked the percentage of updates being inserts compared to deletes, and yesterday 13% were inserts, whereas the previous two days inserts accounted for about 12%. I don't think that gives us anything conclusive. Interestingly, a related table that gets on average 4 rows inserted for each row inserted into this main table, and has it's stats updated at the same time, has it's IDENTITY PK column still as Stationary!? Update 3: Over the weekend we get more inserts. This morning the leading column is back to Stationary. On the last stats update, we had 46840 inserts and only 34776 deletes. Again, interestingly, the related table I mentioned above now has it's leading column branded as Ascending. Is there no documentation that can explain this? Update 4: It's been a week or so now, the archiving job has cleared the backlog, so we're consistently deleting about two thirds of the number of rows being inserted. The stats are showing mixed results across the related tables, with one showing stationary, and two showing ascending, despite them all being updated proportionally similarly. 

Note, are you wanting those default values in there? would make more sense (in my opinion) to have it as a required field on the execution of the proc so you don't accidentally run it with no values and presume its right 

TLDR; Check for conversations being left open completely. In our system we re-use conversations and have a table dedicated to holding these conversations that are usable, however the dev team setup a new service broker without my knowledge ages ago while I was off, didn't set up these conversation points and didn't set any thresholds on the alerting. When the new system has been turned on the conversations are being opened but not closed properly and since there aren't any in the pool it is just creating a new conversation (we reached 7.1 million conversations for one service broker) My steps for fixing was to create and record the 20 conversation handlers that I require for that service broker and record them into our table. This stopped the growth of the tempDB to stop the risk of the DB going down. Then came the long process of closing off all the un-used conversations 

I'm looking to automate a wipe of our test environment to bring it up to be in-line with production at the end of every sprint. As it stands this is currently being completed by the test enviroment grabbing the full backup and doing a restore with move&replace however this is using up most of the space we have. The intention is to create the environment from scratch and populate only the needed tables. I can use the 'Right Click > Tasks > Generate Scripts...' to create the framework of the database. Is there a way to create that script from within a stored procedure so that can be used to re-create the database Also I'm working on this bit as it stands but obviously that script will just create the database at its current size so all of the File Sizes need to be modified once the script has been generated Cheers for any help 

This will, if the table exists drop it, if it doesn't exist wont so you don't get the error. I'd also do your table declarations outside of the 'into' statements and run it as I had some issues around inserting temp tables a while ago, both methods help different aspects 

I may be understanding this slightly wrong so I apologise if I am. If you have two databases that you need to be identical and are on 2014 then use the AlwaysOn High Availability Group. Since you're data centres are at separate locations use the Async mode This will mean the database is kept completely up to date (all be it possibly with a few second delay) and you can have the secondary node as a read-only replica, this means that your alarm system can read into that database run all the checks etc you would normally. the Always on system keeps everything up to date, so if the connection drops, when it comes back online it will merge over all changes It also means that if your main centre goes down you can set it to automatically failover to the secondary, when the main datacentre comes back online it will re-sync with the (now) primary node, at which point you can fail it back over to your main centre. You can run this on multiple databases, so we have our main DB and our Admin DB synced across our nodes, however what runs all the jobs and direct actions on each side is not replicated so can stay independent of each other