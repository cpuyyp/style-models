I am Avinash, I have a slight confusion about incremental backups. I am going through the MySQL 5.7 Docs to study the backup process using mysqldump. I am only willing to have Point in Time Recovery capabilities. I am aware of the xtrabackup tool, but I want to use the logical backups as well. The Docs explain how to make a full backup of InnoDB tables 

The biggest table size is around 97 GB, for now, which is expected to grow more and more. I was asked to look into some compression options for all the 362 tables in order to reduce the size of the database. I took the biggest table which is 97GB in size and has 208 columns. On Oracle out of the 208 columns 142 columns were VARCHAR2(500) and 50 columns were VARCHAR2(4000) and when the database was migrated, because of row size limit imposed in MySQL, a lot of VARCHAR2 columns were converted to TEXT. I don't if the database was designed properly or not because rest of the tables don't have so many columns in them. When I tried to compress this table (on test server) using , there was no difference in the size of the table. KEY_BLOCK_SIZE=8 didn't work, it threw an error regarding row size limit, hence I chose KEY_BLOCK_SIZE=16. I took another table, which doesn't have any TEXT columns, and it got compressed to 50% and it doesn't have too many columns in it. I don't understand what could go wrong with the 97GB table. Is it because of the TEXT column or could it be anything else? Well I read that the TEXT and BLOB columns are best candidates for compression. Is there any other method to compress the TEXT columns? Thanking you. 

In theory, the data model allows for phones to be related to more than one patient. We did have some issues in the past, and most of the phone numbers have been denormalized. I need help to understand: 

I am planning a migration to 2016 Reporting Services and have noticed a particular problem. On two separate new installs for SSRS 2016 Developer Edition, I am unable to save a subscription to a report. The report was downloaded as RDL from the current 2008 R2 server, then saved to the new 2016 environment. The new server is build 13.0.4466.4. I also installed the same version of SQL onto a second server and have reproduced this problem. All servers are running in Native Mode. Within Report Manager, I am able to view the contents of the report (on-demand). When I try to create a subscription, the web page is stuck with the "Loading...." prompt after I click 'Create subscription'. It does this regardless of the Render Format I choose. I am attempting to deliver via e-mail. I have domain credentials for the SSRS service account configured, and I have put the IP address for the SMTP server in RS Configuration Manager, so I know it is configured for email ok. In the SSRS log file, I see these types of errors at the time I try to create the subscription: 

in the first command the author uses only --master-data and in the second command he/she uses --master-data=2. I don't understand the difference. Could you experts please help me out? Does --master-data=2 indicate that it's an incremental backup of level 1? Does it indicate anything related to master-slave configuration? What command should I use to make incremental backups? The second command again mentions backup of Sunday itself, that's why may be I am getting confused. The author uses Sunday as full backup (in the first command) and under incremental backup (for Monday) explanation (second command) also he uses Sunday in the dump file name. Thanking you. Regards, Avinash 

I am working on installed on RHEL7. The database has been migrated from Oracle to MySQL and I don't know how it was migrated. 

I am Avinash. I am willing to install an additional MySQL instance on a UAT server for testing purposes. I am using MySQL Community Server 5.5.58 installed on RHEL7.2. I am going through the MySQL Docs $URL$ , which tells me that I would require certain unique operating parameters such port, socket, pid, datadir and log files related parameters. To achieve better performance, it also recommends to use different tmpdir. This is well understood. I can either use different binaries or the same binary for this purpose. If I am using same binary, I'll have to use mysqld_multi and use the same config file. If I use mysqld_multi, I'll have to put different sections for mysqld daemon such as mysqld1, mysqld2 and so on. All using unique values for above mentioned operating parameters. Please help me out here. Am I correct about mysqld_multi? My second question is about bind-address variable. $URL$ Here, it explains that My question is that will I need an additional NIC and an additional IP address for the second MySQL instance? For the existing instance, I have disabled the bind-address option. Do I need to add the bind-addresses for both the instances. Also, the second instance will be used only by me (for now). Thanking you. 

Sorry if this is a noob question. I have transaction log shipping set up for a failover scenario. All the databases are in 'Restoring...' state until I do . At this time I disable log shipping. Once I have resolved the situation and I want to enable log shipping and carry on again. Do I need to run a command to set them back to Restoring or do I leave it and the log shipping will automatically sort it out? 

I was log shipping. I disabled the jobs, and restored the database on the secondary machine and used them for a bit. Now I want to undo this. I don't care about history or syncing, I simply want to turn log shipping back on and carry on my merry way. I enabled the jobs, but 'running' them (right click > start job at step, if that's how you run them) just pops up and says success but nothing happens. I am unsure of how I can start the log shipping going again. I tried deleting the databases from the secondary machine but that didn't help either. Any ideas? Surely it's a simple thing? Edit: Still at a loss. Says the job was successful but nothing happens, no database is created. It was working fine before I took the databases out of recovery on the secondary machine. 

PowerBI Integration is not enabled on either of the 2016 servers, and is obviously not a part of the existing 2008 R2 configuration. I do not wish to use this feature. How do I solve this problem and begin to create subscriptions, so I can ultimately migrate all of the reports? 

Thousands of records I did NOT want to update were updated. Many of the other groups of Patients were NOT updated. 

I would tend to believe the data (phones being shared) is likely the cause of this problem, but I am having trouble proving that. Can someone help me with this? Thanks for your time. 

The relationship: Patient to PatientPhone - 1 to Many Phone to PatientPhone - 1 to Many In order to validate before and after, I have been using this query to sample the phone numbers by grouping the first couple of digits. It gives me a wide distribution of values, as I would expect: 

Patients table - ID as the key, and GroupID to differentiate groups of patients. Also has an IsDeleted field (bit) to indicate soft deletion.t item Phone table - ID as the key, and has three business fields: AreaCode, PhoneNumber, and Extension. Also has an IsDeleted field (bit) to indicate soft deletion. PatientPhone - Has an ID as the key, but also uses the combination (PatientID, PhoneID) to link these tables. 

I set up a job, and in Job Properties > Notifications I checked E-mail When the job fails. I'm wondering if instead of this e-mail, I can make it run an .exe or similar with the details. The reason is I have a piece of custom monitoring software I wrote for other applications, and I want the events to go through that instead. Is this possible somehow? Or should I write to the event log and just try to parse it? What is net send, could that do it? Thanks 

I have some code that is using entity framework to perform a large number of INSERTs in succession. SQL profiler shows a SELECT, then another SELECT, then an INSERT for each one due to it having to read some data first. These occur very quickly so I can see my console logging program scrolling like crazy. Every so often, it will completely stop for a few seconds, then it will resume again. There is no pattern to it. I've had it do all 450,000 inserts without any issues, but sometimes, I find that it pauses for long enough to trip the timeout on the sql command. I'm running it through the profiler and examining the results, and I can see sometimes that the duration is very high for what is a very small insert query that should be instant. Is there a reason why it pauses every so often? Could it be something like garbage collection or something internal? I don't really want to increase the timeout as a finger in your ears approach to solving the issue. And I can't turn the inserts into a BULK operation very easily as they are all distinct calls to a service that logs every time it is called. I am wondering if there's something obvious that may be the culprit. Any thoughts/speculation on this are welcome. 

Thanks to the suggestions from others, and I appreciate the time anyone took reading this post. So after looking at this different ways, and taking several breaks, I now see that my validation query was at fault. The join was incorrect. The Update statement was designed correctly, I verified the data was valid in the tables, the relationships correct, and I was able to verify at a detailed level the update worked. Something about my validation query (that did aggregations) seemed off. My validation was incorrectly joining against the wrong foreign key. The join: 

My understanding of SQL security is such that in order to have access to the server, you need a login. To access a database on that server, your login needs to be associated with a user in that database. Over time I have been consolidating security so that developers, QA, etc. are in various roles implemented as active directory groups. Those groups have logins on the server, so individual windows logins are not so prevalent on SQL instances. I grant proper accesses to the databases through these roles. A common service I provide is to refresh non-production versions of databases from production. I'll do a backup and restore of a production database onto a development server, de-identify the data, rename it, etc. I have noticed something recently in my environment that is troubling. I was granting access to a key table in production for a development team, when I noticed by using 'fn_my_permissions' that one of the developers has read and write access to the entire production database, rather than being restricted to read-only as I designed. I compared all of this developer's group memberships and found another group he was a member in that was assigned read/write - but only as a database user. There is no associated login for that group on that SQL server. It is typical here to copy a 'baseline' production database and repurpose it elsewhere - whether as another production database, or a non-production version. I am starting to find other examples of this anomaly. To sum it up, the problem I see is that when a database is copied to another server, the users in that database that were previously linked to logins on the original SQL instance seem to retain the same level of access on the new SQL instance - even if there is no related login for them on new SQL instance! This strange effect seems limited to users representing windows group logins. Here is a summary of what I am seeing: Server1 has a login tied to windows group A, which has been granted read/write to database X. Database X is then backed up and restored onto Server2, with logins but none related to group A. The database still shows a user for group A, and when I run 'fn_my_permissions' for a user in that group he has read/write to the database. This particular server is running SQL Enterprise 2012 SP3 with the latest CU. I've seen the same thing on servers running 2008 R2, as well as a current build of SQL 2016. Why is this happening? And what can I do to ensure that users do not inadvertently circumvent the controls I have put in place? It seems obvious I need to remove all of the unnecessary users after the databases are copied, but why?