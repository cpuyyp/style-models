The history table columns will be and I am going to save a lot of space versus ordinary implementation which is logging all data (this is due to my test and my bussness cases). As I the supports I am wondering are their any pros/cons/differences between using it and trigger-based logging? I have check few artciles (here and here) and cannot see what more can give me. 

Case scenario The functionality is used to encrypted all data in a particular column. I need to encrypted only some of the data, so I need to use a or . I want to protect them with a certificate which is stored in the in order to control the access to the keys from there. Is this possible to create such certificate? 

As it is said here it is a good practice to backup the database master key, because when the database is moved to another instance it will need the original DMK that has been used to protect the certificates in order to skip regeneration of the encrypted stuff. In the context of database backup encryption, when the DMK is stored in the database with the rest of the certificates used for backup encryption, do we need to create a backup of the DMK? Also, I am wondering, as it said that 

I have two tables containing exactly the same data. Both table have , 60 columns and 300 000 rows. The difference is that all of the columns of the second table have type. I am creating temporary table and importing the data from the two tables in it. When data is extracted from the columns it is cast to the corresponding SQL type. The data extraction from the first table is performed for and from the second table for . Basically, the differences in the executions are in the estimations: 

Is there a way to find the T-SQL statement that is part of the active transaction that is blocking the reuse of the transaction log file VLFs? I am performing tlog backups on every 15 minutes, but sometimes the following query returns or (even such is made): 

Something tells me that I am taking the wrong approach. Should I be creating the database with a script instead of restoring from a .BAK? Can I use an account with less privileges than SA to execute all of these things, and if so what privileges would this account require? Should I remove these things from the installer package and have a separate script that can be run under the SA or another account that has the required privileges? What I am looking for is a process that will create what I need with minimal privileges. If possible, I would like to minimise the changes required to the process I am currently using. 

It was a database scoped trigger which may or may not explain why I couldn't see it in sys.objects. This didn't work: 

I have a database trigger called ddl_refresh_views that provides dynamic binding for views (if you alter view V, it refreshes all the views dependent on V, and all the views dependent on those views etc). This triggers functions as intended, but for some reason: 

There are a couple of issues here. Firstly, some errors terminate the current statement and some (an inconsistent and rare few) terminate the whole batch. set XACT_ABORT on makes statement terminating errors become batch aborting errors (which is good because it forces some consistency). The problem here is that each of these go statements mark the beginning and ending of a batch. When a batch-aborting error occurs, I believe that SQL is reverting to either the start of the batch or the transaction beginning. Also, because the table create is in a subsequent batch, it is executed just fine. As mentioned by the other answer, preventing errors is better than detecting them. The drop table should be proceeded by an if statement that checks to see if the table exists before dropping it. 

I have a table-valued function and ordinary table and need to perform on them. Because, the engine thinks that the function is returning always one row, I try to store the results in temporary table and then to perform the join operation - in both cases, the count that are read from the table/index is wrong. 

I want to either compressed them or move them to cheaper(slow) storage. Because of the tables structure (there are columns) I was not able to apply or achieved good results using row, page, column store or column store archive compressions. So, I have decided to move them to cheaper storage. I guess I have to options here, but let me know if I am wrong about something: 

I am going to test the both variants of course but before doing that I am wondering is there another option or any issues in the above ones? 

As you can see each entry is updated in different times on different date. Could anyone tell why this is happening? 

Add column to the table called with default constraint and When data is going to be encrypted the flag is raised and the output encrypted value will be set in the existing column 

I want to execute the code above after each update, but sometimes I need to execute the command above many times in order to reduce the database tlog file (each time I have to wait 15 minutes for the tlog backup to complete). The database are in mode with , end the tlog backups are made on the secondary databases. Could anyone tell if the idea for regular shrink is bad and why sometimes I need to executed the shrink several times? 

Does this mean, that if I used same password to create and protect each DMK in my databases across all instances, the key will be the same? If not, it is OK to create a DMK on one database, and to restore it on all instances in order to have the same key everywhere and to need make a backup of only one key? 

We had our MySQL server on an old server. We decided to move all of the databases to our new server. Old MySQL Server engine 5.5.31 New MySQL Server engine 5.5.38 We created one general dump file without logs from all of the Databases in the OLD SERVER and we restored the .sql file in the newly created MySQL Server on the new server. On the new server when we put skip-name-resolve option in the my.ini everything is fine but we can't login to MySQL server with root account. We tested the issue on old server and we didn't have any issue with skip-name-resolv We removed "skip-name-resolve" option and we tried to login to MySQL server. after login when we tried to list the available users. Like previous MySQL server here after restoring the DUMP we have two root users when we execute: select user,host from mysql.user; For the first one % is defined as host and for the second one LOCALHOST is defined as host. All of the users are defined with % as their host.We want to use "skip-name-resolve" and one another hand resolve our issue with root account. 

I have a SQL 2008 Express with 80 databases in it. I want to move these databases completely to another server which is hosting a SQL 2008R2 Enterprise instance. As i'm told it is better to script the databases and run the script on the new instance. I can do this in 2 ways: 1. Running a script on the 2008 server to get all of users and passwords and user mapping to databases and then Script all of the databases (without users information) and then restore databases on new instance and then run the users script for logings and users mapping to databases. i have been told it is a better way -2: Script all of the databases using Script to WIZARD and check-mark DATA+SCHEMA+USERS and then script the database to .SQL file and run this SQL script on the new instance. What is your suggestions? Which is the best way to this without facing any issue. Kind regards 

and using command. In both cases it takes more then 19 hours. Is there a way to optimize the clustered index rebuild operation? 

I have a primary database in recovery mode which is part of group. Is there a way to minimally log insert operation under recovery model? I have a process that is executed each day and insert few millions of records in a table. While the operations continued the transaction log file size is increased dramatically ( from 1 GB to 40 GB). As I have read I can used some variations of which are not fully logging the operation but I am concern about the effect of switching the recovery model? 

I want to query spatial data for nearest neighbor. I am using this article and the following query works perfectly: 

I have never used with precision. So, I went back to the source of this format - Alter Non-Temporal Table to be System-Versioned Temporal Table. The only difference with the script I have been using was the date time function. I used as I do not need to be so precised with ( for example) and in the example it is . So, I have changed it. I have create a script which is dropping a database if exists, restoring a database from backup and then executing my code in a loop (as I said I get the error sometimes and it was very hard to reproduce it). I have run a the script a lot of times and I was getting different number of fails (sometimes 70%, sometimes 50%, sometimes below): 

I am trying to understand what process is making my transaction log file to grow. For example, currently it is 120 GB and only 5-10 MB of it are in use. Before: 

When the temporary table is used, the correct count value is used for the rows returned by the function. All involved indexes are rebuilt. Why I am worried about this? This join is part of more complex query which execution is very slow. I believe this is due to the fact that the statistics that are used by the engine are wrong and not the optimal execution plan is used. Even when the query use parallelism again nested loop is used to join the tables. If I use a or hints on join the correct statistics are used and better execution plan is built but I prefer to find a way to provide the correct statistics instead of forcing the engine to do something. Could anyone tell why always one row is expected? Maybe because table-valued function is used, the engine thinks one row is going to be read from the table/index?