You could collect your time points into half-open intervals like $\dots, [20, 25), [25, 30), [30, 35), \dots$ and correlate the average observations for and . An example with a few more data points: 

(You can also see that 2. and 3. are true directly from the definition of $d(x,y)^2$). Looks fine so far. But: 

For the given decision function and , we get a prediction accuracy of 84%. This is not terribly good, and it is of course specific to our decision function, which seems fairly easy to predict. To see this, define a different decision function: 

Kernels are considered valid if they are positive-definite, so we must have $$ \sum_{i=1}^n \sum_{j=1}^n c_i c_j K(x_i, x_j) \ge 0 $$ for all $n \in \mathbb{N}$ and $c_1, \dots, c_n \in \mathbb{R}$ and (in your case) all $x^1, \dots, x^n \in \mathbb{R}^2.$ Letting $n=1$ and $c_1 = 1$ and $x^1 = (0, 1)$ shows that $K$ is not positive-definite, since $$ K\big((0,1), (0,1)\big) = 0\times 0 - 1\times 1 = -1. $$ The problem with your approach is that you define $\Phi$ on $\mathbb{C}$ but write $K$ as the dot product for real-valued vectors. The feature map argument for positive definite vectors only works if the product is an inner product on $\Phi$'s codomain. The dot product for real-valued vectors is not an inner product on $\mathbb{C}^2$ (because $i\cdot i = -1 < 0$). You would have to use the generalized dot product $x \cdot y = \sum_i x_i \overline{y_i}$, or another inner product on $\mathbb{C}^2$. 

I suggest you take a look at the Python library Spacy it has a model for spanish language that includes NER. 

If want you mean by 100% certainty is: 1, 0, 0, 0, as by your example, you cannot obtain this type of output using soft-max. For this to happen you will need that the softmax equals 0 for those i that are not correct, and this simply cannot happen because: $$ e^{h_i} \neq 0 $$ $$ \forall h_i \in \mathbb{R} $$ See this link for better explanation. Note that $ e^x $ can be close to 0 but it will never be 0. The kind of output that you want is the one that the max function would output, sadly the max function is not differentiable so a soft version is needed hence softmax. I think that the problem is that you are using softmax after tanh, and this so you must eliminate one of the "squashing" functions, see this discussion The reason to use softmax is if the classes are mutually exclusives, also one of the advantages of using softmax is that it reduces sensitivity to outliers by giving at least some probability to the other values. See this links for a more detailed explanation 1 and 2. 

First, choose the default value $x=1$ and compute $f(x=1, y=0) = 0.2$ and $f(x=1, y=1) = 0.3$. Second, choose the default value $y=1$ and compute $f(x=0,y=1)=0.1$ and $f(x=1, y=1) = 0.3$. Change the default values to the best value. In this case, this requires no change since $x=1$ and $y=1$ are the best values, respectively. The result did not change. Report $(x=1, y=1)$ as the best parameter combination. 

Let me outline an algorithm that deals with both challenges. The prediction accuracy is not very good; but maybe you see a way how it can be improved. And at least it predicts something, right? 1. Simulating samples To be able to test the algorithm, I wrote functions that generate samples and labels. Generating samples: Each sample contains between 3 and 10 points. The number of points is random, drawn from a uniform distribution. Each point is of the form . The coordinates are again random, drawn from a normal distribution. 

3. Create the dictionary of hashed words. The dictionary maps each tuple to the set of corresponding words. 

For more information on soft-max look at the following links: 1, 2 and 3. For a step-by-step guide, including usage in Python see this reference 4. For more on soft-max vs sigmoid check this: 5, 6 and this 7. If you want a more reliable source on why to use soft-max regression for mutually exclusive classes you can look here. This page is part of Unsupervised Feature Learning and Deep Learning Tutorial of Stanford University, it contains material contributed by Andrew Ng and others. 

I will try to illustrate the lambda collocation metric, first we have to define a function c(x, z) that receives two K-word expressions where x is the K-word expression you want to score and z corresponds to all possible values of a K-word expression in the corpus. c(x, z) returns a binary vector (j1, ..., jk) such that ji = 1 if xi == zi, 0 otherwise. Now a binary vector can represent a decimal number up to 2^K = M, so c(x, z) = ci means that the binary vector returned was equal to the binary representation of the decimal number i. Now as it says on the documentation ni corresponds to the number of times the c(x, z) function was equal to ci through the corpus. Now an example, consider the following sentence: 

Apparently the LSTM cannot deal with a drift. What to do? (Yes, in this toy example I could simply subtract the drift; but for real-world time series, this is much harder). Maybe I could run my LSTM on the difference $X_{t} - X_{t-1}$ instead of the original time series $X_t$. This will remove any constant drift from the time series. But running the LSTM on the differenced time series does not work at all: 

Have you tried the boring, straightforward approach? Get a list of all words and count how often they occur with a high or a low label. (Exclude words that occur only once or twice, and also the words that occur very often). For example: 

Media's explanation is true for regression problems. These are problems where you predict a continuous target variable. Your image shows a classification problem. Here, the target variable takes only two values (typically and in the Perceptron algorithm). In that case, an optimal solution $w^*$ is a vector of weights that perfectly separates both classes. If such a solution exists, the Perceptron algorithm will find it. But: If there is one optimal solution, there are usually infinitely many other optimal solutions. You can easily see this in your image: You can move the line a little to the left or to the right, and you can rotate it a little, and it still perfectly separates the classes. So while the Perceptron algorithm will find an optimal solution if there is one, you cannot know which one it will find. That depends on the random starting parameters. This is different e.g. for support vector machines. Here, there is either no optimal solution or exactly one optimal solution. 

If I understood correctly your question, you want a function that takes a signal (a fixed window) and outputs a 32-bit representation in a way that the correlation between it and any other signal is preserved. Mathematically speaking given signals $s_1$ and $s_2$ in $S$ and a correlation function $corr(s_1, s_2)$ you want some function $f : S \rightarrow B$ (where $B$ is the space of 32-bit binary numbers) that you could use another correlation function for instance $corr_f(b_1, b_2) \approx corr(s_1, s_2)$. If that is what you want you should at hashing techniques in particular learning to hash. Essentially in hashing want you do is you represent your input by binary numbers in a way that the hamming distance between the binary numbers preserves some target similarity (distance) function such as the correlation. In particular for cross-correlation (inner product) you have some methods based on random projections. So once you have learned (designed) your hashing function $f$, what I would do is: 

If you want to reduce the dimension of your dataset to 2 dimensions, the algorithm clusters together feature1 and feature2, and leaves feature3 unchanged, the new matrix is: 

1. Download a comprehensive list of English words. (I avoid calling it a 'dictionary' to avoid mix ups with the Python type of the same name). I found one here. Remove all words that contain non-alphabetic characters (including hyphenated words ... maybe not optimal). 

Of course, if you have 40,000 items, that would probably require additional work to make the plot comprehensible. You could begin by ignoring all connections between nodes with a similarity value of, say, 0.01 or less. 

Suppose I use a linear Support Vector Machine with slack variables on a dataset that is linearly separable. Could it happen that the Support Vector Machine reports a solution that does not perfectly separate the classes? As an illustration: Is the situation in the picture possible for a Support Vector Machine with slack variables? Although there is a "better" boundary that allows perfect classification, the Support Vector Machine goes for a sub-optimal solution that misclassifies two samples. 

I think instead of building something from scratch, you should utilize the domain knowledge of experts. One example: Turn Wikipedia's topic portals (e.g., the medicine portal) into a family tree. 

Before trying using Hadoop or Spark, I recommend you to follow some optimization tricks (tips): [1] A Beginnerâ€™s Guide to Optimizing Pandas Code for Speed [2] Using pandas with large data This link (Don't use Hadoop - your data isn't that big) can be useful too. Also for a comparison of SAS and pandas you can read the pandas documentation on such comparison or this. 

So we have 5 times c0 which means n0 = 5, 2 times c3 means n3 = 2 and n2 = 1. Finally bi is the number of bits equals to 1 in c1 which b0 = 0, b1 = 1, b2 = 1, b3 = 2. This completes all the values you need for computing the lambda collocation scoring metric. Also take into account you could add smoothing to ni to avoid 0 counts. Now with respect to 

Python is not a matrix-based language, as far as I know the language does not offer by default any capabilities for handling matrices. I think your referring to the numpy/scipy stack, this is a separated library. 

My question: Why does my algorithm break down when I use it on the differenced time series? What is a good way to deal with drifts in time series? Here is the full code for my model: 

You could use a two-sample $t$-test to determine whether software usage is significantly lower for lost users. Plotting 'software usage rate'/'software usage rate relative to country average' against 'percentage of lost users' might give you an idea about a suitable threshold. 

2. Create the hash function. Map a given word to a tuple of length . The first entry is the character count for , the second is the character count for , etc. For instance, the word 'agaze' is hashed to the tuple . 

This is fun! Here is an implementation of Emre's idea in Python. I tried to avoid loops wherever possible to make the code fast. First, some imports and constants. 

You mention that files may have different file types. Be careful when converting between types: Make sure for instance that separators and quote characters are treated correctly.