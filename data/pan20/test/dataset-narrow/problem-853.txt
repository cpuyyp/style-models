If you see a packet in tcpdump it only says, that the packet arrived at the machine. But, the ACK gets only created by the kernel if it could deliver the packet into the socket buffer of the application. This could fail because the packet was blocked by the packet filter or because the system has not enough memory, but the most likely case is that the socket buffer of the application is full because it got a bulk of data in a short time and was not able to process all of them yet. In this case the packet is lost even if it arrived at the system, so no ACK will be generated and the packet has to be retransmitted by the sender. If the application is ready and has room in the socket buffer it can now be delivered, otherwise it will be lost and again no ACK will be sent. 

If you are using different certificates with different hostnames there should be no problem, even if the certificates are wild card. If you instead use different certificates with the same hostname (i.e. different IP address and server, but same hostname) you might run into problems because this setup is valid but unusual and at least security extension like Certificate Patrol will probably complain. 

A certificate does not come with a cipher. It comes with a public key and the type of key restricts a bit which ciphers can be used (i.e. ECDSA vs. RSA authentication, RSA key exchange). The usable ciphers are instead depend on the TLS implementation and configuration in client and server. All *-SHA384 ciphers are defined only with TLS 1.2. This means to use this the protocol version supported by both client and server must be TLS 1.2. Any client not offering TLS 1.2 or later in the TLS handshake will not offer this cipher and a server which does not support TLS 1.2 will not have implemented this cipher anyway. 

It depends on the server. Lat time I looked postfix was fine but sendmail could not properly check the hostname against the certificate. And yes, self-signed is one problem and the other main problem are missing intermediate certificates. But since mail delivery works anyway (since certificate errors are ignored by the sender) most admins don't realize the bad configuration or don't care. 

This tool is not designed to create a gateway from SSL to SSL. What you need in your case is just a simple TCP forwarder which can be done with socat: 

Same as with SMTP, i.e. they will fail if they properly check the certificate but many have an option to ignore certificate errors. 

Disabling a cipher (RC4) or a protocol (SSLv3) is not done by changing the certificate but by changing the configuration of the web server. The certificate is (mostly) independent of cipher and protocol. When using apache you would need to adjust the SSLProtocol and SSLCipherSuite configuration options. 

You get the error about from the server, so it refers to the validation of your client certificate on the server side and not to the (successful) validation of the servers certificate at the client side. That means the server does not like your client certificate. Please check your client certificate against the list of acceptable CAs, make sure it is not revoked and maybe do a tcpdump/wireshark to verify, that it gets actually sent to the server. If this does not help you might check log files at the server side for signs what went wrong. 

Content encoded with can be easily decompressed with piping the content through . For deflate I don't know an utility but it can be done with some zlib programming. SDCH is no that easy because to decompress it you would need have access to the dictionary used for compression, which might be somewhere else in your packet capture or nowhere. Gzip and probably deflate too should be transparently handled by wireshark, so that you can see the decoded headers there. An the httpflow tool coming with the Net::Inspect perl package can also decode the gzip and deflate payload for you and can also be used to extract HTTP requests/response pairs from a pcap file and save each of these pairs as single pcap file with payload already decoded. 

To create a new CA based on this key just follow the usual instructions. The only difference is that you don't create a new key but use your existing one. When creating the new CA make sure that you use exactly the the information as you did in the original certificate. They must match the issuer information in the already generated certificates. If they don't this new CA is not even considered as a possible issuer of the leaf certificate. But once these information match validation should be successful because still the same key was signed to use the certificate. 

You've configured to use 200 concurrent requests. The size of the request is unknown so we can assume that there will be only minimal payload. You are also using no HTTP keep alive which means that there will a new TCP connection for each request. I doubt that apache bench is doing TLS session resume so that there will a full handshake each time. Which gives you: 

The wireshark dump you provide is not helpful, because it only shows the information at the transport layer (TCP) and not the TLS layer. It also does not show any kind of error messages from the clients nor does it show how the clients use these proxies and how they validate the certificate. In generally a TLS connection needs to validate the servers certificate, which includes verifying the trust chain and the name in the certificate. If the name does not match the expected name the validation must fail, because otherwise you could use any certificate to pose as some other host and thus do a man-in-the-middle attack. This also means if access is only done by IP address then the certificate must incorporate the IP address as type IP in the subject alternative names section (and better as type DNS too because of some broken TLS stacks). If some clients fail while other succeed than it might be that some clients use the proper hostname for verification while other don't or that some clients ignore validation errors. It is not possible to know which of these is the case based on your information. 

The name which gets used to access the site should be contained in the certificate. Which means if you access the IMAP service with the hostname then it is not enough to have a certificate for only. If these are both the same IP address then you could of course simplify the configuration by accessing the IMAP service with and not . This is true for all protocols. 

Most servers can be configured this way but most servers have to talk to a variety of servers and they don't know up front if the server will support TLS. But sendmail for example can be configured to talk only TLS to specific servers or not to talk TLS to specific other servers. So this configuration can be hardened to known good servers, but this must be done manually. 

If you want to offer HTTPS from start you must provide a certificate accepted by the client from start. Because otherwise the client will not accept the SSL connection and you will not be able to redirect the client to a different site or a HTTP-only version. This means to support this case you 

The redirect will be done in the HTTP protocol. HTTPS is HTTP wrapped inside a SSL connection, so if establishing the SSL connection fails because of a bad certificate there will never be the redirect to $URL$ So to make this working you have to use a certificate which the client accepts, e.g. matching the host name and issued by a trusted CA. If you take off the ssl it will just start a normal non-ssl http server on port 443 but the browser will try to talk SSL and thus you get this error about record_too_long. 

You are mixing router (layer 3), hardware load balancer (layer 4 and maybe higher) and Haproxy (software load balancer). A hardware load balancer is nothing more than an appliance with some software load balancer on it and maybe also some hardware acceleration for specific actions. There is nothing which inherently make balancing (not routing) based on SNI information impossible on a hardware load balancer and like another answer suggests there are products which support this. But of course it needs to be implemented and it costs performance - they deeper you look at the traffic the slower it gets. 

It looks like that your server has a private IP but your client a public IP. From that I assume that your server is inside some local network behind a router which will forward most of the ports to the server so that FTP data connections work (at least , which are and in your configuration. But, the server still knows only its internal IP address of 172.30.0.248 and expects the FTP client to connect to this address (in this case 172.30.0.248 port 58382). If the FTP client only implements strict RFC959 it will try to connect to this address and will of course fail to connect because it is not reachable from his site. Other FTP clients might instead ignore the IP given inside the response to and instead connect to the IP address of the server. While this is against the standard it works around situations like yours and can also be more secure. In summary: you expect the FTP client to work around a configuration problem on your site. Some do, others don't. For correct configuration of vsftp behind a router see for example $URL$ (first hit with google). 

Your statements do not match with the error messages. Since you are renewing the certificate is probably not self-signed, so it should not be a CA certificate as shown by the warning message. Also, the second warning shows that the certficate does not seem to match the hostname. Please check the contents of your certificate files with or similar tools. 

Directory listing and file transfer use the same basic mechanism, that is both open a data connection and transfer data. In your case the clients opens a connection to the address specified by the server within the response to the PASV command. Since directory listing work, ths excludes the common case where all data connections are blocked by firewall or NAT device. I can imagine the following reasons for the problem you have: 

This is the expected behavior of browsers. These warnings are there to discourage users from visiting this site because the browser can not determine if this is actually the expected certificate for this site or someone impersonating the site, like in a man in the middle attack. Essentially a self-signed certificate is saying "trust me, I'm the correct certificate" and there is no way to verify this claim. 

Usually the IP and port provided in the response to PASV are local to the server. But since FTP is protocol which causes lots of trouble when used with NAT routes, firewalls etc sometimes the IP you see in the response is not the IP of the server: 

This header containing a realm is needed so that the browser can either prompt the user to provide the credentials for this realm or use cached credentials. Sending no but instead sending a HTML page which even contains script is definitely wrong behavior of the server. Note that response code 401 must only be used if the browsers build in authentication dialog should be used and then a realm need to be provided. If one builds its own authentication system outside of HTTP (i.e. the typical logins inside HTML like it seems to be in this case) then response code 200 must be used. 

The host has both an IPv4 and IPv6 address. The web server listens on both IP port 443 so that the TCP connect is successful, but you somehow messed the SSL configuration for IPv6 up, so that the SSL upgrade on the IPv6 address fails. There are enough clients out there which use IPv6 if available and prefer it to IPv4 and these clients will fail. SSLLabs will not tell you about it, because it does not support IPv6 yet. But analyze.pl will tell you about it: 

The public key is - like it's name suggests - public. It can thus not alone be used for authorization, since everybody knows it. But only the owner of the private key is able to sign some random challenge and this signature can then be verified by everybody having access to the public key - in this case the server which has sent this challenge to the client. Thus it is essential that the client has access to the private key. And not only the client must have access to the private key but it should also the only one who has access (therefore: "private" key), because everybody who knows the private key can claim the clients identity. For more information see the basics of Public Key Cryptopgraphy. 

All what matters is that the certificate matches the hostname from the URL and that the certificate is valid and trusted. It does not matter if you have multiple IP address for the same hostname and use the same certificate on all of these. Such setup is actually very common for larger sites. 

The name you use in the connection to the server is the one which is used for validating the certificate and this need to be included there. But, which name you use for connecting depends on your setup: it might be an internal name only, but it might also be the external name in case you have a split-DNS, i.e. different IP for the same hostname when resolving from inside or outside. 

A proper client creates a tunnel with the help of a http proxy (CONNECT request) and then establishes the TLS connection inside this tunnel and verifies the certificate against the original hostname. That's how proxy connections work inside the browsers. And this is what your wireshark dump shows. Usually the client does not provide the IP address of the target inside the CONNECT request but the hostname, so this might be a client which either resolved the hostname up-front or which was given the IP address and not the hostname inside the configuration. In the latter case the client will not be able to verify the certificate properly, because it does not know the hostname expected in the certificate but will expect an IP address which is not there. 

Mail clients which properly check the certificate will fail but many mail clients have an option to ignore certificate failures. Most mail servers do not properly check the certificate and will accept anything. 

Apart from that I recommend to use SFTP (file transfer over SSH) instead because it is more secure (encrypted login) and causes less trouble (only a single connection, no extra data connections). 

This is wrong. A HTTP request does not determine any relation between a URL/hostname and an IP address. This is done by DNS. But if the redirect is done to the same hostname then the same IP address will usually be used. But it might be a different IP address if multiple IP addresses are associated with the same hostname in DNS. 

STARTTLS must be implemented on each hop (MTA). It's can be used only if the receiving MTA does support STARTTLS and the sending MTA can do it. It does not depend on how the mail was delivered to the initial hop (MTA) from the client. Generally TLS with SMTP only encrypts the connection between hops, so it can be read in clear on each hop if it is not encrypted end-to-end too (with PGP or S/MIME). 

Man in the middle is prevented if the client checks that it gets exactly the expected certificate. This is called certificate or public key pinning. This is not a feature of any TLS version including TLS 1.2 but must be implemented explicitly in the client. For more information and example code see OWASP: Certificate and Public Key Pinning. 

Even if this would be possible this would not help. While the server would probably accept the PORT command with the public IP address it would not be able to connect back to the FTP client using this IP address and port since there is no matching NAT state at the IGW. If you have two NAT's as in your case (NAT server and IGW) then both of these would need to translate the address in the PORT command and create a state to pass the connection from the server to the newly set IP,port to the IP,port from the original PORT command. I really recommend to move away from FTP and use alternatives which don't need dynamic data connections like FTP does. Such dynamic connections give you only problems if one of the NAT gateways will not be able to translate the PORT/PASV - maybe because of missing FTP helper or maybe because you are using FTPS (i.e. FTP with TLS) where the NAT gateway will not even be able to see the original IP,port and can also not modify the control connection as needed. Use protocols like SFTP (FTP over SSH) instead which have no problems with NAT since they only use a single TCP connection.