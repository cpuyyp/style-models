Now just place logic between and keywords. Just like Aaron pointed out in the comments, a job still has to be created on all servers with the same logic to ensure that it only runs on primary replica and not on the secondaries. There are scripts and tools that can be used to copy jobs from primary replica to the secondary replicas. 

which made no sense as it is self-join back to the table. After carefully looking at your plan I realized that you are bringing back data from a view and then joining it back to the base tables that are present in the view. It is a known issue that nested views will always produce estimated row count of 1 no matter how much you update statistics. My recommendation is to rewrite query without use of which just hinders performance. Remember: Views are not for performance, they are for simplicity and easy of access of information. 

but it only shows statistics for the whole file and not sure how to split into individual tables. Trying to do this in order to identify tables that should be places in their own files on different disk. 

An easy way to get job to execute only on primary node is to put simple check for the job to verify which node is at the time job is being executed. It can be done with simple sql statement: 

This questions came from a developer friend who is working with SQL Azure Database. Scenario: There are two Azure DB servers, one in North America, USA region and another one in South East Asia region. One way replication has been setup for a read-only replica to enable a local client to get data faster. The issue is whenever the client has to write data to the database, they need to write to North America, which incurs large latency issue. As a solution, they want to setup bi-directional replication between two servers to keep them in sync and thus allowing a client to read and write from a server located in their region. I'm more of SQL Developer than DBA, thus, I'm not familiar with different options that could be available for bi-direction in SQL Azure DB. I have told him about SQL Broker and how I saw it used for near real-time replication and with some effort it could be set up for bi-directional messages, though not sure if it can be implemented in SQL Azure DB. 

For those willing to risk it, see @dio-phung's workaround suggested in another comment on that same answer: 

The error doesn't occur for some time, which is usually the result of the client being unable to reach the destination server (e.g. an incorrect database server name). The error occurs in the web application and when testing an ODBC connection using the Windows Server 2003 Web Edition OS tools. In order to try to determine what is causing the issue, we want to see whether the connection attempt is reaching the database server. We thought we could use extended events to monitor the login attempt. is a new event in SQL Server 2016 which "is generated when server is done processing a login (success or failure)". We created an event session with this code: 

You can also use a UDF to enforce a custom CHECK constraint on a column, which could go and check the other. But there are some scenarios where that falls apart. 

Also, if you Include Client Statistics (under Query menu), you can get a history of when you ran the query, along with various statistics such as run time and number of rows. 

(2 additional events removed and some information obfuscated) Yet there was still nothing in the "Test Login Audit" event session we created on event . So the extended events system and ring buffer are working. And logins are occurring. So why aren't we getting any events? Do we misunderstand what a "login" is? Is this not yet a complete feature? We get this back from @@VERSION: 

I have very little knowledge of MDX, but through some trial-and-error, I have found that removing the two entries from the makes the query return quickly. Or, removing both the and entries also makes it return quickly. But, obviously, this is changing the query, so is not the solution, but perhaps it provides clues as to where I should look in terms of indexes or suchlike if there is anything like this in SQL Analysis Services (I'm more familiar with SQL Server databases)? One thing I did try, is to put before and . This caused the query to return very quickly with no results. Unfortunately, I don't know if this is correct because the query without this change takes too long to be able to see if there are any results. I don't know what difference is supposed to make, but is this the obvious answer or does it change the query to be different? It is equally possible that the original writer of this query should have used anyway, but never got to test the query with a real data set. Any help would be greatly appreciated. 

What I have found is that the third statement (commented as being slow) is the part that is taking the most time. The two statements before return almost instantly. The execution plan is available as XML at this link. Better to right-click and save and then open in SQL Sentry Plan Explorer or some other viewing software rather than opening in your browser. If you need any more information from me about the tables or data, please don't hesitate to ask. 

Is QuestionNumber a unique identifier somehow for table Questions? Or is the QuestionBank designed to have, say, 5 different sets of questions with numbers 1 through 25? If Questions.QuestionNumber is a mere non-unique ordering value for a given BankId, then there's no way to know, when saving a QuizQuestionStudent record, what QuestionBank a particular question belongs to without presuming that the answer is for a question in the proper bank. When QuestionNumber = 5, you'd have no idea how to check its BankId if multiple banks had a fifth question. You could presume it belongs to the same BankId as the Quiz, but then what is left for the database to check? Now, if QuestionNumber is unique, you have a different problem. You want the Question's BankId to be required to be the same as the Quiz's BankId. Repeating the BankId in the QuizQuestionStudent table wouldn't help, because that value is coming from the app and it's what you're trying to check. You want to make sure data in two other tables conforms not just to QuizQuestionStudent values, but also to each other's values. You can't do that with a foreign key. (A significantly different design for all of the tables might eliminate the issue, but I wasn't able to come up with one.) However, it can still be strongly enforced at the database layer. You can create an "indexed view" or "materialized view" and then constrain its values. The view can reference multiple tables. It would pull the BankId from two different paths and make sure they are the same. 

so why is it when I have added .5 second wait time to each loop iteration it actually runs faster and allows the print message to come out one at the time? 

I used the following query that I got from EXCHANGE SPILL, but slightly modified it to convert time from UTC to local. 

In SQL Server it is best to store DataTime as one field. If you create an index on DataTime column it can be used as Date search and as DateTime search. Therefore if you need to limit all records that exist for the specific date, you can still use the index without having to do anything special. If you need to query for time portion you will not be able to use the same index and therefore if you have a business case where you care more about the time of the day than DateTime, you should store it separately as you will need to create an index on it and improve performance. 

I have SQL Jobs that need to only on primary replica on AlwaysOn Availability Group. When jobs were originally created ServerA was primary, at later point ServerB became primary so jobs that were on ServerA failed and jobs had to be manually recreated on ServerB to run properly. What is the way to have jobs run only on Primary replica of AlwaysOn Availability Group? 

Is it possible to figure out which tables contribute the most to read / writes operations if all of them in same file? I been looking at the following DMV 

Therefore the function can't go get the information it needs. It must be fed the information. And there's simply no way (that I can think of) to collect the data from multiple rows and feed it to the function. You can feed a deterministic function XML to manipulate, but you can't create the base XML without a subquery. So in short, I don't think there's any way using an indexed view to materialize XML which contains data from multiple rows in any table. 

So datetime precision allows the .880 as a valid value. Even Microsoft's GETUTCDATE examples show the SYS* values being later than the older methods, despite being SELECTed earlier: 

While you could create complex, multi-level XML within the function, the function cannot reference any tables (or it wouldn't be deterministic). For example, this: 

We never found out what was causing the service to stop, but we were able to prevent it from happening. We put a Deny on the Stop permission for SSAS service for the SQL Server (not SSAS) service account user. So it seems something acting as the SQL Server's service account user was issuing the Stop command for the SSAS account. This was a while ago, and we've since switched servers, and we stopped digging to figure out what was really happening. So it will remain a mystery for the ages. 

(I tried a couple of different actions and it doesn't seem to relate to which actions are included-- but maybe it's based on a total character count of action names?) Full list of actions I was working with: 

I'm trying to understand what effect database mirroring can have on replication in SQL Server 2008. Here is current environment: (don't ask me why it was done this way or to change it) Server A: SQL Server 2008 (Publisher) Server B: SQL Server 2008 R2 (Distributor) Server C: SQL Server 2008 (Asynchronous Mirror) Server A is setup for SQL Replication and for Database Mirroring. Mirror is going to DR site and replication is used for different purpose at local site. When database mirroring fails (Server C goes off-line or network issue or anything else) and I can see state on Server A (principal, disconnected). At that time replication failed until mirroring was turned off. My understanding was that High performance (asynchronous) database mirroring, should not prevent transaction from being committed on source. The only time transaction should not be committed to the source is when High Safety (synchronous) mode is on. Which would explain that replication not being able to pick up the changes as nothing is being committed on the source. Is there any scenarios in which High-Performance (asynchronous) database mirroring can break database replication? 

Not sure if the question title describes perfectly what I am trying to achieve. I am looking for better way of doing what someone else implemented and I am trying to simplify it, secure it and make it easier to manage. Developer has built a website for displaying information to end customer, part of this information is presented internally through SSRS report. Since the site usually resides on DMZ there is no direct connection to SSRS report, therefore developer wrote CLR procedure that calls SSRS server to generate PDF file of SSRS report that he stored in the table temporarily, then just to return that binary to site which in turns displays that report. This process is extremely complicated to debug and maintain as new reports require new CLR procedures which creates problems when trying to distribute CLR to different clients that are using the product. In conclusion, if I want to present SSRS report outside of the network in a secure manner, what is the best route to go about doing that on a webpage? 

This might seem like a stupid question, but I've been looking into open source solutions for schema migration, namely Liquibase and Flyway. However, my boss told me that SQL Server Data Tools (SSDT) achieves the same job. I'm not sure if agree, but I can find very little on the Internet that directly compares it to Liquibase and/or Flyway. My view is that SSDT is a development, data modelling and design tool for SQL Server and also supports schema comparison (and generating scripts thereof) and source control. It tackles a different problem although there may be some overlap with Liquibase/Flyway in some aspects of schema migration. But as an overall schema migration tool, Liquibase and Flyway are fully dedicated tools whereas SSDT is more for the design and development of a database. Any opinions would be much appreciated even if it's just to say there's no comparison and SSDT is not a schema migration tool per se at all. 

I have an MDX query that was written by an MDX novice that is excessively slow (the query, that is, not the MDX novice). And I am also an MDX novice. Here is the query: 

I have a SQL query that I have spent the past two days trying to optimise using trial-and-error and the execution plan, but to no avail. Please forgive me for doing this but I will post the entire execution plan here. I have made the effort to make the table and column names in the query and execution plan generic both for brevity and to protect my company's IP. The execution plan can be opened with SQL Sentry Plan Explorer. I have done a fair amount of T-SQL, but using execution plans to optimise my query is a new area for me and I have really tried to understand how to do it. So, if anyone could help me with this and explain how this execution plan can be deciphered to find ways in the query to optimise it, I would be eternally grateful. I have many more queries to optimise - I just need a springboard to help me with this first one. This is the query: 

I presume this is because they come from different underlying system information. Can anyone confirm and provide details? Microsoft's SYSDATETIMEOFFSET documentation says "SQL Server obtains the date and time values by using the GetSystemTimeAsFileTime() Windows API" (thanks srutzky), but their GETUTCDATE documentation is much less specific, saying only that the "value is derived from the operating system of the computer on which the instance of SQL Server is running". (This isn't entirely academic. I ran into a minor issue caused by this. I was upgrading some procedures to use SYSDATETIMEOFFSET instead of GETUTCDATE, in hopes for greater precision in the future, but I started to get odd ordering because other procedures were still using GETUTCDATE and occasionally "jumping ahead" of my converted procedures in the logs.) 

(That will break down if you have more complex or custom nesting requirements.) Regardless of this particular example, I don't think there's an effective way to get (dynamic) XML into an indexed view. First, a "indexed view" is really a view with a unique clustered index. You can't create "normal" indexes on an XML column. Let's say you try: 

We adjusted the SQL Server's port to 1433 and the connection issue was resolved. (Dynamic port allocation requires at least UDP port 1434 to allow the initial port query, and then the firewall would have to sniff and auto-allow the resulting dynamically assigned port, much like many can do for FTP PASV commands. I'm not sure if any can do that for SQL port negotiation like this.) However, as this is primarily a question about certain extended events not firing, I plan to leave the question open.