Can I get an appropriately sized feature space for my subsample size? At the very least I would want a feature space that is an expert in some specific region, with the intent to cover the whole feature space with subsequent subsamples and feature selection. How many subsamples can I process while staying in my efficiency constraints? 

You could import each companies data into a specific table and then develop regular expression Scripts two change specific Expressions into your own taxonomy. $URL$ 

A google search for r-squared adjusted yielded several easy to follow explanations. I am going to paste a few directly from such results. Meaning of Adjusted R2 Both R2 and the adjusted R2 give you an idea of how many data points fall within the line of the regression equation. However, there is one main difference between R2 and the adjusted R2: R2 assumes that every single variable explains the variation in the dependent variable. The adjusted R2 tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable. 

You should develop your algorithm based on the reports; then only query the person when the algorithm returns its least confident results. Because you would expect that the most valuable patterns would be near the decision boundaries. 

Somehow you have to come up with some sort of numerical classification system for your movie genres. I would start by creating a relationship tree between genres. For example action movies and then action movies with comedy and then action movies with comedy with animation etc. You could develop a whole Forest of trees that relate movie genres to one another. You can then test the genres paths of individuals to compare. 

"Good", I think, is based on the state of the art at the moment. So I would look at respected models from industry leaders and use their reported accuracies as a base line for what is "good": since it comes down to what is possible. 

Can I get a large enough subsample that meets my unbiased criteria? Can I get a small enough subsample that meets my efficiency constraints? 

So that it is close as possible to the real data, go back an rebuild your logistic regression model and simulate the missing values you are finding in the real data. After all, the model is supposed to represent the real circumstance. This approach has a few advantages over the imputing approaches. The foremost is that you can accurately gauge which and how much missing data your model can stand up to. 

I am interested in joining some Data Science Groups on Linked In. The relevant list is long. I don't think it is to my advantage to join all, but I am not sure how to evaluate the individual group's potential to elevate my career. Is there some clear criteria for which I can evaluate potential groups? 

Here is how I would think about this problem. If I select my features first, I then need a large enough subsample so that the variance of the model is not raised counter productively. That raises a few important questions. 

Think about this problem in two parts. Part 1) How many samples do you need in order to have the desired confidence in the Null hypothesis? Part 2) How long does it take to get that many samples? Part 1 can be answered with A-priori power analysis. Use this technique to determine the sample size needed to get the p-value desired. Part 2 can be addressed with an algorithm applied to the data you have collected thus far. Your predictors in this case would be: Column 1: the probability that your statistic of interest has leveled off (is not changing significantly with additional input). Column 2: date/day/time start. You probably will need to codify this. Column 3: number of samples at finish. These should be the number which you determined from your prior analysis. Your response/output in this case would be: Column 4: the date/day/time finish. Of course would need to do some data analysis to determine the appropriate algorithm to use on this data set. The end product should be a function that takes as input a target probability, date/day/time start and a target sample size, while outputting a date/day/time finish. I hope this at least gives you some ideas on how to proceed. 

So as my comment suggested, I think there is a relatively easy way to solve this. That is with a Fourier transform. As I now recall, removing part of a time based signal is easy when representing the signal in the frequency domain. Because all that is necessary is to apply a filter to the signal to remove unwanted components of the signal. In your case those spikes (abrupt changes) will look like high frequency signals in the frequency domain. So in theory, you should be able to apply a low pass filter to the transformed data to get rid of the spikes ( fakes ), then transform the filtered signal back into the time domain. Then just like that you have gotten rid of all those abrupt changes in your data. To reiterate. 

You should look at the Open Science Framework:Here is a Introductory YouTube video This Open Source Project addresses all your concerns. Structured projects: Keep all your files, data, and protocols in one centralized location. No more trawling emails to find files or scrambling to recover from lost data. Secure Cloud Control access: You control which parts of your project are public or private making it easy to collaborate with the worldwide community or just your team. Project-level Permissions Respect for your workflow: Connect your favorite third party services directly to the Open Science Framework. 3rd Party Integrations 

The term "Data Cleaning" is used to describe outlier checking, date parsing, missing value imputation to structuring datasets (organizing data values within a dataset) to facilitate analysis. The latter is commonly referred to as "Data Tidying" but what about the former? I have seen outlier checking, date parsing and missing value imputation referred to as "soft processing", however I wonder if this is a commonly used term. If I describe a component of data cleaning as "soft processing" is it reasonable to assume that knowledgeable people will know that I am referring to outlier checking, date parsing and missing value imputation? In the field of Data Science, is there a commonly accepted phrase or term that umbrellas outlier checking, date parsing and missing value imputation. 

It is impossible to say whether or not you should trim your data, without doing some Exploratory Data Analysis first. Those missing values may be important to your understanding of the dataset, so you have to look at your data comparing variable to variable to determine if the missing values hold information. For example, if your data is about vehicles, there could be a variable representing number of doors. If your missing data is structural an observation of a motorcycle may have a missing value for that variable. In which case, the observation for that variable could be discarded as meaningless. Or, you could impute the missing value to something meaningful like the number 0. It depends on what insight you are ultimately trying extract from you data. Handling missing values is quite a complex subject. I would suggest that you "Tidy the data" first. From the Wickham paper 

Initially this can help you do some Exploratory Analysis to determine what is important and what is not. Once you have made those distinction you can find and implement strategies to handle the missing values appropriately. 

What Is the Adjusted R-squared? The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors. Suppose you compare a five-predictor model with a higher R-squared to a one-predictor model. Does the five predictor model have a higher R-squared because it’s better? Or is the R-squared higher because it has more predictors? Simply compare the adjusted R-squared values to find out! The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not.  It is always lower than the R-squared.