You are looking for an ALL-SAT or all solutions SAT solver. This is a different problem from #SAT. You do not have to enumerate all solutions to count them. I do not know of a tool that solves your problem because people add these algorithms on top of existing SAT solvers but rarely seem to release these extensions. Two papers that should help you in modifying a CDCL solver to implement ALL-SAT are below. Memory Efficient All-Solutions SAT solver and its Application to Reachability, O. Grumberg, A. Schuster, A. Yadgar, FMCAD 2004 Here is a recent article posted on the arXiv. Extending Modern SAT Solvers for Enumerating All Models, Said Jabbour, Lakhdar Sais, Yakoub Salhi, 2013 You could try contacting these authors for their implementation. 

The two notions are closely related but different. Abstraction between two structures The insight of abstract interpretation is that it is useful to endow the structures you consider with a notion of order. Consider two structures 

If you mean "similar" in the colloquial sense, I think JeffE's answer captures what some people mean. In a technical sense though, it depends on what you care about. If asymptotic time complexity is all you care about, the difference between recursion and iteration may not matter. If computatability is all you care about, the difference between a counter variable and a one-symbol stack do not matter. To compare algorithms, a first step would be to make the notion of equivalence precise. Intuitively, let $A$ be the space of algorithms and $M$ be a space of mathematical objects and $\mathit{sem}: A \to M$ be a function encoding that $\mathit{sem}(P)$ is the meaning of algorithm $P$. The space $M$ could contain anything ranging from the number of variables in your algorithm, to its state-graph or it's time complexity. I don't believe there is an absolute notion of what $M$ can be. Given $M$ though, we can say two algorithms are equivalent if $\mathit{sem}(P)$ equals $\mathit{sem}(Q)$. Let me add that I think each of the five criteria you mentioned can be formalised mathematically in this manner. If we want to talk about an algorithm being more general than another (or an algorithm refining another), I would endow $M$ with more structure. Imagine that $(M, \sqsubseteq)$ is a partially ordered set and the order $x \sqsubseteq y$ encodes that $x$ is a more defined object than $y$. For example, if $M$ contains sets of traces of an algorithm and $\sqsubseteq$ is set inclusion, $\mathit{sem}(P) \sqsubseteq \mathit{sem}(Q)$ means that every trace of $P$ is a trace of $Q$. We can interpret this as saying that $P$ is more deterministic than $Q$. Next, we could ask if it's possible to quantify how close two algorithms are. In this case, I would imagine that $M$ has to be endowed with a metric. Then, we can measure the distance between the mathematical objects that two algorithms represent. Further possibilities are to map algorithms to measure spaces or probability spaces and compare them using other criteria. More generally, I would ask - what do you care about (in intuitive terms), what are the mathematical objects representing these intuitive properties, how can I map from algorithms to these objects, and what is the structure of this space? I would also ask if the space of objects enjoys enough structure to admit a notion of similarity. This is the approach I would take coming from a programming language semantics perspective. I'm not sure if you find this approach appealing, given the vastly different cultures of thought in computer science. 

You should examine the details, particularly because the notion of a modal specification is not standard, and is defined in the article. 

What is a set of lines of code that is dead? What is a set of variables in the program that have constant values? What is a set of assertions in the program that is not violated? 

Instead of adding $m$ and $n$, we can ask for a range $[a,b]$ in which the sum lies. Instead of multiplying $m$ by $n$ we can ask for $k$ bits of the result (specific, common examples are the sign or the parity bit). Instead of asking for the satisfying assignments to a formula, we can ask for a set that contains the satisfying assignments. 

The undecidability of the halting problem. Beautiful for many reasons. It is an impossibility result. The proof uses diagonalisation. The statement applies to a broad range of models of computation. It can be formulated in a variety of ways, particularly, using standard programming languages. It was a watershed result in the history of computing. Extending this statement leads to Rice's Theorem, Turing degrees and many other cool results. Etc. Etc. Etc. 

Specifically, this dissertation shows that there is a family of semantic structures, a family of logics, a family of topological spaces, and a programming language that sit in very tight correspondence (think isomorphism). You can start with one family of objects and derive the other. Samson's work allows you to start with semantic structures and derive a logic, for example. 

Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, 1990 Handbook of Theoretical Computer Science, Volume B: Formal Models and Sematics, 1990 

Can you hear that noise? It is the sound of a thousand brilliant theorems, applications and tools laughing in automata-theoretic heaven. Languages and automata are elegant and robust concepts that you will find in every area of computer science. Languages are not dry, formalist hand-me-downs from computing prehistory. The language theory perspective distills seemingly complicated questions about sophisticated, opaque objects into simple statements about words and trees. Formal languages play a role in computer science akin to the fundamental and game-changing viewpoint brought by algebra and topology to classical mathematics. Here are some practical, fairly complicated, practical problems that are approached via language theory. 

Abstract interpretation is a very general concept and depending on whom you ask, you will receive different explanations because versatile concepts admit multiple perspectives. The view in this answer is mine and I would not assume it is general. Computational hardness as a motivation Let's start with decision problems, whose solutions have a structure like this: 

There are many ways to define the semantics of a program. If we consider operational semantics, a simple imperative program defines a relation $R \subseteq \mathit{State}\times \mathit{State}$. I'm using "simple imperative" here to mean something that does not modify itself. There is a standard way to view $R$ as a function $\wp(\mathit{State}) \to \wp(\mathit{State})$ that maps a set of states to their successors in $R$. If a program can modify itself you can consider a pair $\wp(\mathit{State})\times(\wp(\mathit{State})\to \wp(\mathit{State}))$, in which each element $(S,F)$ represents a current set of states in $S$ and a transformer $F$ representing the behaviour of the program. A program of the form you describe can modify either $S$ or $F$. You may find the following paper useful.