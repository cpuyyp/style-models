If you really want to create the role yourself, you have to use to disable user interaction. See $URL$ for more details 

Like @Tensibai indicated the issue is related to the replication. According to this blog it could be solved as follows. 

Advantages of this Pizza analogy is that it consists of multiple analogies, i.e. runtime aka pizza and homemade aka legacy. When one Googles "DevOps analogy", various images are shown, but not of them is very catchy. Definition of "fetching" 

It depends on what is the definition of a typical DevOps toolchain, but people that are responsible for documenting about GDPR are likely to consult DevOps about techniques that are used and could you explain why this option is secure and what do you do if there is a data leak? Plan Create Verify Package Release Configure Monitor 

What about using a CI tool? Create a parallel job and the jobs will run parallel. For example, one could use Jenkins: $URL$ 

When it is run by bitbucket, it indicates that the authentication to the docker registry fails so now this step has to be added over and over again: 

One could fix the issue by creating a pull request Or one could find another github project that has less or no issues Or one could adjust or create an own image 

It is possible to prevent the overwrite of tags by listing the existing tags using $URL$ and exit the pipeline if there is a duplicate. 

Infrastructure as code means that the infrastructure configuration is stored in a version controller system (VCS). If changes will be applied it is traceable who changed the code and when. Although it is unclear to me what the aim is of your question, it is possible to use a continuous integration (ci) tool like, Jenkins, gitlab, circleci that pulls the code from git, installs ansible or use an image that contains ansible in order to run a playbook. 

My personal view is that a tool could be successful if it will solve a (major) problem. For example, a half year ago I gave a presentation of docker in the company, the developers had some doubts until they experienced a issue that it was not possible to run all microservices on their local machine. The same is applicable to the question: 

there is no one-size-fits-all. It depends on the goal. If your company is commercial then the most important thing is that new features while be added. So a monolith would be good first. If issues occur one could apply the cheese-slicer method and extract the problematic functions into microservices. The most important thing is to get it to the customer as soon as possible. 

To me, but correct me if I am wrong, the Jenkins servers should not be rebooted, but they should create VMs and reboot them. When I read the question I get the impression the slaves are rebooted, i.e. To me it seems that the jenkins slave should create a VM, run the tests, reboot the system, run some additional tests and finally remove the VMs. From a production perspective, but again correct me if I am wrong the team is not using Jenkins to provision the systems and subsequently reboot the nodes? It could be possible that they use Jenkins to start the provision step, but they will not reboot the Jenkins systems right? I would recommend to start with the Production perspective, e.g. How does the team provision the systems at the moment? E.g. Chef? If that is the case, how to they run that? If they would run chef apply then this could be run by a Jenkins slave, but I would not reboot the jenkins slave but automate the process of creating a VM, provision it, reboot it and kill it. The latter is only applicable for testing purposes. 

Based on a suggestion in one of the comments to one of the answers by @PeterMuryshkin I read more about Industry4.0 and I think it could be an DevOps analogy. Another DevOps analogy could be industry 4.0: 

In Jenkins it is possible to assign labels to nodes, e.g. somenode to nodes and then one could call somenode in the pipeline and then Jenkins will run the build on some of the nodes. Problem The assumption was that Jenkins would apply round robin, i.e. the number of builds that is run on node 1 2 3 4 and 5 is equal, but actually the majority, I.e. more than 90 percent is run on node four. 

In order to introduce industry 1.0 the functional process, i.e. how to produce coton manually has to be clear in order to automate this, 2.0 automated more and 3.0 as well. Nowadays DevOps is also about automating more and more, but in order to do that the process should be clear as well. As 4.0 is about moving to the cloud, e.g. AWS, GCP, AWS, CI/CD and self-healing systems this could be an analogy too. 

According to this article a monorepo contains different projects. What are best practices for triggering different pipelines? The current git repo contains the following projects: 

The aim of this question is to get feedback. On one hand I think that these topics will cover "Java for DevOps", but on the other hand I would like to know whether other people agree with this or what topics they would cover if they will organize the course? The target audience consists of Senior/Medior Quality Assurance, Senior/Medior Operations and some Junior Developers. Rationale @AnoE asked me to explain the rationale for asking this question. According to Wikipedia, DevOps is the intersection of Development, Operations and Quality Assurance. Development: creates features, i.e. add business value Operations: ensures that the software runs stable Quality Assurance: tests the software by clicking in the UI for example DevOps: intersection, i.e. automated, stable, tested software sooner to the customer As development creates new feature, it would be beneficial for the company and also the developers that Testers and Operations automate more and start to fix bugs, then Development could create more features. If everything goes well, perhaps some testers and operations could create features in a couple of months/years as well. 

One could use the same docker-compose file for dev, test, acceptance and production (DTAP) systems. Things that deviate per environment, e.g. passwords could be defined in an .env file. If .env files are not a solution for some reason then one could create different docker-compose files, e.g. docker-compose.override (dev), docker-compose.test, docker-compose.acc and docker-compose.prd. I personally prefer the first option, e.g. using .env files and a docker-compose.override for dev. 

When one runs then the docker images are created sequential. Is it possible to build these images concurrently using docker-compose? There are multiple issues including this, but it is unclear whether it is possible or not. 

I am preparing a course about "Java for DevOps". From a DevOps perspective, what topics should be covered during the meeting? I was thinking about the following topics: 

If this limit is not reached, build will fail. Is it possible to define a minimum code coverage using Pygradle? When the above snippet is run, the build fails and the following is returned: 

I prefer DIND as every run start with a clean sheet. Otherwise the docker image will be built on the gitlab host itself. 

Whether it makes sense: From a Linux perspective it does not as a Windows container will be significantly larger than a Linux one at the moment. From a Windows perspective it does not make sense as well as Windows itself is not as lightweight at the moment as Linux. Imagine that one runs 50 war files on a dedicated Windows server at the moment. If one would use Windows Docker containers for that purpose then a dedicated server should be used as a base image, each war file should be run in a separate container. This means that the number of required resource would grow exponentially. I assume that the creators of the native Windows Docker container take this into account and that this issue will be solved in the future. 

I have read $URL$ but I do not see the advantage of using Packer over docker build or docker-compose up --build for building docker images 

The main difference is that docker images are immutable and using is CMS is mutable. Once a docker image has been created it will work on all servers, while running a CMS could fail due to different environments, e.g. internet down, firewall, package repository unavailable. Could a CMS be combined with docker? Yes, to configure docker images using packer for example. Packer is able to run existing CMS code inside docker. Another use case is deploying and configuring docker environment variables using a CMS. 

According to me this is not possible as the hyper-v should be enabled on the host system. If this is enabled in a VirtualBox, but not on the host system, then this will not work according to what I have experienced. 

At the moment it seems not be possible to update all issues at once using the API and therefore this issue has been created. 

Running the jobs on the master nodes means that the jobs have unrestricted access into the JENKINS_HOME directory 

Although this link indicates that V supports HV, the above error message is contradictive. Why does VB look incompatible with HV, why V says that it supports HV and how to prevent that HV needs to be disabled in order to use V again. 

DevOps could also be compared with a commando squad, that consists of a small number of specialists. I always have to think about the first level of Commandos 1 behind enemy lines. There were three characters: 

After doing some research and comparing various documents it seems that a bot account is basically a real user account and needs a separate email account in order to create such an account as using an email account to register a new account results in, e.g.: . Combined with quote retrieved from the link that is defined in the question: 

Document the functional process When I create a new project and want to deploy it I always start to right down the manual steps in a README, but one could also create a txt file or sheet like you did. Feel the pain At a certain moment when I have deployed the project manually the pain of certain steps will become clear. For example, I add per step how much time it takes. Automate the high ROI steps If for example a certain step takes half an hour an one deploys twice a week, one could investigate how long it will take if one automates this step (investment) and what time it will save (return). One could decide to create issue tickets and automate steps every sprint. Example deployment plan For one of my current projects I have to deploy multiple microservices. I will not provide all details, but initially the plan was two pages long. Now I only use a Continuous Integration/Deployment (CI/CD) plan and a lot of abstractions. If the deployment will fail some day I could still read the initial plan so that the functional flow is still clear. Configuration as Code Another benefit of automating the manual step is that one could save the code in a Git repository as a pipeline script. When another person reads the scripts then it becomes clear how the project is deployed and one could also trace who made some change, what modifications were done and when.