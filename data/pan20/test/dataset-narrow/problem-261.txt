What kind of changes are you looking to make? Personally, at the minimum I'd setup a listener and have the applications use that. Otherwise it's going to be an even more painful AG failover experience. 

Those error messages, much as @Nic pointed out, are not really error messages in the log or in a DMV but are surfaced up through policy based management which runs as part of the AlwaysOn Dashboard. If you wanted to see how this works, we can open up one of the PBM system policies for Availability Groups. 

No, because there can only be a single mirroring endpoint which is what both availability groups and mirroring uses. The reason that it's called a "mirroring" endpoint and not an "availability group" endpoint is two fold. First, mirroring came first. Second, if the name was changed it could cause a compatibility issue with older applications and code where the keyword would change, so they won't change it due to backwards compatibility. I'd be happy to understand why someone is saying that a database can belong to mirroring but not belong to an availability group. 

Now, you're probably thinking - ok that's great but I don't know the magical limit so it really isn't helpful. Well, it is and it isn't. If you specifically look at it from a numbers point of view then yes it isn't very helpful... however this is a terrible way of looking at it. It should be looked at as, "Am I collecting only the data I need?" and in most cases you'll never run into an issue with this error. If we take the definition in the question that doesn't work, some of the information collected seems as though it really isn't needed. Do you really need callstack, current thread id, cpu cycle time, worker address, and scheduler address? Callstack is variable, the rest are fixed, so just eliminating the callstack you could fit in more columns if needed. I'm not saying you need any more but you could. The whole point is to limit the definition to be as small as needed. Collecting everything is going to either result in errors (as you've had here), system slowness, too much data for analyzing, or even system halting. Just because you can doesn't mean you should. There is nothing stating that these limits will or won't change between major or minor versions, so keeping the true minimum need is the best prevention. Please don't just check every box (gui) or add every action possible. 

From the WSFC point of view, assuming that the subnet does not change, there shouldn't be anything needed here. From SQL Server/AlwaysOn AGs... assuming the subnet does not change: 

Replica is under too much load Hardware specification differences (i.e. synch runs ssd and async runs rotational) or synch is using 8 cores @ 3.6 GHz and async is using 4 cores @ 2.4 GHz Stats and/or indexes were updated and haven't yet been fully redone on the async Geographical network latency 

Between replicas, you mean? Again, you'll need to use PowerShell. There is an upcoming blog post [(placeholder)] blog post I wrote detailing this. When it is officially published, I'll update the answer with a link. In the interim, here is the basics of what you'll need to do this. You may want to re-think using clusters like this (workgroup) as this is just the tip of the administrative iceberg you'll be dealing with. 

Passwords are stored in the master database, there is no need to connect to that database on the secondary. Unless the database is a contained database, server logins are also stored in the master database. Contained databases take care of themselves as contained users can be "logins" and are "contained" within the database, thus automatically included. 

AlwaysOn Health extended event session SP_Server_Diagnostics output Polling the DMVs and recording changes Scraping the cluster log 

Membership is what the user belongs to... Since users are a database principal this could be database level roles, application roles, etc. Securables are what the name implies, items that are authorized access to. So, let's say we have a login which is a server level principal. The login can belong to a server level role.. it can be a member.. of that role. Then we give the server level role access to server level securables, such as and endpoint. Maybe we grant control endpoint to this role. Since the login has membership in this role it is granted the control endpoint permission against the endpoint securable. 

Is that 2800 IOPs @ 4k? 8k? 256k? Is that at outstanding IO of 1? 4? 8? 24? 128? Should SQL Server now monitor the entire server for everything IO related? What if it's in a hyper-visor and can't "see" the base infrastructure... now what? Is that 2800 IOPs with 99% IOs within 10 ms? 20 ms? 100 ms? This isn't as easy as saying "50%" as there are too many variables of what 50% means... Not nit picking, just not as easy of a subject as one may think. 

MIRROR TO is not supported with backup to URL: $URL$ Specifically under the "Support for Backup Arguments" section, you'll see "Mirror To" has a "-" for not supported. 

The initial size is going to be whatever Model is set to, in this case your model is 3 MB. Who knows why, could be a host of reasons which in the end aren't very important to be honest. Set Model to the starting size you'd like for all new databases or specify the size in the create database command/window. 

There isn't a trace flag to turn this off, and I highly doubt that if there were one it would ever be documented or given out. If the killed session messages annoy you, filter them out as Aaron has stated. You can create your own noise words list or known message list that you don't care about specifically for your dev environments. Additionally, I'd look into giving them some SQL Azure databases to let them do development so that I didn't have to worry about this kind of stuff and could focus my time on actual production issues or new projects. Disclaimer: I work for Microsoft. 

I'm not sure I understand this part of the question. All of the resources can only belong to a single cluster - there is no cluster inside of a cluster thing. Edit - I looked at the link that you posted and I'm not sure why the author stated "â€¢Cluster name for each node". My only guess is they meant each node needs a name and IP (for the node). Otherwise it's not a correct statement, the author should probably be contacted. 

It's not really a gap, it holds everything that has changed since that time (not as transactions but as what extents were changed) and then a little log. The "gap" is the changed extents part and what "makes up for it" is the little bit of log in the differential backup to make it consistent. 

Then they should be moved to different drives where there is space, or moved around so that all drives have space, or the drives expanded. This really comes down to administration. This will be an ever present issue while the data files are vastly different in free space and size. The reason comes down to proportional fill and round robin allocations. A very quick summary of proportional fill in SQL Server is that the file in the filegroup with the most free space should be allocated from more often than the ones with less free space. If there is a file with little to no free space, it'll be skipped. This means when you only have one file in your filegroup that really has any space, you're effectively hot-spotting all allocations there. The round robin part is just the allocations going between the files. Since you're only keeping a single file with unbalanced amounts of free space, this will become a single hot spot for all allocations. This is why I advised the above in equaling your files as much as possible. 

Technically, now, one of the drawbacks is that they aren't going to change their connections strings... when, say, they should be adding in connection specific keywords such as . You'll also have a ton of DNS items to regulate, which sounds easy - and in general, it is. I do, though, tend to see it lead to network name duplicates though. 

The primary server is always where the write workload goes. If you're asking how the replica gets updated once it transitions from Primary->Secondary the answer is it finds the primary and checks in with its' LSN. There is a negotiation that goes on and depending on what happens in the interim between down and connected to the new primary it may send log blocks until the old primary is caught up or it may require log backups to be applied or a complete new synchronization. 

So you know the backup cert/DMK files are now useless. I would IMMEDIATELY take new backups and save the passwords. This way you can restore older backup files should anything happen. I would also backup the SMK as that's the only means you have to decrypting the DMK/Cert at this moment. 

In this instance, if you wanted to use XE you can grab it through "sqlserver.error_reported". The error number for a subquery returning more than one value is 512 - so we can filter on ONLY error numbers of 512. XE Session Code: 

The CNO has no idea about what an availability group is, which should be primary, etc. The CNO absolutely DOES NOT always point to the primary replica. You can connect using it only when the core cluster resources (which holds the CNO) happens to be owned by the same node as the AG primary but if it isn't - and the CNO doesn't follow the AG around - then it'll fail. Here's an example: 

There is not nearly enough information to make me believe this is the case. Even on some pretty terrible infrastructure I normally see 200-300 ms and nothing near 1 second PER individual transaction. 

I'm not sure what you mean by SQL Server cluster and windows cluster. If you are talking FCIs, you wouldn't be able to AG between the FCIs because they'd share the same set of nodes. 

Buffer the password (hashed) from the initial password change request. Check to see if login is mapped to any databases that currently reside in any availability group. Get the distinct replica names for each availability group. Connect to each replica and issue the password update. Run a job on all replicas and check for logins that have been updated recently. This information you can get from sys.server_principals (modify_date) and can use sp_help_revlogin to get the command needed up update the login. This could be run every so often (10 minutes, 15, etc.) Use event notifications for the ALTER_LOGIN DDL event and have an activated stored procedure that did #2 (the one above) but on demand at the same time rather than on a schedule. 

You can change the permissions and requires the settings to be properly set. The security implications are that of any other file share. 

If you want this to happen, and it must be in a cluster, like Cody said above me, you'll need to use Availability Groups. This, however, it not a multi-master model like Peer-To-Peer replication and thus you would only have a single node available to take write requests - though you could potentially have multiple other nodes server read requests. 

If you're going to be maintenance to a host, I'd move all roles (drain) off of that node. So, yes, everything should be moved from it. 

The system (storage, memory, cpu) isn't fast enough or have enough bandwidth to redo the log blocks. REDO may be blocked by a user thread if it's a readable secondary. 

Correct. Not to delve too deep into this since it isn't configurable - but let's take a quick second and think about that sentence and what you're really talking about here. How do you know what your IO capacity is? Let me take your information: 

If you're using must_change or password policies you'll have an event raised or a message returned about this and can do it programmatically. Ex: $URL$ 

The one I use to see the active log is sys.fn_dblog (Link to Remus's log post). Note that this function isn't documented and can/will change and doesn't show things such as Hekaton logs which need to be cracked using sys.fn_dblog_xtp which again isn't documented. 

There may be additional items specific to your environment but that should be the gist of it. You've pretty much hit the nail on the head in your original question/post, this just adds in a little filler :) 

The error message is pretty self explanatory. Mirroring doesn't support memory optimized objects (In Memory OLTP you may see it called [IMOLTP] or Hekaton). Mirroring as noted in the comments is deprecated - it's only AGs going forward. Since 2016 supports a multitude of new options in terms of clustering and availability groups, you should be able to do the exact same thing with AGs as you can with mirroring - albeit a few more steps in setup and configuration. If you want to use mirroring, then I would suggest using a blank, new, database and configuring it there. Also, don't rely on the GUI. 

It looks as though you have a problem with your storage subsystem (somewhere from the drivers down to the actual disks, but it could be anywhere in that stack). The good news: 

If the OS didn't recognize mount points, then how would it even let you use a mount point? That just makes no sense. If the OS doesn't recognize mount points, why would it track them and query their metadata? Also, please note that a mount point is a construct of the filesystem which an OS may or may not support. Not all filesystems you come upon may support mount points, however the most common filesystem in Windows Server is NTFS which in fact does support mount points and it has for a while. Just to bring this untrue item home even further; Windows Clustering has something called Cluster Shared Volumes (CSVs) which actually use mount points for the volumes... that's a native item using technology. I have to say, whomever told you this needs to be educated in the issue. 

This won't help the blocking, it could lessen it but only if the secondary is in commit mode and only if the workload on the secondary is causing extra waits. If it isn't, then it's not going to help the primary for blocking. What turning off readable seconaries will do, is stop the workload on the secondary allowing it to have the most speed in keeping up by not having any other workloads to contend with. It may also help with redo blocking on the secondary since you're running DML and could lessen the send queue/redo queue. If you had replicas you may want to look at making them for the time being depending on if the schema changes are size of data operations or not. 

I'd say, if 1000 concurrent connections doesn't break anything in your app/database processing then go for it. Realistically, unless this app becomes extremely popular or there is an issue in the app/database there shouldn't be a need to hit 1000 concurrent requests from the same pool on the same app server. If it is, chances are you'll kill the app server before the database server (maybe not, depends on hardware and config) or there will be some concurrency issue at the database level (again, depends). This will allow the pool to grow but hopefully not overwhelm the database server. If you had 10 app servers all set to this, then 10k connections may be an issue. There are a good amount of moving parts, even in this simple example but I'm trying to be optimistic.