Moser's constructive proof of the Lovasz Local Lemma uses computer science ideas, gives a new proof of Lovasz Local lemma, and solves a problem that people have been thinking about for quite some time. 

If you partition the input into sets of instances, each of which has the same solution, then each of these sets of instances is indeed polynomial-time solvable. I'm sure this isn't what you're looking for, but I don't see how to naturally exclude it from your question. 

This is called run-length-limited coding, there has been lots of research done on it by the information theory community, and I will just refer you to the existing literature. After a brief glance at the wikipedia article I linked to, my impression is that you shouldn't rely on the references it contains, but do your own search of the literature. One simple thing you could do that does better than doubling the string size is: (1) add a 0 to the end of the original sequence, (2) break the original sequence greedily into blocks of either 0, 10, or 11, (3) encode using the encoding 

No. Turing-recognizable undecidable languages can be unary (define $x \not\in L$ unless $x = 0000\ldots 0$, so the only difficult strings are composed solely of 0's). Mahaney's theorem says that no unary language can be NP-complete unless P=NP. 

You should note that this only solves the question when (for the OPs definition of $k$ and $n$) $k$ divides $\frac{1}{2}n(n+1)$. If anybody knows how to solve it for all $k$, they should definitely submit another answer. 

The following gives an algorithm that uses approximately $2^n$ time and $2^{n/2}$ space. First, let's look at the problem of sorting the sums of all subsets of $n$ items. Consider this subproblem: you have two sorted lists of length $m$, and you would like to create a sorted list of the pairwise sums of the numbers in the lists. You would like to do this in roughly $O(m^2)$ time (the output size), but sublinear space. We can achieve $O(m)$ space. We keep a priority queue, and pull the sums out of the priority queue in increasing order. Let the lists be $a_1 \ldots a_m$ and $b_1 \ldots b_m$, sorted in increasing order. We take the $m$ sums $a_i + b_1$, $i = 1 \ldots m$, and put them in a priority queue. Now, when we pull the smallest remaining sum $a_i + b_j$ out of the priority queue, if $j < m$ we then put the sum $a_i + b_{j+1}$ into the priority queue. The space is dominated by the priority queue, which always contains at most $m$ sums. And the time is $O(m^2 \log m)$, since we use $O(\log m)$ for each priority queue operation. This shows we can do the subproblem in $O(m^2 \log m)$ time and $O(m)$ space. Now, to sort the sums of all subsets of $n$ numbers, we just use this subroutine where the list $a_i$ is the set of sums of subsets of the first half of the items, and the list $b_i$ is the set of sums of subsets of the second half of the items. We can find these lists recursively with the same algorithm. We will now consider the original problem. Let $S_0$ be the set of coordinates which are $0$, and $S_1$ be the set of coordinates which are $1$. Then \begin{eqnarray*}\prod_{i \in S_0} p(v_i=0) \prod_{i \in S_1} p(v_i=1) &=& \prod_{1\leq i \leq n} p(v_i=0) \prod_{i \in S_1} \frac{p(v_i=1)}{p(v_i=0)} \\ &=& \prod_{1\leq i \leq n} p(v_i=0) \exp\,\left(\sum_{i \in S_1} \log \frac{p(v_i=1)}{p(v_i = 0)}\right).\end{eqnarray*} Sorting these numbers is the same as sorting the numbers $\sum_{i\in S_1}\log p(v_i=1) - \log p(v_i=0)$, so we have reduced the problem to sorting the sums of subsets of $n$ items. 

For $n>1$ and $~0<p \leq 1$, can we upper and lower bound the following binomial series in terms of $n$ and $p$ $$\Sigma_{i=\lceil p n \rceil}^n {n \choose i} (p )^i(1-p)^{(n-i)}$$ 

Let $G=(V,E)$ be a graph. For a given $\rho \leq |V|$ and $\epsilon$ with $(0<\epsilon<1)$, is there any sublinear query algorithm known/possible to decide if the graph has a clique of size $\rho$, or all the cliques are of size at most $\rho(1-\epsilon)$. More precisely, for a given probability tolerance $\delta$ and error tolerance $\epsilon$, algorithm queries $f(\frac{1}{\epsilon}, \frac{1}{\delta}).o(n^2)$ many position of the corresponding adjacency matrix of the graph, and accept with probability at least $1-\delta$ if the graph has a clique of size $\rho$, or reject probability at least $1-\delta$ if all the cliques are of size at most $\rho(1-\epsilon).$ Thanks in advance. 

Let $G(V_1, V_2, E)$ be a bipartite graph such that degree of all the vertices in $V_1$ is bounded by some constant (say) $d$. Now, for given two positive integer $l$ and $k$, we wish to decide if there is a vertex cover $A\cup B$ with $|A|\leq l$ and $|B|\leq k$, where $A\subseteq V_1, B \subseteq V_2.$ Now, I have following two questions: 1) What is the complexity of the above problem when both $k$ and $l$ are part of input. (Intuitively, it appears to be hard.) If it is hard, then is there any PTAS known to approximate the problem. 2) What is the complexity of the problem when one of the parameters $k$ or $l$ is constant? Pls let me know if there is any doubt about the problem. 

We have given a weighted graph $G=\{V,E\}$, where $V=\{v_1, v_2,...,v_n\}$, and for all $i,j$, the weight of edges $w(v_i, v_j)\in (0,W)$. And we have also given a weight threshold s $w$ (where $0<w<W$) on the edge as the input. Now, our goal is to find (probabilistically) cliques (of any size >=3) in the graph such that the edges which are in cliques having weight at least $w$, i.e., we need to find cliques having edge weight at least $w$. Pls let me know if I am able to put the question clearly. 

Beck's theorem is a classical result in discrete geometry which describes about the geometry of points in the plane. The result states that a finite collections of points in the plane fall into one of two extremes; one where a large fraction of points lie on a single line, and one where a large number of lines are needed to connect all the points. Formally, the statement of the theorem is as follows: Theorem: Let $S$ be a set of $n$ points in the plane. If at most $(n-k)$ points lie on any line for some $0 \leq k < n-2$, then there exist $\Omega(nk)$ lines determined by the points of $S$. Now, my question is that if the generalization/variation of the above result is known in the case of higher dimension $(dim\geq 3)$? 

You need workspace (registers set to 0) to read the user's input. Once you've read it, and completed the program, the user needs to erase the input and the output in order to restore the workspace. However, if you assume that the user wants to keep a copy of the input and the output, this is possible to do reversibly. Similarly for the program, to erase the program from the computer there needs to be another copy stored somewhere. If you don't allow any reversible operations between the user and the computer, and you insist that everything be done reversibly, then you can't load input into the computer, and you have a useless machine. 

This type of problem has been the subject of quite a bit of study. You can find references by googling random self-reducibility, and the Wikipedia article is a good place to start reading. There are still a lot of associated open questions. 

I can't answer my original question, but I can answer the question 3 (and 4), which I added when I offered a bounty because I thought the original question was likely too hard. In fact, I have two proofs of question 3. Here's the setting for question 3: We have a polynomial-time referee who, at the beginning of the game, gives players 1 and 2 correlated random variables. Players 1 and 2 then play the game, without any interference from the referee. At the end of the game, the referee looks over the transcript and decides who wins. I can show that deciding who wins such a game is EXPTIME complete, even if you are given the promise that the winner wins with probability at least $2/3$. ========Proof 1============ The first proof uses the fact that oblivious transfer is universal for secure two-party computation. Thus, if players 1 and 2 can perform oblivious transfer, they can simulate an arbitrary polynomial-time referee, so the previous results that refereed games are EXPTIME complete can be applied. Now, to achieve 1-2 oblivious transfer, at the beginning of the game, the referee gives the two players a large number of "oblivious transfer boxes." We describe one of these oblivious transfer boxes. P1 gets two random numbers, $r_1$ and $r_2$. P2 gets one of these random numbers, $r_i$ and the variable $i$ ($=1$ or $2$) saying which of P1's random numbers he got. To perform oblivious transfer, P1 takes the two pieces of data he wants to transfer, and XOR's them with $r_1$ and $r_2$. P2 can then decode one of these, but P1 cannot tell which one P2 can decode. This is 1-2 oblivious transfer. (Obviously, the referee also has to give the players oblivious transfer boxes directed the other way, from P2 to P1.) When I first asked question 4, I was worried that the secure two-party computation results didn't apply to this kind of interactive computation with a referee, but in fact it's quite easy to show that they do. ===========Proof 2=========== Now for the second proof for question 3. Here, we need to go back and modify the Feige-Kilian proof. In this proof, they consider a Turing machine T which is running an exponential time computation. Feige and Kilian encode the $2^n$ bits on the tape at time $t$ in a multilinear polynomial $Q_t($x_1$, \ldots, $x_n$)$ over a large finite field GF($p$). Now, the referee gives a point to P1 and a line containing this point to P2, and the two players give the evaluation of the point and the line on $Q_t$ back to the referee. The referee uses binary search to find a time $t$ where P1 and P2's evaluations of $Q_t$ agree, but their evaluations of $Q_{t+1}$ disagree, after which he asks P1 a clever question which will reveal whether she is the one who is lying. The first thing we will use is that, even with uncorrelated random coins, the referee can make players 1 and 2 perform bit committment, by having them XOR the data they want to commit with the random coins. Thus, we can talk about P1 and P2 putting things in sealed envelopes. One thing you might try to simulate the Feige-Kilian proof is: the referee gives P1 a lot of different points $p_i$, and P2 a lot of lines $\ell_i$, so that $p_i$ is on $\ell_i$. Now, at each step of the binary search, the players put the values $Q_{t}(p_i)$ and $Q_t(\ell_i)$ into sealed envelopes, and then the referee chooses one randomly for the players to open. The two players decide whether the values are consistent, and proceed with the binary search accordingly. Now, we've ruined the $(p_i, \ell_i)$ pair, because both players know the value of the point and line, but we still have a lot of other (point, line) pairs that we can use. (How can the referee choose one $(p_i, \ell_i)$ randomly if he gives the players instructions only at the beginning of the game? He can encode his instructions in the XOR of values he gives to the two players at the beginning, and the two players can't read the instruction until they both reveal the values at the relevant time.) This strategy doesn't quite work, because P1 and P2 don't have to be consistent about the time at which they start lying with two points (or lines) That is, P1 could let give the right value for $Q_t(p_i)$ and the wrong value for $Q_t(p_j)$. This would completely mess up the binary search, and make the protocol inconclusive. However, there's a neat trick we can use to force P1 to be consistent. Add a bunch of dummy points ${p}'_k$ to P1's set of points, and add dummy lines ${\ell}'_k$ to P2's set of lines. Let each dummy line have two points on it. If P1 happens to give the right value for one dummy point on a line and the wrong value for the other dummy point, then he has revealed himself as a liar, since there is no way for P2 to give the value for a line that is correct for one of P1's two points on it and not the other. We can do a similar trick to make P2 answer consistently. Then the only thing left is showing that the last step of the Feige-Kilian proof still works. This turns out to be straightforward, although going through the details would make this answer much longer. 

We have given a set of $n$ binary vectors each of dimension $d$, i.e. a binary matrix of $d*n$. Our goal is to group vectors which are almost similar, $\forall v_i, v_j\in\{0,1\}^d$, we say $v_i$ and $v_j$ are almost similar, if $HammingDistance(v_i,v_j)<\epsilon$. Since comparing two $d$ bit vectors are expensive, and $n$ is large, we can't afford to do $O(n^2)$ comparisons, we use Locality Sensitive Hashing algorithm to computer similarity preserving matrix in subquadratic time. Now, my question is that how to tune parameters of LSH, i.e. # of hash tables $m$, # of entries per hash table $l$, such that error is minimizes (the number of similar items which hashes to different bucket, and the number of dissimilar items which hashes to same bucket is minimizes). Pls let me know if I am missing something obvious. 

Carathéodory's theorem says that if a point $x$ of $R^d$ lies in the convex hull of a point set $P$, then there is a subset $P′ \subseteq P$ consisting of $d + 1$ or fewer points such that $x$ can be expressed as a convex combination of $P′$. A recent result by Barman (see paper) shows an approximate version of the above theorem. More precisely, given a set of points $P$ in the $p$-unit ball with norm $p \in [2,\infty)$, then for every point $x$ in the convex hull of $P$ there exists an $\epsilon$-close point $x'$ (under the $p$-norm distance) that can be expressed as a convex combination of $O\left(\frac{p}{\epsilon^2} \right)$ many points of P. Now, my question is that does the above result implies (or have some connection with) some kind of dimensionality reduction for the points in the convex hull of $P$. It seems intuitive to me (however I don't have a formal proof of it) - as for any point $x$ inside the $P$ there is a point (say) $x'$ in a close neighborhood of $x$ which can be written as convex combination of constant many points of $P$, which in some sense dimensionality reduction of $x'$. Pls let me know if I am able put my question clearly. Thanks. 

Let $M$ be a binary ($0-1$) matrix of size $n \times m$. We define binary rank of $M$ as the smallest positive integer $r$ for which there exists a product decomposition $M = UV$, where $U$ is $n \times r$ and $V$ is $r \times m$, and all entries of $U$ and $V$ come from $\{0, 1\}$. My question is that is there known algorithmic way to determine the binary rank of $M$; and Singular value decomposition that support the binary rank. Any reference in this regard would highly help. 

Here's a way you can find the optimal algorithm. Let's assume (for convenience) that all $x_i$ and $y_i$ are distinct. First, we need to prove two theorems. (1) If at time $t$, you would get on a train with travel time $y_i$, you would also get on any train with travel time less than $y_i$. (2) If you would get on train $j$ at time $t_j$, you would also get on that train $j$ at any time $t \leq t_j$. For (1), the proof is easy. For (2), the proof is based on the observation that the longer you wait, the higher the probability that any particular train shows up. So if there's some strategy of waiting for trains other than $j$ that has better performance than getting on a train $j$ with travel time $y_j$ at time $t_j$, that same strategy is going to give travel time $\leq y_j$ for all times $t > t_j$. Now, if train $\alpha$ is the train with minimum $y_\alpha$, then you want to get on this train any time it arrives. Furthermore, if you've waited $x_\alpha - \epsilon$ time for sufficiently small $\epsilon$, your optimal strategy is to wait for train $\alpha$. What we're going to do now is essentially continuous dynamic programming. If you've waited for time $t$, there are only $n$ possible optimal strategies. That is, get on any train with travel time $\leq y_i$. This optimal strategy changes at $n$ or fewer values of $t$. So, starting with time $x_\alpha$ and the strategy "wait for train $y_\alpha$", calculate the first time $t < x_\alpha$ where the strategy changes. You can do this by comparing $n$ possible strategies, each of which gives an expected travel time which is a function of $t$. Keep working to the left in this fashion, until you get to time $t= 0$. 

So Church clearly thought the Church-Turing thesis extended to machines. On the other hand, there seems to have been no effort made by Church or Turing to consider the ramifications of quantum (or other non-elementary) physics on computation, so it was clearly a very limited class of machines they were considering. 

You can be a quite successful theoretical computer scientist without programming. For a few people, programming is quite difficult, and if you are one of them you shouldn't despair and switch fields. However, for most math and computer science graduate students, learning to program is not particularly difficult, and is a skill which is very useful. You should learn a programming language, and if you enjoy it, you should try to get enough practice to become reasonably proficient at it. Then, when the point comes (and it will) that it will be useful in your research to write a program, you will be able to do it. If you don't learn to program now, it is quite likely that when you eventually need to write a program, you won't have time to learn, and so you may not actually write it, and end up being less effective in your research. While getting a grad student or an undergrad to do this for you isn't too hard, there are a lot of times when it's much easier and less time-consuming to do it yourself rather than explain the problem to them. What language should you learn? I'd recommend an object-oriented language, since these are the ones that are currently in most use, and I suspect this will be more true in the future. Maybe Python or Java—they're both object-oriented languages, and while they're used less in practice than C++, my impression is that they're both much, much easier to learn. (Caveat: I don't know C++, despite having worked at Bell Labs, so maybe I'm wrong about this.)