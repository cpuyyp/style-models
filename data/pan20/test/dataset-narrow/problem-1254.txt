Here are a couple of points regarding the practical implications of Lenstra-type results, and possible implementations in CPLEX, Gurobi, etc. One of the key steps in most of such algos for IP is branching on "good" or "thin" directions, i.e., hyperplanes along which the width of the polytope is not too large (polynomial in variables and size of data). But Mahajan and Ralphs (preprint here) showed that the problem of selecting an optimal disjunction is NP-complete. So, it would appear hard to create practically efficient implementations of this class of algos. Most of the algos implemented in packages such as CPLEX could be classified as branch-and-cut methods. This family of techniques typically work well on IP instances that are feasible, and are often able to find near-optimal solutions. But the focus of Lenstra-type algos are on worst case IP instances that are infeasible to start with, and the goal is to prove their integer infeasibility (and they study the complexity of this task). AFAIK, there are no class of problems with practical relevance that fit this description. So, CPLEX/Gurobi folks would probably not implement Lenstra-type algos any time soon. 

M(G)ods, if you deem this response unfit to be an answer, please move it to a comment. FWIW, one could write a binary integer linear program model for the problem: $$ \begin{array}{lrcll} \min & \sum_j a_j x_j \\ \mbox{s.t.} & x_i & \leq & b_k, & \forall i \in J_k,~ k=1,\dots,m; \\ & x_i & \geq & b_k - M (1-z_{ik}), & \forall i \in J_k,~ k=1,\dots,m; \\ & \sum_{i \in J_k} z_{ik} & = & 1 & k=1,\dots,m; ~\mbox{and}\\ & z_{ik} & \in & \{0,1\} & \forall i,k. \end{array} $$ The constraints with the extra binary variables $z_{ik}$ force a/the largest of the $x_i, i \in J_k$ to equal $b_k$. Typically, constraints of the $\min$-$\max$ or $\max$-$\min$ forms (i.e., minimize the maximum of a set of linear functions, or vice versa) could be modeled as linear inequalities, i.e., as linear programs, without requiring extra binary variables. In this case, we have to set the maximum of a set of variables ($x_i, i\in J_k$) to some predefined value ($b_k$). AFAICS, one could not avoid the use of extra binary variables when writing these maximum constraints as linear (in)equalities, for instance, as shown above. 

This is the "reverse" direction, but the well known Aanderaa-Rosenberg-Karp conjecture applies to graph properties that are monotone upwards (i.e if G satisfies the property, then so does any graph on the same nodes whose edge set contains E(G)). 

The definitive article is a paper by Hlineny and Kratochvil from 2001. In it they show that the problem of recognizing a disk intersection graph (your question) is NP-hard, which suggests that it will be difficult to come up with a clean characterization. They also point out that $K_{3,3}$ cannot be represented as the intersection of disks, answering the other part of your question. 

The Löwner-John ellipsoid of a convex set $C$ is the minimum-volume ellipsoid (MVE) that encloses it. The ellipsoid can be computed using Khachiyan's method, and there are a number of approximations available if $C$ is (the convex hull of) a set of points. Are there fast (i.e non-ellipsoid-method based) approximations to the MVE of a bounded polyhedron presented only in terms of the halfplanes whose intersections define it ? In particular, I'd be interested in methods that run in time polynomial in the dimension and the inverse error $1/\varepsilon$. 

Don't forget the Kneser conjecture and the Kahn/Saks/Sturtevant proof for the Aandera-Rosenberg-Karp conjecture. 

While there are numerous individual examples of sharp thresholds (2-coloring vs 3-coloring is another one), probably the best structural result along the lines you're looking for is Schaefer's dichotomy theorem. Roughly speaking, the theorem looks at very general classes of problems (called constraint satisfaction problems) and completely characterizes (merely in terms of the kind of input) when the problem is NP-complete and when it's in P. I haven't read it, but the linked wikipedia page indicates a recent survey by Hubie Chen that presents this result in a more general framework. 

Here is an instance that could be close to a counterexample to the claim. Consider the LP $P = \max \{ 1^T x \,|\, Ax \leq 1, x \leq 1, x \geq 0\}$ and its dual $P' = \min \{1^Ty + 1^Tz ~|~ A^Ty + z \geq 1, ~y \geq 0, z \geq 0 \}$ for the $12 \times 6$ matrix $ A = \begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 1\\ 0 & 1 & 0 & 0 & 1 & 0\\ 1 & 1 & 0 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 & 1 & 1\\ 0 & 1 & 0 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0 & 0 & 1\\ 0 & 0 & 1 & 0 & 0 & 0\\ 1 & 0 & 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1 & 0 & 0\\ 0 & 1 & 1 & 0 & 0 & 1\\ 0 & 0 & 1 & 1 & 0 & 0 \end{bmatrix}\,. $ An optimal solution of $P'$ is given by $y_1=y_2=y_{12}=1$ (all other variables are zero), with the objective function value of $3$. The Optimal solution of $P$ is given by the vector $x = [0.5~0.5~0~1~0.5~0.5]^T$. If you solve $P$ as an integer program, the optimal objective function value is only $2$, and $x = [1~0~0~1~0~0]$ is an optimal solution. In summary, the LP $P'$ has an integral optimal solution, but its dual, $P$ does not have an integral optimal solution. The primal-dual roles are reversed from the set up that Ankur wanted. But given the nature of LP duality, this instance could still be considered a counterexample to the general statement of the original claim. 

The framework of parametric search allows you to do this in many cases. While it was first developed by Meggido for parallel implementation, the idea is more general. Roughly speaking, you start with an oracle $g(x)$ that tells you, given a point $x$, to which side (left or right) the optimum-achieving $x^*$ lies. The idea is to "pretend" to simulate the cost function on the optimal solution. At some point, you're stuck because you don't know which path of a branch to take (since you don't know $x^*$). You identify the breakpoints of this branch, and then using $g$ decide which ones to explore further. I'm handwaving a lot, because parametric search is quite complicated. This survey talks about ways of making it practical, and also has a decent explanation of the basic idea. This older survey gives a number of examples of parametric search. 

A popular game at holiday parties in North America is the white elephant gift exchange. In brief (ignoring variations) it works as follows: There are $n$ people and $n$ wrapped gifts. Players are ordered arbitrarily. In the $i^{\text{th}}$ round, player $i$ either 

At the risk of sounding immodest, I wrote a short survey on stream clustering a few years ago. It's a little out of date, but not overly so, and doing forward citations will get you to the recent work in the area. 

Edit 2: Despite the upvotes, the first comment does not really solve much (it essentially boils down to a recursive answer: "in order to find the outer face, use the leftmost vertex, then find the outer face", which isn't as trivial to solve as its commenter seemed to think. @Zsbán provided what looks like the most efficient and elegant solution. Thanks! 

Find rightmost point $v_1$ of graph in the plane. Find rightmost point $v_2$ connected to $v_1$. Take the two faces bordering edge [$v_1$,$v_2$]. Let the longer one (in number of vertices) be $F_a$ and the other one $F_b$. Pick an arc $A_a$ in $F_a$ that is not in $F_b$ (we know there is such an arc since the two faces cannot be identical and $F_a$ is longer than $F_b$). If the middle of $A_a$ is inside the polygon formed by the vertices of $F_b$ in the plane, $F_b$ is the outer face, otherwise $F_a$ is. 

I have a planar graph, for which I have computed a combinatorial embedding and coordinates in the plane. So all my arcs are now oriented in the plane, respective to their end vertices. Computing a list of all faces of the graph (as a list of oriented walks along their arcs) is fairly straightforward. On the other hand, finding a simple way to pick the outer face out of the list of faces does not seem as easy: it is included in the list of faces, but since its orientation is "reversed" (i.e. relative to the outer region, rather than the inside of the graph boundary), all its properties in terms of neighbouring faces are identical to other faces. Short of using plane coordinates to painstakingly identify the outer nodes/arcs of the graph, is there a more straightforward way to pick the outer face out of the list of faces using the combinatorial embedding (given as a table of arc successors in the oriented plane)? Edit: Upon reading the comments, I realised that my wording was ambiguous (verging on the incorrect): while I have the combinatorial embedding and would like to use it (or the information derived from it), I also have coordinates derived from a drawing of that graph (hence the plane graph, not simply planar graph), ensuring that there is, indeed, a specific outer face. What I meant to say is that I am looking for a method that would do more (or rather: less) than just looking at the coordinates of all faces and (for example), compute their orientation. To be precise, the embedding and plane drawing give me: