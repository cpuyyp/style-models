For Digital Ocean in particular, you can boot the droplet into a rescue image. For older distributions that boot from an external kernel, you can select to boot it into a rescue image yourself. For newer distributions that boot from a kernel inside the droplet, you can file a support ticket to have the rescue image mounted as an ISO. Once you are done with your operations, update the support ticket to have the rescue image removed, and reboot. 

It looks like nginx isn't quite sure what index file to load for your site. Since it's a PHP-based site (WordPress), this should be . But your directive reads: 

I can imagine what would happen if you tried to run two opcode caches simultaneously. It looks a lot like this. 

This causes security contexts under to be applied as if they were under . You can then fix the contexts of any existing files with : 

It's also possible your workstation doesn't have any locales installed at all, which is usually how systems end up using the default "C" locale. Consult your operating system distribution's documentation to learn how to install locales. 

The official answer is no. Opcache only supports caching in shared memory; no other backends even exist. And trying to stuff the cache into memcached would just slow it down anyway, so there seems to be little point. 

You probably don't have a file . So of course it can't be found. Remember that these paths are relative to the document . You probably want something like: 

A very few sites will actually check your IP address to see if you're really Googlebot, but this is far less common than it should be. Another thing to do is to check for the presence of a file and use it as a list of URLs to crawl. Some sites provide this file for Google and other search engines to use to spider their content, but nothing says you can't also use it... 

The Hyper-V service on Server 2012/2012 R2 has no need of the GUI, and it is easily managed from a nearby 8.1 workstation with RSAT, PowerShell or whatever. (Indeed, the Hyper-V Server product is so stripped down it doesn't include the GUI at all, nor many other things.) 

If you really mean to access the server remotely, then open up the proper firewall port (default 3306). 

First off, I get my stateless autoconfiguration just fine, as well as my privacy address. The privacy address works on the global Internet, which I confirmed by visiting my IP address tool in Google Chrome. So we start fiddling. First, the GUI doesn't provide any configuration options for IPv6; see that "Properties" is grayed out. Everything you might want to do must be done from the command line. The suggestion from @MikePennington doesn't work because this command isn't supported on XP at all. It requires Vista or later. The so-called Advanced Networking Pack for Windows XP can be downloaded for SP1 systems, but this functionality was included in SP2 and whatever it provides should be available already on any SP2 or higher system. Once installed, an command becomes available, but it doesn't provide anything relevant to disabling neighbor discovery. Unfortunately, Windows Firewall on XP doesn't distinguish between IPv4 and IPv6 connections, and so it's impossible to select ICMPv6 messages to block in the firewall. This rules out any Windows Firewall workarounds. Finally, XP doesn't even support DHCPv6. IPv6 itself has matured since this code was released in 2003 (and Microsoft seemingly never updated it on XP) so it is limited by what was known about IPv6 almost ten years ago. As far as I can tell, IPv6 support in Windows XP should be considered "technology preview" and not used in production, or at least used with extreme caution and full understanding of its limitations. Of course, XP is nearing the end of its useful life at this point, (and some would say it already passed the end of its useful life) so if you can upgrade the box, that's probably the easiest solution. TL;DR: Look at the pretty picture. Weep. Upgrade to a current version of Windows. 

Libvirt can't make use of the space on your disk, because the disk is full. When you tell libvirt to use a whole disk device, it expects to be able to use the entire block device, ignoring any existing partitions. To resolve the problem, delete the partitions, or erase the partition table from the device: 

Add to your command. This will cause the tables on the destination to be recreated from scratch with whatever data is in your dump. 

You can only have one SPF record. Since you created four of them, which one actually gets used is essentially random. You should combine them together into a single record. 

Nothing but the encrypted data is stored when using plain dm-crypt. P.S. Don't use plain dm-crypt. The reasons why are at the very top of the same man page. 

Being a Network Solutions customer, I would guess you want to delete the two register.com nameservers. 

Your site was compromised, but the malware site that it linked to has been taken down. You will need to clean up and reinstall WordPress, and most likely the entire server. 

Google Mail's SMTP server is requiring you to connect with TLS, but you have configured fetchmail to never use TLS. Check your fetchmail command line and configuration file for and make sure it is set to . On the command line: 

Enterprise Linux distributions deal with this by backporting security fixes from the new version to the original version to which your distribution is locked. You install the updated system packages containing the backported security fixes, and note this in your report to the compliance vendor. Each report you receive of a potential security vulnerability should include a CVE number. Look up this number in Ubuntu Security Notices (see also Red Hat CVE for RHEL/CentOS) to determine the updates that your system needs. As a side note, if you are running a PHP-based web site, you often want bug fix updates in addition to security fixes. The distros almost never distribute bug fix updates unless they cause crashes or security problems, and sometimes not even then. In this case it's often wiser to use a PPA that tracks your desired PHP version (e.g. 5.3 or 5.4) instead of the system packages. 

Your configuration states that the error document is . Nothing in your configuration attempts to load at any time, ever. Rename the file. 

Your DNS TTL on your address record is set to 86400 (seconds, which is one day). Thus, when you make a change, resolvers on the Internet which have cached the old record may not update it until as long as a day has passed. It's best practice to shorten the TTL to as low as possible (usually 60 seconds) in advance of the actual change of IP address, so that when you do change it, the change will be "propagated" almost immediately. (There's no such thing as DNS propagation; it's actually caching.) If your TTL was set to 86400, then the TTL must be lowered at least 86400 seconds before you change the address. The TTL can be raised again after or at the same time as the address change, if desired. (And while you're fixing things, your SSL configuration needs a lot of work. Visit $URL$ to test your site, and $URL$ to get a secure configuration.) 

The calling service is broken, but there's not much you can do about it unless you can fix the caller. From RFC 2616: 

You can't directly find the processes that can transition to a given type, but you can sort of do it indirectly. It's time to get familiar with the tool. This tool lets you query the SELinux policy in a variety of ways. Here, we will see which types can transition to the type. Among them will be types for the processes you are interested in. As you can see, this also gives you some hints as to what the process will be allowed to do. 

SSL bump is supposed to work properly if you have imported Squid's CA certificate into Google Chrome or the operating system's store. On Windows, certificates should be imported into Trusted Root Certification Authorities; there are special instructions for the Chromebook. If this doesn't work, you have probably run into a bug in Google Chrome, but you should double check that squid's root certificate was imported correctly. 

The problem here is that the second parameter here, , causes each of the files in your directive to be tried in turn. If none are found, it then moves on to , which causes the same block to be re-entered, and since it still doesn't exist, you get an endless loop. I would rewrite this as: 

Forget this setup and buy a purpose-built device which performs the same function. These Ethernet-wireless LAN client bridge devices (which can be hard to search for since there's no real standard name for the device's function) range from as little as $20 USD on the low end to $1500 or more for a ruggedized industrial grade device. In an office, you probably don't need IP68 protection, though... 

It appears that your radvd configuration is missing the ABRO options, required for your system to be recognized as a 6LoWPAN border router. A minimal configuration would be: 

Docker can't be run in a container without a specialized setup. Get a proper virtual machine if you want to use Docker. 

We set default policies for the tables to ACCEPT; the traffic will actually be dropped by rules within each table. This gives us more flexibility. In particular, the OUTPUT table should always be set to a default policy of ACCEPT unless you intend to block outgoing connections. 

It "works fine" but the performance is going to be much poorer than if you installed Integration Services. This is because it provides paravirtualized drivers for performance-critical virtual disks, network adapter, etc. 

Here is what's going on: The two major ISPs in UAE, Etisalat and du, use SmartFilter by Secure Computing (now McAfee) to block content. This works by transparently proxying all users' web traffic through the filters. What happens then: When a user requests a web site, the SmartFilter checks its local blacklist (for instance, in UAE, all sites ending in are blacklisted) then looks up its classification (e.g. politics, news, social media, games, etc.) to see if it's appropriate. What happens is that new sites which have never been seen before are unclassified until someone at McAfee visits and classifies the site. This can take a few days. Companies and ISPs which use tools like SmartFilter can choose to allow or block unclassified sites, and it seems that here, they are blocked. The other problem with proxies set up in this manner is that they must interfere with the end-to-end connection between a user and the remote site. This can and sometimes does result in performance problems which are difficult to resolve. You may have a lot of difficulty finding the responsible person at the ISP who can actually help to resolve these sorts of issues. Services other than web sites (HTTP) can be interfered with in other ways, as well. 

Your cron job is running, but the job is having a problem. You normally should get an email with the output (and errors!) but you do not get email because: 

Your installed packages (specifically ) are out of sync with the distribution repositories. You need to get back into sync before you can move forward. To resolve the issue: 

The Linux bridge is a basic layer 2 switch. In order for it to send traffic to an interface connected to it, the traffic must be appropriate for that interface (i.e. the destination MAC address is reachable via that port). While layer 2 switches often have a port mirroring feature which forwards all traffic crossing the switch to a designated port, the Linux bridge has no such functionality. However, you can fake it with Linux's traffic control (tc). I do this to forward traffic to a KVM virtual machine running suricata. The limitation of this method is that you can only mirror traffic on a single physical port. In this script, the is the port to be monitored, which must be a physical port, and is the interface to which the traffic will be sent (which can be a virtual port or a bridge). The monitored port does not need to be in promiscuous mode with this method. And the mirror port does not need to be bridged to the monitored port. In my case, the host has a bridge br0, bridged to eno1 and to which all the virtual machines have a virtual NIC. I have created a host-only virtual network (as virbr2) for this VM and added a second NIC in the suricata VM on this network in addition to its regular NIC, and directed the traffic to it. 

If you haven't installed your own ssh key on the instance, you need to connect using the ssh key that Amazon installs into each instance by default (and provided to you). Example: 

In addition to the EC2 security group, your port 80 also needs to be opened in the CentOS firewall. You can use to do this from the root shell by selecting its Customize option. 

ships unconfigured. You must create at least one domain before starting the service. See the documentation for full information on using sssd. 

Zend OPcache is included with PHP 5.5 and later (and as from Remi) and is already installed on your system. You do not need to install the PECL package, which is only for PHP 5.5 or earlier. If for some reason OPcache is not installed, try installing the package from Remi. 

Don't do this. Use Linux Containers; this is one thing that they're well designed for. For bonus points, use libvirt or one of its frontends such as virt-manager to make your life easier when managing the container(s). 

This apparently is an issue with iOS. It does not automatically pick up WPAD. You can work around this by going into the Wi-Fi settings for your network, setting the HTTP Proxy setting to Auto and entering into the URL setting for the device. 

Hurricane Electric has a management-level presentation The Business Case for IPv6 which you may find helpful. It's a bit dated, but still useful: eweek's How to Build a Business Case for IPv6. You can find much more from an Internet search. 

DRAC is based on IPMI, so it will work approximately the same, but with fewer features. You will be able to get a remote console and change BIOS and firmware settings, update firmware, have alerts emailed to you, etc. All the most critical and basic stuff will work. In particular, the C6100 has what appears to be a very stripped down DRAC, which is also not branded as a DRAC. Nevertheless it will be sufficient for all of the necessary server management tasks, as described above. 

Ahh, this hearkens back to the good old days, when Unix systems were (nearly) the exclusive domain of universities and large enterprises, and Linux - with a version number starting with 0 - was almost unheard of. Back in these days the Web was just getting started; the Internet was email, Usenet, FTP, IRC, and not a whole lot more. Everyone who needed to have access to the Internet would be given an account on one or more of the Unix systems, and often log on to it from a green screen dumb terminal. They would typically have "full" access to a shell, along with the email service they were expecting, and most simply used it as such. Though, for security reasons, some places did not grant a full shell, and just dumped people into the program they were expected to use, such as an email client, factory application to run the conveyor belts, whatever. But the shell was still lurking in the background, and some of these programs had ways to get to it... When Internet access became more common on regular PCs and Macs in the late 1990s, people began downloading email via the POP3 and IMAP protocols directly to their home computers. These people (usually) wouldn't know what to do with a shell if they had one, and didn't really need one to get their email and Web access anyway. So mail servers started implementing "virtual" users, for whom mail was handled but there was no corresponding Unix account.