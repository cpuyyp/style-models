Usually it helps to have data centered at 0 and with standard deviation 1. I would reescale it such thar its standard deviation is 1. Appart from that, everything looks good. 

As the default for scale is to set axis to 0 (I wonder why!). If the problem persists comment it and I will edit. Edit Although your methodology is not wrong, it is more suitable to use sklearn StandardScaler. See the documentation of this class. Usually, it is better to fit the scaler with the training data and transform the test data according to that fit. 

Your precision and recall are not really moving significantly, neither is your loss. There are some predictions with probability around $0.5$, and I think that these are the reason for this slight trends. As I see it, the quantities that you claim to be changing are almost constant, so I wouldn't worry about it. I might be wrong, though, but the changes in loss function are ridiculous. 

According to sklearn's Logistic source code, the solver used to minimize the loss function is the SAG solver (Stochastic Average Gradient). This paper defines this method, and in this link there is the implementation of the sag solver. This implementation of the solver uses a method to obtain the step size (learning rate), so there is not a way that you can change the learning rate (unless you want to change the source code). 

Let me give an explanation based on multivariate calculus. If you have taken a multivariate course, you will have heard that, given a critical point (point where the gradient is zero), the condition for this critical point to be a maximum is that the Hessian matrix is positive definite. As the Hessian is a symmetric matrix, we can diagonalize it. If we write the diagonal matrix corresponding to the Hessian as: $$ D = \begin{bmatrix} d_{1} & & \\ & \ddots & \\ & & d_{n} \end{bmatrix} $$ the Hessian being positive definite is equivalent to $d_1 > 0, \dots, d_n>0$. Now let's think about deep learning cost functions. Deep learning cost functions depend on lots of parameters in a very complicated manner, so the Hessian will have a complicated expression itself. For this reason, we can think that the values of $d_1,\dots,d_n$ are not biased towards negative or positive values. For this reason, given any critical point, the probability of every value $d_i$ to be positive can be assumed to be $1/2$. Moreover, it is reasonable to assume that the values of $d_i$ do not depend easily on the values of $d_j$, due to the high non-linearity of the Hessian matrix, so we will take the probabilities of them being positive as independent events. For this reason, given a critical point, the probability of it being a maximum is: $$ P(d_1 > 0, \dots, d_n > 0) = P(d_1 > 0)\cdot \cdots \cdot P(d_n > 0) = \frac{1}{2^n} $$ The probability of any critical point being a maximum decreases exponentially with the dimension of the input space. In deep learning, this space can range from 1000 to $10^8$, and in both cases $1/2^n$ is ridiculously small. Now we are convinced that, given any critical point that we come across, it is very unlikely that it is a maximum. But what about minima? The minima of a function are the maxima of minus the function. For this reason, all the arguments used previously can be used to minus the cost function and we conclude that every critical point has probability of $1/2 ^n$ to be a minimum. For this reason, given a critical point, the probability of it being a saddle point is $$P(saddle) = 1 - P(maximum) - P(minimum) = 1 - \frac{1}{2^n} - \frac{1}{2^n} = 1 - \frac{1}{2^{n-1}}$$ Which is very close to 1 if $n$ is large enough (which typically is in deep learning). 

All you have to do is complete $\omega$ to an orthogonal $\mathbb{R}^3$ basis, as $\omega^T V = 0$ and $v_1$, $v_2$ are orthogonal. You can do this using Gram Schmidt orthogonalization. The two other vectors of the basis will be $v_1$ and $v_2$. Edit: The initial vectors to do the Gram Schmidt process can be any pair of independent vectors, for instance $(1, 0, 0)$ and $(0, 1, 0)$. 

I guess it depends on the algorithm, but linear models, as well as neural networks, treat all variables as continuous. The algorithm will not explode or anything if you supply 0.2 at prediction stage. However, your algorithm is trained on data. The algorithm can at best do what it has learnt from the training data. For this reason, do not expect anything meaningful when you feed an example with a value that has not been seen in the whole training set, or that it does not follow the training set distribution. 

We see that the loss decreases very fast when the learning rate is around $10^{-3}$. Using this approach, we have a general way to choose an approximation for the best constant learning rate for our netowork. 

Both approaches can be done: I recommend to validate after every batch when your are just playing around with your gradient method, to see if the validation accuracy goes up or down and to figure out how everything is going. In this setting, you can adopt early stopping, although there are many ways to prevent from overfitting. Early stopping checks, at the end of an epoch, your validation accuracy, and saves the model if it is the best so far. If the validation accuracy does not increase in the next $n$ epochs (and here $n$ is a parameter that you can decide), then you keep the last model you saved and stop your gradient method. Validation loss can be lower than training loss, this happens sometimes. In this case, you can state that you are not overfitting. 

This is generally recommended. There is no proven reason not to do it but many reasons to do it, and you should initialize as much weights as possible. Typically it is done until the last layer. Initialize everything that you can from pre-trained network. I would freeze the first layers in the first epochs, and after that unfreeze them. If possible, I would rather use differential learning rates, as this post shows. Totally, as shown above. Adam is more widely used, I would use that. Cosine scheduling with reestarts has been shown to be very effective, but any learning rate with reestarts should be similar. It does make sense. I wouldn't worry about that until you are overfitting, and you are not overfitting yet. If you overfit, I would recommend dropout instead of weight decay. 

Yes, I don't think this is the final result of k-means. K-means is an iterative process involving computing centroids and associating points to clusters. I think that the picture shows the state of the process when centroids have been recomputed but association has not been done for the new centroids. 

The pacakge in R has functions, like , that do this. Essentially, what this functions do is they compare different models with criteria that you can choose and take the model that does the best according to the criterium chosen. Please note that keeping the model with the highest $R^2$ is not a very good criterium, as models with more variables will always have higher $R^2$. The criteria that can use are adjusted R^2 and Bayesian Information Criterion, among others. 

In the fast.ai deep learning course, Jeremy Howard mentions a paper on how to choose the learning rate for training a neural network. This learning rate criterium is explained in this blog post, as well as this other one. However, there is no apparent mathematical reason to choose the learning rate they propose. Is there any mathematical formulation to obtain this learning rate? 

Courses: Andrew Ng Machine Learning Course from Coursera is what introduced me to machine learning, and I cannot think of a better way to do it. The course focuses on the theory rather than practice, but it offers solid theoretical foundations. fast.ai Machine Learning course is more programming oriented, I haven't taken it but it looks good. Books: Hands on machine learning with Scikit-learn and Tensorflow by Aurélien Géron has a decent amount of theory explained in a very simple way, and it complements it with lots of code examples. If you want to dig deeper in statistics and theory, The Elements of Statistical Learning might be a good resource. Practice: Kaggle is for sure the best place to practice and learn. After reading something or doing some course I advise to download the Titanic dataset and try to submit predictions.