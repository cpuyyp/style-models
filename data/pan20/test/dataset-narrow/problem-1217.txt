Basically, I would like a reasoned introduction to the design space of formalizations of the concept of 'finite set' in type theory. 

The other reason (than the one already stated) for are the (all too often unstated) requirements that 

As far as I understand, in Agda it is possible to represent all of that (i.e. all of Chapter 2 -- there is a library on github which does; AFAIK, the same is true of Coq). It is only when you get to later chapters that things get dicey. There are two obvious items: 

Ok, so apparently my Google-fu was not good enough. I just stumbled upon the CACS/AIC, which is the Canadian Association of Computer Science. But the web site is astonishingly bare bones, with no real sign of the purpose, etc. So still a 'no', but not quite exactly as I had said. And CIPS is different, as it really focused on the IT side of things, not so much on the CS. 

The circle. This is represented (in Agda) using a postulate, and so is not as nice as other things. $\infty$-groupoids. But this is an open problem on how to represent infinitely many coherence laws in a finite way. 

I'm a newcomer to communication complexity, and so far I've read the chapter in Arora-Barak and some papers giving lower bounds in various applications. A priori the definition of multiparty communication complexity is strange. Forgetting the fruitful applications of the "number on the forehead" model, I would imagine people first thought to have each player see only their own input and not all but their own inputs. The computation of the protocol would happen in parallel, and communication could be player-to-player or broadcast. I have reason to believe that lower bounds in such a model will aid me in proving lower bounds in a problem I'm currently looking at. Does this model have a name? What is known about it? 

My first question is whether the above claim is true (is there a naive argument that doesn't use SOS to solve this?). The second question is where numerical accuracy fits in. If I want to get an assignment that satisfies all constraints to within additive $\varepsilon$ accuracy, how does the runtime depend on $1/\varepsilon$? In particular, is it polynomial? The motivation for this is to, say, apply a divide-and-conquer approach on a large system until the base case is an $O(1)$-size system. EDIT: From Barak-Steurer, it appears that the "degree $l$ sum-of-squares algorithm" on p.9 (and the paragraphs leading up to it) all define problems for solutions over $\mathbb{R}$, and in fact the definition of a pseudo-distribution in section 2.2 is over $\mathbb{R}$. Now I am seeing from Lemma 2.2, however, that one is not guaranteed a solution/refutation at degree $2n$ without binary variables. So I can refine my question a little bit. If your variables are not binary, the worry is that the sequence of outputs $\varphi^{(l)}$ is not finite (maybe not even monotonic increasing?). So the question is: is $\varphi^{(l)}$ still increasing? And if so, how far you have to go to get additive accuracy $\varepsilon$? Though this likely does not change anything, I happen to know my system is satisfiable (there is no refutation of any degree), so I am really just concerned about how large $l$ needs to be. Finally, I am interested in a theoretical solution, not a numerical solver. 

be a total function from to be true. be as close to injective as possible. be as close to a monotone decreasing function as possible. 

It is that last requirement which really 'seals the deal' in forcing , since that is the choice which minimizes the distance to not being monotone decreasing. 

Does anyone know if the verified software repository is still alive? Bar that, are there other repositories out there of verified (or just certified) software? The archive of formal proofs is nice, but not quite 'right'. Finding singular examples of verified software is possible (with seL4 and CompCert being obvious examples), but finding whole groups of them is hard. I would have expected to be able to find whole repositories of certified software components by now, but I have been unable to do so. [While not TCS per se, the amount of underlying TCS issues involved in software verification/certification is enormous, so hopefully this will be considered to be on topic] 

Perhaps an even better way to see type inference is as a specialization of a single framework: Abstract Interpretation (abbreviated AI). The hallmark of most unification-based type inference algorithms is that they generate the principal type for a term; translated into AI terms, this means that you never need to widen, nor do you need to go to power domains to find the answer. Unification turns out to be the 'join' operation -- there is also a nice categorical interpretation of unification which is even more enlightening. Several comments shed light on this as well: all algorithms can be better understood in terms of generating then solving systems of constraints. Unification is indeed eager, and constraint-solving (and that includes control-flow based approaches) tend to be lazier. Perhaps an even better way to see type inference is as an alternate interpreter for a program: rather than producing values, it produces types. It is very easy to see type inference as a form a partial evaluation. Perhaps an easy introduction to that circle of ideas is via Aaron Stump's A rewriting view of simple typing. 

I have read a lot over the last couple of years about the early history (circa 1977) of TeX, and a lot of what Knuth has written. My conclusion is that the moment we speak about “TeX (as a programming language)”, something is wrong already. If we look at the early “design documents” for TeX written before (see and , published in Digital Typography), it is clear that Knuth was designing a system primarily intended for typesetting The Art of Computer Programming (he has said (e.g. here) that the main users he had in mind were himself and his secretary), with the idea that, suitably modified, it may be useful more generally. To save typing, for things one repeatedly had to do (e.g. every time TAOCP needed to include a quotation from an author, you'd want to move vertically by a certain amount, set a certain lineskip, pick up a certain font, typeset the quote right-aligned, pick up another font, typeset the author's name…), there were macros. You can guess the rest. What we have in TeX is a case of “accidentally Turing-complete” (more), except that it happened in the midst of a community (computer scientists and mathematicians, and DEK himself is to “blame” too) who were (unfortunately) too clever to ignore this. (Legend has it that Michael Spivak had never programmed before he encountered TeX, but he was so taken with it that he ended up writing AMS-TeX, at the time one of the most complicated set of macros in existence.) Because TeX was written to be portable across a large number of systems (which was a big deal at the time), there was always a temptation to do everything in TeX. Besides, because of his compiler-writing experience, Knuth wrote TeX like a compiler, and occasionally described it as one, and if the program that works on your input is a “compiler”, then surely you're programming, right? You can read a bit more about how Knuth didn't intend for any programming to be done in TeX, and how he “put in many of TeX's programming features only after kicking and screaming”, in this answer. Whatever his intentions were, as I said, people did start to figure out ways to (ab)use the TeX macro system to accomplish surprising feats of programming. Knuth found this fascinating and (in addition to adding some features into TeX itself) included a few of these in Appendix D “Dirty Tricks” of The TeXbook, but it turns out, despite the name, that “nine out of ten examples therein are used in the implementation of LaTeX”. Let me put it another way: LaTeX, the macro system that Leslie Lamport wrote on top of TeX, as an idea, is a great one. Authoring documents in a semantic, structured, human-oriented way, rather than (Knuth) TeX's page-oriented way, (or as Lamport called it, logical rather than visual) is a great one. But implementing something as complicated as LaTeX using TeX macros rather than in a “proper” programming language is, in my view and at least if it were done today, somewhere between a giant mistake and an act of wanton perversity. Even Knuth is shocked that people don't just extend the TeX program instead of doing everything in TeX macros. Today there are much better ways to do “programming”; you can use an external program in any of the many languages widely available on most people's computers, or you can use LuaTeX and program in Lua (and do a better job than you ever could with TeX macros alone, because you can manipulate internal structures and algorithms at the right level). And if you do it right, you could have programs that work better or faster than those implemented in TeX macros. The task of making programs in TeX faster is almost amusing when seen in this light, and reminiscent to me of the final words of the paper describing another “accidentally Turing complete” programming “language”: Tom Wildenhain's lovely “On the Turing Completeness of MS PowerPoint (video) from last year: 

I heard of a result in approximate graph coloring, but cannot find the source. The result is: For every constant $h$ there exists a sufficiently large $k$ such that coloring a $k$-colorable graph with $hk$ colors is NP-hard. Could someone please point me to the relevant paper? 

In Feldman-Gopalan-Khot-Ponnuswami 06 the authors show that agnostically learning parities reduces to learning parities with random classification noise. They also remark (among other things) that learning conjunctions in this model is NP-hard. Though I can't remember the source right now, I also recall that a random projection argument gives a reduction from learning parities with malicious noise to learning parities with uniform random noise. Is there a reduction from learning conjunctions with malicious noise to learning conjunctions with random noise? 

I have been reading a bit about the sum-of-squares method (SOS) from the survey of Barak & Steurer and the lecture notes of Barak. In both cases they sweep issues of numerical accuracy under the rug. From my (admittedly limited) understanding of the method, the following should be true: 

The "number in hand" model is the one I was thinking of, and there is quite a bit of literature about it. In particular I found this paper of Jeff Phillips, Elad Verbin, and Qin Zhang from SODA 2012 [1]. In particular they prove lower bounds on the problem I was interested in using, the undirected graph connectivity problem, of $\Omega(nk / \log^2(k))$. Here $k$ is the number of edges provided to each player and $n$ is the number of vertices in the graph. [1] $URL$ 

(With apologies for a long answer that goes in a direction different from the scope of the site: frankly I was surprised to see the question here in the first place….) 

Also, any future work should probably take into account (build on) LuaTeX which is the best modification of TeX we have currently. All of these are just idle thoughts (I haven't implemented any of them, to know the effort required or how much speedup we'd gain), but I hope this goes some way towards answering your question or giving you ideas for future directions. 

This is an answer to "[Fisher-Yates algorithm] isn't better than the naive algorithm. Am I missing something here?" which you asked in the question. In your "naive" algorithm which uses real numbers: how many bits of accuracy do you use? If you're counting bit complexity (as you seem to be doing for Fisher-Yates), and the algorithm uses k random bits for the real numbers, then its running time would be Ω(kn log n), since comparing two k-bit real numbers takes Ω(k) time. But k needs to be at least Ω(log n) to prevent two elements being mapped to the same real number, which means that the algorithm takes Ω(n log2 n) time, which is slower than the Fisher-Yates shuffle by a factor of log n. If you're just counting the number of arithmetic and comparison operations and ignoring their bit complexity, then Fisher-Yates is Θ(n) and your algorithm is Θ(n log n), still a factor of log n apart. 

As per the comment, $K(x|y) \leq K(x|y^*) + O(1)$. Now denoting the first metric (with the *) by $d_1$ and the second by $d_2$, we have $\displaystyle \begin{align*} d_2(x,y) &= \frac{\max \left \{ K(x|y), K(y|x) \right \} }{\max \left \{ K(x), K(y) \right \}} \\ &\leq \frac{\max \left \{ K(x|y^*), K(y|x^*) \right \} + O(1) }{\max \left \{ K(x), K(y) \right \}} \\ &= d_1(x,y) + O(1/K) \end{align*}$ where $K = \max \left \{ K(x), K(y) \right \}$ All of the theorems in the paper give the metric inequalities and universality claims up to an additive factor of $O(1/K)$, so this fits. 

Learning Parity with Noise (LPN) is usually stated with constant noise rate $\eta < 1/2$ on the labels, and it is believed to be hard to learn because of the high statistical dimension of the problem (they are not SQ-learnable even if the parity is assumed to be sparse). I'm interested in a noise rate which decays with $n$, the number of features, and I want to know where the known easiness threshold is for this problem. Specific notes: 

I found this paper of Cuomo and Oppenheim, where they use a Lorenz system to define an encryption scheme for signals. There is also this blog post describing and implementing the technique. The technique uses an interesting property of the Lorenz system known as "synchronization," which as far as I know occurs in other dynamical systems as well. Moreover, the chaotic nature of these systems intuitively makes guessing the parameters of the system infeasible, so a natural shared secret key is the set of constants used to define the system. I was wondering if there has been any theoretical work in cryptography that deals with dynamical systems. Specifically, are there established cryptographic hardness assumptions for specific dynamical systems? It seems like it would be a different flavor of hardness assumption from the usual ones, because the methods naturally introduce some noise into the decrypted message.