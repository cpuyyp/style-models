I would advise against NFS. Simply put - we had a web server farm, with JBoss, Apache, Tomcat and Oracle all using NFS shares for common configuration files, and logging. When the NFS share disappeared (admittedly a rare-ish occurrence) the whole thing just collapsed (predictable really, and I advised the 'devlopers' against this config time shortcut). There seems to be an issue with the version of NFS we were using that, if the target disappeared during a write, the client would drop into a never ending wait loop, waiting for the NFS target to come back. Even if the NFS box reattached - the loop still did not end. We were using a mix of RHEL 3,4,5. Storage was on RHEL4, servers were on RHEL5, storage network was a separate lan, and not running on vlans. If there is a load balanced front end, checking single storage - would this not bottleneck your system? Have you considered a read-only iSCSI connection to your storage, with an event driven script to move the uploaded file to the storage via ftp/scp when a file is uploaded? The only time I have implemented a successful centralised storage for multiple read heads was on an EMC storage array... All other cost effective attempts had their drawbacks. 

is this even feasible? Yes. I would suggest you look at TXT records for holding this information what are the pitfalls? Not sure how do I size the DNS servers. Not sure, but as far as I understand it DNS is a fairly low requirement service using UDP to handle requests and replies - as such it is up to the client to determine if it got an answer or not and re-ask if it was unsuccessful. 

I used to host the canteen menu on a finger server, so that everyone could see the days menu <suck eggs> I'd also suggest that you write your program with specific server/groups in mind and not rely on the default DNS server :O </suck eggs> 

A client of mine has been in this precise situation. He left his WiFi open to all as a gesture of good will, two months after he did, he got a warning from his ISP about his data usage, and a week after that he received a cease and desist letter from the RIAA He no longer has open WiFi. 

I am trying to avoid having to implement a UDP load balancing proxy, or write a new custom plugin to replace ec_dns_lookup. 

You should migrate your mail using the utility from Dovecot. This will preserve the UIDs and even POP3 UIDLs if necessary. Run using the option, to 'reverse backup' from the remote IMAP server to the local Dovecot server. You need to have a special configuration file created, something like this: 

Within templates called from the define, these can be simply referred to a and (but watch out for reserved words!). 

However, when the application is running with live data over a longer period, we find that this no longer matches. 

Since you are using MRTG with Routers2, there is a generic cfgmaker host template available at $URL$ which will automatically generate MRTG configurations for many things, including the storage OIDs. These take advantage of the Routers2 additional features to give you combination graphs. It should work with any SNMP-capable host. You can use it with standard MRTG cfgmaker like this: 

Other people have reported a similar issue with FCGID processes hanging and being unkillable in other systems, such as Wordpress and Sympa. A suggested fix was to add the option 

Example: A similar method can be used to protect a location using a different method, such as or . If you use , you can return the email address in the same query, from where it will be loaded into the environment. You can then use in your Apache config to push it into the HTTP headers to be picked up by the definition. See here for the documentation. 

Sympa authentication is configured by the file. This can contain one or more stanzas defining alternative authentication methods, such as the internal database, LDAP, cas or generic_sso. Sysmpa identifies users by their email address. The first two (internal and LDAP) take the user email address and password, and authenticate directly. CAS authentication uses a CAS service. Generic_sso authentication uses the Web server's own authentication to return a userID, and then obtains the user email address either from metadata or via an LDAP lookup. One example would be using Shibboleth (via mod_shib) and pulling the email address from the Shibboleth metadata. However, any web server authentication may be used, so you can easily use mod_mysql or similar to authenticate against an external user database. In order to get the email address, you can either use an assosciated LDAP lookup, have your web server authentication module return metadata (as an HTTP header), or ensure that the authenticated userID is the same as the email address. In short; use generic_sso, and then configure the necessary authentication in your web server, making sure to return the email address in the metadata if you cannot map user to email via an LDAP lookup. The (admittedly poor) documentation on this is here : Sympa authentication Example: This stanza uses to authnticate via Shibboleth; if the metadata is returned then it will be used, otherwise an LDAP lookup will be performed to obtain the email address. In order for the authentication to work, the location is configured in the web server to be protected by Shibboleth using . 

It is not possible to snoop on your machine without extra software. Remote Desktop does not allow a 'view' or share - Remote Assistance does but requires you to initiate it. VNC/RAdmin etc require that you install software on your machine, and although it can be push installed from a remote location, it's not generally done, and will alert you that it is installed or that someone is using the connection. The same applies to LogMeIn and GoTomyPC Back Orifice will do exactly what you are afraid of, but is detected by even the lowliest of Antivirus products. Dameware allows surreptitious observations of a machine (most others tell you that you are being observed) and also allows the remote administrator to push the installation onto your machine as standard. The real question you should be asking is this ... If you are scared of being discovered - should you really be doing it in the first place? Is it breaking acceptable usage policies? Are your actions illegal / against your employers company interest? If so - do not be scared of doing things you could be disciplined for by SIMPLY NOT DOING IT! Of course it is also possible to identify the port your PC is connected on, set that as a monitor port and send all its traffic to a packet capture station (thats what most big corporates do in cases where monitored activity is required) There - they see everything. I have before now installed SSH on windows machines, so that I can run command line utilities such as PSLIST and PSKILL to see if people are doing things they shouldnt and stop them) So the short answer to your question is 'No, not easily' The long answer to your question is 'Yes they can monitor everything you do, the level of their monitoring will take different amounts of effort, and often the effort is too much for the end results' 

I have set up a business that ran purely on Open Source. SuSe desktop, Mitel SME for fileserver / eMail. It all ran beautifully until... 

Recipient domain checked for local: If the recipient mail domain is handled by this MTA, then any aliases are expanded. If it is still local, it is delivered, and the process stops. Recipient domain checked for explicit route: If an explcit SMTP route is defined for this recipient domain, then the mail is passed to the defined server using the defined method and the process stops. Smart host: If a 'smart host' SMTP route is defined, then all mails are passed to this server and the process stops. MX Resolution: The recipient domain is checked for MX records. If any are found, then they are tried in order until one accepts the email. Then the process stops. A record resolution: The recipient domain is checked for an A or possibly AAAA record. If one is found then the mail is passed to the MTA at this address and the process stops. Bounce: If it gets this far, the message is undeliverable and is bounced. 

Now, the filesystem reports a usage of approx 110GB, but the zramfs device reports 165GB. At the same time, the zramfs memory is exhausted, and the filesystem becomes read-only. The zram figures confirm that we are getting a 2.2 : 1 compression ratio between orig_data_size and compr_data_size; however, why does the filesystem show much more free space than the zram device? Even if this is space already allocated for re-use by the filesystem, shouldn't it be reused rather than allocating new space? The data consists of a large number of small files which are added and removed at irregular intervals. 

This assumes your DS to be names of course, and is not as efficient as doing a when you already have the required RRA. 

This is often caused by a setuid CGI script that hangs; it exceeds the IOtimeout, and apache tries to kill it, but is unable because of the change in uid, resulting in the error. You may want to increase the FcgidIOTimeout or FcgidProcessLifetime to allow the thread more time to complete. Another workaround is to make the Apache server run under the same UID that the setuid script is chaning to. This allows it to kill the process, though it may not be advisable for security reasons. Similarly, running apache as root is also a workaround but not very secure. If you do this, note that your fcgi sock directory (under /var/lib/apache2/fcgid/sock or similar) and process table file need to be writeable by the apache process owner. The root cause, though, is the CGI script itself taking too long. The cause for that depends on the CGI code which I have no visibiilty of. 

Personally, I tend to use binary for every transfer. I was stung once with a file that had been placed from a windows machine onto a unix machine using binary transfer. As a result the txt file still had CR LF endings. ASCI mode looks at source and dest platforms and performs line-ending translation, so the resultant file I got had CR CR LF line endings (LF was translated by ASCI to CR LF as it was unix -> windows) Sounds petty - but it was a 20Gb log file, and I only had one time window in which to collect it. I use eol conversion utils on the local machine if the necessity arises. EDIT: I was getting the file back onto windows from the unix host 

The first indication that your DNS was not working, would be sporadic enterprise wide lookup failures. Note also, that almost every computer system that makes these requests, will cache the result for 30 mins or up to the records ttl value if its less than 30 mins. So careful handling of these values would be needed. 

There was also some fairly major headaches getting single sign-on working with NIS/Kerberos and all in all, I think we spent more time in support than we saved on licenses from MS From a purely non-tech point of view, the differences between Open Office files and Microsoft files caused a significant headache. I know that OO saves as MS Office files, but it just did not work in the real world. To take eMail as an example - People also get comfortable with the Outlook instantaneous update you see with exchange - not so happy with a 5 minute poll on a local IMAP / POP server. 

Can you boot in vga mode? if so - try a less capable video card? I had issues after installing Vista on an nVidia machine. After rebuilding it a couple of times, I discovered that it was a known problem, and there was an updated nVidia driver (the 'even more recent' one on windows update was no good) 

You should try to run wkhtmltopdf.sh instead of just wkhtmltopdf. Seems like you are calling the original binary instead of the xvfb wrapper you created. To make sure which one you are running try to run "which wkhtmltopdf". 

Not familiar with MS DNS, but it seems that you should do the opposite of what you are doing. I mean, you are probably doing: 

Sure, the DNS solution as per tomstephens89 answer is one possible answer, but it works only if you type any domain name, and also that won't work if the client alter the DNS servers pushed by the server. What do you really want is a solution, which is sometimes called Captive Portal. I am not strictly inside that topic, but SQUID which is a powerful HTTP proxy (which can be configured to be a transparent one - i.e. no configurations on client side needed) can help redirect the user to your page. 

What am I doing wrong (except for using a SOHO router for tor, lol)? PS: Using DD-WRT firmware EDIT: Another OOM killer log - this time nothing about resetbutton: 

I am running an ARM router with i2p and tor on it - a Netgear R7000. Of course I've added a full 512 MB of SWAP to it to prevent OOMs, understanding that it could slow down the system... But then I still get OOM killer starting with plenty of SWAP free, and killing tor! More interesting, after killing tor it seems that the system is OK for an unlimited amount of time... But still seems like tor couldn't be swapped. I even tried to turn off the overcommit, didn't helped at all. See the dmesg log below 

Here I first go by "try_files" directive to check if I can get the static file, but if not, I go then by the fastcgi_pass directive. But I'm hosting the aspx files in different folder, not the same with static files. 

It seems that when you access the root directory, "index" directive gets applied, but when you are accessing directly, it applies the "fastcgi_pass" directive. You should probably check before the pass directive if the file is even present. Seems like I have a similar need as yours, I use this config for it: 

If you place another gateway device between your Ironport and the Internet, then your only option is to disable SenderBase and any other IP-based authorisation in your Incoming Policy definitions in the HAT. You cannot tell the Ironport to obtain the previous-hop IP address from any other method than the incoing TCP connection itself (for obvious reasons - otherwise it would be far too easy to forge). One option would be to reverse the order of the devices -- IE, put the Ironport between the Internet and the Proofpoint box, and set the Ironport to have a fixed SMTP route to send all incoming email via the ProofPoint. Otherwise, you lose out on the Ironport's Senderbase rules, which are (IMHO) one of the primary benefits of the Ironport. 

If you have the $, then you can get the Shield software for Elasticsearch. This will allow you to use PKI certificates for authentication and authorisation to the ES cluster. Combined with iptables to restrict access, this should be everything you need. If you're trying to do it for free, then use Apache as a reverse proxy in front of ES to place SSL over the top with some sort of authentication - either Basic user/password, or Certificate-based if you want to be really secure. Again, iptables can block direct access to ES from outside and you can use the Apache access rules or iptables to block unauthorised IPs. 

You need to make sure you set the relevant spaces correctly in order to make everything line up correctly. For example: 

In your example, you should be using the namevar, . For example, if you define a resource like this: 

This example sets 400 rows; you may wish to use more. When you have a 5pdp RRA, then you can use , otherwise if you only have a 1pdp RRA, you can only use a resolution of 60 (one step). You may also like to take a look at which (in the same way as ) allows you to define output calculated values which can be summaries.