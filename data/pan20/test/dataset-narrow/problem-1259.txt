I understand that this would be desirable under some situations, like using HDR, but in this particular case it's not the result I want. I'm using OpenGL 4.4 but I assume this problem and a solution is API agnostic. 

You can see on my cubemap that the +Y, -X and +Z faces are stretched to the top left, and the other faces are stretched to the top right. One of the things I was doing wrong was only sampling half of the noise, and then stretching it to cover the texture. This happened because where I was getting the direction from origin, was only ever in one direction along that axis. E.g. Let's say my origin is at and the radius is . 

When my is at I get an on the whitest parts of the texture is say and on the darkest parts of the texture could be . With a of I will get ranging from (when we're nowhere near the light, i.e. the other side of the sphere) through to when we're on the tip closest to the light and the diffuse color of the material is also full white. I can skip specular here. Because the end result is now . The problem is that I'm now losing some detail because any values that should have been are now full opaque white. 

RenderDoc can get you a lot, if not all, of that info. I've previously used it during development to pull out my normal-maps, light maps etc. It also pulls out all the textures used in any fragment shaders etc. 

This really all comes down to the type of game you're making, and what you'd like to (/have time to) add, in order to promote this growth. From the standpoint of making something like a traditional RPG, take a tip from Fallout 1 (NOT 3, whatsoever): Build a game in which it is impossible to do everything with one character. Build a game which doesn't support "classes", but rather customizations -- as the player customizes, open doors and paths and bonuses for those customizations (/modules/attributes/etc). However, limit the AMOUNT of modules/perks/bonuses/boosts/etc, so that a character MUST either be semi-specialized, or suffer the penalty of being a very underpowered jack-of-all-trades, who has limited/no access to the additional content. In the level/quest design phase, make it clear that a particular skill/stat/attribute (or one of a set, or several in conjunction) must be used, in order to access that content. (eg: to go on a specific quest with a modern-day Robin Hood, you must have 75% lockpick, 60% speech, agility of 8 and a perception of 6). Make it clear, as you pass each of these checks, that a user DOES need to meet these requirements. Have entry to another quest be based on picking up the back end of a car, and moving it out of the way... ...or based around having a carry-weight of more than 300lbs... These are things which would appear to the player as something they CAN do, in a meta-gaming sense, but CAN'T do with their current player, without coming back 20 levels later, and dedicating all of the rest of their stat-points to. 

So I need to set up the dimensions of a orthographic projection that will encompass the entirety of a bounding box when viewed from any arbitrary position. That's an eye (programmer art); and the vertical line next to it is my view. How can I work out what the size of this view should be so that it encompasses the furthest corners of my rotated rectangle (from the viewports perspective)? I want to do this in 3D obviously, I thought 2D would explain it better. So far, I've considered that if I have: 

I'm working on a game and engine as I go under OpenGL. I've had a bloom effect in place for a while that works as a post-procesing effect on the entire screen. It's been fine up until now. I have an object that is made up of a few meshes, and one of these meshes is mostly white. The result is it blooms like crazy, but I really don't want it to. The only way around it I can see is that the objects I do want to have bloom would need to be rendered to a bloom-framebuffer from which I can bloom (and store the depths) to then mix back in with the rest of my scene. Is this the only way to go about it? It seems like a lot of work to simply isolate something from a screen-wide affect. 

I'm looking into hdr tonemapping and so far all the equations I've found result in really, dulled results. One i haven't tried uses an average luminance of the image. The pdf I've been reading suggests to generate mipmaps on my texture to get down to a single pixel that will then have average luminance. However, in my tests this performs very slowly. I.e. I can't use it for realtime graphics. Am I likely doing something wrong, or should I expect to be able to generate mipmaps for a framebuffer in realtime (I obviously need to generate them each frame). If I shouldn't expect to be able to: Is there another method to glimpse average luminance of the image on screen? 

The padlock in the top right corner of the image indicates that this inspector panel is in "locked" mode. Clicking on this icon will toggle the lock on and off. Further information on this feature is available on the manual page Inspector Options. 

If this doesn't provide enough information then setting a breakpoint and setting though the code will usually find the problem. 

As you can see the field for the generic class is missing. How can I expose a field with a generic class to the inspector? 

As an aside if your score is rising unpredictably it would require that your and flags are being reset. 

Collision events will only be detected under specific circumstances. Each collider can be considered to have these relevant properties: 

I have some utilities implemented as generic classes. I would like to have fields of these types exposed to the inspector but they appear to not display. This test code: 

These No Warranty / "as is" clauses are intended to reduce any liability brought against the software developers/publishers. 

When game starts and the method runs it calls When runs it immediately yields for one second, then after that delay it calls . This will cause to run repeatedly starting after a two second delay, and then again every second. When runs it: A) increments , B) calls (see step 2) , and then C) increments score again. 

If the sectors are of equal size then one approach would be to calculate the angle, divide by a full rotation and then multiply by the number or sectors: 

The coordinates of each corner of my bounding box from it's model matrix perspective With this, I could easily work out what the furthest ones are IF I was able to multiply it with my view matrix. However, it's the view matrix I'm trying to calculate... 

You'll end up with each faces UV coordinates being filled with the noise that is at the spherical equivalents position in the noise. (I hope that was in english). I will try to remember to update this later with full code once I've cleaned it up. 

And you can add in a shadow map or light attenuation easily. Even color the light via the values. My problem is that I have a cloud texture that's all white values with varying levels of alpha. This is textured on a sphere. When rendering, what happens is that at the very appex of the light, i.e. the point closest to the light source the following seems to be happening: 

March through each of the faces of the cube and each UV coord for that face Normalize each position vector Multiply by the radius of your sphere. You now have the coordinates of the sphere edge These coordinates can be used to sample from your 3D noise function 

I would go with a textured cone. Or maybe a cone + a cylinder. The texture could be animated somewhat if you really wanted; but most likely I would use my frag shader to offset the texture lookup coordinates in a repeating ring according to time. So this "ring" of offset texture coords moves down the cone+cyclinder over time and repeats a bit. Probably start with a function of sin. This would be like a heat wave repeatedly washing down the length of the exhaust. Maybe add some other offsets to give the whole thing a slight wiggle as well. This will mean that from directly behind, you would see very little except a ring from the inside of the cylinder and the smaller cone nestled inside. So from behind it would basically be two circles. As your view moves to the side you can see more. Now, to make it non-visible from other angles you can just use the camera position in the frag shader to determine the view vector and change opacity accordingly. These simple shapes would have more performance than particles. As for the texture, I wouldn't actually use one. But it helps to think of the above with texture lookups (in my opinion). Instead, I'd just use a colour ramp in the frag shader to colour texels different colours based upon how far the are. An additional variable, "thrust", could be used to determine how far down the colour ramp it should be sampled. This way, one end of your colour ramp is always going to have alpha 0. With low thrust, this alpha 0 part starts half-way down your cylinder. At high thrust the alpha 0 is further toward the tip. The values closer to the rocket could be more orange or something. Finally, I'd use the thurst variable to bring the overall luminance of the colour up. So higher thrust drives us closer toward a white-hot flame. 

Well, first, you're right that if you want to have thousands of permutations of these scenes, which operate in "real-time", your options are either to make a stupid amount of videos/slideshow images which would be altogether prohibitive in size... So you've got a few answers... You can certainly build a 3D scene, where you build a render-engine which takes models/textures, and animate them. You probably don't want to have thousands of stars on screen, though. The alternative is similar... ...build an engine, and instead of making 3D models, just use images (transparent) of batches of stars. You can move the image of a star (or stars) around. Likewise, those sprites (think of it like using collage cutouts, you're moving around on a board), you could treat some of them like a slideshow -- so mini videos or mini slideshows of just small groups of stars and tiny details which you couldn't make models. And then you can randomize collections of those pieces, their positions, movements, et cetera... Voila, reusable pieces (smaller size), useful for creating many different scenes. 

Set different paces (rather than speeds) for your text-display. Have user-selected "speeds", which operate like a multiplier (slow_rate = slow_pace * speed) When starting a cutscene, allow for users to hold the text box to speed up delivery... Or allow them to tap the box to dump that full card worth of text into the box... Allow them to tap again to skip to the next box... Consider allowing a swap between auto and manual (and sped-auto, by holding)... Give them an option to outright skip over the cutscene, and teleport them to the next section 

Okay, I solved the problem. As the guys commenting above pointed out, the code was weird. It wasn't really clear what was happening. This was a result of my brute forcing axis until the individual faces aligned. However, it's what resulted in the above. So I've taken the time to draw on paper how each face should be stepped through and I found the answer. It might not be immediately clear in the picture of the cubemap above but the whole noise is stretched. If you look at this reference: 

I have, so far, been able to create the vertices and UV coords etc for a sphere that would be textured with a cube map. I have also successfully loaded a cube texture from file and applied it to my sphere and it looks awesome. So now, I want to randomly create a noise cube texture map to apply to my sphere. I have a class which can create 1d, 2d, 3d and 4d noise. My problem is I don't know the math needed to sample a sphere map out of the cube, and then how to translate this to the 6 textures that make up with my cubemap. I guess first I need to know how to first sample the points of a sphere in 3d; and then how to translate these to x,y,z coordinates for the 6 sides. I can probably work it out from there. I would have thought this would be simple enough to Google, but I'm not finding what I need. SOLUTION For whomever comes along next, here's the answer. @Jason has the right idea below. Here's how I solved it: 

If familiar with OpenGL Immediate Mode this approach is pretty straight forward, but if you later decide that you would like to texture your primitives I would recommend using a MeshRenderer with a custom mesh. Morten Nobel has published a tutorial on using this approach with 3D meshes his blog. The same approach can be used create a plane of arbitrary shape. 

It you look at the or () in the inspector under Constraints > Freeze Rotation you will see a checkbox for each applicable axis. These checkboxes will stop the physics engine from applying any rotation about that axis. 

A can be used to do a quick search for any colliders below the player. can then be examined to determine the slope of the collider. Here is a sample implementation: 

Applying the decoration to a script will ensure that the GameObject has the specified component. If the component is missing: 

A great explanation of this technique as a demonstrated by SMW is has been posted (also by Shaun) on youtube. The transitions between these modes can add additional complexity. Of course when done right the player will be unaware that anything special is happening behind the scenes. However this complexity can cause problems as demonstrated at about 3:35 in the second video. 

For data that is created at design time you can use the TextAsset resource type. Note that despite its name it can be used for arbitrary binary data if preferred. To persist data that is created at run time PlayerPrefs provides a key-value store. This will be implemented in a platform appropriate manner and performance may vary. If storing a large number of items at once is causing delays it may be improved by consolidating the data into a single sting. To simplify parsing of this data many people use SimpleJSON. 

I'm going to get at the start, to give me result of and at the end of the loop: and the result is ! Whoops! On both the positive and negative x axis (and this is the same for the other axis), I'm always sample from the negative direction on my where, but in one case I'm taking all the noise, in the other only half and stretching. So that's why it matched up as well. It was from the same point and travelling back in the same direction. AND that's what gave my the stripe where it looked like it was being dragged up, it was in fact and mirror image, with one side of the mirror being stretched and distorted more than then other (so it wasn't immediately apparent). A few things need to change. I needed to sample around the origin from the beginning and correct the positions for the texture later. This meant that I no longer has to screw around with which axis goes where either. So the amended code should look like this: 

From the start of the loops this will be which gives me , at the end it will be which is then . This is the negative x axis BTW, the the direction is in both cases, negative on the x axis, so this is good. But then on the positive X axis 

I can't provide you with specific code or a tutorial, because I've never done it before. But if you're at the stage where you're going to be trying different effects, you should be able to do the coding, but I'll tell you how I'd do it. It's also been a long time and no one has answered, so I hope this helps. First, because you want: