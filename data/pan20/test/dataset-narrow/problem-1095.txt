The notion of "timing out processes" refers to the ability of knowing when to conclude that a process must have crashed. If you have a completely asynchronous system, then it does not help to equip processes with perfectly synchronized clocks, as these cannot be used to distinguish a slow process from one that has crashed. 

DHTs: To give you some pointers, you might want to look at distributed hash tables (DHT) such as Chord, CAN, Tapestry, and Pastry. Skip Graphs: Since you mentioned skip lists, you might be interested in skip graphs, which is a data structure providing range-queries and $O(\log n)$-time operations for lookups, inserts, etc. The advantage of a skip graph (vs a skip list) is that a skip graph contains an expander as a subgraph with high probability. This implies that routing can be done efficiently (i.e. link congestion is low) and that the skip graph remains connected even if a lot of nodes fail. 

Working on network design this summer I have come across certain applications that have inspired me to ask the following question: Given an undirected connected graph $G=(V,E)$ what is the minimum number of vertices, $k$, sampled uniformly and independently at random from $V$ (without replacement), such that the induced subgraph on those vertices is connected w.h.p.? Notice, I am not looking for a way to sample a connected (induced) subgraph, I am looking for a probabilistic guarantee that I have sampled enough vertices to end up with a connected induced subgraph. Does this sounds familiar to anyone? A quick search was not very promising so I decided to ask here if anyone has any references or even directions to point me at. I am assuming the answer would be a function of some structural property(/ies) of the graph, like connectedness, minimum degree, expansion, etc.. 

Recently, I went through the painful fun experience of informally explaining the concept of computational complexity to a young talented self-taught programmer, who never took a formal course in algorithms or complexity before. Not surprisingly, a lot of notions seemed strange at first but made sense with some examples (PTIME, intractability, uncomputability), while others seem to come more natural (problem classification via reductions, time and space as resources, asymptotic analysis). Everything was going great until I accidentally admitted that SAT can be solved efficiently* in practice... And just like that, I lost them. It didn't matter how convincingly I was trying to argue for theory, the kid was convinced that it was all artificial crap math that he should not care for. Well... ¯\_(ツ)_/¯ No, I was not heart-broken, nor did I really care about what he thought, that's not the point of this question. Our conversation had me thinking of a different question, 

It shows that there is no asynchronous deterministic consensus algorithm that tolerates even a single crash fault. Note that in the synchronous setting,there is a deterministic algorithm that terminates in $f+1$ rounds when $\le f$ processes crash. It introduces bivalence and univalence of configurations (*), which are used in many lower bounds and impossibility proofs later on. 

There are also locality issues since, in a distributed system, each node runs its own instance of a distributed algorithm and has only a local view of the network due to being directly connected to only a small number of other nodes. (Typically you would want a node degree of $O(\log n)$ to make the system scalable.) These issues come into play when maintaining global state such as counting the number of data items, finding the maximum, etc. 

The answer is negative. Consider the line $a - b - c - d$ and the MDS $M=\{a,d\}$. The edge $(b,c)$ isn't covered so either $b$ or $c$ need to be added to $M$ to yield an MVC. However, minimality requires that we then need to remove either $a$ or $d$. 

I have some experience in scientific computing, and have extensively used kd-trees for BSP (binary space partitioning) applications. I have recently become rather more familiar with octrees, a similar data structure for partitioning 3-D Euclidean spaces, but one that works at fixed regular intervals, from what I gather. A bit of independence research seems to indicate that kd-trees are typically superior in performance for most datasets -- quicker to construct and to query. My question is, what are the advantages of octrees in spatial/temporal performance or otherwise, and in what situations are they most applicable (I've heard 3D graphics programming)? A summary of the advantages and problems of both types would me most appreciated. As an extra, if anyone could elaborate on the usage of the R-tree data structure and its advantages, I would be grateful for that too. R-trees (more so than octrees) seem to be applied quite similarly to kd-trees for k-nearest-neighbour or range searches. 

Finding a maximal independent set in a distributed network of $n$ nodes with maximum degree $\Delta$. There's a known lower bound [3] of $\min(\Omega(\log\Delta),\Omega(\sqrt{\log n}))$ that holds for randomized and deterministic algorithms. The following is a simple randomized distributed algorithm [1] that proceeds in synchronous rounds. (In a round, every node $u$ can perform some local computation and send messages to its neighbors. These messages are guaranteed to be received before the start of the next round.) 

Applying Koenig's Infinity Lemma It's not always straightforward to see whether a specific property is a safety property: Consider the implementation of read/write atomic objects on top of basic shared memory variables. Such an implementation should handle requests and their responses in a way that makes them look as if they happen at some instant in time and don't violate their order of invocation. (Due to the asynchronous operation, the actual duration between request and response might be nonzero.) Atomicity is also known as Linearizability. Section 13.1 of [A] gives a proof that Atomicity is a safety property. The proof uses Koenig's lemma to show that the limit of any infinite sequence of executions (each of which satisfies Atomicity) also satisfies Atomicity. [A] N. Lynch. Distributed Algorithms. Morgan Kaufmann, 1996. 

I was recently introduced to the notion of a Graphon ([Quick introduction], [Original definition]) and I have been fascinated by the idea behind them and the way they combine fields such as graph theory, measure theory, probability, and functional analysis. Skimming through the limited literature around them, I have been wondering whether these structures could be useful outside of extremal graph theory, (edit:) besides property testing? 

I realized, not much. I know that there are some very efficient SAT-solvers that solve enormous instances efficiently, that Simplex works great in practice, and maybe a few more problems or algorithms. Can you help me paint a more complete picture? Which well-known problems or even classes of problems are in this category? TL;DR: What are problems that are counter-intuitively solvable in practice? Is there an (updated) resource to read more? Do we have a characterization for them? And, finally, as a general discussion question, shouldn't we? EDIT #1: In trying to answer my last discussion question about such a characterization, I was introduced to smoothed analysis of algorithms, a concept introduced by Daniel Spielman and Shang-Hua Teng in [1] that continuously interpolates between the worst-case and average-case analyses of algorithms. It is not exactly the characterization discussed above, but it captures the same concept, and I found it interesting. [1] Spielman, Daniel A., and Shang-Hua Teng. "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time." Journal of the ACM (JACM) 51, no. 3 (2004): 385-463. 

It can be shown that this algorithm terminates in $O(\log n)$ rounds with high probability, by arguing that half of the remaining edges are deleted in every round. In contrast, the fastest known deterministic distributed algorithm [2] takes $O(n^{1/\sqrt{\log n}})$ rounds and is considerably more complicated. 

[1] Dana Angluin: Local and Global Properties in Networks of Processors (Extended Abstract). STOC 1980: 82-93. $URL$ 

Note that the paper considers strongly Byzantine agents and weakly Byzantine agents. From the abstract: 

I'm unaware of such a published list of problems. Keep in mind that there are many different (and somewhat incomparable) models in distributed computing, ranging from the "benign" synchronous (fault-free) model where nodes execute in lock-step rounds and all messages are delivered reliably in each round, to the asynchronous model where there are no bounds on processing speeds and message delays and nodes themselves might crash or send corrupted messages. To further add to this variety, there are other requirements and assumptions that are orthogonal to synchrony and faults: the initial knowledge of nodes (network size, diameter of the network, maximum node degree, etc.), the ability to query a failure detectors, whether nodes have unique identifiers, atomicity of send & receive steps, the maximum size of a single message, and many more. The actual model at hand often implies a very different nature of question. (See [1] for more elaboration on these $2$ sub-communities in distributed computing.) In models that are close to the fault-free synchronous model, researchers often look at the complexity of local computation, for example, "What is the time and message complexity for computing a vertex coloring?" When looking at failures on the other hand, the questions are more related to solvability issues like "Is consensus solvable in this model?" or "Can we implement this fancy failure detector under these assumptions?" 

I have recently been reading up on some of the ideas and history of the ground-breaking work done by various logicians and mathematicians regarding computability. While the individual concepts are fairly clear to me, I'm am trying to get a firm grasp of there inter-relations and the abstract level at which they are all linked. We know that Church's theorem (or rather, the independent proofs of Hilbert's Entscheidungsproblem by Alonzo Church and Alan Turing) proved that in general we cannot calculate whether a given mathematical statement in a formal system is true or false. As I understand, the Church-Turing thesis provides a pretty clear description of the equivalence (isomorphism) between Church's lambda calculus and Turing machines, hence we effectively have a unified model for computability. (Note: As far as I know, Turing's proof makes use of the fact that the halting problem is undecidable. Correct me if I'm wrong.) Now, Gödel's first incompleteness theorem states that not all statements in a consistent formal system with sufficient arithmetic power may be proven or disproven (decided) within this system. In many ways, this appears to me to be saying exactly the same thing to me as Church's theorems, considering lambda calculus and Turning machines are both effectively formal systems of sorts! This is however my holistic interpretation, and I was hoping someone could shed some light on the details. Are these two theorems effectively equivalent? Are there any subtleties to be observed? If these theories are essentially looking at the same universal truth in different ways, why were they approached from such different angles? (There were more or less 6 years between Godel's proof and Church's). Finally, can we essential say that the concept of provability in a formal system (proof calculus) is identical to the concept of computability in recursion theory (Turing machines/lambda calculus)? 

This is certainly true for the edge expansion of $G \cup H$, since it can only increase by adding edges. I know that spectral expansion and edge expansion are related by the Cheeger inequality, but using this route we only get a bound on the spectral expansion of $G \cup H$ that is worse than the one given by $\lambda_G$ and $\lambda_H$. 

In the strongly Byzantine case, they show that $f+1$ is a lower bound on the number of good agents required when the network size is known, which implies that this is true when the network size is unknown. These lower bounds aren't tight it seems as their best algorithms require $\ge 2f+1$ good agents in the case where the network size is known, or $\ge 4f+2$ good agents if the network size is unknown. 

Suppose that you've implemented a shared memory machine $M$ that only satisfies eventual linearization, defined as follows: in every run $\alpha$ of $M$, there exists some point in time $T_\alpha$, such that linearization holds from time $T_\alpha$ on. Note that there is no upper bound on $T$. (*) (This is an artificial liveness counterpart of the standard safety property definition of linearizability.) Such a shared memory implementation wouldn't be very useful to the programmer: Note that if only eventual linearizability holds, there are no guarantees whatsoever on the consistency of read/write operations in any "early" prefix of a run (before the unknown time $T$). Or, in other words, whatever has happened until now, you can still extend the current prefix of a run to one that satisfies eventual linearizability. (*) If there was such an upper bound, then eventual linearizability would become a safety property.