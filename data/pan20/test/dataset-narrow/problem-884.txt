I have a directory with every package I require. Don't really want to go through several dozen files and manually invoke pkgadd for them. Is there a way to automate this process or call pkgadd to have it install everything in the directory? Thank you. 

Once there are several hundred adhoc machines being brought up, each with a unique name. Their hostname gets added to the DNS. Then upon TTL expiring, will the DNS entry still be reachable, or if the host has been down for some-time, it [dns-entry] will be cleared? What's the best way to manipulate these timeouts? Thank you. 

Will display the order for DNS resolution. Useful for when you're creating or debugging your Network settings. 

everything I want gets picked up from the dynamo process. Although for some reason the CPU Utilization isn't being presented and recorded. Any ideas? Thank you EDIT: SYSTEM - Linux x86_64, Virtual Machine, running atop ESX4.0 on an IBM HS21 Blade 

Am looking at Puppet and Cfengine. Unfortunately neither can do Windows deployments. Can you suggest some alternatives? Thank you. 

Anyone notice that the HS22 blades take at least 3-4 minutes to get to booting the OS? Is there anything that can be done to get it through the BIOS faster? 

VMware server is not the product that you'd wish to do this with. As mrdenny suggested, either go up to vSphere. OR. Re-evaluate what you are using VMware Server for, and if you need the failover feature-set, and obviously have access to a SAN - your best bet without spending actual $ would be to go with XenServer - which will provide you with HA. One thing to note is that it will require a dedicated machine (something to consider, as it seems whatever workload you're trying to virtualize is important enough that you're considering failover) 

Lastly using ulimit on your root account is not recommended, ulimit is mostly used for large multi user environments. I hope this helps! Cheers, =============== 

Here's a couple of options that you could try. Another way would be to have the echo $? run just after the script executed, 

6 subnets. Once thing to note, is that some of the ip address doesn't make any sense. If this image is purely displaying subnets, then, the reason why there is six is because of the routers on the network. So if you count the amount of links between the routers and pc's you get 6. 

Give that a try, you might want to be more specific with the regex rule, but that should work ok, if they are just hitting /username1, etc. 

Might I suggest the following, Nominate one server as a replicate code repository. You can then cron the updates to that repository at any interval. The rest of the servers can test if there is a local repository and then rsync the files from the nominated server. This information can be stored in the shared file server space. This will be pretty easy to automate and should be fairly robust. Another radical solution -> would be to use bittorrent sync. The repository server would be read/write while the others will have a readonly share. Might be quicker as the network load will be shared amongst the servers. btsync can be setup via a configuration file and the linux client works pretty well. EDIT: you can skip the repository server for the radical solution and stick with btsync. Cheers! :) Danie 

Port 25 is the standard port SMTP traffic runs on. If you intend for you system to be an email server than those might be legit servers trying to send you or your users email. If you do not intend your system to be an email server, figure out how to get port 25 turned off. Historically email servers would be configured to politely send on email for other servers. Today this is bad, bad, bad. It's called being an open email relay. It would be wise for you to verify that you are not doing this. But, don't go to far and try to block port 25 traffic if you do mean to accept email from the outside world. 

You do not need to recompile anything. In fedora you can get the sqlite php module by installing the php-pdo module. The following should do the trick. 

Be sure to try generating lots of different types of failed login attempts. Try from X (gdm or kdm or xdm), try from the console, try from ssh, try from sudo and try from su. Different subsystems can (and will) be configured different ways. It's not uncommon for ssh to be configured to use an internal login command that cuts around the /var/log/btmp business. Try the command as well. You might look in /var/log/secure to see if your failed login attempts are being stored there. But, I'm afraid I don't know the structure of the Debian log directory. Try on anything in the /var/log directory. It's quite likely that ssh is logging something someplace. 

I'm assuming, possibly incorrectly that your users are sitting in from of the windows boxes and need access to some linux gui applications. VNC works very nicely. Most Linux distributions have a the server sides vnc x-server included these days. Get a compatible windows vnc client and you should be good. If you need some additional security look for the ssl-enabled vnc stacks that are starting to crop up. Getting a windows side xserver like the one in cygwin will also works but is more complicated and will be much harder to explain to windows users. If your users are already linux savy this won't be to bad, otherwise go with vnc. 

I've used both before. I found that using rotatelogs is a very useful tool in managing access logs and if you don't want apache downtime. On a high volume site I would recommend this, the only Issue I found was that if you want to manage the logs you will need to have a separate script cron'd to either clean up or move the logs to a backup server, obviously this depends on the requirement. With logrotate you do have more options with how logfiles are handled, the only issue is when logrotate runs as explained in the previous answer, you will need to restart apache. Basically it all comes down to choice, if you need apache to running without interruptions then rotatelogs with a external cron script to manage, else logrotate will suffice. Just be sure to setup the logrotate cron not to rotate the logs when you have high traffic. Lastly if you want to have apache log to your syslog you can always use the logger command which is on Redhat Based linuxes. 

First thing When using apache's Listen directive with just a port, forces apache to listen on all available IPs/Interfaces, it's a wildcard entry and this will undoubtedly be interfering with your load balancer listening on localhost:3000, this would've caused apache not to start up as it could bind to that port, if I'm not mistaken. But try the following and let me know. You will need to specify the ip you want apache to listen to. In you case, it should be 

Or if you are feeling brave, embrace the power of memcached. You can store the images,javascript,etc as cached objects in memcached. This will require a small php script to go grab the required objects from the cache. This will greatly increase performance as disk writes are minimized, plus now you can utilize the extra RAM you added to the server :) 

Nothing requires that ping be possible between two hosts. It might be that somebody between you and google is dropping ICMP packets. If everything else is working I'd not worry much about this. If you are particularly worried check with whoever runs your networking equipment or firewall and see if they are letting ICMP traffic through. Also check to see if you can ping anybody else in the outside world other than google? 

About serving static files. Yes, you can use a lighter web server to do this. But, before you go to the effort be sure that it will do you any good. Is apache really using resources you need elsewhere? Maybe just configure apache to not start quite so many child processes. Be sure the added complication will pay for itself, because down the road it will almost assuredly confuse somebody when they try to figure out how everything is working. 

I'd suggest using the expensive raid controller to do the bulk of the raid work. LSI cards and the software they come with works quite nicely. When properly configured, they will send you email when intereting things happen to the array. Like when disks fail. There is nothing wrong with either of the two linux software raid options, but you've gone out and purchased a somewhat fancy raid card. Let it do the work. Configure the disk array to expose one big device to Linux. If you would like to break up the final device into small volumes use lvm for that. One big physical volume, one big volume group and cut the volume group into whatever number of logical volumes you need.