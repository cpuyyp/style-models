As you noted, your 2008R2 one is going single-threaded, whereas the 2014 one is going multi-threaded (thus finishing faster, but maxing out CPU while it runs.) To find the right balance for your stats jobs, think about: 

SQL Server builds different execution plans for TOP 100, using a different sort algorithm. Sometimes it's faster, sometimes it's slower. For simpler examples of it, read How Much Can One Row Change A Query Plan? Part 1 and Part 2. For in-depth technical details, plus an example of where the TOP 100 algorithm is actually slower, read Paul White's Sorting, Row Goals, and the TOP 100 Problem. The bottom line: in your case, if you know that no rows will be returned, well...don't run the query, eh? The fastest query is the one you never make. However, if you need to do an existence check, just do IF EXISTS(stick query here), and then SQL Server will do an even different execution plan. 

Elijah. There's two separate questions here: 1. Is DTC supported with AlwaysOn Availability Groups? As Microsoft says in big letters, no. I totally understand that you want to try it anyway, but keep in mind that you're now putting something into production that Microsoft simply will not support, AND you're using two separate niche features together (AGs and DTC). If anything whatsoever goes wrong, you're going to be in a world of hurt. This just isn't something I'd ever even think about trying in production. Keep in mind that if your managers find out that you deployed something Microsoft specifically says in big letters, "YOU CAN'T DO THIS," and you have any kind of outage where you have to call Microsoft for support, you're going to have some ugly explaining to do. 2. How should DTC be configured in a multi-node, multi-subnet cluster? Read Allan Hirt's post on configuring DTC with multiple instances of SQL Server in a cluster, and make sure to read all of the links in the post as well. 

Jeff, Brent here. We put a lot of documentation in that URL - make sure you copy/paste the URL into your web browser to read the full documentation. If you're in simple recovery model, and you've got log files larger than the database, there are lots of possible causes: 

Depends on your database engine. For example, in Microsoft SQL Server, there are several database engine features that can track which rows have been changed, and then you can grab just those rows in your periodic query. Another technique I've seen is to add an UpdatedDateTime field on the tables you want to search. Use a default value of the current date/time, and add an update trigger so that whenever the record is updated, the UpdatedDateTime is reset to the current date/time. Keep in mind that you'll probably want to index that field since you'll be querying it frequently. Then, in your app, just poll for all records where UpdatedDateTime > the last time you updated. If you take the latter approach, you'll probably want to do full repopulations periodically to catch any goofups where the polling app failed for a while. 

You know what's really funny? This is exactly the kind of thing SSIS is designed to do. You're probably going to run SSIS on your own desktop or laptop as you learn. SSIS is part of SQL Server Developer Edition, so you can install that for free on your laptop. It also comes with the SQL Server engine, obviously. So attach the AdventureWorks db to your local SQL Server Developer Edition, and then use SSIS to move the contents of the local tables you want over to the server run by your IT department. Congratulations, you have your first ETL project! Bad news though: you're also the client. You're probably a terrible boss. If you want to learn something easier, start by using SSIS to import the contents of a spreadsheet or a text file into that target SQL Server instead. 

Getting your server's wait stats since startup with sp_BlitzFirst @SinceStartup = 1 Getting server health data in Markdown format so you can copy/paste it into your Stack question with sp_Blitz @OutputType = 'markdown', @CheckServerInfo = 1 

Your connection string likely doesn't specify the database. It has to include the database name so that SQL Server can tell which routing list to use. (Your server could have multiple Availability Groups running on it.) 

To tune it, start by looking at Perfmon counters on your own machine. My tutorial on it is at $URL$ and it includes a list of counters to gather, how to analyze 'em, and how to interpret your bottleneck. 

I'm going to be brutally honest here: those are not the right metrics to look at, and wherever you're looking that gives you those metrics is wildly outdated. Those are not the usual suspects. In the year 2018 (and indeed, for the last several years), the way you diagnose a slow SQL Server is by using wait stats. SQL Server is constantly tracking what queries are waiting on, and you can see it in the DMV sys.dm_os_wait_stats. My favorite way to visualize that is with sp_BlitzFirst @SinceStartup = 1. (Disclaimer: it's my favorite because I wrote it, and it's free & open source.) Try editing your answer to include a screenshot of your wait stats from sp_BlitzFirst, and we may be able to get you a lot closer. 

I highly recommend using Adam Machanic's excellent diagnostic stored procedure sp_WhoIsActive. I've got an sp_WhoIsActive tutorial on my blog. Run it during the slow time window as I describe in the video, and it'll show the active queries, what they're waiting on, and their execution plans. Keep in mind, though, that this is more of a DBA tool than a "here's-your-problem" tool. This is where a database administrator usually has to step in and do diagnostic work. If you're confused by the output of sp_WhoIsActive, check out the book Troubleshooting SQL Server. 

That'll take a 60-second sample of your waits. Post a screenshot of the wait stats section, and we may be able to explain what the server is waiting on. Update: your added screenshot shows 2 seconds of WRITELOG waits in the span of 60 seconds. Basically, your SQL Server just isn't waiting that much. My guess is that your simulated workload involves serial singleton activity: working on one row at a time, from just one thread in a load generation tool. That's not a great way to simulate workloads - you'll want to use a multi-threaded load generation tool, with lots of activity happening at once, and doing more than one row of activity at a time. 

If you have a read-intensive query running on one NUMA node (in this case, 0), then it can experience a lower page life expectancy relative to other NUMA nodes. That's totally normal. To see what queries are running right now, you can use Adam Machanic's excellent sp_WhoIsActive. It's totally free. Some folks even run it every X minutes and log the data into a table so they can go back to see what was running at the time PLE nosedived. 

SQL Server updates stats on objects when ~20% of the data changes, but filtered indexes & stats are a special case. They're also updated when 20% of the data changes - but it's 20% of the base table, not 20% of the filtered subset. Because of that, you probably want to manually update stats on them periodically. I love Ola Hallengren's maintenance scripts for this - the index maintenance stored proc has a parameter for updating statistics, and another parameter for choosing what level of sampling you want, and another parameter for choosing whether to update stats on all objects or only the ones with changed rows. It's fantastic. 

That's why the coalesce has to be outside of the search operation: you need it to happen even when there's no rows in the result set. Now, let's look at your query. I'm going to take the subquery out on its own, and I'm going to hard-code values for one of the rows where you want the COALESCE to work, but it can't: 

AWS Aurora's replication is more akin to Always On Availability Group. The primary pushes storage changes to other replicas. You don't get to make schema changes on the subscriber. More details are available in several re:Invent deep dive sessions. 

Say you've got a 200GB database that you're dealing with, and you can't get enough storage throughput to keep your cores busy. It's not unthinkable to need not just 200GB of RAM, but even more - because after all, SSIS and SSAS really want to do their work in memory, so you have to have the engine's data available, plus work space for SSIS and SSAS. This is also why people try to separate out SSIS and SSAS onto different VMs - they all need memory simultaneously. 

Whenever you've got an overall general performance question like this, start by asking, "What's the server's primary wait type?" My favorite way to get that is with sp_BlitzFirst @SinceStartup = 1. (Disclaimer: I'm one of the authors of that open source script.) The first section of that will return your top waits since startup. If you post that output here, you'll get a lot better diagnostic answers. 

If your table doesn't have a clustered index, then deletes don't deallocate empty pages by default. Your options are: 

You've got lots of questions in here: Q: (The lack of foreign keys) confuses me a lot! It is a good practice (not mandatory) to have Fk's in the DWH for a variety of reasons (data integrity, relations visible for semantic layer, ....) A: Correct, it's normally a good practice to have foreign keys in a data warehouse. However, clustered columnstore indexes don't support that yet. Q: So MS advocates Clustered Column store indexes for DWH scenarios, However it can not handle FK relationships?! A: Microsoft gives you tools. It's up to you how you use those tools. If your biggest challenge is a lack of data integrity in your data warehouse, then the tool you want is conventional tables with foreign keys. If your biggest challenge is query performance, and you're willing to check your own data integrity as part of the loading process, then the tool you want is clustered columnstore indexes. Q: However SQL 2014 than adds no real new value for DWH?? A: Thankfully, clustered columnstore wasn't the only new feature in SQL Server 2014. For example, check out the new cardinality estimator. Q: Why am I so angry and bitter about the way my favorite feature was implemented? A: You caught me - you didn't really ask that question - but I'll answer it anyway. Welcome to the world of third party software where not everything is built according to your exact specifications. If you feel passionately about a change you'd like to see in a Microsoft product, check out Connect.Microsoft.com. It's their feedback process where you can submit a change, other people can vote it up, and then the product team reads it and tells you why they won't implement it. Sometimes. Most of the time they just mark it as "won't fix, works on my machine" but hey, sometimes you do get some answers. 

This is sometimes due to an MSDB.dbo.backupsethistory table that's never been purged. SQL Server inserts a row into that table, then goes back and updates it later during the course of doing backups. If your MSDB is on really slow storage, and you can't cache it in memory, and it's got a lot of history, then accessing that table can be your biggest bottleneck. I wrote a blog post called Brent's Backup Bottleneck: MSDB about a really bad occurrence of it. To check that, run sp_Blitz (disclaimer: free open source script I wrote) and one of the warnings is MSDB History Not Purged, which warns if you've got more than 60 days of history stored. To fix it, add a History Cleanup Task to your maintenance plans. However, if you're in this spot - where updating this table is taking forever - then clearing out history is probably going to take forever, too. You may have to run sp_purge_jobhistory manually yourself, nibbling off a little history at a time, to keep downtime low. 

Depends on your client libraries, the way your app handles memory, and what you're doing inside the fetch. Pulling back one row at a time is a very dangerous design pattern for database servers to handle. They have to materialize all your results first, then spoon feed them to you one row at a time while you slice & dice results. You might be holding locks or slowing down the database server. Instead, pull all of the data into memory on your app and do the slicing and dicing after you've let go of your locks, OR do your slicing and dicing in batches on the database server. Batch processing is way more efficient than row-by-row processing. In the DBA community, this is known as ReBAR - Row By Agonizing Row.