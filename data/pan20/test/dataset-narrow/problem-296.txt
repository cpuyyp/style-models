Update: One of your comments raised a really good point that I sadly didn't even think of first. This is actually a pretty common place to look at - the higher your MAXDOP - the more threads you can use. The more you can use, the more memory that can be consumed because you have more going on at once inside of a query - more parallel thread. You said your MAXDOP is 10 server wide. This "feels" high. The latest guidance is 8 if more than 8 logical cpus, or something between "0 and N" if you have less than 8. I tend to look at the number of cores per NUMA node and may go lower than 8 for some systems that are running OLTP workloads based on the number of cores per NUMA node, and sometimes I go higher. But you might look at these guidelines and consider looking at your instance wide MAXDOP and consider changing it and testing to see the impact. Going lower than 10 will reduce your memory grants for these queries. This doesn't eliminate the need to tun queries, however, and I think there is probably room for that. Basically you need to either continually increase memory until you can satisfy all your grants, consider lowering MAXDOP, tune queries so they consume smaller memory grants, consider reducing the workload - lower the amount of queries executed at once, or get into Resource Governor settings which can artificially lower the grant amount but most assuredly increase TempDB dependence which the memory grants (memory grant % especially). Those are the main approaches I take. If you can, starting with query tuning and perhaps considering MAXDOP changes are a good place to start. I tend to leave Resource Governor as a final resort, have good luck with tuning queries and playing with server wide or query specific DOP. Original Answer Query memory grants are a part of SQL Server helping to ensure the right amount of memory is available before running a query. It is a way to try and make sure that you aren't running an excessive amount of high memory consuming queries that will all but guarantee a lot of memory consuming operations spilling to TempDB. In this case I imagine your Top 3 queries are consuming a lot of memory. And you were right to look at their plans. Hashes and Sorts are probably one of the issues here. Looking at tuning queries so they are appropriate in their plans and, in turn, memory consumption is a great solution here. I doubt this is a bug - it is the way SQL Server was designed. This article is a bit old, but the knowledge contained within still applies. It is a good place to begin understanding query memory grants. If tuning the queries doesn't save the day here, there are other tricks that can be employed. I've also asked some questions in the comments section of your question that may help expand an answer from me or someone else. 

Erland Sommarskog, a SQL Server MVP, handles this question really well. I'd love to just cut and paste his stuff here but I won't. He has written a great couple articles (depending on the version of SQL Server you are on) about dynamic search conditions and getting the best possible performance. I've used his approach numerous times and have been pleased at the results. I +1'd Valkyrie's answer because it is heading down that path but Erland expands on it quite well. 

it does the filtering and joining at the linked server. This avoids you having to bring data back to just filter it out, etc with the four part query aproach. The problem, though, is that update is trying to run on the destination. It's basically a pass-through query, so you are trying to pass that update through. So you'll need to do the update locally, but can join to the open query to join to the data on the other side. For more you can see the syntax guide here with some notes. 

Are you using a synonym? The few cases I have seen people complaining of this, this fix worked - $URL$ - What version/build are you on? 

Some People are in favor of keeping their "C" drive dedicated to just the OS and OS binaries. This can give you some different options for recovery in the event of a crash on the C drive, it can help keep the OS from causing or receiving space related issues from sharing with other apps. You are isolating SQL Server's binaries from other programs and ensuring availability of some of the critical folders like the Logs folder where error logs go - this folder needs to be accessible for SQL Servers start up. You are protecting yourself from others, basically. 

+1 on the Try Rewriting your query comment from datagod. I am also wondering if you are bumping into a permissions problem on the linked server side leading to this slowdown. I blogged about this linked server slowdown awhile ago. May be worth verifying the perms (is it a SQL linked server? Or is it another DBMS? If the latter, then you won't get great statistics across anyway) Do you have SQL Server 2005 on the test environment still to try this query on and rule out the environment? Have you rebuilt statistics since upgrading? 

shows you the data organized by Log Sequence Number (LSN). There really isn't a "time stamp" in there, because the LSN is the time stamp that SQL needs. All of the transactions happen in that order - the purpose here is for recovery to run and not miss a transaction to rollback/roll forward/recover. There really is (that I know of) no translation out there for LSN to time/date. You could sort of run with a (default,default) parameter and maybe find a transaction you know for sure when happened and trace it back and do some mental gymnastics. But really, I'd suggest something totally different. For now, take the user's word for it. But verify moving forward. Can you not put a column in that DB with a default value for the current time? Could you (don't love this, but if need this degree of logging maybe it is necessary?) set up an auditing structure with Triggers? Could you enable Change Data Capture? There are other ways to grab this information that are far more effective than piecing together pieces and parts reading the transaction log. I'd suggest this effort be spent there. If you insist you have a few options to narrow it down: 

So is basically saying "Open this query on the linked server" - It's the preferred method for doing a select when able for me. Instead of saying something like 

When formatting your drives for Data files and TempDB specifically, consider doing the 64KB allocation unit size. (Source) Now is your chance to maybe clean up some of those security holes and flaws that could exist. Look to build with best practices and least access. 

That is just one approach. It may not be the right or best approach - and enough time searching the web will show you at least a half dozen more. But if a client were to offer to pay me an hourly rate to create this for them, I'd suggest they really consider SQL Sentry or one of the tools like it. The price of most of these tools isn't a whole lot more than a couple days of SQL Server consulting help. 

From the comments I see you removed the default time limit like I mentioned but still the analysis didn't happen in enough time or ever complete as I feared. 18GB is definitely the largest trace I have ever heard of attempting to be analyzed by the Database Engine Tuning Advisor. I suggest you break your trc file up into smaller sizes. Either by looking at the data a bit more filtered or for a smaller window of time while still maintaining enough of a workload to capture both data modification and selects. Alternatively - I haven't been a big fan of the DETA and sometimes the results, even with a properly sized workload file, fly in the face of reality (most index suggestions are very wide covering index in a lot of cases, redundant indexes in overlapping column coverage at times, etc.) I would suggest a "Top 10" style tuning exercise may be a better approach. Find the worst performing queries by duration and/or reads and work on tuning them through query tuning and index tuning/design best practices. Edited: Added a couple more tips based on comment from OP I would also look to set an end time. DETA isn't supposed to be a "done in 5 minutes" deal. It is trying to be incredibly thorough and look at your database as deeply as it can and analyze the workload as long as it can to come up with the recommendations. Have you tried setting an end time for the analysis? I'd check out this link on MSDN for some other tips - $URL$ Bottom line though - This will be a slower operation. The larger the workload file and more complex the database, the slower it will be. If you are running on the same system that other work is going on then you are each competing for resources. 

The Manage Connections dialog is for a connection to the Database or Instance you are performing maintenance on. So if you type in a sql authenticated user called 'bob' there, the user bob will be the one that connects to the instance to do the maintenance tasks you are specifying. You can prove that out by running SQL Server Trace/Profiler (in a dev environment) and seeing the connection being made. That is basically the user that is connecting to the objects within SQL Server. The connection to the folders and external resources will be, in this case, whatever your SQL Server Service Account is running as. It's a bit confusing but it sort of works like this: 1.) Your SQL Server Agent Service Account (or the proxy account used) will do all of the executing. It will call out to the maintenance plan, it will handle the job logging for job history/etc. 2.) When the maintenance plan is run - any of the tasks inside of it will either use whatever permissions the agent account has, or whatever you have explicitly defined in the maintenance plan connection manager -as you have done. 3.) For any access to resources outside of SQL Server - the SQL Server Service account will actually be the account interacting. So to write a backup to a folder, your SQL Server Service account will need that permission. The agent tells the server to run the maintenance plan which tells the server to do a backup. The service account of the SQL Server service then performs what it was requested and it writes out the backup. Also You didn't ask here - but I'll mention it again like I did in the last answer. I don't get anything for recommending a free resource - but the Ola Hallengren maintenance solution I mentioned on a related question from you is a really good resource. It will intelligently do your index rebuilds/reorganizes so you only touch what needs to be, it will add more control and intelligence to your statistics updates, it does a great job with backups, etc. But the nicest thing going for it? It gets rid of the black box of maintenance plans and takes a variable out of play for you to have to worry about seeing inside of with some of the more advanced security questions you are trying to get answered.