Aurora's answer is excellent, but I do feel I have one thing to add that's very specific to CS: this is a wonderful opportunity to use (or abuse, depending on your perspective) version control. There are a lot of great reasons to introduce your students to version control, most of which I will not mention because they not directly relevant to the OP's question. The one reason that is relevant: I've had students use GitHub, and as a result have been able to see exactly who was committing which bits of code and at what times. That solves your immediate problem of ensuring that everyone is contributing to the project, or at the very least, allowing you to identify those who didn't contribute their fair share/only made "easy" updates like code formatting or comment changes. This also helped me to see who was having problems with the assignments in... not exactly real time, but definitely as the assignments were progressing, rather than after the due date (when it would have been too late to help) or 100% dependent on the students asking for help. I as a TA/recitation leader could then help people when things were more manageable and prevent problems from compounding or a chorus of last-day requests for emergency help and extensions from arising. Now, I realize that this sounds more like "determine how much work each individual did" than "divides up tasks proportionately." However, experience has shown me that if you tell people about this system upfront, the students will in fact end up dividing the tasks up proportionately. Just having them know that they're being watched seems to be enough motivation to achieve your goal, in practice. 

Since you've described this a standalone course, I'd treat it as a "computer literacy" sort of thing. I wrote another answer yesterday that got me thinking about what topics everyone should know about computer science, which I'm adapting here. I like that you're starting with basic computer architecture. People should understand that there are logical rules and processes between them pressing on the keyboard or clicking the mouse and pixels lighting up on the screen. A logical step from there might be basic network architecture, for the same reason; just as the inside of a computer doesn't work by magic, neither do e-mails move from one computer to another that way. (Do people still write e-mails? Maybe Facebook or Twitter posts1 instead?) I also agree with your choice of DOET. It was a part of my own CS education in the distant past, though I didn't realize it until I read the book years later on my own and found a particular section to be very familiar. From there, you might also discuss basic security topics, such as passwords; why they're important, how they work, and how they can fail. Obligatory xkcd 936: 

On these criteria, Processing doesn't look too bad. Faculty preference What can I say to you about your own preference? I would just advise thinking about what you hope to achieve with the course. As I've said elsewhere on this site, a traditional computer science program, a zero-to-job-ready boot camp, and a short-track for non-majors might have intro courses with very different objectives, and that's as it should be. Industry relevance Relevance is kind of a hidden benefit here. I've never heard of any big companies writing serious enterprise projects in Processing. However, it is based on plain old Java, which remains one of the most widely used languages today. In fact, it's sort of a best-of-both worlds situation. Pears et al. tell the story of RIT dropping Eiffel because students didn't want to learn a language that they couldn't use after graduation, but also note that one big downside of Java is that it come with a lot of syntactic baggage that gets in the way of learning actual computer science (algorithmic thinking, how data structures work, etc.). Processing strips away a bit of the syntax that Java beginners find overwrought while still being basically the popular language that they will be able to use in the real world. Technical aspects Not much to say here beyond what was in the previous section. Processing is very graphics-oriented. It'll be up to you to design a curriculum that teaches CS concepts using graphics-oriented tools. Tools and materials There's plenty here. A solid official website at processing.org with documentation and tutorials, a relatively small but respectable community on Stack Overflow, a subreddit, and a FOSS culture that has ported Processing to other languages/platforms. Simon Fraser University recently ran an intro course using Processing. I have no special insight there, I found it by a simple web search. But if you're interested, maybe you could reach out. 1: Pears, A., Seidman, S., Malmi, L., Mannila, L., Adams, E., Bennedsen, J., ... & Paterson, J. (2007). A survey of literature on the teaching of introductory programming. ACM SIGCSE Bulletin, 39(4), 204-223. 

However, the rest of the article points out that even when supposedly unprepared students are given access to advanced courses, they have overall better outcomes. In many high schools, computer science is still seen as an advanced elective, similar to the AP courses described above. Even if the goal is not college prep, there is value in providing some exposure to another field, particularly CS. Getting more students into classes may benefit from getting faculty and administration buy-in. This is obviously especially true if teachers, guidance counselors or principals are initially discouraging/blocking some students from taking CS courses, but even if that's not happening, some active encouragement can only help. The title of the question also mentions diversity. "Diversity" is a broad term, but given the context of this site (and to simplify research) I'm assuming for the purposes of this question that you mean racial and gender diversity. Research on diversity and education covers a lot of ground, but speaking broadly and relatively, it shows that (at least in the USA) minority students are treated worse. A few random examples: 

The downside of this answer is that it doesn't get into the mathematical foundation of CS, or how CS is about unchanging fundamental concepts. But I'm beginning to think that's not a big problem... such topics might be beyond the scope of a five-minute intro for a complete layperson. 

but I would posit that it is generally accepted to be unethical to intrude upon even systems with no security features. In my own cybersecurity training, I have also been made to sign documents at the beginnings of classes saying that violations of ethical guidelines would be severely punished, by course failure, institutional sanctions and/or legal action. I don't remember the exact terms but they were along the lines of using any systems to hack any systems without explicit permission of the targeted systems' owners. Nor do I know how enforceable they really were, but they made good symbolic points, after the end of the ethics discussions and before the start of any practical security material. 

I'll say upfront, I'm familiar with both life sciences and CS, but I've never applied one to the other. In life sciences, computing is used for a lot of heavy processing. Stuff in genomics and molecular modeling, for example. It's not fast work. Efficiency, then, must be important for researchers to know about. The course you're designing should cover Big-O and and related topics in its discussion of algorithms. That theory won't be much use if the researcher can't communicate with the computer. I'd say Java may not be the best language to use. The upside that's usually cited is that it's very common, and similar to other languages in industry, but these students aren't going to be professional programmers, so that doesn't really count for much here. With Java, you'll also be spending a lot of time teaching semantics about the language itself rather than CS principles; time you can't afford to waste if you get only a single course with these students. Alternatives: Python seems to be used more in science, and Ruby has a nice learning curve. Both are well-established languages with plenty of documentation and communities available online. Doctors and vets who are frontline healthcare provider types rather than in-the-lab researcher types are more likely to be users of software than writers of it. Depending on specialty, they'll be dealing with assorted medical records suites and similar programs. They should understand principles of cybersecurity, including more technical things like how passwords really work (or, just as important, fail, e.g. lack of salting or e-mailing in plaintext) and what makes them important as well as less technical ones such as social engineering, which are often both easier and more effective. 

Computer science is a bit unlike math, history and other fields. For reasons that are, perhaps, as much historical as pedagogical or anything else, CS doesn't generally follow the "taught to very young children (i.e. six-year-olds) and then developed through primary and secondary schools" model. As a result, high school and university CS courses often cover the same ground; at the introductory course level, at least. The difference is that universities, where students are expected to choose a major, have the opportunity to offer more courses with greater depth and breadth as well. Introductory courses, whether in high school or university, typically cover fundamental principles such as basic data structures and the importance of algorithms, and involve some programming. I have no personal experience with "programmer boot camp" programs, but I would think that HS CS coursework would be a good head start for such non-degree-granting developer-job-training programs. It's also very common for academics in non-CS fields to find themselves writing small computer programs to assist themselves with their research on an ad hoc basis, for tasks such as data processing/manipulation or statistical analysis. Having some knowledge of a programming language and the importance of Big-O from high school would be helpful for that as a, say, research assistant to a chemistry professor. I also know someone as an undergrad who once had a take-home final exam in a civil engineering course where the entire exam was to perform a complex calculation about deformation of a beam under certain conditions (soil, beam material, weather, etc.) The professor's goal was to make the students apply concepts from throughout the course in what should have been some pretty drawn-out calculations. My friend knew a bit of programming and, using all the appropriate engineering concepts, wrote a little program to pop out the correct answer for him. The professor gave him full marks. 

There are some good ideas in Peter's answer, but I notice that they all imply or at least come from the perspective of the student being the actor. In many cases, this is just fine. In some other cases, the students may need more of a push, or may need to have some obstacles removed. It has been reported that teachers themselves may try to steer students away from, among other things, advanced courses when they suspect a poor fit. According to a 2009 article by noted education journalist Jay Mathews, 

In my story above, I segued from this into the Halting Problem and efficiency. Looking back, I think that was too big of a jump, at least when presented as abstract concepts. I like the phone book example mentioned in Peter's answer. I think I'd modify it this way: 

See also Security SE's post about that comic. A quick mention of the fact that there are problems computers can't solve, no matter how fast we make processors. Not necessarily the whole, heavy mathematical proof, just the concept (again) that computers aren't magic boxes. Maybe a mention of the fact that even though computers are fast, efficiency still matters for computer algorithms. At least some cursory coverage of programming, algorithms, and data structures should be included, as they are fundamentals of the practical side of things. I've never used AppInventor, but I looked it up just now and it seems like it would do that job. I'm curious why you say it's gone poorly in the past. AI and ML are cool topics, but I don't see a ton of promise in adding them to the course, given that your students are, by your own description, intending to be miserable, largely uninterested in CS and unlikely to be directly applying what they learn. There is a bit of human element here, which, to be perfectly frank, I probably didn't spend enough time thinking about. Your course plan already seems full to bursting, but if you can make the time, maybe start out day one by asking the students what they hope to get out of the course? It'll be a long and draining semester for everyone, including you, if the students' initial expectation of misery doesn't change. 1: Actually, the first "relatable, common type of web traffic" example I thought of was Netflix movies, but they're technically a terrible example, because Netflix avoids backbone peering. I thought that might be beyond the scope of your course, but I guess that's up to you. 

(Thanks, xkcd 538.) Doctors and researchers are already looking at images on computer screens a lot, and I don't think I'm predicting the future too much by saying that's only going to increase. Understanding how lossy compression works, or JPEG format tradeoffs, and why the "zoom in and clean that up" CSI magic from TV doesn't happen in real life could be useful. Beyond all that, there are some fundamentals that everyone should know, life sciences or not. The basics of how a computer operates, so that it's not a mysterious magical process between pressing keys or tapping the screen and pixels lighting up. Some brief coverage of programming, algorithms and data structures. The fact that no matter how fast we make computers, there are some problems they can't solve, and we can prove that using math (even if you don't go through the proof itself). 

It can be helpful to try to make the analogy with the listener's own profession, as long as it doesn't backfire by derailing the conversation. This then leads into 

I'll attempt a self-answer here, based on hindsight and a few previous experiences where I went in knowing I wanted to do this. I've found that starting off with an analogy helps a lot of the time. Friends and co-workers have reported similar results in my super unscientific poll. The one we use is along the lines of 

The above is relevant because, where it is taking place, school staff may not even realize that their actions are reducing CS diversity. In other words, it is unconscious bias. Eliminating "gatekeepers" and having "suggest that all students register" policies can help with this. 

This seems insufficient to me, because of the following example at a minimum: In small towns, people do not lock their doors, or at least they did in bygone days of yore, or so the story goes. If you steal from a house where the door is unlocked, is that suddenly somehow not unethical? What if there was no door on the house at all? If the stuff was just lying around out in the open at a campground? What I'm getting at is, you said 

The title here almost feels like a trick question to me. Not that it's intentionally trying to mess with people, but "which one first" is a bit of a false dichotomy. This is a somewhat unfortunate example, but it's the one I was reminded of. Long, long ago, in the ancient history of Stack Overflow, there was a... kinda rude and presumptuous question about a web developer who didn't understand that the web involves separate server and client machines. If you have 10k rep on Meta Stack Exchange, you can still see it: MSE post 41660. Granted, to be ultra-pedantic, you can't physically teach two concepts simultaneously, so technically either front-end or back-end will have to be first lesson. But as a practical matter, for all intents and purposes, the two topics go hand-in-hand. Teaching only one and not the other for any significant length of time may have some use, but I wouldn't call it web development. More recently, I have seen front-end vs. back-end start to be replaced by top-down vs. bottom-up, which I have to say I prefer. In short, "bottom-up" would be "here's what the OSI stack is, these are the protocols the Internet uses, you can set up off-the-shelf software like Ruby on Rails to handle a lot of the underlying stuff for you and then write your logic in Ruby and presentation in HTML/CSS." Top-down is just the reverse of that. It's like when you start someone with "Hello, world!" in Java, and tell them to blindly copy but not to worry about what or mean for the first few lessons. To be fair, my more recent experiences have been with more advanced students, not intro high school or college students without any prior experience. But I still believe in my fundamental point: you can't show people only half of a picture (even if you add the other half in later) and expect them to absorb its full value. As for your JavaScript comment, it's been a few years since the last time I cracked it open, but I'd recommend giving JavaScript: The Good Parts a read. It's by Douglas Crockford himself, and under 150 pages long.