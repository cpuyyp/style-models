Here I need to group by countries and then for each country, I need to calculate loan percentage by gender in new columns, so that new columns will have male percentage of total loan amount for that country and female percentage of total loan amount for that country. I need to do two group_by function, first to group all countries together and after that group genders to calculate loan percent. 

I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the captions in the properties section. Sample Create statement: 

I created a sample dictionary with key value pairs for work class. But, I don't know how to use this in a map function and replace the categorical data in the CSV file with the corresponding value. 

This is the code I have written in normal python to convert the categorical data into numerical data. It works fine. I want to do the conversion in spark context. And, there are 9 categorical columns in the data source. Is there a way to automate the dictionary update process to have a KV pair for all 9 columns? 

Could someone help me in achieving this output? I think this can be achieved using dplyr function, but I am struck inbetween. 

I have a dataframe with columns as defined below. I have provided one set of example, similar to this I have many countries with loan amount and gender variables 

I am using Ipython notebook to work with pyspark applications. I have a CSV file with lots of categorical columns to determine whether the income falls under or over the 50k range. I would like to perform a classification algorithm taking all the inputs to determine the income range. I need to build a dictionary of variables to mapped variables and use a map function to map the variables to numbers for processing. Essentially, I would my dataset to be in a numerical format so that I can work on implementing the models. In the data set, there are categorical columns like education, marital status, working class etc. Can someone tell me how to convert them into numerical columns in pyspark? 

Finally, I resolved the issue. I had to set the pyspark location in PATH variable and py4j-0.8.2.1-src.zip location in PYTHONPATH variable. 

I have a python script written with Spark Context and I want to run it. I tried to integrate IPython with Spark, but I could not do that. So, I tried to set the spark path [ Installation folder/bin ] as an environment variable and called spark-submit command in the cmd prompt. I believe that it is finding the spark context, but it produces a really big error. Can someone please help me with this issue? Environment variable path: C:/Users/Name/Spark-1.4;C:/Users/Name/Spark-1.4/bin After that, in cmd prompt: spark-submit script.py 

I have a data set, in excel format, with account names, reported symptoms, a determined root cause and a date in month year format for each row. I am trying to implement a mahout like system with a purpose of determining the likelihood symptoms an account can report by doing a user based similarity kind of a thing. Technically, I am just hoping to tweak the recommendation system into a deterministic system to spot out the probable symptoms an account can report on. Instead of ratings, I can get the frequency of symptoms by accounts. Is it possible to use a programming language or any other software to build such system? Here is an example: Account : X Symptoms : AB, AD, AB, AB Account : Y Symptoms : AE, AE, AB, AB, EA For the sake of this example, let's assume that all the dates are this month. O/P: Account : X Symptom: AE Here both of them have reported AB 2 or more times. I could fix such number as a threshold to look for probable symptoms. 

I would not recommend doing this in general as it will likely lead to overfitting. While a particular reordering might improve fit on your dataset, the goal is to create a model with good predictive power. If you received never before seen data from an independent second, third, one hundredth Titanic, would the ordering still be the same? I doubt it. You'd probably be incorrectly using the somewhat arbitrary ordering fixed on the first, real Titanic on new data where the ordering is no longer the same. I think the story goes that the young and the elderly got preferential treatment and this is why their survival rate was higher. Therefore your AgeBand curve should be approximately U-shaped. (Which it is, approximately.) If you insist on linear regression, I don't see how you could ever (directly) include this nonlinear (U-shaped) effect. The fewer assumptions (particularities) go into the choice of features, the better. It would then make more sense to me to reorder such that you pick bands alternately from the two ends (starting from the middle): (or in reverse order, starting from the ends). Or, even better, pool the age bands into coarser groups: . I think these are justified by your exploratory data analysis. When I look at the range of values in the AgeBand plot ($0.33$ to $0.55$), this variable does not actually explain as much as Pclass, Sex, FareBand. In the end you might end up dropping it from your variables anyway, so once again, the more attention you pay to it, the more you're at risk of overfitting. 

What you call sum product is the same as a dot product of two vectors of equal length, $(w_{0j},w_{1j},\dots,w_{nj})$ and $(x_0,x_1,\dots,x_n)$. Based on your diagram, your difficulty is that you want to be able to define connections between neurons that are not in consecutive layers (in other words, the connections might skip layers). I assume that there are no directed edges (synapses) that would go in the opposite direction (from layer $n$ to layer $m$ for some $m<n$). In this setting, you have to forward propagate information to layer $n$ from input $X$ and layers $1,2,\ldots,n-1$. I don't know when and how often you want to 'update' your network topology. Do you want to change it during training?! (Otherwise you'd fix a partially connected NN and you wouldn't write update in your question.) For forward propagation in a fixed network topology, I would associate to neuron $j$ a vector of weights $(w_{ij})_{i\in I_j}$, where $I_j$ is the set of neurons that bring information to $j$ (those $i$ where there exists a directed edge from $i$ to $j$). Then you could either cherry-pick the neurons $i$ from the set of all neurons (by maintaining a list of indexes to find them in the vector of all neurons) and put them into a vector $x$, which is now as long as $(w_{ij})_{i\in I_j}$, and you can use dot product on the two vectors. (With for Python, would work.) You would need to do this separately for all $j$ in the given layer. As these are receiving information from differing numbers of inputs and neurons, they indeed cannot be arranged into a matrix/tensor, not even receiving neurons of the same layer. Or you could force all potential preceding neurons into one vector. Neuron $j$ of layer $n$ can only receive information from neurons $x=(<$input nodes$>$, $<$neurons of layer $1>$, $\ldots$, $<$neurons of layer $n-1>)$. This is a vector, and you can define a vector $w$ of equal length for $j$, which would indeed have many zeros, and $x$ and $w$ can be multiplied. For all neurons of layer $n$, you could stack these $w$ as row vectors on top of each other into a matrix/tensor of size (the number of neurons in layer $n$) times (the size of input $+$ the number of neurons in layer $1$ $+\ldots+$ the number of neurons in layer $n-1$) and use between this matrix and the $x$. How to do training (backpropagation) with such topologies is something you have to consider a bit. I suspect it will work without much adjustment compared to a fully connected neural network without layer skipping. In the second option, you would only need to zero out the weights which must be kept zero after each backpropagation update. This indeed carries overhead if you're not careful. But if you maintained a binary ($0$ or $1$) 'mask' matrix of which weights are live (which correspond to existing connections), then you would only ever update these weights in the backpropagation. Additionally, you may want to introduce a bias term $w_{-1j}$ (also known as intercept term, a constant offset) as if $x$ had an extra dimension that is kept constant $1$. After the aggregation with the sum product, you should also pass the result through a non-linear activation function before entering the value into the neuron in the next layer.