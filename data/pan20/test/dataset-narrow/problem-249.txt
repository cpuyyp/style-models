But the query itself takes 03:31. Question: is current_timestamp the timestamp of the beginning of the query? here's the entire big, bad query: 

Here's the James May method. Looking for feedback & ways to do it smarter. 1. Write your query in SSMS. To get dynamic date ranges that always fall to the first of the month, I do it this way (there is almost certainly an easier way): 

Hit next and you should be on the page to Specify Table Copy or Query. In this example, hit the button to Write a query to specify the data to transfer, but you can also create a view or table, if you prefer. On the Provide a Source Query page of the wizard, paste all your sql. In this example, I need to paste: 

2. Create your excel file: Paste the results into an excel sheet. Format it the way you want it, Create your pivot tables and charts 'n stuff, then delete all the rows with data (keep the headers). Important: Save the excel file in a location where your SQL Server Agent can access the file. You may need to edit the security properties of the windows folder and add SQL's service account to the Group or User Names. I keep a template file in a place where no end users can open it up, change a column and break everything. It adds a few steps, but makes it a bit more end-user-proof. In this example, my template is: 

I use RazorSQL as an interface to my PostgreSQL database server, which contains three databases. If I use pgAdmin, I can list all three databases (namely, , , and ): 

I wonder whether there is any way to save the "view data options" in the "edit data" window in pgAdmin III: 

Since there seems to be no way to define such a keyboard shortcut, I filed a feature request: $URL$ , which got rejected: 

I wonder whether I can edit the content of table in the data output of a query in pgAdmin III? The query is a . MySQL Workbench allows users to edit the content of table in the data output of a query, but I fail to see the same option in pgAdmin III. 

I wonder whether I can change the font in the "edit data" window in pgAdmin III. I use Windows 7 and OS X 10.10. 

Go back to DBeaver, try to connect, that should work. I still don't know why I should use instead of the SSH tunnel tab in DBeaver though. 

I was executing a bunch of UPDATE statement. Around ~32.8K got executed and committed, then got the error: 

In MySQL Workbench, one can use the keyboard shortcut Ctrl + ENTER to execute the statement at cursor (delineated with semi colons). Is there a similar shortcut in pgAdmin? 

I wonder whether I can clear the output of a query in pgAdmin III (aside from running another query). 

Is there anyway to export the results of a query in Oracle SQL Developer without having to execute the query twice? Running a SQL query in worksheet, then Right click on Result Set window -> Export Data -> Text will run the query a second time. 

I can be reasonably sure there are not conflicting updates to this record (record is owned by a single user). There would be a lot more wrong than this if that weren't the case. Newer versions use a stored procedure here, but it would not be possible for me to change that in this version. 

P.S. I did edit this a little, changing names of the procedures, the name of the table only, and removed comments only. 

These images show the two elements with the highest cost. They have identical properties between both the query plans. The difference in the final row totals is the result of joins with other tables. But those joins all have a low relative cost. 

I have a query, that as far as I know has failed exactly one time. It's a simple select count(*) from one table, no joins. But at least this once, executing that query resulted in no data read from SqlDataReader. Not even null, just nothing. First call to Read returns false. No exception was raised. Has any one ever heard of that before? Any scenarios you can think of that would cause it? I'm not even sure what to ask for to look at beyond SQL server logs. It's not something we can duplicate. I am assuming I'll have to chalk it up to a fluke and move on if/until it becomes a chronic problem. Here's a similar query: 

I have two reports from the same customer, where they claim that an error occurred that implies a SQL Statement didn't execute. These are very simple statements, setting a value in a single column. I can see in my application logs that the command executes, and no errors occur. In one case, the records affected value is even verified to make sure that exactly one row was affected. And yet, when they view the data elsewhere, the value is unchanged. What would cause SQL Server to respond successfully to an update statement, and yet when that data is read within the next minute or two, that change is not reflected in the returned data? I'm kind of at a loss of where to even look. SQL Server logs? This is an extremely rare occurrence. It's not reproducible. Customer care informs me that this behavior occurs once every 6 to 9 months, with tens of thousands of statements working in between occurrences. But they know immediately after it happens, and could look at logs. This customer is using SQL Server 2008 (R2 I believe), my application is using ADO.NET (.NET 4.0) and written in C#. These statements do not have an explicit transaction, and are not occurring as part of a batch. One area where this has occurred is setting a single bit column to 1, for exactly one row. The other is updating a single record with an integer value. Sample query: 

Is there any way to disable the fade in and fade out effects in pgAdmin 4 when opening windows? Example of fade in and fade out: 

You can use csvsql, which is part of (a suite of utilities for converting to and working with CSV files): 

How can I copy the content of a cell in the output pane where the latter is detached from the query window in pgAdmin III? When the output pane is not detached from the query window, I can use control + C, but it doesn't seem to work when it is detached. Here is a screenshot showing the output pane detached from the query window: 

Is there any point in specifying the precision and scale of (i.e. declaring it instead of simply )? The Oracle documentation advises to specify the precision and scale, as "it provides extra integrity checking on input". But in my case wouldn't the foreign key constraint take care of this integrity checking? 

This made me think that the SQL area on Oracle DB grew too much to the point of maxing out the shared pool's memory. Running the follow query would confirm it, but ironically and unsurprisingly I get theÂ ORA-04031 error on it as well: 

I wonder whether there is any way to import my server list from pgAdmin 3 to pgAdmin 4 (so that I don't have to create servers one by one again). 

At that point if you try to connect you'll have the same error message. Go to the shell and run the command: 

changing the number you put into the function will change the length of the string that comes out. Obviously, it won't handle duplicates, as Aaron & Erik mentioned. Right now, it can do a string up to 255 charachters, but I suppose you could return varchar(max). I'm not brave enough to use varchar(max) on SE, though... 

The Problem: This query will only return patients whose last 4 visits happened within 1 year. Question: How would I find all the patients with any 4 visits that happened within 1 year of each other? Credit to Jonathan Fite, I used his suggestion to rewrite the query like so: 

It turns a list like this: Albuterol Sulfate Amlodipine Besylate Aspirin Benztropine Mesylate Bisacodyl Ciprofloxacin Collagenase Divalproex Sodium ... into a string like this: Divalproex Sodium, Collagenase, Ciprofloxacin, Bisacodyl, Benztropine Mesylate, Aspirin, Amlodipine Besylate, Albuterol Sulfate I can write it as a dynamic stored procedure, but you can't call sp_executesql from a function (or at least, I don't know how). Question How would you write this function in a way that it could be used on any table? 

but I think I'm barking up entirely the wrong tree. Also, the performance is horrendous (not a deal breaker, since it will be automated, but still something to be mindful of). Question: How would you break this up by a foreign key? Some sort of pivot? The number of diagnoses is quite variable... Ideally, it would be something I can easily smush into a larger query, but there's obviously ways to work around that if needed. Here's my current solution, but I'm pretty sure it's not a good one: 

That's it. Records affected is verified as 1. UPDATE: I don't think it can be dirty reads. The read that shows the previous value happens minutes after the write. There are things about this situation that I have a hard time believing, so it's totally possible there is more to the story I'm not being told. 

Is there a class of error that will thrown an exception out of the query, but not raise it to the application when executing a command or reading from the resulting SqlDataReader? UPDATE: Here's the code executing this. 

This may be a terrible question, because I'm not sure how information I can include to help. We have data segregated by customer. One customer apparently has higher volume of data. The same query ran for a small customer returns in 2 seconds, and the result is 11 rows. The larger customer takes 47 seconds, and the result is 6600 rows. This is a complicated query with 11 joins. This is just for a report, but the report is timing out and the operator is complaining. It's possible the difference is just the volume of data, but I want to investigate. When I look at the query plan, the percentages for the query cost are exactly the same between the two. There are no suggestions for indexes. In both query plans, I can see the highest cost is from a join to a table that has 3.8 million rows. However, both sets of data are joining with that same table. In both cases the resulting "actual number of rows" is 3.8 million. In both cases this join has a clustered index scan that is 39% of the query "cost". So my question is this: Do the percentages and query cost even mean anything? are they referring to the estimated data from the query plan, and do not reflect the real cost? Otherwise how could the same join make up 39% of the "cost" for both, but one takes 2 seconds and one takes 47 seconds? Is SQL server lying to me and the actual difference in cost is the "nested loop" for the inner join that produces the final row count even though it lists that as 0% of the cost? 

The Wikipedia page on data integrity lists 4 types of integrity constraints: entity integrity, referential integrity, domain integrity, and user-defined integrity. Is there a name for integrity constraints that stem from common sense / knowledge? 

One integrity constraint stemming from common sense / knowledge is that the patient more than one year old at the time of hospital admission (i.e. ), shouldn't say that the patient is a neonate. Another integrity constraint from common sense / knowledge is that a patient cannot be admitted in the hospital if he already is in the hospital (i.e. some checks would have to be done around and ). 

Since amazingly with pgAdmin one has to login to view issues / feature request, here is a screenshot: 

I tried DBweaver on Kubuntu and had the same issue. It looks like the issue is around the fact that the authenticity of the SSH server hadn't been verified beforehand, and DBweaver and unable to cascade the correct error message (e.g., ) and instead misleadingly blames some authentication error. I fixed the issue on Kubuntu as follows: 

What could have caused the shared pool to be full? If there's no room left in the SQL area cache, shouldn't older statements be removed? I am running Oracle 11gR2 11.2.0.3. Here are the SQL UPDATE statements I was executing and committing (I know I can optimize them): 

I am sure the SSH server is working fine. I manage to connect to the same PostgreSQL database server through the same SSH server with pgAdmin 3 and RazorSQL. I have checked the IP, username and password, they are correct. I disabled all firewalls. What could be the issue? My settings: