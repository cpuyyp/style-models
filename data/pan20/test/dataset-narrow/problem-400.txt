The simplest approach is to store each relationship exactly once, and enforce that with a check constraint Friend1 

This is why whenever we add a new index, we need to have some kind of baseline testing to verify that none of these issues happen. 

Shutting the system down and doing all changes at once may be very risky. If something goes wrong, and frequently it does, there is no easy way back. As an Agile developer, I sometimes need to refactor tables without any downtime at all, as those tables are being modified and read from. The following approach has low risk, because the change is done in several low-risk steps that are very easy to roll back: 

If possible, I would redesign the table. If we can have VersionNumber as an incremental integer with no gaps, that the task of retrieving the next chunk is a totally trivial range scan. All we need is the following index: 

The select keeps a shared lock on a non-clustered index on itemId, waits to acquire another shared lock on the clustered index, so that it can retrieve more columns. The update has modified a page of the clustered index and of course keeps an exclusive lock. It waits to modify the non-clustered index as well. 

The fastest solution is as follows: you create an additional column, IsLastID, and build a filtered index or an indexed view using it. You can use constraints to ensure the integrity of IsLastID, as described here Grant Fritchey wrote up a detailed comparison of various solutions here 

If you switch to snapshot isolation, this effect should be gone. The following repro script shows how COUNT(*) running under REPEATABLE READ returns wrong results with high concurrency. Prerequisites We need a table with data and a function that provides random integers: 

The individual figures represent individual content elements, while their individual type is represented by their individual shape, and their ID is written inside. Each color represents a relationship (i.e., a group of related content elements). As you can see, for any color (i.e., relationship), the shape (i.e., type) is the same, while there is not more than one figure (i.e., content element) from each site. I hope this makes it more clear - and not more confusing. 

The currently used indexes are included, but that's exactly my question: Given the above tables and the following queries, what indexes should one use (instead)? 

The first table (i.e., ) has an auto-incremented (and thus unique) column , so there is the . I guess that's okay. ;) Both queries using this table only include just the primary key's column, so that's very good already. Then there is the query using both tables, which also includes the column. Does it make sense to use the unique composite here? Or is it redundant, or at least not of great advantage? Please keep in mind that there are just a few different values for the column. The second table (i.e., ) currently has a that consists of the (unique) combination of all three columns. This key is used for the last query using both tables, and also serves as constraint, because for every relationship(_id) and site(_id) there can only be one (or no) content element with a specific ID. There are two queries using just the column in the clause, that's why there is the . Then there is one query that also includes the column. Should one therefore add the composite ? I suppose there is no need for the , which would only be used by a single, very infrequent query, right? Note: The is basically a foreign key to the table. However, as I am working in an environment that allows for MySQL versions/engines that do not include foreign key constraints, I cannot use a real , and all its benefits. But this shouldn't be relevant for my question(s), right? 

It has five constraints which work together to implement the business rule. Let me demonstrate how the more complex ones work. Of course, some constraints are simple and as such do not need any explanations. ** 

Let us drill down one more level and consider just one command, the MERGE. It was released as part of SQL 2008 with several problems, described in the following links: 

The following behavior may be caused by missing indexes on referring side of your FKs: "the price changes take approx 1 hour to process (vs. 1-2 minutes) and sys.dm_tran_locks shows the transaction taking almost 90,000 different locks, compared to around 100-150 when foreign keys were being dropped/recreated" When a row is deleted or its PK/Unique is mutating, the database engine need to make sure there are no orphans. When there is no proper index to support it, it scans the whole thing. 

I have been doing unit testing T-SQL for more than four years so far. I think it much easier to use C#, which is more versatile. For example, in C# I have no problem unit testing a stored procedure that returns two or more result sets - it is plain impossible if you use T-SQL. I described the process in this article and in this blog post. 

If you use a multi-statement UDF, then your inner select is executed exactly once for each outer row. The multi-statement UDF is treated as a black box: the execution plan will now show access to the objects used in your complex view. On the other hand, a subquery and/or an inline UDF is flattened out by the optimizer. When this is the case, the execution plan will include access to the objects used in your complex view. 

// EDIT: Here is a little more context. I didn't put it in the question before just because it was already pretty bulky. Since an answerer asked for more, I will do this now, though. Be prepared, however, it's quite a lot information. The tables are about relationships between content elements (CE). Each CE has an ID and a type. For each type, there is only one CE with a specific ID. There may, however, be CEs and (with types and ), and both have the same ID. Relations are allowed between two or more CEs that A) have the same type, and B) are from different sites (represented by their unique ID). That's why the column is part of the table (rather than being a column in the table). Sites are stored in another table, which I cannot alter in any way. I just use the sites' unique IDs, and that's all. The CEs are stored in type-specific tables, which I also cannot alter in any way. I just use a CE's ID and its type, and thus have a unique identifier. The for the column in the table is used, because there are lots of relationships for a specific type, and I need a unique identifier for every relationship. When inserting a new relationship for a given type, I just have to provide the , and automatically get a unique . That's what is for, isn't it? Or did I misunderstand you, @Rick James? I guess I don't really need . As I reference other tables' columns, however, and as these columns are defined as , I thought it okay, if not wise, to use the same definition. There are two usages of . For the first one, there shouldn't even be more than one entry (because I query for and compare , and is unique). So yes, I could/should remove this, thanks. The second time, however, there might be several entries - all with the same . That's why I happily stop if I got one result. Is there anything else I should provide you with? 

Either way, only you can determine the impact of your solution on the performance. Typically we do not have deadlocks in our system at all, although we do have a lot of potential for having them. In 2011 we made a mistake in one deployment and had half a dozen of deadlocks occur in a few hours, all following the same scenario. I fixed that soon and that was all the deadlocks for the year. We are mostly using approach 1 in our system. It works really well for us. 

Another way to ensure very fast selects is to store periods as sequences of days - then we can select number of open intervals directly from an indexed view dbo.ConcurrentPeriodsByDay, which is as fast as it goes. 

Anyway, it looks kind of hacky to me. Is there a better way to retrieve, for example, numeric[] as a type if my parameter is an array of numeric? 

Ralph Kimball recommends storing dates as integers. He has written a lot, both online articles and books. You can use a calendar table and issue consecutive numbers to your dates, as follows: Date Number 20120229 1234 20120301 1235 

Note that your design does not prevent cycles. We can easily enforce that via constraints as well. I can elaborate if you are interested. Regarding the efficiency of finding qualifying descendants, we can add some redundant data and get much better speed. For example, E is a descendant of A, but not a direct one - B is between them. We can store the following rows: 

Can a T-SQL solution for gaps and islands run faster than a C# solution running on the client? To be specific, let us provide some test data: