It is not surprising. First, you are using different measures of feature importance. It’s like measuring the importance of people (or simply sorting them) using their a) weight, b) height, c) wealth and d) IQ. With a and b you might get quite similar results, but these results are likely to be different from results obtained with c and d. Second, the performance of your models is likely to be different. In extreme case, output of one of your models could be completely rubbish (in your case it is more likely to be NB). Then the feature importance metrics produced with such model is not credible. In less extreme scenarios when the difference in models‘ performance is not so dramatic the trustworthiness of importances produced by two different models is more comparable. Still the importances might be quite different due to the first argument, i.e. different language used to capture the importance. Edit: You have not asked about it in your question, but there are feature importance approaches which are model agnostic, i.e. can be applied to any predictive model. For example, check the permutation importance approach, described in chapter "15.3.2 Variable Importance" in The Elements of Statistical Learning 

As a side note, you have not mentioned any regularisation parameters of xgboost, so I understand you don’t use any. In general, it is not good and might lead to overfitting. Regarding your question, my hypothesis is that in your case xgboost classifier is simply more powerful than other two approaches and thus is more confident, which is indicated by higher probabilities assigned to particular classes. Maybe xgboost is even overconfident, i.e. overfits, but one cannot be sure without thorough testing on unseen test data. 

You can just calculate average miles per gallon for each given vehicle, e.g. E(Miles per gallon | vehicle = Mazda 6 2016) =(8.6+8.1)/2 

From your comment I understand that you are trying to solve the binary classification problem using your aggregated data and you get very poor results when you simply use the mean. Depending on specifics of your data and the shape of your time series, there are several alternatives you could try. Note, that you might need (significantly) more than just a single number per time series to solve your problem. 

If I understood you correctly, these two facts perfectly reconcile in case your classes are not separable in one-dimensional feature space. The following exaggerated toy example should illustrate such situation (and power of RF). 

It depends on the definition of accurate model, but in general the answer to your question 1) is No. Regarding your second question (based on results in the paper of Niculescu-Mizil & Caruana linked below): 

I suggest chapters „3 Linear Regression“ and „8 Tree-Based Methods“ in An introduction to statistical learning 

Why don’t you just put the data with pay_2 40k and 45k into two additional rows? Additionally, you could add an indicator variable loc_1_2 and set it to 1 or 2 depending on the location. In general, dummy encoding might be dangerous. Instead you could convert your addresses or locations into physical coordinates (latitude and longitude) or just distance from, let’s say, centre of the city or center of the country (depending on your data). Edit Ok, I see now that putting the data with pay_2 into additional rows will not work. 

If you pick option 2 or 3, you’ll need to add a binary predictor indicating whether pay2 column was modified or not (NaN or not). Re dummy encoding: You haven’t asked about it explicitly and it’s difficult to suggest anything concrete without knowing the cardinality of your nominal variables, so I just put here the link to my answer about impact coding 

Yes. You‘ll just need a sufficiently large training dataset and network that is flexible enough (start with one hidden layer). You’re right your problem is not linear, thus you’ll need non-linear activation function. 

You can test whether it is the case for your particular model and dataset by looking at the so called reliability plot: 

Additionally to the mean, you could use the quantiles or some other summary statistics, like standard deviation, min, max. One could try to sample the data, i.e. instead of taking entire time series pick only the values that are minutes, hours or days a part. Or pick only mid-day values. The frequency of the sampling depends on your data. Or just pre-aggregate by calculating averages for every hour, day, month, etc. Additionally, you could calculate the periodicity of your time series and use it as a new feature. Or calculate some trends. Try to fit some standard time series models to your data, e.g. ARIMA and use the coefficients as informative features. Last but not least use the domain knowledge re what could be relevant feature for your classification problem: the biggest jump (max first order difference), change of regime, etc. 

Good question. Yes, there is a way. The approach that can help you is called partial dependence plot (PDP), see the links below for further details and examples. The approach is model agnostic, i.e. works well for any predictive model, powerful yet simple. The main steps for one-dimensional partial dependence plot are as follows 

For your information: not all classification algorithms need standardised data, e.g. you may want to consider random forest or GBMs. If you stick to SVMs, regularised logistic regression or similar then I agree, it’d make sense to standardise. 

I do not understand what you wanted to say with this. Anyway, the most straightforward way to consider the certainty is to use it as a weight (or exposure) and use the methods that support it, e.g. GLMs or GBMs. For example, the concept of weights is explained in this blog post 

You asked about „tried-and-true“ method, thus I‘m suggesting the logistic regression. I‘m not familiar with TF yet, so not sure whether this approach is easily doable there. Below is the link to the answer explaining how to do it with R‘s glm: to put it shortly, besides specifying the response column (rates between 0 and 1) you have to provide the weight column representing the total number of trials. how to do logistic regression for fractional outcome 

My rather limited experience with scaling of features suggests that it has virtually no impact on xgboost results. I suppose by normalisation you mean subtracting the mean and then dividing by standard deviation. If you calculated the statistics based on entire dataset (including holdout) you would get data leakage, which might indeed, at least theoretically, degrade the performance on holdout. According to my understanding of xgboost, the correctly performed scaling should have no impact on the performance. I suggest you double check your implementation or provide more details on how you do it, preferably with including a reproducible example. 

If you classifier can handle NaNs (NAs), e.g. R version of xgboost, then you can just use the data as you show in your table above. If you have to standardise the data, then you can use mean-imputation. The average and standard deviation that are used for the standardisation should be estimated on non-NaN data, of course, (pay_2 column excl NaNs). In general, in order to prevent data leakage you should not use the data from holdout data subset (or “test”, “validation” or whatever you call it) for calculating the mean and standard deviation. If your classifier doesn’t require standardisation, you can put some numerical sentinel values instead of NaNs, e.g. -1, 0, or -9999. 

With one-dimensional PDP described above you can easily see the marginal impact of a predictor being analysed on the model output. Furthermore, one can use similar technique to perform multi-dimensional analysis, e.g. to investigate the impact of interactions. partial dependence plots- scikit-learn documentation partial dependence plot - tutorial by Dans Becker on Kaggle 

You can train xgboost, calculate the output (margin) and then continue the training, see example in boost from prediction. I‘ve not tried it myself, but maybe you could train on the first subset of your data (say 10%) and then continue on another subset, etc. 

Yes, it is possible, although, in general, not advisable. If I may suggest, instead of cubes, cones and 3D pie charts it’d be better to use simple dot plots. 

Ideally, your X-Y points should lie near the diagonal Y=X, otherwise the output of your classifier cannot be interpreted as a probability of an event. However, not all is lost and if needed, one can try to modify (calibrate) the output of the model in a such way that it better reflects observed probability. In order to assess whether the calibration exercise was successfull one might look at the reliability plot based on the calibrated model output (instead of using raw model output). Two most widely used techniques for the calibration of clasiffier outputs are Platt scaling and isotonic regression, see the links below. Note, that it is not advisable to cailbrate the classifeir using the trainig dataset (you might need to reserve a separate subset of your data for the purpose of calibration). Some relevant links Predicting Good Probabilities With Supervised Learning CALIBRATING CLASSIFIER PROBABILTIES Classifier calibration with Platt's scaling and isotonic regression