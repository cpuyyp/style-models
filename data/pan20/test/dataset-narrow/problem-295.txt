Try the following tables user(user_id(pk), name, email) flashcard_group(group_name(pk), group_tag) flashcard(flashcard_id(pk), question, solution) flashcard_groupings(group_name(fk), flashcard_id(fk)) <- to join flashcards to groups guess(guess_id, user_id(fk), flashcard_id(fk), is_correct) <- user_id and flashcard_id to be unique key. The above assumes that there is 1 user to 1 set of guesses, and that the flash card grouping is grouping the flashcards together (ie cards a, b and c and in group green, etc). If a user can have multiple go's at this then you need to add another table to hold the session information between the guess and user, a bit like what the table flashcard_groupings is there for. To get the queries you are after, join the tables as needed to get the groupings you need. For guess percentage, just join tables user and guess. To get the flash cards join user, guess and flashcard. And so on. 

Do you have any locations that the packages are stored in/at (like shelf number/silo/etc)? If you do, then you can have a table with locations and package at that location. To find the adjacent packages you need only to know the location id +/-1 location. The location ID would need to be unique and sequential. If not, then you would could define a constraint (which probably means using a database engine that is more powerful the Access, maybe SQL server express?) Then you could create a trigger function that checks that the X next to Y == Y next to X on any updates. Another option is to create a linked list structure in your table, so that the data element for X has next filled and Y and the previous element for Y has X, with the prev/next/current fields having a unique constraint on them. Eg 

If backups/restores are the only activities occurring between the SQL Server and your offsite Data Center, you could setup Firewall Quality of Service rules to limit transfer speeds. The total amount of data you transfer can only be reduced by lowering the amount of data you send (such as only sending every other backup). I will highly suggest that you adjust where your backups are created, though. You should backup your databases locally (preferably to another server within your local data center or to a separate backup drive locally on the server itself if you don't have other options available), and then configure a job to copy the backups (or every other backup) off site for long-term disaster recovery purposes. Not only could this help reduce your bandwidth issue, but more importantly, having at least one backup locally will eliminate the possibility of a network outage or some other external factor from preventing you from recovering the database in the event of an emergency. Backing up a database directly to an external data center is a risky proposition as you can generally count on some sort of network issue every now and again. I've seen far more network related outages than disk or server outages, though my experience may not be reflective of everyone's. 

First, you must remember that everything in computers is represented in binary; however not all numbers are easily represented using binary syntax, specifically when it comes to certain fractions. One such fraction, ⅓, is actually impossible to represent in a computer. Even with the largest amount of memory possible, a computer will never be able to accurately represent ⅓ in binary form. However, it can get more precise (e.g. close) to ⅓ with the more bits (e.g. and therefore decimal places) that are thrown at it. Precision can therefore be thought of as how exact a number you want to represent. The more digits the more precise that number becomes. 

I half remember a problem like this. I think I ended up migrating the schema then dumping the data as csv and loading the data from the csv file. I remember having to update the csv file (using unix tools like sed or unixtodos) or using open office calc (excell) to fix up some items that were errors on the import step - it could be as simple as opening and re-saving the file. 

The performance drop is probably becase in the 1 day of data, the database engine may be doing a sort 1000* on 1000 pices of data, but the month is sorting 1000* for 1000 pices of data/day * 30 days (where 1000 * 1000 << 1000 * 1000 * 30 ). It may be faster to run the query 30 times for 30 days of data rather than once for 30 days of data beacuse the amount of data that the query has to loop through is less. As others have explaned, you need to analyse your query and try to 1> reduce the amount of data the query looks at by excluding as much as possible 2> Choose indexes so that the data that the query has to search through is optimised. Maybe change the idex type (if the engine has different types) 3> change the order that the joins take place, may change the exection order that the planner chosses. 4> maybe load some of the "where in (" data into a temp table as a seperate query before the main query or create a string if there arnt may values and dynamically create the query (so rather than "... where in (select .. " use "... where in ( 1,2,3...)"). As others have pointed out look at the query plan and see which parts take the longest time - they will be probably be joins and scans (DISTINCT clause) and they to optimise these by making sure there are indexes to assist and maybe restructuring the data. You should have an index on [FACT].[DataMine].PartitionID, but it may make a difference to drop that before the insert and then re-create it, as sometimes this is faster. Possibly try a bulk insert as well. You would need to load the results of this query into a temp table first though, otherwise the sub queies would have no index on this field when they are selecting, which would make the situation worse. 

I'm shamelessly plagiarizing myself from a different question, but I suspect the same scenario is happening here. Depending on the statement the App Team is running and how you have your linked servers configured, you could be running into a situation where you have an orphaned distributed transaction sitting out there that you need to kill. To identify if this is the case or not, first run the following query: 

You can SQL Inject it without much issue, sadly. Here's a simple test I just ran with an elevated account with the results included: 

If it were me, I would put everything in the database that I can. For instance, all SQL logic should go into packages/procedures, and I would control the execution of these routines via jobs if possible. Notifications based off of results from these routines can be sent via . Putting this in the database allows you to lock it down with proper security. If you truly need to execute Shell Scripts (steps 1 and 2 above) outside of the database, there are various ways you can do this from a PL/SQL routine, but they get hard to manage if things don't go exactly as planned, so the shell script needs to be able to handle errors gracefully. Just try to minimize security issues by properly locking down any external files being called, etc. I'm not a fan of calling shell scripts from PL/SQL code, but ultimately your process and comfort level is going to dictate your approach. There are other approaches too, especially if you have money to spend. Another option would be to look into enterprise-level scheduler applications. There are a fair number out there, and if this process is complicated enough or starts affecting other processes within your enterprise these can help make sense of the chaos. At the end of the day, it'll come down to your comfort level with the technology(ies) and the amount of resources (e.g. time and money) you can spend that will determine your best approach. 

I am assuming here that the system is not a highly used transactional system and that you have windows of low usage to run the analysis queries. If you need to maintain high levels of performance, you may like to do the above in a separate database (separate hardware) and port across the new data that you get from backups. 

And then to be real sure put in a constraint or trigger function to check that prevID matches the nextID of the previous element. 

Try running a sql profile trace on the server activity ($URL$ - you may have some table locking issues caused by concurrently running queries locking tables/pages/indexes. Try running a perfmon - there may be something/s running that is using all the available disk/IO/CPU/Memory resources when things slow down. Look for any memory swapping - this will slow things down. If it is one of these then you will need to track down the cause. Note CPU at 100% for short period of times is OK (actually can indicate optimum performance). Also check for fragmentation and consider rebuilding indexes. Turn off services/applications that are not needed - they use up resources that are better used for your application. 

The CPU at 100% is only a problem if it stays at 100% for a prolonged period of time. More important is disk IO and memory usage. If these are high then you will need to do some tuning of the databases, such as getting better hardware of splitting the databases across servers. The next action to make is to run profiler ( $URL$ ). You could do a couple of passes with this. Firstly you could just look at connections to see which DB's are being used the most and which are not used. Next look for long running queries - these are the ones to tune as they will be using the most resources. If the database is transactional, look for the queries that are run very frequently (put the profiler output into a database table and count by query) and look at tuning them. You could also explore the system tables which store some of the information you need ( $URL$ ) If performance is still an issue, then you need to look at new hardware (bigger server, faster disk, more memory, faster network) or to split the databases across servers. 

UOW is the request_owner_guid returned in the above query. I've run into this situation with both linked server issues and jobs that are making calls externally from the SQL Server process. In either case, I would find that an external process would interfere (e.g. network drop, process is killed or errors unexpectedly, etc.) and then the transaction wouldn't properly terminate at the SQL Server. SQL Server won't know that the transaction has actually failed and things will just sit out there until you either walk through this process or bounce the server as you are already doing now. Hope that helps. 

This is an old question, but I fell prey to this issue recently and found that SSMS wasn't overwriting a "bad" settings file. To fix it, I had to close SSMS and simply delete the settings file using Windows Explorer. SSMS then loaded with the default settings when I started it back up and I was able to reset things to the way I wanted once more without issue. I suspect something occurred on the network that prevented SSMS from overwriting the old file (as my path is mapped to a UNC copy of the My Documents folder). The settings file location can be found within SSMS at Tools → Options → Environment → Import and Export Settings 

You need a few tables, 1 - The questions ( question id, input type, visible, question type, question text, expected answers....) 2 - The Answers ( question id, user id, activity id, answer....) 3 - The users ( user id, user name......) 4 - A table to hold a question/answer activity (activity id, data/time, user id) You may also like to have a table that specifies the questions that should be applied for each activity - either grouped by user or maybe a question collection. The foreign/primary keys will be the columns that have the same name in multiple tables and should be indexed. If you use this structure, you should be able to add a question or user or change an answer without having to change your schema or presentation code - make sure that the presentation code is dynamically created at run time - you just need to add a record in the appropriate place. This approach may take longer to develop initially than a hard coded approach, but will much simpler to maintain as you only will need to change data to change behavior. (A tip, to create your presentation layer, you will need a query that gets the appropriate questions to be displayed, then loop through this result set and call a method to render to question on the screen, the methods to chose being appropriate to the presentation of that question [text box, radio group, etc])