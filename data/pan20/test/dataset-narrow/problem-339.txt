I'm designing tables for a new log system and am having trouble figuring out what indexes I'll need (I'm a developer, not a DBA), and am also open to any suggestions/criticisms/etc. for making the tables as "fast" as possible (from my app's point of view). Background info: the DB itself is a MySQL instance hosted on AWS. It's a shared server and I don't have tuning privileges beyond basic table setup, indexes, key constraints, etc. This is a logging system, so very write-intensive. Although I will write scripts to dump certain log records (errors, etc.), there won't be much reading going on. The logging system will consist of two tables, which is the log record itself, and which is a "static" lookup/reference table used by . Below, where I specify Type as just , I haven't figured out what size to make the field (, , etc.). My proposed schema for the two tables is as follows: : 

The problem child here is . This field will store a lot of information, and for reasons outside the scope of this question, I can't break this information out into better-normalized fields/tables. It's just a big text blob that will store lots of different kinds of data, and again, there's nothing I can do about that. When I do perform reads (for the purposes of analytics/debugging), these are the queries I plan on using the most (and hence perhaps a basis for creating indexes from): 

My understanding of optimistic locking is that it uses a timestamp on each record in a table to determine the "version" of the record, so that when the record is access by multiple processes at the same time, each has a reference to the record's version. Then, when an update is performed, the timestamp is updated. Before an update is committed, it reads the timestamp on the record a 2nd time. If the timestamp (version) that it has is no longer the timestamp on the record (because it's been updated since the first read), then the process must re-read the entire record and apply the update on the new version of it. So, if anything I have stated is not correct, please begin by making clarifications for me. But, assuming I'm more or less correct here... How does this actually manifest itself in a RDBMS? Is this 2nd read/verification enforced in the application logic (the SQL itself) or is it a tuning parameter/configuration that the DBA makes? I guess I'm wondering where the logic comes from to read the timestamp and perform a 2nd update if the timestamp is stale. So I ask: does the application developer enforce optimistic locking, or is it enforced by the DBA? Either way, how? Thanks in advance! 

Both forms of locking cause a process to wait for a correct copy of the record if its currently in use by another process. With pessimistic locking, the lock mechanism comes from the DB itself (a native lock object), whereas with optimistic locking, the lock mechanism is some form of row versioning like a timestamp to check whether a record is "stale" or not. But both cause a 2nd process to hang. So I ask: why is optimistic locking generally considered faster/superior than pessimistic locking? And, are there are use cases where pessimistic is preferred over optimistic? Thanks in advance! 

I want to have two MySql servers running on the same system (for master-slave replication tests purposes). To be able to do this I need both servers read different files. As I see at startup MySql tries to read from one of default locations. How can I specify it own location for MySql server? PS. I am about to use MySql 5.6 under Debian environment. 

I want to make some approximation of table size for MySQL cluster. Please read my reasoning and tell me, if I am right or am I making mistake somewhere. I have simple table with PK (int) and additional column (int). In MySQL int = 4 bytes, so one row is about 12 bytes (4*2 for columns and 4 for PK index). If I have for example 1'000'000 rows this will be 12'000'000 bytes ~= 11.5MB. Am I right in this calculations? Can I store such table with so little space? 

I have MySQL Cluster instance and want one column (BLOB) to be stored on disk instead of memory. I tried to follow this tutorial: $URL$ And this is output I am getting: 

Here is syntax for this command: $URL$ Any ideas why this is not working? I cannot find where I am violating this command syntax. 

I know (on high abstract level) how hierarchies in Oracle works, but this one I do not understand. When issued following query: 

I have some code running on Oracle 11g, we are migrating to 12c (12.2.0.1.0). In one of the processing procedure is invoked and in parameter name of the view is provided. The view is a simple select query from one table, one column is computed by function, other are taken as they are in the source column. The user that invokes is the owner of the destination table, view and table under the view. In 11g code is working, in 12c I receive following error: . Any ideas about reason? Are there changes in 12c version of implementation, that forbids the use of view as a source for ? 

I have also indexes on this partition. Should I recreate indexes after compression, or is it not necessary? 

is returning 1000 rows. What is really happening in background of this query? How Oracle engine is multiplying rows when tries to resolve hierarchy. 

For some testing I have installed MySQL Cluster. I can get it to work with configuration suggested in Lunix guick start guide. My configuration: 

2500*50KB ~= 1GB I have this error any time I am trying to insert more than 1GB of blobs. How I can extend size of this tablespace? 

And it works. Now I want to start it with only one node. So I changed to 1, deleted second node configuration from conf and starting second node from start script. New config file: 

I have no idea what I am doing wrong. I want to start only one node because ram saving during tests. EDIT: Ok, I have no idea which one of logs I should investigate first. 

By getting the last insert into that table from sql queries submitted. However this isn't as efficient or accurate as other answers, but as i said if you want the extra info to log like plans etc this could be handy. It helps to see other thought processes for fun. 

how about going the OUTER APPLY route. You can modify the inner query to handle nulls differently, i've just disregarded them but if you want to handle them you could use ISNULL or CASE in the future. 

Microsoft documentation mentions they're removing it, it says "Next Version" but i assume this was written for a previous version. They advise to use maintenance plans in the future. Microsoft Documentation for sqlmaint Edit: Last update on that article was 03/14/2017. But given all the examples point to it being originally written with SQL Server 2008 in some and edited with later versions as examples (it's all over the place to be honest), it's probably safe to say the note at the top is wrong and it's actually removed already. 

I'm drawing up a proposal for upcoming infrastructure changes. This will include a production server and reports/data warehouse server, each with Always On. To keep hardware and licencing costs down, is it possible to run in a configuration of Server-A running Prod-AG Primary and Rep-AG Secondary, and Server-B running Rep-AG Primary and Prod-AG Secondary? I presume each server would need 2x of the following WSFC instances,sql instances, AG's, listeners, DNS names/ports. I hope this makes sense, here's a diagram of what I think it will looks like. 

Try this using CTE, you're only calling the function a minimal amount of times this way. Could potentially be improved if you can filter the first queries somehow if appropriate. 

I finally got the right balance of performance and accuracy. The below will return the top 100 queries based on average total elapsed time. Its for SQLCMD mode, you'll have to change that if you don't want to use it in that setting. 

I believe this is to do with the plan cache. Just ran a little test on my end and I can replicate it. To remove the table from tempdb works, obviously target the plan handle for the stored procedure. You could do this but it would just come back again and freeing the cache all the time generally isn't the best idea. If you're concerned about it taking up space you can run this query I wrote for a monitoring tool that should work. On my tests it always shows 0KB after the SP has finished. This leads me to believe it just keeps the shell of the table there for some internal use. So it seems harmless to me. 

Thanks to @SQLWorldWide who's suggestion about reading the DB from the xml, that added alot of time (3mins) but while i was playing around within dm_exe_query_plan i noticed the value column was equal to the Database ID. it might not be 100% encompassing of all traffic but its fast (1 second) and works for what i need.