I'm assuming that "srvadmin" is the account that you created when the OS was installed and configured. If so, a few ideas: 

IIRC, ntpdate is used to set times, but ntpd is used to maintain the time on a system. Look in Server Admin --> --> Settings for the NTP on/off check box. Don't worry about the config files. If you can't get a solution from the GUI, then the following website might help: $URL$ Good luck. 

If this is at a school, give Apple support a call. They offer free support to public schools in the US at 800-800-2775. When asked for the product that you're calling about, say "XServe" or "Mac OS X Server" to get to the department that you want. 

Login as ANYONE and type "id srvadmin" to confirm that the account still exists. Put in the OS install disk, reboot from it, and use the password change tool. 

Again, that is pseudo-code. Its meant to demonstrate the idea that I'm thinking of within 5 minutes. Do not expect the above to be similar to the code you end up producing. Hope that helps. 

This depends on what you're authenticating against. For example, I once got squid to authenticate against a Mac OS X Open Directory server (effectively LDAP). A few tips to get you started: 

I'm looking for hardware to run FreeBSD. I really like the chassis of the Dell R210, because two of them can stack up on a shelf in my wiring closet and take up the space of a single tower (laying on its side). However, after spending weeks trying to make it work, I'm seriously considering returning the thing. It's HD controller doesn't work with FreeBSD in RAID configuration and it doesn't even work with 8.2-RELEASE at all. (This is the H200, BTW.) Can you recommend any hardware that is sold now (can't buy used hardware at my job) that will run FreeBSD with RAID support and two ethernet interfaces? Thanks in advance! 

I need to find a way to give some telecommuters access to the office network. However, there is a high degree of confidentiality involved. So I am looking for a way to give them dedicated hardware (computers, thin clients, etc.) to bring home for the task. These devices must give access to file servers, printers, etc. in the office, but nothing else. Users must store files in the office server, etc. They should be prevented from storing items on the computer at their house, so something like a VPN isn't quite enough. I was thinking that a thin-client system would be great, but I don't know how to go about doing that. Any suggestions on products or techniques? I don't have any prior experience with thin clint systems. 

Is it possible that there are instructions in ~weirduser/.bash_profile that are causing it to logout? You don't offer any details about how it behaves (error messages, timing between steps, etc.) when you try to login from console or SSH. However, I noticed that it has bash for a shell. The man page states that ~/.bash_profile is only used for login shells, which I think means console and SSH but not su. Check other ~weirduser/.bash* files, while you're at it, too. Look in /etc/profile, too, but that is highly unlikely. Its just a guess, but its worth checking. If you provide more data about the way in which it fails, I'd be happy to try to come up with more ideas. Good luck. 

I use FileWave to deploy files (e.g. "Google Chrome.app") and installers (e.g. "MacOSXCombo10.6.7.pkg") to over 600 Macs. Mixed with Deploy Studio (via a NetBoot server), I have a fully automated Mac workstation deployment system. I've also heard of (and been really fascinated by) RAdminD, Star Deploy, and Munki. Both of those are free systems that do parts of what FileWave can do, but aren't quite as robust. For example, they aren't as good at giving lists of what updates installed, didn't install, and why it didn't install. Also, FileWave can do roll-backs ("Geee.... That software has a bug. I'll just pull it back off the computers and replace it with this cached copy of the previous version for now.") and manage iOS (iPads, iPhones, iPod Touches) to a degree. Hope that helps. 

Use an MX record but NEVER EVER use a CNAME on a domain. It violates the structural basis of DNS. A CNAME, or Canonical Name, is like a synonym in DNS. And DNS is hierarchical. So saying foo.com is a CNAME to bar.com will invalidate all hosts such as xyzzy.foo.com and all MX records in foo.com. It effectively replaces foo.com with bar.com. This is what I was taught when I was working on DNS in the 90s, at least. The advise has served me well in the 12-13 years since I heard it. 

You can export and import JUST the workgroups (based on your purposes) via the menu bar -> Server -> Export (or Import). I definitely bow respectfully to the shell scripting in your follow-up post, but it might be more high end than you're comfortable using. Also, you can have remote-bulding server simply "Connect To" an Open Directory Master instead of making them Replicas. You get most of the same advantages. I've personally run both Connected and Replica servers for years. I've even done both of those over heavily utilized T-1 connections. (In fact, there are only two reasons to use a Replica instead of a Connected setting. One is for performance when in a remote, latency heavy site. The other is for redundancy -- the master can go down and users can still login.) Maybe that would give you your ideal solution? What kinds of problems did you have with Replicas? Maybe I could help, if you're interested. 

I don't know if you're still trying to solve this or not, but a thought just hit me. Some MCX settings can be overridden. For example, if you turn something on at the workgroup level and turn it off at the computer or user level, it will then therefore be off. Also, there is a "guest computer account" which can have settings applied to it. Any computers not specifically listed under the "Computers" tab will have these settings instead. I don't think that these issues are happening in your case, as you have the data in mcxquery telling you that the MCX settings are active. However, pointing this out may help you make some connection that neither of us made before now. Good luck. 

At the beginning of the day and on semi-random days, some of my Mac users are experiencing heavy lag. For example, 5-10 minutes to login, several minutes for Safari to load, visible delays while typing in a text field in Safari (e.g. the login form of a certain webapp). This doesn't happen with all users. The common element appears to be that these users all have their home directories on the same XServe. Users in the same building who have their homedir on either of two other servers do not appear to be effected. All three servers are in the same subnet and all users are in the same subnet. They all have the same Open Directory master. Yet the users on one server have this symptom at the same time that the users of the other two servers do not. Any suggestions? 

Are you sure that this is what they need? I don't mean to offend, but I've found that most of my teachers' needs were better addressed through other methods. For example, I set up a "Drop Box" share point. I then put a folder for each teacher in it. That folder is set to be world-writeable, but only readable by the teacher's account. Students can then hand in work by copying their files into the teacher's folder. They've come to call this "drop-boxing" it and really like it. Its easy to learn, its scaled well over many years and hundreds of students. The teachers even use it to send each other large files. If I had a better grasp of your needs, maybe I could see why the above doesn't work for you. Or even suggest a better method. 

Several ideas popp to mind. You'll have to research them to figure out which fits your situation the best, as I'm missing some information which would factor into it. CARP - Good if you're running FreeBSD or OpenBSD. I think that a variant is ported to Linux, but is implemented in userspace instead of at the OS level. I'm not positive. You can get some good details in the FreeBSD handbook, if you're running that. See also HAST for file synchronization. Reverse proxy - You can use this feature in apache or squid. Bsically, you have web server X and reverse proxies Y and Z. Then X has the authoritative data and requests to either Y or Z will cause them to ask X for the data. I use this to put a firewalled Mac mini on the Internet and have it dish out content from some proprietary/embedded Windows services. Then vulnerabilities in Windows become less of an issue. So in my case, it's more about security and my lesser skills are securing Windows. However, I've heard of some people using this to make "load balancing." IP - If you don't mind having to do this manually, you can always just reconfigure the IP of the second server to become the first. Not elegant or quick, but cheap and somewhat easy. CARP (see above) is effectively an automation of this. You may even be able to use a mix of cron, shell, and ping to vaguely simulate this. Load balancer hardware - I've heard of this from people with bigger systems than the one I administer. I don't know a lot about it, but it's out there. Hope that helps.