I remember I already found an article illustrating the best approach, but I lost the link and couldn't find it :( So, very common situation. Imagine we have orders table, and an order has a status. Three choices: 

However, depending on the order type, server needs to receive additional information. If the type is server needs number of photos. If the type is server needs number of minutes. I use Laravel for backend development and I found it really inconvenient to work with . In general situation I would write validation rule as: 

Which is the best approach? I know that enum is evil, because it's hard to modify database if we need to add a new status. Probably dictionary table is the best: we can see all possible values and we can easily add a new one. However, when using varchar, it's way easier to work with it, like: 

So I made a lookup table with following columns: id, name, title, budget_factor Therefore, orders table has type_id which is foreign key to id of types table. Everything's good. However, when I started developing I came across next case. 

On the other hand I'm not a DBA expert and I'm not sure if it's a good practice. I have quite a lot of order characteristics, all of them are in lookup tables. For some lookup tables I can't use this method, as they have up to 10 values with long names (like ) I can't use enums (as possible values can change + I have some associated information for every value). So is this a good practice, or should I stick to id's? I don't care about DB size for now. 

Use varchar for status column Use enum for status column Use separate dictionary table, which has id and name, and in orders table keep status_id as a foreign key. 

instead of retrieving from dictionary table and comparing it to So all the time I start a new project I ask myself the same question. For example, right now I have roles. I mean not "roles-permissions" system, where we can add a lot of roles with different permissions. I just have 2 big roles, with different logic. So I can make a separate table with 2 values, or varchar and compare string values. I'm talking about MySQL. Sorry for silly question, not an expert in DBA. I suspect that using dictionary table is also better because it'd be faster to search statuses by int, instead of strings, but not sure. 

If you really need to do this (a la Windows Registry values) I would probably try to keep it simple and keep it to one table something like this (untested pseudo-DDL): 

I'd go for option 1. With this approach there are no "hacks" in the app for anonymous users. The standard security model continues to apply with the only difference being that if you don't know who the user is (ie, they haven't authenticated) that you pretend they are the anonymous user, which in effect they are. Option 1 is also the approach used by most (all?) web servers and file servers: they use an "anonymous" or "nobody" user. 

No. Duplicate entries are basically ignored. The documentation of describes how the rules work pretty well: 

Extend or adjust for data types as needed. Remove for each data type if you wish to allow null values. The only complexity is the constraint to enforce that the appropriate column is used. If values are mandatory, then the check constraint can be written as 

is Oracle's way of defining a "full text index" on a column -- as opposed to an ordinary B-tree. The good news is that Postgres also has full text indexes, and these are documented extensively at $URL$ The basic syntax for creating a similar index in Postgres is 

All I've done is move all the code into a stored procedure, and redefined the trigger to CALL that procedure. 

If the two tables share ID space, surely then they should have a common parent table? I would create a parent table and make the two other tables children of it with foreign keys. The parent has the autoincrement field, and you have to insert there first before inserting the child. Add a "type" column to the parent table so you know in which table to look for the child with the given ID. The biggest advantage to this approach is that you don't need any triggers. No performance or synchronicity problems to worry about. In PostgreSQL you would implement this using inheritance, but I don't think MySQL has this feature. 

Oracle will only scan the partition for the year 2013. On the other hand, if you are altering the table to add or remove partitions, you will more often than not need to name the partitions. This isn't always true: for example, Oracle 11g has interval partitioning where Oracle can automatically create new partitions for new data as needed -- perfect for the example I just mentioned. 

When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

Does this same fill strategy apply to tempdb? And does this answer depend on the type of query, e.g. parallel vs. non-parallel? Or is the answer different based on the kind of tempdb I/O, e.g. for temp ables I create vs. tempdb usage by the database engine for worktables or spilling? 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it. 

If you don't need the data, just the count, then just replace the columns in the select clause with : 

Oracle's wrap program doesn't wrap triggers. The way to do this is to move the code into a package or a stand-alone procedure, and make the trigger a one line call to invoke this code. See the documentation on the limitations of wrapping: $URL$ In your case, something like the following (untested) should work: 

etc Some values are mandatory, others are optional. Check constraints are used to further specify valid values for parameters. For this to work you need to be Agile in order to be able to add and drop columns as needed. But this is how we do it. 

But as others have mentioned, optimising expressions isn't going to help you; the Postgresql syntax is merely cleaner. You'll need to show us your execution plan(s) and table definitions for us to be able to help you further. 

Once we have these two values we can determine which of the two is closer. So, something like, untested: 

I think the Oracle Locale Builder utility is the tool that you want. The documention on the 10.2 version (since you've tagged with oracle-10g) of this utility is here: $URL$ 

This is almost certainly not what you want in an OLTP application. Or in a data warehouse type application it implies that is a dimension, and is a fact table. However you would normally see a hierarchy in a dimension, which you don't have. So then there's no advantage in not using the built-in datatype instead of inventing a surrogate. 

Determine dependencies between systems both on the "from" side and on the "to" side. This will affect the "flow" of the migration. Decide how the data will be migrated. Amongst others, choices are: 

This is not a deadlock. One transaction will simply block -- waiting to acquire the lock. The other transaction will proceed. As soon as the other transaction is done -- either by or , the first transaction will proceed. A deadlock happens when a transaction has acquired a lock on object A, and attempts to acquire a lock on object B at the same time as another transaction has already acquired a lock on object B and is attempting to acquire a lock on object A. Both transactions will then block, waiting on each other. That's the definition of a deadlock: two transactions blocked waiting on a lock that the other has. 

So we're wondering if there's a lower-cost solution that would store as smallint but expose the colunms as ints to readers. Like this: 

Our SQL 2014 server has a blazing-fast tempdb drive (2800 IOPS) but a much slower data drive (500 IOPS). Our application runs a few long-running reporting queries that are I/O-intensive and we'd like to avoid them starving our server for I/O capacity when they run. Ideally we'd be able to limit these queries to 50% of available I/O capacity. Unfortunately SQL Server Resource Pool's IOPS throttling is not percentage-baed nor volume-specific. If I limit to 250 IOPS, then it will unnecessarily slow down performance of queries that make heavy demands on tempdb. Slowing down these long-running queries if the server is busy is OK, but slowing them down by 10x+ if they need lots of tempdb access is not OK. So we're looking for workarounds that will defend other queries from these lower-priority, long-running queries, but without unnecessarily hurting performance of these long-running queries if they happen to use lots of tempdb. It's not practical to change the queries themselves to reduce tempDB usage-- these queries are generated by a custom reporting feature that may sometimes generate really complex query plans that spill results to tempdb. So far the best idea I have is to remove IOPS throttling and instead use the "Importance" of a workload group to defend the rest of the server's I/O capacity from these queries. Is this a good solution to the problem I'm trying to solve? What are the pros and cons of using Importance? Or is there a better way to achieve our goals? 

Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: 

A multi-billion-row fact table in our database has 10 measures stored as columns. The value ranges for some of these columns won't ever be above the +/-32K range of a . To save I/O, we're investigating whether it's practical to store these columns as instead of . But we're concerned about what problems might crop up from doing this, including: 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten?