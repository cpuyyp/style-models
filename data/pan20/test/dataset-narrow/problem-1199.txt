The Algorithm: Idiom of Modern Science The emotion universe The Computational Universe Great Ideas in Theoretical Computer Science The Computational Universe Computers Ltd. What They Really Can't Do The Unusual Effectiveness of Logic in Computer Science And Logic Begat Computer Science: When Giants Roamed the Earth 

The compactness theorem of propositional logic is a consequence of Tychonoff's theorem. Compactness for first order logic is usually proved differently. Compactness is an important tool in classic model theory. Stone's representation theorem for Boolean algebras relates models of propositional logic, Boolean algebras and certain topological spaces. Stone-type duality results have been derived for structures used in algebraic logic and programming language semantics. Nick Pippenger applied Stone's theorem to the Boolean algebra of regular languages and used topology to prove several facts about regular languages. See Jean-Eric Pin's comment for more recent work on topology in language theory. In formal methods, there are the notions of safety and liveness property. Every linear-time property can be expressed as the intersection of a safety and a liveness property. The proof uses elementary topology. Martín Escardó has developed algorithms and written programs to search infinite sets. I believe compactness is a key ingredient of that work. The work of Polish topologists (such as Kuratowski) gave us closure operators. Closure operators on lattices are a crucial part of the theory of abstract interpretation, which underlies static program analysis. Closure operators and other topological ideas are the basis of mathematical morphology. The notion of interior operators also from the Polish school is important in axiomatization of modal logics. A lot of computer science is based on graph-based structures. Some applications require richer notions of connectedness and flows than that provided by graphs and topology is the natural next step. This is my reading of van Glabbeek's higher-dimensional automata in concurrency theory and Eric Goubault's application of geometric topology to the semantics of concurrent programs. Possibly the application that receives the most press is the application of topology (initially algebraic, though more combinatorial presentations also exist) to characterise certain fault-tolerance scenarios in distributed computing. In addition to Herlihy and Shavit mentioned above, Borowsky and Gafni, and Saks and Zaharouglou also gave proosf for the first such breakthrough. The asynchronous computability framework produced more such results. Brouwer's fixed point theorem has given rise to several problems that we study. Most recently in the study of algorithmic game theory, the complexity class PPAD and the complexity class FixP of fixed point problems. The Borsuk-Ulam theorem has several applications to graphs and metric embeddings. These are covered in Jiří Matoušek's book. 

Proving Optimizations Correct using Parameterized Program Equivalence, Sudipta Kundu, Zachary Tatlock, and Sorin Lerner , PLDI 2009 A Formally Verified Compiler Back-end, Xavier Leroy, Journal of Automated Reasoning 2008. Translation validation for an optimizing compiler, George Necula, PLDI 2000 Translation Validation, A. Pnueli , M. Siegel , F. Singerman , TACAS 1998. 

Infinite Words: Automata, Semigroups, Logic and Games, Perrin and Pin, 2004 $\omega$-Languages, Staiger, 1997 Beyond $\omega$-Regular Languages, Bojanczyk, 2010 On syntactic congruences for ω—languages, Maler and Staiger, 1993 

Here is a rather different application from what you may have had in mind. Linear programming has many practical applications. There are many algorithms for linear programming and those based on George Dantzig's simplex method are among the most commonly implemented. An important parameter of simplex is called the pivoting rule. Victor Klee and George Minty provides a set of polytopes on which the pivoting rule suggested by Dantzig would require an exponential number of pivoting steps. Since then, examples demonstrating an exponential lower bound have been discovered for nearly every deterministic pivoting rule. Simplex can however use randomized pivoting rules. Gil Kalai in 1992 introduced a randomized pivoting rule and proved a sub-exponential upper bound for simplex with this rule. Also in 1992, Micha Sharir and Emo Welzl defined LP-type problems which include standard linear programming and with Jiří Matoušek also proposed randomized variants of simplex and proved subexponential upper bounds for this variant. Subexponential lower bounds were also discovered on LP-type problems, but till about 2010 there were no concrete examples of linear programs on which these lower bounds could be demonstrated. See these two posts on Gil Kalai's blog for another telling of this story, the connection to the Hirsch conjecture and links to the literature. What does any of this have to do with parity games? A couple of steps are required to set up a connection. An open problem in parity games research till about 2009 was to determine if certain policy iteration algorithms for solving parity games might have exponential behaviour. See the papers of Marcin Jurdziński for more on this. Oliver Friedmann, starting in 2009, exhibited examples of parity games on which certain policy iteration algorithms required exponential time. By exploiting a connection between parity games and certain LP-type problems he derived sub-exponential lower bounds for various pivoting rules for simplex. (Note however that one of the results, which concerned the Random Facet algorithm was shown by Oliver Friedmann, Thomas Hansen and Uri Zwick to be erroneous.) I hope you'll agree that's a pretty fascinating and convincing example of an application of parity games. There is a more direct answer to your question as well. Suppose one wants to design a discrete controller that regulates how some physical system (thermostat, chemical plant, etc.) behaves based on the state of the system and the state of the environment. The question of whether a controller exists to provide the guarantees a designer wants can be reduced to solving parity games. So you can think of a parity game in terms of systems, environments and controllers. Another setting is program analysis. Suppose you want to automatically determine if a program satisfies some correctness property in the modal $\mu$-calculus. Model checking is one approach to solving this problem and $\mu$-calculus model checking is deeply connected to parity games. 

Program analysis is an inherently applied field, so for many practitioners, the final word on the utility of an analysis is how it performs in practice. The approach you are suggesting would make explicit situations in which the analysis is sound. However, isolating these situations requires the analyst to know about the semantics of the programming language, the semantics of the static analyzer and further identify the gaps between the two. In my experience, several people interested in designing a practical analysis find the overhead of studying semantics (and soundness) excessive, and people who focus on semantics and soundness tend not to investigate heuristics required to make the analysis work on large families of practical examples. This may explain the dearth of material. That said, there are people who care about both practical utility and soundness, and even the specific problem you speak of (soundness with respect to points-to-analysis results). I think you will have more luck searching with the terms conditional soundness or relative soundness or parameterized soundness. 

(This started out as a comment and got way way way too long). You may enjoy William Thurston's article On Proof and Progress in Mathematics. 

Shannon's source and channel coding theorems. A mathematical definition that distinguished between the transmitted, receiver and medium and which ignored the semantics of the message was a big step. Entropy, in the context of data is a fantastically useful notion. And because information theory should be better known. 

The sets produced need not be the largest. This idea is extremely general and applies to problems that have little to do with program analysis. 

Which lines of code in the program are dead (will never be executed)? Which variables in the program have constant values? Which assertions in the program are violated? 

The Java Modelling Language can be used as a specification language to specify properties of code. The language alone does not solve the problem of verifying whether these properties are satisfied. A model checker allows you to check if a system with a transition system semantics satisfies a logical formula. The formula is typically written in a propositional temporal or modal logic though first-order and other extensions exist. Nonetheless, most model checkers used in practice do not support specification languages with quantifiers. I do not know of a model checker that supports a specification language as rich as JML. A certified library contains code that provides certain guarantees. Those guarantees may have been provided using a theorem prover, or a model checker, or by doing a manual proof. 

All the modal logics above are decidable and have the finite model property. Other logics with robust decidability properties are the guarded fragment of FO, the loosely guarded fragment and guarded fixed point logics. These logics were designed to transfer the essence of well behaved properties of modal logics to a classical logic setting. Guarded fixed point logic is decidable but does not have the finite model property. 

There is a very classic connection between logic and algebra, which goes back to the origin of modern logic and the work of George Boole. A formula in propositional logic can be interpreted as an element of a Boolean algebra. The logical constants true and false become the algebraic notions of the top and bottom element of a lattice. The logical operations of conjunction, disjunction and negation will become the algebraic operations of meet, join and complementation in the Boolean algebra. This connection is less emphasised in modern treatments of logic, but it is particularly interesting in the context of your question. Algebra allows us to move away from many problem specific details and find generalisations of a problem that will apply to many different situations. In the specific case of SAT, the algebraic question one may ask is what happens when we interpret formulae in more general lattices than Boolean algebras. On the logical side, you can generalise the satisfiability problem from propositional logic to intuitionistic logic. More generally, you can generalise the propositional satisfiability problem to that of determining if a formula, when interpreted over a bounded lattice (one with top and botto), defines the bottom element of the lattice. This generalisation allows you to treat problems in program analysis as satisfiability problems. Another generalisation is to quantifier-free first-order logic where you get the question of Satisfiability Modulo a Theory. Meaning, in addition to having Boolean variables, you also have first-order variables and function symbols and you want to know if a formula is satisfiable. At this point you can ask questions about formulae in arithmetic, theories of strings, or arrays, etc. So we get a strict and very useful generalisation of SAT which has lots of applications in systems, computer security, programming languages, program verification, planning, artificial intelligence, etc. 

[Edit 21 July 2011: I edited the question to ask for more examples] This question is asking for documented discussion of or more examples of a heuristic observation. Some mathematical problems that admit efficient algorithms appear to be convex in nature. I'm thinking of linear and semi-definite programs and various combinatorial problems that reduce to these. First, are there other families of problems that admit efficient algorithms for the convex/conjunctive case? (I would be particularly grateful for examples of decision procedures for logical theories) Second, I would appreciate pointers to articles or sections of articles that discuss an opinion such as "lurking under a lot of efficient algorithms is a convex structure." [Edit, 21 July 2011: Added the following.] I would like to add some clarifications. I'm sorry I didn't include them earlier. I am interested in logical decision problems. It appears to me that efficient decision procedures exist for the conjunctive fragment of several logical problems. Here are two examples. Efficient solvers for quantifier-free first order theories (such as SMT solvers for equality, equality with uninterpreted functions, difference arithmetic, etc.) typically have an efficient solver for the conjunctive fragment and use various techniques to cope with disjunction and negation. In static analysis of programs, the commonly used (and efficient) abstractions are based on integer intervals, affine equalities, octagons or polyhedra. In predicate-based abstraction and program verification, there is something called the Cartesian abstraction, which is intuitively about having conjunctions of predicates rather than arbitrary Boolean combinations. All these cases appear to me to be about gaining efficiency by exploiting the conjunctive fragment of the problem. The conjunctive fragment of the first order theory of linear, real arithmetic can express convex polyhedra. This is why I originally asked about convex programming. I am interested to know of other problems or examples where efficient solutions (in the theoretical or practical sense) are based on a convex or conjunctive sub-problem. If there is another general condition (Suresh mentioned sub-modularity) please mention it and problems whose solutions exploit that condition. 

SAFECode: Enforcing Alias Analysis for Weakly Typed Languages, Dinakar Dhurjati, Sumant Kowshik, and Vikram Adve, PLDI 2006 Pointer Analysis, Conditional Soundness, and Proving the Absence of Errors, Christopher L. Conway, Dennis Dams, Kedar S. Namjoshi, Clark Barrett, SAS 2008 A posteriori soundness for non-deterministic abstract interpretations, Matthew Might and Panagiotis Manolios, VMCAI 2009. 

We can define a lattice equationally as a set $(L, \sqcap, \sqcup)$ equipped with a meet and a join operation. We can then derive the partial order by defining $a \sqsubseteq b$ to hold whenever $a \sqcap b = a$. An alternative is to define a lattice as a partially ordered set $(L, \sqsubseteq)$ satisfying that every pair of elements in $L$ has a unique greatest lower bound and least upper bound. We can then derive the meet and join operations from the partial order. 

I cannot see the article and have to guess. The text makes clear that they are using the symbol $\supset$ as notation for implication. So you could read it as 

The first two papers are directly related to your question, and the third is just a slightly different approach to soundness, which may interest you in this context. The soundness/utility trade-off is an active concern for the problem of call graph construction. In the presence of reflection and dynamic class loading, call graph construction is a non-trivial problem and it is difficult to achieve sound and useful results.