If the hosting provider for some reason refuse to route a prefix to your physical machine, then you can instead bridge between virtual and physical interfaces. This has multiple drawbacks: 

The transport layer (TCP/UDP) does not decide which route a packet takes. And the sending host only decides where to send the packet for the first hop. Often there is only one option for the first hop, in which case the sending host doesn't get to decide anything. However if Equal-cost multi-path (ECMP) routing is being used, the sender can try to influence the path a packet will take by varying the fields inspected by ECMP. Usually that means retry the connection using a different source port number. If you are using Multipath TCP (which must be supported by both endpoints before you can use it) different underlying combinations of IP and port number will be attempted which can give you higher throughput and better resilience to failing network links. I know of no way to achieve the same with UDP. Protocols on top of UDP can build their own ways to influence routing decisions. For example if a DNS request receives no answer a client could retry with a different source port number which may cause the packet to take a different route. There is also the possibility for ECMP to include flow label as discussed in RFC 6438. Using that it could in principle be possible for the sender to change routing decisions without the need to change IP or port number. However changing flow label for an existing connection could potentially cause breakage in other ways. 

I want to make sure I'm understanding how this works. Specifically the term "Sending Port ID" has me very confused. Consider this diagram, I want to make sure I have root port selection figured out: 

The switch on top is the root and all settings are at the defaults. Both of SW2's ports connected to SW3 are designed because it has the lowest path cost to the root (its the only path to the root). Here is what I'm unsure of: SW3's 0/4 is the Root Port because the Sending Port ID of 0/1 wins over 0/2 Is this correct? The Port IDs of SW3 have no bearing in the selection it is only the Sending Port ID's of SW2 that matter right? 

Those numbers are assuming the provider only hand out prefixes on nibble boundaries and that you want a /64 per VM. The overlap between the ranges is because I allowed for an HD-ratio anywhere between 80% and 95%. The reasons I would recommend getting a prefix routed to the physical machine and subdivide that into a link prefix per VM are twofold. 

Though it is possible to allocate addresses to virtual machines from the link prefix connecting the physical machine to a router, that is not the approach I would recommend. Rather I would recommend that you get the hosting provider to route a shorter prefix to your physical machine. The prefix length you need depend on the number of VMs: 

I'm going through the great CBT nuggets series for the CCNA. In the video on WAN protocols Jeremy sets up a PPP connection between two routers using CHAP authentication. On each router he adds a local user with the name of the adjacent router's hostname, runs ppp authentication chap on each router's interface and it just works. What is the significance of creating a local user with the hostname of the connected router? Does something special happen when you create a local user with the hostname of a router? For example: I have R1 and R2 connected with a serial link using PPP encapsulation. I enable ppp chap authentication on R1's connected interface. I add a local user on R1 named R2 and give it a password. I then add a local user on R2 name R1 and give it the same password. Now one way authentication is in effect and it starts working, but I don't understand how. I'm creating 2 different user accounts with the same passwords. During ppp authentication I guess the routers just authentication with each other's hostnames? I tried creating a local user on each router with the name "joe" and gave it the same password, but authentication did not work. 

According to the current RFC, window scaling cannot be negotiated dynamically. The RFC explicitly states: 

Linux will choose a new local port number for each connection you open. To see what range of port numbers your kernel is configured to use you can type: 

Here means no reverse DNS is performed on the IP addresses. means I want to know about TCP connections. means I want to know which PID each connection is associated with (this information will be incomplete if I don't have privileges to see the information or if the socket is open by more than one PID). And means to use wide format (without that argument the IP addresses in the output may be truncated). 

In the past I spun up JunOS Olive in my home lab to learn JunOS. But it looks like Juniper has something called vMX. Is this comparable to vMX? Obviously its not free, but for learning purposes (learn juniper routers and switches) would vMX be the same or even better than Olive? 

I looked again and found I was using the wrong ports. The switches were configured with a GT96100-FE (2 ports) and a NM-16ESW module. I was using the GT9600 ports (fa0/0 -fa0/1) which don't even support switchport mode commands. I plugged everything into the NM module and it worked as expected. Sorry my bad! 

Want to be sure I'm approaching this correctly. I want to have a list of allowed MACs that can access any switch company wide (these devices can plug into any switch and be allowed access). To do this I would have a list of the MAC addresses and then programmatically apply this list to each switch's switch ports I have enabled for port security. Is there a different or more efficient way of going about this? 

If you announce the two different /48 the traffic will be routed across the internet to the right data center, which keeps things simpler for you. If on the other hand you announce just the /47 in both locations you have to get the traffic to the right data center. This may be desirable if you have a private connection between the data centers that you find to be more reliable than the public internet. Doing both of the above will serve as a sort of failover. Usually the traffic will go straight to the correct data center. But your private connection will be there as backup. However if other networks think you are sending them too many announcements they may decide to ignore your /48s and use just the /47, and your private connection will see some more traffic. If you don't have a private connection between the data centers the best choice will most likely be to advertise the two /48 and not advertise an aggregated /47. All of the above applies to IPv4 as well, just with different prefix lengths. What to do if you can't get more IPv4 addresses If you go ahead and advertise a /25 from each data center there is a significant risk the advertisements will just be ignored. Even if it works today there is a risk it will stop working in the future, so you will need a different plan. If you don't have a private connection between the two data centers there is the possibility to use an IPv4 over IPv6 tunnel between the two data centers as a private connection. The obvious drawback of the tunnel approach is that the tunnel is not going to be more reliable than the internet connection between the two data centers. And avoiding using the tunnel by only advertising the specific prefixes isn't an option because those specific prefixes would be too long. An option worth pursuing if you are using the same transit provider at both locations is to advertise both the aggregated /24 and the more specific /25s. What you would need from the transit provider to advertise to the world is the /24. The two /25s you'd only need the transit provider to accept and use within their own network in order for the traffic to be routed to the correct of your two data centers. Obviously before you do anything like that you'd have to discuss it with your transit provider to ensure that it is a configuration they are willing to support. Other caveats with a tunnel Another caveat in case of any tunnel is MTU issues. You need to ensure that you aren't doing something silly on your tunnel which would cause large packets to be silently dropped. Moreover you'd better configure your servers with a low enough MSS that it will work even if the people you are communicating with are silently dropping too big errors. For a setup like the one I describe setting the MSS to 1200 should be safe. If your setup is going to involve any sort of DSR load balancing it is worth keeping in mind that the load balancing may need a tunnel as well. In that case make sure your DSR load balancer is configured such that the tunneling it is doing will be instead of the tunneling to connect your data centers - not another layer of tunnel on top of it. Conclusion The simplest solution is to just get enough IP addresses. But alternatives exist if you absolutely need them.