Probably not the most elegant solution but perhaps the static configuration file can be replaced by dynamic content: You could put a FIFO at the log file path. Before you start MySQL you would have to start a mini daemon which writes to that FIFO in an endless loop. Race conditions should not be a problem here. The daemon would write two blocks of data to the FIFO, first a dummy line like # environment aware config file following in order to open the FIFO. As soon as this first write returns it would get the PID of the reading process (e.g. via fuser), get the variables which occur in the file from /proc/${PID}/environ, create commands for e.g. sed to replace the references and write the sed output to the FIFO. 

This is executed so fast that the connection didn't break (not even a LAN connection). Even safer would be to do this within screen (which I did first but which turned out not to be necessary). The safest solution would be (within screen, of course) to drop everything in the firewall during these commands. So insert after the first line (before ip addr del) and add at the end (after ip link set) 

There is one more extreme level of process control than (re)nice: chrt. You may set the process to SCHED_IDLE. In combination with ionice IDLE that should do the job. There is a really device mapper target which unfortunately didn't make it into the mainline kernel yet which gives you even more control: ioband Another option would be: Put this process into a VM. Direct VFS access is possible in KVM and you can precisely define how much I/O the VM gets. 

Whether it is possible to restore the RAID depends on the controller which I don't know anything about. Have a look at the manual and contact the manufacturer's support. If that does not lead to any good result the next question arises: Can the data be accessed. That should be possible in general without too much pain as there is probably no metadata in between on the disks. So you "just" need to find out where the data starts on the disks and how it is distributed. With this knowledge you can set up a DM device (talking Linux here) which points for every sector to the respective disk (and sector). This will not be possible for the whole disk at once as DM is probably not designed for having millions of sector definitions per device. And even a million of them just get us: half a gigabyte. So if this turns out to work in principle then you have to set up a DM device of reasonable size (I have no idea where the limit is, this may mainly depend on RAM or perhaps internal DM limits), copy that virtual device's content to a completely seperate disk, remove the virtual device and recreate it with a bigger offset (shifted by just the size of the DM device). This should get you your data back. But for obvious reasons this is gonna take some time. 

I think that is the expected behaviour - at least if both processes have an I/O share near 100%. There should be a small difference but 100% is the limit and if you are already at 96% then getting just half of the I/O results in 98%. No big deal. A serious difference should be discernible in the absolute transfer values. This difference may depend on the CPU priority, too. I/O prio idle doesn't mean that the system is not affected at all. If non-idle processes do not consume the whole available I/O then the prio idle process gets I/O bandwidth, too. Thus it may happen very often that a non-idle prio application demands I/O and does get the next slot but because there is an IOP just being executed for the idle prio process the lantency increases. The less I/O a process causes the bigger should be the idle prio process's impact on it. Thus it might help to reduce the CPU priority of the cron process (maybe even making it SCHED_IDLE). I have no idea about the swap problem though. 

Your problem is on the client, not on the server. The system on which sshd is running doesn't care about ssh-add/ssh-agent. It just checks whether the client system is capable of making the authorizing digital signature. Whether this signature is made by ssh, by ssh-agent or even by a smartcard does not make a difference. If you do not use one of the standard files (but e.g. ~/.ssh/id_rsa_newbox) then you have to tell either ssh (ssh-add ~/.ssh/id_rsa_newbox`) to use this non-standard file. 

You should start with partitioning the new server (including mkfs on the volumes for the main system) and installing a service Linux. Then you should boot the old system from CD/DVD and mount the partitions you want to copy. You need a working SSH connection from the CD boot on the old system to the new one. On the new system you have to mount the target volumes. Then you can copy the data, e.g. using tar: cd /path/to/source_volume_1 tar -czf - --one-file-system . | ssh -e none root@newserver 'tar -xf - -C /path/to/target_volume_1' 

Your description must be incomplete. Rules in INPUT and FORWARD will not make packets for 77.88.55.66 go to 10.0.1.1 instead. That is possible with the target in the chain of the table only. Obviously there is such a rule. For successful redirection you don't need rules in INPUT anyway as those packets are not to be received by the redirecting system. The problem with DNAT without SNAT (like in your case) is that a system usually does not care about it over which interface it has been reached when routing the answer. Your VPS-A sees a connection opening from 45.248.82.171 and if it is willing to communicate with that client then it sends a response to â€“ yes, 45.248.82.171. The client has sent a SYN packet to 77.88.55.66 and receives a SYN ACK from 66.55.44.33 and obviously just thinks WTF? Why from 66.55.44.33? Because the reply is not sent over the tunnel because the routing configuration of VPS-A tells it not to send packets to this destination over the tunnel. If they went back through the tunnel then VPS-B would rewrite the source address to its own and everything would be alright. So you either have to use iptables to mark the connections which come over the tunnel (why over the tunnel anyway?), rewrite the connection mark to a packet mark and use advanced routing with this packet mark for having those packets sent over the tunnel. Or you do both sides of NAT, make SNAT before sending them from VPS-B through the tunnel and get them back from VPS-A the easy way. Disadvantage: The webserver logs show the "wrong" IP address (always 10.0.2.1). BTW: 10.0.2.1 and 10.0.1.1 are strange end point addresses of the same VPN, aren't they? 

This may be a problem with your system, with the gateway, or with the connection itself. Can you reach other systems in that subnet? If they are reachable while the gateway is not this is a hint that something gets reloaded on the gateway (due to firewall / tc updates or whatever). Maybe reconfigurations of the switch (VLAN e.g.) can cause that, too, but then the connectivity to all systems should be affected. 

Edit 1 (comment answering) It is possible to make the mail server see the original address (this is trivial, you just have to leave out the SNAT rule). The challenge is to get the reply packets back to the vserver. You need advanced routing for this. And I guess you also need the Netfilter connection mark. You mark all new connections from the vserver, copy the connection mark to the packet mark and use the packet mark for the routing decision. You need define an additional routing table in /etc/iproute2/rt_tables. You could name it to_vserver. The iptables block for each port, the others just once. 

This perfectly makes sense for certain hardware: fast CPU, slow disk (HDD not SSD), just one disk. The data has to be read and written. The amount of written data is the same in both cases but reading a compressed file means that less data has to be read. Furthermore it is usually much faster to read a single big file than to read a directory. This effect is bigger if there are many small files. You can reduce it by reading the directory structure into the cache so that the disk does not have to jump between the inodes and the data blocks: 

Can you check (with tcpdump) what happens on the webserver? I assume that such a rule is missing (assuming eth1 is your internal interface): 

If you have read access to a directory then you can see the entry names. The other metadata (type, size, owner, permissions and so on) is readable only to those users who have the execute right for the directory. If you had executed as root then you would have received a normal directory listing. In other words: The file system has not lost track of anything. It just denied you this information. 

may be useful. Are the users configured on the new system? What does "to no avail" mean? Error message? Can you change any file's owner? 

There is obviously an IP address mismatch on both tun0. This is a point to point connection. No need to broadcast and make layer 2 address resolution. Everything that leaves this interface reaches its target. There are only two addresses involved. On both systems one must be the local tun0 address and the other one that of the peer. You can hardly have a running VPN connection if the addresses don't match. So if the server output is correct then the client output must be 

You may use tcpdump (-X) / wireshark to have a look at what Outlook is doing (not the packet metadata but the packet contents). 

that connections to local addresses of non-loopback interfaces are handled by the loopback interface. So no, there would be no performance difference. 

It's not one vnet for each guest but one vnet for each network adapter in a VM. You can configure the vnet numbers but usually they are not relevant. The vnet interfaces are bridged to either a physical or a logical (host-only) interface.