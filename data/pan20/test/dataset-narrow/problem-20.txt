Its not actually possible to do. Because that paper you print on needs to have properties not avalialbe. The color on the edge are very pure and can not be mixed. I dont also see any way they could be reflected either. Also the xy diagram is not the full color space just a slice of it. 

Image 2: 2 point perspective note how the lines in prespective direction no longer are parallel Viewport transformation allows you to pan/rotate/scale the resulting projection. Maybe because you want a off center projection like in cameras with film offset, or you have a anisotropic medium for example. It can also be convenient for the end user to zoom into image without changing the perspective in the process. 

No! If you dont have this info and must have a extra frame, just assume the reference frame is a identity matrix*. The point of the extra reference frame is just to be able to move the object but if you have no info about how the object should relate to other objects it does not really matter operator can move it where they want. Everything is relative, if theres no relation then no problems. * choosing identity is like choosing a arbitrary constant to be 1, its numerically easy to work with. 

Orthographic projections are parallel projections. Each line that is originally parallel will be parallel after this transformation. The orthographic projection can be represented by a affine transformation. In contrast a perspective projection is not a parallel projection and originally parallel lines will no longer be parallel after this operation. Thus perspective projection can not be done by a affine transform. Why would you need orthographic projections? It is useful for several artistic and technical reasons. Orthographic projections are used in CAD drawings and other technical documentations. One of the primary reasons is to verify that your part actually fits in the space that has been reserved for it on a floor plan for example. Orthographic projections are often chosen so that the dimensions are easy to measure. In many cases this is just a convenient way to represent a problem in a different basis so that coordinates are easier to figure out. 

Image 1: Different sampling strategies. Images in RTL reading order samples along voxel grids, Uniforms sampling and jittered ray length sampling. Think of this as solving a ODE you sample the function uniformly and if your value changes too much you might do additional samples. This is probably the reason why you wouldn't really do collision to the voxels. 

For a polygon to be convex the outside angle of the polygon has to be more than or equal to 180 degrees. Now at intersection of 2 lines the outermost angle has to be less than 180 degrees for the lines to intersect between the endpoints. Now the answer to this question depends on how you define what is inside of the polygon. If you consider some a even odd filling rule then the answer is No such shape exists. But if you consider only the outermost curve to do filling then yes, its possible if you intersect at the endpoint of a line. But to me this sounds more like splitting hairs as that shape reduces to a simple polygon. 

Image 1: Ratio of the inbound angles We can check the situation visually by drawing a 2 point perspective grid. For this we need the center line between the vanishing points. 

Image 2: Same image displaced and interpreted linear (note I scaled the height a bit to fit to my default bb size), working as intended. Technically linear is better. A synthetic map has no benefit of compressing certain values on the expense of others. Linear is inherently better because the spread of your bump is better that way. Which is why you'd be wasting resources encoding it in a gamma corrected manner. Secondly a image does not need to specify what color space it is in. Its just that image editors will assume it is sRGB encoded. You have no reason to do same interpretation, no info means just: "Nobody bothered to write down what they were thinking". So maybe the point should be made you should tag it linear. But here is the rub... your computer does not come with a easily accessible linear profile. What about Photoshop and blur? This is where it gets tricky. Photoshop tries to do quite much work to ensure that you do not need to know anything about this stuff. And this is where you shoot yourself in the foot. Se Photoshop is not meant for texture making, its meant to edit images not random data. So since blur on a image is different linear and nonlinear it wont work its geared to wrong data. But you can do one of 2 things to fix this: 

Ambient light does not really exist, if we do not talk about cosmology. What we call ambient is just light form many secondary reflections. 

OK I think I know what your problem is, gimbal lock does not really lock anything from a mathematical point if view, only for certain operations. See euler suggested that euler angles can define all space angles. And that is true. Its just that euler does not say that the space is uniform. Highly mathematical people also say that the euler space has impossible solutions since some positions are overlapping, but in practice this is not really a problem. Now since you can reach any angle, and you can always unwind your angle you can reach any angle in the body. Provided that your ready to unravel the situation and start over for a computer program doing animation and a mathematician this seems reasonable. However, this is not what most people are doing. Instead if you try to do some physical things like calculating the distance between 2 positions, the energy needed to rotate, use a external force to move the system or make a delta move in the gimbals space then this non uniformity kicks in. Simply the space is not uniform the same way that a real space is at the poles the distance to a position next to you in real space can be surprisingly far away in Euler rotation space. So when somebody is euler locked it does not mean they can not move anywhere they just can not move in the direction they would want to move in that particular orientation of the gimbal, and close to it. Its a bit like living in the suburbs and wanting to reach your neighbors house (that is 100 m away) but being forced to go trough the center of the city (15 km away). Energetically or time wise highly wasteful. This is a problem in 3D graphics only if you choose to interpolate in Euler angles. But for a control engineer or a mechanical engineer this can be a nightmare, since you can suddenly have a discrepancy between what your mechanism and your control logic is saying. Now it is possible to construct the gimbal so that the pole does not really happen, just not with the orthogonal axes the space is still not uniform but it does not experience the extreme case. However it might just be easier to use some other formulation like quaternionions, or axis angle instead. (quaternionions aren't really practical if your building a real mechanism though, its possible but very convoluted) PS: I would draw an image, its just that drawing something the wraps perfectly upon itself and is a 3d field is a bit difficult to do. 

Yes, most formats work this way! For example JPEG can be decoded up to the point where the chunks are available: 

Using this, and assuming the inner piece is discarded we can craw the conclusion that there are five cases: 

Which results in: I'm using here the 140 css3 defined colors. Bigger and more comprehensive lists can be found online, if you want a smallerlist that would also work. Theres all kinds of possible things you could do, you could for example linearize the sRGB space before calculating distances to name. You could ask for 3 closest names and give double names if the names are halfway. 

Image 1: Projecting a geometry on a sphere. Converting these great circles into either Cartesian coordinates is quite well known math. Projecting them onto 2d representation by Mercator projection is well known math on account fo this being central to map making. Since overlap is not a problem you can just slap these triangles on top of each other and merge the vector results. Or use the polar coordinates for pixel graphics and even use z buffered overlap just like a normal camera. The only problematic point is the one where your sphere is generated (if its on a open face edge). You can simply project the point on center into its inverse normal direction on the projection sphere. Or if for example Mercator projection is desired, you can align the sphere with surface normal. then just project them to a point on the south pole. You can even interpolate the results of individual polygons positions to get any point on the triangle. And a analytic surface coverage for the triangle. I haven't implemented this, but Ive done it manually a few times* so i know its possible. Should be pretty easy to do as the maths involved aren't all that complicated only problem is solving which side of the problem to keep. 

Image 1: Image showing UV map of two overlapped triangles and sampled texture with same UV coords Here are the sources for those images, please do not overwrite the sources. 

There is a trick that many in fact let you sidestep the issue altogether*. The scene and the set of things to be raytraced does not have to be the same. Most ray tracers allow for objects to not participate in shadow casting, reflections, etc. In your case you could just take the outside walls of from shadow casting and therefore they would not self shadow allowing you to put lights outside the box with no problems. Anyway you could add a slight bias to the sampling and consider the objects slightly further than your calculation result gives, for shadow casting this avoids all kinds of problems. * Yes you have a problem you should fix that. But sometimes quick and done is all thats needed 

This is more of a long comment Yes you can do this. The problem lies in how you measure the surface. I tried to do this manually by progressively extruding tubes along the edges, and finding their intersection with the surface. It would work but is a bit hard to compute. Obviously you may want to take more steps than i did for a more accurate solution. 

You can simplify this a bit with trigonometric identities. Or if you use rational B-splines you can skip the calculation of C Note: that this is only one possible formulation 

Why do we not calibrate all devices to sRGB in the factory? The assumption is that every monitor is sRGB anyway. So why not calibrate them to that? That would make people see same color as intended on a wide range of devices. 

Its called a geometric constraints solver (a good primer on subject). You can find a open source solver as part of Open Cascade but its a bit convoluted to get going. A simpler solution for just solving, but also 3D solver capable, is geosolver. Making your own (algebraic solution being easiest to write) is also not that hard, just a bit of work to make it automatically write the equations (and be efficient at solving). Take the following drawing as a example (made and solved with Mathematica): 

There is no general algorithm for packing problems. Only some of the special cases have known, and optimal, solutions. If you are packing one shape then finding a reasonable solution is possible. Like the known cases of hexagonal packing etc. However, if you have multiple diffenently sized objects then easy just flew out of the door. Some heurestics have been developed, since there is a very big demand for such algorithms in manufacturing. But they are not usually very fast (performant?) or simple to implement. Many of them use a genetic algorithm. A free one that you may test can be found atsvgnest.com but faster ones and ones working on shaking and collision also exist. Anyway your question is sufficiently vague, so i dont actually know if you want to do packing. I mean it is perfectly possible you mean tiling, which is pretty trivial. 

Image 1: For each wall generate a shadow volume extrusion. Quick and extremely dirty sample implementation of shadow volumes here: 

Also it would be nice to see, different surface thicknesses, sharp corners, one way curved surfaces and doublecurvature surfaces. A neutral but nonuniform environment would be nice too. Object should have a reasonable uv map. Apparent scale might be needed in some projects. 

There is only one major thing that I can think of in nonstereographic view that is affected by size and that is: 

Yes, theoretically, for the sphere no for other things in image. If it is possible to augment the situation then it is possible to measure anything you can track much more reliably. First, knowing the scale of a object that you can successfully track is a requirement to getting a scale for the image as the cameras lost all scaling info. But this requirement alone is not enough. In practice you do not know the other settings of a camera (unless calibrated with stereo geometry first), all kinds of problems arise when you do real camera tracking. Such as real cameras lens not being ideal. By tracking more than 2 features on the scene you can automatically estimate the actual focal distances and camera distortions. More importantly you can get an error estimate on your data. One way is to have two photos from different angles in a scene with enough depth variation (for a stationary object this is quite easy). You can then use stereo geometry to solve the depth many such tracker apps exist as part special effects packages and making your own is not entirely out of the question (fun link, fundamental matrix song). The second way is to have a known plane, the plane can be solved by knowing 4 points and their relative locations. In reality it is better use more points for error and lens correction estimates. A third way is to use structured light (a cloud of laser light points, or a line for example) to triangulate the point distances. Or you could use a light field camera. 

Image 1: Visible specturm on the edge, interpolations on surface. Now magentas are a bit peculiar. They only exist in our brains interpretation, thus they can not be measured. Magentas are sort of virtual colors that make the color circle full. This means that when you move from absence of blue towards absence of green your line kinks since the graph has no well defined direction for magenta's. This makes a 4th corner where none should be located. Magentas are thus a bit of virtual colors that only exist in our brain. Not in physics, they are 2 spectra interleaved. There are off course many such metamers. 

Smooth in this case just makes the surface normals at vertices point the same way, when interpolated it looks smooth. Meshsmooth would add vertices. 1) how is the smoothing possible without increasing the detailing of the mesh geometry? Human eyes cant actually see curvature except on the edges of objects. All they can do is approximate the smoothness and process the gradient slope. So having a continuous field does give a air of smoothness. The eye however is extremely sensitive to abrupt changes in color, and interprets that as a hard crease. By interpolating the vertex normals your surface will get the appearance of smooth flowing. Since this normal is used to calculate the final reflected color you get a smooth color field. 

Image 1: Depiction of local axes. Now the first one ($A$) is a sweep of an oval. Which is technically the most challenging of the four. In essence You offset edges directionally and cap them with a fixed directional oval. Although this has all kinds of interesting edge cases. A lot of mathematical lore is known about sweeping shapes as this is somewhat hard to do in 3D and doing so with an arbitrary shape is useful for the cad crowd. At it simplest its just sampling many times, which allows for change in shape along line, but quite sophisticated approaches are available. 

Image 1: A suction cup stamp. Now if you do something like this then your image is linear despite what everybody tells you. You have decided to discard the image rules that says otherwise. And that is why the image is linear, the author chooses for it to be linear so its easier to work with. 

I'll cover 1 and 2. First the Internal edge as its simple you want it to be smooth so the tangent length and direction in both patches must match. If that's not true then its not sufficiently continuous. You could simply take that this is the furthest point from the surrounding patches control points and due to mean value theorem it should have a tangent direction perpendicular to these (Line A-C is parallel with D-E). The tangent length should be about 1/3 of the length of the distance to the adjoining vertices (A-B or A-C, in this case the shorter of the spans. It could be longer or average. though that could pose problems). Obviously if you have more spans just repeat for one span forward and after for more internal spans if needed. For free edges find the point on the perpendicular line that is as far from the point as half distance between the points (length B-D is equal to length (A-B)/2 and length B-E is equal to length (B-D)/2). The free tangent points towards this point. The length of the tangent is as long as one third of corresponding point difference (length (A-B)/3 and (B-C)/3). Obviously the distances and points could be arbitrary but these are somewhat natural. Repeat for next point sequence in line and next and next. You end up with 2 spans that means 2 times 16 points. There are other ways but this is relatively easy to convert into a visual form without going into deeper maths. But this is pretty well known math you can find many ways to form these by searching in literature.