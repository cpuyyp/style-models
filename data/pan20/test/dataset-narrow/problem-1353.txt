I don't do a frustum cull with the main camera, but the shadow camera alone. I calculate the AABB (axis aligned bounding box) of the shadow camera, then do the culling with that. A directional shadow camera is a box in itself (orthographic projection matrix projects to a side of the box). After the box is rotated (for different light direction), the AABB can still be computed easily (but it will be a little bigger than originally, so it is a bit wasteful). I compute it like this: Rotate every corner of the bounding box, then find the new min and max corner from the bunch of new coordinates. An AABB is essentially just a pair of min and max points. You can check AABB-AABB intersections after that which should be quite fast. 

After you have your Color array, you can do whatever you want with it, for example you could change certain colored pixels to Color.White. Then you set this data to the texture. You can use Texture2D.SetData() for this: 

First, from the picture it seems that in fact your light is not a "directional" light but a spotlight because you are using perspective projection, not an orthographic one (If not, then it is just the picture :) ). It is just a little thing, it should work more-or less the same. The important thing is, you don't render the positions from the light's point of view. From the light's point of view, you have the shadow map. You retrieve the position from the world position of the pixel (so you have to keep a texture of world positions, or retrieve the position from the main camera's depth buffer), then transform it by the light's viewProjection matrix (sunMVP in your code) in the fragment shader, then by taking the .z value of that transformed position and dividing it by .w, you get the real Depth of that pixel from the light's point of view, I call it the realDistance. The value which you sample from your shadow map will be the occluderDistance. If occluderDistance is less than realDistance, then it is occluded. So in the pixel shader you have to do something like this: 

I think the problem is that your pixel shader returns a structure, so each element of that structure need to have an SV_TARGET semantic and not the whole struct, so: 

Maybe you can save some time if you don't bind a vertex buffer and use SV_VERTEXID (which is created automatically by the IA stage) to build your quad in the vertex shader, something like: 

If I am running DirectX 11 in debug mode (D3D11_CREATE_DEVICE_DEBUG), I am constantly getting these warnings: 

If you are drawing directly to the backbuffer you already have a depth texture bound to it. In your drawing code you need to clear the depth too (well it's optional but it should be cleared to avoid some artifacts, also this doesn't cost you much to do so) while clearing the backbuffer (or rendertarget) color: 

You problem is that you are trying to create a vertex buffer with invalid initial data because you have actually no vertices in your array. 

You can do this while procedurally generating content. Think about it like this: You procedurally generate like you said, but instead of storing the whole universe, you store a part of the world which you are in, say a sqare kilometer around you in the memory. When the generation is complete, you can throw a rock and store its position accordingly. When you are about to generate an other sqare kilometer of your universe, throw out the previous one. You can even save the position (or properties) of the rock you thrown, by generating a unique indentifier for your objects in your procedural functions. If the generated id of the rock is deterministic, you can load back its properties once you move back to that area. 

If you inspect this loop (line 748 in your code) then you see that it is not entered by the program because vertexCountpiyon variable is zero. So your vertex array is not filled and you are trying to read from it that's why you get an access violation error. (Please note that debugging your code questions is usually not warmly welcome on this site, this time you got lucky I guess :) ) 

First, set the primitive topology to TRIANGLESTRIP (ID3D11DeviceContext::IASetPrimitiveTopology), set the following shaders then call devicecontext->Draw(4,0). You don't even need vertex buffers for this because of the automatic system value of vertex id in the shader. 

A skybox is already simulating a sky infinitely far away from you. If you would increase the size, wouldn't you expect to be the same? Cube mapping works by sampling a special cube texture (consisting of six 2D textures, one for each face) by normalized 3d vector coordinates. If you get 3D coordinates from a cube scaled by 1000, and normalize those, you get the same coordinates as you would from a unit cube. 

Perspective shadow maps like that are usually not used to shadow the whole scene, but for a single lamp or flashlight. If you want to shadow the whole scene, You probably don't want to use perspestive shadow maps, but rather orthographic ones (I guess you want shadows from the sun?). When you are creating an othographic matrix (MSDN) you specify its dimensions, like a cube (because it will project everything in that cube). I guess it is easy from there to calculate its required size. It's even better for sunlight for an other reason, too, because shadows generated from that will look like they have been generated from a light source which is infinitely far away (or as far away as the sun). If you shadow a whole scene like that, probably one shadow map will not be sufficient (or efficient), so when you got it working with one, you should look into cascaded shadow maps then. 

So you could create multiple ContentManagers, for example one for the main menu, one for the game screen, and load the relevant resources with them. 

It is because I am setting a resource as a render target while it is currently bound to the PS. However, my rendering flow is set up such as it relies on the api to force the resource slot to null, and it works. Is there any reason I should be bothered with these warnings or I can rely on the api to enforce this every time? I only tested on 2 GPU-s: Intel HD 3000 and Nvidia GT525m. 

Where and are vector references coming from my graphics engine. I fill out the mesh with that information and seemingly I can create it without problem. After I step the physics world I connect my BULLET softbody nodes to my vertices in the graphics engine, then updating the vertex buffer. I get the information from the softbody nodes by their this way: 

I am just before implementing screen space ambient occlusion in my game, but first I wanted to try enabling it from NVidia control panel only to find out that it is greyed out so that I can not enable it. With this I could enable SSAO for some other games, but not every one. I know this technique requires the depth buffer and (optionally) a normal map texture to sample information from which I already have access to given I have a deferred renderer working. After that I actually thought to roll back to a previous version of my game which still uses forward rendering so the depth buffer is actually bound to the backbuffer which I render to from the get-go so that maybe the NVidia control panel would somehow make use of it. It was not working with forward rendering either. (I also tried FXAA in the control panel and that works - but it doesn't need any depth or normal texture) So my question is that how can I enable this function so that it would work by enabling it in the NVidia control panel? 

Unless you want View position to work with. I haven't checked the code for bugs if you want to use that. 

If you want to use it with an XNA effect (Basiceffect for example) then you just have to set it before drawing to that effect: 

You must call IASetVertexBuffers function once on the vertices, but call the Draw function twice. Also set the transformation matrix before each draw call and update the constant buffer (UpdateSubResource). You don't need to set your shaders or constant buffers twice. That is it if you want to render your vertices twice, if you want to use them twice for something else on the CPU you should duplicate them. The best would be if you didn't duplicate them and used them only on the GPU if you could get away with that. 

Please note that normally you will have to have texture coordinates specified in your model file to use textures this way, except if you are using projective texturing. 

The standard way of handling this is by using cascaded shadow maps. The idea is that you render multiple shadow maps. Objects closer to the camera should be rendered to a smaller shadow map near the camera, distant objects to a bigger shadow map. If the shadow maps are the same resolution, then you get fine details to shadows which are close to you because you get more shadow map pixels per world units, while farther away shadows will get much less detail, but you won't notice that because they are far from you. The trick is how you select the cascades so it will look good. A nice article which is more in-depth: MSDN 

Notice that you are launching a sound effect while the player is dead, not when the player died. The obvious solution would be to call your soundEffect.Play here: 

There is DirectXTex which has a lightweight DDS loader which can load mipmaps for you. You can integrate it very easily (just inlude some headers in your project and that's it) and call CreateDDSTextureFromFile function to load a texture from disk. There is an even more robust loader one which I haven't tried yet. 

Another option is to create a normal vertex buffer with flag and no cpu access flags and update the buffer by calling , but it will be slower. Like this: 

The problem: However I do not get correct vertex positions and neither correct normals. I am using a simple quad mesh with 4 vertices (the positions): { (-1,0,-1), (1,0,-1), (1,0,1), (-1,0,1) } But if I get values from the softbody's nodes after creation, but before stepping any physics I don't get back my original values from the nodes (the positions): { (-1,0,-1), (0,1,0), (-1,0,1), (0,1,0) } I even noticed the node vertices kind of resemble the originals, just "overlapped" somehow or not sure how can I describe if it is even the case, but it is like the second vector has an extra zero at the beginning and the whole mesh got screwed up. Could it be something is not right with my loading? Or do I miss something in the concept of soft body physics? I have studied the bullet soft body sample which is quite informative and they sure load their meshes like I do (the bunny mesh for instance), at least it seems like it. I don't have any idea and got stuck here for a day now. Without connecting to the softbody, my mesh remains correct. 

If you know which direction the characters are facing, you calculate the dot product of those directions. If the characters only move on the ground plane it is pretty staightforward to decide which side are you on. If the dot product is 0 then you face exactly the side of the opponent. Then you check for some interval against zero and decide if you are on the side or not (so that you can attack from the side even if you are not exactly on the side). If the dot product is positive then you are behind, if negative you are in front. Calculate the dot product by multiplying the facing vectors' of the characters components against each other then sum them up, like: 

I don't know Box2D specifically, but using a physics simulation for this effect would be overkill. They certainly didn't use any physics calculations in Mario. :) I would do this by linearly interpolating between the normal position and moved position. 

You need to rotate around the X axis to have "up-down rotation", Z axis if you want "roll rotation", and Y if "left-right" rotation. Rotate the "front" (or look at vector) and "up" vector around the desired axes. Make sure to rotate those vectors before translating the camera. If you applied your rotation and translation then recreate the view matrix with those values. 

When creating your vertex buffer, specify and in the buffer description (in Usage and CPUAccessFlags members). This will create you a dynamic vertex buffer, which you can update by -ping it, copying data to it and -ping at the end. Something like this: 

You might need to switch to , it depends on what your depth buffer looks like. It can be 0.0-1.0 (close-far), or 1.0-0.0 (close-far). Sidenote: A world position texture should look something like this: 

It's definetly free, but it runs only on windows, which is not. Visual studio is optional, but it is the preferred editor by many developers, especially in professional game development studios. You will also find most samples for it as a Visual studio solution. 

You should get the mesh's vertex positions to create a btTriangleIndexVertexArray which is required by the btBvhTriangleMeshShape's constructor. You should look into the samples (demos) provided in the Bullet library for more. Note that Bullet meshes can only be used as static colliders (terrain for instance), not for dynamic bodies. For that you should either use Convex Hulls and/or basic shapes to create bodies that resemble your mesh to some extent. Please see ConvexDecomposition demo for that. 

Earlier, I have been setting render states (shaders, shader resources, etc.) without checking if it is really necessary. Maybe they had been bound earlier and I could use them without setting them again. So now I am checking for currently set render state before setting it. If there is a mismatch, then I set it. Setting a pixel shader for example: 

You can achieve certain effects with shapes that would be more difficult with sprites. For example if you want a circle line or a pathway to be drawn in certain amount of time, or an image shattering to several pieces. These are that came to my mind at the moment, but obviously, you can think up anything. 

What I don't know is how would I mark a pixel in the stencil buffer with a specific value. For example I draw my scene and want to mark everything which is drawn with a specific material (this material could be looked up from a texture so ideally I should mark the pixel in the pixel shader), so that later when I do some post processing on my scene I would only do it on the marked pixels. I didn't find anything on the internet besides how to set up a stencil buffer and explaining the different stencil operations. I was expecting to find some System-Value semantics like SV_Depth to write to in the pixel shader (because the stencil buffer shares the same resource with the depth buffer in D3D11), but there is no such thing on MSDN. So how should I do this? If I am misunderstanding something please help me clear that up. 

I recently switched from Havok physics to Bullet because I wanted to have soft body collisions, but I don't have any budget to license the (superior?) Havok solution, I couldn't even try those functions out there so it is completely new to me. I want to create a soft body from an arbitrary mesh and for that I use the soft body helper: