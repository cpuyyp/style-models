I'm not sure what you already know, so I'll answer how I'd do it. If you do not understand things, or already know some things I say, please leave a comment. I would tackle it as a multi-class problem if your output is categorical, hence indeed only $10\%$, $25\%$ and so on. If you want to treat it as a regression problem, I'd say you need continuous deltas of popularity, so you need all kinds of values as the delta, and not only a fixed number of them. The training data should dictate how you approach the problem. Note that you can always reduce the regression to a multi-class problem (which asks for an instance which of the deltas it should assign) by bucketing to intervals. For argument's sake let's assume you have multi-class data. One document has one of the classes, i.e. $10\%$. You then go ahead and encode these classes into a binary vector, which probably even is a part of the library you're using. You'd then go ahead and split your data with the hold-out method to have some vlalidation examples set aside to see how your model actually works. You then go ahead and train some model, say a random forest. With the validation data you calculate some measure of error and see if it is enough. If not you go ahead and choose other models to train, other features to add and so on. I'd suggest to try use another approach than bag-of-words such as word2vec or GloVe. You can also add the sentiment classifiers values as feature columns. You could then compare the different features and see how it performs. Let me know if this helps you or if I misunderstood your question. 

The U-Matrix uses distances for visualization. It can be combined with the so called P-Matrix, which uses densities, to use the density of data points as well, and allows the combination of densities and distances. Based on the U-Matrix one can do classification or clustering as well. An R package is distributed by the research group of Prof. Ultsch. I wouldn't say that it's important in Machine Learning per se, but in the fields of Data Mining and KDD it is, at least academically. You can clearly see this if you look on the reception of this or similar work on semanticscholar or researchgate. Disclaimer: I studied with Prof. Ultsch, the head behind U-Matrix and U*-Matrix. NB: Sorry for the missing links to the papers, I haven't got enough reputation yet to post more than two links. Papers: 

One important and well known (in their respective research domains) usage of SOMs today is the usage of SOMs for data visualization and clustering. The U-Matrix uses a SOM with a lot of neurons in order to achieve emergent behavior. Based upon the SOM, a so called U-height is calculated for each neuron. For a neuron $n$, a weight $w_n$ of the neuron $n$, as well as a set $NN(n)$ of immediate neighbours of $n$ the U-height is calculated as: $$\sum_{m \in NN(n)} d(w_n - w_m)$$ $d$ is the same distance as used in the SOM training. This U-height is used to visualize the SOM and can be capable of catching quite interesting inherent structure. It is visualized by using a color scheme similar to topographical maps: 

I don't understand the purpose of imposing a condition that requires any cluster to contain at least some (let's say even 1) point from every dataset but at the same time find a solution that also allows that condition to be broken by outliers (I assume these are specific cluster outliers or data set outliers?). Have you considered implementing an overlapping cluster algorithm? You don't get the full distinctness of a traditional single membership but your might better fit your data to an algorithm that achieves the desired condition. It will probably require some exploration of the data and testing for cluster membership after the fact. 

Very difficult question for someone to answer as ultimately your examples and many questions contain certain constraints you won't get from your sensors. For example: Your first question is predicting "optimum" time to turn on AC. who defines optimum? Optimum for the classroom at full capcity, half capacity? Your second question looks like it one, requires a map of the classroom and where the sensors are, two you won't be able to predict 7 sensor values from 2 sensor values, I don't know why you would want to do this. Your third questions is a social sciences problem. Define "light needed to be comfortable". I imagine you would get 150 answers if you had 150 students. Your scenario lacks context/comparison. Let's say you had sensors outside the classroom for your various data points. Then you could do things like, can I predict the temp inside the room based on my other room and external data points. Finally, yes machine learning can be used with your wireless sensors. 

You need either an initial data set or a large set of constraints to "fake" a data set. if you have an initial data set you could make somewhat valid assumptions regarding traffic flow, purchasing behaviors and the likes. If not, you are going to need to research these different metrics from existing companies that are like what you want data for and then I assume put together a little program with those metrics as constraints and simulate your data. Once you have those constraints you could use simple averaging or weighted options for developing a constraint per variable. Make sure they are semi accurate. For example, if you were modeling company growth for a "really good idea" research growth trends for like companies and for companies you want to emulate and then scale BACK from there. You can also always purchase a dataset: $URL$ 

With the Voronoi cells that runDosrun brings, it is true that they are likely unbounded, so your new data that is "outside of the cluster" isn't actually outside the cluster. You should probably look at a different clustering algorithm if that is indeed what you are looking to do. If you are intent on using k-means, you could attempt to classify anomalous data into its own cluster. There are libraries out there that you can use to constrain the boundaries of the k-means during cluster analysis to the initial domain. Without knowing what environment you are trying to do this in or what the context of the data is, it would be really difficult to define "inside a cluster" with out you specifically choosing certain boundaries. 

I would start off with All Of Statistics by Larry Wasserman. This quickly gets you upto speed with statistics assuming you have some mathematical background. I think all it needs is introductory calculus and linear algebra. R is pretty straightforward to pick up and there are a number of resources you can use. The R Programming course on coursera is an excellent short course to get you familiarized with R. Besides that there are a number of books and tutorials on the subject. I would recommend you start working through All of Statistics and start playing around in R along with that book. If you are at a SaaS company, there are a number of data sciency roles and responsibilities that should be available. Most SaaS companies will have analytical tools to provide basic insight. Once you start learning statistics and data science, you will be able to identify the gaps in the pre baked tools and ways to improve them. I would also read The Field Guide to Data Science, it's a short book that will give you a high level idea of data science and its utility. 

Assuming the central node is at level 0 (root), this graph becomes a tree. Now it is easy reason about the different types of trees that can be formed and different types of social networks that can be modelled. In order for the graph to be a tree, communication between nodes has to be restricted, such that the tree structure is not violated (e.g. two nodes on the same level cannot communicate). I can't think of any social network where communication is naturally restricted in this way, but such a social network can be created artificially. Examples of such artificial social networks would commonly follow a chain-of-command structure. For example in a military setting, the official information flows would have a tree structure. 

If the data is unlimited, how would you have an epoch to begin with? For example, if you are analyzing tweets, you could never finish an epoch will all the tweets, since there will be an endless supply of new tweets. A much better approach will be to do some online or streaming learning. Would it make sense to create a subset by ignoring new incoming tweets or data in general? That really depends on your problem. For example if you have a datastream of tweets, or any other where new data is being produced in real time, you would miss out any trends or patterns that emerged after you sampled from the stream. Are those missed patterns relevant to your problem? They may or may not be. 

Can you elaborate what you mean by 'methodologies'? In the meantime, take a look at The Field Guide To Data Science by Booz Allen Hamilton. This guide talks about data science processes and frameworks. Data Science Design Patterns by Mosaic talks about, you guessed it, data science design patterns. This is quite useful to get a sense of common design patterns. They are also working on releasing a book on the same subject. Then there are several resources out there that will come up as results to more targeted searches, such as machine learning paradigms, recommender systems paradigms, etc. Data Science is a large and varied field, and you'll find many resources out there for each subsection of it. As far as I know, there isn't one book that covers it all.