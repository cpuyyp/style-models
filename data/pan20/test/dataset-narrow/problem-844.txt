I assume you're deploying settings using Group Policy? If so, I believe this page contains the information you're after: $URL$ Specifically, the setting: 

As TomTom says, the solution is to SPLIT the scope not MIRROR it. Please note that this is a very important distinction because for high availibity, you need enough IP addresses in each half of the scope to support all connecting clients. Server 2008 R2 makes this extra easy for you: 

Wow, that's a lot of questions. You need to get back to basics - get an unmanaged switch, stay away from your corporate LAN and build a Server 2008 R2 Domain Controller with DHCP and DNS and go from there. 

It's almost certainly a permissions issue, do your users have "Modify" / "Delete" rights? Also, Process Monitor is great for issues like this as it should let you see what process is handling the file and with a bit of luck, why it can't remove it. It could also be related to your Share Permissions, if applicable. Remember, these are different and mutually exclusive to your other permissions. Your best way forward is to get yourself a test account, replicate the issue, and then try giving that account full control over the directory. If the issue persists, then you can probably rule out permissions and look elsewhere. 

Add the specified machines to an Active Directory Security Group and add the Group to the GPO with a "Deny" for "Apply Policy" (Don't fall for doing a full deny as it will stop the GPO name from enumerating, making troubleshooting difficult). Then, add the machines to that Group as required. 

Memory over commitment means allowing running virtual machines to have more assigned RAM (in total) than the physical host actually has. So, add up the RAM in your VM's that you want to run at the same time, then allow some room for the hypervisor and if the total is less than the physical RAM you're not over committing. 

Remove your snapshots and then expand the disk. You should read up on how snapshots work, because that will explain why expanding the underlying VHD will be bad news for the delta disks. 

One Domain and Forest should be fine, with separate organisations assets split into separate Organisational Units. You may choose to separate domain controllers across physical locations, depending on your WAN capabilities. That said, regardless, you simply must have 2 Domain Controllers at least in your Domain, having no redundancy for your AD is suicide. 

Do you have another computer on the same network? If so, can that computer access your webserver? That's your first step, then worry about being visible to the outside world. 

Where ## is the relevant interface number from the top of "Route Print" : Assigning a metric 1 ensures that this will should always be the default route for this IP. 

I'm working on a site where every user is configured to point to a proxy.pac file, hosted internally. My question is, how should we expect these clients to behave when the pac file is not contactable? Is Internet Explorer meant to timeout and allow the connection? Is there anything we can configure to improve the experience for users offsite? My testing seems to give inconsistent results, which is why I'm reaching out for community experience. 

I realise that it's unlikely anyone here has a definite "Check this box to resolve this" answer, but I'm struggling with this issue and I'm hoping that someone with more knowledge than me can give me some more insight or perhaps some terms and phrases to look for. I'm investigating an odd issue with some embedded devices - long story short is that some of these devices (Possibly running a specific firmware version) aren't available through the NetScaler SSL VPN connection. That's by the by, really, but during troubleshooting I noticed something that I'd like to know more about. For testing, I set up a quick IIS server so that I could run WireShark and I get a subtly different traffic flow depending on whether I go through the VPN or not (Same machine, using Putty in RAW mode). So, from WireShark on my IIS server (.117 below): Without the VPN, from the Client I get the following traffic: 

You're not trying to do anything too special, and it's definitely possible. The way in which this is generally achieved is as follows: You configure a subdomain which is designed for receiving visitors redirected from an external domain. For example, . The page on the end of this will look at the headers and query the database to decide where to redirect to. If a user attempt to access directly, the page should return a 404, another error or redirect to your home page. The user configures their domain (or subdomain, whatever) with a new record of And finally, to prevent account hijacking, your service will normally generate some kind of verification code which the user adds as a record. This is exactly how Google do it for Google Apps. 

You can't, unless your XenApp servers also have internet facing IP's. Once the web interface has dealt with the authentication it doesn't do any tunnelling, so the client has to be able to connect directly to the XenApp server. The alternative is the Citrix Access Gateway or the Citrix Secure Gateway. The latter sits on top of the Web Interface and is designed to do exactly what you require. 

Firstly, ensure that you've installed the 32 bit drivers along with the 64 bit drivers on your R2 print server. (To install x86 drivers, go to the printer properties, click "Sharing" and then "Additional Drivers"). Additionally,I recommend using a combination of Group Policy Preferences and Group Policy to configure client network printers and "Point and print" restrictions. 

In fact, I can almost guarantee that any mid -> high range off the shelf router will have higher uptime/availability than any roll it your own system. Also, you'd never want to multi-role your router, even if it was a PC with a conventional operating system. So, that point is fairly moot anyway. Finally, the Dlink DFL-860E is far more than a router. I think you'd struggle to build a reliable PC that really does have all its features for the same cost. You'd struggle even more if you factor your time as a cost. 

No, in short. A lot of the marketing towards it is geared around the fact that it will be a decentralised, outsourced, managed system. Locally hosting would pretty much defeat the point. 

Your other DNS servers will (indirectly) use your new forwarder immediately, but only your DNS records which aren't cached or have expired. From your current DNS server, you can't. You could probably go to your other servers and flush their DNS cache, though. 

It would be trivial to capture this information using a variety of monitoring software or packet analysis. That said, I'm voting to close this question as off topic because it doesn't feel like a professional question. 

If you're buying the domain purely to reserve the name (And that's fine and recommended because you shouldn't be using AD Domain Names that you don't control) then it simply doesn't matter. It doesn't need to resolve to anything externally at all - you're just ensuring that the domain belongs to you and that you can buy certificates for it in future etc. I'd advise against having them point to your local internal Active Directory DNS, though. The only time this stuff really matters is when deciding what to do with regards to DNS from inside and outside. Inside, has to always resolve to your Domain Controllers using Round Robin DNS - so, essentially, you're cutting off the ability to access internally. In your case, that may be fine, but you should be aware of it because it's not really something you can fix - just work around horribly at best. A better approach may be to simply use a subdomain of a company domain you already own. So if you have already (Or plan to expand that into a website), simply use . Again, there's no need to actually DO anything from the outside on this - the point is simply that you control . Internally, can still use external DNS servers (Unless you want Split DNS) without breaking anything.