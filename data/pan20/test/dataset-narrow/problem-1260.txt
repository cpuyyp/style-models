My game generates small "minimaps" for each room you go to, which are rendered onto render targets then stored in textures and displayed on the world map. When the game is saved, each new minimap is saved on the hard drive. If the game is in full screen and the user minimizes the game, the minimaps that were generated, not loaded off the hard drive, will become blank. Since I don't know exactly what render targets are doing in the background, I have no idea how to prevent this, or restore the textures if they are lost. How can I prevent/workaround this problem? EDIT: I did do 

Where and represent the position of the target. You'll also need to keep track of the distance travelled, so just make a variable like . Then, whenever you update the position of the projectile, you'll need to add its value to . When exceeds , you know the projectile must have gone through the target point. 

I have never seen a game do this either. This is how I would design it so that hopefully there is an incentive to level up: At the beginning, when the character is young, they will be able to use the brute-force method. The character will have no issue wiping out waves of low-level grunts. The issue arises when the player encounters more intelligent enemies who can avoid or outsmart these simple attacks. The player has no means of fighting them because they aren't smart enough. As the player ages they will no longer have the strength they once had. However, they will have gained wisdom, which will allow them to outsmart those highly intelligent enemies. In fact, when speaking in terms of potential, the older player can be more effective. Strategy is more powerful than brute-force. In this design, enemies don't become much more powerful, but they do get smarter and smarter (which possibly means better AI). Here, the wisdom stat is your magical power. You might be able to turn this into something other than magic, but the same basic idea applies. Slowly degrade the players strength but slowly reveal to them new dynamics of strategy. This design gives players an incentive to level up, even though they will degrade other stats. This will also increase the level of challenge throughout the game because it gradually forces the player to use more intelligent strategies. So, to answer your question: The downside to this (well, not a downside, just a challenge) is you have to be very clever with how you intend the player to use strategy. You have to design it in such a way that the player must use strategy to advance. You have to design it so that the ability to use strategy is more desirable (and more satisfying) than the ability to use brute-force. You have to make enemies intelligent as well (or make them seem intelligent while simply having puzzling loopholes in their defenses), which is obviously difficult to pull off. 

Depending on what you're aiming for, you have several options. Best quality on every device? Make a separate version, optimized by hand for every single device separately. Least amount of work? Find the crappiest device you want to support, develop for that, and port that around. There's no magic way to detect the performance, but you can investigate the characteristics of various devices; how many pixel/texel pipes, how many shader cores, and so on. You'll find that while the desktop hardware has ludicrous amounts of shader cores and texel/pixel bandwidth, the mobile devices are genrally very, very constricted (some have as few as one shader core =) EDIT: that said, there's some stuff you can do, like quality settings, and benchmarks, but don't expect desktop content to run on a phone (not yet, at least..) 

Indexing this data is still pretty trivial. The compression ratio is naturally worse if there are more uniques (as you have to pump up the indexes to 16 bit ints and store more pilars). 

You should consider shader programs as similar part of the state as textures. Changing the state is expensive, so you may be able to get away with combining several textures to one to avoid texture changes; the same applies to shaders - you may be able to combine several shaders to avoid state changes. Similarly to combining textures, combining shaders comes with an overhead - if you only render 16x16 pixel area out of a 4096x4096 texture, you're not using the hardware efficiently. Similarly, if the object you render only uses 10% of the code in your shader, it's possible (and even likely) that the graphics card is computing a lot of stuff unnecessarily. In short, "depends". 

You can then create a doughnut shaped room by subtracting a smaller generated room from the center of a larger generated room. You should keep the walls of the smaller generated room though. Since I found this interesting I went ahead and tried implementing it myself. I would recommend not looking at my code until you have tried to write it yourself though. 

I want to test if my enemy can see the player, however I want this to be pixel-perfect. I already have all of the solid objects drawing into a separate render state. It should be noted that every solid object is changing and warping all over the place constantly. I cannot check individual walls because a distortion effect is applied to all of them. The walls blend together. Essentially it would lerp between the enemy's position and the player's position and see if there are any pixels that have an alpha value of greater than 0. The rotation of the enemy and the player do not matter. All of my attempts at doing this on the cpu have worked, however they slowed the game down dramatically. All of my attempts at going this on the gpu just didn't work at all. What is the most efficient way of doing this? Is there any way to do this in the gpu? Edit: There is no geometry at all. The walls are completely amorphous. I did not realize that global variables were constant in hlsl until after I wrote this: 

There is no such mechanism in OpenGL, unfortunately. On the other hand, with the multitasking systems we use, there's hardly any way to know what's going on anyway - it might be that your virus scanner just decides to start downloading updates at some point, and that messes up your timing and there's nothing you can do about it.. =) It wasn't long ago when you couldn't even depend on the refresh rate on PCs. Currently the standard seems to be 60Hz, but even that may be changing with the 110Hz, 120Hz and 144Hz displays out there today. So it's best to separate game logic from rendering (so rendering framerate may change while the game logic / physics will run at a stable rate), and do any terrain generation / preloading stuff on a separate thread. If you're working on a platform where you have absolute control (or close to it), such as a game console, the situation is different. 

Short answer: yes-ish, but the point of it all is rather debatable. Yes, it is possible to run a simulation of all angles and shot strengths, and to let the physics run through, and to get some values out. There's two slight problems with this, however. First: What would you consider a failure? If you are looking for crashes, that's a clear thing, but any other kind of tests would be harder to make. How do you know if some values are wrong? Would a human watch every single iteration to make sure nothing funny happens? Or would you just set some arbitrary boundary values that are checked against ("no object may ever fly over 2 kilometers" or some such..). If you keep the results for sanity testing, any small change in the application may change the results dramatically. If 50% of your values are suddenly different, can you say it's due to an introduced bug, or if, due to some small floating point accuracy difference, the physics simulation just happens to act slightly differently? Let's say you're looking to stress the physics engine, and have some boundaries to check against. Second: Is it really sane to iterate through every single possible angle and shot power? The number of iterations required is probably immense, and since the physics simulation takes some time to execute. It is still probably possible to test them all, especially if you throw the problem to some cloud processing service, but whether it is cost-effective is another matter. More likely it's worth it to make some hand-crafted boundary case tests (the cases you'd expect to have problems; shooting straight up, straight right, max power, min power, etc), combined with random testing that can be left running overnight. 

After reading that I decided to convert my midpoint displacement method into what I believe is a proper diamond-square method. However, it still does not look seamless despite me using "consistent initial corner values." The method works perfectly otherwise. Here are two generated maps next to each other: As you can see, there are major discontinuities (even though it looks like they could almost fit together). What I wrote must not be a "true" diamond-square method, or maybe I am misunderstanding the Wikipedia article. So in other words my question is this: What is wrong with my code or my understanding that prevents me from stitching together maps? Thanks a lot! 

Draw walls by picking pairs of points around a circle and linearly interpolating between them. The number of pairs should equal the number of sides of the shape. You will need to use trig for this. Create a floor by filling the shape using some sort of fill algorithm. 

If your projectile has a consistent velocity throughout, here is how to make it stop exactly on the target when it reaches the target: First measure the distance between the starting point of the projectile and the target. 

If my understanding is correct, this moves the memory from VRAM into RAM. Casting the render target to a texture does not move the memory into RAM, so it was still subject to VRAM shenanigans. 

USB experiment boards. These are relatively cheap and give a bunch of I/o pins you can control through some API on pc. USB connectivity kits such as PSoC can be used to make your device look like a keyboard or joystick. 

If a device supports both OpenGL ES 1.x and 2.x, it is very unlikely that there's any actual ES 1.x hardware. Which means that your ES 1.x code generates ES 2.x calls, shaders included - and the shaders most likely do stuff you don't actually need. On the other hand, if what you're doing doesn't require a lot of performance, using ES 1.x may let you write your applications faster. In the long term, I'd recommend using ES 2.x though. 

and welcome to the world of "binned", "tiled", or "scene capture" rendering. The screen on most mobile devices (Adreno, PowerVR, Mali at least) is split into smaller tiles, and when you render something, the driver actually records your rendering commands, and when it has no other option, it starts rendering. Then it re-plays those commands from said buffers for each tile. That's a bit trivial way of explaining what's going on, but the details don't really matter. Oddly enough, this is more efficient in both rendering performance and power use than rendering directly to a large framebuffer - you can have a small amount of very, very, VERY fast memory which contains one tile. 

but alas, this conversion doesn't seem to prevent the issue from occuring. I believe this is because is still a reference to a render target. Perhaps I need to perform a deep clone? If so, how? Also, the preserve contents thing doesn't work for this situation either. 

Ok I was able to fix the issue on my own! I figured out that all color values become less as the mask blurs. So where the mask was fading out, not only were the A values fading, but the R G and B values were fading too! I'm not sure why that is, but here is the change that fixed it: 

This sets speed equal to which increases at a rate of . You want the because is negative while t is less than 1 (and greater than zero). Just have start at zero and increase it by or the equivalent every update. Also, for the sake of performance you could store the value of in a variable and use it instead when calculating speed. 

The solid pixels would be passed into the "tex" variable. I would read the "impacted" and "impact" variables after one pass on a 1x1 texture. hlsl just doesn't want to let me do this. Apparently global variables are implicitly constant. Is there another way of doing this? on the variable syntax page it says "Global variables are considered const by default (suppress this behavior by supplying the /Gec flag to the compiler)." How do I add the /Gec flag to the compiler? I also wrote this, but it slowed the game down: 

I haven't tried this out, so some experimentation would be needed to see if it actually works. What I would do: Start from a vectorized format that follows the drawing direction. As an example, the hershey fonts - the "scripts" and "scriptc" fonts might be a good starting point: 

It's convenient when you're not using some higher-level bootstrapping library like SDL or GLFW, and EGL is available. On the other hand, platform-specific library example code is probably more easily available. YMMV. 

You could go a level deeper and start storing unique 2x2 pilar sets (adding yet another indirection); let's say there's 256 of these as well. The result is 64x64x4 + 256x2x2 + 256x4 = 18k Accessing this gets a bit hairy though (still relatively fast) 

Judging from those short quotes, I'd guess it's a case of data loss in translation. Not all data in FBX ends up in XNB. EDIT: Microsoft has posted a sample XNB parser (along with the format spec) which could be a starting point for the converter: $URL$ 

Offhand looks like your UV coordinates are messed up. Try rendering the textured cube with just a static test image? 

Pixel art is its very own art form. There are several tutorials on the net about it, but basically you plot one pixel at a time. Here's a few tutorials, but I'm sure you can google for more: 

I figured out how to implement a midpoint displacement algorithm to generate a map for my game. I wanted to create an infinitely large world, so I tried to patch two maps together, but they didn't look very seamless. I remembered the Diamond-square algorithm... On the Wikipedia page for the Diamond-square algorithm it says: 

I've never used Unity, so I don't know what it has to offer for this specific problem, but I would create this effect with one of these methods: 

You can create a room that is in the shape of a regular convex polygon with arbitrary sides, size and rotation. Steps: 

I'm trying to make a shader that changes the color of a sprite the way the XNA spriteBatch.draw() color parameter changes color. I don't know exactly what it is called (the closest word I can think of is 'tinting' but that's not right I don't think) and it's kind of hard to explain the way it looks. Say you set the parameter to 'Red' -- white pixels in the original texture would become completely red, while black pixels would remain black. Grey pixels would look dark red, blue pixels might look purple, green pixels might look brownish. If you set the parameter to 'Black' the texture would become completely black. Does anyone know the math behind this, or what this technique is called? It's probably very simple and I'm just not seeing it. Thanks! 

Depending on your compression target, you have plenty of choices that are always a tradeoff of space vs implementation complexity (and processing time). I have to make plenty of assumptions here, but the general principles should apply. The raw data is 128x128x4x4 bytes (1 byte per tile). 

What you're seeing is floating point accuracy, or lack thereof. Even highp is specified as 16 bit in OpenGL ES according to the spec (pdf link). To make things worse, denormals/subnormals are not required (these are used to improve accuracy near zero, but require special handling and, to save silicon space, are often not implemented). Note that's the minimum spec, and implementations may be more accurate. You can get more information about the accuracy of your specific implementation by calling glGetShaderPrecisionFormat(). To solve the problem, try not to use huge numbers with tiny changes. 

.3ds format is (fairly) standard format used by very, very old 3d studio. It's supported by many modelling packages as one export format. 3ds max's format is different. 3ds max is actually a framework with lots and lots of plugins, and each of these store their data in different ways, and thus the .max format isn't very well defined (I doubt anyone has even tried). In short: lib3ds won't load .max files, because they are not .3ds files. 

Sure, but that depends on how much effort you're willing to put into it. There's always a cost.. Procedurally generated textures fully in the shader, building a big texture from tiles within a shader, or using detail textures to mask the low resolution of the main texture are some techniques that come to mind. If you can get away with it, just store your texture as grayscale and tint it in a shader. Next, you can combine several textures into a single texture's R, G and B channels, and separate them in a shader to grayscale and tint it. A word of warning regarding paleted textures: they're hardly supported by any hardware these days and may end up being "plain" truecolor textures in the end (eating memory etc). Simulating a paleted texture using shaders is one interesting option though, but I wouldn't go there unless I needed to do palette-cycling effects or some such.