Complement following a request in a comment (july 7, 2013) This complement concnerns the existence of simpler algorithms than Earley's. As I said, searching the web at "parsing intersection forest" should quickly give you references, from which you can dig further. The basic idea is that all paths parsing with construction of a shared forest is nothing but the old intersection construction of Bar Hillel, Perles and Shamir for a regular language and a context-free language, using a finite automaton and a context-free grammar. Given the CF grammar, you apply the construction to a trivial automaton that recognizes only your input string. That is all. The shared forest is just the grammar for the intersection. It is related to the original grammar through a homomorphism, recognizes only the given string, but with all the parse-trees of the original grammar up to that homomorphism (i.e., simple renaming of non-terminals). The resulting grammar contains a lot of useless stuff, non-terminals and rules, that are either unreachable from the axiom (not to be found in a string derived from the initial symbol) or that are non-productive (cannot be derived into a terminal string). Then, either you have to clean it with a good brush at the end (possibly long but algorithmically simple), or you can try to improve the construction so that there is less useless fluff to be brushed in the end. For example, the CYK construction is exactly that, but organized so that all rules and non-terminals created are productive, though many can be unreachable. This is to be expected from a bottom-up technique. Top-down techniques (such as LR(k) based ones) will avoid unreachable rules and non-terminals, but will create unproductive ones. A lot of the brushing can actually be achieved by adequate use of pointers, I think, but I have not looked at this for a long time. All existing algorithms actually follow essentially that model. So that is really the heart of the matter, and it is very simple. Then why bury it in complexity ? Many "optimisations" are proposed in the litterature often based on the LR(k), LL(k) family of parser construction, possibly with some static factoring of these constructions (Earley has no static factoring). It could actually be applied to all known techniques, including the old precedence parsers. I put "optimization" between quotes because it usually not clear what you are optimizing, or even whether you are actually optimizing it, or whether the benefit of the improvement is worth the added complexity of your parser. You will find little objective data, formal or experimental, on this (there is some), but many more claims. I am not saying that there is nothing of interest. There are some smart ideas. Now, once you know the basic idea, the "optimizations" or improvement can often be introduced statically (possibly incrementally) by constructing a push-down automaton from the grammar, following the kind of parser construction technique you are interested in, and then applying the cross-product construction for intersection to that automaton (nearly the same thing as doing it to the grammar) or to a grammar derived from that automaton. Then you can introduce bells and whistles, but that is mostly technological details. The Philosophi√¶ Naturalis Principia Mathematica of Isaac Newton is reportedly a great piece of physics and mathematics. I do not think it is on the reading list of many students. All other things being equal, I do not think it is very useful to teach Earley's algorithm, though it is an important historical piece. Students have enough to learn as it is. At the risk of being shot down by many people, I think much the same for the Knuth LR(k) paper. It is a superb piece of theoretical analysis, and probably an important reading for a theoretician. I strongly doubt that it is so essential for the building of parsers given the current state of the technology, both hardware and software. The times are past when parsing was a significant part of compiling time, or when the speed of compilers was a critical issue (I knew one corporation that died of compiling costs some 30 years ago). The parsing specialist may want to learn that specialized knowledge at some point, but the average student in computer science, programming or engineering does not need it. If students must spend more time on parsing, there are other extensions that might be more useful and more formative, such as those used in computational linguistics. The first role of teaching is to extract the simple ideas that structure scientific knowledge, not to force the students to suffer what the research scientists had to suffer (doctoral students excepted: it is a rite of passage :-). License CC BY-SA 3.0 from the author 

This is a classical cross-product construction, originally published by Bar-Hillel, Perles and Shamir (1961) in It can be found in most elementary books on automata theory. (the reference to the initial paper appears in the paper suggested by @Jurij, which I could not access as I wrote this answer). Actually, it can be done with any CF grammar, no normal form being required (but ... see below). On the FSA side, any recognizing finite-state automaton will do, including non-deterministic. The complexity is $O(n^{p+1})$ for both the construction time and the size of the resulting grammar, where $n$ is the number of states of the automaton and $p$ is the length of the longest rule right-hand side of the grammar. Hence, reducing the grammar in Chomsky normal form (CNF) will give $n=2$ and allow for cubic time and space (and result size) complexity. More precisely, some of the constraints of CNF are not even needed. It is enough to put it in 2-form, by introducing new non-terminal so that no right-hand side containt more than 2 symbols. The size of the resulting grammar can be further reduced by minimizing the finite automaton. Minimizing a DFA has a time complexity $O(ns \ log\ n)$, where $n$ is the number of states and $s$ is the size of the alphabet. However the cost is much higher for a NDFA, as it is $O(2^n)$. There is no escape from that if you have a NDFA to begin with, since this high cost is needed for determinizing the NDFA which requires a powerset construction on the set of states. 

Spaghetti sort is at least $O(p)$ where $p$ is the length of the longuest spaghetto. In order to sort spaghetti, you must level them onto a table. If you consider the longest spaghetto, it has to slide down until its bottom touches the table, at which point a reaction force will propagate from the bottom up so that the whole spaghetto stops, including its top end. But propagation from bottom to top cannot be faster than the speed of light, hence it has to be proportional to the length of the spaghetto. 

With only flat domains, you cannot define limits to construct "infinite" structures, such as looping structures, for data or for programs. Fixpoint constructions in denotational semantics (since you used that tag) use non flat domains. Maybe you should give examples of domains that are taken as flat, while it would be better to do differently. Many problems we deal with are expressed with data from flat domains. But non-flat domains are used when necessary or convenient. Infinite, or indefinite structure such as stream are not flat. Some domains used for program analysis, for example with abstract interpretation, are not flat. But is pretty much because non-flat domains are more appropriate, as information that can be obtained is expected to have different degrees of precision. One good example to look at (but I have no expertise) is the case of real numbers. They are often considered in a flat domain, but they are really limits in a non-flat domain. How much is that used, explicitly or implicitly? Edit after the question was restricted to base types The nature of the question changes somewhat if you restrict it to base types. In many case, it would be interesting to see whether there are alternatives to using flat domain, and what is the usefulness of such alternatives. Is there a choice between flat and non-flat for booleans? Is there one for integers? for characters? etc. What are the pros and the cons. But, to begin with, you may have to define what is a base type. This is why I suggested looking at the real numbers. Should the domain be considered flat or not. Even in classical mathematics, reals can be defined as limits. So you could ask the question whether reals are a base type. I have not looked at these issues for a long time, but I think the following paper may be relevant: "Concrete Domains" by Gilles Kahn and Gordon Plotkin. I also found a historical introduction to it which I have not read. And there seem to be some significant literature that followed. 

Actually, optimizing compilers do that kind of things. I am for example thinking of the work of Robert Paige at NYU. The problem is of course to identify situations where a given type of transformation may be useful. That require identifying computational structures, and knowing some of the algebraic properties of the data being manipulated. A classical example, very simple, is the strength-reduction in loops, where you replace a product by a sum. It usually does not change much the complexity (see more below), but can effectively speed up your algorithm, at the cost of one extra variable. This is a special case of finite differencing techniques. I think you should find example in the litterature on high-level optimization, which is quite different from peep-hole optimization in the code generation process. See the two wikipedia article for various types of code optimization. Regarding the identification of such situations, they can sometimes be found by formal analysis of program. It can also help to make them more apparent by using very high-level abstractions in the programming process. Many such transformations improve the program without changing the complexity. But it much depends on what complexity you are considering and what is a unit operation. For example if you consider arithmetics of unbounded integers, addition has linear cost, while multiplication has a higher cost (quadratic in the naive case). Using reduction in strength in loops will reduce complexity. I unfortunately do not remember more spectacular cases. There is also some significant litterature on program transformations, with a variety of techniques that started in the 1970ies. Some transformations may not bring much by themselves, but may enable other transformations. Pattern matching in equational algebras is one way of identifying possible tranformations that has been considered. This may be one reason why very high level abstractions can help the process. 

Regularity is decidable for DCFL, but it is undecidable for general Context-Free Languages. Regarding DCFL, I have two references (from Hopcroft+Ullman 79): 

Regarding specific activities I am skipping the obvious activities required by academic life. or scientific life in industry: teaching, publishing, reviewing papers, writing grant applications, taking academic responsibilities, managing people, advising students or policy makers. But even then, there is no simple answer to your question. Here I am just sketching a few aspects that come to mind, but I am sure there is a lot more to be said. And I am not sure I am answering you. Some of the best scientists have written books about their work, and that may give you hints about scientific activities. Researching in theory may involve a variety of things, depending on your skills and interests that vary a lot from scientist to scientist. It is somewhat hard to talk of it, since each person perceives more her own activity and interests than that of others. Most reasearch requires a wide knowledge, since interesting and really original results often comes from putting things in relation, or transferring a technique from one (sub)field to another, or getting different technical views of the same problem. So learning as much as you can in breadth as much as in depth is important. Remember that while you have the time and ability for it as a student, or as a junior faculty/scientist, both will be reduced later, because of responsibilities and life in general. Teaching what you do not specialize in may be a way to keep learning. On SE you can probably learn more by answering than by asking. The kind of work people do can vary a lot, because people are fortunately very different, with a great variety of interests and technical abilities, thus complementing each other. Some people are problem solvers. They look at theoretical or practical questions raised by other people, or by themselves, and try to solve them, or get closer to a complete or partial solution. Other people will be better at structuring existing knowledge, and putting thing in relation, and then finding new questions to ask. Both are essential. Finding simpler proofs of technical results, or simpler presentations of theories, or merging concepts is important. It generalizes results, reduces the numbers of things to learn, emphasizes the essential ideas and possibly brings new understanding. Since our learning time is limited, progress is possible only when we condense knowledge. A simple example is the study of abstract families of formal languages. When language theory first developped, closure properties under various operation were proved again and again for each family of languages (regular, context-free, RE, ...), with ad hoc techniques depending on the family. Then it occurred that these closure properties had intrinsic relations independently of the concerned families, and they were studied as such. Today, we only have to check a few of the simpler closure properties for a given family, and we get "for free" a whole set of other properties. Another important point is that there is not such a clear-cut distinction between theoretical, practical, or experimental work. A good theory may lead to the implementation of systems that can mechanize the resolution of problems. And it will take a good theoretician to implement such a system, with a mix of theoretical and practical work, including system implementation, or language design. Many examples come to mind, such as proof and/or program synthesis systems, specialized language for synchronous parallel systems, a restricted algorithmic language for which computational complexity can be systematically derived. Not only is it important to be able to produce such pratical systems, which make theoretical results more widely available and usable, but it is often very important for theoretician either to use proficiently these systems, if only to unload the now less creative parts of his work, or to contribute to the development and extension of these system. Another aspect is to be able to compare theoretical approaches by pratical experimentation. Here, the issue is to compare different techniques to accomplish the same goal. Comparing implementations is often meaningless as their efficiency often depend on the programming language, or the programming skills of the implementor. But if they can be expressed all in a common theoretical framework, then it is sometimes possible to compare them experimentaly within that framework. Here, theory and practice support each other, as they often do in science. Pure theoretical analysis is not always easy to achieve. Furthermore, experimental analysis, when well conducted, can exhibit unexpected behavior that may motivate better theoretical analysis. The world is not simple or clear-cut. That is why it can be fun, with room for all kinds of skills. Questionning your own knowledge, and answering questions of others, by whatever means. Two things often forgotten: ethics of science, and explaining it to people. Both are essential, and hard.