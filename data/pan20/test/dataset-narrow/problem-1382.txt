In general, deleting an object in OpenGL involves nothing more than calling one of the functions, where is the type of object. OpenGL is smart enough to not break or otherwise implode when told to delete an object that may still be used by the renderer. If you delete an object that is still attached to another object (a buffer object in a VAO or a texture attached to an FBO), then the actual deletion is deferred until all attachments are broken or the objects attached to them are themselves deleted. So if you delete a buffer object that a VAO uses, that VAO will still work, preserving the lifetime of the buffer object until it's no longer using it. So ultimately, you don't have to care. 

In forward rendering, these parameters get to the fragment shader's lighting function either by being passed directly from the vertex shader, being pulled from textures (usually through texture coordinates passed from the vertex shader), or generated from whole cloth in the fragment shader based on other parameters. The diffuse color may be computed by combining a per-vertex color with a texture, combining multiple textures, whatever. In deferred rendering, we make this all explicit. In the first pass, we render all of the objects. But we don't render colors. Instead, we render surface parameters. So each pixel on the screen has a set of surface parameters. This is done via rendering to off-screen textures. One texture would store the diffuse color as its RGB, and possibly the specular shininess as the alpha. Another texture would store the specular color. A third would store the normal. And so on. The position is usually not stored. It is instead reconstituted in the second pass by math that's too complex to get into here. Suffice it to say, we use the depth buffer and the screen-space fragment position as the input to figure out the camera-space position of the point on a surface. So, now that these textures hold essentially all of the surface information for every visible pixel in the scene, we start rendering full-screen quads. Each light gets a full-screen quad render. We sample from the surface parameter textures (and reconstitute the position), then just use them to compute the contribution of that light. This is added (again ) to the image. We keep doing this until we run out of lights. HDR again is a post-process step. The biggest downside to deferred rendering is antialiasing. It requires a bit more work to antialias properly. The biggest upside, if your GPU has a lot of memory bandwidth, is performance. We only render the actual geometry once (or 1 + 1 per light that has shadows, if we're doing shadow mapping). We never spend any time on hidden pixels or geometry that isn't visible after this. All of the lighting pass time is spent on things that are actually visible. If your GPU doesn't have lots of memory bandwidth, then the light pass really can start to hurt. Pulling from 3-5 textures per screen pixel isn't fun. Light Pre-Pass This is sort of a variation on deferred rendering that has interesting tradeoffs. Just as in deferred rendering, you render your surface parameters to a set of buffers. However, you have abbreviated surface data; the only surface data you care about this time is the depth buffer value (for reconstructing the position), normal, and the specular shininess. Then for each light, you compute just the lighting results. No multiplication with surface colors, nothing. Just the dot(N, L), and the specular term, completely without the surface colors. The specular and diffuse terms should be kept in separate buffers. The specular and diffuse terms for each light are summed up within the two buffers. Then, you re-render the geometry, using the total specular and diffuse lighting computations to do the final combination with the surface color, thus producing the overall reflectance. The upsides here are that you get multisampling back (at least, easier than with deferred). You do less per-object rendering than forward rendering. But the main thing over deferred that this provides is an easier time to have different lighting equations for different surfaces. With deferred rendering, you generally draw the entire scene with the same shader per-light. So every object must use the same material parameters. With light pre-pass, you can give each object a different shader, so it can do the final lighting step on its own. This doesn't provide as much freedom as the forward rendering case. But it is still faster if you have the texture bandwidth to spare. 

What do I need to do to x, y, width and height from the variable width and height of my screen in order to keep this aspect ratio correct? SIDE NOTE: the aspect ratio of the screen will always be 16:9! 

In a 2d game, doing collisions with the edges of your screen is easy. Your textures are a 1:1 mapping of your screen resolution because your verities are too.. if your x position is > than ( screen width / 2 ) then you have hit the edge of your screen and shouldn't be able to escape it. How do you solve the same problem once your game is in 3d coordinates? I've tried the following but I'm missing something, the output is far too large. I'm expecting a number like 6.0 instead I'm getting 41.0 because I'm multiplying 25 or so by 1.5 

I'm going to tell this story more thoroughly: I originally made a 2D game. I had vertices for my background filling up 1920 x 1200: 

Both choices are killing me. Choice 1 meant I had to scale all of my collisions aswell, and both choice 1 and 2 meant I had to estimate how scaled they should be. so much so that my original 1920 x 1200 background I put back into orthographic projection coz I could not get it to scale well... The last and most frustrating issue is that I have NO IDEA how to calculate where the borders of my screen are... What is the usual approach to this? 

OpenGL is a rendering system. Rendering has nothing to do with getting text input. If you want text input, then you're going to have to use a library or OS system that provides it. SDL, Allegro, SFML, and most of the other "multimedia" libraries all have some means of getting input from the user. They're also cross-platform, if you're into that sort of thing. Even the "toy" libraries like GLFW and FreeGLUT have ways of getting basic input. As for the "text box" part, that's going to be more interesting. Generally, if you're making a game, you don't really interact with the native windowing GUI. So you either have to use a specialized GUI library like CeGUI, or you have to write the text box code yourself. I would personally suggest investigating game-GUI libraries, as writing a GUI yourself is not particularly fun. 

This is where you've gone wrong. The source and destination factors are multiplied into the source and destination colors, then added together. The OpenGL Wiki article on blending goes into greater detail on this matter. 

If you want to clip things to a certain region, you use the scissor box. It's a property of the : GraphicsDevice.ScissorTestEnable and either RenderState.ScissorRectangle pre-XNA 4.0 or RasterizerState.ScissorTestEnable in XNA 4.0. You don't need to measure text for it; just set the scissor box to the area you want to clip to. You can draw all the text you want after that. Indeed, scissoring is useful in all kinds of places for GUIs. Just don't forget to turn off scissoring when you're done. 

I applied Orthographic projections to my vertices and obviously it came out lovely on my screen! my 1920 x 1200 texture mapped perfect to my full screen app. beautiful. Same goes with all of my players and enemies etc, I had vertices like: 

Fyi, distance coming in as a parameter is actually set to 25.0f at the moment, I was trying to think ahead 

It may help to see the actual problem I'm having... I basically want 2d characters in a 3d world so I can do cool rotation things.. but I cannot figure out how to know when my character has hit the edge of the viewable screen: $URL$ 

I am trying to write a game. When someone resizes the window of my game, I need all the graphics i have drawn on the screen to reposition correctly so that the ratio of the graphics' width and height remain correct and that the new position determined by x and y, have been adjusted for the new size screen. In the following image, the top left corner is 0,0... all coordinates / widths / heights are measured from this point. 

this image was 128 x 64... and so was my texture, so it mapped perfectly... it fit on the screen as if it were 1:1 mapped... spot on! The problem I wanted to make my game 3D... so I now applied a perspective fov left hand projection and a camera... rather than the orthographic projection which allowed it to be 2D: I set up a camera like so: 

Note that the locations are not the numbers. They're the indices into the draw buffers array. So 0 in this case means . It only refers to because that's what is in . You could just as easily set to index 0; the would then be written to 

Before I begin, it's important to understand: this means your matrices are row major. Therefore, you answer to this question: 

A system is only useful if it is useful. If a system where an entity is "simply a collection of components" is less useful than a system where an entity is mostly a "collection of components", then do that. Stop trying to make "pure" systems and focus on making good ones that do what you need. Use components until components are no longer useful for you. Then use something else. You've already spent more time thinking about this than it deserves. 

They get around it by not using stencil shadow volumes. It's really not a good technique, and it doesn't work very well with deferred rendering, which is the direction a lot of the industry is going in. Yes, it's nice in that it's pixel accurate, unlike shadow maps. But the many downsides (performance among them) make it a less than attractive idea for game developers. 

However, I said that this translation matrix is wrong, because the translation is in the wrong place. I specifically said that it is transposed relative to the standard convention for how to build translation matrices, which ought to look like this: 

I did not change my vertices but I added a z of 1.0f to my objects with just a simple translation... The issue (and I guess you already realise this) was that my textures / vertices were HUGE... I either had to scale all of my objects by some roughly correct size with a scaling transformation, or change my vertices (and this is what I did in the end) to something more like this: 

Okay so now I have a topOffset for both widescreen cases. I draw all of my images and interactive objects at their ( y position + the topOffset ). New problem is that the menu is also needed to take into consideration, so I use that systemmetricssomething call which returns 20 which I then subtract from the topOffset... Okay my drawing is vertically centred... perfect! Question is, regardless of my entire game now being vertically centred (in the most crazy long hauled way possibly known to man), I can still draw outside of the game area into the black zone that now exists. I know I am approaching this wrong, but I am too new to this to know any better. How do you guys vertically centre your games if they are a different ratio to the artwork you have created for the backgrounds etc? 

No, there is no way from OpenGL to change how your GPUs distribute your shaders across its internal computing resources. The job of the driver is to find an optimal distribution for the work you provide it. 

Gameplay Keeping gameplay interesting for hours-long play times is exceedingly difficult. There's a reason why a lot of 20+ hour RPGs have a story and try to use that to maintain player interest. MMOs use a social component to keep people playing, but then they're not playing for your game, but for the other people in your game. It may as well be Monopoly at that point; it can still feel like work, just working to get something for someone else rather than themselves. Sooner or later, players are going to figure out your game. The best way to handle this is to make sure that your core gameplay is flexible enough to keep throwing curves at the player that they are forced to react to. Make sure that there is no single optimal strategy. Change the gameplay as they play by introducing new mechanics and enemies that they have to react to in unexpected ways. Most RPGs tend to be mechanically front-loaded. They show all of their basic play mechanics (magic systems, skills, etc) in the first few hours. They'll expand on the list of particular fields of mechanics (adding new spells and skills), but you generally don't get entirely new mechanics in the middle of a game. Getting new mechanics is a strong way of preserving interest. Even something as simple as a mobility-based skill can radically change how the player plays and thus increase their interest in playing the game. Use new enemies to help introduce new mechanics. Use those enemies to ensure that the player knows those mechanics exist and that the player can use them. And don't forget to test the player on all of their mechanics often, lest they fall out of habit with using them later. While it's good to focus on the intellectual act of playing the game, don't discount the visceral or spectacle elements either. Hitting monsters with impressive-looking spells can hold a player's interests too. Probably not as much as interesting gameplay, but it is a way to help.