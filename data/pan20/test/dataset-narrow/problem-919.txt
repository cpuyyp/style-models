The GD designation is, unfortunately, mostly a memory - and even when GD code was available it tended to be a year or two behind in (usually critical) features. Even ED vs LD isn't 100% indicative of ideal code versions in all cases. Bugs can exist in any version. The bugs that involve your particular set of features are the ones you care about. Check out the bug navigator to list the open/fixed issues in the version of code that best fits your requirement. Your Cisco SE / support team should also be in a position to run through a quick bug scrub to see what the prevailing wisdom is as far as best of the latest-and-greatest but ultimately the bug toolkit is the best tool you've got. As to upgrading from 12.2 - what features would you gain that would be particularly helpful? Some shops I've seen have made the move to be able to support SSH certs or new knobs on routing protocols while others just want to be on a reasonably modern rev just as a maintenance practice. Toward this end you might also want to run through the feature navigator to determine exactly what's changed from version to version. 

LR connections are single mode. You'll need mode conditioning patch cables on both sides but likely a different type than was in use for your LRM (which wants multimode, but with a smaller core). 

QSFP and QSFP28 are the same physical form factor - the former supports 40G (or 4x10G) while the latter supports 100G (..or 4x25G / 2x50G). For the most part QSFP's will work in QSFP28 ports but the inverse is not true. SFP, SFP+ and SFP28 also share a common form factor (although different than QSFP, obviously). SFP was the original 1G version while SFP+ is 10G. The modules supporting 25G are sometimes just generically referred to as "25G SFP" but are actually SFP28. So - there is compatibility...for breakouts. As an example, four independent 10G SFP+ SR's can connect to a single 40G-SR QSFP (obviously only if the associated 40G Ethernet port supports breakouts) or a QSFP28 100G can connect to 4 SFP28 25G (assuming both breakout and FEC are supported). Outside of this, however, you generally can't connect, say, a 1G SR optic to a 10G SFP+ SR or a 25G SFP28 to a 10G SFP+. In some limited cases within certain vendors some optics allowed for multiple speeds (ex: in the Cisco world some of the workgroup-class switches can support 100- or 1000- on a TX SFP) but this is highly uncommon in modern optics. To your question - if your Mellanox has a QSFP28 port and you wanted to connect some 10GE devices then you'd need a QSFP 40GBase-SR4 adapter and an appropriate set breakout cables with an MPO connector running to 4 LC duplex connectors (this could be via mountable cassettes, structured cable or a pre-made patch cable). The LC would connect to 10GBase-SR SFP+'s on the downstream switches. You'd need to configure the Mellanox's port to run in breakout mode and then would configure the resulting 4 10GE interfaces independently. 

I've gotten this to work, but the method wasn't pretty. Because the multicast address for airplay is subnet local -only- it can't be propagated as-is by a standard router. I ended up setting up Avahi on a Linux vm with interfaces into each required subnet. Avahi was, in turn, configured for service reflection (enable-reflector=yes). This allows the daemon to gather the various mDNS requests and propagates them into the various connected subnets. I'm not especially a tremendous fan of this solution, but it does work for a dozen or so VLAN's without too much drama. The moral of this story is, unfortunately, that these protocols are, at best, problematic. 

The kinds of factors that drive the need for a dedicated server don't necessarily correlate to volume of traffic or typically observed threats. A site with massive back end I/O requirements might hand out small amounts of tabular data to a handful of users. The decision to pursue a dedicated firewall should be approached in the same way. The other point, of course, is that adding a dedicated firewall later isn't (or shouldn't be) very invasive. 

On board LOM (LAN on motherboard) chipsets may not provide as many offload/optimization features as a separate purpose-built NIC but that doesn't imply that it would itself consume host CPU resources. Indeed, some of the LOM chipsets (or ones very close) are used on standalone NIC's. Measuring latency in a meaningful way is a lot harder than it might seem. You would likely need a custom low level driver, very specialized measurement software and a very high resolution external time source. It isn't a trivial issue. While there are likely some wins in terms of latency (probably in the hundreds of nanoseconds at most) the bigger win is usually in terms of throughput and host CPU (those optimizations and offloads mentioned earlier). There are also often management features and troubleshooting tools that make server NIC's desirable as well. 

What are you trying to accomplish? You can simply create additional interfaces in kvm and tie them to tagged VLAN's on the host's NIC. 

What are you trying to accomplish? Is it minimizing (smoothing out) the spikes in demand created by a bunch of servers simultaneously booting or something that can push emergency adaptation of poweroff? If it's the former then look for power sequencers - there are in-rack units designed to power up groups of outlets with programmable delays - say a few minutes between each of your groups to allow each to settle before booting. This used to be pretty common with some vendors in the larger side of midrange. 

The way to achieve symmetry here is actually not so much a function of BGP as it would be NAT. Let's assume you continue with the scheme of advertising the /22 out of both routers as well as one /23 out of each. Set up outbound NAT such that traffic leaving a given border router is given a source address in that router's /23. To effect symmetry for inbound traffic you'd also need source NAT to translate connections from global sources to separate pools allocated to each border router. You will lose visibility at the server level into where the connections are actually coming from, but that information can be retrieved from the router via Netflow or similar mechanisms. There's a significant degree of potential complexity that's being introduced here that will also translate (no pun intended) to where- and how- you place default gateways for the servers. This might call for an intermediate tier of BGP speaking routers w/appropriate policies to keep locally originated traffic going in the right direction. In general, though, there's no elegant way to assure symmetric routing when you're advertising identical routes with identical metrics out of multiple routers - and, quite honestly, outside specific requirements for state-aware devices like certain firewalls or load balancers, this just isn't something that is typically a design goal for most networks.