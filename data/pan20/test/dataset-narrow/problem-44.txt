In ray tracing, we can optimize this by imagining the focal plane is in front of the camera. You can think that the photons from the scene travel towards the camera hole, and resolve on the image plane. 

IMO, a variation of your second approach is going to be the simplest, and could be quite fast. Have you profiled anything yet? A CPU should chew through 10000 matrix multiplications extremely quickly. If you wanted to get even more performance, you could SIMD / thread the update. That said, the slowest part is probably going to be the bandwidth of sending 10000 matrices over the PCIE bus. But, again, this is all supposition. Try it out, and profile. 

After we sample the inputDirection ('wi' in the code), we use that to calculate the value of the BRDF. And then we divide by the pdf as per the Monte Carlo formula: 

Short answer: Importance sampling is a method to reduce variance in Monte Carlo Integration by choosing an estimator close to the shape of the actual function. PDF is an abbreviation for Probability Density Function. A $pdf(x)$ gives the probability of a random sample generated being $x$. Long Answer: To start, let's review what Monte Carlo Integration is, and what it looks like mathematically. Monte Carlo Integration is an technique to estimate the value of an integral. It's typically used when there isn't a closed form solution to the integral. It looks like this: $$\int f(x) \, dx \approx \frac{1}{N} \sum_{i=1}^N \frac{f(x_i)}{pdf(x_{i})}$$ In english, this says that you can approximate an integral by averaging successive random samples from the function. As $N$ gets large, the approximation gets closer and closer to the solution. $pdf(x_{i})$ represents the probability density function of each random sample. Let's do an example: Calculate the value of the integral $I$. $$I = \int_{0}^{2\pi} e^{-x} \sin(x) dx$$ Let's use Monte Carlo Integration: $$I \approx \frac{1}{N} \sum_{i=1}^N \frac{e^{-x} \sin(x_i)}{pdf(x_{i})}$$ A simple python program to calculate this is: 

Check that the pdf is valid and the radiance is non-zero Evaluate the BSDF using the sampled InputDirection Calculate the pdf for the BSDF given the sampled InputDirection 

As N gets large, the estimate will converge to the correct solution. We can apply this same principle to light sampling. Instead of sampling every light, we randomly pick one, and multiply the result by the number of lights (This is the same as dividing by the fractional pdf): 

In order to create the ray, we need to know the distances a and b in world units. Therefore, we need to convert pixel units into world units. To do this we need a define a camera. Let's define an example camera as follows: $$origin = \begin{bmatrix} 0 & 0 & 0 \end{bmatrix}$$ $$coordinateSystem = \begin{bmatrix} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1 \end{bmatrix}$$ $$fov_{x} = 90^{\circ}$$ $$fov_{y} = \frac{fov_{x}}{aspectRatio}$$ $$focalDist = 1$$ The field of view, or fov is an indirect way of specifying the ratio of pixel units to view units. Specifically, it is the viewing angle that is seen by the camera. 

Randomly choose either $f(x)$ or $g(x)$ to evaluate. Divide the result by $\frac{1}{2}$ (since there are two items) Average 

Multiply the weight with the direct lighting calculation and divide by the light pdf. (For Monte Carlo) And add to the direct light accumulation. Then, we sample the BRDF 

We can use the definition of tangent to calculate the screenWidth in view units $$\tan \left (\theta \right) = \frac{opposite}{adjacent}$$ $$screenWidth_{view}= 2 \: \cdot \: focalDist \: \cdot \: \tan \left (\frac{fov_{x}}{2} \right)$$ Using that, we can calculate the view units of the pixel. $$x_{homogenous}= 2 \: \cdot \: \frac{x}{width} \: - \: 1$$ $$x_{view} = focalDist \: \cdot \: x_{homogenous} \: \cdot \: \tan \left (\frac{fov_{x}}{2} \right)$$ The last thing to do to get the ray is to transform from view space to world space. This boils down to a simple matrix transform. We negate yview because pixel coordinates go from the top left of the screen to the bottom right, but homogeneous coordinates go from (-1, -1) at the bottom left to (1, 1) at the top right. $$ray_{world}= \begin{bmatrix} x_{view} & -y_{view} & d\end{bmatrix}\begin{bmatrix} & & \\ & cameraCoordinateSystem & \\ & & \end{bmatrix}$$ You can see an implementation of this here (In the code, I assume the focalDist is 1, so it cancels out) 

Convert photometric units to radiometric units using luminous efficacy Keep the entire render pipeline in photometric units 

We can use some trig identities to make it a bit more efficient to calculate with the computer: $$\begin{align*} \sin(\cos^{-1}(x)) \equiv& \ \sqrt{1 - x^2}\\ \cos(\cos^{-1}(x)) \equiv& \ x\\ \\ \phi =& \ \cos^{-1}(2\xi - 1)\\ \sin(\phi) =& \ sin(\cos^{-1}(2 \xi - 1))\\ =& \ \sqrt{1 - (2 \xi - 1)^2}\\ \cos(\phi) =& \ 2 \xi - 1 \end{align*}$$ So the full change is: 

Calculating pageLocInPhysicalTex If we make the lookup table the same size as the number of tiles in the virtual texture, we can just sample the lookup table with nearest neighbor sampling and we will get the location of the top-left corner of the page within the physical texture. 

In a traditional camera, the photons from the scene travel through the lens of the camera, then hit the sensor at the focal length. A consequence of the lens is that the image is upside down and backwards. 

The first option is what most modern games / animations will do. Each frame, you re-submit all the draw calls / re-upload any textures to the GPU that have changed, then ask the GPU to draw everything and show it on the screen. The second option is what most old 2D games did, as well as most window applications (ie. GUI programs that don't have a lot of change going on). One implementation would be to have a local CPU copy of the screen, a GPU copy, and a series of buffers to transfer data between them. So for every frame where there were changes: 

If the first ray hits the light, you should see the light's emission directly. So if we skip it, all the lights will show up as black, even though the surfaces around them are lit. When you hit a perfectly specular surfaces you can't directly sample a light, because an input ray has only one output. Well, technically, we could check if the input ray is going to hit a light, but there's no point; the main Path Tracing loop is going to do that anyway. Therefore, if we hit a light just after we hit a specular surface, we need to accumulate the color. If we don't, lights will be black in mirrors. Now, let's delve into SampleLights(): 

I would like to add full monte-carlo volumetric scattering to my path tracer, but I'm having a hard time researching how to do it. Let me explain what I would like to do: 

Finally, EstimateDirect() is just evaluating $BSDF(p, \omega_{\text{i}}, \omega_{\text{o}}) L_{\text{i}}(p, \omega_{\text{i}})$ For punctual light sources, this is simple as: 

As for your concern for a divide by zero, the definition of the rendering equation prevents it. Let me explain: For $(N \cdot V)$ to equal zero, they must be orthogonal. In this case, the visibility term of the rendering equation will cull the cases at / below the horizon. 

Russian Roulette randomly terminates a path with a probability inversely equal to the throughput. So paths with low throughput that won't contribute much to the scene are more likely to be terminated. If we stop there, we're still biased. We 'lose' the energy of the path we randomly terminate. To make it unbiased, we boost the energy of the non-terminated paths by their probability to be terminated. This, along with being random, makes Russian Roulette unbiased. To answer your last questions: 

The main purpose of tesselation is to increase the resolution of the mesh, while only transferring a small amount of triangle data around. In addition, tessellation allows you to dynamically change the LOD of the mesh, so you can optimize your shader calls. So, we can pass the GPU, say, 3000 triangles, and have it tesselate it to 300000 triangles. We are essentially trading storage space/bandwidth for compute power. Since GPUs have lots and lots of compute, and memory is limited/slow, this is a pretty good tradeoff. As for "not being heavily used in real-time graphics". I somewhat disagree. Many AAA games have been using tessellation for a long time. That said, tessellation is hard to get right. Done wrong, tesselation can lead to lots of problems, such as cracking, or over tessellating to sub-pixel triangles, which destroys your fill rate. Combined, these problems make it difficult to implement good and fast tessellation for your everyday person. Thus, most of the uses of tessellation you see nowadays are in AA or AAA games, game engines, and offline rendering tools. That said, there is active research going on that is trying to better utilize the tessellation hardware, and make it easier to use. For example: Efficient GPU Rendering of Subdivision Surfaces using Adaptive Quadtrees In the end, tessellation is a great feature, but is difficult to get right