Nothing with the server setup has changed since the migration. We are running two servers, a PRIMARY and REPLICA within an AlwaysOn Availability Group. The VMs are housed on Azure and have 8 logical processors, 28GB of memory, and all SSD drives. The data file layout looks like: 

I am wanting to utilize partitioning based on a (and later in conjunction with date ranges). Instead of needing to manually insert the latest value within the , I thought of creating a to pull the value and to add it to the . However, I am running into an unexpected error: 

When I perform a basic without an explicit , however, the Execution Plan shows that it is using the index in order to sort instead of the index. 

Has anyone experienced behavior such as this, or know what could be causing index statistics to go awry so quickly across so many tables? Any recommendations for diagnosing next time this occurs? Manually running the stored procedures and diagnosing the execution plans have proved rather unhelpful as the statistics its reading (Expected/Actual Rows) are the same. 

We are designing a multi-schema, multi-tenant database for SQL Server 2016 that will service a basic CRUD application which will see small to medium transactional throughput and comprise of 15-20 tables. For the sake of data isolation and security, we are exploring the utilization of a composite primary key that has and . These two columns will repeat throughout tables in a hierarchical manner, enforcing referential integrity with appropriate foreign keys, and allow us to enforce deeper row-level security. In most scenarios that I have read, the usage of a GUID for seems to be controversial due to performance implications, especially with regards to space. However, we believe that for the management of physical file partitioning, resource pool delegation, replication, portability, and general referential integrity, that a pairing of an integer and GUID will allow for better isolation and security within the multi-tenant database. We are bound by a range of security provisions, so this further leads us down the path of using this composite key type. In thinking of children composite keys, I know the order of the columns specified matters. However, I cannot seem to come to a consensus if we should feed the the same composite key structure down the table chain or if it is better to segregate it in a more traditional manner. For instance, given the two table structures: 

Yesterday we migrated a tenant (Tenant B) to a new version of our software, and this included moving them to another database where another tenant (Tenant A) has already resided. This new version only had one tenant (Tenant A) on the database until yesterday. Prior to go-live of both tenants on the same database, performance was relatively speedy with optimal query execution times. We were anticipating the added volume from the new tenant (B) to decrease performance slightly, especially since the new tenant (B) produces a much higher order volume. However, the past 48 hour period has actually showed a significant increase in database performance. 

We have the following (unfortunate) scenario: we have a database where on-the-fly changes are being made by developers as quick remedies to dictionary tables (order types, document types, fee distributions, and so forth). This wouldn't be an issue if these particular developers didn't forget to check their changes into TFS, but alas, we face it. This causes an issue where the database project(s) in TFS do not properly reflect what is in the database. Essentially, I want receive an e-mail or other type of notification (table insert with necessary information) when a change is made to 15-20 tables, what the change was, and who committed the transaction. What is the best method to monitor this type of behavior? We are willing to invest in tools that allow for this ability. I've checked out RedGate SQL Monitor, but it doesn't seem to have this feature set (or I am missing it). I've also seen usage of triggers, but we have a large amount of tables that would require monitoring and I don't know the performance degradation that might incur and if we can collect the appropriate data. I do like the trigger approach where it would insert the information into a table ([dbo].[TABLE_UPDATES]) because it would allow us to build an SSRS report to run against and pull data as needed, but I'm not sure I 100% trust this as a solution. We currently use RedGate DLM Dashboard for monitoring schema changes, and it has proven quite invaluable to receive those updates. We just want to expand it to the data itself. Disclaimer: I know - terrible practice by developers to apply things directly to a database without checking in their changes, but we are currently having to deal with this hurdle and reign these individuals from wildly shooting at the hip. We are wanting a long-term solution for accountability purposes as our database and company grows. Thanks in advance. 

I have a question about whether or not it is necessary to back the tail log up when performing a restore of a database in Full Recovery mode. I've sprawled for a good while now, and have gotten conflicting reports. Consider this scenario in SQL Server 2012: 

This is when the error described above pops up. Based the error itself as well reading the Technet arguments, I understand the issue to be in utilizing . Since the partition action of the transaction relies on utilizing the range value within the partition function, the fails, and thus the entire transaction as well. 

As an addendum to Brent Ozar's plan of action, I wanted to share the results. As Brent noted, within VMware we had configured the Virtual Machine improperly with 12 single-core CPUs. This resulted in the remaining 8 cores being inaccessible by SQL Server, and as a result, led to the memory issue described in my original question. We placed our services in maintenance mode last night in order to reconfigure the VM appropriately. Not only are we seeing the memory creep up in a normal fashion, but as Brent also hinted, the number of waits went down exponentially and our overall SQL Server performance has skyrocketed. The vNUMA configurations are now happy little components that are slicing through our workloads. For those that might be utilizing VMware vSphere 6.5, the brief steps to complete the action item described by Brent are as follows. 

I have a column called that I am trying to evaluate an check against to return a blank value for instead of . A snippet of the statement: 

This returns: My clause clearly brings back only the rows that are explicitly , so it is not an issue of being an empty string and evaluating to . 

How can evaluate correctly but fails? Per MSDN, should evaluate the literal value correctly and return an empty value. 

I am utilizing partition schemes in order to logically isolate data based on a column. is the partition column and clustering index across all tables that require tenant isolation. I would like to create unique, non-clustered indexes on the table that also holds and . I cannot create the unique indexes on the partition scheme unless they are a part of the partitioning column as a composite primary key. This fact leads me to worry about performance implications (especially in larger contexts), but this may be a misguided thought. Even with the potential performance implications, given that the combination of , , and will always be a unique composition, I'm falling towards including the non-partitioning columns onto the clustered index. However, I would like to know if it's possible otherwise, if there are detriments to not including it in the composite key, or if it's better to avoid placing them on the partition scheme at all. For optimal context, let's set up the security schema, file group, partition function, and partition scheme to a database called . 

Within the SQL Server Import and Export Wizard, select the tick box next to Source on the page. Once selected, click . This will bring you to a dialog box that allows you to . 

However, even after dropping the trigger, restarting MSSQLSERVER, and even the entire server, events are still being captured. I dug even deeper and I browsed to within the registry, and the was not explicitly specified. So, I manually added it with a value of for . Restarted MSSQLSERVER again to no avail. Set the value to for and restarted... still capturing all login events. What else could contribute to this behavior? 

I'll end on a capture from RedGate SQL Monitor over the past 24 hours. The primary point of note is the CPU utilization and number of waits - during our peak hours yesterday, we were experiencing heavy CPU use and wait contentions. After this simple fix, we have improved our performance tenfold. Even our disk I/O has reduced significantly. This is a seemingly easily overlooked setting that can improve virtual performance by an order of magnitude. At least, it was overlooked by our engineers and a complete d'oh moment.