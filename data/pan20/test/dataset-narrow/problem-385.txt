In your particular case though - you are moving from one special privileged account to another. As you have seen - you were safe and things worked. I would also take this time to question the choice of the service account. Are you on a domain? Why not use a domain user account per the best practices? The local accounts with special privileges are not the most secure or reliable service account choices. This article from Microsoft explains more there. 

Old question, I understand, but an answer in addition to the above answers exists since SQL Server 2016 SP1. SQL Server 2016 introduced "Always Encrypted" as well as other security features like Dynamic Data Masking and Row Level Security. These were features originally released in Enterprise Edition Only. They now exist as features available in any SQL Server 2016 SKU - Enterprise, Standard or even Express. Worth checking out and worth looking at the upgrade process now. You can read more about this on Microsoft's post about SP1 for SQL Server 2016.. 

And when you do some of these tests, have your application running (not live production...) with users or test scripts performing work during the failover. What happened? See anything that needs to be taken care of? 

Jon gives you some links to SQL Server audit information in the comments. That is a definite way to see who is doing what and when on a server. Depending on what you are looking for that may be a good enough answer. Auditing isn't free, though. Capturing some of the data required can be expensive to trak and take some time to track. One approach here - depending on what you are looking to do with this information is to look at the DMV - this DMV shows you various statistics about access (insert, update, delete, scan, seek and lookups) for your indexes. You can join this DMV to tables that contain information about your tables or views like sys.tables or sys.indexes to see the details. There are some caveats about this approach (data gets reset when SQL is restarted, there is a bug in SQL 2012 where the stats are apparently reset after an index rebuild, and other caveats described by Joe Scan in a couple posts) You can see an example of querying this data in this post from 2007 written for SQL 2005 but still most applies today. 

Then from within the actual exports, for Database B, the times reported by the LOGTIME parameter all give a time of a few seconds per table, but the datapump reported times are all 0 seconds: 

I have an Oracle Reports server 10g (10.1.0.4.2) that for the last few years has been running okay with the rwclient.sh client application residing on the same host. Due to some performance issues caused by long running/heavy reports we are looking at trying to separate the reports server from the application server. From memory rwserver.sh should be able to accept connections from a client on the same LAN, but not on the rwservers localhost (but I'm going back many years since I saw that sort of setup and I was just a developer, not an admin, so I could be misremembering it). When I try and submit a report across the network I get: 

I have two 12.1 databases on the same server that I've set up with rman and have been running okay for the last year or so (with numerous restores to a seperate test server - so at least restore/recovery wise rman is set up okay.). These databases are both around 100G in size, with the recovery area (originally) sized at around 200G. (Currently bumped up to 300+G to give a little more lee-way time whilst I work out what's going wrong). Just recently one of the databases has started filling up the db_recovery_area with backup pieces - normally this has been fairly static increasing over the day and then dropping back to zero when I run the rman level 1 backup over night (or level 0 over the weekend). However, over the last couple of weeks, the recovery space has trended upwards eventually getting to the point where I need to delete all backups/archivelogs out of rman to avoid a production database freezing due to lack of space. There's been no configuration change or upgrades on this server for some time and I can't work why the behaviour has changed - but only for one of the databases. Looking at V$RECOVERY_AREA_USAGE the only thing consuming a substantial amount of space is the backup pieces. 

I have this sub-procedure in a fairly large program, and it is taking hundreds of times longer than comparable sub-procedures. Is there any way I could improve efficiency here? None of this procedure's sub-procedures are absurdly slow, so I am sure that it has to do with the structure of this one. This program looks through a table containing hierarchies of doctors. If it finds a matching root doctor, it adds to that root. Otherwise, it tries to find a matching not-root doctor. I think this might be the source of the inefficiency; the fact that it's opening two very similar cursors and looping through both quite often. Additional info: UNIQUE_GOOD_MATCH is, on average, taking about four seconds. UNIQUE_PHYSICIAN has roughly 200k records, and is indexed by first and last names. It doesn't seem to me that it should be taking this long, especially when other procedures are completing at a fraction of this time. 

I have a query which generates a result set (key, a, b). I want to use an update statement where, for each entry in a table with Tkey = key, set ColA = a, ColB = b. How would I do this? Right now I'm trying: 

I've got some resource leaking and I'm trying to find what's causing them. I think I have some statements or resultsets that aren't being closed properly. Is there any way to see all of the cursors that my JDBC connection currently has open? I am using Eclipse. 

but I can't see any options in there that would override the MAXPEICESIZE To be clear - I'm NOT concerned about the location just the file sizes. Is there another setting somewhere that I am missing? Do I need to remove the db_recovery_file_dest parameter maybe? Thanks Backup summary of the latest backup: 

I'm running a 12.1 SE database on Oracle Linux 7. I am getting nightly errors/warnings from my rman scripts about missing datafiles when trying to recover. I have been running rman to a local drive using the following script: 

(The java processes above are all for ) I assume the 0.0.0.0/127.0.0.0.1 multiple ports things is why the server can see the initial probe, but the client then fails to connect properly. I suspect there's got to be an option somewhere that determines what interface the server tries to bind to but I can't find any option in the report server .conf to enable this. Questions: 

I'm having issues with Oracle 12.1 rman - specifically the MAXPIECESIZE parameter not being honored by my Level 0 backups. This value is set in rman as 

Looking at the output of , there seems to be number of ports opened by the reports/java process - most of them open to 0.0.0.0, but a couple are bound to 127.0.0.1: 

If I run a crosscheck archivelog all, this picks up only the recent archivelog files that I expect to be there: 

Here's the weird thing, the following query DOES work, the only difference being whether rating needs to be larger than or smaller than 1. Suffice it to say that I'm utterly perplexed. Is this a bug? I'm on 11.2.0.4.0. Could it be something to do with JARO_WINKLER? 

I'm using UTL_MATCH's Jaro Winkler similarity function and it seems to be performing well. However, I would like to adjust the prefix scale according to the situation. Is this possible? Is it possible to see what the default prefix scale is? I could not find any documentation on this, but it seems that in order to be a J-W distance, it must use a prefix scale. 

Sorry if the way I phrased that is confusing. I'm writing a program that acts kind of like a filter - it retrieves an answer to a query, counts the rows, then I want to do a slightly more narrow query. I want to do those queries until I get a count of 0, which means the query just before it is the narrowest I can get. I then use a similar version of that query, except instead of counting I actually retrieve the records. The best implementation I know of is to just write all of the different queries, put them into different statements and feed those statements into different result statements, and retrieve the counts from there. Then, I use the last one that counted > 0 and use that main query without a count, and use those results. This seems like a horrible implementation. It seems like I'm just going to be bombing the database over and over. I'd prefer to hit it once, and then narrow the resultset on the client. Any ideas? 

I've got two databases running on the same 12.1.0.1.0 oracle home on the same machine, on the same disks. Each database has multiple schemas of approximately the same size. On database A, the datapump exports take on average of about 2 minutes. On database B, the exports take on average, about 20 minutes. These exports get run twice a day, and the performance has been consistent for at least as long as I've been paying attention. I've also tried different times of day incase it was related to system load, but that seems to make no difference. The configuration for the databases are largely the same, the only differences I can see are file locations, and the pga limit/target, both of which are big enough for the respective databases, and the log_buffer (which I don't think should impact exports, but maybe?) I've added METRICS=y and LOGTIME=all to the backup scripts - that revealed some interesting timings: Database A (fast) 

After the same number of level 1 backups, the backup piece usage is sitting at a few meg, instead of almost 100G as is the misbehaving database. Why are the backup pieces on this database sitting in the recovery area eventually filling it up completely? Is this normal behaviour with the other database being the abnormal one? This is not a high traffic database - should I need to have the recovery area set to more than 3 times the size of the database? Thanks 

I want a PL/SQL procedure to give me progress updates as it runs. However, DBMS_OUTPUT seems only to give me the output when the whole procedure is done. Is there a way to make it give me updates during runtime? Thanks! 

I want to determine if a resultset exists very quickly. At the moment, I'm doing a count - this is taking roughly 55ms, which is unfeasable.The table has ~100k records - I don't care if it has 2, 5, 100k rows that fit a query; I care if it has 0 or 1. Maybe 2 in certain situations. Is there a way to do this? Would limiting a count using ROWCOUNT (so it only counts about the first 2 rows it finds) speed up the count at all? 

Will this trigger propagate? Like, if it updates a ROOT_ID for another record, will that trigger its own trigger? Further, if it does trigger that, will it use the new ROOT_ID? I want the ROOT_ID to propagate down the tree I've built. Edit: How this works is that each record has a unique ID, a parent ID, and a root ID. I basically have a tree, each member of that tree has a root_ID pointing at the unique ID of the root and a parent ID pointing at the one above it. The root's root and parent IDs are its own unique ID. in the case that a user manually changes a record to point at a new root and parent, I want all the children of that node to have the new root ID. Is there a better way to do this? 

(ED- Another note - I often don't have an "S" drive available. At the end of the day, having your system database files for Master, Model, MSDB and Resource db living on the same drive as some of your user database files, but in a separate folder for logical separation to keep things less confusing isn't the end of the world.) 

Unfortunately, no. You are installing a program and making service and registry changes, etc. You need to run setup as an account that has local administrative rights. No workaround to this that wouldn't involve some back door into administrative rights. Best bet is to ask for someone to temporarily give you the rights or install it for you. Source: The Microsoft SQL Server books online article for how to install SQL Server 2008 R2Express 

Short Answer: Nothing at all is wrong with a domain account. When all the computers participating are in the same domain, go that way. Easier to manage and maintain and secure. Your confusion here lies in the way the documents you are looking at are structured. In the document you reference in the comments above you see this section: 

As far as I know, there is no hard "limit" in terms of some absolute value of time that the optimizer searches for a plan. Each execution is budgeted some amount of percentage of time to explore the plan options. There are a number of factors that go into this - I trust that those factors are always changing somewhat behind the scenes between versions of SQL Server - there are optimizer improvements made and some new options or approaches to options could come out, the budget calculated could change, etc. SQL Server (thankfully) uses a process where it looks for a "good enough plan" or a reasonable plan. The more complex your query is the more insanely high the number of permutations of choices the optimizer has to execute your query. So we want it to find a reasonable enough plan and move on - so our query can actually start executing. The whole point is "spend enough time to give me a plan that should work alright, don't spend too much time optimizing and exploring options" We really don't have control over that. And we really shouldn't. We should look to the plan the optimizer came up with and focus more on tuning our queries, tuning our designs and tuning our indexes to best serve the business need. Another way - in about 15 years of SQL Server Tuning, I've not really ever had to consider "darn it, if only the optimizer had 4 more ms of optimization" - normally all you need to know to make your queries faster is right there in the query plan you did get, or exploring questions like "what about a serial plan instead of a parallel plan? what happens if I throw a join hint in just to force order and see what decisions the optimizer makes so I can rewrite accordingly? What about row goals? What if I split this logic up, etc" 

Is it actually even possible to connect to the reports server from external clients? and if so; Where abouts do I need to configure the reports server so it listens for external connections? 

I've enabled tracing on the reports server and can see requests on the server coming from the external clients (i think), but I only get one line per report run attempt. (But consistently get this same line so I think the rwclient is at least seeing the rwserver): 

On the other database on this server (that is behaving as I think it should), using identical config/scripts the recovery area usage looks like 

My understanding from the documentation is that this is normal on the first run (when there is no datafile image copy), and normal on the 2nd run (when there is a datafile copy, but no incrementals), but on the 3rd and subsequent run there should always be a datafile copy and an incremental to apply to it. This has now been failing for the last 6 nights. From $URL$ 

(in hindsight, symlinks may have been a better option but at the time I preferred to make the location of the backups on a seperate drive explicit/obvious). This channel change didn't seem to have any effect. Initially I suspected because I have a 7 day retention period set and the old image were still valid, however after 7 days datafile images were still being created in the old location. As diskspace was becoming increasingly tight, I changed the backup script to use a different TAG to try and force a new set of backup images to be create on the new disk. This seemed to work - the new set of backup images were created okay on the new disk that night, but subsequent recovery operations seem to be failing.