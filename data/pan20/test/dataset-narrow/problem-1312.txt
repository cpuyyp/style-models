We start out defining what scales we'd like to lerp between, then enter a loop. Here we scale using Vector3.Lerp and we increase the lerp fraction by the delta time / 2 ( so this takes place over two seconds. ) We use yield return null to tell Unity to come back next frame so the rest of the game isn't blocked. Do this until the object is fully scaled. I find this much nicer and easier to understand than managing a bunch of state in the update method. $URL$ $URL$ 

This is a bad way to go about it. Every time you want to move your model AT ALL you have to change every triangle. You should have one transform per mesh. I assume you have a model class, so what you'll want to do is give each model class either 3 vectors for scale, rotation, and translation or a single matrix. When you want to move the model you simply have to change the vectors or matrix accordingly. Now, right before you draw you should upload your transform to your vertex shader. Now you can use it as needed uniformly across all your vertices. This is a pretty standard way of dealing with this. A vertex should only describe it's local coordinates. That is, its position relative to other vertices in the model. You then construct your model matrix per model object, and your view matrix per viewport. Your scene should follow this structure: 

You need to translate them before scaling them. Otherwise, the entire coordinate space is scaled. For example: 

Unity will invoke the RPC method on ALL methods with that name that appear in any of the game object's components. Yes, you could do that, I personally wouldn't recommend it. It will add unneeded dependencies in your project, and pretty much destroys the point OOP. Right now the RPC scope thing is really quite terrible. Imagine if you have an RPC method in a base class called A. Suppose classes B and C inherit from A. Now, if you attach scripts of both B and C to a single game object( not unrealistic ), RPC calls to methods in the base class simply won't work properly. Fortunately, this seems to be getting fixed: $URL$ They seem to be sorting out the networking component of Unity. Hopefully we're not waiting too long. 

This would send an RPC to all clients who have this actor as relevant. Sometimes I need to do things like this: 

Is A guaranteed to be called before B on the "specificNetworkPlayer"'s machine? If not, what is the order, if any? 

You haven't shown any code, so I don't know for sure but I think your sprites have an origin that lies in the middle of it. Here is how the actual sprite is created in model space: 

I've been using Unity's component based design, and the biggest problem I've faced is how to control an object's( not component's ) state. The problem is, some components must act differently based on the object's state. However, checking an object's state within a non-specific component( imagine we have a "DamageEnemyComponent" ) won't work at all because it makes assumptions about the object it's attached to. The alternative is to make each component perform one, and only one function. The components are then disabled/enabled or added/removed depending on the object's state( probably managed by a more specific component. ) Imagine we have a "Boomerang" object. Assume it has only two states; idle( being held by the player ) and firing. In the idle state, the boomerang would have very few components, because we don't want it to interact with the world while it's simply being held. It may have components to listen for a "throw", though. Once it is thrown, some state manager on the object needs to update the object to have components for moving, damaging enemies, maybe picking up items etc. When the boomerang is back in the hands of the player, it's component list would need to be restored. This would work, but it would basically be building a new game object for each state, which is incredibly tedious and messy without Unity's editor. Not to mention the inheritance and interfaces can't be used well here( since the state specific components will be built into the state manager script. ) I can't see a way to control state using Unity's components. How is this usually done? It feels as if Unity should provide a way to build different states in the editor, much like building an object. 

Looks like you you have duplicated the background position twice in the else part. I've inserted the 2 below 

Scaling up any bitmapped image such as png will always look pixelated. To avoid this, if you have the memory, save the non-scaled image twice the size you currently have now and scale down 50% to get it looking like the non-scale image in the above diagram. To get it looking crisp at 2x-scale, you don't scale at all because it is already at that size. Your other option is to use vectors, but this can become expensive in terms of consuming processing time and may lack finer details compared to bitmap, i.e. look cartoony but that could be a positive on your game. You also need to find an appropriate library to do this. The advantage of using vectors is that it will most likely take up less memory and will look good, as long as it isn't too tiny, at any scale compared to using bitmaps. 

Chrome is likely to use hardware acceleration. Create a canvas 240x240 and run your experiment in Chrome then create a canvas 300x300 and do it again. The larger canvas I expect to be faster due to the fact hardware acceleration kicks in after 256x256 and chrome uses software when the sizes are less. Also it worth pointing out that -webkit-transform:translateZ(0) turns off hardware acceleration. I haven't tested any of the above; I only know this due to the fact one of the chrome engineers commented on a bug I reported in chrome when you cross the hardware and software threshold by dynamically resizing the canvas from larger to smaller than the 256x256 boundary or vice-versa. The solution to this bug was to turn off acceleration using the translateZ as mentioned above. In my case, I simply did not allow users to resize less than 256x256. 

You don't give enough detail for me to conclude if this is a viable method but I'll throw it in there anyway as a possible solution. Someone else may find it useful if you don't. You don't say how many zlayers there are and if there are only very few layers then this may be an option. What you can do is store more one tile data in a long variable by using bits. Let say you have 4 layers and 255 tiles per layer to keep my explanation simple. You can do that in a single long by using bitwise AND and rotation. first 8 bits could be layer 1, second 8 bits could be layer 2, and so on. in order to get the right tile in this particular data format for the layers you would do something like 

Otherwise CoolPlayList probably be treated as an SQL command and would come back with an error such as invalid SQL statement. 

I'm not sure why you think: As you can see, sooner or later, with many maps I will be dealing with many IDs. And that can possibly get a little confusing and hectic. LeftID will always be -1 of the currentID, the rightID will always be +1 of the currentID. UpID will always be -(total map Width) of the current ID and downID will always be +(total map width) of the current ID with the exception of zero meaning you've hit the edge. A single map can be broken into many maps, and are saved sequentially with mapIDXXXX where XXXX is the id. That way, there isn't anything to be confused about. I've been to your site and it seems, I could be wrong, that the problem is down to your map editor imposing a size limitation which is hindering your automation to break down a large map into multiple small ones on save and this limitation is probably restricting a technical solution. I've written a Javascript map editor which scrolls in all direction, 1000 x 1000 (I wouldn't recommend that size) tiles and it still moves as good as 50 x 50 map and that is with 3 layers including parallax scrolling. It saves and read in json format. I'll send you a copy if you say you don't mind an email approximately 2meg. It is meant to be on github but I haven't got any decent (legal) tileset which is why I haven't bothered to put it on there yet. 

Edit: Additional Information Here is the code in the draw method i am using. The un-commented part is using my shader, while the commented part is the BasicEffect that is set up when the model is loaded. 

I think it would make the most sense to use percentage based reduction, if you don't want to remove damage entirely. For example different pieces could provide different percentages of damage reduction: 

As you can see I've temporarily commented out the line which uses the texture color for shading and tried to replace it with vertex color. I've also highlighted in the code which lines i've changed to attempt to use vertex coloring. At the moment this only produces grey shaded models. I know there is vertex data on the models because i can use MonoGame's built in BasicEffect to see the coloring. What am i doing wrong? How can i modify the existing shader to use vertex color instead of texture color? Edit: Additional information I've tried using the BasicEffect in monogame to draw my objects and the vertex colors are coming through fine. It seems that i should be able to see the colors with my shader but i am not. With the BasicEffect 

I should mention that I'm pretty new to Matrices and will probably need a layman's answer to my question. I have a 4D matrix which represents the rotation of an object in 3D space. I have a normalized 2D vector which represents the direction i want the object to be facing by rotating the Y axis. I need to be able to change the Y axis rotation of the object only, by applying some sort of calculation to the XZ vector i have. What do i need to do to make this work? And if possible, can any answers come with an explanation of the process so that i can do it again on my own in the future! 

Has anyone ever come across simillar errors using the DirectX version on MonoGame and SharpDX when dealing with primatives? And do you know of a way I can fix this issue? 

For example this will give you the following for some example levels: Level 2: (rand = 1) min = 3 max = 4 Level 10: (rand = 4) min = 14 max = 18 Level 10: (rand = 1) min = 11 max = 12 Level 80: (rand = 35) min = 115 max = 150 If this doesn't meet your needs i can work up another solution 

Total reduction = 35% Then for example if your character is hit for 40 points of damage you would do the following: 

Below is a small section of code that is meant to draw a multicolored triangle to the middle of the screen in a MonoGame game. This uses the basic function to draw the triangle. This works fine in XNA, but the moment i try to use it in a MonoGame DX game (Which compiles using SharpDX) it will crash with the following error: An unhandled exception of type 'SharpDX.SharpDXException' occurred in SharpDX.dll Additional information: HRESULT: [0x80070057], Module: [General], ApiCode: [E_INVALIDARG/Invalid Arguments], Message: The parameter is incorrect. Below is the code that is causing the error. 

I should start his question off by saying that i am a begginer in HLSL. I've borrowed some shader code for flat shading from a website i've long since lost. It's works brilliantly for textured models but i cannot seem to get it functioning using vertex colored models. it just seems to always be grey. My HLSL shader is as follows: 

This gives you the damage value of 26 that your character would take Edit: having read the comments of the Op. There will have to be a hard cap on the percentage avoidance if he wants to avoid 100%. The other way to do this would have a maximum avoidance for each type of armor that does not add up to 100%. As it is also not described how these armor prices come to be. If they are pre determined values then it would be trivial to make sure they don't add up to 100%. If they are randomly generated simply put a cap on the maximum for each type.