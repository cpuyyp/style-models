You have defined in two different blocks. Since the intent of one of them is to redirect traffic to , having it there is clearly erroneous. 

It can't start because it's already running (and the pid file somehow doesn't reflect that). Kill it yourself, then restart it. 

Your system is having some sort of problem with your Red Hat subscription. Make sure you have an active entitlement assigned to the machine, and if you do, contact Red Hat to resolve the problem. 

Use a Linux desktop. Every time I attempt to do something with PuTTY I am utterly frustrated by the constraints imposed on me, both by PuTTY and by Windows. If you have to manage Linux boxes, doing so from a Linux box gives you much more opportunity for better integration. The mouse works as expected, for instance, while it doesn't in Windows. Transferring files is more straightforward. And so on... 

Actually neither. FCRDNS stands for "Forward Confirmed Reverse DNS". How this works is that, first, the PTR record of the connecting IP address is looked up. If it hasn't got a PTR record, then the check instantly fails. A mail server with this feature on will then reject the connection, such as this rejection taken from my own mail server: 

You would use to determine that a packet originated locally on the system, where outgoing packets are associated with a socket, instead of a forwarded packet that originated from another system, which has no associated socket. 

Second, I'll check the whois record for the address. APNIC also says it's registered to UTM. Not looking good for your supposed American... 

This is normal and expected behavior. Remember that sanitizes the environment when switching users, which is why you end up with a minimal default PATH. The remainder of the items in the PATH come from shell startup scripts, which are not being run when you (or rather Ansible) call , because it is not asking for an interactive or login shell. If you have to run a command which isn't in a default location, supply its path explicitly. 

OK, so it looks like you can use the command line utility to force the state of the logical drive. An example: 

Like many other embedded systems, OpenWrt uses dropbear as its ssh server, not the more heavyweight OpenSSH that's commonly seen on Linux systems. Older versions of dropbear only support RSA and DSA keys; support for ECDSA was not added until version 2013.62 (which has only just been released a few days ago). It should show up soon in Barrier Breaker (trunk); but you will not see it in Attitude Adjustment. 

Windows Server 2003R2 is past end of life. Microsoft will no longer support it, but it will certainly run (as a Generation 1 virtual machine only) on Hyper-V 2016. Of course, this is a good time to plan your migration off of the obsolete server platform. 

You've set up the installation CD, or some other external media, as an installation source in . Just comment or remove it. 

Change the server's mailer so that its outgoing mail uses a domain you expect. Set up an SPF record for prod.xxx.com. 

For testing, it is probably fine to do this. But a real production system will likely behave in the manner you experienced, and your program needs to be able to deal with that. 

EPEL is not compatible with Amazon Linux. It used to be, back in the very early days when Amazon Linux looked more like CentOS, but it is too different now. 

You may get argument about this, but... I wouldn't bother on a local network. Compression just burns up your CPU and probably won't save you enough time on the backup to justify it, since the network link is probably faster than you can push data through it, compressed or not. Now if you were running rsync across a WAN then it would be a good idea; since the network link is much slower you would save a lot of time. 

ramfs has absolutely no mount options, so it isn't possible to set a mount uid/gid as it is with tmpfs. From the man page: 

You set the policy of the chain to , so even if nothing in the chain matches, the traffic will be allowed. You probably should set it to instead. 

will monitor the memory controller and report memory error events to syslog, and in some configurations can offline bad memory pages. This is, of course, in addition to its usual use to monitor machine check exceptions and a variety of other hardware errors. Most Linux distributions have a service set up to run it as a daemon, e.g. for EL 6: 

First upgrade your kernel. That particular kernel contained a bug which caused various ioctls to print those warnings (and maybe fail) in certain mdraid and LVM configurations. If a fixed kernel doesn't resolve the problem, run an extended self-test on all your drives. Note that the self-test may take several hours for each drive and will degrade performance slightly while running, so should be run at a time of low system activity. For example, to schedule the self-tests to begin at 11 pm: 

Your output shows that you don't have an SMTP server listening on ports 465 and 587. Reconfigure the SMTP server and try again. The same applies to your web server and port 443. 

Your remote server is using UTF-8 as its character set, as nearly all modern systems except Windows do, but neither Console2 nor msysgit support UTF-8 fully (or at all). The solution, unfortunately, is going to be to replace one or both of these tools. In the case of msysgit, there is a UTF-8 capable fork available. 

This is as shipped. Check to make sure it wasn't altered. In my own PHP scripts I take a very simple shortcut to check if it was a POST request: 

Given the question in the documentation there, it would be wise to verify this applies to your specific (older) kernel before proceeding. 

The cache will then be bypassed for the IP address 192.0.2.81. You can add as many IP addresses or CIDR ranges as you wish in the block. 

The simple answer is "set the date manually", which you need to do, but to prevent this occurring again, there is more that you should do. 

On CentOS 6, bash-completion is in the EPEL repository. You either forgot to add EPEL, or your manifest is trying to install bash-completion before adding the EPEL repo. It's in the base repositories in CentOS 7. Oh, and your file ownership and permissions are wrong, but you probably knew that already... 

Based on the semantics of what these URLs do, I think it would be best if nginx did not attempt to cache these requests at all; rather, to always pass them up to your application, so that you can do the appropriate tracking. Fortunately this is a one-liner: 

You can use on any disk image format that your version of qemu understands. This usually includes raw disk images, qcow2, VMware VMDK, Windows VHD, VirtualBox VDI, and many others. 

This specifies to create a directory , with mode 0755, owned by root and group root. The directory will be created at system startup or whenever the service is restarted. You can also run manually. There are many other options available; check the tmpfiles.d documentation for full details. 

The symptoms you describe are a classic case of your DNS servers not responding. When none of the DNS servers are available, then hostnames cannot be resolved to IP addresses, and you can't reasonably connect to anywhere (at least by name). It appears that you're using DNS servers given to you by the datacenter where your server resides. The reliability of such servers is often questionable. Try using known-good servers such as Google Public DNS instead. 

The usual answer is fail2ban. You may have to customize it a bit in order to read your web access logs. 

You're running CentOS 7, but you installed EPEL for CentOS 6. This obviously is not going to work. Remove it and reinstall EPEL for CentOS 7. 

simfs is not an actual filesystem; it's a map to a directory on the host (by default ). To check the filesystem, you have to check the host filesystem from the host, which also means you have to bring down every container on the host. If you believe it's necessary to check the filesystem, schedule a maintenance period and notify all the customers with containers on that host. 

Anything that will run Windows 8 is already 64-bit capable, unless you happen to have some first-generation Intel Atom netbooks (and I doubt that very much). That's about the only thing I can think of. AMD released its first 64-bit capable Opteron in 2003; and since then virtually every processor they have made has been 64-bit capable. Intel was a year later, releasing its first 64-bit Xeon (Nocona) in 2004, and expanding to just about the entire product line by 2006. Aside from the aforementioned early Atom chips, every Intel processor today is 64-bit. Wikipedia has a broken down processor list if you're interested in ancient history. 

Check for the existence of a file named . If this file exists, and contains an email address or local username, then incoming mail for the user who owns the file will be forwarded to the address given. For instance, it might look like this: 

And as a bonus, TZ will be set correctly in the container as well. This is also distribution-agnostic, so it works with pretty much anything Linux. 

The MX record should be for your domain, not for a subdomain. An MX record should not point to a CNAME. Point it to a record with an IP address. 

You can't have more than one opcode cache loaded at the same time; only one or the other will actually run, and the other will throw an error when it attempts to load. You will need to remove APC when you install OPcache. 

To compile programs from source code, on RHEL systems, you need to install their corresponding packages. For instance, for SQLite support, you need to install . 

You need to use instead of here. Your existing configuration tries to load , which is probably not what you want. Regardless of which it's in, always specifies the document , i.e. the location of . Use instead to specify the location of the . 

The best explanation I have seen is in the introduction to RFC 4862 (which you should read in its entirety later): 

However, this fails, precisely because it causes to fail. Since the script doesn't actually check the return code from carefully enough, if you actually set then the initialization fails with an incorrect error message claiming the IP address is already in use. 

Now the problem is obvious: The directory doesn't have search permissions for anyone but the owner. You obviously would fix that with . You also need to check the SELinux boolean . Without this boolean, a web server won't be able to access anything in user home directories.