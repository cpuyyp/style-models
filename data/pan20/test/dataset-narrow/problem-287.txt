Why are you optimizing for random read? These values are way too high even for a very heavily used databases. You can easily drop these under 1MB. Maybe 256K as a start and increase if necessary. Same goes for and . 1Mb for the former and 128kb for the latter should be sufficient if your tables are well indexed for the common queries. Especially with these values can easily run your out of memory. +) Innodb 

I tend to use both 1) and 3) depending on the actual situation. 1) for situation where I can either inherit the model from the original or I don't actually need the original model I can just have a new one. 3) when I need the original model. I tend to put the queries into files in SQL directory in the app (just like templates). So I have a clear indication that I'm doing something database dependent and I can easily switch/rewrite them later if necessary. I hope this helps. 

As long as the bottleneck is the amount of RAM you can keep scaling up. MyISAM has some bug related to large key buffer size but InnoDB handles massive buffer pool just fine (even in the TB range). I see you already have 4 buffer pool instances. By increasing the buffer pool it's good to increase this too. The only "issue" you may experience is if you also have larger innodb log files and more dirty pages in the buffer pool then stopping the database may take longer and in case of a crash innodb recovery will take longer. For scaling out the most efficient simple measurement is the concurrently running threads: You can take a benchmark and see where is the tipping point for TPS on your system. (Usually with increasing concurrency you will see transactions / second increase for a while until it hits different contention issues and then drop usually quite quickly, you can see a sample benchmark and what the results look like here: $URL$ You want to keep it always under that level. Measuring this will give you a good indication of when will you need new server and how many. 

I would say this is easier on the representational layer but if you want it in pure MySQL this will do it: 

Set up slave You can set it up from the nightly backups. That's actually a common practice to provision slaves from backups to also have them tested. You can go back as far as you have binlogs stored on the master (expire_log_days variable). The impact of having a slave starting from older backup is that it has to fetch the logs from the master but that shouldn't be a heavy operation. Unless your master is already on the edge you shouldn't really notice it. The other impact is that it will take longer for the server to catchup which is of course expected. Switchover Switchover can have some impact if you use auto-increment IDs and your passive master is not fully caught up with replication. Best is to make the old active read-only for a second or two while checking the passive for master position. Once there is no change you can move change it to be the active and make it writeable. Depending how you point to the active master (DNS switch for example) it might take some time during which time both masters will accept writes and the old active being read-only will report errors. I hope this helped. 

In more details: [^1-5] boxes matches because 7 is not a number between 1 and 5 followed by a spaces and the word boxes matches because 4 doesn't match (regular machine not in the first step) -> 8 is not a number between 1 and 5 followed by a spaces and the word boxes the actual match is: [^1-48] boxes Doesn't match because the rules specifies now characters from the set of (1,2,3,4,8) and the string fails on this check. Fix your regex In your case probably you want to use 

Ibdata1 file holds a lot of information and allocates space for different innodb functionality (double write buffer, undo logs, tablespace header, etc.). Therefore even with you cannot drop/replace the ibdata1 file. One common reason why ibdata can grow is because it has the undo space. If you have long running transactions the previous version of rows can pile up in the undo log easily. More about this can be read on person's blog: $URL$ In MySQL 5.7 there are a lot of improvements around undo tablespace management including truncation and separate files to hold this. 

Using InnoDB the rule of thumb is always use auto_increment primary key and only deviate from this rule if you explicitly want to and know why you're deviating. What you may save with that 4 or 8 bytes / row you will lose more on fragmentation and bigger index size. The only time you really benefit from a larger composite or natural primary key is if that's the only way you query the table. If any other secondary indexes comes into play it's better to have that single column autoinc PK. 

Note: in reality you would probably store the 30 bits in an unsinged int so it will take 4 bytes. So on disk it would consume around 390kb. 

Disable query_cache: you don't get too much benefit from it anyway and it's a common contention point. Turn on slow query log with analyse your queries and fix your unindexed joins. From your mysqltuner output: pt-query-digest can be a great help in that. Look at the mutexes a.) Check the output of b.) If you have performance_schema and mutex_instances table check . If not I strongly recommend to set it up. Maybe your queries lock up each other. And simply semaphore spins use up the cpu time. Fix any obvious queries showing up as a possible "intruder". You can look for select for updates, insert .. on duplicate key update .. queries which are very common cause of this behaviour. can give you more information where mysql spends most of the CPU time in. If you get to this point you can form a much more specific question about how to overcome that certain problem. 

2015-11-22 is not older than 6 months compared to 2015-05-01 unless you truncate the response date to month also. 

The 80% is just a general rule of thumb for helping people start with a more sensible value than the default. Buffer pool size calculation can actually be more complicated. Ideally frequently accessed data pages should remain in the buffer pool and you don't want too much eviction to happen because of its impact on performance. This is again generic but a more realistic measurement to have the buffer pool large enough to accommodate hot data + enough space for avoiding unfrequent reads triggering too much eviction. Just because you have a database 1TB big you don't have to have 1Tb of buffer pool if you only read the same 100MB all the time. Keep in mind also that inserts and updates also affect buffer pool usage because the operation happens there. You should try to match the amount of data you can write into innodb logs (controlled by ) and the space required for this in buffer pool. Even if you have large innodb log files but you don't have enough space for insert than eviction will cause flushing data pages to tablespace which in essence have the same effect as checkpointing. One empirical way to correctly size buffer pool is to set as high as your datasize (or as large as possible) and run normal production load on it while decreasing the size constantly while measuring transaction rate, response time and . When you see performance starting to drop that's the size that you need at least. 

1) No. You have to restart MySQL and that means downtime. 2) To minimize the downtime you need an identical server which you can switch to and back. The procedure on high level: 

As @jkavalik said with your current setup the only option you have is to recreate the master as the slave of the new master and do a switchback if necessary. Keep in mind that you will have some data loss because you can have some unreplicated transactions on the failed master unless you enable semi-sync replication. 

Before you do that you may (also) want to look at partitioning your table or using BRIN indexes depending on the use case. 

These are the most common situations for splitting I can think of right now. The question is do you experience any issues right now? Is there an explicit need to fix something? Can it be done by other ways of optimization? Configuration, indexes, query tuning, etc? Testing You can test your query time by replaying queries from tcpdump or slowlog. You can also enable binary logs and replay insert/updates from ther eand see how fast they finish. This is going to be a balance between many things including operability of the system so find what's work best for your use case. I hope this helped. 

There's no silver bullet to tell you how many queries per second can you process. You can set up different benchmarks to try to estimate. A 'good enough' approach is to register the queries with tcpdump and replay them from external server(s) from multiple parallel threads. You can also play with the concurrency. Register the query response time and plot it as the function of queries / second. When response time is above acceptable threshold that's your maximum queries / seconds for specific concurrency level. I wrote a script some time ago to do the same thing with HTTP requests (the collection is not part of it) and report maximum requests / second: $URL$ It shouldn't be hard to translate to mysql queries. If you use replication to balance reads you can also start reducing the number of slaves until the query response time is above the threshold and check then the current query rate is your limit. You can only reduce the number queries if you change the application that sending those queries. Introducing caching, rewriting queries, etc. There are plethora of options but that really depends on your case. Maybe it's completely valid from your application to send that amount of queries and you just need to scale up or out your infrastructure. 

Depends what do you want to optimize for. For smallest space consumption the easiest way is to create a field where you store the features as bits. For example: 

The performance difference can be significant. For more info you can take a look at this benchmark: Group by limit per group in PostgreSQL In SQL Server: I'm not a user of SQL server but as far as I know it supports both. The syntax might be a bit different but the same idea applies. 

The insert time could only decrease not increase by having more tables. Insert (and update) time can be improved by having more smaller table because the B+Trees become smaller. Given which storage engine you use it also matters if you insert in primary key order or random order. In case of InnoDB if you insert in random order there is a performance penalty of constant B+Tree rebalancing. In that case smaller tables could help. However it does involve more logic on the application side and I don't know how much overhead that would be for your developers. Table partitions could be a middle ground here. Do you delete from these table frequently? If you do purging whole tables (or partitions) is much easier and can be done without affecting your production queries and replication (if you have) while has to lookup the rows, it involves some locking (level is dependent on storage engine dependent), etc. In this case splitting is certainly beneficial. Do you want to archive the data? MyISAM tables can just be simply copied over to another mysql server and it will just work. This is a very common and easy way to archive old data to a different server. Splitting to tables by date work quite well in this case too. Do you change your table structure frequently? ing many smaller table is more manageable than one huge. 

The same query cannot be produced by Django ORM. You can only use raw. However there are some options to still keep the benefits of the ORM. 1) Using a view 

Uncommitted data is what the name suggest: uncommitted. This means the transaction that was running failed when MySQL crashed. Therefore the data should not be available because it could cause inconsistent state. Any ACID compliant database should roll back these transactions. Ib_log files do contain uncommitted changes to the tablespace but chances are these are already checkpointed and overwritten many times. There is a possibility to get back (some) uncommitted data from the ibdata and the .ibd files. For this you can use something like innodb_ruby to read the pages on disk and the history for each row from undo space. Be aware this will be a tedious work though. If MySQL was started it will do the cleanup and rollback from the undo space which means pages in .ibd files will get back their original (last committed) state and your undo history will be purged. After this all the uncommitted data is lost for good. ps.: If you really have transactions running for 10 days fix the application first. Of course this depends on the use case but majority of transactions shouldn't run longer than a couple of seconds. 

Slow log also has a bit more information than general log. 3# You can use tcpdump and percona-toolkit to retrieve the queries: 

That's not specific to replication. It happens even if you would want to restore it to an empty db. On the replication side 1) One way to do it is mysqldump but you need to LOCK the tables or make sure you don't get any writes during the process of dump (read_only=1 for example). Otherwise you get your slave out of sync from the start. Also there is certainly an impact on the application which is connected to the master. More on this you can read on the mysql doc: $URL$ To same or higher major version (these cannot be done if master is 5.6 and slave is 5.5): 2) You can also retrieve the data without impacting the master with hot backup solutions like percona Xtrabackup ($URL$ This is probably going to be faster as well. Once you applied the logs it behaves as normal mysql binaries and you can do the change master command. You will have an xtrabackup_binlog_info file which contains the master position you need to point the slave to. 3) You can stop the master after acquired the master position, copy the binary to the slave, start mysql and change master to ... with the position you saved. This could be also done by snapshotting the filesystem (LVM, ZFS, BTRFS, etc.) if you want to quickly restart mysql. Usually this is the fastest way. If your version mismatch you might need to run mysql_upgrade since with the latter two you're copying binary data not logical. A sidenote: Why are you replicating from 5.6 to 5.5? Replicating from higher version to lower is not guaranteed to work.