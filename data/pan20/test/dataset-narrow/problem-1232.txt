This is really a comment but too long to post as a comment. I suspect that the representation theory of the symmetric group might be useful to prove a better lower bound. Although I know almost nothing about representation theory and I may be off the mark, let me explain why it might be related to the current problem. Note that the behavior of a circuit consisting of probabilistic swap gates can be fully specified as a probability distribution p over Sn, the group of permutations on n elements. A permutation g∈Sn can be thought of as the event that ith output is g(i)th input for all i∈{1,…,n}. Now represent a probability distribution p as a formal sum ∑g∈Snp(g)g. For example, the probabilistic swap between wires i and j with probability p is represented as (1−p)e+pτij, where e∈Sn is the identity element and τij∈Sn is the transposition between i and j. An interesting fact about this formal sum is that the behavior of the concatenation of two independent circuits can be formally described as a product of these formal sums. Namely, if the behaviors of circuits C1 and C2 are represented as formal sums a1=∑g∈Snp1(g)g and a2=∑g∈Snp2(g)g, respectively, then the behavior of the circuit C1 followed by C2 is represented as ∑g1,g2∈Snp1(g1)p2(g2)g1g2 = a1a2. Therefore, a desired circuit with m probabilistic swaps exactly corresponds to a way of writing the sum (1/n!)∑g∈Sng as a product of m sums each of which is of the form (1−p)e+pτij. We would like to know the minimum number m of factors. The formal sums ∑g∈Snf(g)g, where f is a function from Sn to ℂ, equipped with naturally defined addition and multiplication, form the ring called group algebra ℂ[Sn]. Group algebra is closely related to representation theory of groups, which is a deep theory as we all know and fear :). This makes me suspect that something in representation theory might be applicable to the current problem. Or maybe this is just far-fetched. 

Yes. Let O=RE∪coRE, where RE is the class of recursively enumerable languages. Then O is closed under complement and O contains L. However, note that LO=LRE, and in particular LO has a complete language under polynomial-time many-one reducibility. On the other hand, O=RE∪coRE does not have a complete language under polynomial-time reducibility because RE≠coRE. Therefore, LO≠O. 

In the Complexity Zoo, a class or a statement is caged when it looks very scary. Examples of caged statements can be found in the descriptions of coNP/poly and SFk (not for the easily scared, reader discretion is advised). 

The pairwise boundedness clearly implies that there are only O(n2) clauses. Together with the quadratic lower bound on the number of clauses, it roughly says that no pair of distinct variables appears in significantly more clauses than the average. For Gap-3SAT, it is known that the sparse case is hard: there exists a constant 0<s<1 such that Gap-3SATs is NP-complete even for a 3CNF formula where each variable occurs exactly five times [Fei98]. On the other hand, the dense case is easy: Max-3SAT admits a PTAS for a 3CNF formula with Ω(n3) distinct clauses [AKK99], and therefore Gap-3SATs in this case is in P for every constant 0<s<1. The question asks about the middle of these two cases. The above question arose originally in a study of quantum computational complexity, more specifically two-prover one-round interactive proof systems with entangled provers (MIP*(2,1) systems). But I think that the question may be interesting in its own right. References [AKK99] Sanjeev Arora, David Karger, and Marek Karpinski. Polynomial time approximation schemes for dense instance of NP-hard problems. Journal of Computer and System Sciences, 58(1):193–210, Feb. 1999. $URL$ [ALMSS98] Sanjeev Arora, Carsten Lund, Rajeev Motwani, Madhu Sudan, and Mario Szegedy. Proof verification and the hardness of approximation problems. Journal of the ACM, 45(3):501–555, May 1998. $URL$ [AS98] Sanjeev Arora and Shmuel Safra. Probabilistic checking of proofs: A new characterization of NP. Journal of the ACM, 45(1):70–122, Jan. 1998. $URL$ [Fei98] Uriel Feige. A threshold of ln n for approximating set cover. Journal of the ACM, 45(4):634–652, July 1998. $URL$ 

Then it is not hard to see that two matrices A and B are equivalent in the sense stated in the question if and only if there is an isomorphism between G(A) and G(B) which preserves labels. But the isomorphism problem of vertex-labeled graphs is known to be equivalent to the isomorphism problem of usual, unlabeled graphs. To reduce graph isomorphism to the current problem, given a graph G=(V, E), consider each edge as two directed edges, and construct a 2|E|×2 matrix M(G) by writing the two endpoints of each directed edge in one row. Then again it is not hard to see that two graphs G and H are isomorphic if and only if M(G) and M(H) are equivalent in the sense stated in the question. 

We assume that vertex weights can be arbitrary positive integers, or more precisely, they can be positive integers at most 2n. Then the current task cannot be performed even in a slightly weaker time bound O(n2), unless the transitive closure of an arbitrary directed graph can be computed in O(n2) time, where n denotes the number of vertices. (Note that an O(n2)-time algorithm for the transitive closure will be a breakthrough.) This is the contrapositive of the following claim: Claim. If the current task can be performed in time O(n2), the transitive closure of an arbitrary directed graph given as its adjacency matrix can be computed in O(n2) time (assuming some reasonable computational model). Proof. As a preprocessing, we compute the strongly connected component decomposition of the given directed graph G in time O(n2) to obtain a DAG G′. Note that if we can compute the transitive closure of G′, we can reconstruct the transitive closure of G. Now assign the weight 2i to each vertex i of the DAG G′ and use the algorithm for the current problem. Then the binary representation of the sum assigned to each vertex i describes exactly the set of ancestors of i, in other words, we have computed the transitive closure of G′. QED. The converse of the claim also holds: if you can compute the transitive closure of a given DAG, it is easy to compute the required sums by additional work in time O(n2). Therefore, in theory you can achieve the current task in time O(n2.376) by using the algorithm for the transitive closure based on the Coppersmith-Winograd matrix multiplication algorithm. Edit: Revision 2 and earlier did not state the assumption about the range of vertex weights explicitly. Per Vognsen pointed out in a comment that this implicit assumption may not be reasonable (thanks!), and I agree. Even if arbitrary weights are not needed in applications, I guess that this answer might rule out some approaches by the following line of reasoning: “If this approach worked, it would give an algorithm for the arbitrary weights, which is ruled out unless the transitive closure can be computed in time O(n2).” Edit: Revision 4 and earlier stated the direction of edges incorrectly. 

It may depend on the computational model, but I doubt this is known, given that the time hierarchy theorem for two-tape Turing machines is not powerful enough to distinguish, say, O(n2) time and o(n2) time even without giving the latter the constant-time access to the answers on smaller instances. 

The problem is a generalization of the set packing problem (exercise: why?), and therefore it is NP-hard even to approximate in a very weak sense. See Maximum Set Packing and Maximum Clique in the compendium. 

This is essentially asking which special cases of ILP can be solved in polynomial time. One of the fairly large classes of ILP which can be solved in polynomial time is ILP with totally unimodular constraint matrices; see Wikipedia. But if you want a very general result, it is unlikely that you get anything more than tautological statements such as “the optimization problem is in P if there is a polynomial-time algorithm to solve it.” Indeed, your question is formally equivalent to asking which problems in NP are in P, and obviously we do not know any necessary and sufficient characterization which is easy to check. 

This is not an answer, but the description of a problem which I think the asker wants to ask. To vaibhav: While I believe that this is at least close to what you want to ask, please edit the question so that it is clearly understandable. Otherwise, we can only make a guess about what is being asked. We are given Cx,i, Cy,i, Cz,i ∈ ℕ and Px,i, Py,i, Pz,i > 0 for i=1,2,3 such that Px,1 < Px,2 < Px,3, Py,1 < Py,2 < Py,3, and Pz,1 < Pz,2 < Pz,3. We are also given K∈ℕ. The values Cx,i and Px,i describe the content of a stack named x. From top to bottom, the stack x contains Cx,1 items each of profit Px,1, Cx,2 items each of profit Px,2, and Cx,3 items each of profit Px,3. The contents of stacks y and z are defined in the same way. We are allowed to retrieve K items in total from the three stacks. How can we find efficiently the way to maximize the total profit? 

This is a slightly improved version of my two comments on the question combined. Let’s restrict our attention to distributional problems in DistNP (aka (NP, P-computable)) for simplicity. Then you are looking for a problem in DistNP ∖ Average-P/poly. Tautologically, such a problem exists if and only if DistNP ⊈ Average-P/poly. And if DistNP ⊈ Average-P/poly, then every DistNP-complete problem lies outside Average-P/poly because Average-P/poly is closed under average-case reductions. (Considering a larger class SampNP (aka (NP, P-samplable)) instead of DistNP does not change the situation much because DistNP ⊆ Average-P/poly if and only if SampNP ⊆ Average-P/poly. This equivalence is a direct corollary of the result by Impagliazzo and Levin [IL90] that every distributional problem in SampNP is average-case reducible to some distributional problem in DistNP.) I do not know which natural assumption implies DistNP ⊈ Average-P/poly. The assumption that the polynomial hierarchy does not collapse is not known to imply even a weaker consequence that DistNP ⊈ Average-P, according to Section 18.4 of Arora and Barak [AB09]: “[…] even though we know that if P=NP, then the polynomial hierarchy PH collapses to P […], we don’t have an analogous result for average case complexity.” References [AB09] Sanjeev Arora and Boaz Barak. Computational Complexity: A Modern Approach, Cambridge University Press, 2009. [IL90] Russell Impagliazzo and Leonid A. Levin. No better ways to generate hard NP instances than picking uniformly at random. In the 31st Annual Symposium on Foundations of Computer Science, 812–821, Oct. 1990. $URL$ 

This is more like a comment in response to the revised question (revision 3) than an answer, but it is too long for a comment. Simply excluding sparse languages is not enough to exclude languages like {x∈{0,1}*: |x|∈S} instead of {1n: n∈S}, where S is an infinite subset of {0, 1, 2, …}. I would like to point out that it may be difficult to distinguish between the case where a language belongs to P/poly because it is “essentially sparse” (such as {1n: n∈S} and {x: |x|∈S}) and the case where a language belongs to P/poly for other reasons. The problematic thing here is, obviously, how to define the term “essentially sparse.” You may want to define “essential sparseness” as follows: a language is essentially sparse if it is reducible to a sparse language. However, care must be taken because if you use the polynomial-time Turing reducibility in this definition, the definition is equivalent to the membership to P/poly! So an obvious thing to try is to use the polynomial-time many-one reducibility. I do not know whether this is equivalent to the membership to P/poly, let alone whether P/poly contains any natural language which is not essentially sparse in this sense. 

Here is a hint for your question in the comment to the question. The independence number of a graph can be computed by a polynomial-time oracle Turing machine which calls the #P oracle just once. That is, the independence number is in class FP#P[1]. For a graph G on n vertices, let gk(G) be the number of independent sets of size k in G. Note that 0≤gk(G)<2n. Let $$f(G)=\sum_{k=0}^n 2^{nk}g_k(G).$$ I leave the rest of the proof as exercise. 

Initialize h: A→A to the identity mapping. Is h=g? If so, we found out that g is in the monoid generated by S; accept and halt. Guess one element f∈S. Update h to f∘h. Go to step 2.