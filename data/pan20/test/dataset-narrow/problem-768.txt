I have the following problem concerning backups in gitlab: I've created a backup of my gitlab-data with the command with a certain user. As visible in the command, Gitlab runs in a docker container. The backup is successfully created and available in the mounted volume and accessible by root (outside the container). Now, i want to copy (cp) or rdiff (rdiff-backup) the backup file to another location. Here I face the following problem: I don't have the permissions to access the backup folder, which has permissions , when trying with the same user that created the backup. I tried to set the permissions for the backup-files with the help of the setting in the gitlab.rb file. However, that didn't solve the issue, since the backups-folder itself is still not accessible, only the backup-files have the defined permissions (checked with the root user). I also tried to set the permissions with the root user to , but after creating a new backup, the permissions are reset to the defaults. How can I access the backup folder? Are there any settings or configurations to define which user can access the folder? Or do I really HAVE TO use the root-user? I've created an issue on the gitlab-site ($URL$ but there is no response and the backup is an important task, so I hoped you might be able to help me. 

I think the DNS changes are no different, other than the new IP address, but I have no idea how to approach the Apache setup to accomodate the shared subdomains. Update We're retiring the old domain, so when someone goes to www.oldsite.com or sub1.oldsite.com, we want a permanent redirect to the appropriate folder in www.newsite.com. From there, the index.php contained within the folder will do the work. A url like www.oldsite.com/somewhere will be handled by www.newsite.com/oldsite/index.php. Can someone guide me through this? I'm trying to figure out WHAT to setup to test, and as soon as I click "save", all hell is going to break loose if it's not correct. My DNS/Apache skills are iffy at best, and this is a production environment, so I can't afford to risk the down time of my typical learning curve, lol. 

I'm building a new Magento website on a Amazon ec2 instance and will need to point domain of their old OSCommerce site to the new ec2 instance's elastic IP address. Normally I would have though this a simple task of updating the record of their domain, but when I logged into the account with their register I see they have 90 records set up already, mostly & records. They have no IT guy to ask, but I'm almost 100% sure what I need to do but as I normally work with web dev stuff like php and javascript etc I just want to make sure I have it right. To give you a sample of their DNS records they have set up: 

Which is odd as I'm not used to comments in public keys or line breaks? We've tried using her public key with & witout the comment & line breaks. I've added it to like this: 

I have the following problem with deleting an image or tag in docker-registry v2: I have a server that runs a docker-registry. I created an image and pushed it, that I now want to be gone. Now I want to remove the image (or at least the tag, if image impossible). The current frontend version doesn't support such a functionality. I tried it via a command, like the states, and entered the password, but the execution resulted in an empty line, no authentication error (authentication is activated) or success message. The tag is still available. Since I have access to the server, I came up with the following idea: Can I simply and savely just remove the folder (or another file/folder)? Or does that result in a break of the registry? What initial steps, like stop the registry service, do I have to do? 

We've recently created a new "main website" that consolidates two older sites, and need to do some final DNS and Apache configuration work for each of the old ones. (To clarify for some, this is a small company without the resources to hire experts, and I'm trying to assist in an area that is not my specialty, and determine how these changes need to be made.) We want to point the old sites to a folder on the new site that will "translate" the requests, as well as a couple of old subdomains pointing to new subfolders within the "translation" folders. I've looked at related questions, but they all include considerations that are way out of my league, and are on shared hosts. Our sites are both located on EC2, so we have full control/responsibility of what gets changed. The confusing part is a single IP being used for mulitple domains, and all of them having the same subdomains. I'm baffled as to how to insure that the primary domains are work correctly, and that the subdomains for each are still intact after everything is complete. 

We are running the site on Nginx & HHVM. I checked the /var/log/nginx/error.log and I see this error: 

I'm such a dope, forgot I had varnish running on the other application so port 80 was in use. That was the issue, the error logs pointed me in the right direction while the nginx -c just confused me. 

Our staging server is on an Amazon EC2 instnace. When you ssh into it you can execute a command or without having to enter a password. Is there anyway I can require a password from a user when they try a command with ? I have a 3rd party dev who needs access and I want to restrict root privileges for them. I tried setting a password with but I still don't require a password. 

Having asked a long and convoluted question earlier, I now have some code here using that I need help with. We're merging an old site into the new one, and don't care about mapping all the ancient urls to anywhere on the new site, so all of it should be permanently redirected to a special index.php that will decide if/where to redirect with php. The exception is one subdomain, it should do the same thing, but to a different index.php. This is the "pseudo-config" I'm trying to accomplish - slight variations will work, but I can't get it to ignore the full path of the request, and unconditionally go to my choice of index.php with the querystring. 

What I'm trying to accomplish:, but seeking guidance/clarity on correctness: For $URL$ ==> c:\www\oldsite\index.php 

I was getting 403 & 404 errors on a Magento application I was working on I tracked down the issue to two blocks in the NGINX configuration, if I comment them out the issues are resolved but I would like to understand better what I'm commenting out. ISSUE 1 I was getting a 403 error on URLs such as 

I'm moving a site to a new server running on NGINX. The old site's Apache2 VirtualHost has configured that I want to replicate in the NGINX configuration. From what I've read on the NGINX Docs this seems to be simply achieved with . I just want to make sure what I have is correct. From APACHE2 

Should I update our webserver? Everything is running fine now so I do not wish to run the risk of breaking anything. Can I just update the security updates? Is this the correct process for updating?