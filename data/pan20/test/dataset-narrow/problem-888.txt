We have a separate VMware ESX 5.5 server that we use as a test environment, where we replicate production machines. It runs replicated VMs on a vSwitch that is physically separated from the production network. Last month, we used Veeam to copy VMs to the server and simulate production, for the purpose of upgrading AD from 2008R2 to 2012R2. This worked very well, and so we upgraded production with minimal trouble. I should say that last month, this test environment had NO internet access, so the testing was imperfect in that minor way, but served us well regardless. This month, we introduced an Internet connection on a completely separate cable modem line to the mock-production vLan on the test ESX server, so that we could do more testing (for an application's upgrade which uses IIS and SQL services) but see real connectivity from clients coming across the Internet. So I have replicated servers handling DC's, DNS, Certificate Services, IIS, and SQL and even Network Policy even though I don't think that plays a hand here. The Internet connection works, but I now see that for about an hour or so after replicating the VMs from production, the network location changes from lss.local to Public. Also, at no point is the test IIS server able to serve out a site, and I see that domain trust relationships are now broken, and recreating them fails every time. Network Location Awareness sets the locations at Public, and setting them manually (via local policy) to Private. They can query DNS successfully, and I am rather stymied as to why the introduction of an internet connection would break all NLA and all secure channels, where it did not behave like this when the Internet had no presence. Meanwhile, servers in production carry on happily, so I don't worry about cross-contamination. So in short, all these issues now come about from the presence of a new route online OR the replication of a freshly-upgraded AD 2012R2 domain. I suspect NLA has a hand in the trouble here. Where would you start looking to pinpoint the cause of the change in network location? 

I am trying to find a good way to deploy Lync 2013 standard & basic clients silently on computers with Windows 7 / Office 2010 & ahve a few roadblocks. 1) I am successful installing the standard client (downloaded from VL), but when installing it automatically I am getting the error "The language of this installation package is not supported by your system". I am using the syntax: setup.exe /adminfile lyncstd.msp /config lync.ww\config.xml I made the MSP using setup /admin and gave it a full install. I also made the config.mxl file with the following syntax: 

So this makes no sense to me - as in why installing from the GUI would present no such problem, but automating suddenly throws up a burning hemorrhoid. In the install log, I see towards the end a message: The language of this installation package is not supported by your system", so I have tried disabling options for language support in the admin install mode as well as to Remove the following earlier version of "Microsoft Office Multi-language User Interface Pack". 2) Or is the only way to install it using a GPO running a batch file in Computer Configuration > Policies > Windows Settings > Scripts, and have it run a silent install using setup.exe with the adminfile & configfile tags? Thanks! 

$USER1$ points to /usr/local/nagios/libexec and it is the same for all my command declarations. If I run it as ROOT: PASS 

Neither of the above work, it still asks for password. IT WORKS if I simply grant user full permissions: 

Is there any way to allow the upload (put) to the default directory, without having to chdir first? I do have flexibility to move these directories around. For example, the sftp chroot dir doesn't have to be in the users home directory, I can even change around the users home dir (but the user must still be able to use authorized_keys to login). Please note: I do understand many SFTP clients, and the command line SFTP client allows for defining a relative path at login. That is out of scope for this question, I'm desiring this config be done server-side and the client simply just needs to login. 

If you post your server.conf it will greatly assist others in helping. Firstly, the errors you are getting are specifically about the ROUTES being pushed, not necessarily the IP assignment. The logs show that it's trying to get an IP, so I assume that's configured on the server, but it doesn't look like routes are. I'll go over both items... IP Assignment: To define what range of IP addresses you get from the OpenVPN server, you set it at the "server" item in your server.conf and, in my working config, defined the topology as "subnet": Example: 

Solution Background/Info: As Grant posted in the comments to the original question: Giving the logged in user or Group (Domain Users/etc) access to the share, and not explicitly defining the username/password in the command solved the problem. This also solved the problem with the GPO drive mount. Not defining the "Log in As" field and allowing it to auto-authenticate automatically with the logged in user worked. This DOES solve the problem, however this does not answer the root cause of why using a shared domain account as I was originally attempting did not work. Answer Summary / Step-by-step: 

I see this similar question was asked before, however it was either not answered or it pertained 100% to Windows workarounds. This is specifically Linux and I am trying to restore a Clonezilla image to a 4TB hard drive. I have used this image many times before on other disks, however due to MBR restrictions on this larger drive, it needs to be GPT so I cannot restore the disk image. The exact error that Clonezilla states is "Error: Destination disk size is 4.00TB, which is larger than the MBR partition table entry maximum 2TiB. You have to use GUID partition table format (GPT)" I understand "what" the problem is but I do not know the steps to resolve. I'm going to try cloning the master machine using Macrium reflect and restoring, hopefully it can migrate the partition structure automatically. NOTE: This drive is blank, therefore I have no concern over data loss. Experimentation is fine as I have nothing to lose on the drive. EDIT/UPDATE: So it seems Clonezilla actually allows the process to continue, however it will write everything as-is to MBR and the disk will be seen as 2TB. With this in place, the question then changes to converting the MBR to GPT on the OS/boot volume (yes I am booting into the system that I am working with, data loss is not a problem since I can just restore from image if something breaks, which is has many times so far in my trials) I have opened the disk with gdisk and ran the conversion to GPT. This was succesful, however this kills the GRUB boot partition and indeed the system does not boot after conversion. I am following this: $URL$ What I need help with now: So right now I seem to have restored my Clonezilla image to the new disk in MBR format and have done and in-place conversion to GPT using gdisk. I am now looking for guidance on creating the GRUB partition and re-installing GRUB so that the system can boot after conversion. Once I have this all laid out I can format it to a full step by step answer with all the components together to help others in the future. Thanks!! 

Notes We did check to make sure the database had a clean shutdown before trying the procedure above using the following command: 

I can assure you that the date, time (including AM/PM), and timezone are all correct on both the client and server. I have Googled this error and a lot of articles point to a DNS issue. However, DNS seems to be working fine on this server. Many clients are using this server as their DNS server without any issues. I can't find anything in event viewer either. The only other far-fetched idea that I have is that when I demoted this DC something didn't get cleaned up properly, and it is still thinking that I am trying to remote into the old server. Although, I didn't get any errors when I demoted the DC. Any help is appreciated. 

I failed to mentioned that this was a Virtual Machine. The time was off on the host itself. After I fixed the time, I could remote in via host name. However, I don't know how the server knew anything about the host. For all it knows, it is a physical machine. And it was using a centralized server for NTP. But hopefully this can help someone out in the future. Thank you all for your help. 

I have just demoted our Windows Server 2012 Domain Controller. Removed all services, and then did a fresh install of Windows Server 2012 R2. I gave the server the same IP address and host name that it had previously. I have just promoted it to a DC again. The problem I am having is that I can not remote desktop to the server using it's host name. I get the following error: 

Create new test database on server1. Add mailbox to database on server1. Create new test database on server2. Copy log files and edb files from test database on server1 to test database on server2 Run the following command on server2 to allow restore on database. Set-MailboxDatabase testDatabase -AllowFileRestore $true Finally, the problem comes when we try to mount the database with the following: Mount-Database testDatabase 

Overview We are currently in the process of finding an easy Exchange fail over solution for our company. We have a working Exchange 2013 running on Windows Server 2012 (server1). We also have another identical Exchange 2013 set up on another Windows Server 2012 machine (server2). The second server is intended to only be used if we need to fail over to it. No mail is being sent to this server. We are using the following article as a fail over solution: TechNet - Database Portability Right now we are testing this procedure by doing the following: 

This shows that it was indeed a clean shutdown. This procedure is supposed to be very simple, as we have read many guides, and we have yet to see anyone run into this problem. Any help is appreciated. 

Nagios 4.1.1 Ubuntu 14.04 x64 Please note This is a new server. Our old Nagios server has this EXACT SAME config but on version 3 on RHEL, and it works. I have a custom command to check an OpenVPN Server external connectability: 

What a pain! A simple bash script and now I need a container or VM just to do some simple date manipulation. This is making me feel like I'm shooting myself in the foot developing this way, and the whole reason I got a Mac was because it's much closer to a 'nix environment than any Windows version so I could do simple local development on it. My question(s): 

I want to open up TCP to the docker daemon so that Jenkins can build containers against it. I'm getting lots of info about how to do this. Hoping to get the best method. Goals of dockerd: 

Assuming each container is listening on a different port on the local server doing the routing, it seems like you could use the source IP as the deciding factor in which port (container) it gets routed to. For example, you have 3 containers each listening on separate ports 7771, 7772, 7772: 

Firstly, I want to make it clear I don't care about data loss, and I understand the risks involved. What I'm looking for is guidance and if what I am hoping for is even possible. My Scenario: I have 3 1TB SAS drives in the server. Wanting to combine to a main 3TB volume. OS is Ubuntu Server 14.04 I want to avoid using the RAID controller (RAID 0) since I know that if a single disk fails, then the entire array is compromised. I can comfortable use LVM but I'm not sure if it can do what I'm trying to do. My goal is if ONE disk fails, then I lose the data on that bad disk but the other disks continue to operate in the array and the files on the good disks are still available. I know this isn't technically "striping" because no data would span across disks (all blocks in a file on one physical disk) -- Is this possible? One more time to reiterate - lost DATA is OK, but a lost VOLUME is not. If it's possible, great, if not, that's fine too as I am just looking for guidance. 

So, the above authenticates users against AD and works as designed. It searches for the user group "VPN Users" within (the default '/Users' OU) If the user has the proper group set then it allows a user in! I want to add a user to another separate OU, for example: Team1/Desktop-users/Standard users could be my new OU with users in it. I want to allow users in that OU as well as the original one. Would this be as easy as adding another object in the config?