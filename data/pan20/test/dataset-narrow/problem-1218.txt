An answer to the traveling salesman (and similar) problems can be easily verified on light lambda-calculi. Also, if I understand correctly, the light lambda-calculi can compute every polinomial-time computable function. That way, if one can prove that the traveling salesman problem can't be encoded on the light lambda-calculi, that would also prove the problem can't be solved in poly-time, which would also prove P!=NP. Is that correct, or am I confusing some concepts? 

When applied to a church number evaluates to normal form quickly in several existing evaluators, including naive ones. Yet, if you encode that term to interaction nets and evaluate it using Lamping's Abstract Algorithm, it takes an exponential numbers of beta-reductions in relation to . On Optlam, specifically: 

is a closed term on the shape . is on the shape , where is the only variable. and have no lambdas within applications. 

Now, suppose we take the elementary affine logic and extend it with dependent types and . It is easy to prove that language is consistent and normalizing as a consequence of EAL. It can, I believe, express all the type families from Coq, with lambda-encodings and since it has . A simple expression of recursive algorithms is possible with church-encodings for iteration and scott-encodings for matching. And it is obviously simple, as it only has , and . Moreover, such language would have the very comforting property that it can be reduced optimally using the abstract fragment of Lamping's algorithm. All things considered, such language looks like a perfect candidate for the role of a small functional core that serves as an universal code-interchange format, as proposed by Gabriel. Is my suspicion correct, or is there any problem with this reasoning? 

How well can an arbitrary unknown (quantum) state $\rvert \psi \rangle = \alpha\rvert 0 \rangle + \beta \rvert 1 \rangle$, be imperfectly/approximately cloned? Given an unknown state ${\rvert \psi \rangle}$, say one can clone ${\rvert \phi \rangle}$ , such that $${\langle \phi }{\rvert \psi \rangle}= \delta$$ If $\rvert \phi \rangle$ and $\rvert \psi \rangle$ are identical and same, $\delta = 1$, however we know, due the no cloning theorem, that $\delta<1$. My question is what is the 'highest' value of $\delta$ one can achieve using polynomial time 'resources', i.e. how well can an UNKNOWN quantum state be imperfectly cloned? (Let's agree we $\rvert \psi \rangle$, both $\alpha, \beta$ are non-zero, i.e. not basis states) 

Decisional Diffie Hellman It states: given $(g, g^a, g^b, g^c)$ where $g$ is some generator of a cyclic group $\mathbb{G}$, verify if $g^c = g^{ab}$ Under the standard assumptions of hardness of discrete log problem, this problem may also seem hard. However, with bilinear maps this problem is easy and can be verified as $$e(g,g^c) \stackrel{?}{=} e(g^a,g^b)$$ where $e: \mathbb{G} \times \mathbb{G} \rightarrow \mathbb{G}_T$ More about this can be read on The decisional diffie-hellman problem, Boneh'98 or a google lookup on Pairings 

Algorithm Design by Kleinberg Tardos This book helps develop a concrete understanding of how to design good algorithms and talk of their correctness and efficiency. (I studied this in my first year at college, very much readable) For an online copy/lecture notes/reference, (as suggested by Suresh Venkat) go with Jeff Erikson's lecture notes. They are really awesome! 

I only have a (very) introductory knowledge about the Hardness of Approximation and PCP theorem, and I am wondering if it has any specific implications (or can somehow be studied) with Zero Knowledge Proofs? 

Whether or not $P^{A,B}$ equals $NP^{A,B}$ is going on the particular oracle languages A and B that you are using. Iirc, in the BGS paper the language A is TQBF (or any other PSPACE-complete language). The language B, somewhat ironically, is actually defined via a diagonalization construction. Now, if you use the construction from the paper and apply it to TMs with an A oracle, then the resulting language B will be such that $P^{A,B} \ne NP^{A,B}$. On the other hand, if you stick with the original language B from the paper, it will depend on the details of B's construction. If B is in PSPACE (I don't know offhand if it is or not), then a TM with an A oracle could simulate any queries to B, and you should have $P^{A,B} = NP^{A,B}$. The bottom line, though, is that this has nothing to do with consistency--it is just a technical question about the specific oracle languages A and B (and you could get different answers depending on the particular choices of A and B that you use). 

The short answer to your question is, yes. Once you fix a particular encoding scheme for your TM and your inputs and outputs, then there is a smallest TM that will compute your function. There may be multiple "smallest" TMs (of some size, say, k symbols), and if you want your choice to be unique, you could just choose the lexicographically first such TM. Determining what this size k happens to be is undecidable in general, though. This is essentially what Kolmogorov Complexity is all about, except that there the desired algorithm is something specific--what is the smallest TM that will output string x, starting with a blank input tape. I can't comment on the discussion at LTU, except that it seems to be more focused on technical limitations of the lambda-calculus than on ambiguities about TM encodings. I think I'm going to take exception to Andrej's reply to your question. Placing an answer to the halting question, or otherwise supplying the answer to the problem being input on the input itself, is a form of a promise problem. The TM has no way of verifying the input--and if the input is "lying" then the TM is free to output whatever it wants. So this doesn't really address the question at hand. Likewise with the issue of Wolfram's minimalist machines. When deciding whether those are Turing-equivalent or not, the question of how much one is allowed to massage the input becomes key. But I wouldn't consider those machines to be TMs in the first place. All of this falls under the stipulation, "once you fix a particular encoding scheme...", and from the way you phrased your question it seems like you are also making this stipulation. 

I am new to provable security and am working on cryptanalysis of a certificate free signature scheme. Unfortunately, I don't have much knowledge about finding attacks on schemes. It would be very helpful if someone can point me to a survey or other reference to study about cryptanalysis and basically develop some intuition to attack an scheme. 

Let $g$ be a generator of a group of prime order $p$; $a,b ∈_R Z_p^+$ Consider an Algorithm $\mathcal{A}$ which on input $g,g^a,g^{ab}$ outputs $g^{br},r$, for some non zero $r ∈ Z_p$ And an algorithm $\mathcal{B}$ which on the same inputs, $g,g^a,g^{ab}$, outputs $a^{-1}$ Can I claim the algorithms $\mathcal{A} \equiv \mathcal{B}$? While it's trivial to construct $\mathcal{A}$ using $\mathcal{B}$, can we also construct a $\mathcal{B}$ using $\mathcal{A}$? 

This question has been open for long now, and after some research I think I have the following answer There has been two candidate schemes proposed for multilinear pairing by Garg, Gentry, Halevi and Coron, Lepoint, Tibouchi. However either do not have a security proof and their security is given by extensive cryptanalysis. So, to sum it up, although it is possible to implement a "trilinear" map, these maps are NOT (provably) cryptographically secure. 

However this can only be true if there exists an efficient construction of a trilinear pairing... Is there a feasible construction available for a trilinear mapping or at least is it/has it been used in theory and in proofs? Is there any other way to examine the same? 

Consider the following function $$f_s: k \rightarrow \lvert \psi_k \rangle$$ where $s,k$ are bit strings, and $\lvert \psi_k \rangle$ is a $n$-qubit state. Assume the function is a one-to-one mapping. Given only $\big(f, k, \lvert \psi_k \rangle \big) $ is there a way to produce a (zero knowledge) proof $P$ that only verifies that $\lvert \psi_k \rangle$ has been generated from $k$, but not learn any more information about $\lvert \psi_k \rangle$ or $s$? 

I would like to second the advice given by Justin. There are a lot of great suggestions in the replies, but many of them are just about effective work habits and don't specifically have anything to do with research. I was thinking about this recently after viewing a talk by Steven Johnson on the subject of his latest book, "Where Good Ideas Come From". (One version of the talk can be found at TED.) His basic premise is that there are certain environments which act as natural incubators for new ideas. He gives the example of the coffee house in Renaissance England, which aside from the effects of the coffee itself, brought people of varied backgrounds together and encouraged the exchange of ideas. It is this cross-pollination of ideas which gives rise to new, novel ideas. Now quite possibly this hasn't been mentioned in most of the other responses because it is just assumed that everyone is already going to be doing this, but I thought it would be worth emphasizing the point. 

(Oh good, this question hasn't been closed yet!) On a practical level, I agree with the comments by Warren and Suresh that this question isn't answerable given the current state of the art in TCS, and it sounds more like an AI question. But I have to wonder, does it necessarily have to be that way? The question of how semantics arises from syntax has to be one of the most foundational questions there is, and I wouldn't want to rule out TCS having a role in answering it. I tend to think of AI of being more like engineering and TCS like theoretical physics--we ought to be supplying the equivalent of Newton's Laws that provides some overarching principles that the AI engineers can use to go and build their intelligent machines. Now, so that this answer isn't totally without actual content, here is a paper that maybe touches a little bit on this question: Universal Semantic Communication I by Juba and Sudan. They make an attempt at modeling "meaningful communication" in terms of computational power. I'm not sure how successful their approach is, but I do think it's important in that they are using techniques from computational complexity to analyze these issues. 

Here is the bruijn-indexed normal form of a slightly altered version of the above, which must receive as the first argument in order to work (otherwise it doesn't have a normal form): 

Is it possible, from the configuration of the net alone, to infer the tags and thus readback the same lambda term without them? 

A class of lambda terms can be evaluated using Lamping's abstract algorithm - that is, converting them to interaction nets and applying a set of rules. In order to get the result, you have to read back lambda terms from normalized interaction nets. For example, this net: 

The Calculus of Constructions is a very simple core functional language with dependent types. Per curry-howard isomorphism, it could, potentially, be very useful for writing programs and proofs. It, though, has a few problems: induction isn't derivable, it isn't possible to prove , and pattern matching on algebraic data structures take linear time. In order to solve those issues, practical languages such as Coq are based on the Calculus of Inductive Constructions instead, which add a layer of primitive datatypes on top of CoC. That, unfortunately, makes the core language very complex. An alternative solution to those problems is a new primitive, self, which is a construction that allows a type to reference its typed term. This construct, together with the Parigot encoding, and a slightly weakened but still useful notion of contradiction, is sufficient to solve the problems above. The proposed language, though, is still somewhat complex. In particular, it has different Pi types, complex kind machinery and requires a restricted form of recursion (for the Parigot encoding). Is it possible to be simpler? I.e., can the calculus of constructions with only self types and nothing else from this paper still be able to derive induction and employ the parigot encoding? 

It's important to understand that computer scientists use the term "nondeterministic" differently from how it's typically used in other sciences. A nondeterministic TM is actually deterministic in the physics sense--that is to say, an NTM always produces the same answer on a given input: it either always accepts, or always rejects. A probabilistic TM will accept or reject an input with a certain probability, so on one run it might accept and on another it might reject. In more detail: At each step in the computation performed by an NTM, instead of having a single transition rule, there are multiple rules that can be invoked. To determine if the NTM accepts or rejects, you look at all possible branches of the computation. (So if there are, say, exactly 2 transitions to choose from at each step, and each computation branch has a total of N steps, then there will be $2^N$ total brances to consider.) For a standard NTM, an input is accepted if any of the computation branches accepts. This last part of the definition can be modified to get other, related types of Turing machines. If you are interested in problems that have a unique solution, you can have the TM accept if exactly one branch accepts. If you are interested in majority behavior, you can define the TM to accept if more than half of the branches accept. And if you randomly (according to some probability distribution) choose one of the possible branches, and accept or reject based on what that branch does, then you've got a probabilistic TM. 

A trilinear pairing is defined a function $e:G_1^3 \rightarrow G_2$, such that it satisfies the property $e(k_1^a, k_2^b, k_3^c) = e(k_1,k_2,k_3)^{abc}$ In general I am trying to solve the following problem, given a tuple $$(g, g^s, g^{r_1}, g^{r_2}, k_1, k_2, k_1^{r_1s},k_2^{r_2s'} )$$ where $g, k_1,k_2 ∈ G_1$ and $r_1, r_2, s,s' ∈ {Z_q}$ Is it possible to predict if $s = s'$? One of the easy ways is to use trilinear mapping function $e$ to evaluate $$e(g^{r_2}, k_1^{r_1s}, k_2 ) = e(g^{r_1}, k_1, k_2^{r_2s'})$$ 

I thought about this question once, and I convinced myself with an argument similar to this: Unlike the SVM (kernel), the features are not projected to a different space, instead they are weighted! The weight of the features make them acts like like they would under "gravity" and balances the "space", that way the features that have more weights associated to them actually has more "dominance" over the output (of the activation function). I like to think of it this way- consider a non linear data set with just 2 attributes, now, after the training phase, each attribute has some weight assigned to them. these weights inherently change their "role in the activation function" thus might actually make one of them to shrink in in size relative to the other, now as this happens if you then again look at the points, you might see that the new data does not look as non linear but instead you can actually make a lot of sense of it.. This was a very high level "reasoning" with a lot of "handwaving". But intuitively on a high level this is what I think is happening under the hood.