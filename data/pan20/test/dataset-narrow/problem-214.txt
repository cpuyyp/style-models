You can ship the logs to as many sources as you want. Just add a new secondary server on the primary one. The impact will be on the Korean server as it will have to copy the logs from the backup directory and replay them. 

sp_depends will give you list of dependent objects in all current versions of SQL server but is listed as deprecated. Here is a MSDN article with a way to find objects referencing a function and with small modifications you can get the list of object referenced by a function: 

Yes it is possible to set one of the instances to run on port 1433. It is also safe as the dynamic ports will be set in the range of 49152 and 65535 as stated above. You use the configuration manager to set one of the instances to start at port 1433 (clearing the configuration for the dynamic port) and it will run on that port afterwards. 

No, you dont have to do it again. The differential backups are just that so the only effect your mistake made was to make the restore time longer than neccessary. If your last transaction log backup was restored successfully then you are ok. 

No. You can setup a two node failover cluster with the standard edition of SQL Server. Any more than that and you need the enterprise edition. $URL$ 

I am getting a typical group by error that one would see when a field is not in a group by or an aggregate in the select. 

I need to get some totals output in SSRS that I am having difficulty putting together. I am not sure if I should do the totals for the below query within the query, or in SSRS. The output is going to be presented as one long table in SSRS. I have some example DDL and fake desired output below, but to describe what I am doing, here is a quick snapshot of what I am trying to mimick in SSRS. 

I have a report that is "working" fine except for an issue. I have four dropdown list parameters where I am getting the values from queries. When I select a value from a dropdown list, it selects the next value below what I wanted to select, and "removes" the first value as an option. For instance, one of the dropdowns holds a list of payrolls. When getting the list from a query, it returns payrolls 1 through 121(or so), which is perfect. However, if I try to select any of the payrolls, say payroll 10, it will show me that I selected payroll 11, and the option to select payroll 1 will no longer be available. ALSO, possibly tied to the above, the report sometimes refreshes when I select "view report" and I have to try to choose the payroll again. These are not cascading parameters. The values are all derived from two queries where I am selecting a distinct list of employees for two of the parameters and a distinct list of payrolls for the other two. I am at a loss as to why this type of behavior is even possible. 

You are stuck there. There is no option to re-size the files during restore. The backup is made by dumping the data pages directly to the backup device. There is the option of exporting/importing the data using SSIS but that takes time and it would probably be cheaper to buy a new larger hard drive and take the test server down and exchange it's drive. 

Sort of, you will double the resources but every instance will only be active on one node at the same time. Any SQL Server instance will have exclusive access to it's storage on the node where it's currently active - If all the storage for multiple instances comes from the same SAN one instance can affect the others No you cant, sorry 

Short of restarting the SQL Server service or the box this is enough. If you want to be absolutely fresh you might want to drop user created statistics from your last run. 

run MMC.exe select File - Add remove snapin and find the SQL Server configuration manager and add it to the console root. Save the MMC console for later use with the extension.msc. If the SQL Server Configuration manager does not show up in the MMC console you have to register the DLL's and recompile the .mof for it Open a command prompt with run as admin, type the following command, and then press ENTER: 

Below is some sample code and data. I am trying to stay away from only using group by because I have ~30 columns that needed to be included, and having 25 columns in a group by seems...unfortunate. In the data you can see that Payroll 0048 has two records, and the column has a value of , essentially meaning the person was paid in two different checks. Conceptually, I need those two records summed up into one for the end result. However, things are complicated by the fact that I am using a cross apply to fill a business requirement that regular/over-time/double-time are expressed in new records instead of columns. The first select statement is without the SUM and GROUP BY functions to show what the data kind of looks like, but without the desired grouping and sum. The second select statement attempts to get the desired sum and group by, but gives the error. I am just having a heck of a time finding my problem. Could be an oversight or just a brain fart using CROSS APPLY wrong or something. I am also open using another method if there is a better way. As a side note, the is just a hack so I do not have to group by that also. 

Hi I need to set auto increment for a table as S1. I tried by altering the table and also by creating by new table I am aware how to set auto_increment but when i try to set auto increment as S1 i am unable to do. since i need the primary key values should be S1,S2,S3 as following records. can any one please suggest the data type for that, 

If the tables were dropped, ApexSQL Recover can recover them even from databases in the simple recovery model. ApexSQL Recover can recover both table structure and table records On the other hand, ApexSQL Log cannot recover records lost when a table was dropped, it can only recover the table structure In case the records that were lost using DELETE (not DROP TABLE), both ApexSQL Log and ApexSQL Recover can help The advantages of ApexSQL tools over recovery to a point in time is that ApexSQL will recover just the tables you specify (creates CREATE TABLE and INSERT INTO scripts) , while a point-in-time recovery will roll back all the transactions that happened in the meantime A DROP TABLE statement marks the MDF file pages used by the dropped table for deallocation. These pages are actually still in the MDF file until overwritten by new operations. To prevent new operations overwrite the data necessary for successful recovery, we recommend creating a copy of the original MDF and LDF files immediately 

As you rightfully state the engine will do a table scan to locate the record and will at least cache the page containing the row where ContractID = 2000. How long the rest of the table will stay in memory will depend on other queries and the size of the buffer cache. You can query the buffer cache to see how much memory this table will take in cache. Here is a treasure chest of queries to monitor your server. This one here for instance will break down the buffer cache for your current database showing how much space each object takes in memory: 

It is not stated if this is for the log file or the database files but lets start by trying the log files and if not then try setting the database files to fixed growth: 

For most parts I'm referencing Paul Randall's Inside the storage engine blog series. The only way to reclaim unused space from database files in SQLServer is using the DBCC SHRINK command which reallocates data within the database files freeing pages and after removing them from the Global Allcation map removes them from the database file. This operation is slow, creates fragmentation within the database and is even slower when dealing with LOB pages as those are stored as linked lists within the database files. Since you are dropping the NTEXT column you will have to wait for the ghost cleanup process to drop the data before shrinking. Now having lots of free space in the database files will actually do you no harm, if you have the disk space, backup compression will take care of the free space within the files. If you absolutely want to make the files smaller you can create a new filegroup with the database and make it the default and then move the tables into the new filegroup but this can take time and cause downtime. I have used the technique explained here by Bob Pusateri with good results. 

I have a dataset text query and I would like to have an ORDER BY in it. The issue is in SSRS, the ORDER BY is not working as desired. However, when I run the query in SSMS, the output is ordered as I would like. This image is in SSMS where it is working properly. The payroll = 000004 is just to isolate a specific payroll to make it easier to read. The ISNULL(OrderByNumber, 1000) is just to put the total row down at the bottom. 

And here is the results I get in SSRS preview. Not ordered how I want at all :( As far as I can tell, SSRS should support ORDER BY in the dataset query. I also tried removing the ISNULL( ,1000), and the order by still didn't function as desired. Thoughts? 

At my work, I am going to be moving millions of records to a different server, and removing them from the source server. My DBA has set up a backup scenario where change backups are taken every 5 minutes. He had this warning for me regarding deleting the records: 

With tons of values in the Cross Apply, it just gets annoying. How can I easily export the definition of an object to have double/quad quotes?