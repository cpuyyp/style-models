I have a common puppet recipe for a set of our servers. One of the puppet-managed files is that contains the original , and entries. The problem of the common file here is that all cronjobs are run at the exactly same time in all of our servers. For example, daily backups consumed all our backup server resources as all servers are feeding it at same time. What is the preferred way to randomize the exact minute of the daily/weekly/monthly runs between servers, while still keeping the puppet recipe common between all servers? I have been thinking a few different options: 

My FastCGI (PHP-FPM) application may encounter a situation where it needs some time to heal itself up. I'd like to tell to nginx that it should wait a few seconds and then resend the request to the FastCGI backend. I have already experimented a hacky setup where nginx is configured with (see docs) with an upstream with same fastcgi configuration twice: 

Elasticsearch uses Joda timeformat to save date. Hence it's automatically converting my date to datetime. In the date field, since I don't have time, it is automatically adding zero's to it. Since I need to display data via Kibana that why I need this split..I converted format of date and time as varchar(20) as a workaround(bad idea I know) and its working fine now .. 

First of all I apologize for this,I am pretty bad in regular-expression and try to wrote custom pattern(as I am unable to find something in existing grok pattern or may be I am missing something) for parsing svn logs which is in the format of 

As the error is intermittent and I checked svn repo file is there(i.e file checked in properly)so I am not sure what is the issue.Any help is really appreciated. 

Sorry I am newbie to Elasticsearch and I am using elasticsearch-river-jdbc to connect to myql database,everything is working fine except for the fact that everytime it run as per schedule its inserting duplicate record.This is what I am using 

Especially when setting up a completely new server from the scratch, puppet tends to start dependant services as soon as all their dependencies are satisfied. This sometimes leads to situation that services are started unnecessarily too early while the setup of other packages (not maybe dependant of a service) is still running. Is there any one-liner definition or recipe that would automatically make all services to start/refresh in the end of the whole catalogue run? I tried to figure out if run stages could be utilized but that would, in my understanding, require that all service definitions are explicitly defined to "last" stage. 

The PHP application will reply with when it needs some time and space, making nginx to "move forward" to the next upstream that is apparently the same server. Unfortunately nginx does second call in milliseconds. I'd like to delay nginx second call by a few seconds to make sure that PHP backend is fully up and running after the erroneous situation. So, how to add proper delay before the second try? Behind the scenes, I need to recycle the whole PHP-FPM process due how MongoDB driver handles replica set failover. That's why I can't handle the case fully at PHP level but need to release the PHP process for a short period. 

So looking at the output your major problem is I/O. Please run the following command during the time of issue and then paste the output to analyse 

First of all I know this is a complete mess but due to some developer legacy code(hard coded value),I am forced to install 32 bit rpm on 64 bit machine. 

The possible cause of this error is system run out of socket memory.Either you need to increase the socket memory(net.ipv4.tcp_mem) or find out the cause of memory consumption 

Finally issue is resolved Error: TECH PREVIEW: DIF/DIX support may not be fully supported. I constantly saw this message in dmesg during the time of issue and Keep on ignoring this message On further debugging, I found out Kernel is in tainted state 

What I see from network point of view is too many TIME_WAIT=9.76K(I am not sure what is assured means).time_wait is a condition when your system is run out of socket because of many tcp connection are repeatedly open and closed.You can confirm it using netstat -an.You can tune these parameters 

Is there any other way to ensure that I'll get an exact (~5 seconds window) dump from a certain, defined time this way? 

Update 1: Some background to my question. Puppet, during the initial setup of a node, works rather intensively and thus reserves resources of a fairly small virtual machine (in my case, Vagrant on dev machine). When the setup phase came to a point that some of the services, in my case the background workers, were ready, Puppet starts them and the workers start to receive jobs from a queue. This workload slows down the still-going-on Puppet node setup process as well as the Puppet slows down the workers, until the whole setup is done and the box is in a stable state. This is why I started to think that it would be good to let Puppet to work the initial setup to its end before all services on the node are started and why I was wondering whether there is any simple way to do this with Puppet. 

But still getting "_beats_input_codec_plain_applied, _grokparsefailure" I am not a regular expression expert but any help is really appreciated. 

tcp_tw_recycle - BOOLEAN Enable fast recycling TIME-WAIT sockets. Default value is 0. It should not be changed without advice/request of technical experts. Enabling this option is not recommended since this causes problems when working with NAT(Network Address Translation). tcp_tw_reuse - BOOLEAN Allow to reuse TIME-WAIT sockets for new connections when it is safe from protocol viewpoint. Default value is 0. It should not be changed without advice/request of technical experts. But before that take a tcpdump and check the new connection,also check with your application team if these many connection are desired. 

Kernel hangs are difficult to debug as no oops message is displayed on screen as in case of crash and if you are really lucky you will see something in /var/log/messages as during hang your entire system hangs along with syslog daemon and nothing will be write inside these files. With that said hangs can be as simple as temporary performance issue caused due to memory or cpu contention,using inefficient algorithm or may be as complicated as deadlock.So like I mentioned above if you are really lucky 1: Check in /var/log/messages or may be run dmesg to get some pointer 2: If your system is hanging on regular basis then configure kdump along with sysrq keys to know the exact problem. For more info please refer to $URL$ 

I have a linux router between my private network and an unknown network. The unknown network can be another private network, I do not have control over that at all. Most of the devices in my internal network connect the outside world over an HTTP Proxy residing in the Router, so there is no need for NAT between the private network and the unknown network. However, I have one device (Device 3) that should be able to connect to a set of public IP address directly. Because the unknown network can be also a private network, NATting might not be an option (due double NAT). Device 3 needs to connect a set of known public IP addresses and ports that cannot be changed. Device 3 can be isolated to another network logically (with DHCP configuration from the Router) but not physically. Apparently Device 3 cannot utilize the HTTP Proxy (CONNECT) provided by the Router. Can I somehow configure my router to allow Device 3 to connect any public IP, and if yes, how? 

There is no phar.ini in . -- Edit 2 I have installed PEAR from the Debian repositories now and run . Still interested to figure out why the go-pear.phar installation is not working. $ whereis php 

Installed in Debian 7.4 in a Vagrant. My run script is working, but the moment I create a service/pants/log/ directory I start getting the following error: . My service continues to run but nothing gets logged. I've tried two different services and both have the same issue. I've tried various different service/pants/log/run scripts (mostly using svlogd), I've tried changing permissions on everything (a+rwx), the directory to store logs in exists and has the same permissions. If I run svlogd straight off the commandline it works as expected. The bash log below shows what happens as I rename to and back again . 

I'm not sure if this is a Squid subject or IPTables. In my Squid configuration I have something like this setup: 

We have bought some reserved EC2 instances in different availability zones in eu-west-1 region. When launching a new instance, via API or AWS control panel, we do usually set "No preference" as the availability zone. In case of we have an "unused" reserved instance in an availability zone, does AWS still prefer that very zone even "No preference" is selected when launching the instance? Or, should we explicitly define the availability zone to match the inactive instance reservation? I tried to look for the answer from the AWS documentation, but without luck yet: 

I'd like to make a backup of a MongoDB replica set in a way that I can ensure the database consistency at the time when the backup starts. As the documentation says, flag to will copy also the oplog during the backup process. However, this ensures the integrity to point of time in the end of the backup process, that can be tens of minutes after the process started. Some approached I've been thinking: 

Again like Grant suggest vm.swappiness parameter for Database like Oracle,RedHat suggest it to reduce it to as low 10(by default set to 60) and mostly used by algorithm 

so it try to pull both package, and as this is a 64bit machine even setting multilib_policy=best will not help,so I try to exclude it in yum.conf using 

Anyone knows why I am seeing multiple device for the same LUN?I checked with storage team they only exposed 360000970000196801239533036304532 for this server but why I am seeing these many partition(p1..p7) 

Also just want to add there is one network parameter "tcp_moderate_rcvbuf" which is enabled by default,performs receive buffer auto-tuning.As per kernel-doc 

This is definitely look like a hardware error.You can try tool like mcelog which just act as a translator for any machine check exception(mce).You haven't menrioned which version of OS you are using but in RedHat/Centos you can install it via yum