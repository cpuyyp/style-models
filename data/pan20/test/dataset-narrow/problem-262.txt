Do maintenance plans provide any functionality beyond their component parts? It would seem that the only value added is the convenience of grouping subplans into logical bundles, and the simplified GUI for creation. All the actual work is being done by the underlying SSIS tasks which are being scheduled as agent jobs. Is there anything more to it than that? 

I'm trying to make a multi-statement table valued function and failing due to what I think is a limitation of functions and CTEs. The intended function defines an input parameter(@Param) and and output table(@ResultTable). It then executes a complex insert statement involving a CTE into that table variable which is (of necessity) terminated by a semicolon. I then attempt another complex update statement to that table variable involving a CTE and receive an error saying that "Must declare the scalar variable "@ResultTable". Apparently it has dropped out of scope somehow. I have used this sort of pattern in the past, so my only thought is that the CTEs seem to limit the scope in some way. Is this a known limitation? 

This is almost exactly the example used for 'factless fact tables' in analysis services, which illustrates the point. The thing represented by the table is a 'booking', which has no natural primary key. The true key would be a composite key of location-timeslot. You need to create an artificial primary key if you don't want to do that or the DBMS does not support that. You need at least these tables- Location(room #, Capacity) Timeslot(day,starttime), module(code,name). It's not clear if the person is tied to the module or tied to the booking. It's not clear if the extension is tied to the person or the location. 

There's no answer that applies in all cases. In general, however... If the lookup list is small and you can cache it(or use a cache data source), there's not much performance penalty to doing it in SSIS. If you want crossref a list of 50 location codes to names of cities, go for it. It's nice to see all the process on-screen in one place, rather than buried in sql statements. TSQL will be better-performing in most cases, since it knows the most about the data and the query optimizer is always going to be smarter than you. If all the data is in one DB, you can hide a lot of complexity in a sql query source. If the data is spread out across different systems, the middle ground is to to do an SSIS merge join from each system. Trying to do that at the RDBMS level is madness. Always do the sorting in the source query, though. SSIS Sorting is almost always a bad idea. 

I know there are a number of topics on this question, but I'm always seeking more insights. I have a large table with a billion+ records. The amount of records could be reduced and archived, but the size will still be large. My task is to change a existing data type of a single column where the old value of data is safe to convert into the type. Here are some of my approaches: 1 - Drop the constraints on the table that impact the targeted column, drop the indexes that also impact the targeted column, add a new column with NULL's at the end of the table, update the new column with the old column values in chunks of 10K, 50K or 100K increments, drop the old column when data has been copied and reapply indexes for that column only. 2 - Copy all data into a new table with the data type change in chunks as before, verify data is completed, drop the old table, rename the new table to the old and apply indexes. 3 - Import all data from another data source like a flat file to a new table with the data type change using BULK INSERT and MERGE SP's, basically like option 2 with having 2 duplicate tables, verify data, drop old to replace with new table and apply indexes. What would be the fastest and safest option? Are there other options I'm not considering? I've updated 100 million records for other tables really well with option 1. The bigger the table, the harder option 1 becomes due to the time duration of updating. 

I have a bulk loading process that loads millions of records into a few fact tables within a warehouse. Those tables are primarily clustered on time. The non-clustered indexes are in place for how the data is used to speed up performance. I typically drop some of the non-clustered indexes to both speed up inserts and to reduce index fragmentation after large data loads. However, this process of dropping and rebuilding is causing a huge amount of time as the data grows. Example: One table took 2 hours to apply a new non-clustered index on 100m+ rows. Likewise, if I leave the non-clustered indexes in place, they will increase the inserts 3x to 10x in some cases that force your hand to drop and rebuild. While it's awesome to drop and rebuild indexes, they don't really pan out that well as data grows in those tables. What options are available to me? Should I bulk up the server with more memory (currently 32GB) and cpu (4 vCPU)? Should I rethink my indexes? Should I find a balance of keeping some indexes on for reorganization versus dropping and rebuilding? (Note: I don't have enterprise edition.) I'm thinking my only option is enterprise edition with table partitioning where I can rebuild indexes per partition as opposed to the whole table. 

I'm starting a few new database projects and I'm attempting to create them at Data Tier Applications. There are two items I'm not able to find documentation for. I would like to set the db owner to SA and set the initial filesize and growth rate. Even if those items are outside the scope of the app, I would expect that there would some way to specify that at publish time, either in SSDT or SSMS. I can find no documentation either way. Is this the case? 

I have a server with a working installation of the Sql Server 2012 SSIS Catalog. I need to set up an additional instance including the SSIS Package Store service as an interim step while the packages are being re-written. The Package Store is a per-server feature, not a per-instance feature. Can these two features operate side-by-side? 

I have a number of XML Schema Definitions describing business objects. It's unclear to me what the correct way to deploy these is if my intention is to make typed xml columns. The name Xml Schema Collection implies that more than one Xsd Document definition can exist in a collection, but any example I look at shows a Collection as the specific constraint on the column, not a document within it. Do I create a schema collection called 'BusinessObjects' and create individual Xsd Documents inside it? Do I make one Collection for each Document? 

You need to work your way up the networking stack to determine where the issue is. Can you ping the destination from the source? Can you telnet to the postgres port(5432 by default) from the source? Can you connect to the postgres service with a management tool(pgadmin or psql) from the source? 

This query works in the simplest case, but does not allow adding attributes to the root element, which is required in my case. 

I have a cmdexec step in a sql agent job that includes a redirection into a file at the end. It works as expected from a CMD shell running in the context of SQL Agent service account and produces a file in the desired location. If I run it as an agent job, however, the step 'succeeds' but never produces the output file. In both cases, the service account obviously has filesystem permissions and system rights sufficient to perform this action. Is the cmdexec environment more restrictive in someway than just running cmd.exe? 

Sorry if this is redundant, but due to the crazy naming of the tools, it's hard to find the answer to the question. Question 1 Will SSIS packages, reports and so forth built with Microsoft SQL Server Data Tools - Business Intelligence (SSDT-BI) for Visual Studio 2013 work on SQL Server 2008 R2? Question 2 I'm currently using SQL Server Business Intelligence Development Studio (BIDS) for Microsoft Visual Studio 2008. I want to potentially upgrade to Data Tools - Business Intelligence for Visual Studio 2013. I assume I would need to A) purchase a new copy of Visual Studio 2013 and then B) download the free SSDT-BI software? That's assuming SSDT-BI for VS2013 works for 2008 R2. 

USQL Just the base USQL job that defines the schema then selects all fields to a new directory. No transformation happening outside of leaving out the headers. The file is CSV, comma delimited with double quotes on strings. Schema is all strings regardless of data type. Extractors tried is TEXT and CSV with both set to be encoded:UTF8 even though both are default to UTF8 according to Azure documentation on the system. Other Notes This same document was uploaded in the past to BLOB storage and imported in the same fashion into Azure Data Warehouse without errors via Polybase. 

SSIS is not really my forte. I noticed you have a table lock option on OLE DB destinations where the entire table becomes locked in what I assume is during the transaction of inserting data. What happens if you have a flat file data source with a conditional split that is parsing out data into 5 or more OLE DB destinations that are targeting the same table with table lock on? Would each OLE DB destination get blocked by one another in this scenario if data is being fast loaded (inserted) into the destinations themselves? I removed the table lock in my instance and everything seemed fine. It was splitting the data and inserting records at about 1 million records per minute. 

Interesting find today, I have a file group that is used for indexing. It has one file that has a .ldf extension, which as you know for SQL Server, is the extension for the transaction log files. My understanding is the extensions don't really matter. Whatever is first is first and anything else is secondary regardless of the extension. Does that apply to .ldf in this case when clearly it's being used for the clustered indexes? I ask because I would assume SQL Server treats .ldf differently than say, mdf's. (And before you ask, yes, there is already a .ldf assigned for the transaction log on another spindle)