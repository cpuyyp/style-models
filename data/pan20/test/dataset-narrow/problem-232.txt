I am designing a calendar database that has a lot of booking and billing information. The database itself consists of form records, which are editable, and only when an event is confirmed by staff it appears on the web calendar. The web calendar does not have a separate table, it merely queries the database for recent confirmed events. Originally the submission form was used by both staff and public and went into one giant table. I've split the booking form data into the following tables in order to normalize it: 

(Not sure if worth it to handle edge cases, but I assume joins are not required to simply rebuild each form, if the following tables are strictly dependent. I.e. if rates were separated out, I would already know the form_id for a given or and simply SELECT for it, correct?) : describe calendar info pertinent to each event date. About 80% of events only have one date. Per the above: Should the "default" (i.e. primary) event_id for each series be placed in ? Should it be a boolean (is_primary, is_default) in ? Or should it be a field in that is UNIQUE foreign key ( = form_id), default NULL? (to mark it as the default, or primary record) I want to structure it so that the php can't accidentally cause data integrity issues by setting non-unique default child record ids (primary date, default billing) for each form. : speaker info unique to a given date or form (as submitted). The idea here is that old info from the date-of would be preserved even if the canonical info for a given speaker in the contacts db is updated. the form could AJAX query the contacts db to allow read-only hinting if a speaker appears multiple times, in which case the info listed in the form would be overwritten with the then-existing contact info for the speaker and a reference to the unique key. event_speakers would be many-to-one relational to each event date. If a speaker is overall host, then they would be listed as default for the entire form, but I am not sure where best to put this, as a flag or foreign key (same question as above.) Like primary event date, primary host info would be unique to each form, one-to-one, but would be in the same format / table as all other event_dates or speakers respectively. Perhaps I should separate them into a cloned table for primaries only? The problem with that is, the client may wish to allow a different speaker to be made primary, or allow a different event in a series to be the primary date without having to INSERT/DELETE anything. OK, so here's my main problem: : list all actual dates in which the event occurs, queried by the web calendar. I currently have as listing custom event info only, i.e. dates with something different going on than the primary event_date -- different lineup, different description, or different hours. (Different lineup would simply be a reference to different event_date_id in ). However, this leaves me with that simply repeat at intervals (some ad infinitum). I can put the repeat-interval (or date-mask, e.g. mTW) in , but this only helps to set dates, not retrieve them. And I realized I needed to track dates that are not in the calendar, such as "every second wednesday EXCEPT July 1st. ALSO INCLUDING July 20th." I figure the best way of handling this is to use the info in form_control to set or generate dates but then write a record in ongoing_dates upon-submit for every custom or blackout date, (otherwise scheduled dates for which it is reserved, or dark), with default = reserved. (reserved would be treated as a suggestion if the event is not yet confirmed, i.e. pending). That way, only primary, default, or custom dates (dates for which there is any data other than "it's happening again") would have their own record created. The rest would go in ongoing_dates.its_happening to be simply queried by the web calendar. Possible values for its_happening would be e.g. "it's happening again", "it's not happening this particular interval", or... "it may be happening", which can be represented as a simple TRUE-FALSE-NULL. ongoing_dates would also have a column for: event_start and end_time (using form_control start and end time as default, allowing custom times without creating a record) and, importantly, an optional event_dates foreign key. (i.e. every event_dates record would have a 1-1 relationship with ongoing_dates, but 99% of ongoing dates would not have a separate event_dates record, unless one is created by simply adding the ID.) This way the calendar can add and delete ongoing_dates at will without losing any information (e.g. if the exact same event is made once a month instead of twice a month) My question is, does this approach make sense from a normalization standpoint? Should I separate time and visibility info entirely into ongoing_dates, so that each event_dates record refers to a date that is only listed in ongoing_dates? Or must every date in the calendar have its own full record (with mostly-default information) and the calendar queries only that? Or does it not make sense to have a separate (small) record for potentially indefinite repeating dates? Those seem to to be my three options. I don't want the database to fill up with extraneous info simply because one event happens every day of the week. 

Now I'm not sure what tbl_stock is, it's not even in the query. The query seems to be using another view (that one works fine) so maybe it's some syntax I'm not used to to designate the stock column in that ? I should have access to the old server, if anyone has an idea of the config options I should compare with the new ones. I'm not looking for fix to the query itself, just hints on what differences I could have with the old servers that could explain huge copying to tmp times. Thanks ! EDIT : I think the problem is that on the old server the query works fine, and on the new it generates 4000 warnings. Guess that prevents it from being cached. EDIT 2 : Here is the query 

We have a server with about 20 databases, all duplicates of the same original db, and we just noticed the original is missing all the foreign keys constraint for some unknown reason. So of course all the databases on that server are missing them too. Now, these databases have been used for some time now, and there is data in them. We noticed that in the export done from phpmyadmin, the constraint are at the end, so I did an export from a valid database and I now have a few hundreds lines of ALTER TABLE to add the constraints, the problem being that if I start applying that to our databases, MySQL will reject it because the keys that should have been deleted by the constraint weren't. Is there any way around that ? That's a lot of databases and a lot of tables, it's just not possible to go in all of them and delete the data manually, and since those constraints should have been here from the start, I feel that all the data that would prevent those keys from being added could be dropped to get back to the "normal" state, am I wrong ? I'm not a database administrator, so I have no idea if that's even possible. I've been told that ALTER IGNORE TABLE works like that for UNIQUE, would it work for constraints too ? Or is there a way to tell MySQL to ignore the errors when adding the constraints and then DELETE all the unmatching entries ? Thanks a lot, 

I migrated last week a rather big website to new servers. Everything seems to go fine, but since then the client noticed something we didn't see when testing before, one of the views is very slow. I tried using the profiler and indeed, the query spends 43 seconds on "copying to tmp table" and the rest is almost instant. On the old server the same query took less than a second, so clearly it shouldn't take 43 on a bigger server. I assume something isn't configured properly, what could be the cause ? I disabled the query cache (with query cache enabled it seems to spend 43s copying to query cache, if that matters). It's a replicated setup, with two MariaDB being master and slave for each other. The queries are run only on the first one, we use master / master for the ease of failover, not for load balancing. We have that exact config for tons of servers so I don't think that's it, but who knows. I ran mysqltuner and increased what it told me to increase a few times, the only things left are the join without indexes and the tmp table to disks. Don't think I can ever do anything about those. I checked, that particular query doesn't seem to go to disk (which is an SSD), and anyway I've put tmpdir in a tmpfs to be sure. It's really "copying to tmp table", no "to disk" in there. I checked with iotop, it doesn't seem to be reading from disk either. I've used explain on the query, here is the part that takes forever : 

I have a table-valued function and ordinary table and need to perform on them. Because, the engine thinks that the function is returning always one row, I try to store the results in temporary table and then to perform the join operation - in both cases, the count that are read from the table/index is wrong. 

Does this mean that I do not need to care(renew, backup, etc) about Service Master Key and Database Master Key as I am using external keys to protect the Symmetric Keys? When a new key vault is created the location is specified: 

I am wondering if I am using the hint, the table will be blocked for inserts. In the documentation is said that: 

Knowing the security objects hierarchy, we can see that in order to create a and encrypt data, we need to create: 

I am trying to detect what is causing this huge grow. One of the possible reasons explained here is . I have try to find this particular error: 

I have on virtual machine and want to install SQL Server Integration Services. On the drive is mount. I am reading a book where it is said that: 

I have a primary database in recovery mode which is part of group. Is there a way to minimally log insert operation under recovery model? I have a process that is executed each day and insert few millions of records in a table. While the operations continued the transaction log file size is increased dramatically ( from 1 GB to 40 GB). As I have read I can used some variations of which are not fully logging the operation but I am concern about the effect of switching the recovery model? 

and I am wondering which is the best value of rows to be deleted in a batch delete statement? I have seen different variants and the only thing I have found as recommendation was from here: 

I am testing feature and once the operation of encrypting column is done I have a file with hundreds objects failed to be refresh by the procedure. Should I investigate these errors and perform the refreshing by myself? What are the risks of not doing this? 

I think I have found how to fix my issue, but I am not going to accept this as answer as I am not able to explain what is causing the problem and guarantee this will work anytime. It's fix found after a lot of testing and I will be glad if someone can bring more light here. 

Basically, the index is created for 20 minutes, so I need a way to guarantee user experience is not blocked. The alternative is to have separate window for maintenance in which the database to be in single user mode. 

I am going to create history table which is populated by trigger (after insert, update, delete). As only 20% of the columns are going to be updated, I decided to log only the changed values - if the values are not changed, value is going to be used in the history table. For example: