The two arrows point to where a database mail profile can be selected. In these two instances the profile you select is basically telling SQL Agent this is your SMTP configuration and who you are going to send the emails "as", or the "FROM" field. Configuring the Alert System does require restarting the SQL Agent service. Once that is done in order to receive notifications for SQL Agent jobs you will need to configure Operators. 

There is but it would be a bit painful to implement and might be granting more access than you think. It will depend on what level of access you want (read only, or ability to manage services). This is mostly a guess for SSCM but in Window Server 2003 you used to use "SUBINACL" command line to grant access. I believe this is still availble in higher OS but you would need to check. Outside of that just specific to SSCM, this uses WMI calls in the background so at most you are going to need permissions to execute WMI calls. Understand that this is not just specific to SSCM so granting access to this may give more permissions that is needed. I am also aware that the old "sc" command was used to grant permissions to services so that might also be used as well. This was required when WMI calls did not work properly and services did not show up in SSCM like they should. 

This is likely left over from SSMS 2016 in the sense that included the module, but MS was not placing it in version folders. Which modules from the PSGallery generally do now. If you check you will see which path they go to. I have SSMS 2016, SSMS 17 and the module from the PSGallery installed on my Windows 10 machine. 

It depends on what you mean by . Windows Authentication You will need to setup the between each domain. Your domain hosting the database server will need to trust the domain holding the web application. This should allow the domain accounts access the web application can connect to the database server, without having to use a separate account. SQL Login If you do not want to deal with the domain relationship you can simple configure SQL Server in mixed mode. You can find those instructions here. You would just create a SQL Login and grant the required access to the database(s) for the web application. 

What the vendor is suggesting is a common practice for large systems/applications that require database engine, ETL, and reporting. Each of those components will suck up resources based on their need, so trying to have those run on one system is not a good practice (but it does depend on workload). Each component can run on a dedicatd server, but do require each server to be licensed. The exception will be SSIS and will depend on what deployment module you are going to use. With project deployment module you will utlize SSIS Catalog, this requires a local database instance to run on the same server as SSIS binaries. With SSRS, you simply decide which server you want the SSRS databases to be created on, they do not have to be the same server as SSRS. I would likely put the SSRS databases on the server with SSIS instance (if you are going for project deployment model. Your comment notes AG configuration and setup. If you are referring to SSRS and SSIS databases being part of an AG you can refer to the documentation links below. If however, you are talking about these components interacting with user databases that are part of an AG, that is fully supported. You just need to ensure you are connecting to the listener name for your AG. 

I would expect this might be easier to do with Extended Events but have never tried to transfer this method over to Extended Events. I am not sure if SQL Server 2008's version of Extended Events opened up access to client level errors as 2012 and higher does. The above is just a quick and dirty method that still works. 

I turned to my copy of SQL Server 2008 Internals and the DMV sys.database_recovery_status was pointed out to find the first LSN of the next log backup. Which going by BOL the column provides you with: 

EDIT The suggested edit to utilize and also add in try/catch block, while that may be useful, and in most cases a code practice, I will leave that up to the person that wants to use this function to add that additional code or functionality. Just trying to provide a basic example to go on. I did add the property to the output to include the actual server name returning information, do this on other functions just generally don't use this for more than one server at a time so it slipped my mind. 

The commands around Availability Groups for SQLPS require you to pass in the provider path for the replica, or server itself (if you are building the AG). As the example in the help documentation shows: 

Currently does not support the ability to connect to a read-only secondary replica. The SQL Server, and some of the PowerShell community, however started working with the Program Manager at Microsoft to get and SSMS improvements organized. A Trello board was opened in the past month that we are showing by votes what priority we want Microsoft to take on the improvements. This has actually already gotten some things added to the SSMS March preview. I created a card for the Application Intent functionality so go vote it up and the connect item linked. Right now your only option is to call within PowerShell using the . 

When you do a backup of the database, the backup is only going to grab the data itself along with some configuration information of the database. One of those configuration items is the initial size of the data and log file. I would expect if you checked your database properties you might find the initial size of your data file (MDF) is set to 9GB in size. 

With SSRS that would be a supported setup if you installed SQL Server 2014 SSRS on your BI server. You can view the documentation here that shows the supported database instances to house the SSRS databases. 

The other notes you have in your question though indicate that this request is based on an application table. You state things like and . All of that would tell me this is custom tables within an application database. Which unless you provide the table definition(s) there is not much we can do to help. 

This is a bit more than what I usually use but in case someone else comes across and wants to use it. The equates to in a DOS prompt and the flag simply just has it return or . This will default to 4 pings so setting just makes it do it twice instead. The variable is a method used to state that will accept an array of server names. So an example call of this function could look something like: 

To keep from being prompted you can use various ways to encrypt it, just depends on if the password has to be kept secure. If not just modify it to be something like this: 

The interal code of the function utilizes the output of to ensure the replica names match. Ensure the output of that matches the replica name of your secondary. As well if you are using a case-senstive collation on your server it will need to be an exact match, or you get the same result. 

Now I am making assumptions of what your dataset/columns are called so you will have to translate those appropriately to your report setup. For the and cells within each category the expression will be something similar to: 

The syntax error you are getting is from the this is taken as a token in SQL Server Agent context...so it will always bark at that; removing it should fix the syntax error. On the copy issue you will likely need to specify the property you want to pass into . Just passing is not going to work because that contains the full object of the output from . So change it to something like: 

To run your script on a scheduled basis you will need to build a PowerShell workflow using the Azure Automation service. There is a good walk-through of this on Azure's documentation site here. I will also note that there is a better version of this offering coming as "Elastic Jobs". It will offer much more control and features that are closer to what SQL Agent does for on-premise. They are opening up invotes for private previous now but you can read more about this from Mark Vaillancourt here. Just remember that Azure SQL databases still have a log just like any other database in on-premise SQL Servers, so make sure you your delete query is optimized for log usage. It can ensure the delete runs efficiently and does not lock up the table for an extended period.