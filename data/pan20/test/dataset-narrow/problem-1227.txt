The problem is the part which you deduce "an instance F of TQBF can be solved by a polynomial NDTM" from "NP=co-NP." Note that NP=co-NP means that you can solve the TAUTOLOGY problem using a polynomial-time NDTM, where the TAUTOLOGY problem is the following: Given a propositional formula φ, is it always satisfiable? The TAUTOLOGY problem has nothing to do with solving the first-order formulae (such as TQBF). Even if NP=co-NP, you may not be able to solve TQBF in the way stated in the question. 

This is a basic question, yet I tend to answer simple ones too :) In the book you mentioned, take a look at section 1.2.1 (page 18). It describes how various objects are encoded. Among other things, it includes a section "Strings," in which "relation encoding" is described: At times, we associate $\{0, 1\}^∗ \times \{0, 1\}^∗$ with $\{0, 1\}^∗$; the reader should merely consider an adequate encoding (e.g., the pair $(x_1 \dots x_m, y_1 \dots y_n) \in \{0, 1\}^∗ \times \{0, 1\}^∗$ may be encoded by the string $x_1x_1 \dots x_mx_m01y_1 \dots y_n \in \{0, 1\}^∗$). Functions are a special type of relation, so they can be similarly encoded. In particular, a (finite and discrete) function can be seen as a list of input-output relations: $f=\{(x_1,y_1),\dots,(x_n,y_n)\}$. This can be easily encoded by incorporating a special symbol, say $\circ$. That is, $enc(f)=x_1 \circ y_1 \circ \dots \circ x_n \circ y_n$. (The encoding can be done without any special symbol, but it adds unnecessary intricacy.) This way, the size of $f$ is the size of its encoding: $|enc(f)| \approx \sum_{i=1}^{n}{(|x_i|+|y_i|)}$. Needless to say, infinite functions have infinite size, unless you can compress the list somehow. (This brings up the "Kolmogorov Complexity Theory", which is a story for another day!) 

I took a look at Wikipedia's entry for Borel–Cantelli lemma, and almost grasped the idea. However, I couldn't still figure out how it relates to derandomization. In addition, I don't understand the meaning of "asymptotic" and "uniform" in the aforementioned paragraph. PS: Googling for Borel-Cantelli and derandomization will show several interesting results, but I don't have enough background to understand them well. 

Suresh recommended "Tikz/PGF" for LaTeX. In addition, for drawing graphs and the like, I recommend the following packages: tkz-graph tkz-berge tkz-tab For a sample usage, see the gallery of named graphs. See also other pages on $URL$ (They are mostly in French, yet you can figure out the meaning using Google Translator.) 

And many other schemes. However, note that schemes based on the hardness of discrete log (say, the Diffie-Helmann protocol or Elgamal encryption/signature scheme) will continue to be secure. 

Let σ denote the proof. Then, P vs. NP is in an NP language (since there exists a short proof for it). The reduction from the theorem (say, P ≠ NP) to the NP-complete problem (say SAT) is independent of σ. That is: 

I'm not a teacher yet, but as a TA, I once did this. I didn't find the problem in a textbook; instead, I came up with the problem myself. It turned out that, in spite of looking innocent, the problem had been the subject of a lot of debate back in 1980s, but was settled then. Well, after knowing that, I announced that solving that problem has extra credit. No one came up with the correct result, but I gave half the (extra) mark to those whose answers were reasonable. Then, in the class, I admitted that this had been indeed a hard problem, and pointed the students to the relevant history. PS1: The problem was about the DES cipher: Are there a plaintext (P) and a ciphertext (C) such that, for two distinct keys K1 and K2, DES enciphers P to C under both keys? That is, C = DES(P, K1) = DES(P, K2). The answer seemed to be "NO," but turned out that was not the case. See the relevant research here: How easy is collision search? New results and applications to DES. PS 2: The Immerman–Szelepcsényi theorem has been proved in much the same way! Quoting from Lipton's blog: 

Note: The situation is quite easy if we want to implement a queue with two stacks (with corresponding operations ENQUEUE & DEQUEUE). Please observe the difference. PS: The above problem is not the homework itself. The homework did not require any lower bounds; just an implementation and the running time analysis. 

"Michael I. Troﬁmov" claims that he has found a poly-time algorithm for graph isomorphism, which works for all graphs. The paper is given in arXiv. The companion website gives a proof-of-concept program which runs the algorithm. (The password for the program is given in the paper.) I wanted to know whether the community is aware of Troﬁmov's results, and whether it's been proved, refuted, or unresolved? 

A definition of $\mathsf{Average\mbox{-}P/poly}$ can be found in Relations between Average-case and Worst-case Complexity. Thanks to Tsuyoshi for pointing out that I actually need to use $\mathsf{Average\mbox{-}P/poly}$ instead of $\mathsf{P/poly}$. I think there are problems such as (the decision versions of) FACTORING or DLOG which are conjectured to lie in $\mathsf{NP} - \mathsf{Average\mbox{-}P/poly}$, but the conjecture is not proven based on separations between complexity classes. (Please correct me if I'm wrong.) 

On input $1^n$, generate an $n$-vertex graph $G_1$, such that it has only trivial automorphisms. Pick a random permutation $\pi$ over $[n]=\{1,2,\ldots,n\}$, and apply it on $G_1$ to get $G_2$. Output $\langle G_1,G_2,\pi \rangle$. 

This web page has links to two surveys on this topic, one of which is less technical and the other is more technical. Edit 1: @Saeed: Orientable surfaces are defined in the "more technical" paper linked above. It is also defined in $URL$ 

it is proven that if GI ∈ NPC, then the polynomial hierarchy (PH) collapses to its second level; which will be a major breakthrough in structural complexity theory. 

The answer may vary: It can be positive, negative, or open (either likely to be positive, or likely to be negative). Background The question arose when I was reading an ASIACRYPT 2009 paper. There, the author implicitly (and in the context of some proof) assumed that such one-way permutations exist. I'll be happy if this is indeed the case, though I couldn't find a proof. 

I found the papers very old and extremely hard to follow. I read Chapter 14 of Arora & Barak's book, yet apparently it does not cover everything I need. 

The following question uses ideas from cryptography applied to complexity theory. That said, it is a purely complexity-theoretic question, and no crypto knowledge whatsoever is required to answer it. I deliberately write this question very informally. Lacking details, it is possibly stated a bit incorrectly. Please feel free to point out the corrections in your answers. 

I would write the following as a comment, but it was too long to fit in. Let's first describe the meaning of “algorithms in class $\bf C$ with an oracle for a language A.” (The need for this was pointed out by Tsuyoshi Ito). We will use the same convention used by Ladner and Lynch. The convention is well-described by Bennett & Gill: 

The idea is that the adversary must be unable to distinguish the above two cases with advantage more than negligibly over 1/2. 

Here, I write an excerpt of the following paper: Valiant, L. G. and Vazirani, V. V. 1986. NP is as easy as detecting unique solutions. Theor. Comput. Sci. 47, 1 (Nov. 1986), 85-93. DOI= $URL$ 

How about foundations of disjunctive logic programming? While rather old (1992), it has over 100 examples. For instance, SLI and SLD resolutions are discussed and exemplified in section 4.2. PS: You may read some parts of that section on Google Books. 

I saw both DDH and DDH-1 assumptions in the literature. DDH-2 may also exist, but I can't recall whether I ever saw it. One example of sub-exponential hardness assumption is that, some languages in $\rm{E}$ don't have sub-exponential circuits. Such assumption is used by Impagliazzo & Wigderson in P=BPP unless E has sub-exponential circuits: Derandomizing the XOR Lemma. 

One major separation technique that does not relativize is proving circuit lower bounds. For instance, we know that all problems in $P$ have polynomial circuits. On the other hand, if we prove that an $NP$ problem has super-polynomial circuit (i.e. showing a super-polynomial lower bound), then $P\ne NP$. Unfortunately, Razborov and Rudich showed that this technique is unlikely to solve the P vs. NP problem. (See natural proof). A recent breakthrough in class separations based on proving circuit lower bounds is discussed in [1] and [2]. Another major technique which does not relativize is arithmetization. The technique was first used to prove that $P^{PH} \subseteq IP$ (Lund et al.), and later to prove IP = PSPACE. This technique was proved to be insufficient to resolve P vs NP by Aaronson and Wigderson (termed algebrization barrier). 

Preamble Interactive proof systems and Arthur-Merlin protocols were introduced by Goldwasser, Micali and Rackoff and Babai back in 1985. At first, it was thought that the former is more powerful than the latter, but Goldwasser and Sipser showed that they have the same power (with respect to language recognition). Hence, in this post, I'll used the two concepts interchangeably. Let $IP[k]$ be the class of languages admitting an interactive proof system with $k$ rounds. Babai proved that $IP[O(1)] \subseteq \Pi_2^P$. (A relativizable result.) At first, it was not know whether unbounded number of rounds can increase the power of IP. In particular, it was shown to have contradictory relativizations: Fortnow and Sipser showed that for some oracle $A$, it holds that $coNP^A \not\subset IP[poly]^A$. (Therefore, relative to $A$, $IP[poly]$ is not a superclass of $PH$.) On the other hand, the following paper: 

Each node is either a variable name ($x, y, z, \ldots$), a number, or an operation (+,-,×). The in-order traversal of the tree should result in a valid polynomial. Operation nodes have in-degree 2. Other nodes have in-degree 0. All nodes have out-degree 1 (except root, whose out-degree is 0). 

In these cases, one might prefer to read a more clear version submitted to the authors' home page. Unfortunately, the author version is not always available; and even in that case, they might be very lengthy (the so-called full version of the paper). In the case of bitmap fonts, I found a good strategy: Find the postscript (PS) version of the paper, and convert the bitmap fonts to their vector counterparts using pkfix or a combination of pkfix-helper + pkfix, whichever applies (read the documentation). However, I still can't find a workaround for other problems. Specially, small-font papers bother me a lot. My best bet is to use a desk lamp: I don't know why, but magically the words seem larger under its light! 

Many public-key cryptosystems have some kind of provable security. For example, the Rabin cryptosystem is provably as hard as factoring. I wonder whether such kind of provable security exists for secret-key cryptosystems, such as AES. If not, what is the evidence that breaking such cryptosystems is hard? (other than resistance to trial-and-error attacks) Remark: I'm familiar with AES operations (AddRoundKey, SubBytes, ShiftRows, and MixColumns). It seems that the hardness of AES stems from the MixColumns operation, which in turn must inherit its difficulty from some hard problem over Galois Fields (and thus, algebra). In fact, I can restate my question as: "Which hard algebraic problem guarantees the security of AES?" 

where it's proved that If $FP^{NP} = FP^{NP[log]}$ (that is, allowed only a logarithmic number of queries), then $P = NP$.