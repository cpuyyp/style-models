Suppose you are given both a graph $G(V,E)$ and the exact number $C$ of vertex covers of $G$. Now suppose that $G$ is subject to a very small perturbation $P$, leading to $G'=P(G)$. More precisely, the perturbation $P$ is restricted to be one of the following: 

Take the formula in your example, remove the clause $\lnot A \lor \lnot B \lor \lnot C$ and add the following $2$ clauses: $\lnot A \lor \lnot B \lor \lnot E$ $\lnot B \lor \lnot C \lor E$ You will get a minimal unsatisfiable formula with $n=5$, $m=9$ obeying condition 5. In general, you may randomly pick a clause $l_1 \lor l_2 \lor l_3$ and split it in $2$ clauses: $l_1 \lor l_2 \lor v$ $l_2 \lor l_3 \lor \lnot v$ where $v$ is a new variable. Each time you do so, both $n$ and $m$ are incremented by $1$. Repeating this process allows you to "stretch" the initial unsatisfiable core, and to obtain minimal unsatisfiable formulas (obeying condition 5) whose $r=\frac{m}{n}$ tends to $1$ as $n$ grows (which is pretty rare, as formulas with $r=1$ are satisfiable with high probability). 

It's widely known that CNF formulas can be roughly partitioned in 2 broad classes: random vs. structured. Structured CNF formulas, in opposition to random CNF formulas, exhibit some sort of order, showing patterns that are unlikely to happen by chance. However, one may find structured formulas showing some degree of randomness (i.e. certain specific groups of clauses seems much less structured than others), as well as random formulas with some weak form of structure (i.e. certain specific groups of clauses seems less randomic than others). Hence it seems that the randomness of a formula is not just a yes/no fact. Let $r: \mathcal{F} \rightarrow [0,1]$ be a function that, given a CNF formula $F \in \mathcal{F}$, returns a real value between $0$ and $1$ inclusive: $0$ means a pure structured formula, while $1$ means a pure random formula. I wonder if someone has ever tried to invent such a $r$. Of course the value returned by $r$ would be (at least this is my intent) just a practical measurement according to some reasonable criteria, rather than a solid theoretical truth. I'm also interested to know if someone has ever defined and studied any statistical indicator that can be used in the definition of $r$, or in determining other useful overall properties of a formula. By statistical indicator I mean something like that: 

In random instances, the HCV is very low (all variables are mentioned almost the same number of times), while in structured instances it is not (some variables are used very frequently and some others are not, i.e. there are "clusters of usage"). AID (Average Impurity Degree)Let $h_F^{+}(v_j)$ be the number of times $v_j$ occurs positive, and let $h_F^{-}(v_j)$ the number of times it occurs negative. Let $i: \mathbb{N} \rightarrow [0,1]$ be a function that, given a variable $v_j \in V$, returns its ID (Impurity Degree). The function $i(v_j)$ is defined as follows: $i(v_j) = 2 \cdot \frac{min(h_F^{+}(v_j), h_F^{-}(v_j))}{h_F(v_j)}$. Those variables occurring half of the times positive and half of the times negative have maximum Impurity Degree, while those variables occurring always positive or always negative (i.e. pure literals) have minimum Impurity Degree. The AID is simply defined as follows: 

The $\oplus$3-REGULAR BIPARTITE PLANAR VERTEX COVER problem consists in computing the parity of the number of vertex covers of a 3-regular bipartite planar graph. 

If I correctly understand the very core of your question, you want to take a relatively easy instance (since you put yourself in an area where $\frac{m}{n}> 4.3$), and transform it into a hard one by embedding a solution. I doubt that this would work. Experimental data suggest that, when constructing a random instance "around" a predefined solution $x$, such instance will be easier than usual (compared to similar instances having the same $n$ and $m$). It's like if the hidden solution helps the SAT solver, guiding it through the search space. Normally, to construct such an instance, we generate random clauses as usual (e.g. choosing $k$ literals at random and negating each of them with probability $p = \frac{1}{2}$) but we discard those clauses which are not satisfied by our hidden solution $x$. For what concerns your approach of constructing $\phi|x$ from and hard instance $\phi$: I've never tried that, but I "feel" that $\phi|x$ will become easier, if not trivial. I believe that doing that would augment the hit count of $x$'s literals (the hit count of a literal $l$ is the number of occurrences of $l$ in a given formula), and that this would drive the SAT solver to the target. Maybe the solution spaces of $\phi$ and $\phi|x$ would be similar (if not almost identical), as happens in Ryan Williams' example of SAT0 (almost identical solution spaces, but completely different hardness). Did you try your approach in practice? It would be interesting to see how the same SAT solver behaves on $\phi$ and on $\phi|x$. EDIT 1 (23rd Sep 2010): After thinking a little bit more, I feel that actually $\phi|x$ solution space would be very different than $\phi$'s. You are adding a literal to each clause, so you're giving more degree of freedom to such clauses (i.e. each clause has more chance to be satisfied): probably the resulting solution space would be massively transformed. EDIT 2 (1st Oct 2010): I've thought about the following very simple and not original idea. Given an initial instance $\phi$ and an assignment $x$: 

Remove from $\phi$ all those clauses unsatisfied by $x$. This will enlarge the solution space, and should embed $x$ in it. Suppose you removed $m_x$ clauses. Now randomly add $m_x$ new clauses, taking care that they are not unsatisfied by $x$ (this will narrow the solution space again, but without pushing $x$ out of it). 

In a read-twice opposite CNF formula each variable appears twice, once positive and once negative. I'm interested in the $\oplus\text{Rtw-Opp-CNF}$ problem, which consists in computing the parity of the number of satisfying assignments of a read-twice opposite CNF formula. I was unable to find any reference about the complexity of such problem. The closest I was able to find is that the counting version $\#\text{Rtw-Opp-CNF}$ is $\#\text{P}$-complete (see section 6.3 in this paper). Thanks in advance for your help. 

This question is probably on the border line between on-topic and off-topic, however I've seen similar questions here, therefore I'll ask it. 

Finally, to make things worse, it seems that the choice of the term "hull" was unfortunate: the correct term to indicate what I had in mind is "outer face" (although such term is probably meaningless for non-planar graphs). 

Alternatively, I would be content also to know any procedure to generate uniquely satisfiable instances. The only approach I'm aware of goes under the name of planted SAT instance generation: you randomly generate an assignment of $n$ variables, then you generate only those clauses which agree with such assignment. This approach is unsatisfactory for my purposes, for the following reasons: 

On satisfiable instances of $PHP$, DPLL based SAT solvers will furnish a satisfying assignment in linear time. To see why, observe how the CNF encoding of an unsatisfiable instance of $PHP$ with $n$ holes and $n + 1$ pigeons is sintactically identical to an instance of $k = n$ Graph Coloring, where the input graph is a clique of $n + 1$ vertices. Similarly, the CNF encoding of a satisfiable instance of $PHP$ with $n$ holes and $n$ pigeons is sintactically identical to an instance of $k = n$ Graph Coloring, where the input graph is a clique of $n$ vertices. Now, coloring a clique of $n$ vertices with $n$ colors is straightforward: scan the vertices, and assign to each of them one of the remaining colors (already assigned colors are automatically ruled out by the clique-ness of the graph, using unit propagation). Whatever of the remaining colors you choose, it will be good and will lead you to a satisfying assignment. From the DPLL solver point of view: each time it will try to guess the boolean value of a variable $v_i$, such value will be right (whatever it is), because there will certainly be a satisfying assignment in which variable $v_i$ has the guessed value. Unit propagation will do the rest of the job, by guiding the solver along the satisfying path (in other words: by preventing it to guess wrong values). The search then proceeds one variable after the other, linearly, each time making the correct guess. 

If we rearrange the drawing in the following way (stretching the blue nodes to the outside) then the blue nodes become the hull: 

Suppose for a moment that $\mathbf{P} = \mathbf{BQP}$ is true: it is possible to efficiently simulate quantum computation by classical deterministic computation. Now, from that very little I know about quantum computing (I'm much less than an amateur when speaking about it), if I'm not wrong a Quantum Turing Machine is able, by its very definition, to generate true randomness. Henceforth, if $\mathbf{P} = \mathbf{BQP}$, then there exists a classical deterministic algorithm which is able to quickly output true randomness (by just quickly simulating some Quantum Turing Machine). But... what would randomness be then? Would it even exist at all? Would you be willing to continue to call it randomness? John Von Neumann once said: "Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin". But the above classical deterministic algorithm would indeed be an arithmetical method of producing random digits, no? When thinking a bit about it, I was also considering Max Tegmark's view (expressed in his last book) according to which randomness does not exist at all: he believes it is only an illusion, more precisely the subjective feeling you perceive each time you get cloned (i.e. each time the Universe you live in gets branched in 2 or more Universes). From my amateur point of view, if I were told that we live in a $\mathbf{P} = \mathbf{BQP}$ world, then I would believe that there is no such thing as randomness. How could it even exist, if it is perfectly producible by a mechanical procedure? Where would it reside? If true randomness can be implemented into a C++ concretion of the above classical deterministic algorithm, then where would it actually be? Would it be hidden somewhere? Would it continue to be present somehow, inspite there is no trace of it in the C++ code? For me, simply it would not exist. Now, to summarize, this question is not about how likely you consider $\mathbf{P} = \mathbf{BQP}$ to be. Rather, the question boils down to the following: 

Update 16/04/2013 23:00 Each satisfying assignment corresponds to an edge cover of a $3$-regular graph. After extensive search, the only relevant paper I was able to find on counting edge covers is the (3rd) one already mentioned in Yuval's answer. At the beginning of such paper, the authors say "We initiate the study of sampling (and the related question of counting) of all edge covers of a graph". I'm very surprised that this problem has received so few attention (compared to counting vertex covers, which is widely studied and much better understood, for several graph classes). We do not know whether counting edge covers is $\#P$-hard. We do not know whether determining the parity of the number of edge covers is $\oplus P$-hard, either. 

Give $x$ to the oracle. If the oracle said $x \in P$, you've done. Otherwise, let $S$ be the violated half-space returned by the oracle. Let $y$ be the orthogonal projection of $x$ on $S$. 

Which would be the consequences of #P = FP? I'm interested in both practical and theoretical consequences. From a practical point of view, I'm particularly interested in consequences on Artificial Intelligence. Pointers to papers or books are more than welcome. Please do not say that #P = FP implies P = NP, I already know that. Also, please do not say "there will be no practical consequences if the algorithm runs in time $\Omega(n^{\alpha})$, where $\alpha$ is the number of electrons in the Universe": permit me to assume that, if a deterministic polynomial time algorithm for a #P-complete problem exists, its running time will be "clement" ($O(n^2)$, for example). 

The process can be iterated for each constraint $c$ of a 0-1 $LP$ instance $I$, each time substituting $c$ with its corresponding optimal constraint $c^*$. At the end, this will lead to the optimal polytope $P^*$ of $I$. Then, since the vertices of $P^*$ are all and only the integer vertices of the initial polytope $P$ of $I$, any algorithm for $LP$ can be used to compute the optimal integer solution. I know that being able to compute $P^*$ efficiently would imply $P = NP$, however the following additional question still stands: 

Which is the complexity of counting the number of vertex covers of trees? Is it still #P-complete, as for general graphs? 

I have part of a proof attempt of $\oplus \mathbf{P} \subseteq \mathbf{NP}$. The proof attempt consists of a Karp reduction from the $\oplus \mathbf{P}$-complete problem $\oplus$3-REGULAR VERTEX COVER to SAT. Given a cubic graph $G$, the reduction outputs a CNF formula $F$ having both the following properties: 

Let $F_1$ be a satisfiable CNF Formula with $n$ variables and $m$ clauses. Let $S_{F_1}$ be the solution space of $F_1$. Consider the problem of determining, given $F_1$, another CNF Formula $F_2$ with the same set of variables as $F_1$, with $S_{F_2} = S_{F_1}$ (same solution space as $F_1$), but with as few clauses as possible (the only aim is to minimize the number of clauses, so how many literals each clause may have is not relevant). 

Reading your question, I also agree in saying that Satisfiability Modulo Theories are closely related to your needs. I would suggest to read the book Decision Procedures - An Algorithmic Point of View. 

It is NP-complete: it's easy to see a straightforward reduction from Vertex Cover to it (i.e. can we cover all the edges with $k$ or fewer nodes?). The following decision problem is called Monotone-2SAT: 

The existence of $A$ allows us to densify $G$, by increasing its minimum degree to any constant we desire. 

HCV (Hit Count Variance)Let $h_F: \mathbb{N} \rightarrow \mathbb{N}$ be a function that, given a variable $v_j \in \mathbb{N}$, returns the number of times $v_j$ appears in $F$. Let $V$ be the set of variables used in $F$. Let $\bar{h}_F = \frac{1}{|V|} \sum_{v_j \in V}{h_F(v_j)}$ be the AHC (Average Hit Count). The HCV is defined as follows: 

The problem #MONOTONE-2SAT is known to be #P-complete. This means that #SAT can be reduced to it. My question is: given a #SAT instance $F$, which is the transformation that converts $F$ to its corresponding #MONOTONE-2SAT instance $F'$? A second question is: let $K'$ be the number of solutions of $F'$, and let $K$ be the number of solutions of $F$. Does $K' = K$? Or we must use a back transformation that converts $K'$ to $K$?