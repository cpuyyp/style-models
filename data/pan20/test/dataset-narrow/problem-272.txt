We have a database running SQL Server 2008R2 which requires an upgrade (to SQL Server 2014) because it is a pre-requisite for the latest edition of the software which the database is the back end of. Integration Services (2008R2) is also running on this server, with many .dtsx packages stored in MSDB. They are mainly used for ETL purposes and transforming data for reporting reasons. Due to the nature of the environment, it will need to be an in-place upgrade. The database engine upgrade will be a breeze, but I was wondering specifically about the .dtsx packages in MSDB. Will they automatically upgrade when the database engine and integration services is upgraded, or will there be a need to manually upgrade them after this. Are there any known issues with upgrading Integration Services between 2008R2 and 2014 SP2? Fortunately we have a mirrored environment running in a VM which we can mess around with in terms of testing the application (and packages) before we move to Live. Thanks 

I've just inherited about 20 instances of SQL Server, as part of a wider acquisition project. I'm in the process of assessing performance and I don't like the way maintenance plans have been implemented. I'm seeing daily blanket index rebuilds (I can deal with this one) and also daily manual updating of statistics. Around half of the databases have been set to Auto Update Statistics = False, for reasons which are not clear other than I am told it is to reduce 'Performance Issues'... I always thought, and worked to, best practice of setting this to True and felt the Manual Update was not necessary if this setting was True. Am I wrong? Can anyone explain what the benefit would be in having this set as False, but doing a daily manual update instead? I should mention that some of the databases are highly transactional (millions of Inserts, Deletes, Updates per day) Others are low in terms of transaction rates, and some are all but read-only. There is no rhyme or reason though as to which have the Auto Update setting set to False. It appears to be a lottery. 

Unless you have Systems Centre Operations Manager with the SQL Server Management Pack, a good way is to set up some sort of data collector that queries the appropriate DMV's. Assuming you are running SQL Server 2008 + you can set up a Management Data Warehouse on the instance using SQL Server Management Studio. The result is a graphical user interface in the style of a performance dashboard. Link: Generic How To Guide on MSsqltips Set up out of the box is easy and informative, and when you get bored of the default measures, it can be easily extended. 

I was recently asked to take a look at at some SQL Servers for a friend and carry out a few config and security audits. There were a couple of servers belonging to the same application group which interested me because there were many inconsistencies and generally odd configuration options set. One thing which attracted my interest: Named instances configured on a dynamic default port (i.e tcp port 1433). SQL Browser was running but clients connected using ,. My thoughts on this were that this was totally wrong. I mean, why bother creating a named instance only to put it on a default port, and why make that port dynamic? I contacted the vendor who very assertively told me that their documentation and installation guide stated that instances should be named and configured on dynamic port 1433 ("As you know, this is the default port that connections expect SQL Server to be listening on and changing this would void your client's support contract with us...") I can't bring myself to fashion a reply at the moment. Part of me feels it's not worth the argument as I'm doing this for a favour really. To arrive at the point, my question is two-fold: When would it ever be useful to operate named instances on default ports, and why would it ever make sense for them to be dynamic? 

This is mainly theoretical, but I would like to have a documented list of options if this happens in the future. Today we had a critical disk error on the SAN, meaning the disk holding the Transaction Log files for one of our production instances crashed and initially looked like it was dead. Obviously, down go the instance, databases, and therefore the applications running on it. Our data center guys were busy working on the what, why and how's of the disk failure, meanwhile I was quickly coming up with a list of options for database recovery. Ok, so the Data Center guys recovered the disk. It was a VPLEX error, rather than a physical hardware fault. But what I found meanwhile was that I didn't have a great deal of options. The instance would not start, given that all log files for both Sys and User databases were out of reach. Would the instance have restarted if the Sys database log files were on a separate disk that was 'Up' I could access the .mdf files, so I had the option of copying them over to another server, then attaching them with new log files on another volume. Either that or restoring the databases to another server\instance using our fairly resilient backups. Either option meant work for the App guys, because all the applications and associated services would need to re pointed. I had another option of trashing the instance on the server and reinstalling it with the same instance name, then restoring all the databases from full ad log backups. Theoretically, this meant no work for the App team but had a serious time overhead for me (the only DBA). Am I missing any options here? I only started this job recently, and its fair to say documentation here is limited. I've been busy this past few months putting together inventories of our SQL Estate, looking at patching/upgrade gaps etc, and getting involved in several projects. I think its fair to say that a documented disaster recovery plan for just this type of scenario is now at the top of our hierarchy's agenda. Any help appreciated. 

Now I know that it would be possible to mount a new disk with the same drive letter, but I'm looking for a way where I could safely bring the instance up sooner than that, given in a critical situation such as this, the datacenter may have a list of priorities as long as their arm and it could be several hours or worse before the new volume could be ready. Thanks all... 

I can only tell you from experience I have moved to this solution myself from what used to be a simple 'truncate SQL table and rebuild it' using an SSIS integration script for circa 1 million rows. In real time I have an improvement of 32mins (34 minutes for SSIS led truncate, rebuild package vs. 2 mins for Linked Server / MERGE statement) 

This may be a very silly question, but when I'm looking at the current value of a 'per second' counter in PerfMon, for example - how is that value actually measured? Let's say I use PowerShell's Get-Counter method to return a result for a specific server, as follows: 

I've tackled this in the past by doing the following: First, before doing anything, get a quick configuration report. You'll want to put everything back 'as is' when you configure your new Standard installation. Here's a useful script I use to get some key configuration settings - feel free to amend: 

I'm familiar with database encryption options for SQL Server such as TDE and ensuring backups / backup locations are encrypted and secured properly. I'm doing some work for a new client who has asked me to detail options for end to end encryption, in other words, to ensure data sent from database server to application server remains encrypted en route and cannot be intercepted (or rather, interpreted) by any sort of packet monitoring. I know about using SSL certificates to encrypt connections to the database, though it is something I have seldom configured. I really don't know if that is an adequate option for end to end encryption as surely this cannot be configured using SQL Server alone? In my mind there has to be something at the application end, or on the network route that enables end to end encryption like this? My knowledge of network security is basically limited to firewall rules and general routing, so I'm looking for options available to me to either research further or put into practice. I wary that this question will be closed because it is opinion based, so I'm really asking for solid options within SQL Server itself (or within the remit of a DBA) to configure end to end encryption of data. 

Just for anyone facing this mysterious issue in the future... This particular issue was resolved by removing packet inspection rules on TCP Port 1521 (Oracle default) within the VMWare NSX. This was causing a bottleneck of data coming out of our Oracle infrastructure (IBM AIX) and back into our NSX infrastructure. 

Firstly, forgive my poor question title. I couldn't think of the best way to summarise this. Essentially I'm putting together some scenario based DR documentation, with this specific scenario being a SAN issue where we have lost disks which hold either log or data files. I'm assuming that MASTER has been affected (ie. the drive where it's files exist isn't accessible) and SQL services won't therefore start. My specific questions are as follows: 

To me that means the instance wants 4GB and can only consume 878MB. There are no other memory intensive processes running on this server. The output of dm_os_process_memory is as follows: 

You might be able to get a performance improvement by using OPENQUERY from the SQL Server side to the Oracle database set up as a linked server. Then you could use the SQL Server MERGE function to merge in any new rows from Oracle to SQL Server, that is, any rows that do not match, column for column, with what you already have in SQL Server. Something like this: 

Since we don't know what happened with your original installation, i.e. whether it was successful or not) it is difficult to pin down the exact issue. I am sure SQL Server Database Engine has never been successfully installed, so is not eligible to repair. Also, it could be an SA or Service Account corruption Issue. You really need to be re-installing SQL Server fresh and not repairing. So you could try to do a fresh install, but first I would uninstall anything related to SQL Server by going to Add/Remove Programs. Have a look at this link Uninstall SQL Server for instructions on how to do a full uninstallation. Check Services, to ensure there are no existing services related to SQL Server. Then, I would do a complete reinstall, not a repair, and I would do it as a new Named Instance. Follow the installation through and see how you get on. Make sure you are an administrator on the machine, and pay very special attention to the service account page of the installation. For this, make sure the SQL Service is running as the default NETWORK account. If it has picked up an account that is not eligible to start the service as part of the installation you'll get issues. In short, you appear to be repairing something that either doesn't exist or is not stable to repair. You need to ensure any remnants of a previous install is removed fully and then start again. 

When I create a database, I have always changed the database owner from me (usually my domain account) to another, either a standard service account used for ownership, or to the usual sa However, I've just inherited a series of instances where a lot of the databases have been created by another member of staff, who's now left, so naturally his domain account has been deleted. Didn't run into any issues with this until yesterday when I tried to create a stored procedure on a database that would execute as owner (blindly believing it to be 'sa'). The create failed because the owner didn't exist, so I had no choice but to make an ad-hoc change as the client needed the stored procedure creating. There are tons of production databases under his name so I want to be both pro-active and change-control savvy. First question is, what are the potential issues this could cause now or in the future? Second question, can I safely just change ownership to that of a standard account or 'sa'? Could it, will it, have any weird impact? 

I have a SQL Server with both TCP\IP and Named Pipes protocols enabled and configured. From the server itself I can connect to the database using either TCP or Named Pipes, by specifying the connection as However from another server I can only connect using TCP\IP using servername\instancename. Adding the prefix just causes a connection timeout failure. Do I need to open any specific ports in order to use Named Pipes? This is a named instance running on a dynamic port. The dynamic port is opened in the firewall. The reason I'm looking into this is because I'm experiencing intermittent issues with an ODBC connection - timeouts, connection dropouts etc. From what I've read these issues have in the past been resolved by enabling Named Pipes - which I've done, but then by default my connections are using TCP, and by force, I can't get it to connect from this server.