This does not generate a failed request log strangely enough - I have set the failed request tracing to trace errors with error codes 400-999. Also worth noting is that if I open the Configuration feature from within IIS I see an access denied error. I have exactly the same set up on my local dev machine to the same UNC path and the same user it works. Just on the test server it does not. What am I doing wrong? 

I have a virtual directory in my site (test environment). It is a UNC share which is also used as a public FTP. It is configured to connect as a domain admin account and "Test settings" says everything appears to be working. However when I try to connect to it I get: 

In the end, I have put a script in init.d and used the runlevel to execute it first. The script updates the jvm.options file using sed like so: 

Our MySQL server seems to be using a lot of memory. I've tried looking for slow queries and queries with no index and have halved the peak CPU usage and Apache memory usage but the MySQL memory stays constantly at 2.2GB (~51% of available memory on the server). Here's the graph from Plesk. Running top in the SSH window shows the same figures. 

We had a major network issue where our secondary domain controller (responsible for Win2k3 boxes) died and had to be rebuilt (I beleive this is what happened, I am a developer not network admin). Anyway, I am working remotely via VPN at the moment and since this happened, I am getting an authentication box when trying to access certain areas of SDL Tridion via IE (Tridion 2009 SP1 is IE only) it seems like somewhere my credentials are not being passed correctly or the ones cached on my laptop do not match the ones the Domain Controller has. This only seems to affect Windows 2003 servers. Our IT support thinks that the only way to sort it out is to connect my laptop directly to the network. I am not planned to go to the office for a few weeks at least and this issue means I have to work with Tridion via Remote Desktop. We thought changing the password on my account might work but this didn't help. So basically my question is, is there any way I can reset my credential cache without having to reconnect to the network? Or is it IE that is causing the problem perhaps, since I can RDP to servers and use Tridion 2011 instances in other browsers fine? I am on Windows 7 using SonicWall VPN client. 

I'm building an API which will only be accessible to local services by using in the directive. How can I make this return a 404 rather than 403 status code when accessed from a remote address? That way I can hide the fact that there is a service at the requested location. I'd rather preserve the semantics of than use . 

tries to give you as specific information as possible (the opposite case would be to always print , which is technically correct but not very useful). ANSI is not a specific encoding, and UTF-8 is a superset of ASCII, so it will report ASCII for both if the bytes contained in the file all are inside the ASCII charset. 

Once you create a host from an AMI you can create a snapshot of the volume (should be done before starting the instance to avoid having irrelevant state on the disk), and create a new AMI from that snapshot. Then you have your own AMI with the same contents. As for which one to choose, there are only two 14.04 images available: EBS (presumably PV) and HVM. Which one is best for you depends on your requirements. 

As others noted at the linked page, this is a bad idea. Passwords are generally much less secure than using public/private key pairs. That said, if the server sent just it isn't using the new configuration yet. Try restarting . 

The semantics of your code simply requires the to "be instantiated" (that is, run) before the child directory is created. There is no optionality implied by this - will run if exists, otherwise does not run. 

Another solution is to ask specifically for files only by using . You can also suppress the printing of the starting points by using . 

In our data tier of TFS infrastructure, the folder C:\data\TfsWarehouse.0.db has grown upto 2 gb. Since C: drive is of only 10 gb ..can we move this folder to some other drives. Can we delete the files of subfolder of TfsWarehouse.0.db. Or is there any other method to create some space. Thanks upfront Kabir 

i want to change the local computer policy in a windows 2003 terminal server which is part of domain. The policy i want to change is "Sets a time limit for active Terminal services session" to Enabled and Active Sessions limit to 5 minutes, so that all active session can be terminated after time limit. However, I don't want to enforce it on "domain\Domain Admin" who are part of local group Administrators as well local admin users of the server. Mostly I would like to enforce this policy on Domain\domain users who are member of Remote Desktop Users of this terminal server. can somebody please help . Thanks 

We have a terminal server in our environment, where Domain\Domain Users can login through Remote Desktop. We want to force log off these users after half an hour. However Administrators (this includes Domain\Domain Admin also) should not be affected by this. They should be able to connect to terminal server with out any interruption. Can somebody guide us about it. Thanks upfront Kabir 

To get the additional features of dashboard in TFS 2010, we installed SharePoint Server 2007 with Service Pack 2 on the apptier of our TFS setup. After configuring it, we are not able to see the additional dashboards. What needs to be done to enable additional dashboard. Site Action->Site Setting->Site Features->Agile Dashboards with Excel Reporting->Activate does not give any result and do not throw any error. Thanks Upfront. 

ask for the password once, create a named pipe, link that to a background process, and pass all the SQL to the named pipe. 

(Assuming you're the one who will be setting it up, and you're only asking about which distro to use for the server.) Most distros are suitable for a simple server, so I'd recommend using the server flavour (if any) of a distro you're familiar with. There really isn't much to it - you'll need to do at least some rough scaling of the amount of RAM and disk + speed of CPU/network, but that's another subject. 

will be much faster than FTP for synchronising files repeatedly - it simply checks the file size and timestamp on the receiver to find out whether it should overwrite the file, thereby minimising the amount of data transferred for each synchronisation. If there is a lot of files you may want to consider creating a tarball automatically (using for example ) and syncing that, for example using (untested). can be used to make the testing a single command. Something like this should do the trick (you have to use Tab for indentation): 

This sort of thing usually happens because the terminal has been corrupted in some way, often by ting a binary file. Have you tried a ? If it really is the Bash history that is broken that is stored in . 

If you're asking how to run more than one statement with a single command, you can either simply separate them by semicolons: 

That's not running twice, you're just seeing the and the processes, which are separate. Try to see the process tree, explaining how they are related. 

To reliably send a lot of traffic to a system under test you need to put the client and server as close as possible. The easiest way to do that is to put the client system on the same network as the server (unless you're testing the router or firewall). 

But I think this is really dirty. There must be a nicer way of doing this? I tried setting the ES_JAVA_OPTS in the same way but it didn't work. My ultimate aim is make scaling up easier. 

Can anyone tell me if there is a way to change the above so that the contents of all Multimedia Components for the given publication go to the filesystem? I can't seem to find this in the documentation. 

Can I access my network drives on my host machine on the VM? This is possible on Virtual PC but I can't find the setting on VMware. I am running Windows 7, VMWare workstation 7.1.3 build-324285 

So it follows that if I run a script to set the ES_JAVA_OPTS at startup before the elasticsearch service starts. I tried a lot of things to try and get the ES_JAVA_OPTS to be set but when looking at the logs I can see it was using the default. I've tried various things: 

If I change the "Physical Path Logon Type" from ClearText to Network. I get the following IIS error: 

I am using DD4T on an SDL Tridion project and am using the following configuration in the storage config in order to publish Binaries (binaries in this case being anything store in Multimedia Components) to the filesystem but keep Pages in the Content Delivery database. I am finding that as requirements change for what Binary files are needed e.g. the customer wants to offer Adobe Illustrator files for download, I am needing to add more types to the list by changing the config and restarting the deployer which is not ideal. 

This is an ASP.NET YSOD. I am not sure why ASP.NET is getting involved at all as it's a static .jpg file I'm requesting. I tried turning on failed request tracing and this is the specific error: 

As per the link $URL$ , I downloaded the Windows Installer Cleanup Utility. Remove previous installation and reinstalled Office 2003 with SP3. Everything works fine. Thanks. 

I have installed MOSS2007 along with TFS2010. While browsing through dashboards I found following error- Excel Web Access-An error has occurred. Please contact your system administrator if this problem persists. The eventviewr is flooded with error- Excel Services: Unexpected exception while trying to access Shared Services Database;. Error = Cannot open database "SharedServices1_DB" requested by the login. The login failed. Login failed for user 'Domain\Servername-01$'.. I have installed MOSS2007 on apptier of TFS Setup. Please suggest. Thanks Upfront. 

For TFS 2010 application tier. data tier and build server, What should be the best practice for IP address. Shall we give static IP available or we should use DHCP forthis. 

Install TFS2010 10.0.30319.1 (RTM) on Windows Server 2008 R2 Enterprise(app tier) SQL 2008 SP1 with Cumulative update 2 on Windows Server 2008 R2 Enterprise(data tier) Reporting Service is installed on app tier. After this installation worked fine we installed SharePoint 2010 on app tier. After installation we followed $URL$ for configuration. We are not able to perform the last step described in the link as following error occured- TF249063: The following Web service is not available: $URL$ This Web service is used for the Team Foundation Server Extensions for SharePoint Products. The underlying error is: The remote server returned an error: (404) Not Found.. Verify that the following URL points to a valid SharePoint Web application and that the application is available: $URL$ If the URL is correct and the Web application is operating normally, verify that a firewall is not blocking access to the Web application. We have also noticed that Document Folder in Team project also have red x. 

It looks like the package is not installed with APT: "Package hadoop is not installed, so not removed". If you have installed by some other means, such as or , you'll have to look into how that package specifically can be uninstalled. 

There's nothing fundamental about a desktop installation of Linux (unlike other operating systems) - you can "convert" one to the other by installing/removing packages and enabling/disabling services. For example, if you want to make sure to avoid any resource-intensive graphical logins it may be enough to remove the package and any Unity packages. You may also want to remove X and graphics drivers. Then check to see if any useless services are still running. 

If you want to do some automated tests rather than manually checking the changes every time you can use Selenium. PHP has a lot of stupid bugs, and personally (after an MSc + 10 years as a programmer) I believe it is positively brain-damaging. It is so riddled with terrible mistakes that I hope you will consider using a more sane language, such as Java, Python, Ruby or even Perl. Bad habits take a long time to die, and PHP teaches a lot of bad habits. 

However, this changes the mode of every file in the directory. Since most of the files (thousands, changing every day) should be handled by another application, how can I tell Puppet to leave all files not in alone? I can't use because that requires that I know in advance the names (or at least globs) of files I don't want to manage. 

You are passing the input to the loop on standard input (file descriptor 0). Any commands inside the loop which read from standard input will consume that stream, making the content not available for the second time around. This is a common issue with passing standard input to what is effectively multiple commands. The easiest way to fix this is to use a file descriptor that the commands inside the loop are unlikely to use, for example: