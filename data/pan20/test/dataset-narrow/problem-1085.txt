The proof that it is decidable whether an arbitrary deterministic context-free grammar (or equivalently a deterministic pushdown automaton) has an equivalent finite state automaton describing the same language is essentially a proof of the state complexity of finite automatons describing deterministic context-free languages: the bound on the size of these finite automatons in terms of the deterministic automatons gives bounds on the length of the decision procedure. For details, see "Regularity and related problems for deterministic pushdown automata." by Leslie G. Valiant. 

I really like the introduction to the Partitioning problem as given by Brian Hayes given here. He uses the problem of partitioning a set of children into teams of equal total ability (assuming you can quantify the ability of every child using a number), and also explains the greedy algorithm usually used by children to solve this problem. It's a very simple problem to understand, it's easy to understand the algorithm, surprising that it is (most likely) very hard in general and embarrassing that we are still unable to prove the last bit. 

The answer below is 'cheating', in that while it doesn't use any space between operations the operations themselves can use more than $O(1)$ space. See elsewhere in this thread for an answer that doesn't have this problem. While I don't have an answer to your exact question, I did find an algorithm that works in $O(\sqrt{n})$ time instead of $O(n)$. I believe this is tight, though I don't have a proof. If anything, the algorithm shows that trying to prove a lower bound of $O(n)$ is futile, so it might help in answering your question. I present two algorithms, the first being a simple algorithm with a $O(n)$ running time for Pop and the second with a $O(\sqrt{n})$ running time for Pop. I describe the first one mainly because of its simplicity so that the second one is easier to understand. To be give more details: the first uses no additional space, has an $O(1)$ worst case (and amortized) Push and an $O(n)$ worst case (and amortized) Pop, but the worst case behaviour is not always triggered. Since it doesn't use any additional space beyond the two queues, it's slightly 'better' than the solution offered by Ross Snider. The second uses a single integer field (so $O(1)$ extra space), has a $O(1)$ worst case (and amortized) Push and a $O(\sqrt{n})$ amortized Pop. It's running time is therefore significantly better than that of the 'simple' approach, yet it does use some extra space. The first algorithm We have two queues: queue $first$ and queue $second$. $first$ will be our 'push queue', while $second$ will be the queue already in 'stack order'. 

Since deciding membership for a context-sensitive grammar is in $PSPACE$ ($NSPACE[n]$ even), any language not in $PSPACE$ is not expressable by a context-sensitive grammar. An example is therefore the language of all true propositions in Presburger arithmetic (en.wikipedia.org/wiki/Presburger_arithmetic). Since we suspect that $PSPACE \neq EXPTIME$, we expect any $EXPTIME$-hard language to be sufficient. Alternatively, any $EXPSPACE$-hard problem is provably not context-sensitive by the space hierarchy theorem. 

It is easily seen that the running time of this TM meets the requirements for the theorem, since TMs can be simulated in $O(n \log n)$ time. We claim that $P(H)$ returns no iff $A$ halts on $i$. Assume $A$ halts on $i$ after $t$ steps. $H$ therefore rejects any $X$ with $|X| \geq t$. We therefore decide at most a finite language, and hence $H$ decides no language in $S$ and $P(H)$ will therefore return no. Now assume $A$ does not halt on $i$. Then the variable halt will never get the value 'halted', so we decide the same language as $C$, which is exactly $s \in S$, so $P(H)$ will return true. This proves that $P(H)$ can solve the Halting Problem, which in turn proves the theorem. 

Analysis Obviously Push works in $O(1)$ time. Pop works in $O(\sqrt{n})$ amortized time. There are two cases: if $|first| < \sqrt{|second|}$, then we shuffle $first$ into stack order in $O(|first|) = O(\sqrt{n})$ time. If $|first| \geq \sqrt{|second|}$, then we must have had at least $\sqrt{n}$ calls for Push. Hence, we can only hit this case every $\sqrt{n}$ calls to Push and Pop. The actual running time for this case is $O(n)$, so the amortized time is $O(\frac{n}{\sqrt{n}}) = O(\sqrt{n})$. Final note It it is possible to eliminate the extra variable at the cost of making Pop an $O(\sqrt{n})$ operation, by having Pop reorganise $first$ at every call instead of having Push do all the work. 

Let $\Sigma$ be an alphabet, ie a nonempty finite set. A string is any finite sequence of elements (characters) from $\Sigma$. As an example, $ \{0, 1\}$ is the binary alphabet and $0110$ is a string for this alphabet. Usually, as long as $\Sigma$ contains more than 1 element, the exact number of elements in $\Sigma$ doesn't matter: at best we end up with a different constant somewhere. In other words, it doesn't really matter if we use the binary alphabet, the numbers, the Latin alphabet or Unicode. 

Let $A$ be an integer, $A > 0$, then $\log_2 A$ is rational if and only if $A$ is a power of 2, and transcendental otherwise. Firstly, if $\log_2 A = \frac{p}{q}$ for nonzero integers $p$ and $q$, then $A = 2^{\frac{p}{q}}$, $A^q = 2^p$. We therefore have that $A = 2^r$ by prime decomposition. Furthermore $A^{rq} = 2^p$, so given an $A$ we can pick $q=1$ and $p=r$ to prove $\log_2 A$ is rational. We just need to show that $\log_2 A$ is never transcendental otherwise. This follows from the Gelfond-Schneider theorem, for an equivalent formulation (as can be found on the Wiki page) is "if $\alpha$ and $\gamma$ are nonzero algebraic numbers, and we take any non-zero logarithm of $\alpha$, then $(\log \gamma)/(\log \alpha) = \log_\alpha \gamma$ is either rational or transcendental." It is also easy to verify by taking the converse of the theorem and setting $\alpha^\beta = \gamma$ and hence $\beta = \log_\alpha \gamma$. 

Our bound on $c_t$ is fairly large, but we have experimental evidence that the 'right' bound is $\frac{1}{\sqrt[4]{t-1}} \cdot \frac{\log n}{\log \log n}$. 

We say that the sequence ${\cal F}_1, ..., {\cal F}_t$ is convex if (*) holds. Our approach constructs convex sequences by appending families to shorter convex sequences, essentially using that if ${\cal F}_1, ..., {\cal F}_t$ is convex, then ${\cal F}_1, ..., {\cal F}_{t-1}$ is convex. We note that ${\cal F}_1, ..., {\cal F}_t$ is convex if and only if for all $A \in {\cal F}_t$ we have that ${\cal F}_1, ..., {\cal F}_{t-1}, \{ A \}$ is convex. We say that $A$ is compatible with ${\cal F}_1, ..., {\cal F}_{t-1}$ if ${\cal F}_1, ..., {\cal F}_{t-1}, \{ A \}$ is convex - we save computation time by computing the sets that are compatible with a sequence and then taking the elements of their powerset as the new ${\cal F}_t$, rather than determining if ${\cal F}_1, ..., {\cal F}_t$ is convex directly. Our next speedup is essentially dynamic programming. We try to find an equivalence relation $\sim$ on convex sequences with the following two properties. First, if ${\cal F}_1, ..., {\cal F}_t \sim {\cal F}'_1, ..., {\cal F}'_t$ for two convex sequences, then $A$ is compatible with ${\cal F}_1, ..., {\cal F}_t$ if and only if it is compatible with ${\cal F}'_1, ..., {\cal F}'_t$. Second, if ${\cal F}_1, ..., {\cal F}_t \sim {\cal F}'_1, ..., {\cal F}'_t$ and ${\cal F}_1, ..., {\cal F}_t, {\cal F}_{t+1}$ is convex, then ${\cal F}_1, ..., {\cal F}_t, {\cal F}_{t+1} \sim {\cal F}'_1, ..., {\cal F}'_t, {\cal F}_{t+1}$. Furthermore, we would like it if we can determine whether a set is compatible with elements from an equivalence class, and determine a representative of the equivalence class of ${\cal F}_1, ..., {\cal F}_t, {\cal F}_{t+1}$ given ${\cal F}_{t+1}$ and a representative of the equivalence class of ${\cal F}_1, ..., {\cal F}_t$. The ensuing dynamic programming algorithm is then obvious. The number of equivalence classes (along with the time taken by the above two operations) then gives a bound on the running time of the obvious dynamic programming algorithm. For the equivalence we use to get our bound, we use a characterization of convexity that is based on `intervals' as follows. Given a subset $A$ of $\{ 1, \dots, n \}$, we say $A$ is contiguous with respect to a (not necessarily convex) sequence ${\cal F}_1, ..., {\cal F}_t$ if $\{ k \mid \exists B \in {\cal F}_k : A \subseteq B \} = \{ i, \dots, j \}$ for some integers $1 \leq i \leq j \leq n$. We say that $(i, j)$ is the interval for $A$ w.r.t. this sequence. It is easily seen that ${\cal F}_1, ..., {\cal F}_t$ is convex if and only if all subsets of $\{ 1, \dots, n \}$ are contiguous with respect to the sequence. Now, given a convex sequence ${\cal F}_1, ..., {\cal F}_t$, we mark all subsets of $\{ 1, \dots, n \}$ as not touched, disallowed or active as follows: all elements of ${\cal F}_t$ are active, all elements of ${\cal F}_1, ..., {\cal F}_{t-1}$ are disallowed and all supersets $B$ of sets $A$ whose interval with respect to ${\cal F}_1, ..., {\cal F}_{t-1}$ is $(i, j)$ with $j < t - 1$ are also disallowed. It is immediate that a set $A$ is compatible with the sequence if and only if it is marked as not touched. We define two sequences as equivalent under $\sim$ if their marking is equal. It is easily seen that this equivalence relation satisfies our two properties. For computing whether a set $B$ should be disallowed by the interval condition, we can use the equivalent condition 'there exists a set $C \in {\cal F}_t$ such that for no set $D \in {\cal F}_{t+1}$, $B \cap C \subseteq D$'. $3^{2^n}$ is an immediate bound on the number of equivalence classes. We also use various extra prunings. We only consider antichains for ${\cal F}_{t+1}$ and we require that the elements of its elements come from ${1, \dots, i}$. Lastly, we use the optimization that ${\cal F}_1 = \{ \{ 1 \} \}, {\cal F}_2 = \{ \{ 1, 2 \} \}$ for optimally long sequences (and similar for ${\cal F}_{t-1}$ and ${\cal F}_t$). We imagine that investigating the behaviour of ${\cal F}_3$ may result in more drastic savings.