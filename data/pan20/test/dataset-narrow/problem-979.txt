I suspect you are in a very specific corner case. Ctrl-J is the code for the Enter key (line feed) and so it seems is treating this specially. I can type Ret and I get a directory listing. But now, sitting at the bare prompt, if I try to type just Ret nothing happens until I follow up with a j to send a literal line feed. My strong recommendation would be to simply not try to use this particular key as the command key for . (My personal preference is which doesn't clash much with anything useful; and typing ctrlL L on the rare occasions when I do need it is perfectly acceptable.) 

which you can pass to another instance of to modify the files which contains these tokens you want to replace. The full pipeline then is 

If you want to edit headers such as Subject as well as the body, take out the flag; but if your normalizations might change vital headers, perhaps you want to make it a bit more specific and/or robust. 

The package description for documents that it is a wrapper which will pull in as a dependency anyway. 

I replaced with which runs a built-in delete action with no external processes. If you want to something else at some point, perhaps use instead of (and notice how the single quotes around aren't really useful; the shell will process and remove them before runs anyway). 

I took the liberty to change the slashes to something else to remove the need for backslashes even further. Demo: $URL$ 

which you'll notice is a script which contains one script for each file. If you are on a *BSD platform (including Mac OS) you will need to change to which in Awk would be in order to properly embed single quotes without disturbing the surrounding shell quoting (so that entire line becomes (sic)). 

The job for a user runs in that user's home directory () and so that is, in effect, the "default directory" for everything which happens through If you want it to happen somewhere else, just say so. 

Procmail's regex dialect doesn't support the {m,n} repetition operator. You will simply have to spell out the pattern in longhand (maybe use a variable if it gets too massive). 

The glob is expanded by the shell to an alphabetic list of all (non-dot) the files in the current directory. The arguments to are a search expression and a list of files. So ends up using the first file name as the search expression. You are looking for the first file's name (as a regular expression) in the other files. Grep searches standard input only if you do not supply any explicit file names. See: 

Of course, without prior knowledge, you don't know which of the IP addresses will be faster, or whether choosing a different one will in fact make any difference at all, or whether asking again would produce yet another set of IP addresses. 

If the code page in the CMD window is 850, then the character in the file name is a single byte which is not a valid UTF-8 sequence. The system could probably display an unknown glyph ï¿½ but it's not really strange, unexpected, or odd that it instead displays nothing at all. The simple fix is to ignore it. The slightly less simple fix is to upgrade your system to Unicode everywhere. Rename all files to have proper Unicode names and then set up the CMD window to also use cp65001 (not a Windows person so don't ask me how. I'm not sure if you also need to change the Windows default code page). 

There are multiple efforts for using e.g. GitHub for collaborative work in publishing, including academic collaboration but also various other coauthoring scenarios. Traditional publishing toolchains have been dominated by proprietary file formats and editing tools, but there was always a strong undercurrent of purely text-based publishing tools (Troff, LaTeX, etc) which are very nicely suited for source code version control systems, and actually bring some nice benefits (the ability to comment on sections without having the comments intrude on the final published result, programmatically producing some parts of the content, etc). Here is one example: $URL$ but there are many others for academic publishing. I believe I have seen something for less formal publishing (journalism collaboration? volunteer work?) but I can't find a link right now. GitHub itself of course is centralized, but once you understand the concept, there is nothing really stopping you from going fully DVCS. 

You are mixing apples and oranges. Create a partition table on and copy to , or copy all of (partition table and all) to wholesale. I can report success with the former, albeit my experience is from many years ago; but the latter is tricky to get to work entirely correctly. Perhaps you're better off creating a partition table and installing Grub separately. There are various recipes for making USB sticks bootable; it's not hard. I'm rather confident the latter will not work unless you can set up and to have exactly the same disk geometries etc; or limit the transfer with something like (probably better with a bigger block size and a correspondingly smaller count), and live with the fact that the partition table is not exactly right. If you mount the boot partition read-only, you should not be able to create any damage to the file system anyway (... I hope). As a partial workaround, if you can play around with the disk on your source machine, you could set it up to have two partitions in the first 32G and then mount the rest as a separate partition or whatever; then maybe live with the fact that the stick will try to mount a nonexistent partition (maybe that will kill it, haven't tried). 

(where $USER is the name of the account, and the file must be a regular file, or not exist). Though this is probably already the compiled-in default action, so you don't need an explicit recipe for this. (You can run to see what the default mailbox for the current user is.) Anyway, if you think you want maildir to work, you need to have a directory named which is writable by the user whose Procmail instance is trying to deliver to this location. (For proper Maildir functionality, there should be directories named and , too, with similar permissions.) More commonly, you would have a maildir structure with the correct permissions etc in the user's home directory (or some approximation, for virtual users) and write there instead. 

I have configured Thunderbird to display the optional column "Order Received" in my inbox, and to sort my inbox by this field. I find it very convenient for browsing my large inbox, but jumping to a message with a particular number is rather cumbersome. Is there any way to assign a keboard shortcut, either in Thunderbird itself or by using some plug-in? For example, let's say I have written myself a note pointing to message 214640; currently the only way I can navigate to it is by scrolling the inbox pane with the mouse. The scrollbar is a rather blunt tool (in a large inbox, a small movement will scroll by several hundred messages) and I will typically already have the message number on the clipboard, so something like control+g shift+insert ret would seem like an easy and natural way to get there, but I cannot find a built-in function for anything like this (and my attempts at Googling inevitably find oodles of resources which explain basic to intermediate keyboard navigation, but nothing about this particular use case). I observe that when I view the source of a message, the window's title contains a (pseudo?) URL like so it seems that there is an internal representation for addressing messages which I would like to have access to from the GUI somehow. For what it's worth, I'm on Xubuntu 12.04, Thunderbird 17.0.8. 

In the best case, the ripper manages to retrieve enough data that the track plays just fine. In the second best case, it gives up and doesn't produce an audio track because it cannot. In the worst case, you get an audio file which sounds like when you attempt to play the track in a regular CD player or even worse (pauses, repeats, stuttering, blips, etc). My experience with cdparanoia-based rippers has generally been mainly best-case, but I've encountered all three scenarios. 

Many email clients allow you to discard messages based on keywords in the subject and body. If you do not expect to receive legitimate messages which contain these words, filtering out the matches should be easy to set up. Your question does not mention any particular software or platform so I hope you can find instructions for your particular circumstances by googling for terms like "Outlook filter keyword" (assuming you are an Outlook user for this example). However, many senders of unsolicited bulk email use various evasion techniques to avoid this sort of simple filter. If you cannot filter on keywords, you will probably want to explore installing a separate spam filter, ideally on the server where your mail is delivered. Many mailbox providers offer server-side spam filtering with versatile and complex content analysis to identify and block unsolicited messages even when they contain various kinds of obfuscations (and often spotting them because they use particular obfuscation techniques). The absolutely best defense against spam is to not publish your email address in the open in the first place. If the cat is already out of the bag, so to speak, perhaps you can at least create a new, separate email address to use with close acquaintances, and avoid giving it out to strangers or putting it on a web page anywhere. You can still keep your old email address active, perhaps with stronger spam filters enabled, and/or only fetching and reading the messages it receives more rarely than your new primary, protected address. Some of us have vanity domain names, which means we can create new email addresses for different uses whenever we like. I would estimate that I create a few dozen new email addresses every week, and turn off or expire the ones which start to receive spam or otherwise stop being useful. 

DNS allows for arbitrary nesting of labels. There is no requirement that any specific label refer to a specific node. A "node" is a sequence of labels, where typically, each label indicates a point of delegation. For example, and are labels designating top-level domains. Within there is a static subdomain structure where effectively the "top level" is , , , etc (meaning "top" as in you can't register anything in directly, though of course the administrators could decide to add new second-level labels). Many countries have this second level, though it is by no means universal (and some countries and domains have yet more internal levels like this). For example, is delegated by the top-level domain (also called "zone") to its owner, who can further delegate individual nodes within this domain. So the owner of could decide that there will be a subdomain and it shall contain mail servers whose names could be , , etc. This isn't very common in practice, but a really large organization could delegate part or all of its DNS further to individual departments or separate organizations. (A common example in real life is service providers. If you run a web or email service on you might have , etc and have each of these delegated to your customers Ajax Inc and Acme Corp. Another scenario is regional services -- to serve North America, to serve Europe, etc.) To finally answer your actual question, whether a node is an actual host (an actual server) or just another level in the DNS is completely up to the organization who owns it. It's not uncommon for a node to have both roles ( could be an actual mail server with an IP address, and there could still be a DNS zone containing and etc. This is not a very compelling example, I'm afraid, but there is no technical reason you couldn't do this. I'll try to think of a better example...) A "subdomain" is just an indication that a particular label is below the top level and contains additional nodes within. So, "sub" as in "not top", and "domain" as in "label containing more labels". The disconnect between the mechanics of DNS (there are "zones" with "labels" identifying "nodes") and the actual real-world semantics (there is a set of root, aka top-level domains, and domains within them which are bought and sold, and subdomains within these which typically are arbitrary subdivisions implemented by the domain owner) is by and large a feature, not a bug. The implementation does not specify any particular policy, and can be used for things which are not currently in any policy. We have seen this spark useful innovations and new policies multiple times. 

The version upgrade is basically just a more thorough version of what you see with . It will not touch your home directory at all. 

The command has an output format quite similar to what you specify, but it prints the control characters in your example string literally for me. Perhaps you have more luck with it (I'm thinking my system is too modern :-). It should not perform paging when output is not to a terminal, so it should just pass through -- hopefully, with the desired substitutions -- when you run it from your crontab. 

That's a syntax error. You can't have whitespace around the equals sign in a variable assignment. See also $URL$ Furthermore, if you only want to iterate over the actual matches, you need to change the expression to only extract those. Try this. 

You are not showing us your script, so this is obviously somewhat speculative; but most probably your script should not be in because it requires some sort of user interaction. The profile should be reserved exclusively for things which need to run even in noninteractive sessions. The proper place for interactive functionality is probably not in your login scripts at all, but something like this e.g. in your can at least avoid producing damage for noninteractive sessions. 

The leading here suggests that you are running the script from the directory where the script lives. I am speculating that you do not have write access to this directory (as indeed you should not). Change the invoking script so that it runs in your home directory (or, say, ) with a full path to the script instead. Better yet, or even in addition, change the test script so that it creates a temporary directory, and removes it when it's done (even in the case of e.g. a syntax error; see ) 

If your Apt index is very old, it will attempt to fetch files which no longer exist on the server, because they have been superseded with newer versions. Try before running . 

You have code which needlessly copies stuff over to a different array. This is trimmed down to a simple loop over the results from . I am not writing but hint at how to collect all information, then write it all out finally at the end, after the main loop. (If you have a lot of data, writing as you read starts to make sense again, but for a simple, quick job, I would collect, then write.)