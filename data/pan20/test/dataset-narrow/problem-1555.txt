All is in the question: Consider the case we specify an output size of 64 but with current convolution layer parameters we can get 128 features. How is this handled in general and using keras in particular? 

How can one use the previous equation in the Q learning algorithm How can we update the weights Wij,a? 

For classification, it is obvious how a decision tree is used to make a prediction.You just have to find the final leaf. However for regression problems, how can you find the prediction considering the continous aspect of the variable to predict? 

I have a collection of statements which I need to classify into 5 classes. Each statement have meta data in different columns: Author|Editor| date of release| statement | Class How can one use the meta data to improve the text classification task? 

I found a code for a classifier for a 6 class problem where the classes are integers from 0 to 5. I found this code that I tested and it improved my accuracy highly. Can anyone explain the spirit behind the idea of transforming the problem into a binary classification and how it improves the results? 

I found a dependency Treebank in this format for Persian. I would like to do some experiment on it. I am familiar with C# and C# has great libraries for XML document files. I just wonder why such a corpus is not in XML format! I don't say a sophisticated one but they could put each line in a node of XML. Then at least, I would know what is the tag for each element. However, I can convert it to XML. 

I use an algorithm to automatically convert a dependency treebank to a constituency treebank. These are two trees. The algorithm decomposes the dependency tree to a set of edges and for each edge find a corresponding sub-tree (these pairs could be given by the user or extracted from a treebank). Then glues these sub-trees to create the constituency tree. 

Which treebanks are based on an XML format? What is the advantage of XML format for a treebank? I think it may have effects on annotation and querying the treebank. for example LASSY and Alpino or TIGER are in xml format. 

For an assignement, we are asked to code a Q learning for the mountain car problem. To do so, the basic idea is to discretize the states space which is defined by x the position and the velocity vx. if the bounds of x and vx are respectively (-150,150) and (-20,20), then we discretize the points using this simple mapping. 

I have a dataset under this form: Custommer_ID purchased_Item_ID Rating 12500 1299 1 12500 2 2 1 3 5 As you can see, the only information I have is the custommer ID , the Id of the item he bought and the rating he gave to the item. Please note that there is no way to add any other features from external sources The goal from this data is to build a model which for each custommer will identify the products ratings based on his purchase history. The idea I have right now is to add a column for each possible item and put its rating or -1 if the custommer didn't purchase the item yet then use classical algorithms such as SVMs, decision trees etc... Any clues on how to do that? 

Your results are reasonable. Your data brings several ideas to mind: 1) It is quite reasonable that as you change the available features, this will change the relative performance of machine learning methods. This happens quite a lot. Which machine learning method performs best often depends on the features, so as you change the features the best method changes. 2) It is reasonable that in some cases, disparate models will reach the exact same results. This is most likely in the case where the number of data points is low enough or the data is separable enough that both models reach the exact same conclusions for all test points. 

A simple way would be to consider Laplace Smoothing ($URL$ ) or something like it. Basically, instead of calculating your response rate as (Clicks)/(Impressions) you calculate (Clicks + X)/(Impressions + Y), with X and Y chosen, for example, so that X/Y is the global average of clicks/impressions. When Clicks and Impressions are both high, this smoothed response rate is basically equal to the true response rate (signal dominates the prior). When Clicks and Impressions are both low, the this smoothed response rate will be close to the global average response rate - a good guess when you have little data and don't want to put much weight on it! The absolute scale of X and Y will determine how many data points you consider "enough data". It's been argued that the right thing to do is set X to 1, and Y appropriately given that. 

I just can figure out an error rate in which I count the correct conversions (at the tree level) divided by the total conversions made. 

Matlab is a great tool for some mathematical experiments, Neural Networks, Image Processing ... I would like to know if there is such a comprehensive and strong tool for data manipulation and NLP tasks? such as tokenization, POS tagging, parsing, training, testing .... However I am new to NLP and I need a tool which let me experiment, get familiar and progress 

Most Treebank conversion which I found in the web are from constituency treebank to dependency treebank, I wonder why there is little jobs in the opposite direction? 

I know their meaning but I don't know why it is called recall? I am not a native-speaker of English. I know recall means remember, then I don't know the relevance of this meaning to this concept! maybe coverage was better as it shows how many instances were covered...or even retrieve to mean how successful was to retrieve relevant data is better but recall?! to mean remember is not straightforward... Moreover sensitivity is also insensible for me! could you help me to associate these words to the concept and have a sense of them? 

In the vein of bayesian optimization, I prefer Hyperopt, available on github at $URL$ or through pip, homepage of author at $URL$ The tree-parzen-estimator algorithm behind it is described in the paper at $URL$ . You can define an arbitrary nested search space, and then tell it to find the optimum of a black-box function. Hyperopt also supports a built-in sklearn search space out of the box, but I have not used that, I've typically defined my own. There's also a few other bayesian optimization schemes around. I can't claim to know which one of them will come out on top as the "best" in the long run. 

You may want to consider gradient boosted trees rather than random forests. They're also an ensemble tree-based method, but since this method doesn't sample dimensions, it won't run in to the problem of not having a useful predictor available to split on at any particular time. Different implementations of GBDT have different ways of handling missing values, which will make a big difference in your case; I believe R does ternary splits which is likely to work fine.