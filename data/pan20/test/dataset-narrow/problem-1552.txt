You can use the package spark-csv by DataBricks that does a lot of things for you automatically, like taking care of the header, use escape characters, automatic schema inferring etcetera. Starting from Spark 2.0 there is an inbuilt function for dealing with CSVs. 

For a specific model you feed it data, choose the features, choose hyperparameters etcetera. Compared to the reality it makes a three types of mistakes: 

One-hot encoding is the normal approach, and yes you would end up with 500 features for your group alone. Depending on how much training data you have this does not need to be a problem. If you have a lot of other features without direct interactions between the individual products you could use an Embedding layer before adding them to the rest, which maps your sparse categorical one-hot encoded features into a dense space via backpropagation. This will reduce the amount of parameters significantly. If you don't have enough training data to do this, you could look at gathering statistics about your product, that say something about other features for the group, or about the target for other rows (be sure not to use the current row, which would introduce target leakage) which would allow you to drop the category all together. Turning them into IDs and then using this as a numerical feature is a bad idea, since there is no inherent structure in the numbers, which means there is no signal in that and it would need a very high complexity to learn how to distinguish 1 and 5 being very similar but 2 and 4 very different. 

The k-th mean just means the mean of the k-th cluster. We want to minimize the sum of squares of each of the clusters, which we do by minimizing this so-called distortion metric. The gradient is a multivariate generalization of derivatives and the gradient of this distortion metric is the zero-vector when this is optimized, and after this you update your clusters. 

Ensembles average out a number of these models. The bias due to sampling bias will not be fixed for obvious reasons, it can fix some of the model complexity bias, however the variance mistakes that are made are very different over your different models. Especially low correlated models make very different mistakes in this areas, certain models perform well in certain parts of your feature space. By averaging out these models you reduce this variance quite a bit. This is why ensembles shine. 

I guess I answered the question in the comments, so here goes. Most ML models cannot deal with categorical values. A common way to solve this is to use one-hot encoding, also known as dummy variables. For very possible value of your categorical variable you create a column which is 0 unless this row has this category, then it is 1. It is possible to remove one of the categories since it is a linear combination of the other dummy variables (if all are 0, the last one must be 1). The downside of this method is that it increases the dimensionality of your feature space. If you have enough data to support this or not that many categories that is not a problem. There are other alternatives, like taking the average feature of every category and adding that to your features as opposed to the categorical feature. 

In the case of 1 note at a time maximum you could use categorical_crossentropy as loss and add a class for when no note is played. This is a log loss on a last softmax layer. This would turn it into a 37 one-hot-encoding representation. In the case of having the possibility of multiple notes I would keep the 36 (or 88 for the pian) dimensional representation and have a sigmoid activation on each of the notes at the end. Then sum the binary log loss of each of the nodes and use that as the loss. Then you can treshold the nodes individually at prediction time to see if a note is on or off. 

Since we are talking about multiple different types of targets (classes versus numerical for example) we already need a composite loss function. I will consider how to balance the different composite parts of the loss function outside of the scope of this answer but if you look into multi-task learning there are solutions to this. What you could do (both in training and evaluation) is to mask the parts of the loss function that are unknown. You can do this fairly easily (but hacky) by passing weights as input during training or prediction for each output in your sample whether it was available or not as 1 or 0 and multiply that component with the weight. For analysis of the training and prediction you need to make sure to take the mean of only the available labels because otherwise your loss values are too optimistic. I do think this approach is very valuable in a lot of cases, this can also help with semi-supervised learning where you do have labels available for a similar task. 

In the case where you can have multiple labels individually from each other you can use a sigmoid activation for every class at the output layer and use the sum of normal binary crossentropy as the loss function. You would just use a vector with binary numbers as the target, for each label a 1 if it includes the label and a 0 if not. 

The suggestion of ncasas is a good one but not very clean. This ordering makes a lot of sense when it's 1-dimensional, but when you introduce more features the ordering will become more and more arbitrary. This is a problem I have come accross multiple times. This paper ($URL$ tries to tackle permutation equivariance, which is not exactly the same as your problem, where you would want permutation invariance. With the equivariance the output from the layer still gets shuffled if you shuffle your inputs, but the values will be the same due to the weight sharing. You could extend this idea fairly easily by using a pooling operation (max pooling or mean pooling) on the output from this permutation layer. This means you get one number output out of your layer. Repeating this with n different weight matrices you can get n outputs, which can then be fed into a normal feed forward network. Caution: This is something I just thought of based on your question and their paper, I never tried it but I don't see why it wouldn't work. In the case of sports teams you would use two lists of inputs, one for each team. Based on what you want to predict you could even fit these permutation layers into a siamese network, introducing even more weight sharing! 

If the correlation between two features $x_1$ and $x_2$ is 1 that means that you can write $x_1 = c\cdot x_2 + a$. The only knew knowledge there is are those two constants, the individual values can be retrieved knowing this. I highly doubt there is anything a machine learning algorithm can learn from this and it is a fact that for some having this kind of correlation between features can hurt your performance quite a bit, so I would test it a bit but I would say it's very likely you can remove one of the two, and which one is not going to matter. 

Yes, you should do this. Given the initialization schemes and normalized inputs, the expected values for the outputs are 0. This means that you will not be too far off from the start, which helps convergence. If your target is 1000, your mean squared error will be huge which means your gradients will also be huge which can lead to numerical instabiliy.