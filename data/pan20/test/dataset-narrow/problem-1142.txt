My two cents are that in my university we've had both mathematics PhD students working on computer science questions (and faculty in the math department with interests in computer science), as well as some computer science students working primarily on purely combinatorial problems. You might be right that it's sometimes easier to work on CS questions as a math student, rather than on pure math questions as a CS student. Keep in mind that at least in the first two years these two kinds of programs might be fairly different in content. As a math student you will be expected to take core math courses as real analysis, complex analysis, topology, algebra, etc. Combinatorics is usually not part of this core. For a CS program there will be a core CS requirement, which usually involves taking some mix of theoretical and more applied courses. While the core in a math program is fairly standard and strictly enforced, the core in a CS program tends to depend a lot on the program, and the requirements might be more flexible. However, all that is not really of primary importance (although it will be loads of work) and is all over within the first two years. I understand it's hard to know what you want to work on before you're in grad school, and many students change their fields. Nevertheless, I would encourage you to look at the faculty pages of schools you are considering, see what professors are working on, and write several emails to faculty and students. PhD level studies are much more about personal relationships and personal drive than they're about a program as a whole. Good programs at the PhD level in my view are distinguished by a strong faculty, and an energetic research culture, rather than by curriculum. You should inquire from faculty and current students about questions like the level of collaboration between math and CS departments. And you should really try to find faculty that have a mix of interests that appeals to you. It's a good idea to write to them to express your interest as well. As far as industry jobs, I'm not sure there is a huge difference between a CS theory degree and an applied math degree. But I am not very knowledgeable about this. 

As the other answer points out, the reduction in the original paper seems to have a bug: $H$ will not be two-colorable unless $G$ is bipartite. I couldn't quite see how to prove the reduction in the other answer works, but here is one that I think does. The vertices of $H$ will be $\{x_{i, v}: i \in [k], v \in V(G)\} \cup \{z\}$. $H$ has an edge $f_v = \{x_{1,v}, \ldots, x_{k,v}\}$ for every $v \in V(G)$, and, for each $(u,v) \in E(G)$, and each $i \in [k]$, an edge $\{z, x_{i,u}, x_{i,v}\}$. Given a coloring $\chi_G:V(G) \to [k]$ of $G$ you can construct a $2$-coloring $\chi_H:V(H) \to \{0,1\}$ of $H$ by setting $$ \chi_H(z) = 1,\\ \chi_H(x_{i,v}) = 1 \iff \chi_G(v) = i. $$ It's easy to check that no edge of $H$ is monochromatic. In the other direction let $\chi_H$ be a proper 2-coloring of $H$, and assume that $\chi_H(z) = 1$. (If this is not the case just rename the colors.) Since no edge $f_v$ is monochromatic, for every $v \in V(G)$ there is some $i$ such that $\chi_H(x_{i,v}) = 1$. Pick an arbitrary such $i$ and set $\chi_G(v) = i$. This is a proper coloring, or otherwise there would be an $i \in [k]$ and an edge $(u,v) \in E(G)$ such that $\chi_H(x_{i,u}) = \chi_H(x_{i,v}) = 1$, which would make the edge $\{z, x_{i,u}, x_{i,v}\}$ of $H$ monochromatic. 

This is a special case of metric $1$-medians, where the metric space is $S_n$ (the symmetric group on $n$ elements) with distance function number of inversions (i.e. the distance between two permutations is the number of pairs $i, j: i<j$ s.t. $i$ and $j$ are ordered differently). This distance metric is also known as Kemeny distance and is related to the weak Bruhat order. This paper by Ailon, Charikar, and Newman considers this and related problems. The problem you're asking about is called Rank Aggregation in their paper. Look at their intro for more references. The problem is NP-hard. A simple 2-approximation is to pick the best of the given $k$ permutations (i.e. the permutation that minimizes the objective function). Here is a proof: Let $\pi$ be the optimal permutation and $\pi_{j^*}$ the best of $\pi_1, \ldots, \pi_k$. For any $i, j \in [k]$, by the triangle inequality, $d(\pi_i, \pi) + d(\pi_j, \pi) \geq d(\pi_i, \pi_j)$. Sum over all $i, j$, and you get $2n\mathsf{OPT} \geq \sum_j{\mathsf{ALG}(j)}$, where $\mathsf{ALG(j)}$ is the cost of choosing $\pi_j$ as the solution, and $\mathsf{OPT}$ is the cost of the optimal solution. Since $\pi_{j^*}$ was chosen so that $\mathsf{ALG}(j^*)$ is minimum over all $\mathsf{ALG}(j)$ for $j \in [k]$, we have $\mathsf{ALG}(j^*) \leq \sum_j{\mathsf{ALG}(j)} /n$ and we have $\mathsf{ALG}(j^*) \leq 2\mathsf{OPT}$. The paper I referred you to above has another, almost equally simple 2-approximation (but the proof is not that simple, though still not bad). They can show that the bad cases for the two approximation algorithms are different, and taking the better solution of the two gives 11/7 approximation factor. Then there is also a PTAS by Mathieu and Schudy (Warren sometimes visits this site btw), by a reduction to weighted Minimum Feedback Arc Set in Tournamets (the same reduction is used in the previous paper, I think). Here is the link 

One idea is something simple from streaming algorithms. Probably the best candidate is the majority algorithm. Say you see a stream of numbers $s_1, \ldots, s_n$, one after the other, and you know one number occurs more than half the time, but you don't know which one. How can you find the majority number if you can only remember two numbers at a time? The answer is the Misra-Gries algorithm. At each time step you store a number $x$ from the stream and a frequency counter $f$. At the start you set $x$ to the first number of the stream and initialize the frequency $f$ to 1. Then whenever you see a new number $s_i$, you check if $x = s_i$. If $x=s_i$, increase $f$ to $f+1$, otherwise decrease $f$ to $f-1$. If $f=0$, set $x$ to $s_i$ and $f$ back to $1$. After the last element of the stream, if there was a majority element, it will be equal to $x$. Another idea is the well-known game to illustrate zero knowledge proofs. I think it's due to Oded Goldreich and is similar to the zero knowledge proof for graph isomorphism. To make the answer self-contained, here is the game. Suppose you want to convince your color-blind friend that you can tell red from green. Your friend has two decks of cards, and he knows one pile is green and the other is red. He does the following without you seeing him: with probability 1/2 he draws one card from each deck, with probability 1/4 he draws two cards from the left deck, and with probability 1/4 he draws two cards from the right deck. Then he shows you the cards and asks you if they are the same color. If you are not color blind, you can of course answer correctly every time. If you are color blind, you will fail with probability 1/2. So now if the game is played 10 times, the probability that you can win every time while being color blind is extremely low. The kicker is that if your friend knew the two decks of cards are two different colors, but did not know which one is red and which green, he still will not know at the end of this! So in summary: 

When $k = \lceil (s-t)/2 \rceil$, this the range medians problem. There are solutions which are much better than the two trivial ones: you can answer the first $q$ queries in time $O(n\log q + q\log n)$. I believe the algorithms can be adapted to arbitrary rank. References: 

There is a very neat randomized algorithm by Cheung, Kwok, and Lau, which computes the rank of an $n\times m$ matrix $A$, $n\le m$, in time $O(\mathrm{nnz}(A) + \min\{n^\omega, n\cdot\mathrm{nnz}(A)\})$. Here $\mathrm{nnz}(A)$ is the number of nonzero elements of $A$, and $\omega$ is the matrix multiplication exponent. Not quite what you are asking for, but it does improve on fast matrix multiplication based methods for $\mathrm{nnz}(A) = o(n^{\omega - 1})$. 

Implement a function (like C's ) that takes as input an array of pointers to any arbitrary set of $n$ objects (say $n$ is also part of the input). That is nice because it's very general, i.e. it gives the user/programmer a single function for all his sorting needs. But because does not know apriori what objects it is dealing with, it does not even know when they are sorted. So you need to somehow tell about these objects. The standard way is to pass to a pointer to a function (i.e. returns 1 if obj1 is greater than obj2 in the sorted order, for example). If this will be the design you follow for , then you are in the decision tree model world, and provably will need to call at lest $\Theta(n \log n)$ times. You might try to tell about its set of objects in some other way, but it is not clear what that would be. Also, if your method is general enough to work with arbitrary sets of objects, it will likely hit the $\Theta(n \log n)$ barrier again. Implement a function that takes an array of say , because you know that you will be sorting integers that fit in a single machine word (i.e. are in the range of ). Now you can use the algorithm from the paper you referred to and that algorithm will take $O(n \log \log n)$ time (i.e. make that many elementary C operations). But will only work correctly if it is passed an array of , to be sorted in the natural order (Note: the algorithm will still be correct and run in time $O(n \log \log n)$ if the size of grows with the years; in fact that is an implicit assumption for the asymptotic analysis here). Now afaik, it is consistent with current knowledge that there exists some algorithm that you can use for that takes linear time. But keep in mind that computer science has had zero success in showing that any problem takes more than linear time in the "C language model" (aside from some dynamic data structure problems). So do not read too much into "we don't know if $n$ integers that each fit in a single computer word can be sorted in linear time." 

A recent result of Li, Nguyen, and Woodruff shows that for any streaming algorithm in the turnstile model (where the stream consists of insertions and deletions of elements) there exists an algorithm that works by only maintaining a linear sketch and uses only slightly more space. So to prove a space lower bound in the turnstile model it is (up to some logarithmic factors) enough to prove a space lower bound for linear sketches. These can be easier to prove, for example by proving a communication lower bound in the simultaneous communication model rather than in the one-way model, or by more directly working with the linear structure of the sketch: check the paper for a lower bound on the space complexity of frequency moments proved this way. 

A classical example is the Hungarian algorithm. The Ford-Fulkerson algorithm can be seen as another example. Note that step 2. is a feasibility problem which often is easier than the original optimization problem, and also often can be solved combinatorially. This is the power of complementary slackness. For example, in the case of minimum cost bipartite matching, step 2 amounts to checking if there exists a perfect matching using only tight edges. In the case of maximum $s$-$t$ flow, step 2 amounts to checking if the saturated edges separate $s$ and $t$. Primal-dual algorithms are nice for many reasons. Philosophically, they provide more insight than a generic algorithm. They usually give strongly polynomial time algorithms, whereas we still do not have strongly polynomial LP solvers. They are often more practical than generic algorithms. This is especially true if we cannot write down the LP explicitly and our only other choice is the ellipsoid algorithm, which is the case with non-bipartite matching and Edmonds' primal-dual algorithm. Primal-dual is also a very useful framework for approximation algorithms, by using relaxed versions of complementary slackness. This has been useful in designing approximation algorithms for NP-hard problems (see e.g. Chapter 7 of the Williamson-Shmoys book) and in designing online algorithms with good competitive ratio (see the book by Buchbinder and Naor). The point here is that the algorithm maintains a solution $y$ to the dual of the LP relaxation of a hard problem, and at each step either finds an integral primal feasible $x$ such that approximate complementary slackness is satisfied, or improves the dual solution $y$. Approximate complementary slackness is a condition of the following form: if $x_i > 0$ then the corresponding dual constraint is tight, and if $y_j > 0$, the corresponding primal constraint would be tight if $x$ is scaled by $\alpha$. This gives an approximation factor $\alpha$. It's all very nicely explained in the above two sources. 

The optimization problems seems to be equivalent to shortest common supersequence as well. The two results I found related to approximating this problem (it is NP-hard in general) are this and this. There are some inapproximability results and much weaker approximation results it seems. The first paper shows $\Omega(\log^\delta n)$ hardness of approximation and a polynomial factor approximation. Here is a simple fact (I will use your terminology instead of the shortest common supersequence terminology): if there are $k$ colors, then the greedy algorithm (take the items from the color class that has the most available balls) is a $k$ approximation. Say at some step of the optimal we take a set $X$ of $|X| = x$ items, and consider the time step in the algorithm when we first took items from $X$. At that time step in the algorithm there were at least $x$ nonempty stacks (since not all $x$ items from $X$ are taken yet) and by averaging the algorithm took at least $x/k$ items. So the problem has a polytime dynamic programming solution when there are constantly many stacks, and a constant greedy approximation when there are constantly many color classes. 

SDPs usually provide relaxations, so for a minimization problem you'll get a lower bound. The Lovasz theta function does provide such a lower bound on chromatic number (see wiki). Upper bounds can be provided by rounding schemes (constructive or otherwise). In general, if you have an upper bound $U$ on the integrality gap of the SDP, you can scale the objective of the SDP by $U$ and you'll get an upper bound as well. However, there exist graphs for which the Lovasz theta gives a lowerbound of $k = O(1)$ and the chromatic number is at least $n^{1 - 2/k}$. There is some hope that higher levels of the Lasserre hierarchy can give stronger relaxations. However, notice that chromatic number is hard to approximate within $n^{1 - \epsilon}$ (see this) in general (i.e. if you're not just interested in the promise problem where the yes instance has constant chromatic number). So, for any SDP, integrality gaps better than $n^{1-\epsilon}$ will either be restricted to cases where the SDP relaxation has constant value or will be nonconstructive (i.e. superpolynomial time rounding or superpolynomial size relaxation), unless P=NP.