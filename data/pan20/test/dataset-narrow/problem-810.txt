Its pretty safe, I cant speak for all networks but its incredibly unusual on a inter-hop route to expect traffic to be routed to multiple paths -- it could happen but its more the exception than the norm. 

Note the "freeze" cgroup will not evict pages to a media persistent location, but it will swap the pages out when enough time has passed and the pages are needed for something else. Even if this does work (its pretty hacky if it did) you need to consider whether or not this is really doing anything to solve your problem. 

Assuming here what your referring to is the tradiitional unix hostid. If I wanted to bind software to a system I would use a dongle or some stronger means to identify a system seeing as a hostid is very arbitrary, but nevertheless.. The hostid is retrieved using the library call "gethostid". Its merely a generic value which, if unset will be based off of the ipv4 address of the host system. See "man 2 gethostid" 

From the looks of things the kernel parameter libata.force=3.0G should work.. With regards to data loss, probably not -- but frankly since the SATA vendors clearly haven't honored the SATA spec correctly (or its buggy or whatever) then who knows. 

Is enough to tell me that this is not your problem and its the responsibility of the sysadmin to provide an adequate explanation. I dont want to sound to rude here but; 

Note that a fifo is typically necessary in programming where the amount written in can surpass the amount read out. As such a fifo wont work entirely smoothly as you anticipate but would solve your main problem whilst introducing another. There are three possible caveats. 

Also, note su, on its own wont help you in this regard since you'll su into the staff_t type which wont do everything you want. To fix this, edit sudoers and add your user to it such as this: 

Oh dear! No swap! So, memory thats committed -- just stays there. Newer kernels these days actually 'defragment' memory to make region of memory contiguous, older ones dont do it. You had 70Mb of memory that could has been swapped! Plus this wouldn't have occurred all in one go but gradually so would not have been a hit for you. But no swap, so no luck. You also have little memory for pagecache which is also bad and slow for your system. This potentially could of given a lot more free contiguous space too which would of been nice for you. My advice to you. Get yourself 768Mb of swap. Honestly, you really do your kernel a disservice by not enabling it. Swap is really important for releasing unused memory (a quarter of it in your case) and also would have avoided the nasty fragmentation problems you've experienced as memory could have been swapped out and released more contiguous space. And even if it did get swapped back in, it could have been put back into a region of memory which give you larger contiguous gaps. 

The big downside to barriers is they have a tendency to slow I/O down, sometimes dramatically (around 30%) which is why they arent enabled by default. In addition to this, things become doubleplusungood when you start to add logical layering on top of standard disks like LVM or Raid. LVM (relatively recently) added barrier support for most LV configurations and mdadm seems to have had it for a little while. 

As a matter of fact, that is not all which is evaluated. Even more questions are asked before we allow a transition. 

A 'done' file exists on the host already. (this is a signal for script 2 only) Open an 'acquire' file on the host using . Rename 'done' to 'done.old'. Do your special work here. Open an 'done file on the host using . Unlink 'done.old'. Unlink 'acquire' 

So, it checks the real UID of the caller and enforces the change or not depending on whether the the real UID is 0. So, you should simply be able to setuid your binary and make sure root owns it, then it will be able to change password and enforce the cracklib decision. setuid'ing your binaries only sets the effective uid to 0, not the real one. 

The only caveat is that you must also add the iptables rules in that use the set after the set has been recreated. Otherwise if the rules exist in iptables you'll get an error inserting the --set-match rules into iptables since the databases wont exist and iptables as a 'service' in init.d is invoked before cron is. 

Your picking up ipv6 network connections which often have a number of colons in them. The command doesnt take this into account when snipping the colons out from the IP address. Might be more suitable to use instead of cut in that case I guess. 

The traffic is likely being filtered. Because you supplied your domain name (assuming here, that despite having two A records with two different addresses is accurate.. Tracerouting to port 80, which we can demonstrably prove is open.. 

I think this depends on the load you anticipate to get but the models chosen by both systems are architecturally different. Firstly, nginx uses an event based model to handle requests whereas Varnish uses a thread based model. Varnish places its cached content in a very efficient critbit tree. I couldnt find out what implementation that is used by nginx. nginx should be more efficient because it uses a non-blocking event based model to spread load evenly with as little contention as possible, however if lookup times from the cache is much slower you could argue it cancels things out. Varnish creates thread pools (normally of many threads, 500 or so) to handle multi processing. The cost here is in context switching especially if you have many requests to process. The way I see it -- varnish will perform better as you scale up the number of cores you have to battle with the contention, plus its really very good caching algorithm makes its lookups and responses very fast. Use varnish if you have lots of cores and have very high traffic/content to deliver. Nginx on the other hand takes a much less sledgehammer approach with managing resources and I reckon on small/medium caches on low-powered systems will probably work out better value in terms of efficiency and requests per second. Overall, varnish works best on a dedicated system with at least 2 cpus/cores. It will linearly scale as you add cpus. Nginx probably works best on a smaller multi-roled system where the cache pressure is not as hot. It will also scale linearly but I suspect its caching algorithm and implementation is not as good as the varnish one which could end up being a performance bottleneck as you reach high levels of traffic. 

You can see this where apache is using IPC semaphores, probably from a apache module. If it crashes it wont clean up the semaphore which remains permanently assigned to the system at least until a reboot. Run and you'll probably see a lot of stuff owned by apache in there. You can use the command to clear it up. 

When you want to read a file just open and read it. You should always use , never for this operation. If you are using a PHP library to do this, check that it just calls and not on the file - but should work fine. When you want to refresh or create a new file, you perform the following operations:-