Our database deployment process logs in as sys to deploy database objects. We ran into a problem when we wanted to deploy a private database link for another user (AU). Our solution was to: 

The first three columns should all show identical information and the last should verify that the year is correct. 

No, you do not need to enumerate the columns used in a materialized view when creating the materialized view log. In fact you cannot create a materialized view log using the primary key method and include all the columns because you would be including the primary key column itself, which is not allowed. The concept of a materialized view log is to store the rowid or primary key of the data that changed. The refresh can then look up the entire record from the table. Adding specific columns to the WITH clause of the log explicitly records the data in the materialized view log itself. If your materialized view query is filtering on these columns or joining on them it could speed up the refresh. Theoretically if all the columns used in the MV are in the log, then it should be able to refresh without referencing the table. The documentation does not indicate that this is being done. It would be interesting to trace this to see. Even if it does, the additional storage requirements may not make this route worth the trouble. You should probably just create the materialized view logs without specifying a column list, like this: 

In the Two Day DBA Guide see see the section called "Performing a Whole Database Backup". It explains how to use Database Control to do a full backup. If you have 11g see this version. Assuming Database Control is working this would probably be the easiest for someone who knows nothing about Oracle. On the other hand, the database may already have regular backups done on it in which case you can just send one of those. If it doesn't or the information about them hasn't been retained, then it would seem that the data would not be important enough to convert. 

Since in your example data, none of the rows with nulls have them last, we can ignore this space optimization. We can also not compare the lack of a column to a column with nulls as that would be the true "space" of a null, which according to some block dumps I did is one byte per null per row. In this case we are actually comparing the space of a null vs. the space of a value. Because any value will by definition use more space than a null, my answer is that, a null value will decrease not increase the amount of space consumed, therefore creating a percentage compared to the size of the table is meaningless. You could do a block dump of a row containing a value and then set the value to null and dump the block again to see how much additional space is available, but that number and the resulting ratio you could create with it and the size of the table would be a measure of space used by values rather than space used by NULLs. 

No, but there are ways to do what you are trying to do. PL/SQL runs with the same OS context and permissions as the Oracle service, so even if you run a procedure as the other windows user, the OS permissions in the PL/SQL will still match the Oracle services. In order to get around this you will have to in some way get the data to where the other user can pick it up or have the other user get the data directly from the database. Here are two ideas, but there are probably many more. You could create the file in a location that can be reached and then have a job running in the context of the other user that moves it to the protected folder. Another way would be to have a client application build the file itself logged in as the other user. ik_zelf's answer has an excellent way to do this. 

In their simplest a file export using UTL_FILE would be composed of a call to FOPEN, one or more calls to PUT_LINE, and a call to FCLOSE. 

Your own account doesnâ€™t have sufficient privileges. You are trying to track down an issue that cannot be reproduced using your own account. 

Create a system DSN. Create an appropriate ORACLE_HOME\hs\admin\ .ora file. Add an appropriate ORACLE_HOME\network\admin\listener.ora entry. Add an appropriate ORACLE\network\admin\tnsnames.ora entry. Reload the listener. Create the database link. 

You should describe the purpose of the column and not necessarily the data type. You can include date/time/timestamp in the name, but you should also include the meaning. For example 

Similar to Laughing Vergil's answer(+1), but Oracle specific and generates the rows needed on demand based on the largest number of entries. 

Create a new database and setup views of the original database's tables for one schema using a database link. Copy the package code for the application using that schema to the new database. Switch that application to use the new database. It should work exactly the same, except that it will be slower due to the link. For each table or group of related tables, add package code that for each database call that maintains data in both the local and remote tables. There are many ways you could do this. One way would be to have a switch table with a row for each application table. You could make a replication routine for the table and when the tables are in sync it could update the switch table to let the code know that it doesn't need to maintain both tables. 

When a Proxy Authentication is used, there is the impersonated user (called the client) and the impersonating user (called the proxy). So if Bob wants all the privileges of App1 and has been granted the privilege to connect as App1, then the client is App1 and the proxy is Bob. I would like a query of sessions that provides both the client and the proxy. v$session shows schemaname as the client user. It has osuser and machine columns from which the proxy may be inferred, but it would be fragile at best. v$session_connect_info changes the column authentication_type to proxy when proxy authentication is in use, but that only indicates that proxying is in effect, not who the proxy is. The proxy_users table gives all the potential proxy/client combinations, which would work if there were only one match for the client, but wouldn't otherwise. As the proxy user a selection could be done of SYS_CONTEXT with a parameter of PROXY_USER to get it, but that does not allow a session to get the proxy for other sessions. Update: MOS Doc ID 782078.1 shows how this can be done by joining dba_audit_trail with itself if auditing is turned on specifically and the user performs an audited action. It seems like a simpler solution should exist. 

Note: I've never used this, so please edit this answer if you have. WISE (Workload Interface Statistical Engine) uses Statspack to show graphical performance trend information. 

The Oracle Concepts Guide section on locks says, "A row is locked only when modified by a writer." Another session updating the same row will then wait for the first session to or before it can continue. To eliminate the problem you could serialize the users, but here are some things that can reduce the problem perhaps to the level of it not being an issue. 

Querying a column that can contain nulls is more complex than querying a column that cannot. So also is querying multiple tables more complex than querying one table. I wouldn't let the avoidance of null drive the normalization. For example, you mentioned that all devices will eventually be disposed and get a disposal date. If there are no other columns in the Disposal table, then in my mind it makes more sense to put DisposalDate in the Device table. Other tables like Sanitize might make more sense as separate tables because there are multiple data points that will not apply to some Devices. Check constraints are great and should be used when possible, but there will always be times when a procedure is necessary. 

You put together some good sample code. Did you look at the timing in SQL Fiddle? Even some brief unscientific performance testing will show that query three in your demonstration takes about the same amount of time to run as either query one or two separately. Combined one and two take about twice as long as three and that is before any client side join is performed. As you increase the data, the speed of query one and two would diverge, but the database join would still be faster. You should also consider what would happen if the inner join is eliminating data. 

If you have a somewhat limited number of tables that could be returned by the query, you could have the query select from all of them, but only return results for the one matching the data returned by the table1/table2 query. That would look something like this: 

DBMS_Resource_Manager has a procedure called Calibrate_IO with a Physical Disks input parameter. The documentation says this value should be set to the... 

Do you always write bug free code? Are small units always bug free? Is it OK for a large unit to have a bug? How many bugs does it take to cause a disaster? 

From my limited understanding, blocks should be embedded in the C# or put in a package and called. I've seen it like this: 

Here is a solution using the same technique as the one from Jack Douglas (+1). It produces an identical number of consistent gets using his testbed, but whether it is easier to understand or not would be in the eye of the beholder. 

Search for Keep Pool. You're probably better off not using it unless you know its purpose and limitations very well and it fits the problem you are trying to solve. 

There are however several types of statements that as of SQL Server 2012 do not correctly short-circuit. See the link from ypercube in the comments. Oracle always does short-circuit evaluation. See the 11.2 SQL Language Reference. Or compare the following (SQLFiddle): 

Insert statements with blank lines and semi-colons will succeed if placed inside BEGIN...END blocks. This change could be done using a script, but the script would fail if it contained DDL statements that cannot be run inside a block without execute immediate. This solution also does not resolve the embedded / issue. 

You can do this as long as you have definition for what should be considered "before". Based on your comments it appears that end_time determines the order you intend. 

This is rather subjective based on the nature of the data, potential for change, deadlines, longevity of the project, storage space, performance requirements, etc. However, the determining factor will likely be the probability that you will ever need multiple groupings for pages. If you will, then implement a join table now. If not, putting Group_Id inside the pages table is faster to build, less complex, will perform better, and uses less space. 

A primary key using the leading column of a unique index would require non-unique index constraint logic on a unique index. This would require changes and/or additions to the logic. If id is unique/primary then (id,val) is inherently unique, and normally you would not need/want a unique constraint/index on (id,val). You might want a unique index on (id,val) if there were a query referencing (id,val) and you wanted to prevent a table look-up. If this is your situation, you may have to decided which is the lesser tradeoff between allowing the table look-up, increasing the primary key to (id,val) or having two unique indexes. For most situations I suspect the table look-up would be preferable. 

Oracle implemented monitoring templates a bit differently than you might expect. When you apply a template to a database it doesn't associate the template with the database, but instead applies the template settings to the database. This is why modifying the template after it has been applied to the database has no effect until it is re-applied to the database. The 11.1 Oracle Enterprise Manager documentation explains how to use the "Compare Monitor Template" feature to determine how closely the template matches a particular target. In this way you can determine if the template has been applied to the database. 

As an alternative to the excellent answers provided by ik_zelf and Niall, you could virtualize both severs so that both virtual boxes can run on either physical box. If the virutalization software supports a technology like VMWares's VMotion, then the virtual boxes can be migrated between physical boxes on the fly with zero downtime. This would increase availability when maintenance is required on one box or the other. While a node in a VMWare cluster going down would cause an outage, because the VMs can be brought up on the other node, availability would be increased. As with the other options there are some situations in which virtualization does not increase availability. For example, it does nothing for shared storage outages, nor does it do anything for logical outages. 

With an Incrementally Updated Image File Backup the same script handles both the initial image backups, and the incremental backups that can be applied to the images to bring them up to date. Is there a way to change a parameter or modify this script so that the image files get created in one location and the incrementals get created in another? A partial solution is to do the inital image file backups, move them to the alternate location and then re-catalog them there. The next run will then create the incremental backup in the original location, but update the image files in the new location. The problem comes when a new datafile is created. The backup will recognize that it doesn't have an image for it and create one, but in the same location as the incrementals. Aside from repeating the move and re-catalog is there a way to have things happen as desired from the start? If you're wondering why this would be useful, consider that the image backup volume could be snapped and cloned to a new volume on another server for fast refreshes of lower environments. Having the incrementals and images in separate locations reduces the space requirements for the snap/clone. Backup Command: 

I know it would be easier to find something pre-built, but you could write a small application in your favorite language to handle the simple task of asking the database for the data and writing it to a file. For this you should take advantage of the suggestion from Jack Douglas to modify the reporting procedures to create CLOBs. Your app could then retrieve the CLOB and write the file. 

Assuming it is locked by a user entering a bad password, you could create a trigger to log information about the connection attempts. Something like this: 

If you do choose to eliminate the column, you should consider creating a virtual column (oracle specific name) that codifies your interpretation of the EmiAmount column and includes a comment explaining the existence of the zero magic value. 

Virtualizing an Oracle RAC isn't necessarily a stupid idea, but it should only be done with a full understanding of the benefits and risks. You are asking questions, so that is a good step in the right direction. Your question doesn't say which virtualization technology, the Oracle version, or the edition. For now I will assume this is for production on VMWare with the enterprise edition of Oracle 11.2.0.2+. Virtualization provides some of the same benefits of RAC, though in a dramatically different way. Both can reduce or eliminate downtime due to some hardware failures, and both can allow servers to be restarted without affecting availability. RAC increases capacity and allows for rolling upgrades, which are things virtualization can't do. VMware on the other hand can allow servers to be restarted, hardware to be changed, and storage to be relocated all without bringing any instances down. Licensing With VMWare the entire server has to be licensed even if it will be running multiple virtual machines only one of which is for Oracle. This can either be a good thing or a bad thing. If you are virtualizing Oracle onto an existing cluster with other VMs, then all the machines in the cluster will need Oracle licenses. This could significantly increase your licensing costs likely to the point of making the purchase of additional hardware miniscule in comparison. On the other hand, a VMWare cluster dedicated to Oracle can allow virtualized instances to float to less busy nodes and under-utilized hardware can handle multiple instances, potentially decreasing the overall hardware costs and licensing costs. Support Oracle's official stance for running databases on VMWare can be found in ID 249212.1, which says certification has not been done for it's products on VMWare. Since virtualization can be viewed as a hardware layer, this would be in keeping with the general lack of hardware certification. The following note was added with 11.2.0.2: