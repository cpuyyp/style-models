The really old games - written for NES, Game Boy, SNES, and even older systems like the Atari family - were usually written in assembly language. This was necessary because C compilers of the time were either non-existant or not up to the task of generating sufficiently efficient code. They were able to squeeze all of that stuff into small cartridges - for NES, even 256KB was a lot of data (though larger games existed) - because they used clever tricks, such as tile-based and low-color graphics, enabling a lot of data to be stored in very little space. Additionally, specialized chips for video and audio made it possible to handle these things more efficiently, without having to do everything in software. Over time, C compilers became more efficient and devices became more powerful - N64, GBA and later devices all used C or even C++ by default - and assembly was reserved for small pieces of code where extra optimization was necessary in order to consistently meet the 60FPS target (or whatever framerate the game runs at). 

There are multiple ways to do this, the simplest would be to XOR the two files and compress them (GZIP or so forth). The theory behind this is that hopefully you can get a large sequence of zeroes (long sequences of the same values compress well). You can take that concept further and try and find areas of the two files where the data is identical and omit it entirely. Finally, you could use the structure of each type of file to your advantage. For example, in an EXE you could package each method individually (only ones that have changed) and reconstitute the EXE yourself during patch application; keep in mind, however, that this is very likely in the realm of overkill and may not be worth the effort (the gain over a simple bdiff might not justify the extra complexity that could break in the wild). As another example you could use diff files for scripts. However, most patching systems in the wild take the simplest route: they just package files that have changed - they don't attempt to only package changes within those files (probably for good reason, most game content is compressed already and creating patches against high entropy or compressed data won't work at all). 

In both of your calls, you're asking for the two different parts to use the same mapBase and tileBase - in other words, the same area of VRAM. To make matters worse, you're actually asking the console to overwrite its own memory - it'll use the same VRAM for the map and the tiles. There's a very handy tool online for checking these VRAM allocation conflicts, and you can fiddle around with it to find some good numbers for you. However, there is one big problem: your 256x256 background is going to use up a full bank of VRAM. The NDS has 2 graphics engine, a "main" engine and a "sub" engine. The main engine has access to much more VRAM than the sub engine, which can only use one 128KB bank for backgrounds - and the console also counts as a background. You will therefore have to exploit the fact that the screen is only 256x192. The simple way would be to just move the console to the end of the bank, but that isn't possible on the sub engine. Instead, you'll have to move your background deeper into VRAM, say to map base 1 (16K), while the console occupies base 0 for the 4KB of tiles and base 2 (4KB in) for the 2KB map. Note that you must limit your background with this approach. That means you'll want to do the following initializations: 

You could also do a full 3D fluid dynamic simulation, but at a much lower resolution than the level and use marching cubes to generate the mesh; and then NURBS to smooth it out. Alternatively: 

If contained a multi-megabyte object you can be guaranteed the GC won't look at it for quite a long time - because you have inadvertently promoted it to G3. Contrast that with this example: 

Balance is mostly attained by intuition and play testing. Games where balance is important usually have extended beta cycles and frequent updates because of this. In your specific example "Magic" attains balance through years of experience in play balancing - if they find that they have released an unbalanced deck they might come up with a ruleset to address it. Unfortunately I don't think you could really fit balance into any of the UML models - considering that UML is typically used to design transactional or architectural software you would have a hard time fitting the balance problem-set into it (and would likely be wasting a lot of your time). One of the methods you might want to look into is "zero-sum balance". Basically each "thing" can exert so much "stuff" into the world given a certain amount of external factors. Taking the example of a strategy game you could do your initial balancing by saying that each unit must exert 300 damage points within 15 seconds. This way you can work out hit damage based on, say, attack speed. More complicated systems obviously have more variables and are likely impossible to calculate - so you have to need to guesstimate (intuition): any errors on your part can easily be fixed by releasing balancing patches. Another option is to try to autonomously break your balance using AI and adjust to avoid that scenario: doing this would likely teach you a lot of the intuition involved in balancing. If there was a simple way to ensure balance you wouldn't see Blizzard releasing patches for Starcraft, IceFrog wouldn't have been so famous because of his balancing prowess. It all comes down to testing and listening to your community: which are often rife with people who are very experienced with balance - especially if your game's subject matter manages to attract such a community. One of the main reasons balance is hard because there is a lot of "ghost in the machine" (where a computer exhibits surprising behaviour) involved in games. For example, if you are using a dice-roll/random damage system the random number generator you are using could introduce balancing factors. Another problem is ingenuity: you might make some unit that can teleport and completely underestimate how important mobility is. Things like this simply can't be modelled at all. 

Since Hearthstone is written in Unity it is trivial to decompile the scripts (because .NET), and this does in fact allow us to see the algorithm for deck building. The algorithm (found in ) is actually very simple. It basically tries to pick random cards to bring the deck to the following distribution: 

assumes the background uses base 0, which obviously is no longer the case. Instead of using BG_GFX directly, store the return values from bgInit and bgInitSub and send them to to get the proper starting address: 

The function takes an argument to specify the number of cards to pick, and it will attempt to randomly pick that many cards matching the first unmatched criteria in the list above; if it cannot pick enough cards from matching that criteria, it moves to the next one and continues until it has enough cards. If all criteria have been processed, it just picks random, valid cards regardless of the type or mana cost. When asking for recommendations, it picks 3 cards; if you ask it to finish the deck for you, it repeatedly picks one card until the deck is full. This only applies to personal deck building; cards for Arena decks are picked by the server (see class ), and might therefore follow a different algorithm. 

Luckily for you this means that a player in bullet time does have an advantage over everyone else; they can't possibly aim at him - however it wouldn't be convincing because the average person would expect to see no discernible difference when observing time dilation. I strongly recommend you post this on physics (indicate it is a game, but you need real theoretical knowledge) and ask for ideas how you would approximate the effect in a concurrent simulation. One possible solution I would look at is having the world catch up with the player (as he has moved into the future) by having everyone experience a progressive amount (far less than the actual bullet time) of bullet time depending on far behind (in time) they are. This way you could approximate time differences without actually having to deal with storing past state etc. You would need some way to explain this effect, 'time rebound' or something, because it's not physically correct. Another idea is to abuse the most basic physics equations (), when a player goes into bullet time everything becomes smaller - if you halve the size of everything from his perspective but keep his speed the same according to his original frame of reference, (effectively double it) the equation will still balance. Warp drive sequences in space films use this space dilation to portray the effect - so people are used to seeing it. Actual Solution I have given this even further thought. If you look at how the Steamworks network system works (does someone have a link?) the server is constantly ahead in time by a certain amount; and each client knows about the 'next' frame that it needs to interpolate to. Let's say for instance that the server is 20ms ahead of all the players. When a player enters bullet time change his update interval to 40ms (his simulation will run at 40ms time-steps and the server will queue his actions 40ms into the future) - which will basically push into the future in reference to other players: in addition to making him move faster from their reference and making them move slower from his. Once he leaves bullet time give everyone else a 5ms advantage over him so that they can play 'catch up' (this is your , coincidentally smart players will bullet time during these catch-up periods). It might look a little better if you map a parabola to his time step - this will make things slow down, stop and then gradually speed up again. Disclaimer: I am no physicist, but I do have a workable knowledge of relativity. 

I am making an iphone opengl es 1.0 game. I am using a fixed timestep with interpolation on the end. One thread only. I am using CADisplayLink to fire the game loop. First question: is it okay to use CADisplayLink to fire the game loop? i am targeting > 3.1 so that's not an issue. Real question: What's the proper way to handle user touch input as it relates to my gameloop and adding state to my game. For example lets say when the user touches the screen i want to fire a bullet. I've read that I don't want to actually update my game state in my touch handler function. But I should instead record that touch in an array or something and then on the next gameloop update(dt) i go through the array and updated my game state based on the array items. is this correct? shouldn't i be storing a timestamp of each touch so i know exactly how to inject it into my game? 

Notice that the contributes to 50% of the value and the contributes 50% of the value. Taken slightly further: 

As per my comment, does not work. However, you should be able to use a to achieve what you are after. You can map a one-dimensional index to a two-dimensional index using the following function: 

The problem is that XNA on Windows Phone doesn't have custom shader support - so you can't write a vertex shader or pixel shader. However, you can use a trick described by Catalin Zima that deforms a vertex grid to achieve the same effect. If you are not targetting Windows Phone 7, you can use a trick that I described on my blog. Copying the relevant bits in: These distortions require 2 images. Firstly you need the entire scene as a render target (i.e. Texture2D) as well as the distortion render target. Typically you would use a particle system to fill the distortion render target; using special distortion sprites (example below). Each color component in the distortion target (and distortion sprites) represents the following: 

There are a number of reasons why a PC port can take a while. (I apologize if I seem to be repeating myself somewhere; this is sort of written on the fly.) Adapting controls and gameplay When you're playing on a console, this alone puts certain limitations on what you can do, since all the user has is a gamepad. Just creating 1:1 mappings between keyboard keys and controller inputs is not always a good idea - if even possible - so sometimes it takes longer to figure out a good solution. Hardware abstraction/Fragmentation When you develop for e.g. a Wii U, you know exactly how a Wii U behaves, because all Wii Us are identical. This is not true for PCs; you have many different graphics cards and CPUs, and sometimes something won't work on some of them. It takes a lot of testing to uncover these bugs, and fixing them also takes time. If you've never used your engine to make a PC version, you also need to code your hardware abstraction accordingly. Some games want to support multiple DirectX versions and OpenGL for Linux/Mac, and all of that takes time to write if it hasn't been done before. Resource contention On consoles, the game doesn't have to compete with an OS for resources, etc. - not a whole lot of stuff goes on in the background. On a PC, you have the OS running, you have a plethora of background programs, and this all means you won't get as large a share as you were hoping for. This means you sometimes need to perform additional optimizations, especially for players on lower end systems Improving assets With a console, you have a fixed target, so you write shaders, etc. to match that target. On a PC, some graphics cards support more advanced features, and maybe you want to use a better shader for those. Well, that means you'll have to write that shader. Platform-specific stuff Console SDKs may have a lot of convenient features that don't map over easily to a PC - for example, it might provide access to hardware timers or a good sound API. Those things aren't usually available on PCs; you need to use other ways of accomplishing those things and maybe that changes how you have to abstract the platform differences. 

I am making an iphone shmup and am trying to decide what type of game loop to use. I want to use either semi-fixed timestep or fully-fixed timestep. With semi-fixed timestep I will make zero or more update(FIXED_INTERVAL) calls followed by one update(dt) call where dt <= FIXED_INTERVAL per game loop. As I understand it the drawbacks with this method are that my physics update(dt) logic is going to be more difficult to program because I basically have to assume a variable dt for every update. And then I've also heard that each run of my game will be slightly different due to floating point values not being the same every time. Then with fully-fixed timestep I am making zero or more update(FIXED_INTERVAL) calls followed by one interpolation(dt/FIXED_INTERVAL) call where dt < FIXED_INTERVAL per game loop. So it seems like the big decision I really have to make is: do I want to tackle the challenge of implementing an update(dt) with a variable dt or do I want to tackle the challenge of implementing interpolation? Now from what I've read the majority of people are saying to use fully-fixed and do the interpolation. But when I think about implementing interpolation it seems like I'd be a lot more complex than an update(dt) with variable dt. This is because if I use interpolation I have to remember both the previous state and the current state. So if I want to use interpolation I have to come up with an additional layer of indirection that abstracts out entire individual game states. Whereas with semi-fixed timestep where I don't have to use interpolation I don't have to come up with a game state abstraction because there's always only one game state and it's simply the "global arrays" that represent my enemies, and enemy bullets etc. So what's the more practical choice: do I implement it semi-fixed knowing that my physics updates could get complicated with the variable dt. Or do I use fully-fixed and try to come up with a game state abstraction so that I can keep track of previous state and current state in order to perform interpolation? 

You could also look into SharpZipLib or DotNetZip - DotNetZip gives you access to a directly, which is helpful as most C# libraries out there can load from streams (as opposed to files). Furthermore DotNetZip uses the Ms-PL (BSD-Like), instead of the GPL with a legally questionable exception. Another option is to use texture atlases - instead of having many small images have a few big ones. These actually load slightly faster and are slightly faster at runtime if you get smart with how you draw them. Finally you could run each asset type through the relevant lossless compressor (PNG, FLAC, etc.) and store them in a flat file (basically a zip file with compression disabled). Remember to avoid compressing compressed data, as that will increase the size in most cases. Some prior art: 

Farseer has springs. Elastic and Inelastic Collisions An elastic collision is when the kinetic (movement or momentum) energy of colliding bodies remains the same after a collision (it can still be distributed differently between the bodies). This means that no energy is lost to other forms of energy (for example, sound, heat and light). Elastic collisions (or "super-elastic", where the energy is greater) only happen at the molecular level - however elastic collisions can be passable in games. An inelastic collision is when a portion of the kinetic energy is converted to other forms of energy. For example, when two billiard balls collide you hear a sound (although this does have a negligible impact on the total momentum). If a car collides with another one a lot of energy is lost to deformation and heat. At the macro-scale these types of collisions are more accurate; and will generally improve the stability of your physics simulation (they reduce the chance of a rounding error or such from introducing more energy). Wikipedia has very good articles on both elastic and inelastic collisions, Physics for Game Programmers has a free excerpt on both.