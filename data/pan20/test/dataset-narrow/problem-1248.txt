I've seen texts define TIME( $f(n)$ ) using multi-tape Turing machines, but Sipser uses a single tape machine. You've almost surely first encountered this material through Sipser because it's so fabulously well written. There is a crystal clear pedagogical reason why Sipser does this, namely the course just naturally flows that way because : 

In fact, I'm probably content with a easy quadratic program that finds the midpoint of the entire polytope since centrality matters more than minimality, just vaguely curious if other linear programming algorithms offer relevant properties. Update : I've reduced the underlying problem to a simple constrained minimization problem solvable with Lagrange multipliers, but the question above remains interesting anyways. 

A Turning machine with insertion and deletion operations can be simulated by an ordinary Turing machine with a quadratic time cost. Do we know how insertion and deletion fit into the polynomial time hierarchy though? In particular, does anyone know a quadratic single-tape Turing machine that cannot be simulated by a linear time single-tape Turing machine with insertion and deletion? I've gathered that separation results are often more powerful, and maybe easier to prove, for the nondeterministic time hierarchy. Is that perhaps an easier place to attack this? If so, that's great because I'm ultimately most interested in rewrite systems anyways. 

At present, your question only permits the coefficients $0$ and $1$. If you also allow $-1$ as a coefficient, the conjunctions of constraints you get are called octagons in the program analysis literature. Conjunctions and disjunctions of constraints are form the logic of Unit Two Variables Per Inequality (UTVPI). The introduction of the following paper surveys algorithms for deciding satisfiability of conjunctions of quantifier-free UTVPI constraints. 

This is quite a strong and positive result and recently has found several applications. However, as always the details are important, as the title of the paper below already suggests. 

The quick summary is that LTL with only past and no future modalities defines properties expressed over finite-words and these are the star-free subset of the regular languages. Standard LTL when extended with past-time modalities does not have more logical expressive power than LTL with only future modalities but properties can be defined in an exponentially succinct manner. For a more detailed answer, there are at least three logics one can consider in answering your question and at least three different notions of expressiveness. 

I'll concur with JɛﬀE that MS degrees are viewed as "consolation prizes" in the sciences in the U.S. because people usually take them when they fail qualifying exams in Ph.D programs. And who pays to do an MS when they'll pay you to do a Ph.D directly? I'd also concur with David Harris that mathematics might prove the most efficient route to doing serious theoretical work, but this depends entirely upon the program. Ask any math or comp. sci. departments who make offers how they feel about students taking courses outside the department though. I do recommend that you broaden your interests in more applied computer science of course, but do so by reading something. There are mathematically entertaining topics around databases, like Bloom filters, as well as fun applied papers, like the CryptDB articles. 

I'm interested in an computational geometry problem that's sensibly expressed as an infinite dimensional 0-1 integer program. I'm not worried about finding an actual minimum for the objective function, any solution with isn't stupidly big will do. It thus seems natural to apply an approximation algorithm that starts by running simplex or similar restricted to $[0,1]$. I'd expect the solutions usually require only a few hundred dimensions, but any naive restriction of the problem space yields millions of dimensions. As I understand it, good implementations of a linear program solver should be polynomial time in both the dimension and constraints on average cases, but nevertheless this problem chokes GLPK. Should GLPK really choke on a million dimensions? I've therefore started looking for less naive restrictions of the problem space, which lead me to LP-type problems. In particular, there is a claim that Clarkson's algorithm applied to linear programs are equivalent to running the simplex algorithm on the dual problem. In what sense is this true? I find this claim highly dubious with respect to complexity for several reasons. First, Clarkson's algorithm does not exploit any $[0,1]$ solutions with fast average case solutions, but merely randomly chooses pivots. Second, Clarkson's algorithm has running time worse than exponential in the dimension $O(dn + d! d^k \log n)$, which doesn't rule out polynomial time for average cases, but I haven't found that fact yet. As an aside, any nice examples of improving a restriction of an infinite dimensional linear program over time? 

The propositional $\mu$-calculus A line of work from the late 60s showed that numerous properties of programs could be expressed in extensions of propositional logic that supported reasoning about fixed points. The modal-$\mu$ calculus is one logic developed in this period that has found a wide range of applications in automated formal methods. A lot of formal methods is connected to temporal logic, or Hoare-style logics and much of this can be viewed in terms of the $\mu$-calculus. In fact, I have heard it said that the $\mu$-calculus is the assembly language of temporal logics. In his paper introducing the $\mu$-calculus, Kozen gave an axiomatization and only proved it sound and complete for a restricted fragment of the logic. The completeness proof was one of the great open problems in logical computer science until Walukiewicz gave a proof (based on infinite automata). The model theory of the $\mu$-calculus has many rich results. Similar to van Benthem's theorem for modal logic, Janin and Walukiewicz proved that the $\mu$-calculus is expressively equivalent to the bisimulation invariant fragment of monadic second order logic. The $\mu$-calculus has also been characterised in terms of parity games and automata over infinite trees. The satisfiability problem for this logic is EXPTIME complete and Emerson and Jutla showed that the logic has the small model property. Bradfield showed that the alternation hierarchy of the $\mu$-calculus is strict, while Berwanger showed that the variable hierarchy is also strict. Important classical tools used in this area are Rabin's theorem and Martin's determinacy theorem. 

There is a linear program for which I want not merely a solution but a solution that's as central as possible on the face of the polytope that assumes the minimal value. A priori, we expect the minimizing face should be high dimensional for various reasons, including that the objective function being minimized is the maximum of many of the constraints : Minimize $\epsilon$ subject to $f_i(\bar x) \leq \epsilon < 0$ with $f_i$ linear and $x_i > 0$ for all $i$ and $\sum_i x_i = 1$. We'd never obtain any centrality-like property form the simplex algorithm of course. Do any of the usual interior point algorithms exhibit such properties though? Do any even guarantee they'll avoid vertices or lower dimensional faces whenever possible? 

We believe code-based public-key cryptography to be post-quantum. In fact, code-base cryptography has the longest history record among post-quantum public-key schemes, but the key sizes seem impractically large, like 1MB in McBits. We use error correcting codes in lattice-based public-key cryptography too, which employ a reconciliation phase like Felipe Lacerda mentioned. In fact, our current best bet for a post-quantum key exchange is the Module-LWE scheme Kyber (lattice-based). 

Infinite Games Logical and infinite games are an active area of research. Games-theoretic notions show up in computer science all over the place in the duality between non-determinism and parallelism (alternation), a program and its environment, universal and existential quantification, box and diamond modalities, etc. Games turned out to be a great way to study properties of the various types of non-classical logics listed above. As with acceptance criteria for automata, we have different winning conditions for games and many can be shown to be equivalent. Since you asked about classical results, the Borel Determinacy theorem and Gale-Stewart games often lie discreetly in the background of several game models we study. One pressing question of our times has been about the complexity of solving parity games. Jurdzinski gave a strategy-improvement algorithm and showed that deciding the winner was in the intersection of the complexity classes UP and coUP. The precise complexity of Jurdzinski's algorithm was open until Friedmann gave it an exponential-time lower bound in 2009. 

The papers of Doron Zeilberger. He is a mathematician and his computer is listed at the coauthor Shalosh B. Ekhad on all papers where the computer played a part in deriving the results. Work of Georges Gonthier. He engineered a machine-checked proof of the four colour theorem and has been recently been working on the Feit-Thompson theorem. He recently completed the formalisation of the Odd-Order Theorem. Archive of Formal Proofs contains proofs checked with Isabelle, and includes correctness theorems for algorithms, the Gauss-Jordan theorem, some order theory, category theory, and many more classical results. Formalizing 100 Theorems, contains machine checked proofs of some famous theorems. 

There are a wide variety of determent-like constructions. Some like the permanent or immanents are variations on the ordinary determinant for matrices over fields or commutative rings. Some like quasideterminants extend the theory of determinants to non-commutative rings. At least permanents have a rich complexity theory, perhaps the others do to. For any of these constructions, is there a quantum algorithm that asymptotically out preforms the fastest classical algorithms for computing it? I'm asking because some post-quantum crypto systems like $URL$ and code-based crypto suffer from extremely large key sizes due to representing public keys as matrices. A priori, it might be possible to "compress" that large matrix into something like a characteristic polynomial but using some non-commutative analog of the determinant. This approach sounds less viable if quantum computers could compute some analogs of the determinant more quickly. 

How much is known about nondeterministic linear time? I'm aware that $$ \mathrm{NTIME}(n) \neq \mathrm{DTIME}(n).$$ Is there an $m > 1$ so that $\mathrm{NTIME}(n) \not\subset \mathrm{DTIME}(n^m)$? Are there any arguments that $\mathrm{NTIME}(n) \subset \mathrm{P}$ should be unlikely? 

There is no reason to quibble over conceptual cleanliness when the pedagogy so clearly dictates the easiest path, and every computer science undergrad must take this elementary course, including all those who still don't understand proofs. 

Computational-Tree Logics Instead of a linear notion of time, the behaviour of a computer program can be understood as a tree, leading to the notion of computational tree logics. The simplest such logic, Computational Tree Logic can be viewed as an alternation-free fragment of the $\mu$-calculus. The difference between LTL and CTL led Emerson and Halpern to develop CTL*, which allows reasoning about both properties of states and traces. The model checking problem for CTL over finite structures is in polynomial time. The model checking problem for CTL* is EXPTIME complete. The axiomatization of CTL* was a challenging open problem that was finally resolved by Reynolds 2001. The analogue of van Benthem's theorem for modal logic and Kamp's theorem for LTL is given for CTL* by a theorem of Hafer and Thomas showing that CTL* corresponds to a fragment of monadic second order logic over binary trees. A later characterisation by Hirschfeld and Rabinovich is that CTL* is expressively equivalent to the bisimulation-invariant fragment of MSO with path quantification. 

Results on the propositional $\mu$-calculus, Kozen, 1983 Rudiments of $\mu$-calculus Arnold and Niwinski, 2001 Completeness of Kozen's Axiomatisation of the Propositional $\mu$-Calculus, Walukiewicz 1995 Modal logics and $\mu$-calculi, Bradfield and Stirling, 2001 The modal mu-calculus alternation hierarchy is strict, Bradfield, 1996 The variable hierarchy of the mu-calculus is strict, Berwanger, E. Grädel, and G. Lenzi, 2005