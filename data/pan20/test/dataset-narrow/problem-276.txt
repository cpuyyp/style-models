In the first instance, confirm that there are no attempted email sends from your script. For the second the DBA might not have set the permissions correctly on the trigger itself, and so it is attempting to send an email using your credentials. Work with the DBA to either grant you permission on msdb..sp_send_dbmail so that emails will flow, or to fix up the trigger. 

I turned your PS into a function, just changing the calls, ran it through the PS ISE, and it worked just fine for me in testing, outputting the data to the location. I would ensure that you have the ability to write to the root of the C: drive. Typically this can require admin permissions. It's possible you are swallowing the error somewhere. 

Alternatively you could look at using a trigger to ensure that the value does not get set to something different. 

Your question is a little confusing (in particular around the joins), does this look something like what you are looking for? 

Yes, you can absolutely do this. I've done this with several 2012 to 2016 migrations and not encountered any issues along the way. Just bear in mind, you will not be able to fail back to the 2012 instance once you fail to that 2016 replica, and until you have more 2016 replicas you will not have any HA in place. 

The call needs to be dynamic for the databases, as does the object_id. This is basic (does not check for offline databases and the like), but gives you an idea as to how it would function, looping through the databases in sys.databases. 

I've been using AGs for 3 years now, with table compression, and have not seen any material impact. The data shunted between the servers is all log transport, and that itself is compressed, the compression of the pages themselves doesn't really factor in. Your only concern would be if you were under CPU pressure already and then added on that little extra for the table compression. 

When using SQL Server Enterprise Edition (or running any edition newer than 2016 SP1) I would recommend going with SQLAudit functionality. This will give you some great granualar information about who is touching your tables, and the commands that are being executed. For something like this you would want to use a database audit specification along with a server audit (used to define where your audit will write to), and then scope it down to the table that you want to monitor. By then scoping changes to the public role you will capture any and all changes that happen. This script should get you there (test in your dev environment, and replace the relevant pieces for the table(s) that you want to monitor). 

Your main problem here would be maintaining what backup was relevant to what, and you would have to continue taking full backups on your primary server in order to reset the differential base and prevent you diff backups from becoming as large as the fulls. Consider it this way: 

Yes. The limitation here is that a database can only reside within a single AG, and that all nodes must be in the same WSFC. Be aware though (going back to your licensing question) that if a server is a primary for databases then it must be appropriately licensed. 

It appears as though you are trying to get the number of rows returned from the view, and have the ability to print that value as an output. Your query looks like it's trying to write this data to a table. Try the following (replace the default view I've used here with whatever you would need) 

You can quickly read the data back using the GUI, or use sys.fn_get_audit_file to query the data directly from SQL Server. 

You could put a constraint on the table to prevent values other than that value from being inserted (example here uses 0) 

If you are running this as a SQL Server Agent job step you will need to use the step type, not a type. Within the it would be best to create a file on the machine with all of the steps that you require and then call that. For example create a file which contains... 

I would use a CTE here to provide row numbers to your table data (partitioned by the empName). That way you can update the cte, which has the advantage of actually updating the base table. No bear in mind that because you have no ordering column there is no guarantee of which row would actually get updated (you would want more restrictive sorting conditions around this), however for a base example you can use the following. 

Your table create script includes an attempt to send an email Your DBA has placed a DDL trigger on the database to send an email notification when objects are created. 

Yes, absolutely. You would typically use transactional replication to move data between the publisher and subscriber(s) (although you can include other objects as well). Your subscriber (destination) database is fully writeable, and you can create stored procedures there, add indexes to replicated tables, and even create entirely new tables. Be aware, you can also delete data from the replicated tables on the subscriber, which could lead to replication breaking and you having to re-snapshot. 

You can use sp_rename to change the name of procedures, columns, tables, indexes, constraints. You'll have to do each object individually (although you can create a single script that renames the objects), and I would highly recommend running through a test environment first to ensure that you do not run into any problems. 

You will need to license your secondary if you allow read intent routing. That is something that gets configured at the AG level. But ALWAYS check with a licensing representative to ensure that you are compliant. You should not need additional bandwidth for AG traffic if you are already using mirroring. Packets are compressed prior to sending between machines (when running in asynchronous commit mode). 

Rather than add new objects to your servers, consider using the function of the open source DBATools.io project. This would make it a simple as 

It is always a recommended practice to keep the parameters to be the same data type. This is to prevent potential implicit conversion issues with predicates which can lead to the query optimizer not being able to utilize statistics, or indexes, which leads to poorer performance, and can lead to serious CPU and storage performance issues. 

That code won't actually work. (op updated script after this answer was posted) CTEs are effectively subqueries (that support recursion). They can only be referenced within the scope of that particular command. As such your first select * statement would work, the next two would error out not being able to find the CTE. In order for those to work you would need create another CTE for each to reference. And in that situation you would hit the people table 3 times, once for each query. To improve this you could put your results into either a temp table, or table variable and then just query that. 

While Merge is not the best performing solution, based on your requirements this would handle what you requested (as it would insert rows that did not exist on the target). I put a quick schema together that may not match yours, but gives you the idea. 

In the query you have it appears as though you are grabbing the first two hundred rows, and then sorting those by dbms_random.value. This would be due to Oracle's query precedence which performs the query and then sorts the results. You would want to randomly sort the results first and then return the first two hundred rows. Using a subquery should provide you with the data that you want.