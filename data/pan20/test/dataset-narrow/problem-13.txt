There is no need to switch, in IT, generally. You can just add more and more stuff to your toolkit over the years and decades. Just because your new job description is "DevOps" instead of "Java" does not mean that you somehow immediately forget your Java background. Hunting for something more interesting is always good. It means more learning. You will very likely (hopefully) alternate between hunting for something and deepening that something for your whole life. 

My limited experience is indeed that often, you can get a first solution very quickly with pre-made automation code; unfortunately, often there either appear unexpected problems during "first contact", or later during use, especially when you wish to expand the configuration for unusual cases. So, indeed, I do tend to approach this with an open mind. I will certainly start out with something pre-made, but sometimes my gut feeling tells me to take a look under the hood, and then I'll decide rather spontaneously if I better do it myself, or whether it is a good idea to roll with it... 

Yes, you are doing the correct approach. Or rather, "a" correct approach, in your specific circumstance. You are proposing to build the image beforehand, not as part of each individual Jenkins job. Then your Jenkins job will consist of running the image; and in that ephemeral state (i.e., inside the container) you are checking out the test script, and running it. The more usual alternative would be to leave the actual to Jenkins, and pull the script into your container with a bind volume. Jenkins has to be able to access your git repository anyways, so you get this checkout for free. There are two sides to the coin here: if you do the from inside the container, then you can run your test outside of Jenkins with no change whatsoever - say, during development, on your own machine. In any case, if you do the from inside, and if you need credentials to access the git repository, be sure to handle those securely. Don't put cleartext passwords in the image, or ssh keys with an empty passphrases. The benefit you are getting from this is that your test environment (libraries, 3rd party test drivers and so forth) is well defined, so you can run it on any Jenkins node you have available. By not creating new images whenever your code changes, you avoid creating and distributing new images for every job. 

You are overthinking this. Pick a DevOps technique, anyone which seems interesting to you, download some stuff, and just tinker around with it at home for a bit. Or find a nice little project at work. You'll find out in no time whether it is your thing or not. Nothing about DevOps is particularly "special" or "magic" or a totally "different" thing. 

The quote is misleading since there is not "a" chef in the kitchen. There are many different chefs, i.e. pantry chef, soup chef, fish chef etc. Wikipedia lists many others. What the quote is thinking of is the "chef de cuisine", which is the overall manager of the kitchen, the CEO if you will. There is also the whole executive hierarchy we know from our IT companies, e.g. sous-chef (second in command), commis chefs (department manager) etc. So, your fullstack dev is first and foremost a someone who can cook well. But even if you cook the greatest delicacies, that does not make you a good manager. On the contrary, there are people who are actually trained to be managers/leaders. The skillset, and daily work, of managing the logistics of a large kitchen is vastly different from the cooking itself. Instead, a fullstack dev specializing in AWS, Azure, CI/CD, "xxx as code" or any of the other tools that we associate with DevOps, would be comparable to a soup chef specializing in soup or a fish chef specializing in fish (with no managerial tasks attached to the job description). And finally, remember that in IT, "Agile" is all about removing leadership from the immediate development process and placing leadership where it can do good: enabling the devs to do their work. For that, they do not need to know that much about software development, or be a great dev themselves. 

Based on this Github documentation it is possible to pull a docker image from a private docker registry: values.yaml 

Back in the day when I ran , I could login to $URL$ immediately, but know a login prompt is shown. When is clicked the following message is shown: 

A certain check works, but it should be checked multiple times within a certain range before a user gets notified. There are multiple Sensu attributes that could be configured. This list has been consulted, but no solution was found. Is it possible to mark a check as warning or error if a check fails multiple times in a certain time range? Perhaps aggregates would be an option, but should this not be used for multiple clients instead of using it for one check? 

There are multiple distribution in the company. Building the same project results in different build times (see table 1). 

AKS $URL$ Looks interesting. I have not tried it myself, but based on the more than 300 github stars this looks interesting to me. The readme contains screen shots how to configure it in Azure. 

This tag contains the date of build, the commit and the semantic version. If a docker image runs in production and a bug is found then one knows the version of the product, the code that is inside and when the image was built and under what circumstances. 

One could use the Google Authenticator app (GAA). This document describes how to configure Ubuntu Xenial in order to use this two-factor authentication method. 

Googling "system thinking" returned a lot of resources that are about "Systems thinking" rather than "System thinking". This source indicates that "Systems thinking" is one of the three ways that could be applied in DevOps. After reading several references "Systems thinking" is about interactions rather than silos. If one translates this to DevOps then "Systems thinking" is aligned with the theory and also important for DevOps as this is the interaction between QA, Dev and Ops. In order to apply "Systems thinking" one should be able to communicate with people from Dev, Ops and QA, listen to them, understand their problems and find ways to let them work together instead of getting a "through it over the wall" mentality and unconnected departments in the company. An element that really requires "Systems thinking" is Continuous Delivery (CD). Multiple departments are involved, while CI is mostly used by Dev, QA and Ops will be definitely need to be involved in this process as one does not want to release a newer version if integration or manual tests failed and Ops want to be informed if there are changes to the system as they monitor systems and try to prevent and solve issues on the platform. In summary, "Systems thinking" is required for DevOps engineers. If one only sees Dev, Ops and QA as individual parts, but no interdependency then this is not aligned with the DevOps theory, i.e. intersection between Dev, Ops and QA. References $URL$ $URL$ 

Also check the content of the log files and ask "is this relevant logging"? Sometimes, complete stack traces are logged. If this is the case, what is causing this and how to suppress it? Some apps define several log levels. If the app is running without issues, just set the log level to normal. If there are issues, change it to debug for a short moment. If finetuning the logging does not help, then introduce log rotation or keep the logs for only one month, but the first thing I would do is inspecting the log and see whether there is superfluous information that could be removed by modifying the code. 

There is no "right" or "wrong". If the commands work when you type them in, then they work, and we won't keep you from doing it. Everything else is opinion. Sure, there are some best practices, for example some people find it unwise to directly fetch scripts from a public (3rd party) website and execute those locally without having a look inside first. It goes without saying that your approach is in fact a rather significant risk. You are effectively giving root access on the target machine (and any other machine your local user has ssh keys for...) to anyone who can push into your repository. How earnest that risk is is something only you can decide. 

Not for a long time. People forget that below the virtualized stuff (VMs, containers, etc.) there still are the same technologies as always, requiring people who know how to manage the basic systems. Sure, it will get different, but not disappear for any time soon. 

You can story your private key in one circumstance: if you configure your repository so that it only allows read access for that particular key, and you don't bother about the repository being publicly readable. In this case, you can easily store your private key - obviously make a throwaway key without passphrase. But as I said, only if you are fine with public access to that repository. If that is not the case, then you are probably stuck with the SSH agent. It is not ideal, as can take the socket, but if you are in a kind of benevolent environment (i.e., no attackable webserver anywhere near), then it should be OK. 

Docker and all the other fancy names are just tools. The examples are just examples. Resource-wise, Docker changes nothing much beyond usual Linx/Unix multiprocessing. It does not have the overhead of VMs, and little overhead over just starting the process directly ( $URL$ ). So it is really up to you to decide how to use it. If you have closely related applications under your own command, maybe in your intranet, why not use a shared DB. If they are customer DBs, or you might need to run different versions of the software, or different incompatible configurations, then separate services might be fine. For RDBMSses especially, they are usually very good at managing resources (RAM, HDD) and so for this case specifically, running only one may just be the best thing. It's really more about what your needs are than general guidelines. 

Put all PL/SQL in one (or more) separate Oracle schema. If your application today uses only one "myapp" schema for tables and PL/SQL, then in the future you will have "myapp" (tables, indexes etc.) and "mycode" (or whatever) which only contains PL/SQL procedures. In "myapp", create synonyms for all the objects that now live in "mycode". In "mycode", you can create stub tables - i.e., the same tables as in "myapp", but without any data. You do not need any indexes or whatnot, just the tables. These are only used at compile time; they are especially not used by the optimizer, later. 

Now, this is laudable, but using Ansible (or Puppet or Chef or ...) would be "learning" as well. The benefit of using established tools over DIY is that if/when you meet other developers, you have a common toolset to work with. Doing it yourself is nice sometimes as well, but basing a whole software development process on 100% self-written tools (from scratch) is not a good idea, unless you have deeply evaluated the existing tools and consider them all unfit for the purpose. When your team grows and you encounter a new developer - is it more likely that they already know about Ansible, or that they know about the code you wrote? Also, the fact that the tools mentioned above do not do their stuff in the "code" directly, but that they have text files which configure them, is important as well. It abstracts the configuration stuff away from actual code, which will be hard to maintain a few years down the road, no matter how good you are.