One approach is to check the string form of the JSON column. An object should start with '{', or maybe by some spaces followed by that character. So 

I am not sure if this is a very clean solution, but it might work. Suppose that you have a table and a table. Create a third table, . This should have three columns, , , and . and are both foreign keys to the respective tables. for the column, restricting the integer values that it can take, for example from 1 to 8. Now impose two constraints on the table. First a constraint on , which ensures that a member can only have one leader. Then a constraint on the combination of columns . This will mean that each leader can only have as many members as there are distinct values possible for . I can see a couple of drawbacks with this - first of all inserting a new is now a little tricky, as you have to find a value of which has not been used. Bear in mind you have to make a check at this point anyway - to see if the in question already has enough members. Secondly the type for the might suggest that there is some difference between the members which have the different values - that the numbers are meaningful. However, I think it is a reasonably clean and normalized design. 

I do not see your exact database version anywhere, but it seems to be the bug described in the below MOS note: RMAN-06025 - RMAN RESTORE DATABASE PREVIEW at standby site is asking for old log (Doc ID 1599013.1) 

The removal of other options ususally include running scripts provided by Oracle and/or dropping schemas. For example to remove APEX, you run the below in a regular (non-cdb) 12.2 database: 

You will get the results in CSV format the first time already. You can even spool the output just as in SQL*Plus. So you could just run the below block of code as a script (select lines and F5) and get a CSV directly in one pass: 

First of all, people often fall for this, but high waits do not necessarily mean I/O problem. Second, this could be really troublesome, if you have many physical devices in the ASM diskgroup where your redo logs are, because the extents of your files will be evenly distributed on several disks. Anyway, you need to find the ASM diskgroup number and file number for your redo logs. For example my redo logs: 

So it wanted to recover the image copies created on to an earlier state: = . That does not work like that. You can not rewind image copies to earlier states. When you try to do something like this, RMAN will look for an earlier image copy, which it can recover (forward, not backward), but there is no earlier image copy with the specified TAG, hence: . So this happened all day until today. Today, this script ran again: 

We are using Postgres 9.3 and making use of the JSON column which was new to that version. Version 9.4 added a LOT of utility functions for handling JSON, as well as the new JSONB column. You can see this by comparing $URL$ with $URL$ 9.4 has a function, which returns the type of a JSON object (as a string: 'string', 'number', 'array', 'object', etc.) I would like to know if there is any practical way in 9.3 of retrieving all rows which are scalars (or, equally useful for us, which are not objects). We would like the column of a given table to always be a JSON object. This means that we can do queries like: 

The only situation where I would not do it this way, was if 1 to 8 is something which could never ever change. For example, you might be sure that there will only ever by 7 days of the week (but who knows?). In this case I would create a TYPE with this constraint. Then you can use the type in multiple tables. If the days of the week (or whatever) DOES change, you can change the TYPE. 

(This query by itself doesn't look too bad, but you are going to use it everywhere you would use a single table name in JOINs and so on). 

After you found the ASM generated name for the disk you want to shrink, reduce it in ASM to the desired size (I will assume it is called : 

Actually this is really easy to verify, you can do this by enabling 10053 trace. You will see that the optimizer does not even consider skip scan at all. The reason for this, is the "_optimizer_skip_scan_guess" parameter. The default value for this parameter is FALSE, meaning the optimizer will not consider skip scan when all it has is "guessed" selectivity, which is the case with dynamic sampling. If you set "_optimizer_skip_scan_guess" to TRUE, skip scan will be considered, this can be also confirmed with the 10053 trace again. PS: your db_file_multiblock_read_count parameter seems to be lower than the default value. On my 11.2.0.4 sandbox, with the default value of 128, after collecting statistics on the table, index FFS had about one-third of the cost of index SS. Edit: added output 

Assuming dedicated server configuration, when regular kill or disconnect session does not seem to work, you can find the OS pid, and kill that process. select p.spid from gv$process p join gv$session s on (p.inst_id = s.inst_id and p.addr = s.paddr) where s.sid = &SID; On Linux/UNIX, you can simply use . On Windows, you use , for example . 

Should be, because depending on the scale of inconsistency of the remaining SYSTEM datafile, the instance may not open at all, or it may crash immediately or in best case scenario, it may open and remain stable so you can query , , etc. 

should catch most cases. However, unlike the column, columns store JSON in its original form, not in a canonical form. So this method doesn't exclude the possibility that there will be some weird JSON, for example starting with another space character, which won't be matched. 

I would certainly recommend B in most cases. Both A and C leave open lots of room for inconsistency. Enforcing that exactly one of two columns is (for case A) is quite a pain. If you need uniqueness, enforcing that one column is either a unique value, or , and the other one is also either a unique value, or , with exactly one of them being , is a huge pain. C also misses some consistency checks as you recognized. I don't exactly understand what the information is that you are trying to represent. But it usually makes sense to me that if you are trying to make one table have references to one of two different other tables, then the information in the first table actually consists of two types of different thing. It should also be considered that it's much easier and more readable to do a , than to have lots of queries of the form: 

If for some row the JSON in the comment column is an object, but which does not have any such key, you simply get back a NULL. However, if there is even one scalar in the table, the query fails with the error 

Forget GROUP BY, this is much more effective with analytic functions. Use outer join if needed (employees without departments, departments without employees). 

After this, you still have to remove the entry that belongs to the database from /etc/oratab, remove init.ora/spfile, password file from $ORACLE_HOME/dbs, and clean log directories (adump, bdump, cdump, udump). 

A foreign key requires a unique or primary key constraint on the referenced columns in the parent table. A multi-column constraint is not interchangable with single-column constraints. 

Another method would be to query (joined with ) an check whether the statistic increases. You can run any CPU-bound query to test this, for example the below query uses nothing else but CPU. Start it, and you will we see increasing, while your session is not waiting, but using CPU. 

This is really easy to do in an instant without copying anything or requiring any additional space, and you do not need to use UNION or rewrite anything at all, because you can do this with 1 table. Let's say your table is this: 

Make sure you do not reduce the size of LV below the ASM disk size . Once it is done, increase the other LV: 

TABLE ACCESS FULL and INDEX FAST FULL SCAN use multi-block I/O, other kind of access paths use single-block I/O. 

If you have a fixed format of your XML, you can describe it in XSLT and use XMLTRANSFORM to take care of the rest. 

This is a pain, as we have had strings written to this column some of the time, mainly due to errors. Is there any reasonable way of either filtering these out in a query, or identifying them all so that they can be removed in one go? 

that one day some other part number such as 9 would be acceptable. You can quickly and transparently add it to the table, rather than changing a bunch of constraints on different tables. that you might want to store some extra data alongside each part number, for example the name of the part. You can add a column to the table which you already have. 

@CoderAbsolute's answer gives you a good design for your tables. Since he or she did not go into detail about why this approach is better, I thought it was worth adding another answer. First of all, design your table structure in accordance with how your data fits together. Don't try to smoosh different types of things into one table, don't add several tables for the same kind of records. Try to 'normalize' - if a table will have the same info repeated many times, then move that info into a different table, and link it from the first table using a foreign key. You should be aware of what first normal form, second normal form and third normal form are. Many real-world databases do not match these standards completely, but being aware of what you should aim for will help you make a much cleaner design. I would say, don't worry about optimization until you've already got a correct design. First of all, you don't yet know how many entries you will have in your tables. Secondly, database engines are designed to make queries as fast as possible, even if you have a lot of entries. Don't second guess the developers of your database software. When you've figured out that you really do have a bottleneck, you should look first at indexing. We think of a database table as being something like an array. In reality, it could be stored as a bunch of 'blobs', one for each row, which might all have different locations on a disk, not necessarily in order. (This is not a super accurate technical description of DB internals. But it should give you a clearer picture of how the pieces fit together.) How does the database find all the blobs? It has a somewhere, a list of 'pointers' which tell it where to find each blob. So typically, finding every single row of a table is an efficient process. It goes through the list and records each one. Now, suppose that you most commonly retrieve all the photos for a given user. This might be a slow process, since it has to go through every single row of the table, and look at the field. In this case, you should add an Index on that field. An index is something like a lookup table, which allows the software to quickly find the location of all the rows with a given . So it doesn't have to check every row in turn. There are different kinds of indexes. Some are optimized for matching a particular value. This is probably the type you want for the column. Some are optimized for finding things greater or smaller than some value. This might make sense for timestamps. (Give me all the photos from the last month.) You can have an index on multiple columns if this is what you regularly query on, or even on some function of one or more columns. Some indexes mean that similar items are stored close to each other on disk, to take advantage of caching to retrieve them more quickly. You should familiarize yourself with the possibilities for your DBMS. Experimentation can also be very valuable here, since the exact speedups depend on your settings and also your hardware configuration.