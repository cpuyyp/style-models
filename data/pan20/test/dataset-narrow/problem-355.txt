You cannot safely determine exactly what will be lost. It depends on how badly the page is torn. What can you do? The usual advice is to copy out all of the data from the table in question (quite a big copy according to your numbers) and store the data elsewhere outside of the database. Use BCP, SSIS, or the Scripting Wizard (But not BACKUP and RESTORE) to get the data to a file on disk. Then: 

Since you are using SQL Server 2014, once you have plain text files you should be able to import them in a number of ways: , , and other tools are available to load data. Depending on your approach, you might choose to use a staging table to further prepare the results before moving the data into the destination table. And if you are also doing document versioning you will likely need to create some metadata that allows you to track different versions. 

I am not a Firebird user, but I looked up the GROUP BY / HAVING syntax. [EDIT]Exclude from the result set customers that have a transaction older than one year. OK, the here is other take on aggregating the rows to eliminate customer from the select. 

Therefore, since Service Accounts are not people, you can use as many service accounts as you can make use of: SQL Server, SQL Agent, Full Text, and so on. 

You could create a domain group that you could use to create as login and user on your SQL Server and YourDatabase. After creating the domain group, you could do something like: 

Since SQL Server 2008 this has been supported by the two views mentioned above. If the overhead of collecting the needed words is not too expensive, it does open up new possibilities for Full Text searching. 

We have a main analytics SP that calls 100 other sub SPs, all of which act upon the same set of data (Claims), check it for some business rules, and output discrepancies. The SPs need not all be run sequentially, they can be broken out into sections, which are dependent on previous sections (sequential), but independent within the section (parallel). After looking at many options like Service Broker, Agent Jobs, Batch Files, SSIS etc., I used this CLR code to parallelize the sections, and it gave great performance improvement. However, when I run multiple (5, 10, 15) main SPs concurrently (each of which analyzes different claims) , performance starts to taper as concurrency increases. I guess this is because of the overhead of creating multiple parallel threads through the CLR. I also see lot of XTP_THREAD_POOL sessions idle in sp_who2. Has anyone used CLR for parallelizing Stored Procedures in Critical OLTP Production workloads ? Are there any best practices for performance tuning SQL CLR ? Is there a threshold for number of parallel threads that can be opened before the overhead makes things worse ? If my system has 20 cores, does it mean creating > 20 parallel threads does not help ? 

Which indexes would be considered under-performing, or which ones could be modified for better performance? 

I need to setup a sql audit to capture all queries running on a server from any unauthorized locations, and one of the columns that is required is Row Count. In sql trace, this is possible. But I can't seem to find corresponding column in extended events. I looked into these 2 DMVs: 

SQL Server Processor NUMA config - all 16 processors are on 1 NUMA node - But I thought since the host has 2 sockets, there would be 2 NUMA nodes ? vCenter Socket/Core config - 16 sockets with 1 core per socket - Is it better to change this to 2 sockets with 8 cores per socket since that matched underlying hardware ? I recently heard at PASS conference, that if an entire VM is being dedicated to a single host, then leave Hyper-Threading disabled. Anybody agree/disagree ? As a basic pen-paper calculation, we need to double our 'compute capacity' from 50 (16 cores x 3.2Ghz) to ~100 GHz. Is it better to go for higher core-count across 2 sockets: E5-4669 : 2 socket x 22 cores x 2.2 Ghz = 96.8 Ghz or higher clock speed across 4 sockets: E5-4627 : 4 socket x 10 cores x 2.6 Ghz = 104 Ghz 

Of course, the correct answer depends on your data, the cardinality of each column, and which index columns give the best result. 

Whenever you create or drop a you are changing the definition of your database landscape. So, yes I would consider that risky unless you have tight controls on when those steps can run. If a connection resets the it is changing that synonym for the Server and database not for the connection. This means that a set of "complex work" running in another process could wind up switching from your remote data (for example) to the local data without any warning. That would leave a mess to clean up. View this like you would view dropping and recreating tables in a running system. It may often work without anyone knowing, but it can indeed cause you problems. Of course, if another connection tries to define a that already exists it will raise an error such as: "There is already an object named 'SY_SUBJECTS' in the database." This will save you from switching context (but you must deal with the error) until after the synonym is dropped. (There is no function, which would apparently be much more dangerous.) Therefore if you try to change the without dropping it first, it will fail. If a code path runs it will succeed once it can acquire the needed locks. From MSDN: "References to synonyms are not schema-bound; therefore, you can drop a synonym at any time." So, the will not stop a running transaction, but will wait for the transaction to finish. 

If your goal is simply to create a Flat File have you considered using BCP.EXE to Bulk Copy your data out. Bulk Import and Export of Data (SQL Server) gives a link to the bcp utility. You do need to give directions on how to interpret the data. Here at Create a Format File (SQL Server) you can examine the two Format File type used to define the data format. (Naturally.) The format file that I use is a Non-XML Format File. This is just a text file and is fairly easy to type and make changes within. For me, this is a quick way to export a flat file. (Of course, if you read further down the page you will see an XML Format File, which might be more your style.) It is also possible to use BCP to copy data back into a SQL Server and it is pretty fast. (However BULK INSERT tends to be a bit faster for importing data.) So BCP is fast, fairly easy, and comes in two flavors. 

There are thousands of counters, articles, products to help monitor these. But is there a simple, instant and accurate script that can pinpoint if any of these 4 need to be scaled up or scaled out ? e.g. sys.dm_os_wait_stats - SOS_SCHEDULER_YIELD has high signal waits => need either more or faster CPU. PAGEIO_LATCH => need more files or faster DISK Are these 2 accurate? Does Page Life Expectancy accurately 'PROVE' the need for more memory ? What is your GO-TO script for diagnosing a performance issue ? I've used sp_whoisactive, sp_blitz, Glenn's DMVs, Spotlight, Idera etc. but am yet to come across a script that will satisfy a CIO's question about where to spend budget money, or that will correctly blame the problems on bad code, or slow SAN, or the ISP. Everytime any (Network/Systems/DBA/App) engineer points fingers at the other team, we have to 'PROVE' our statement, and with Virtualization and Cloud, without ideal test environments, without downtime, it's getting increasingly difficult to provably pinpoint the source of server performance issues, other than maybe Task Manager <excuse the rant> 

What does "parallel plan running in serial" mean ? I see a lot of 0x01000000, and a few 0x08000000's in my trace. How can i determine whether one query is hogging CPUs and if reducing it to 4 will help ? 

We frequently see blocking on our SSRS box which houses both front-end Report Manager ReportingService and the SQL database engine with ReportServer & ReportServerTempDB Catalog databases 

Is vSphere Replication safe for SQL databases? We are a virtual environment (Cisco hardware + VMware hosts + HP san). We have a primary and disaster recovery datacenter, with SAN-level replication already setup for every 4-hours. There are several HA/DR solutions for SQL - alwayson, logshipping, clustering, replication etc. But we want the easiest administrative solution that can deliver good RPO & RTO. So we're looking to replicate at the VM-level instead of SQL-level strategies. Apparently, any VM-level snapshot/backup solution needs to be VSS (Microsoft's volume shadow service) aware to ensure highly transactional applications like Exchange or SQL Server comes back up in a consistent state after restore. VMWare's SRM(Site Recovery Manager) gives you the choice of array based replication or vSphere Replication. I read here that vSphere Replication is VSS aware and can offer utmost 15 minute RPO, which is very acceptable for most of our applications. Does anyone else use this for enterprise oltp applications with 15-minute DR RPO (not high-availability) ? Does it freeze IO on SQL server for unusable period of time ? Does it definitely ensure consistency when VM turns back on at the DR site ? 

EDIT: My comment on Excel formatting is simply that you also have to control the Excel columns. If the Excel spreadsheet is all defined as text column, the results are predictable. In the three columns below I typed the same strings in and columns, but see how differently they are displayed. Then I copied the values into column and got the values, but justified as . 

I personally would use a tool (as indeed I already use) , such as are sold by Apex, Red Gate, Toad, and others. These could be used to script only the rows that have changed. That would make your looking for changes very simple indeed. If you are looking for a free tool you might examine $URL$ I have not personally used it, but some people find it useful. (There are doubtless other such tools in the public domain.) 

There is no SQL Server 2008 R2 Enterprise Manager. The SQL Server Management Studio plays the same role as Enterprise Manager once played in earlier versions of SQL Server. It appears that you can download SQL Server Management Studio (SSMS) at: $URL$ 

For merge replication issues, first stop is to read Chris Skorlinski's blog post at $URL$ He links you to several resources at the top of his blog post. The post is from 2010, but merge replication has not had any significant update as far as I know. Chris's bottom line is that there are too many variables: complexity of queries, power of the server, concurrent processes, et cetera, to be able to provide a recommended configuration. For example: "I (have) seen simple designs with little or no filtering supporting 1000 of users to very complex filtering/join design barely able to hand 10 subscribers." One link that he particularly recommends is: $URL$ Rob's focus is on merging replication to a SQL Server for later shipping to SQL Compact. But focus on the issues of merge replication and he has several suggestions, such as: 

We run on Cisco UCS + VMware ESX + HP 3PAR. Host blade config : UCSB-B200-M4 , Xeon E5-2667 v3 3.1GHz , 2 sockets , 8 cores each, Hyper-Threading Active So total 16 physical cores, or 32 logical cores. We have Software Assurance with M$, so all are Enterprise editions, and lot of additional SQL core licenses paid for, so money is not an issue. Our primary single OLTP SQL VM is 'dedicated' to one of the hosts, i.e. no other VMs are allowed to run on it, cos it requires all 16 cores of power. Even with that, CPU regularly runs ~60-80% , so we're planning to upgrade hardware. Questions below : $URL$ 

There is a discussion going on in our company on what the ideal CPU count and Max Degree of Parallelism are for a 3rd party database server. The server has 12 CPUs, 32GB RAM and all database sizes add up to < 30GB so they can all fit in memory (I tried to force this by doing a select * from every table). On certain payroll days, the CPU gets maxed out to 100% for a few seconds. MAXDOP was originally set to the default 0. We later changed it to 8 based on several 'best-practices' articles. However the vendor suggests to change it to 1 (no parallelism), while others suggest changing it to 4, so that one run-away query doesn't hog most of the CPUs. I'd like to find out how many CPUs are actually being used by queries. There is a Degree of Parallelism event in Profiler. The BinaryData column says : 

We know that the survivor/victim are SQL sessions (SPID), NOT SQL statements. We also know that the UPDATE statements shown in Frame1 of both sessions are involved in the Deadlock. And How are they involved ? They are both REQUESTORS of the U lock on the Index Key. Am I correct so far.. But which statements are the OWNERS ? And when did they start holding the X lock on the Index Key? One of the standard recommendations for resolving/reducing deadlocks is to Shorten Transactions. But if there are a hundred similar statements, without knowing WHICH statement started holding the lock and WHEN, it's not easy to go about shortening a transaction.. 

Kin made a couple of suggestions which should be helpful. First: Make sure that your statistics for the CallTime index are up to date. Your plan shows that the time filter is being made by seeks to the clustered index. So, for some reason the CallTime index is not being used. What is the definition of that index? If it is a multicolumn index, be sure that the most specific column is first. Example: 

If you set these appropriately it should constain the memory usage from the cache. Here is a SQL Anywhere 12.0 page with those settings: $URL$ 

Since all the details on backups are maintained in the msdb database, you should just extract the backup file name from the source server. You could create a linked server from your Dev Server to access the Production Server's msdb database. Or you can use OPENQUERY to query the same data. (OPENQUERY may be faster since the query is actually being run on the Production Server.) For example: 

When a database backup is restored it almost immediately allocates the space needed for the database files. So a 300 GB database will quickly appear in the file system at full size, but the file will be essentially empty to start with. Once the space is allocated, the restore process goes on to read the backup file restoring data at the speed sustainable by the network and the servers. Once the backup is complete, restoring all the backup files, handling roll backs and roll forwards, then the database will be available. A few years ago we had a very big database with the backup files stores remotely that was going to take 12 hours to restore over the network. So, in that case we cancelled the restore. Then we copied the files locally (about 1 hour) and restored quite quickly after that. Note that if you database backups are compressed, the backup file will restore faster since there is less network traffic.