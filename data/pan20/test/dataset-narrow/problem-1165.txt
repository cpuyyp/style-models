Though we've chatted about this in person before, I'll add this in the hope that this will allow someone else to provide a complete answer. In your process of adding vertices, define a partial function $f:V(G) \rightharpoonup V(G)$ from each vertex $v$ which gets used, to the vertex which was added when $v$ got used. Then it turns out that $f$ is a (causal) flow function (p. 39), which is a restricted version of a path cover. Indeed, your description of these graphs of "complexity k" (given a set of vertices which are to be the initially unused vertices, and the final unused vertices) is precisely the star decomposition of a "geometry" with a causal flow (p. 46 of the above article). Although these "causal flows" have been studied mainly in the context of (measurement-based) quantum computation — where they are motivated by certain structures of unitary circuits — there are graph-theoretic results about them which are totally divorced from quantum computation: 

These computations can be efficiently simulated by classical computers (provided that the coefficients are efficiently computable, etc.) However, if you consider circuits which have these gates, and also SWAP gates, they become universal for quantum computation. The same holds if you allow the gates to act even on next-nearest neighbour qubits, or allow Hadamard operations on other lines than just the first. What appear to be trivial modifications of the model, which one could imagine performing by just moving qubits around, transform a model which is tractible to simulate to one which (we think) is intractible to simulate. Nor, obviously, can we decompose SWAP into gates which are allowed. In summary From a theoretical perspective, SWAP operations are exactly as boring as you think they should be, for much of the research in quantum computation. But a model of computation is a mathematical model of something we hope to achieve in hardware: the model is not necessarily representative of reality. And even if SWAP is boring in one model of computation, that doesn't mean it is boring in another model! We are complacent about SWAP in classical computation because electronic computers are a mature technology in which these details are mostly safe to ignore. As soon as you switch models of computation, you must check your computational assumptions. 

Using AND, OR, and NOT gates, we can very easily simulate equality of the input with a binary representation of any integer $0 \leqslant x < 2^n$ with a circuit of size $n$ and depth $\log (n)$. Next, we may represent your input sets as also being constructed by $\cup$ circuits, constructed from singletons $\{x\}$ for $1 \leqslant x < N$. If $S_j$ has size $m_j \leqslant M$, this requires a circuit with a boolean tree structure, with $m-1$ union gates, having depth at most $\log(m)$. Finally, we may represent unions and intersections of your sets $S_j$ by logical ANDs and ORs of membership in the input sets. This part of the circuit has the same size $s$ and depth $d$ as the depth of your integer-set-circuit. 

If you want to give a whimsical look into the past, remind your audience that "computer" used to refer to a person whose profession was to compute things. (And if you want to violate some gender stereotypes that they may have, you can point out that these were frequently women, as well.) You can then achieve a handful of things at once: 

A correction regarding DISCRETE LOG. The results above rely on standard techniques to represent algebraic coefficients, in a way which is independent of the input (but which may depend on input size). In the description of the original question, I claimed that [ Mosca+Zalka 2003 ] show that DISCRETE LOG is exactly solvable by a gate-set with efficiently computable coefficients. The truth appears to be more complicated. If one cares about exact solvability, then I consider exact representation of the coefficients to be important: but Mosca and Zalka do not provide a way of doing this in an input-dependent way. So it is not obvious that DISCRETE LOG is in fact in $\mathsf{EQP}$ or in the new class $\mathsf{UnitaryP_{\mathbb C}}$. Reference. 

If s is in , skip to step #5. Initialize an empty list , an iterator j ← f(s), and a counter t ← 1. While j is not in : — If t ∈ G, insert j into both and . — Increment t and set j ← f(j). Check to see if j is in . If not, we have encountered a previously explored component: we check which component j belongs to, and insert all of the elements of into the appropriate element of to augment it. Otherwise, we have re-encountered an element from the current orbit, meaning that we have traversed a cycle at least once without encountering any representatives of previously-discovered cycles: we insert , as a collection of samples from a newly found component, into . Proceed to the next element s ∈ S. 

By the Remainder Theorem, any solution to a system of equations modulo each of the prime powers $p_{\!j}^{e_j}$ dividing k gives rise to a solution to the same system, mod k. So if solving systems of linear equations over $p_{\!j}^{t_j}$ is contained in coModpj L, it follows that solving systems of equations mod k is contained in coModkL. There's a standard algorithm, described by McKenzie and Cook for reducing linear congruences modulo a prime power to constructing a spanning set for its nullspace (namely, for Ax = y over a given ring, construct a basis for the nullspace of [ A | y ] and see if any solutions exist with a final coefficient of −1); and subsequently for reducing the construction of nullspaces modulo prime powers to constructing nullspaces modulo primes, and matrix multiplication modulo prime powers. Both of the latter tasks are problems which are feasible for coModkL, provided that you can construct the matrices involved. It turns out that the matrices involved in the reduction of McKenzie and Cook can themselves be computed by matrix multiplication, and (crucially) division by a constant factor. Fortunately, for prime powers, the coefficients of the matrices involved can be computed on the work-tape using an oracle for coModpL-machines; and the division by a constant can be performed in NC1, which is again feasible in coModpL. So it turns out that the whole problem is ultimately doable in coModkL. For complete details, see [arxiv:1202.3949]. 

We may then obtain a boolean circuit $C_n$ of depth at most $d + \log m + \log n$ and size at most $s + m + mn$. The complexity of building this circuit is essentially $O(s + m(\log m + n \log n))$, arising from doing the necessary construction of the binary trees for the gates representing each $S_j$ and $\{x\}$ for $x \in S_j$. Note that the complexity does not increase at all if you allow logical complementation, which would represent in principle an infinitely large set including numbers larger than the input size; though to represent numbers which are $N$ or larger, we must use uniform circuit families $\{C_n\}$ rather than a single circuit $C_n$. The construction above will build these efficiently, however. Remarks on the power of integer-set circuits with arithmetic In the (extremely powerful!) model of arithmetic circuits including $+$ and $\times$ gates, this construction fails because the outputs to those gates implicitly involve existential quantifiers over satisfying inputs to subcircuits of arbitrary size. As your linked article notes, the Goldbach conjecture can be tested by evaluating $G(0)$, where $G$ is a (finite depth!) circuit, $$ G :=\overline{\{0\} \times \Bigl(\bigl[ \{2\} \times \GE2 \bigr] \cap \bigl[\; \overline{\PRIME + \PRIME}\;\bigr]\Bigr)} \;, $$ where $\GE2 := \overline{\{0\} \cup \{1\}}$ tests whether an input is $\geqslant 2$, and $\PRIME := \GE2 \cap \bigl[\,\overline{\GE2 \times \GE2}\,\bigr]$ tests whether an input is prime. The enormous power of these circuits seem to me to come from combining negation (which allows you to obtain infinite sets from finite ones) with the $+$ and $\times$ gates (which allow existential quantification over all inputs to sub-circuits, as to whether there are accepted inputs whose sum or product is equal to the input integer). If you do not have additions or multiplications, those quantifications are absent, and the problem becomes much simpler — indeed, very close in flavour to a run-of-the-mill boolean circuit. 

If all that you want to do is count the number of cycles, you can do this with 2|S| bits (plus change) worth of space. It seems unlikely that you will be able to do much better unless S or f have some particularly convenient properties. Start with an array A storing integers {0,1,2} — one per element of S — initialized to zero; we will indicate these as , , and . Initialize a cycle counter to zero. For each element s ∈ S in order, do the following: 

Short answer. The operator version of SAT is efficiently solvable — at least, if we assume arbitrary circuits of two-input gates with no fan-out, over any desired choice of gate-set. Long answer. I assume the following form of the boolean problem: 

Summary. All of these papers misunderstand the notion of quantum superpositions and interference, and lead to analyses which do not conserve probability (i.e. in which the probabilities of outcomes do not add to one) without specifying an interpretation for this fact. This may be considered to correspond to post-selection — conditioning on a particular outcome of a measurement; a subject which has indeed been studied in the context of quantum computing, and which is embodied as the complexity class PostBQP. But mostly, I suppose that these articles are ignored because while they propose to extend quantum computation, they seem to display a lack of awareness of fundamentals of quantum computation. Details. I've added some elaboration on the failures of the analyses of these articles. 

A preliminary guess at what it is you're looking for I will give a preliminary answer, in the hopes that it might prompt you to elaborate on what promises to be quite a good question. I will assume you're referring to an idea along the lines of page 4 of [Arora+Impagliazzo+Vazirani 1993], which introduces the concept of "local checkability" as follows: 

Juan Bermejo Vega has given an accurate summary of what is said in the original paper. I will give you a higher-level description. I will recommend, in your case, to avoid thinking of the amplitudes as probabilities altogether. They are related to probabilities, but this is not especially helpful to think about for these finite automata. I suspect that things will be much clearer if you think of this as a slightly abstract recipe for transforming complex-valued vectors instead. What vectors are we transforming? Well: suppose you have a finite automaton with n states. The automaton represents a (yes, probabilistic) model for transforming vectors, which at any time step may give rise to an accepting or rejecting decision. 

It does matter, because there is more at stake than whether or not we can find solutions. Also of interest is whether we can verify solutions. Other qualitiative distinctions can be made between the difficulty of problems, but for NP versus larger complexity classes, this would be the one I would identify as most important. For decision problems — problems for which every instance has a 'YES' or 'NO' answer — NP is precisely the class of problems for which we can efficiently verify a purported proof that a given instance is a 'YES' instance, deterministically, if we are presented with one. For example, if you have a satisfying assignment of variables for an instance of 3-SAT, that assignment allows you to efficiently prove that the instance is satisfiable. Such a satisfying assignment may be hard to find, but once you have one, you can easily prove to others that the instance is satisfiable just by having them verify the solution you have found. Similarly, for coNP, there exist efficiently checkable proofs for 'NO' instances; and for problems in NP ∩ coNP, you can do both. But for PSPACE-complete problems, no such procedures exist — unless you can prove some quite spectacular equalities of complexity classes. 

We may then assign to any $k$-qubit gate $G$ the cost $d^2\ell k$, essentially accounting for any work being performed by recombination of the coefficients. In the case of the extension $\mathbb E{:}\mathbb Q$ having finite degree, all gates would then have cost $O(1)$, reducing to the usual asymptotic analysis of computations with finite gate sets. This seems to me to be a perfectly fair way to assign costs to constructions of exotic gates. The Bottom Line If you allow gates which depend on the input size $n$, but