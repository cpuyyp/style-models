I would normalise the values between 0-1 (). Regarding the size of the files I don't think this should be a problem. For example, Cifar-10 are 32x32 or the average image resolution on ImageNet is 469x387 pixels although most approaches resize them to 256x256. I think your size should be fine. 

As you mentioned, the paper doesn't clarify. However, my guess is that this is not due to concatenating 2 previous layers (I don't really see an specific reason to do this here) but because of concatenating the ResNet shorcut. Generally, all conv layers have a number of filters, thus determining the output size (num_filters, size) regardless the inputs. On the other hand, MaxPooling does keep the input num_filters (though in this case reducing the size). In the paper, note that the num_filters is doubled at the output of all convlayer except for the one that does not keep the ResNet shorcut (last 512 conv layer). So my guess is that they are concatenating the output of the conv layer and the shorcut which would explain the output size. Hope this helps! 

As other people pointed, there is no clear superior approach. As commonly happens in ML, more complex approaches are somehow more powerful than simpler ones (they can model more complex relationships) but this always comes at a cost. In the specific example of GRUs and LSTMs is exactly the same, LSTM have a more complex structure so it has the ability to model more temporal related features. GRUs are simpler (see picture below), so they will perhaps outperform LSTMs when data is scarce or when there is a high risk of overfitting. Have a look at this paper with a nice empirical evaluation of this two approaches. If you go to the conclusion you will read: 

Obviously, you will lose the ability of predict the first characters but hopefully your dataset will include a lot more examples using those specific characters so it will learn how to use them eventually. I would say this is the most general way of producing char to char text generation models. A couple of examples including implementations: here and here. 

I am trying to do some analysis about user behaviour when typing (keystroke biometrics). Ideally, it will include traits extracted when people are writing code. Although not technically Natural Language, code also has some structured characteristics as language and I wanted to leverage that. I was wondering if there has been some research about performing language analysis focusing on programming languages instead of traditional spoken languages. Mainly, I am interested in having a comprehensive list of stopwords for as many languages as possible. For example, stopwords will include: for, while, return, break, string, if, else, and so on. Although it would be nice to have them separated by languages, I wouldn't mind a list comprising several languages. I know this could be done for example by getting some sample code and retrieving the most frequent terms, but I also wanted to know if there has been some research towards this direction. Any ideas, papers, methods would be welcome. Thanks! 

Some other signals like company url exists, but in terms of name matching wondering if text similarity is a good approach for this grouping problem. 

Since questions and answers contain HTML, URLs, special characters, quotes, single quotes, commas, etc in body property. I'm looking to convert this text into structured data which can be represented as a single text chunk and be analyzed as tokens and then I use TF-IDF. Details: 2 Objects: Question and Answers Question object has an array of answers objects as properties. Each Question and Answer objects contain a body property which is a single string containing each text. What's the recommended way to store this information? 

I provide an offline library of Music to my users. My goal is to understand what my users are looking for, which means translate raw user searches to: Music Artists, Songs, Albums and then add music to the company library. What are the suggested clustering algorithms to group common short sentences into a single entity. Example: 

Currently I have a list of Books which I need to compare to a different list. For example in my local list I have: 

I'm currently using Levenshtein distance, based on that I can find out if Book is in remote system or not. In this case if Book1 vs Book2 result score exceeds X, I assume is the same Book. 

I'm downloading Stackoverflow questions & answers for a specific tag using Stackoverflow API and Python. The goal is to perform document clustering to find relevant terms across the documents and find similarity among them. Example: 

I have 3 different datasets with company information, in all of them I have company name, but is not perfect: For example: 

When I want to do a prediction and I pass the original record which looks like this: (Header just for reference) 

I read about Word Embeddings and before I start coding want to check if converting my text information to embeddings may improve my results. 

I tried this example and works fine for a specific number of clusters (K) where K < N | K <= N. But since searches are unpredictable need to find a way to automate the number of clusters: my goal is to cluster 2 or more similar items and let alone single searches in independent clusters, example: Cluster 1: 

Not all the features present in training data/test data will be present when I will be making predictions. Is this normal in ML ? What is the rule of thumb when doing feature engineering for this type of cases. 

While Goodreads provide genre, I would like to use synopsis and use the text for this. Lets say I will get N books synopsis like this: 

I'm able to train my model and test data. When I generate my training dataset where categorical features were encoded. (Using ), this numerical data is different than my predictions. Example: 

I'm building a function to pass training information for my Tensorflow model. It is similar to Keras: 

From the same question you mentioned, you will find an interesting link to Keras documentation. If you navigate to Multi-input and multi-output models you'll find an example code that does exactly this. 

The answer depends on what you want to do with the hashtags/words and also on what tokenizer you are using. Consider this example tweet: 

Apart the mentioned resources, this also might be of help: MLP Java example. It's from the University of Sydney and includes theory and a Java implementation. 

Have a look at this question. There is a nice discussion about how to implement this. The idea in to create a separated Input to your model and concatenate it AFTER the recurrent layer(s). Also in the Keras documentation, there is an example on how to build such models with a few lines of code. 

Similar discrepancies can be found in terms such as or for instance. Additionally, you need to consider what is a token for your task at hand. In my previous example make sense either with or without . However without the hash is just a poorly written word. Maybe you want to keep the hashtags in your corpus or maybe not, but this depends on what you want to do with it. So first you need to know how your tokenizer handles these cases and decide if you want to remove them beforehand or later to keep hashtags in your corpus or only (sometimes weird) words. In the case of the is exactly the same, you can keep it, remove it or maybe delete the whole instance as you don't want to keep user names in your corpus. As I said, it all depends on your task. PS: In case you want to play around with different tokenizers, try this. 

Your first example is basically not a sequential model. You have an input and an output and that's it. You don't need a recurrent layer for that... What I would suggest you do is: 

Basically in new column, you check if and assign -1, if not you check if and assign 1; otherwise you assign 0. 

Here is where I actually think we have some problems. First, your dataset is quite small. 3816 reduced to 1908 when splitting train/test... This is not good especially with such structure. In the paper you mentioned, they are using a network with around 2.5M parameters and they used ~137K samples. Your 1908 dataset seems tiny compared and you are using the same model structure... In my opinion your model is not able to do better, simply put. It doesn't matter what parameters you choose you need (a lot) more data. You might try reducing the size of your network, also create more data using noise addition, mirror samples, etc... and see if these help somehow. Finally, apart from the size, you have no way to tell if your data is representative enough so that any model can learn from it. Therefore, 35% accuracy is as good as any other value I am afraid. Anyway, your mission seemed to be able to complete your experiment and you did. And you learnt a lot about limitations of deep models, so I'd say: good work! 

Im new to ML. I'm trying to predict if a new Music Album will exceed X amount of dollars in Sales. I'm looking to build a model to go only after potential best sellers. I do have historic data for Music Sales from 2010 till 2016. I have many signals: 

Sundar Pichai is correctly identified as a person, but also the word users. How can I differentiate real names vs words which refer to persons? I have seen that for popular people there is metadata like Knowledge Graph mid or Wikipedia articles, but for others there is no reference, (Example: from "Susan Fowler" recent Uber scandal) Any ideas/pointers will be greatly appreciated. 

I need to do pre-processing, My prediction data contains as hence it ends up with 0, instead of 10. How can I pass data to my predictions that needs to be transformed to categorical values. If I tried to convert it to categorical I end up with a different class value. User-agent is just an example, but could be IP address or called number which corresponds to very large finite set hence a dictionary does not scale. I think this is a common problem, but not sure how can I solve it. I tried using dummies Approach#3 and I end up generating additional columns which doesn't match my prediction dataset. Issue reported here. Complete code here. 

In case of Toll fraud and client being insecure, the attacker can send calls via original agent hence the Service Provider won't reject calls immediately at least based from IP information. (Toll fraud). I'm exploring which approach is the best to implement an ML model, instead of static rules to be able to detect Toll fraud in a live system: 

return ~0.5 similarity. hardwired metallica and metallica hardwire return ~0.433 Other docs with more words return higher values. (Im using cosine_similarity from sklearn.metrics.pairwise) I iterate over each document and get the similarity among all docs, after that I extract the highest values. (cosine similarity > 0.55) So far is working fine but there are cases in which I can't find similar sentences unless I reduce my coefficient, doing so it may associate other values to non-related items. I want to know what is the best technique to group common sentences from a list of sentences. Not sure if that would be semantic similarity. 

I'm using a rich dataset of Movies and I currently need to group if a Movie is the same across different Retailers. Example: Movie: Beauty and the Beast Platforms: Google, Netflix, iTunes, Amazon. I have access to signals like: Studio, Movie Name, Runtime, Language, Release Year, etc. But in the case some Movies which are not the same and signals mentioned before are not capable to find right match I need to do what a human would do: Check Movie cover. Example: 

Will be potential high revenues. I found house prices example: $URL$ is it the same type of problem? I'm looking which input signals may be the highest revenue. Any insights or pointers will be helpful.