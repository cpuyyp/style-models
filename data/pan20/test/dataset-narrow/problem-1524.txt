I think you're on the right track. Yes, you will need to store the derivatives of the activations and of the parameters for backpropagation. Additionally, your choice of optimization may matter. Are you training using SGD, or Adam, or Adagrad? These will all have different memory requirements. For example, you're going to have to store the step size cache for a momentum-based method, although that should be secondary compared to the other memory considerations you mention. So all in all, you seem to have calculated the memory requirements for a forward pass. Andrej Karpathy mentions that the backward pass could take up to 3x the memory of the forward pass, so this might be why you see such a difference (scroll down to 'Case Studies' on the website to see an example for VGGNet). 

It looks like things are breaking down when the value of is high. This makes sense, as you say, because LSTMs need normalized data. The parameter is a bit different. When a small amount of drift is included, since is large, the amount of drift is similar to the amount of random noise being added via . In the plots for , it looks like the predictions would be fine except for the y-shift. Because of this, I think the root of the problem is still in the parameter, not the parameter. Differencing, as has been done, will not help with the parameter; instead, it corrects for drift. I can't tell from your code, but it looks like perhaps the was not accounted for in . That's my best guess. 

I would like to look at just one input example, and find the activation and the weights from just that input example. Essentially I'm trying to figure out which features of the data each hidden unit is picking up. Can anyone explain how to get the activations of intermediate layers in Keras? 

The cell state is the bold line travelling west to east across the top. The entire green block is called the 'cell'. The hidden state from the previous time step is treated as part of the input at the current time step. However, it's a little harder to see the dependence between the two without doing a full walkthrough. I'll do that here, to provide another perspective, but heavily influenced by the blog. My notation will be the same, and I'll use images from the blog in my explanation. I like to think of the order of operations a little differently from the way they were presented in the blog. Personally, like starting from the input gate. I'll present that point of view below, but please keep in mind that the blog may very well be the best way to set up an LSTM computationally and this explanation is purely conceptual. Here's what's happening: The input gate 

I will assume by , , etc, you mean convolutional layers, and by , you mean pooling layers, and means fully connected layers. We can calculate the memory required for a forward pass like this: One image If you're working with float32 values, then following the link provided above by @Alexandru Burlacu you have: : 50x50x3 = 7,500 = 7.5K : 50x50x32 = 80,000 = 80K : 25x25x32 = 20,000 = 20K : 25x25x64 = 40,000 = 40K : 12x12x64 = 9,216 = 9.2K <- This is a problem (and my approximation is a very hand-wavy guess here). Instead of working with 50, 25, '12.5', it would make more sense to work with multiples of 32. I've heard working with multiples of 32 is also more efficient from a memory standpoint. The reason this is a bad idea is 2x2 pooling doesn't divide the space properly, as far as I can tell. Feel free to correct me if I'm wrong. : 1x500 = 500 = 0.5K : 1 x 10 = 10 = 0.01K (next to nothing) Total memory: 7.5K + 80K + 20K + 40K + 0.5K = 157.2K * 4 bytes = 628.8 KB That's for one image. Minibatch If you're working with a minibatch size of 64, then you're reading 64 of these into memory at once and performing the operations all together, scaling everything up like this: : 64x50x50x3 = 480,000 = 480K = 0.48M : 64x50x50x32 = 5,120,000 = 5.12M : 64x25x25x32 = 1,280,000 = 1.28M : 64x25x25x64 = 2,560,000 = 2.56M : 64x12x12x64 = 589,824 = 590K = 0.59M : 64x500 = 32,000 = 32K = 0.032M : 1x10x64 = 640 = 0.64K = 0.00064M (we don't care, this is tiny) Total memory: 10M x 4 bytes ~ 40MB (I'm saying approximate because the website also says an approximate value) EDIT: I misread the website, sorry. According to the website, a backward pass requires about triple this, because of the need to store: 

Applying a machine learning algorithm on only a subset of the data and including other subsets later does not allow the algorithm to assess the importance of each attribute equally. For example, say you have a data set called A, which has subsets B and C. Without loss of genearality, if you fit a model ('apply an algorithm') on subset B, and then include subset C later, then you're saying 'given subset B is already in the model, assess the impact of including subset C'. Instead, if you apply the entire algorithm on the entire data set (A), then you're allowing the algorithm to discover which features are most important for the desired outcome. That being said, it may be wise to process the different elements of your data set differently. That is, categorical covariates may be modelled differently from continuous covariates. If you're using something like a feed-forward neural network, then it's not a big deal, but if you're using a more traditional statistical model you may need to take that into account. For example, in R, you need to specify that a categorical covariate is in fact a 'factor' variable. 

From the comments in my previous question, I'm trying to build my own custom weight initializer for an RNN. Based on the code given here (careful - the updated version of Keras uses 'initializers' instead of 'initializations' according to fchollet), I've put together an attempt. 

In the caption for the figure, they say they had "256 trials of a random experiment...optimizing a neural network....At horizontal axis position 8, we consider 256 trials to be 32 experiments of 8 trials each". This implies that at horizontal axis position 1, they would have 1 experiment of 256 trials. My question is: did they train those 256 models and then break up the results into smaller chunks? Like this: Experiment 1: $[\lambda^1, \ldots, \lambda^{256}]$ Experiment 2: $[\lambda^1, \ldots, \lambda^{128}], [\lambda^{129}, \ldots, \lambda^{256}]$ ... with value $\lambda^{129}$ from Experiment 1 equal to value $\lambda^{129}$ in Experiment 2? Or did they do something else? 

Accuracy is not a good indicator of success with imbalanced data. The accepted answer is correct: F1 score is commonly used. Other options include (see here) and (see here), both defined through scikit-learn. If you're using Keras, I would recommend using (note this will not work well if you have a multi-label problem, although there are some workarounds, for example here). 

If I do online learning in a setting where I have a HUGE amount of data, is that faster than doing minibatch learning (even if I optimize my batch size for GPU use, that is, use a multiple of 32 examples per minibatch)? Details: I have 12600 time series examples, each with 24 time steps, and each time step has 972196 binary labels. This is a multilabel problem. Assuming float32 numbers: 

The value '32' in this case is the size of the cell state and the size of the hidden state being sent forward in the network. Please see my answer here for more information. 

It's really rare that you'd show a plot of the probabilities for each example in your set. Are you sure you want to do this? A better presentation might be a confusion matrix. Here's how it works: 1) The columns are the true class labels 2) The rows are the predicted classes 3) Along the right hand side of the plot you can show the probability of correctly assigning to a class (or the classification error, if you prefer). For example, say I have three classes in my dataset. I have 10 examples of each class, so 30 examples total. My classification results from my model are: 8 of the 10 examples in class 1 were correctly labelled; 1 was misclassified to class 2 and 1 was misclassified to class 3. 7 of the 10 examples in class 2 were correctly labelled; 3 were misclassified to class 3. 9 of the 10 examples in class 3 were correctly labelled; 1 was misclassified to class 2. Then my confusion matrix looks like this: $\begin{bmatrix} 0.8 & 0.1 & 0.1 \\ 0.0 & 0.7 & 0.3 \\ 0.0 & 0.1 & 0.9 \end{bmatrix}$ And along the right hand side of my confusion matrix I can include the classification error: 0.2 for class 1; 0.3 for class 2; 0.1 for class 3. Notice the rows must add to 1. 

I think you're looking for a measure of difference between two probability distributions. In that case, if you have two continuous distributions, try measuring the KL divergence (more information is available here). You may also be interested in a chi-squared test, depending on your application (have a look at Glen_b's answer here). I don't think there are any theoretical proofs of correctness. All you can really do is compare your empirical distribution (your instances) to your theoretical distribution (the distribution you know they came from). Sometimes there's going to be a good chance that an instance that came from one distribution looks very much like an instance from another distribution - the instances would not be 'identifiable'. For example, if I'm generating data from a binomial distribution with large n and small p, those instances are going to look like they come from a Poisson distribution (see more detail here). In that case, the instances themselves are not really distinguishable. ** MAJOR EDIT ** So I misunderstood the question, as you can see from the comments below. If you have one distribution that is made up of two underlying distributions, and you want to label them, I think clustering may be the best way to go. Try something like the k-means algorithm. I'm pretty sure that doesn't require any distributional assumptions. 

Yes, you can zero-pad vectors. However, I would strongly recommend you use an LSTM as part of your encoder if you're using sentences as input. An LSTM can take variable-length input - that's part of its charm! The loss function depends entirely on your goal. It sounds like you're trying to create a sequence-to-sequence VAE. I would go about this by putting a softmax function on your output layer, to generate the next most probable word in the sentence. So, I would use as the loss function, and I would use instead of , because you're choosing the next most likely word from a set vocabulary (this is not a binary issue but rather a categorical issue). 

From there I could create Hinton diagrams using this. However, when I try to work with the activations: 

When we create an RNN in Keras, does it learn an initial hidden layer (ie. $h_0$, like a bias term)? I'm having trouble figuring this out, but I have looked here. Can anyone highlight a part in the code that shows an initial hidden layer is being learned? Or is it just randomly initializing the hidden layer? EDIT: The code is here , I'm just having trouble identifying where the hidden layer is initialized and what it is initialized to. 

I have coded this model in Keras for time series prediction (multi-label prediction at the next time step): The training and validation metrics and loss do not change per epoch, which is worrisome (and, I think, a symptom of overfitting), but I'm also concerned about understanding the graphs themselves. Here are the TensorBoard graphs: 

I can speak from a more theoretical point of view, but honestly I haven't had much success with VAEs. 1) How deep should my encoder and decoder network be? Are there any good guidelines? That depends entirely on your dataset. If you have highly nonlinear data, then a deep network should do well. The successive nonlinearities allow you to capture higher-order correlations in the input, which might be good for your situation. Do you have any idea of the number of truly independent dimensions in your dataset? Have you done something like PCA (a linear autoencoder)? That might tell you where to aim your dimensionality of your bottleneck layer, and that might help you determine your layer sizes to get there. 2) Should one use fully connected dense networks, or stacked conv1Dnets? I'd say try both. 3) What activation functions are good choices? Hugo Larochelle once said that you should always start with ReLUs. See if you get a good enough result with them, as they aren't as prone to the problem of exploding/vanishing gradients, which is something you're going to face with time series data. Remember to initalize them with small positive values, though, to avoid dead neurons. [EDIT: actually, vanishing and exploding gradients can still be an issue with ReLUs, it's just mitigated a bit. Also, initialize the bias with small positive values. Andrej Karpathy said so in a lecture] 4) Can we say anything about the 'best' dimensionality of the latent dimension? In some contexts yes, we can. The 'best' dimensionality will be the one that results in the highest lossless compression. That is, the one that compresses your input data the most without losing information when you reconstruct. Good luck finding that optimum, though. 5) Like everyone else I imagine, my loss function is the sum of a reconstruction term, and the KL divergence regularization term. Is there something else one should consider? You could incorporate various types of regularization that will modify your loss function. You could use dropout, for example, or L1 or L2 regularization. EDIT: you could also consider an attention-based model, for example see this discussion. 6) Batch normalization? Currently, on my problem, it doesn't make a blind bit of difference. But it should. Should one always use batch-norm? 'Always' is a strong term. I would say not always, but batch normalization ought to be a good choice. Have another look at your implementation before you give up on it. 7) In the decoder layer, is it better to up-sample before returning to the original input dimension? That's the default presentation - an autoencoder is usually shown as a symmetric construct from the input to the hidden to the reconstruction. I would say experiment with not doing so, but I don't know if one is better than the other.