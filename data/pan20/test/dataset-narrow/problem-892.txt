TL;DR: Edit: You said you tested the forwarding by connecting from a remote IP address. Just to be sure, you are not trying to connect to your WAN interface's IP address from your LAN, correct? NAT Loopback is broken for that revision and anything past 15760. I highly recommend you downgrade your firmware to one of the recommend revisions per the thread I linked unless you have a very compelling reason to not do so. Edit 2: Hmm. I see your downgrade did not work. Sorry to hear that. Unfortunately this is likely a problem with the DD-WRT firmware of which questions about are considered off topic. My reasoning for this conclusion is that your iptables look fine and port forwarding works correctly with your Linksys router). My advice at this point is to either 1) post to the DD-WRT forum (read all their FAQs first), 2) file a bug report or 3) buy a real router. I have had nothing but problems with DD-WRT and would never recommend it anyone. Additionally I have found COTS "routers" to also be similarly unreliable. 

If I understand your question correctly it sounds like you are having an issue with Windows Network Discovery. First things first, can you confirm that you can connect to shared resources such as a file share or print share of the non-listed computers via a UNC path from the laptop and then back from the non-listed computer to the laptop. I'm assuming you are not on a Windows Domain and are using Workgroups. 

I (and my manager) would like to be able to create custom reports in SCCM 2012 SP1 through the Configuration Manager Administrative Console from the comfort of our workstations. Unfortunately, if you click the 'Create Report' button you get the following error: 

@Gerald Combs' suggestion is excellent. Try using a TCP-based method of checking for connectivity. You can verify that TCP/21 is open and receiving connections by using telnet: 

Let's see if my understanding is correct: You have four rooms (I'm picturing something like trailers at a job site), each with their own standalone networks, you want to connect each standalone network together and you cannot pull cable to any of these locations. As a networking professional, I always advocate professionally installed cable runs as a means of network access before any other. Experience has taught me that this is most reliable, maintainable and testable way to build networks. I would really recommend you push back against the no new cabling requirement and see if you can either run all the drops in each room to a central IDF or MDF or at the very least run a few cables to each room as uplinks. Before you consider bridging the networks with wireless is there any possibility you can use Ethernet over Powerline? If the buildings all share the same electrical infrastructure you can bridge the networks this way. If you must bridge each room's network using wireless what you want is multipoint bridge if you can cover all the rooms with a single bridge (distance and line of sight will determine this) or separate point-to-point bridges for each room. 

I take issue with the idea that users are inherently incompetent. I believe the word you are actually looking for is ignorant. They don't know any better and frankly why should they? It's your job to do the technical knowing. Expecting end-users to be technically competent is like expecting drivers to have a PHDs or Master's worth of mechanical engineering knowledge about their car. I find this works pretty well for me. 

I think you are trying to solve your problem with the wrong tools. VMware templates are very similar to traditional operating system images. They exist to really do two things: 1) Provide a consistent known-good base that all of your servers start with and 2) reduce the repetition of administrative tasks required to go from installation media to that known-good state. You build your template to meet the most common configuration across your fleet. If you have so many different needs that your "common denominator" template is so spare that there is a huge gap between the template and the desired state you have to start doing the math to determine whether the effort to maintain multiple templates is worth the effort you saved in terms of the tasks it takes to go from your "common denominator" template to your specialized template. In my experience, maintaining multiple templates or images is often much more effort than the effort required to perform the configuration. As you have discovered maintaining different templates this way is a logarithmic curve and not a linear one - especially in a heterogeneous environment. This is why imaging works so well for desktops but is less useful for servers. Your solution rests in reducing of the effort of the tasks it takes to go from your "common denominator" template to your specialized desired configuration. This is essentially configuration management. In the Windows ecosystem you are looking at tools like GPOs, PowerShell DSC, and SCCM. I'm less familiar with the enterprise tools in the Linux world but something like Puppet or Chef should work. Using templates or images to reduce configuration effort is often a losing game if there is significant diversity in your desired end-state configuration. If your deadset for using templates for this, you are going to need to construct some scripts, likely with PowerCLI, to perform the tasks of snapshotting, modifying and copying the template and marking it with a version or putting it into some kind of versioning system that can handle files of this size. Again, I suspect the effort of solving your problem this way is less than using some kind of configuration management system. 

I've seen this issue manifest itself in a few different ways. It almost always goes back to either old printer drivers or specialized drivers that do not fully support UAC. A couple of general things to check: 

As for your issue where the executable installer of Adobe Reader breaks your Detection Logic I did a little testing in my "lab" (i.e., my workstation) and was able to reproduce your issue. I think all Adobe Reader executable does is unpack and run an MSI installer. 

I'm at a loss. I would really like to be able to setup custom reports without having to get our DBA involved (he is a busy man). Does anyone have any idea what I'm missing? 

Which sounds equivalent to me (I am following up with support to confirm). I think a reasonable explanation at this point is that the Initiator can't complete the connection to the Target because the Group IP Address / Network Portal is on a different subnet. I really want to avoid a cutover and would prefer to run both subnets side-by-side until I can install and configure each Hyper-V host. 

Citrix has a nice pretty picture overview of the storage options that your might find useful (CTX119088). 

Now that best practices recommends that we keep our targeting logic bundled with the Application what we need to do is create an appropriate WQL query Global Condition and then we can evaluate it using the Application's Requirements. Let's start with the WQL query. I used Scriptomatic to just dump everything in the WMI Class which is part of the namespace. I'm reasonably sure that SMS_InstalledSoftware is the best place to run queries against when trying to evaluate whether or not something is installed as Win32_Product is only for Windows Installer installed software. I find the following Firefox related object: 

EDIT: A Microsoft NPS or NAP server is not really an option for my organization at this point due to cost issues. The best way to describe our environment is a centralized location running our core services with two dozen remote sites connected via WAN links of varying speeds and reliability. We have a varying ability and success of exercising positive physical or policy control over these remote sites, hence they are my primary focus for both wireless and eventually wired 802.1x authentication. If we loose a WAN link (which happens not infrequently) I still need clients at remote sites to be able to get network access, thus necessitating a RADIUS server at most of these locations. A request for another dozen Window Servers will be denied. Historically all of our Linux servers and network gear have been maintained as separate from our domain infrastructure. This means things like a split DNS scope with independent DNS services, independent authentication infrastructure and so-on. While I realize they're are some advantages to an domain integrated PKI infrastructure, I would need a good case as to why I should do it or or alternatively why I shouldn't use an independent PKI infrastructure. 

I worked at an organisation of a similar size (we had a /26), that for reasons beyond me, the powers-that-be felt that a finely grained IP allocation scheme was paramount to operational integrity. The gateway had to be .1, the printers had to be between .2 and .12, the servers between .13 and .20 and so on. We even kept documentation on individual hosts. This is a huge pain in the ass. No matter how diligent I was I could never seem to keep any documentation current. It didn't help that we did not have any DNS services, so using this IP allocation scheme documentation was the only "naming" services we had (which in a strange way, made it seem more indispensable than it really was). For a network of your size I would recommend a few things (most of which you have already done): 

For future reference, diagrams are immensely helpful for these kinds of questions - however, I think I understand what you are trying to do. It also is helpful if you explain what you want to do (your goals) along with what you did. If my answer is not relevant, please provide a diagram and information about your Hyper-V Virtual Switch/s and I'll adjust fire for affect. 

Our organization has begun to seriously consider replacing many of our interconnects with wireless backhauls, meshes or generic point-to-point links as the situation warrants. Our current method of determining feasibility (Line of Sight, obstructions, etc) of a potential link is to climb up on a building roof and look through a pair of binoculars. We then buy equipment, set it up and then tune it using the vendor provided alignment software.There has to be a better way to do this - I'm thinking of something like a surveyorâ€™s transit except for wireless. I want to confirm things like a true Line of Sight between antenna locations, Fresnel zone obstruction, distance, potential signal loss (given a frequency and gain), potential network speed. We have potential links that could only be a few hundred yards and some that easily over five miles. The budget is pretty flexible depending on the features we could get - I would be amenable to spending a few thousand dollars. Is there a tool to help us determine this information (and thus the feasibility of some of these links)? Alternatively, how else would get this information? How do you test the feasibility of potential wireless point to point links? 

It's not clear from your question what the actual problem is. You have six domains, one of which requires SSL. You have one dedicated IP address. This should work just fine (see BillThor's answer). Why do you need another SSL-enabled domain? Does Cpanel require SSL? If Cpanel is the only way to configure your host, perhaps your hosting provider should pay for the extra IP and not you? If your hosting provider is requiring you to have one IP address for all your HTTP domains and one IP address for your HTTPS domain, I suggest you contact them and find out why. It is unnecessary, although I can't think of anything "wrong" it, other than you have to pay for another dedicated IP address. 

I'm not sure what you mean by console. If you need command-line access to the Dom0's operating system SSH should be sufficient for both Citrix's XenServer or any DIY Linux/NetBSD implementation. If you need to see the Dom0's hardware's "console", you need something like iLO or another Out-Of-Band management facility (which VNC by definition cannot provide). 

I am reorienting myself to Hyper-V in its Windows Server 2012 R2 incarnation and have spent a fair bit of time reading Microsoft's Designing Your Cloud Infrastructure documentation. I am in the SME space and working with a pretty typical deployment: three to four Dell R710s as Hyper-V hosts, an EqualLogic PS6100 for shared storage, and 1Gb for traditional data and SAN traffic with physically discrete switches for each network. The existing infrastructure is a VMware vSphere implementation but I think it is close to what Microsoft is calling a Non-Converged Data Center Configuration: 1) Separate networks for storage, vMotion/Live Migration, Management and Guest traffic and 2) traditional SAN storage. This sounds like the appropriate approach but I do not want to rule out a design architecture simply because it is new and I do not fully understand it. My only other experience with converged networking is with Cisco UCS and vSphere so to me "converged networking" means the entire networking infrastructure is basically stateless and virtualized. Microsoft's converged networking seems to mean "using Storage Spaces". My goals for this deployment are: 

I have an interesting problem: Our load balancer (a FiberLogic OptiQroute 2140) seems to consistently favor one WAN connection over another (and not one I want it to). We have two WAN connections from different service providers. One is a SDSL connection at 3Mbps (WAN1) and the other is provided by a cable modem (WAN2) at 4Mbps/1Mbps. Our publically available services all sit on a /24 segment on the WAN1 connection. This includes our mail gateways, DNS, access to Exchange's OWA, various sundry websites and a few well-used VPN services. We get quite a bit of inbound traffic on this side of the load balancer. The other side (WAN2) is largely unused for inbound traffic. The load balancer is accordingly configured to weight this side. We want to push as much outgoing traffic out the WAN2 connection as possible. In practice this does not occur. What will happen is that a combination of inbound traffic (the SMTP gateway sending email, contractors using the VPN, remote users accessing OWA, etc.) will consume about 1/3 to 1/2 of the available bandwidth on WAN1. Then 300 or so users all hop on Facebook. Some of this traffic will head out WAN2, but enough of it is assigned to the WAN1 connection that it will saturate the link and then everything runs slow for everybody. Really slow. Meanwhile there's still 2Mbps worth of bandwidth space being unused on the other connection. This happens frequently enough to be more than a nuisance. WAN1: 

I realize your question is in a VMware environment but I think the general question is relevant to other virtualization platforms including Hyper-V. I recently virtualized an aging server that ran a service dependent upon hardware USB-based licensing keys and found that they do not work natively in a Hyper-V environment. The Hyper-V Deployment Guide has this to say: 

Since everything worked before you formatted your computer it is likely that there was some network setting that made this setup all hang together because you are still using the same hardware. As mulaz rightly pointed out the only way to really know what's happening is to sniff the network traffic using a tool like wireshark. Also a quick troll through the documentation for your devices might yield valuable information - look for a section on networking requirements.