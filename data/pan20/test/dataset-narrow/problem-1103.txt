You ask why database aggregations have monoidal structure. Say we want to combine data values $a$ and $b$, but want to keep things general -- these may be integers, strings, floating point numbers, vectors, matrices, probability distributions, sets, or anything else we want to store and manipulate. So we denote the "aggregation" of $a$ and $b$ by $a.b$. The operation $.$ is usually associative, since we don't want the order in which it is applied to affect the result: we want $(a.b).c = a.(b.c)$. So we have a semigroup. Almost always there is some kind of identity, whether it is the number 0 or 1, the empty string, an identity matrix, a uniform distribution, or the empty set, which depends on the operation. So in fact data usually forms a monoid. The practical point about thinking of data as forming a monoid is that it provides a way to discuss operations on different kinds of data using a common algebraic language. This then translates into generic code libraries that can deal with any monoids, by simply passing an appropriate aggregation operation as an argument. Note that many kinds of data do not have inverses, so a group structure is too much to hope for. If you have group structure then some additional ways of manipulating the data become possible, but since neither matrices with multiplication, nor the positive integers with addition have inverses, non-group-structured data is quite common. We don't usually want to just store data, but to run queries over the database. So we need some notion of what to do when a query generates many answers. Often this requires an aggregation operation $+$ (which may be the same as $.$), and which should be compatible with $.$ in the way they combine. So some kind of distributivity is needed. Commutativity of $+$ and sometimes also of $.$ is also often natural. We then have a semiring or a commutative semiring. A semiring model of data aggregation has been around in the constraint satisfaction community for some time. Note that a constraint satisfaction problem instance is a conjunctive query over a particular database of facts, so this is pretty general: most practical queries over data are conjunctive. 

See OEIS sequence A054924 for discussion and references. As far as I know this grows faster than any polynomial. In particular, see the sequence of the largest value of |G(n,m)| for each $n$. 

It is not clear to me how $D_n$ and $D_n'$, or $N_n$ and $N_n'$, are related to each other, or how closely. If all this relates to a well known question in automata theory then a hint or pointer would be appreciated. The same question is also relevant for two-way automata, due to the same reasoning, and I am especially interested in this version. 

I am not familiar with literature that deals with this specific problem. Here are some general comments and pointers. Some relevant keywords are "contagion" (in the context of network models) and "percolation" (in statistical mechanics). These models tend to focus on the behaviour of randomized processes that behave like the one you suggest. However, different models focus on different parameters of interest, so you might have to examine several to find one that best matches the specific one you are interested in. As an example, a lot of post-2008 work has applied contagion models to financial systems, since it has been argued that these models explain well the gridlock experienced at that time by the global financial system. The Ising model is one of the simplest and earliest models that involve interacting parties in a graph, and has an extensive literature. From a purely TCS perspective, computing the partition function of spin systems like the Ising model is equivalent to counting the solutions of a constraint satisfaction problem. The mixing time is also often of interest. 

Many people have tried to find an algebraic language to describe the shape of a graph. This question is essentially the one that motivates structural graph theory. At the heart of this area of discrete mathematics is the study of graph decompositions. Some of the people working in this area are Neil Robertson, Paul Seymour, Robin Thomas, Maria Chudnovsky, Kristina Vušković, and their collaborators, although this list is biased by my own research interests. Particular kinds of graph decompositions have led to some of the most general results in graph theory. For instance, one of the main technical tools developed for the graph minors project, which led to the Robertson-Seymour theorem, is the graph structure theorem. This shows that classes of graphs that exclude some minor can be built up from simpler graphs. In the proof of the Strong Perfect Graph theorem a somewhat different decomposition was used. The key result is: For every Berge graph $G$, either $G$ is basic, or one of $G, \overline{G}$ admits a proper 2-join, or $G$ admits a balanced skew partition. The decompositions studied to date are in some sense non-algebraic. My personal intuition is that there are indications that there is no "nice" system such as the one you seek. Making this glib statement precise would likely require a nontrivial enterprise in finite model theory, but I suspect it could also lead to interesting new results in graph theory (whether successful or not). 

A recent result shows that computing a particular lower bound for the size of the shortest equivalent CNF formula (measured by number of clauses, as you specify) is NP-complete. This paper also states that your problem of minimizing the number of clauses is $\Pi^p_2$-complete as well, citing the Umans paper above, although why this follows is not immediately obvious to me. 

Did the Rabin/Yao paper ever get to be at least a personal communication/draft/sketch in someone else's paper, or is this one of those indications of the "golden era" where giants roamed the earth and didn't always touch ground when stepping from breakthrough to breakthrough? 

A two-way DFA (2DFA) is a deterministic finite-state automaton that is allowed to move back and forth on its read-only input tape, unlike finite-state automata that may only move the input head in one direction. It is well-known that 2DFAs recognize precisely the same class of languages as DFAs, in other words the regular languages. Less well-understood is the question of how efficient the simulation is. The original constructions from the late 1950s by Rabin/Scott and Shepherdson used a notion of crossing sequences and are quite hard to analyse. Moshe Vardi published another construction that shows an upper bound of $2^{O(n^2)}$ states, but this bound may have some slack. I am asking whether any (families of) 2DFAs are known that require many states in any DFA simulating them, even after Myhill-Nerode minimization of the DFA. Moreover, would there be any interesting consequences of knowing such 2DFAs? 

Many problems in computer vision are naturally expressed as constraint satisfaction problems. There is a history going back several decades of applying constraint programming to such problems. One of the key early papers in constraint satisfaction was completely motivated by problems from computer vision. 

In 1986 Fu and Anderson conjectured a relationship between optimisation problems and statistical physics, based on spin glass systems. Although they used sentences like 

When all-different constraints are expressed (using one of several encodings) as SAT instances, then conflict-driven clause learning usually quickly finds a solution if it exists. However, pure resolution for the PHP has to build a superpolynomially large set of clauses to show that the instance is unsatisfiable. This bound clearly holds for this more general problem. On the other hand, recall that Cook's encoding of the PHP allows polynomial-sized extended resolution refutations.