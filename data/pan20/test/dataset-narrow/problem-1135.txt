How hard is this problem? If we choose $a,b,p,m$ randomly but "wisely" will $f$ be a pseudorandom number generator secure against TC0 or AC0 circuits? What is the smallest value of $k$ where $Q$ can be solved? What is the fastest known algorithm for $Q$? 

I was reading Andrej Bauer's paper First Steps in Synthetic Computability Theory. In the conclusion he notes that 

This made me wonder about non-relativizing results in computability. All results I know from computability theory do relativize to computation with oracles. Are there results in computability theory that do not relativize? I.e. results which hold for computability but do not hold for computability relative to some oracle? By result I mean a known theorem in computability theory, not some cooked up statement. If the notion of relativization doesn't make sense for the result then it is not what I am looking for. It is also interesting to know if the result can be stated in the language of Synthetic Computability Theory or not. 

I am looking for nice examples, where the following phenomenon occurs: (1) An algorithmic problem looks hard, if you want to solve it working from the definitions and using standard results only. (2) On the other hand, it becomes easy, if you know some (not so standard) theorems. The goal of this is to illustrate for students that learning more theorems can be useful, even for those who are outside of the theory field (such as software engineers, computer engineers etc). Here is an example: 

Let $k, l, d$ be positive integers. Then there is a simple graph with vertex-connectivity $k$, edge-connectivity $l$, and minimum degree $d$, if and only if $k\leq l \leq d$. Since it is (almost) trivial to prove that these parameters always satisfy the inequalities, the violation of at least one inequality is a trivial obstacle for such a graph to exist. The other direction, that such a graph always exists is nontrivial. (See the book of Bollobas "Extremal Graph Theory," Theorem 1.5. Note: the way it is presented there is somewhat more complicated, using more inequalities, because it also uses another parameter, the number of vertices. But this simpler version can be extracted from it, to show the essential nature that here again the trivial obstacle is the only obstacle.) Given an undirected simple graph, does it have two vertex-disjoint circuits? This is answered by the following theorem of Lovasz and Dirac: If a simple graph with minimum degree $\geq 3$ does not have two vertex-disjoint circuits, then the graph can only be one of the following three types: (1) $K_5$, (2) a wheel graph, (3) $K_{3,n-3}$, with possibly any edges added to the the 3-vertex class. Here the trivial obstacle is that the graph falls into one of these types, since these types are easily seen not to contain 2 vertex-disjoint circuits. If the trivial obstacle is absent, the graph always has the property (but it is quite nontrivial to prove). Given a sequence $d_1\geq\ldots\geq d_n$ of integers, and an integer $k\geq 2$, does there exist a $k$-edge connected simple graph that has $d_1\geq\ldots\geq d_n$ as its degree sequence? A 1964 result of Edmonds answers the question: such a graph exists if and only if two conditions hold: (a) the sequence is graphical (i.e., there is a graph at all that implements it as a degree sequence, which is trivially necessary, and can be checked by the Erdos-Gallai Theorem), and (b) $d_n\geq k$, which is also trivially necessary, since the edge connectivity is bounded from above by the minimum degree. Once these essentially trivial necessary conditions are satisfied, the desired graph always exists. Given a complete graph $G$, and a fixed graph $H$, can the edges of $G$ be partitioned into disjoint copies of $H$? A 1976 theorem of Wilson proves that if $G$ is sufficiently large, then the only obstacle for such a partition to exist is the violation of one the following trivially necessary divisibility conditions: (a) $|E(H)|$ is a divisor of $|E(G)|$, (b) if $g$ is the greatest common divisor of the degrees of $H$, then $g$ is a divisor of $|V(G)|-1$. 

I am looking for examples of results which go against people's intuition for a general audience talk. Results which if asked from non-experts "what does your intuition tell you?", almost all would get it wrong. Results' statement should be easily explainable to undergraduates in cs/math. I am mainly looking for results in computer science. What are the most counterintuitive/unexpected results (of general interest) in your area? 

Razborov proved that the monotone function matching is not in mP. But can we compute matching using a polynomial size circuit with a few negations? Is there a P/poly circuit with $O(n^\epsilon)$ negations that computes matching? What is the trade-off between the number of negations and the size for matching? 

The Isolation lemma of Mulmuley, Vazirani, and Vazirani can be used to show that certain $\mathsf{NP}$-complete problems can be reduced via randomized polytime reductions to the unique solution version of the problem. This hints that the promise of a unique solution is not likely to make the problem significantly easier. A specific example for such a reduction in the Mulmuley, Vazirani, and Vazirani paper (pdf) is that CLIQUE can be reduced to UNIQUE CLIQUE via randomized polynomial time reductions. In this sense, UNIQUE CLIQUE is (almost) as hard as CLIQUE. 

Random oracle hypothesis: relationships between complexity classes that hold for almost all relativized worlds, also hold in the unrelativized case. This was disproved by the result $IP=PSPACE$, and by showing that $IP^X\neq PSPACE^X$ holds for almost all random oracles $X$, see The Random Oracle Hypothesis is False. Bounded error randomness properly extends the power of polynomial time (i.e., $P\neq BPP$). This was believed for a while, but later, due to sophisticated derandomization results and their connections to circuit complexity, the opposite conjecture ($P=BPP$) has become prevalent (although still open). 

Stephen Smale claims in Mathematical Problems for the Next Century that $$NP \not\subseteq BPP \implies NP_{\mathbb{C}} \not\subseteq P_{\mathbb{C}}.$$ Can someone sketch the argument or provide a reference? Is there any similar result in the reverse direction? $NP_{\mathbb{C}}$ (definition) and $P_{\mathbb{C}}$ (definition) are NP and P over complex numbers $\mathbb{C}$ using the Blum–Shub–Smale machine model. 

Recently I have seen several articles on arxiv that refer to a proof system called sum-of-squares. Can someone explain what is a sum-of-squares proof and why such proofs are important/interesting? How are they related to other algebraic proof systems? Are they some kind of dual to Lassere? 

ETH states that SAT cannot be solved in the worst case in subexponential time. What about average case? Are there natural problems in NP that are conjectured to be exponentially hard in the average case? Take average case to mean average running time with uniform distribution on the inputs. 

Is there a graph class for which the chromatic number can be computed in polynomial time, but finding an actual $k$-coloring with $k=\chi(G)$ is NP-hard? Without any further restriction the answer would be yes. For example, it is known that in the class of 3-chromatic graphs it is still NP-hard to find a 3-coloring, while the chromatic number is trivial: it is 3, by definition. The above example, however, could be called "cheating" in a sense, because it makes the chromatic number easy by shifting the hardness to the definition of the graph class. Therefore, I think, the right question is this: Is there a graph class that can be recognized in polynomial time, and the chromatic number of any graph $G$ in this class can also be computed in polynomial time, yet finding an actual $k=\chi(G)$-coloring for $G$ is NP-hard? 

How to compute the inversion of $2^n-1$ bits using $n$ negations Let the bits $x_0, \ldots, x_{2^n-1}$ be sorted in the decreasing order, i.e. $i<j$ implies $x_i \ge x_j$. This can be achieved by a monotone sorting network like the Ajtai–Komlós–Szemerédi sorting network. We define the inversion circuit for $2^n-1$ bits $I^n(\vec{x})$ inductively: For the base case we have $n=1$ and $I^1_0(\vec{x}) := \lnot x_0$. Let $m=2^{n-1}$. We reduce $I^n$ (for $2m+1$) bits to one $I^{n-1}$ gate (for $m$ bits) and one negation gate using $\land$ and $\lor$ gates. We use negation to compute $\lnot x_m$. For $i<m$ let $y_i := (x_i \land \lnot x_m) \lor x_{m+i}$. We use $I^{n-1}$ to invert $\vec{y}$. Now we can define $I^n$ as follows: $$I^n_i := \begin{cases} I^{n-1}_i(\vec{y}) \land \lnot x_m & i<m \\ \lnot x_m & i=m \\ I^{n-1}_i(\vec{y}) \lor \lnot x_m & i<m \\ \end{cases}$$ It is easy to verify this inverts $\vec{x}$ by considering the possible values of $x_n$ and using the fact that $\vec{x}$ is decreasing. From Michael J. Fischer, The complexity of negation-limited networks - a brief survey, 1975. 

It is well known that many NP-complete problems exhibit phase transition. I am interested here in phase transition with respect to containment in the language, rather than the hardness of the input, relative to an algorithm. To make the concept unambiguous, let us formally define it as follows. A language $L$ exhibits phase transition (with respect to containment), if 

Note that from the definition it is not even obvious that the problem is in NP. The reason is that the natural witness (the graph) may need $\Omega(n^2)$-bit long description, while the input is given by only $O(\log n)$ bits. On the other hand, the following theorem (see Extremal Graph Theory by B. Bollobas) comes to the rescue. 

Is it true that for all $n$ there are $n$ pairwise nonhomomorphic graphs with $poly(n)$ vertices? Is there a polynomial time algorithm for constructing such families of graphs? 

Lynch's Distributed Algorithms book is a classic but it is from 1996 and rather out of date. Are there any recent distributed computing books that can be used as textbooks for a graduate distributed computing and algorithms course? 

I am trying to understand the bitcoin protocol in the context of computational cryptographic security. The question is a reference request to foundations of cryptography articles on bitcoin. My first question is what abstract cryptographic protocol bitcoin is trying to implement? Do we have a definition of electronic money/digital currency in cryptography that captures bitcoin? What are the security requirements for a secure electronic money? If the answer is yes, companies like eBay provide a centralized mean of electronic money transfer. Does considering a decentralized electronic money changes the definition of abstract cryptographic protocol for electronic money? Or is it just the same concept but in a model where there is no trusted third party? Can the adversary break the protocol if it has more computational power than the combined computational power of other (honest) parties? Assume that we have $n$ parties $P_i$ for $1 \leq i \leq n$ plus an adversary $A$ networked and the adversary wants to break the bitcoin protocol. For simplicity let's assume that the network graph is $K_{n+1}$ and adversary does not control the network and simply is a party like others. What would be the exact mathematical claim about the security of the protocol in this simple case? 

Several answers pointed out that the premise of my question (the relative scarcity of natural $\mathsf{NPI}$-candidates) might be questionable. After some thinking, I must accept that they indeed have a point. In fact, one can even go as far as to make the case that there are actually more natural $\mathsf{NPI}$ candidates than natural $\mathsf{NP}$-complete problems. The argument could go as follows. Consider the LOGCLIQUE problem, which aims at deciding whether an $n$-vertex input graph has a clique of size $\geq \log n$. This is a natural $\mathsf{NPI}$ candidate. Now, the same type of "scaling down" can be carried out on any $\mathsf{NP}$-complete problem. Simply replace the question "does the input string $x$ have a property $Q$?" by the scaled down question "does $x$ have a logarithmically sized substring that has property $Q$?" (We may restrict ourselves only to those substrings that represent the appropriate type of structure, such as subgraphs etc.) Arguably, if the original problem was natural, the scaling down does not change this, since we only alter the size of what is sought for. The resulting problem will be an $\mathsf{NPI}$ candidate, since it is solvable in quasi-polynomial time, but still unlikely to fall into $\mathsf{P}$, as the mere size restriction probably does not introduce new structure. This way, we can construct a natural $\mathsf{NPI}$ candidate for every natural $\mathsf{NP}$-complete problem. Additionally, there are also generic candidates that do not arise via scaling down, such as Graph Isomorphism, Factoring etc. Thus, one can indeed make the case that "natural-$\mathsf{NPI}$" is actually more populous than "natural $\mathsf{NPC}$." Of course, this scaling down process, using Scott's nice metaphor, gives an obvious reason for resisting the "gravitational pull" of SAT. While there are papers published about LOGCLIQUE and similar problems, they did not draw too much attention, as these problems are less exciting than the the generic $\mathsf{NPI}$ candidates, where there is no clear understanding of how the gravitational pull is resisted, without falling into $\mathsf{P}$.