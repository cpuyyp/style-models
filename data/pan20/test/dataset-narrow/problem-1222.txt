Coq's tactic language has the limitation that the proofs written using it hardly resemble proofs one does by hand. Several attempts have been made to enable clearer proofs. These include Isar (for Isabelle/HOL) and Mizar's proof language. Aside: Did you also know that the programming language ML was originally designed to implement tactics for the LCF theorem prover? Many ideas developed for ML, such as type inference, have influenced modern programming languages. 

One not considered in your list is Derivatives of Regular Expressions by Janusz Brzozowski, Journal of the ACM 1964, which was recently reconsidered by Scott Owens, John Reppy, and Aaron Turon in Regular-expression derivatives re-examined. Journal of Functional Programming (2009), 19: 173-190, who provide a practical implementation of the technique for an extended notation for regular expressions. 

The New Turing Omnibus by A. K. Dewey has 66 so-called excursions in computer science. It covers topics such as analysis of algorithms, AI, complexity theory, theory of computation, cryptography, computer graphics and so forth. Every topic is written in a rather condensed form, capturing some landmark result in computer science. This book could provide some inspiration. Another possibility is to allow students to get their hands dirty via something like Google's Code-in program. It's a bit like Google's Summer of Code, but, you know, for kids. Perhaps showing some of the amazing coding projects students can be involved in is one possible way of piquing interest. 

I think you are confusing the size of the input with the number of elements in the input. The time complexity of sorting algorithms is defined in terms of the number of comparisons based on the number of elements, not on the size of the inputs, so when you say that it costs $n \log^2 n$ to sort the elements you are already saying that your algorithm is worst than $O(n\log n)$. 

A more general keyword to look for is classification. Check out a recent book on Machine Learning. (Or perhaps ask your question at $URL$ 

The answer is less than $2^{n-k}$ of the sequences of length $n$ have complexity less than $<n-k.$ Due to the uniformity assumption on these sequences we just count. Consider all short programs of bitlength $<n-k.$ Even if all represented sequences of length $n$ the total number of such programs (starting with the empty program) is $$1+2+2^2+..+2^{n-k-1}<2^{n-k}.$$ To clarify,the number of compressible strings by $k$ bits in a the set of length $n$ bits is $2^{n-k}$, their relative number is $2^{-k}$ and their percentage is $100\times 2^{-k}.$ So for each fixed $k$ and all $n$ you get a constant. For example the percentage of strings of length $n$ which are compressible by $k=3$ is $100\times 2^{-3}=12.5\%,$ those compressible by $10$ bits have percentage $100\times 2^{-10}=0.09765625 \%.$ 

I'll try to show this, hope I interpreted the question correctly. Let $A_k=\{0,1\}^k.$ If $f(n)\leq n+\log n$ for $n$ large enough this implies $$\max\{\ell(c(x)):x \in A_1 \cup A_2\cup \ldots \cup A_k \}\leq k+ \log k,$$ for $k\geq N$, for some finite $N$, where $\ell(c(x))$ is the length of the codeword $c(x)$ assigned to $x.$ So this means that $$ \sum_{N\leq k ~~} \sum_{x \in A_1\cup A_2 \cup \ldots \cup A_k} 2^{-\ell(c(x))}\geq \sum_{N \leq k~~} \sum_{x \in A_1\cup A_2 \cup \ldots \cup A_k} 2^{-(k+\log k)} $$ which is equal to $$ \sum_{N\leq k} |A_1 \cup A_2 \cup \ldots \cup A_k|~ 2^{-(k+\log k)}= \sum_{N\leq k} (2^{k+1}-1)~ 2^{-(k+\log k)} $$ which is equal to $$ \sum_{N\leq k} 2^{1-\log k} - 2^{-(k+\log k)}=2 \sum_{N\leq k} \frac{1}{k} - \sum_{N\leq k}\frac{2^{-k}}{k}=2 \left(\sum_{N\leq k} \frac{1}{k}\right) -\ln 2 $$ which is unbounded, since the harmonic series diverges. 

Well done on your independent discovery. This is a Hadamard matrix of Sylvester type, written in a different order. There is a massive literature on this topic. It is used in coding theory, cryptography, is directly related to Reed-Muller codes of degree 1, it can be used to obtain best affine approximations of functions, etc. Ryan O'Donnell's notes on Analysis of Boolean functions available here which are now published as a book, Claude Carlet's chapters on Boolean functions here are easily accessible online. Just use the order $[1,X,Y,XY,Z,ZX,ZY,ZXY]$ which corresponds to lexicographic order over the integers, and apply the corresponding permutation to the columns as well. In that order your matrix is simply the 3-fold Kronecker product $$H_2 \otimes H_2\otimes H_2$$of the basic 2 by 2 Hadamard matrix. $$ H_2=\left(\begin{array}{cc} 1 & 1 \\ 1 & -1 \end{array} \right), $$ This paper considers efficient evaluation of Hadamard representation coefficients. 

Look at Bob Harper's book. It provides loads of declarative formalisms for specifying programming language semantics and so forth. Chapter 6 describes declaratively how to translate concrete into abstract syntax. You could use the same ideas to translate one language into another. Following Harper (and many others), you could write rules defined inductively on the structure of your source syntax, as follows: $\begin{array}{c} a \leadsto e \qquad b \leadsto f \qquad x=foo(a,b,e,f) \\ \hline \mathsf{K}~a~b \leadsto \mathsf{S}~e~f~x \end{array} $ Here $\mathsf{K}$ is a constructor in your source language and $\mathsf{S}$ is in your target language. $foo$ is some function which may compute other required information (it too could be defined as a set of rules). The translation function/relation essentially inductively decomposes your source syntax and produces a term in your target. You can also thread through additional information to help with the translation process. For example, your rule could look like: $\begin{array}{c} E'\vdash a \leadsto e \qquad E''\vdash b \leadsto f \qquad x=foo(E,E',E'',a,b,e,f) \qquad E' = \cdots \qquad E''=\cdots \\ \hline E\vdash \mathsf{K}~a~b \leadsto \mathsf{S}~e~f~x \end{array} $ Here $E$ can store any information that needs to be threaded through. Note that such specifications differ very little from a standard recursive function defined inductively in the program syntax. But they may be easier to read, and they may actually define a relation, rather than a function, by allowing a little nondeterminism. 

Perhaps more interestingly, though I can hardly imagine, are the references in these papers. They point to many varied techniques for simulating zombie and other infections. There is also a lot of work on viruses spreading through social networks (which surprisingly enough is applicable to computer viruses spreading through online social networking sites). Two of many papers available on Arxiv are Affinity Paths and Information Diffusion in Social Networks and Viral Processes by Random Walks on Random Graphs. The book cited by the other reference and a similar one called Networks by Mark Newman have chapters covering the topic too, though without the zombies. 

The formulation on p.70 of the 4th edition of The Probabilistic Method by Alon and Spencer is along the lines you state. 

Disclaimer: This is based on generic information theory knowledge only. Too long for a comment. Summary: The pointwise product of your two plots should go to some limit, as the relevant blocklengths and sequence lengths increase. I don't know if this applies to DNA but in theory if your sequence is ergodic (stationary, and time averages are the same as ensemble averages, for long enough sequences) then the two entropies are related intimately. The whole point is, if you obtain a "block" entropy $H(X_1,\ldots,X_n)$ for a block of length $n,$ then $$ \lim_{n\rightarrow\infty} \frac{H(X_1,\ldots,X_n)}{n}=H_0 $$ which is the entropy per symbol, i.e., the entropy rate, what you call shannon entropy as opposed to block entropy. Now, the question of estimating these is a different question. In fact your program does this by a sliding window technique, which then means that it is averaging overlapping thus dependent samples. Define the shorthand $X_i^j=(X_i,X_{i+1},\ldots,X_{j})$ where $j\geq i.$ I am sure that under suitable conditions an estimator of the form $$ \frac{\sum_{k=1}^{n-\ell+1}\hat{H}(X_k^{k+\ell-1})}{n-\ell+1} $$ where $\hat{H}(X_k^{k+\ell-1})$ is a suitable estimator for block entropy, will converge to a multiple of the entropy rate, i.e., to $\ell \times f$ where $f$ is a correction factor. However, one may need to let $\ell$ grow with $n.$ 

Because an optimal prefix free code, e.g. a Huffman code, can be shown to be within one bit of source entropy. This is certainly in Cover and Thomas, I am pretty sure. 

Hadamard code [Sylvester Hadamard Matrices] has length $2^n$, $2^n$ codewords (if you take only the so called linear codewords but not their opposites) and minimum distance $2^{n-1}.$ Simplex code has similar parameters and is constant distance. Justesen codes are a concatenated construction that give fixed distance. There are codes with few distances as well, e.g., the Golay code. 

As pointed out in the comments if $u\in \{\pm 1\}$ then $x=x(u) \in \{0,1\}$ where $$x(u)=\frac{1-u}{2},$$ with $x(-1)=1,$ and $x(1)=0.$ This will then yield $$f(x)=2^{n-1}f(0)-\frac{1}{2} \sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i,$$ and if we denote the $\{0,1\}$ valued version of $f$ as $\tilde{f},$ $$\frac{1-\tilde{f}(x)}{2}=2^{n-1}f(0)-\frac{1}{2} \sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i,$$ leading to $$\frac{\tilde{f}(x)}{2}=\frac{1}{2}\left[1+ \sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i\right]-2^{n-1}f(0),$$ or $$\tilde{f}(x)=1-2^{n}f(0)+\sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i,$$ if I haven't made any errors along the way. 

I don't know how you can define the informal semantics as accurately as possibly, but there does exist machinery for defining semantics that is purely, well, semantic. Your semantics could be based on mathematical functions, relations, or domains. Indeed, the style of semantics known as Denotational Semantics builds upon these (and other) semantic notions by mapping the syntax in a compositional fashion to some semantic domain. If you don't want syntax for your programming language, then start directly with the semantic domains (for example, those from Domain Theory). In this setting, each type in your programming language will be represented by a semantic domain and algorithms will then be functions from one type to another. 

Other books are available online such as Barr & Well's Toposes, Triples, and Theories and Jiri Adámek, Horst Herrlich, and George E. Strecker's Abstract and Concrete Categories – The Joy of Cats. These are likely to contain all the definitions you need, at least from the category theory side. 

Another mechanical verification of the 4-colour theorem has been done by George Gonthier at Microsoft Research Cambridge. The difference with his proof is that the entire theorem has been stated and mechanically verified using the Coq proof assistant, whereas the other proofs contain only the kernel calculation written in Assembly Language and C, and thus have a risk of being buggy. Gonthier's proof covers both the calculational aspects and the logical ones in just 60,000 lines of Coq. 

A lot depends upon the conference/journal, as each community has developed its own style, so knowing what's expected of the conference/journal certainly will help a little. 

There does indeed exist a category theoretic description of cellular automata. I don't know the details, but I can provide you with a reference. It doesn't look like an easy description, though. S. Capobianco, T. Uustalu. A categorical outlook on cellular automata. In J. Kari, ed., Proc. of 2nd Symp. on Cellular Automata, JAC 2010 (Turku, Dec. 2010), v. 13 of TUCS Lecture Notes, pp. 88-99. Turku Centre for Computer Science, 2010. 

My limited understanding is that problems in which cyclic groups or their products play a central role usually have very good (exponential to polynomial complexity) speedups when quantum algorithms are used. This is due to the fact that the complex Fourier transform plays a central role in the representation of quantum states. The fact that Grover's search doesn't have this impressive speedup is also due to the fact that it is not tightly related to cyclic groups. 

Look up the concept of anticodes. Some bounds exist. Also if you have a linear $t-$error correcting code over $GF(q)$ with length $n$ and covering radius $r$ and use homogeneity under translations by adding vectors $n-d=n-2t-1$ would seem to be your answer but I haven't checked carefully. Cohen and Litsyn have a book on covering codes. A Simple Construction Fix $k\geq 1$ and let $v=\lceil n/k \rceil$ and consider the strings as $GF(q)^n.$ Generate all $k-$tuples over $GF(q)$ to obtain $M=q^k$ codewords. Map these codewords to $GF(q)^n$ by expanding by $v$ for the first $k-1$ coordinates and expanding by $n-(k-1)v$ for the last coordinate. For example, let $q=3,$ $n=5$ and $k=2.$ Your codewords are $$ \begin{array}{cc} 0&0\\ 0&1\\ 0&2\\ 1&0\\ 1&1\\ 1&2\\ 2&0\\ 2&1\\ 2&2\\ \end{array} $$ which become upon expansion with $v=4$ $$ \begin{array}{c} 0000~000\\ 0000~111\\ 0000~222\\ 1111~000\\ 1111~111\\ 1111~222\\ 2222~000\\ 2222~111\\ 2222~222\\ \end{array} $$ Now, for any possible vector in $GF(3)^7$ there is a codeword which is quite far from it. Namely, if there are fewer than 2 symbols for that vector in a block, there is a codeword which is at Hamming distance $v$ from it. If all 3 symbols occur, there is a codeword at Hamming distance $\geq \lfloor 2v/3 \rfloor$ from it. Since all possible constant blocks occur, for any possible vector in $GF(3)^7$ there is a codeword in this code of only $M=9$ codewords which is at Hamming distance $\geq k \lfloor (q-1)v/q \rfloor \approx n(1-\frac{1}{q}).$ Choosing $v$ a multiple of $q$ will make this more efficient. 

The research articles on Shannon theory and related fields in, say, IEEE Transactions on Information Theory, may fit the bill better, though not always. An older text which may also be of interest is Wolfowitz, J., Coding Theorems of Information Theory, Springer, 1960's. 

Papers for Mac OS X. It's not freeware, but it's cheapish. It does a fairly job of reducing the effort of adding the appropriate meta-data to the papers it collects. The only problem is that it is so easy to download papers much faster than you can put them into papers, though this will be a problem with any software, I imagine. 

Deforestation is a technique for eliminating intermediate data structures when composing functions together (in a lazy language). It improves the memory usage of programs, and is used by the GHC Haskell compiler. A more general notion is fusion, which combines multiple traversals of a data structure into a single traversal. Fusion rules have been developed for programs written in terms of and . (If this is the kind of thing you are looking for, I can provide some references (as can google).) Other transformations can be found on the program transformation wiki. 

I would just put the publication on my CV, though in the early stage of your career listing the presentations you've made isn't a bad idea. Cite as a conference paper if the 'book' is the proceedings of the conference. 

You should look at the Curry-Howard Correspondence to see the link between Logic and Programming Languages (types are propositions), and Cartesian Closed Categories, to see the between the relationship with Category Theory. 

One could think of a coinductive predicate as an infinite conjunction or disjunction, simply by imagining that it is unrolled completely. Game semantics in this setting would be straightforward: for an infinite conjunction, the Falsifier needs to select which conjunct to falsify in order to win the game; for an infinite disjunction, the Verifier needs to select which disjunct to satisfy in order to win the game. There is, however, a natural order in 'evaluating' the coinductive predicate which is ignored when considering it as an infinite conjunction or disjunction, and I would like this ordering to be captured in the game semantics, namely that the game proceeds by playing each disjunct/conjunct in turn. One additional problem I have is understanding which winning condition to use to capture the infinite nature of the predicate. Or to put it in lay persons terms: how do I know that there is no white cat in this supposed infinite stream of black cats––it could merely be after the point I stop looking? additional example Consider the following data type (again in Haskell):