If a node has fewer than $n^{1/3}$ neighbors, then add all edges incident on that node. From your remaining nodes, greedily select the remaining node with the most neighbors. Declare this to be a "cluster center," and then temporarily set aside this node and all its neighbors. Repeat until you're out of nodes. Let $C$ be the final set of cluster centers. There is a proof in the paper mentioned above that guarantees that $C$ is only $O(n^{2/3})$ big. Add to $H$ the edge from each cluster center to each of its neighbors. For each $c, c' \in C$, add to $H$ a single weighted edge from $c$ to $c'$ of weight $\delta_G(c, c')$. 

Some of the very early work on complexity theory used a sequential time model -- that is, rather than studying the worst-case runtime of the TM that can produce the correct output on an arbitrary input, they studied machines that would run infinitely and enumerate the correct output for each input in lexicographic order. The complexity of the machine was then based on the worst-case time gap ("delay") between the enumeration of consecutive outputs. This model can be used to study the problem of taking an input $1^n$ and producing on output the $2^n$-sized truth table of a language on all inputs of length $n$ , while trying to minimize the average computation time required per input (so $2^n poly(n)$ is considered "efficient" in this model). This seems pretty similar to the question you're asking. Here is a paper that uses that model. Here is a blog post that is only somewhat related, but includes some references that you might find interesting. One note about this model is that for some NP problems, including SAT, you can print their truth table in polynomial time per bit by exploiting the self-reducibility of the problem. For example, with SAT, you can always efficiently find the next bit of the truth table by fixing one of the variables, computing the reduced version of the problem under this variable fix, and then looking up the solution to the reduced version of the problem in the truth table that you have computed so far. 

This is intended as a comment, but it's too long to post as a comment. You might also be interested in graph spanners or emulators for your purposes. A spanner of a graph $G = (V, E)$ is a subgraph $H = (V, E')$ with few edges, but approximately preserved distances. An emulator is a graph $H = (V, E', w)$ whose edges are allowed to be weighted. The best result for spanners is $O(n^{4/3})$ edges and an additive error of +6 on distance estimates in the graph. The best result for emulators is $O(n^{4/3})$ edges and an additive error of +4. It is not known for either if we can beat $O(n^{4/3})$, even if the error is allowed to be polylogarithmic. If this sounds useful, I can try and dig up the relevant constructions for you. 

This is a special case of the precedence-constrained TSP which has been studied quite a lot. For instance, there are a polyhedral analysis by Balas, Fischetti and Pulleyblank, and a branch-and-cut algorithm by Ascheuer, Jünger and Reinelt. 

The problem is known as the propagation problem. Aazami has proved in his PhD thesis that the weighted version is NP-complete even when the graph is planar and the node weights are in $\{0,1\}$. The complexity for the unweighted version seems to be an open problem. 

In the context of scheduling maintenance jobs on arcs of a flow network I came across the problem to schedule jobs, indexed by $j$, and given by triples $(r_j,d_j,p_j)$ of (integer) release time, due time and processing time such that the total time in which no job is processed is maximized. A schedule is given by start times $x_j\in S_j:=\{r_j,r_j+1,\ldots,d_j-p_j+1\}$. For instance, for five jobs with $(r_j,d_j,p_j)$ equal to \[(1,3,3),\ (20,22,3),\ (10,11,2),\ (1,18,5),\ (8,26,5)\] and a time horizon of 30, the first three jobs are fixed starting at times $x_1=1$, $x_2=20$ and $x_3=10$, and for the last two jobs it is optimal to start both of them at the same time $x_4=x_5\in\{8,9,10\}$. The objective value (number of time periods without any job) is \[30-3-5-3=19.\] The straightforward binary program for $n$ jobs looks as follows ($y_t$ is the indicator variable for the activity of the "plant" machine suggested in the comment by András Salamon) \begin{align*} \text{minimize}\ \sum_{t=1}^Ty_t&\\ \text{subject to}\qquad \sum_{t\in S_j}x_{jt} &=1 && j\in[n], \\ y_t-\sum_{t'\in S_j\cap[t-p_j+1,t]}x_{jt'} &\geqslant 0 && t\in[T],\ j\in J_t,\\ x_{jt}&\in\{0,1\} && j\in[n],\ t\in S_j,\\ y_t &\in\{0,1\} && t\in[T]. \end{align*} Here $J_t=\{j\ :\ r_j\leqslant t\leqslant d_j\}$ denotes the set of jobs that can be "active" at time $t$. Interestingly, the constraint matrix is not totally unimodular in general, but the LP relaxation seems to have always integer optimal solutions. Writing down the dual also yields a problem which can be interpreted combinatorially, but so far I haven't been able to turn this into an integrality proof. One can look at variants of the problem where the number of jobs that can be processed simultaneously is bounded, or there is a given partition of the job set such that no two jobs from the same part can be processed at the same time. Has this kind of objective function been studied in the scheduling literature? Or is there any other related problem? 

I think you want the Edmonds-Gallai decomposition of your graph which can be computed in time $O(n^3)$ (see these notes). 

I think you can solve this as a single shortest path problem with respect to arc weights $$\overline{w}_{ij}=\min\{-w_{ijp}\ :\ p\in\{1,\ldots,m\}\}.$$ For every arc $(i,j)$ fix some $p(i,j)\in\{1,\ldots,m\}$ with $-w_{ijp(i,j)}=\overline{w}_{ij}$. If you have a shortest path $\pi$ with respect to $\overline{w}$, you can put $$y_{ijp}=\begin{cases} m & \text{if }(i,j)\in\pi\text{ and }p=p(i,j),\\ 0 & \text{otherwise.}\end{cases}$$ For the $l$- and $z$-variables you do the following. If $(1,j)\in\pi$ is the arc leaving 1 then $$z_{1q}=\begin{cases}m-1 & \text{for } q=p(1,j)\\ 0 & \text{for } q\neq p(1,j)\end{cases}$$ $$l_{1q}=\begin{cases}1 & \text{for } q\neq p(1,j)\\ 0 & \text{for } q= p(1,j)\end{cases}$$ If $(i,n)\in\pi$ is the arc entering $n$ then $$z_{nq}=\begin{cases}0 & \text{for } q=p(i,n)\\ 1 & \text{for } q\neq p(i,n)\end{cases}$$ $$l_{1q}=\begin{cases}0 & \text{for } q\neq p(i,n)\\ m-1 & \text{for } q= p(i,n)\end{cases}$$ If $(j,i)$, $(i,j')$ are two consecutive arcs of $\pi$ then you put $l_{ip}=z_{ip}=0$ if $p(j,i)=p(i,j')$, and otherwise $$z_{iq}= \begin{cases} m & \text{for } q=p(i,j')\\ 0 & \text{for } q\neq p(i,j') \end{cases}$$ $$l_{iq}= \begin{cases} m & \text{for } q=p(j,i)\\ 0 & \text{for } q\neq p(j,i) \end{cases}$$ And LP duality tells you that this is optimal. 

At $URL$ there is a presentation of an algorithm for determining if two graphs are isomorphic. Given a number of shall we say, "interesting" claims by A Dharwadker, I am not inclined to believe it. In my investigation, I find that the algorithm will definitely produce the correct answer and tell you that two graphs are not isomorphic when in fact that is correct. However, it is not clear that the algorithm will consistently tell you if two graphs are isomorphic when they actually are. The "proof" of their result leaves something to be desired. However, I am not aware of a counter-example. Before I start writing software to test out the algorithm, I thought I would see if anyone was already aware of a counter-example. Someone requested a synopsis of the algorithm. I will do what I can here, but to really understand it, you should visit $URL$ There are two phases to the algorithm: A "signature" phase and a sorting phase. The first "signature" phase (this is my term for their process; they call it generating the "sign matrix") effectively sorts vertices into different equivalence classes. The second phase first orders vertices according to their equivalence class, and then applies a sort procedure within equivalence classes to establish an isomorphism between the two graphs. Interestingly, they do not claim to establish a canonical form for the graphs - instead, one graph is used as a kind of template for the second. The signature phase is actually quite interesting, and I would not do it justice here by attempting to paraphrase it. If you want further details, I recommend following the link to examine his signature phase. The generated "sign matrix" certainly retains all information about the original graph and then establishes a bit more information. After collecting the signatures, they ignore the original matrix since the signatures contain the entire information about the original matrix. Suffice to say that the signature performs some operation that applies to each edge related to the vertex and then they collects the multiset of elements for a vertex to establish an equivalence class for the vertex. The second phase - the sort phase - is the part that is dubious. In particular, I would expect that if their process worked, then the algorithm developed by Anna Lubiw for providing a "Doubly Lexical Ordering of Matrices" (See: $URL$ would also work to define a canonical form for a graph. To be fair, I do not entirely understand their sort process, though I think they do a reasonable job of describing it. (I just have not worked through all the details). In other words, I may be missing something. However, it is unclear how this process can do much more than accidentally find an isomorphism. Sure, they will probably find it with high probability, but not with a guarantee. If the two graphs are non-isomorphic, the sort process will never find it, and the process correctly rejects the graphs.