Repeat for each of the four points, and then find max/min boundaries of the AABB. -- This is super tedious. The better way is to use 3x3 matrix representing rotation and translation. In effect, you take each of the four points, translate them back to origin by subtract the center's coordinates from them so that their new center is on (0,0), then rotate them. Finally, you translate them back to the real center. With the new set of points, once again, simply compute new the AABB for those points. Wikipedia has some great information about matrix math, especially how to do rotations, translations, and more using 3x3 matrices. 

When you specify vertex colors, the alpha value is smoothly interpolated across the rectangle. Using blending, we can multiply the color by that alpha value Since we swap 1.0/0.0 alpha values depending on which rectangle we're drawing, for each pixel where alpha is some value for rectangle 1, it will be for rectangle 2. 

Consider a box centered at some point in space. Imagine a line from the center to the top-right corner. This length is effectively the hypotenuse of a triangle formed from taking the center of the box and drawing a line right and then up. These sides of the triangle are half-extents. In the picture, the red lines are the half extents and the blue line is the hypotenuse -- the distance from the center of the box to the corner. Call this blue line's length 'dist' 

Now I just need to add in things like variances for lighting and such :) I hope this is helpful to someone out there getting to grips with triplanar texturing :) 

As long the system updates and renders at roughly 25 FPS or higher the system is usable perceptibly suitable for game play 

every "command" a user intends to issue is stacked up in a local list then sent to the "server" at the end of each turn who then forwards on the lists from other players to ensure the data is replicated to all players. Every time a player issues a command to be added to the list of things for this turn its immediately passed to the server then at the end of the turn a final "im done" is received from each player and the server triggers a new turn. 

Ok maybe i'm missing something because its getting late and i've been programming now for about 72 hours straight (minus a nap or 2) ... I'm trying to write a shader for unity that will apply triplanar texturing to a mesh, here's my shader code ... 

EDIT: Ok i managed to get this (code sample above updated) to render an "optimised" set of verts but the UV data is all wrong now, which would make sense because i'm basically just removing any UV Vector that represents a UV coord for a removed vert and not actually considering what I need to do to "fix the tri" so to speak. The code now seemingly does work but its quite time consuming, still looking to further optimise. 

I'm writing a component-based entity system and one of the components is the entity's state, which dictates how it reacts to game events. In case anyone has experience with implementing states, how granular should they be? To give you an idea about how granular they are in my case, I have a state and a state (as opposed to simply a state) and I fear that they might be too granular. What do you think? 

I'm making a racing game with cars in Unity. The car has a boost/nitro powerup. While boosting, I wouldn't want to be deviated when colliding with zombies, but I do want to be deviated when colliding with walls. On the other hand, I don't want to ignore collision with zombies, because I still want to hit them on impact. How should I handle this? Basically, what I want is for the car to not rotate when colliding with certain objects. 

You mentioned you want the game to be flexible. Do you mean that as only adding new features (better lighting, physics, etc), or also flexibility in terms of game-design? If you also want the game-design part of the development to be flexible, component-based entity systems would be pretty useful. Unfortunately, I can't give you a competent answer to your second question. If you choose to implement a CBS, there's two important things you need to ensure: 

Your best bet is to not generate tiles but generate and manage chunks which contain tiles. If you think about chunks as a fundamental part of the design then the issue may go away. I faced a similar issue in 3d with building my voxel engine. You likely want to do something like ... 

In most game engines today the typical design is that a scene is a graph / tree of components some of which may be renderable. In a sense you are correct, every object is basically a node in a tree of things. You may have heard the expression "Entity Component System" and the expression "Component Composition" in relation to game engines and more specifically scene based objects. These 2 terms refer to a mechanism for maintaining a hierarchy of objects that are part of a renderable scene not all of which may contain a renderer. Component Composition is particularly useful for games because it refers to the mechanism of using composition to dynamically bolt bits of objects together in order to achieve something useful where each bit would be function / behavior of the object I am bolting to. For example: In unity (arguably one of the most popular engines for indie developers today) we can create an object in our scene which is in itself just a container. I can then add child objects to that object which are also just containers. I can then add things like renderers and mesh information about materials behavior like input control or other behaviors like gravity to an object. So I could say "this is a terrain object", and this child of that terrain is a character object, which has a controller component to allow the user to feed input to the character. So to put this back in to terms that match your question ... Your scene node object would contain a mesh component and a renderer component for rendering that mesh. The renderer would use the mesh and a material to draw the object in our virtual world each frame. Also: Other things that might be scene nodes / objects would be a camera, audio source, a light source, none of this as such would typically be renderable but arguably have an impact on the scene, it is the job of the engine to determine how to interpret your defined scene graph / tree. 

I need some advice on how to design the Entity module in my game, how to apply the MVC pattern and generally how the Entity should interact with its controller and its representation. First some details about the game: it's a 2D action-platformer, it's data-driven (or rather, I'd like it to be data-driven — it isn't yet) and the Entities should be scriptable (I'll be using Python). I'm still at the very beginning of the development and I have some general ideas about how the Entity module should work, but I get stuck on the details. Here's roughly what I have up until now: 

Definitely not. is just a tool and you should use it when it fits your desired result. If your widgets are supposed to be laid out in a tabular way, then use . Otherwise, it's perfectly okay to position your widgets manually if that's what you need. Two alternatives to 

An event system will do for a lot of cases, but that is more suited for propagating information that changes or is generated 'rarely' (like the death of the player). For something like entity health and position — which need to be known every frame — an event system isn't well suited. 

What good free and widely used tools are there for editing 2D skeletal animations? Preferably, one that allows me to write custom animation exporters. One pretty good indie tool that I know of Demina, but it's not ideal. It doesn't allow you to export the data as you want (although, it is open source, so you can change that) and I find it clunky in how you edit individual joints. What other tools would you recommend? 

Ok so i figured it out .... I'm still working my way through it but basically the problem is that unity and the lookup tables supplied by Paul Burke don't quite tally up as had suspected. The process basically involves reshuffling each case so that the tri's orders and winding are correct for unity. I'm little bit annoyed at this since unity uses OpenGL under the bonnet you'd expect the positioning system to be similar, that said I haven't seen anything published by paul burke that even remotely suggested he had ever tried to texture a mesh generated with his algorithm so maybe its just bad lookup tables that were published. 

Actually there's a simple formula for it but its based on how many people on average would get the right answer in a population. difficulty = correct - wrong where correct is the number of people that got it right and wrong is the number of poeple that didn't when asked to a large group of people. The real question is ... how do you figure out how many people on average would get the answer to a question? When you figure that out you have your answer. Typically, the more specific to particular focus / topic a question is the harder it is so if your questions are separated enough you may be able to "figure these numbers out" with some reasoning. 

To create an atlas (without scaling the lightmaps), create an image that is, say 2x larger than your lightmap on each edge. Then you've got a 2x2 grid of lightmaps. Dump the lightmaps into it, and adjust the texture coordinates based upon which lightmap they came from. To directly answer your question, yes -- group close objects in the same lightmap. If you reverse the situation (have far away objects in the same lightmap) then you introduce performance problems almost immediately and unnecessarily. Resolution is actually much less of a problem for lightmaps since it provides only subtle detail rather than the actual high frequency visual information such as bricks/concrete. Scaling: don't scale things outside of the editor after generating lightmaps. Generally, you only generate lightmaps for static geometry anyways, so I don't know why you'd want to scale it. If it must be scalable, then you need some other [dynamic] solution since scaling a model can entirely change where shadows would normally occur in the lightmap generation process. In other words, it will look not just "pixely" or "blurry", but straight up wrong. Any lightmap generation program that isn't brain dead should use area of the triangle as a way of generation "priority" for texel space. For example, if you had 4 triangles in your scene that had 2500, 300, 500, and 200 units of area, you'd expect the triangle with 2500 area to have more texel space in the lightmap allocated for it compared to the others, since increased resolution will actually be visible in the larger triangle. 

Sounds like you need to create your own game object and use a custom triangulation algo to handle the detail based on some size related math. Maybe something like this could help ... $URL$ Others options might be to use a smarter shader and a bump map with your sprite ... vertex displacement type approaches. It's hard to say without knowing exactly what it is you are trying to achieve as to what might fit best. Hopefully one of these helps. 

There is this wonder invention, its called source control. Many of us use github or visual studio online, if you want auto updates there are a ton of examples for writing about 10 lines of code in a python script or similar to auto fetch and compile sources from a repo, virtually all open source builds do that these days for software that runs on nas or media centers. On top of that there's then project management tooling like, jira (seems to be the most popular from what i've seen, when used with something like jenkins (an automated build tool) and bitbucket (atlassian's version of github) you can a pretty good continuous integration (CI) workflow for your CI builds. Pick your poison and do some research. 

lets assume a block size of 1 ... this code generated an actual set of verts that resulted in a block being 2 in size in all dimensions so my odd looking scene was the result of overlapping blocks which in turn caused a sort of "tearing" to occur between blocks with different textures result in each block having a line of screen real estate (ugly to say the least). I then noticed something was still not right so I Took Jason Coombes advice (given on skype - thanks Jason) and added another set of textures to my atlas which had numbers on them. Most of the sides looked ok but the top face seemed messed up ... the uv coordinates were setup wrong in here ... 

When positioning manually One piece advice, though, if you decide to position your widgets manually: don't use absolute pixel coordinates. If you your screen is 400px wide and you want a widget's X coordinate to be 200px, set it to , instead. This will allow your widgets to be position correctly, even if the screen size changes and it takes no extra effort on your part. 

Imagine I have a skeleton — that is a set of bodies held together through various constraints and joints — and I want to flip it. Bodies cannot be flipped in Box2D, so how can I fake that? Here's an example: I have a humanoid skeleton, made out of different Box2D bodies: the head, torso, upper arm, lower leg, etc. These Box2D bodies are held together by joints. Some of the joints have angle constraints, like the ones between the upper and lower arm, which do not allow the 'elbow' to twist unnaturally. When I turn my character the other way around, I should flip the skeleton and its joints should be flipped as well. How can I achieve that in Box2D? 

I have a very general question: In games, what use does the programming concept of a window have? Or, in other words, why do some game dev libraries offer interfaces through which to create multiple windows? — Why would you need more than one windows in a game? Are multiple windows used as different views/states of the game? (I.e. in-game, main menu, pause menu, etc.) 

You are taking a 512x512 image and smooshing it into an area approximately 120x120 pixels. Thus, you should expect that each pixel is about the average of a 5x5 block. You're using linear texture filtering so that would be expected. It might be better to use unscaled images if you're going to reduce the resolution by such a large amount. You can control the quality easily since what you see is what you get. Aside from that it will load faster and run faster on phones. A 512x512 image requires an immense amount of memory bandwidth to draw compared to say 128x128 (16x as much, in fact) which is a performance killer on mobile GPUs. 

First, the interconnect between the GPU and the CPU is usually very narrow. For example, when developing for the iPhone, memory bandwidtch is a huge problem. People pack vertex data using 16-bit integers and use matrices to rescale them. For example, I used 0...1023 to represent [0.0, 1.0] range in OpenGL texture coordinate by applying a scaling factor of (1.0/1023.0) to incoming texture coordinates. This allowed me to use 1/2 of the bandwidth when submitting texture coordinates. Second, the limited heat/power means a lower clock speed. That's pretty intuitive, but worth mentioning. This implies fewer shader units and slower speeds for each unit. However, that also extends to the speed of the video memory and sometimes means dedicated memory isn't even available because it would produce too much extra heat. Third, architectural pipeline features. DirectX 11 / OpenGL 4.0 have features that require extra stages of the traditional programmable pipeline, specifically geometry shaders, tessellation shaders, hull shaders, etc. These extra stages mean more GPU die space and consequently more cost and heat. These features generally are used to add detail to scenes but aren't critical in implementing algorithms like lighting, shadows, normal mapping, etc. so they aren't really killer features like fixed function -> programmable pipeline change. Finally, mobile GPUs lack the generality of desktop GPUs. With technologies like CUDA, OpenGL, and DirectCompute aimed to solve non-graphical problems, GPU makers had to implement more general operations like integer operations, GPU mem pointers, 64-bit floating point values, etc. which require more hardware support. These features aren't really necessary for games and so they generally don't make the cut. More hardware features, even if you don't use them, have a $ and heat cost associated with them. Really, everything that is said can be traced to power, heat, cost in some form.