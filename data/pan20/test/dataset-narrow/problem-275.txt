It's always possible that this table is populated by something external to the database instance that the table is situated on - from an application, via a linked server, from a script on the server. There are probably other possibilities but you get the idea. The best way I can think to track activity on the table is to run SQL Server Profiler (via the Tools menu of SSMS on the 2012 version). I'm not going to write a tutorial on that software here but you essentially want to filter the activity on the server by the name of the table that you are interested in. If anything updates that table then it should appear in the profiler results. 

leave them with the data warehouse database put them on the server that will have Reporting Services installed on it create another server specifically to host those two databases. 

I'm looking to set up SQL Server 2012 installation with an Always On Availability Group, where the 'passive' replica will be hosted at another site on the WAN and using synchronous data commit - the idea being that we will have a hot standby with no loss of data in the event of a failure at our primary site. One potential problem that I foresee is that the secondary site has slower storage than our primary site. I don't care about that in the event of a failure, we can live with slow speeds for a period of time until the primary site is restored. My worry is that, because we are using synchronous commit, that the slower disk speed at the secondary site will affect performance at the primary site during normal operation. Is this a valid concern, or is it likely that the slower speed will be offset by, for example, the disk not having much read activity in comparison to the primary site? 

Do note though that BufferWithTolerance has a tolerance value (in my example 0.9) that is essentially a trade-off value between speed and accuracy. If you want exact results this probably isn't the method for you. I also seem to recall that STIntersects is an imprecise method but I can't find any reference to back that up at the moment so maybe I am mistaken about that. 

Some of the mystery is solved - The reason that some sites could connect and some couldn't is that there was an additional connection string stored in the machine.config file that two of the sites were using. The site that worked when a port number was specified did not use this connection string, so worked. The other sites didn't work when port number was specified because I hadn't added the port number to this connection string in machine.config. Adding the port number to that connection string as well as the ones stored in web.config allowed all the sites to work. Additionally, the reason that iisreset had an effect is that once IIS knows the port number that SQL is using, it seems to 'remember' the port number, and so it no longer has to rely on using port 1434. Once I did an iisreset it 'forgot' what port number was used, so had to make a request on port 1434, which for some reason doesn't work. So my question has gone from a very confusing 'works sometimes' situation to a basic 'cannot communicate on port 1434' situation. 

If you only need to know that a row is distinct, and don't need the actual contents of col3, then perhaps returning the hash of col3 would speed up the query? You could even perhaps pre-compute the hash using a calculated column so that you aren't computing the hash on the fly. If you do need the contents of col3, but have a lot of duplicates of col1+col2+col3, then it still may be beneficial to work with the hash to remove duplicates as a sub-query, then only return the col3 contents for the distinct rows. 

I've seen suggestions to set the "Use 32 bit runtime" option, but this had no effect. I've seen suggestions to set something similar in BIDS, but I didn't use BIDS to generate this package. I've seen options to use the 32-bit version of DTExec but I don't think the 32-bit version is installed on the server. SQL Agent Job definition: 

EDIT #2 I just completed a backup using the GUI database backup in case my script was wrong somehow - This gave the same result of a damaged backup. -- This only started happening last night. The only significant change that I am aware of is that I changed the 'auto-shrink' option of the database (it was originally set to true). Is anybody aware of a problem in this area with SQL Server 2012 (specifically I'm on version 11.0.3128)? It could just be coincidence or related to something else entirely but that seems the most likely cause at the moment. In addition does anybody have any advice on what to do in such a situation? The database is functioning fine (as far as I can determine), but I don't care to be without backups for very long...! 

In your case I think you could consider the use of a Bit String Type data type. For instance, something like: 

The only candidate key of your relation is {D DA HA L NF} (perhaps with R you mean D?) You can verify this by calculating the closure of those attributes, {D DA HA L NF}+, and seeing that it contains all the attributes, while, if you remove any one of them, the closure of the remaining set does not contains all the attributes (this is the definition of a candidate key). The relation is only in first normal form, since the second normal form requires the absence of partial dependencies, that is of dependencies in which non-prime attributes (i.e. attributes not belonging to any key) depends only on part of a key. In this case only the last functional dependency is not partial. 

Normal forms are used to eliminate or at least reduce redundancy in data, as well as to eliminate the so called insertion/deletion anomalies. The BCNF eliminates redundancies and anomalies, but decomposing a relation in BCNF sometimes has the unpleasant effect of causing the loss of one or more functional dependencies during the process. For this reason the 3NF is used instead of the BCNF in practice, since a decomposition of a relation in this form always mantains data and dependencies, and reduces anomalies and redundancy (even if to a lesser extent that the BCNF). Another reason is that the decomposition in 3NF can be obtained with a polynomial-time algorithm, while the decomposition in BCNF requires an exponential algorithm. 

In the terminology used to to talk about conceptual design of database, this is called inverse relationship. So you could say: 

While it is immediate to discover tha is a candidate key since it determines all the other attributes, you can see that this is true also for and by calculating the closure of those attributes: 

So in your example you can see that both and are dependencies in which the left hand side is a key, so the answer to the first question is yes. In the third dependency, instead, is not a superkey, (since ), so, is a prime attribute? To answer to this question, you should know all the keys of the relation, and see if belongs to one of them. And in this case this is very simple, since A is the unique key of this relation, so you can conclude that the relation is not in 3NF. But note that in general, in more complex cases, it is not so simple to answer to such question, since you can have a very large number of keys, actually a number which is exponential with the number of attributes. 

which is more efficient than solution B, and more simple to program than solution C. Note that this solution is normalized, so that you do not have any redundancy, neither you have insertion/deletion anomalies. Addition For a relation to be formally in Boyce Codd Normal Form (which is stricter than the Third Normal Form), for each dependency the determinant must be a (super)key. But first note that the normalization theory usually do not treat null values. See for instance the book of Elmasri, Navathe, “Fundamental of Database Systems". 6th Edition, 2010: 

make a group for each different combination of employeeId and date in the Sales relation, and for each group produce a tuple with employeeId, date, maximum sale price of the group. 

The tool mentioned does not require trasforming your attributes into single letter identifier, you can use your identifiers (you just need to remove the ‘_’ character) like , , etc. (and it transforms them automatically to uppercase). Those kind of tools requires that you give a single relation, containing all the attributes, like R(traincode, name, int, journeyid, price) and then all the functional dependencies holding among them, like for instance , etc. Nobody but you can give those functional dependencies, since they describe the semantics of your data. So you need to understand what a functional dependency is, and describe those holding among the attributes of your problem. Finally, this kind of systems will give you a decomposition of the single relation in several, normalized, relations. 

Has every functional dependency a left hand side which is a superkey? If this is not true, are the attributes on the right hand side prime attributes? (i.e. belonging to any key?). 

Okay, there is a reason why tools that can do this are expensive. It's by no way easy to do. However this blog post can help you. How to create undo update statements Since you have found the transaction, you can filter the result by using your transaction id. 

You can see that 68 seconds of NETWORK_IO is registered. But since the insert loop is a single threaded action that took 36 seconds, this can't be. (Yes, multiple threads are used, but the operations are serial, never in parallel, so you can't acummulate more wait time than the total duration of the query) If I don't use extended events but just the wait stats DMVs on a quiet instance (with just me running the insert) I get this: 

There are other causes for page splits but these are the biggest ones. All of the above will cause fragmentation. Now that's bad, but in a strange way good. Because that is about the only thing that can properly help you decide on the proper fill factor. I'll explain why. What you do is: See if any of the above apply. For the indexes that do, do the following. 

What where the exact steps you took, from the moment you found a problem until the moment you discovered that replication didn't work? What was broken, what was repaired, what was lost? All of this information is available in the output of . What exactly do you mean by: "Replication don't work?" Did you get any errors, or are tables not the same? Is data missing? 

disclaimer: Hopefully you'll agree that this answers your question "How to prevent dropping a login" but in no way is this a proper root cause fix. However, I know that there are some pretty nasty legacy applications where it's not possible to change the code... I must admit, this one just made me smile.. 

However, there are tricky things with indexrs combined with where clause that influence locking that could influence results and definately concurrency so it might be good to provide your table definitions including indexes. So people could assist you. 

By copying the complete DATA directory, you most likely also copied the system databases, specifically your master database from your old server and overwritten the system databases of your second SQL Server. When copying a USER database in general, the easiest method is to make a backup of that user db on the source server and restore that backup on the destination server. there is no need to ever copy the master, model, msdb or tempdb files from one server to another just to migrate a user database. To fix your server again: 

Now from that moment on a so called differential bitmap is keeping track of all pages changed. From that moment every time you create a DIFFERENTIAL backup, only the pages that changed since your last FULL backup will be backupped. 

Now here is the important info: Repair: The page (1:296) has been deallocated from object ID 389576426 The repair works by rebuilding the clustered index on a new set of pages and it simply skips the corrupted page. It fixes all references in allocation pages and linked pages. After the rebuild, it is as if the page was never there. (This is all logged.) The repair can't work ona row level since I corrupted the header and DBCC CHECKDB simply doesn't know what is on the page. the result: There are 9998 rows in 4999 pages for object "RepUserTable". 1 page gone and 2 records gone. The logreader agent has no clue on how many rows where on that page. so it has no idea on how to replicate this to the subscriber. If you now restart the log agent. You'll see that Replication doesn't fail.. However: On the source we don't have record 3 and 4 but they are still present on the subscriber: 

Out of index order inserts. Those entry have to be placed in between data. Since the page is full, other records are forced of to a new page. you have a table schema with a large part of the row being variable data type. Updates altering the row size could cause page splits Updating index keys, making entries move in the index causes page splits. Snapshot versioning adds a 14byte tag to the row, this could cause page splits.