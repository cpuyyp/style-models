responds with 3 lines where I find 1 line containing the internal network IP and the port 8090, it goes as following: 

Could anyone shed some light into this? I honestly do not understand these flags from all these commands and would be more comfortable simply editing a file with 

Update this command also didn't do the trick But now I am getting confused of what is right and what is the wrong command and maybe wanted to get it confirmed here how to do it correctly. Using the command 

to get this iptables thing right. But if not possible and I really get told the correct command, I would also be fine just pasting it into my ssh command window as a 1 liner-command with several flags that are puzzling me. Thanks to everyone who could help me out with the correct solution and maybe an ELI5 explanation why the first command did not work. Thanks again! PS: Displaying the commands as code doesn't quite work for me here on raspberrypi.stackexchange. Also would love to get help for that! Until then hope you guys still can read it. 

I have this web application called HaasBot (running on Mono 5) which I want to run on my Raspberry Pi 3 with Raspbian Stretch Lite (Terminal only!) as a server internally in my network only. Using SSH to do this by the way. The Pi has the OS freshly installed, no other things besides than dirmngr (as I recall correctly) and Mono 5 are installed. Iptables was installed by default. The whole OS is updated and upgraded to it*s latest state of course. The web app can not be considered "installed", because it was just a zip file I extracted, so nothing big there. Also no firewall installed, I also own no hardware firewall. We have a fritzbox and a network-security-server in our business network, where I am connected to. But as said, would need to open the port first anyway since it is required anyway before I can start finding the error with the fritzbox or the network-security-server. My own little network runs closed behind the main network where the fritzbox and the network-security-server is sitting infront of. My Wireless Lan router and the LAN hub (Pi connected by LAN cable of course) are just forwarding the connection to my very own little network. The application itself is running, it needs the port 8090 and 8092. 8090 seems for incoming connections and 8092 for outgoing connections because when I try to access the web application from another computer's browser by typing in $URL$ it loads my splash page, tries to receive data from the internet but doesn't finish doing so. Current state: Still the web app does not receive any data. It is stuck at the splash page where it says "Connecting to Haasonline Trade Server" Also the web app can only be accessed when typing in "$URL$ in the browser, trying "$URL$ doesn't display the web app Checking for closed ports with 

and it didn't open the port. I also was told to run this command to open the port which maybe could help: 

And everything was just fine. Also there was just also a little issue with the web app's splash page. I thank everyone for all recommendations and answers! I will upvote what I can! Thanks guys for your time and have a good day! 

Checking it with the port 8090 says the port is opened. Update: As of now it still says the port 8092 is closed. Before I can go and find the error somewhere else I would need to get and keep that port opened anyway first. I did try Google with several differnt searches and asked here and there, but still no luck getting the port 8092 opened. I only know that if I get it opened and everything works, that I need to enter this command to keep it saved in case I need to reboot my Pi: 

You might want to have a look at the compoundpi project (full disclosure: I'm the author). It's intended for triggering captures from numerous Pi's with camera modules and uses UDP broadcast packets to get them all triggering as close together as possible. A daemon runs on each Pi which fires up the camera and triggers captures upon receipt of a UDP packet containing the CAPTURE command (other commands are available to configure the camera; the protocol is fairly well documented). A setup using Ethernet is ideal, but wifi will work as well, although you may have to use the time-delay functionality to get decent synchronization in that case (due to packet loss / variable latency). I can't say it's been tested with 100 Pi's - at the moment the biggest setup using it involves 20, but I'd be interested to hear of any issues involving larger scales. The project includes a command line client, a GUI client (written in Qt so it should work on Linux/Mac/Windows but it's only been tested on Ubuntu at this point, and it's still undocumented), and a Python-based client library for writing batch jobs. 

It shouldn't be too tricky to adapt this to loop over the frames of the file, reading and converting each in turn (obviously it'll be a bit slow on a Pi, but you could run this anywhere). 

Now some gentle criticism: I know you stated a desire to avoid Java or Python. Frankly, your reason for doing so, speed, is invalid. Algorithms and critical paths will have a great deal more to do with the execution speed of your code than language selection will. I note that the code above easily manages 30fps on my Pi3 despite being Python and being written in C ... because I'm using the video port and is using the still port (this is what I mean about critical paths mattering more). That's not to say you couldn't go faster still in C - I'm sure you could - but getting your code fast requires more than simply deciding to use C: you need to understand the system you're dealing with (and coders that do will typically be able to generate fast code in whatever language they use; I know C pretty well, but it's extremely rare that I have to resort to it for more speed). Furthermore, I wouldn't characterise this as a "very simple" problem either; almost nothing in computer vision is trivial. For example, if you wanted to detect your red light in all conditions, e.g. during daylight as well as darkness (something the human eye has absolutely no problem with), you'd quickly run into all sorts of fun and games with white balance and auto-gain control (something human eyes still do infinitely better than the current generation of cameras). 

Putting it all together Now, let's put it all together with a button to trigger the recording / playback (we'll use GPIO Zero's event handling for that; note that there's also when_held which might be useful for something): 

Finally, I'd note that works in increments of 1/6th of a stop (so setting it to 6 increases exposure by 1 stop). I'm not sure how the algorithm proposed at the end (average-60)/10 equates to 1/6ths of a stop. The range of possible values that can be produced by that algorithm are -6 (when average is 0) to 19 (when average is 255); that's certainly within the allowable range of values (-25 to +25), but what do the values represent? Oh, one other quick thought: you might want to experiment with the property for adjusting exposures (this is what the AGC algorithm fiddles with by default). There'll be a recipe in the forthcoming 1.7 version which deals with locking down the settings on the camera for consistent shooting too. 

Update: I mentioned that i measured 1kHz constant noise with oscillator from MX-RM-5V module when it is connected to the raspberry pi 3. When the module received something the noise dropped tp 300-400 Hz. I found a visual solution and rather simple way from youtube to really see the noise on the data pin. THE LED. When i connected green LED with a 100 Ohm resistor to the data pin the led is constantly glowing. Its not bright but you see small green light. On this occasion the receiver did not receive nothing. So is was pure constant noise. Next step was to start messing with Low pass filter. I connected 240Ohm resistor and 1.5 uF cap as low pass between data pin and ground. So i connected the greed LED to filtered voltage and the constant noise were filtered. Led is blinking by the data signals only when the transmitter sends something to the receiver. Also, my baudrate in the transmitter and receiver code was a bit high. I moved from 1000 bits/s down to 200 bits/s. From now, the raspberry can detect data from this module and everything is a lot better. 

i have a tiny RF 433mHz project with my Raspberry Pi 3 Model B v1.2 and Arduino Pro mini. My Raspberry works as a receiver and Pro mini as a transmitter. I have wiried the popular RF receiver MX-RM-5V to my RPi's GPIO. Receiver and transmitter modules have coiled antennas. I used the GPIO 5V pin to supply my receiver and logic level converter. I'm using logic level converter because the receiver works with 5V and GPIO with 3.3V logic. The problem is that if i use the GPIO 5V pin to supply my RF receiver module the range drops to 10 cm. When i supply my receiver module with external adapter @ 5V the range increases to 10 - 15 meters. Raspberry has micro usb 2A adapter. The MX-RM-5V uses 11.50 mA. Why the GPIO 5V pin can't supply the MX-RM-5V module ? Is there some component on the raspberry's 5V rail between micro-usb and gpio 5v pin that limits the current ? How much noise the Raspberry generates on its rails and what kind ? Can this be the problem ? 

So i made two tests with oscillator and the problem seems not to be the supply. Test 1: I connected the RX-RM-5V receiver's 5v pin and gnd to the external power supply. Capable going 5A, its laboratory PSU. Then i measured module's data pin with oscillator, and the signal is ok and clean, i saw receiving moment clearly. There is very little noise. The Raspberry was not connected with my module Test 2: I connected the module to the Raspberry Pi 3 's gpio pins. 5v to 5v and gnd to gnd Then again i monitored the modules data pin with an oscillator. Results showed that the data pin had huge constant 1kHz noise on the data pin. Module is receiving data on much lower frequency than the noise. (300Hz-400Hz) So i assume it picks up noise from pi gpio or something. I also made a low pass with 700 Hz cut off and this works ok but i had no luck gaining the distance. The data comes to the receiver between 300-400 Hz frequency. Hard to tell the exact frequency. I tried to make a bandbass filter but the high pass side seems not letting the data through with 20 Hz cutoff. Theres an article on the internet which talks about module for Pi which is basically filter module for RF projects. They have bandbass filter which filters broadband out. So there is something which Pi dont like about RF signals. Right now im waiting two diffrent receivers from ebay... 

Okay, we haven't quite beat Java's time but it's still a significant improvement and the code is still nice and simple, which is always a bonus after optimization! For reference, the above test was run on my development Pi which is overclocked to 900Mhz. 

You should find repeated calls to much faster in this version. If it's fast enough, great! If it's not there's another trick you can use to speed things up which is to avoid JPEG encoding; at the moment, your script is capturing an image from the camera, having the camera encode it as a JPEG, then passing that JPEG to OpenCV to decode back into RGB values before processing it (OpenCV's image format is just a numpy array in BGR order). It would be faster to avoid all that encoding and decoding by having the camera return the RGB values directly. Good luck! 

To the best of my knowledge, the spot used in spot metering mode (not the default) is in the center of the capture area and cannot be moved (or at least I haven't seen anything in the MMAL interface that would allow it to be moved). One of the Broadcom engineers that works on the camera module made some interesting comments about the metering algorithm in this forum post. On the subject of adjusting exposure compensation, that can certainly be done while the camera is running and recording so you could certainly control it based on time of day, results of processing last image, and so on. 

Camera captures an image in YUV format Camera encodes the YUV data as JPEG and sends it to your script OpenCV decodes the JPEG into BGR OpenCV converts the BGR to HSV Numpy is used to average the V values from the HSV array 

starts off as True The loop tests ; it's True so we continue into the loop We read the state of GPIO18. Let's assume it's True for now (i.e. the button isn't pushed, given the reversed logic of a pulled-up circuit) The first statement tests but it's False so we skip down to the next statement. The second statement tests . That's True so we continue into the body of the statement. is called which will halt the program until Enter is pushed. 

The quality issue in Chromium is a long standing bug in Chromium (related issue #1498 in pi linux repo and associated forum thread) to do with the way it handles V4L2 devices that advertise a continuous range of resolutions rather than a discrete set. 6by9's last comment in #1498 has a dirty-hack workaround for it. 

However, how many times you can fold it back and forth before you break it ... well, that's down to your luck! In other words, with your situation (a case forcing the cable to make some uncomfortable turns), I wouldn't expect there's an issue as I assume the cable won't be repeatedly folded or bent, but if you're continually installing and uninstalling the camera, you might want to buy some spare cables! 

The output from this on my Pi (which is a Pi 1 model B overclocked to 900Mhz, "Medium") is as follows: 

First, I'd query what it is you're trying to accomplish with . By default, the camera runs an AGC algorithm (when is set to anything except ) which does a pretty good job of figuring out the exposure time. As I understand it, the property is used when you want to deliberately under or over expose the resulting image. Generally the best way to get a decent exposure time is to insert a delay for a few frames after initializing the camera (this is why most of my examples include something like after init; 2 seconds is complete overkill but it gets the point across that some delay is usually required). Still, leaving aside the question of "why?" for a second, let's deal with the "how". I don't see anything wrong with your code, but we can probably speed it up a bit with a couple of tricks. Firstly, consider the stages that your capture is passing through: