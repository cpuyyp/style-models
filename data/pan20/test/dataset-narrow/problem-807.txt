Other questions you need to ask: what is your acceptable false-positive and false-negative rate (how much legit mail are you willing to lose and how much junk are you willing to accept?) What additional latency are you willing to accept? Some very effective low-falsing anti-spam techniques (e.g. greylisting) can delay mail. This can irritate some users who (unrealistically) expect email to be immediate communication. Reflect on how much you wish to externalize your costs when planning an anti-spam system. For example, ipfilter-based blacklists are unforgiving but do not materially affect any other system. Greylisting conserves sender and recipient bandwidth but keeps mail in remote queues longer. Mail bounce messages and challenge/response systems can be (ab)used to mailbomb an unrelated third party. Techniques like tarpitting actively externalize costs by intentionally holding open SMTP connections for long periods of time. DNSBLs require you cede some amount of control to a third party (blacklist maintainers) but ultimately as a mail admin you are responsible for explaining your blocking policy to your users & management. The upshot is that there are ethical considerations that go along with each technology and it's important to be aware of your effect on others. How tolerant will you be toward misconfigured external systems? (e.g. those without FCrDNS, broken HELO/EHLO strings, unauthorized pipelining, those that don't properly retry after a temporary failure code 4xx, etc.) How much time, money, bandwidth, and hardware do you want to devote to the problem? No single technique is effective, but a concerted defense-in-depth approach can substantially reduce inbound garbage. DNSBLs, URIBLs, greylisting, content filtering, and hand-tuned white- and blacklisting work well on my small domain, but I can afford to be more liberal in what I reject. Unless things have changed recently, blacklisting IPs by country of origin is not terribly effective. I had the idea of using ASN and OS fingerprint (via p0f) to judge the quality of an inbound connection but didn't pursue it; the statistics would be interesting to look at but I'm not convinced it would be any more useful than the standard techniques already described. The upside to using GeoIP, ASN, and OS fingerprint info is that while they may individually be weak predictors of connection quality, they are available at TCP/IP connection time, long before you reach the SMTP layer (fsvo "long".) In combination, they may prove to be useful and that would be helpful because spam becomes costlier to block as it approaches the end user. I'm not trying to be a naysayer; 'oddball' character encodings and GeoIP information probably correlate well to spam but may not be reliable enough to use as single criteria to block mail. However they may well be helpful indicators in a system like Spamassassin. The takeaway is that spam defense is a complex problem in cost-risk-benefit analysis and it's important to know what your values are before implementing or changing a system. 

In the end, I have put a script in init.d and used the runlevel to execute it first. The script updates the jvm.options file using sed like so: 

Does anyone have any ideas on why the memory usage is constant like this and not peaks and troughs with usage of the app? Here's the output of the MySQL Tuning Primer script: 

So it follows that if I run a script to set the ES_JAVA_OPTS at startup before the elasticsearch service starts. I tried a lot of things to try and get the ES_JAVA_OPTS to be set but when looking at the logs I can see it was using the default. I've tried various things: 

This is an ASP.NET YSOD. I am not sure why ASP.NET is getting involved at all as it's a static .jpg file I'm requesting. I tried turning on failed request tracing and this is the specific error: 

I am using DD4T on an SDL Tridion project and am using the following configuration in the storage config in order to publish Binaries (binaries in this case being anything store in Multimedia Components) to the filesystem but keep Pages in the Content Delivery database. I am finding that as requirements change for what Binary files are needed e.g. the customer wants to offer Adobe Illustrator files for download, I am needing to add more types to the list by changing the config and restarting the deployer which is not ideal. 

We had a major network issue where our secondary domain controller (responsible for Win2k3 boxes) died and had to be rebuilt (I beleive this is what happened, I am a developer not network admin). Anyway, I am working remotely via VPN at the moment and since this happened, I am getting an authentication box when trying to access certain areas of SDL Tridion via IE (Tridion 2009 SP1 is IE only) it seems like somewhere my credentials are not being passed correctly or the ones cached on my laptop do not match the ones the Domain Controller has. This only seems to affect Windows 2003 servers. Our IT support thinks that the only way to sort it out is to connect my laptop directly to the network. I am not planned to go to the office for a few weeks at least and this issue means I have to work with Tridion via Remote Desktop. We thought changing the password on my account might work but this didn't help. So basically my question is, is there any way I can reset my credential cache without having to reconnect to the network? Or is it IE that is causing the problem perhaps, since I can RDP to servers and use Tridion 2011 instances in other browsers fine? I am on Windows 7 using SonicWall VPN client. 

I have found a lot of value in SAGE, USENIX, and LOPSA, primarily in the degree of professionalism, technical skill, and support within the community. In 1998 I moved from being a statistician and web developer for a large dot.com to being a sysadmin and had the great fortune to have an employer that would pay my way to LISA. In 2001 I presented a paper on reliability modeling; the next year I was on the LISA program committee. At present, I'm on LOPSA's Board of Directors. Speaking only for myself, your membership in a professional organization is what you make of it. The national organizations may focus on issues that seem much higher level than the focus of the average sysadmin, but in some ways that's their purpose - to look at how other professions have developed, to find ways to raise the standard of performance of the profession, to focus on notions of ethics and standards of conduct, to show the state of the market (I'm thinking specifically of SAGE's annual salary survey), etc. My feeling is that the bulk of sysadmins are in your position - it's unlikely they'll fund a trip to LISA even though the "hallway track" is well worth the price of admission. And again, not stumping for my organization specifically, there's a great value in organizing your local sysadmins. I suggest informal dinner, drinks, and "recovery" (aka cathartic ranting) to start with. I know from my experience with Austin's LOPSA chapter that just getting a few sysadmins around a table with burgers and beer does wonders for establishing a community without flying halfway across the country to the Big Conference. Rarely are the sysadmins in competition with each other, even if their employers are. Usually people are very willing to help each other because there's a sense that we're all in the same boat with regards to management, budget, users, vendors, software, etc. And when push comes to shove in the market, it's better to have contacts who know you and your skill level and your personality before you need a job than after. IMO the national groups should foster the development of local groups, I have championed this within LOPSA though I have not been as successful as I would have liked and I blame nobody but myself in that regard. Regardless, you don't need a national organization to build a local community. Find four people, meet regularly, and build up as you can. Where a national organization can help is to contact members that live near you and help build your local. They can also provide speakers and suggest presentation topics and lessons learned from other locals. But ultimately what you get out of the organization is what you put into it, whether that's writing a paper, volunteering, forming a local organization, or sitting through interminable teleconferences trying to set national policy and administrative hoo-hah ;) The Big Conference is but once a year but your local peers are near you all year round. While I may be an advocate for the national organizations in general and one in specific, I strongly encourage you to work locally to develop a community that supports each other and raises the standard of excellence of the sysadmin profession. If you have to choose between action and membership, choose action. Again, this is my personal opinion, not that of LOPSA nor the LOPSA Board. Apologies for the disclaimer but you know those lawyers and their picky "whereby"s and "heretofore"s. :) 

But I think this is really dirty. There must be a nicer way of doing this? I tried setting the ES_JAVA_OPTS in the same way but it didn't work. My ultimate aim is make scaling up easier. 

If I change the "Physical Path Logon Type" from ClearText to Network. I get the following IIS error: 

I have a virtual directory in my site (test environment). It is a UNC share which is also used as a public FTP. It is configured to connect as a domain admin account and "Test settings" says everything appears to be working. However when I try to connect to it I get: 

This does not generate a failed request log strangely enough - I have set the failed request tracing to trace errors with error codes 400-999. Also worth noting is that if I open the Configuration feature from within IIS I see an access denied error. I have exactly the same set up on my local dev machine to the same UNC path and the same user it works. Just on the test server it does not. What am I doing wrong? 

Our MySQL server seems to be using a lot of memory. I've tried looking for slow queries and queries with no index and have halved the peak CPU usage and Apache memory usage but the MySQL memory stays constantly at 2.2GB (~51% of available memory on the server). Here's the graph from Plesk. Running top in the SSH window shows the same figures. 

I am trying to automatically set the heap size on my ElasticSearch v5.6.3 nodes (Ubuntu 16) The machines are hosted on Azure and I want to do this so that when I scale up the machine, it automatically sets the heap size to an appropriate level without needing to manually open the /etc/elasticsearch/jvm.options file and set it and then restart the service. As far as I can work out the suggested ways to set the heap size (in ElasticSearch 5) are: 

Without going into detail, this is definitely an issue to take to management if it can't be addressed effectively with the individual in question. I would try to engage the other person and give him the opportunity to explain his rationale', however, given the arrogance and ego often found in our industry, it wouldn't surprise me if he didn't appreciate such challenges to his "authoritay." At that point, you should build a solid logical, financial, and pragmatic case to counter his broken practices and you should suggest a few alternatives (NB: complaining without suggesting solutions is called 'whining'.) At the very minimum, get a copy of "The Practice of System and Network Administration" and start looking for best practices. Put your concerns down in writing. Each organization has different attitudes toward 'argument from authority' (e.g. do textbook best-practices trump those from the local sysadmin or management or developers or Marketing?) If possible, leave a way for the senior admin to 'save face' to make it easier for them to change, without necessarily admitting they were wrong.) I wouldn't worry so much about not 'being on the team'; it's one thing to be a destructive, disruptive or obstructionist element; it's quite another to make change a positive experience for everyone involved. That said, depending on how dysfunctional your organization is, your choices may be to change your job or change your job - either fix it or find someplace else less broken. That's probably an extreme case though; keep it depersonalized and positive, listen more than you speak, and above all keep a paper trail. :) 

Tangentially related, depending on the number of servers, the number of admins, and whether all admins need access on all servers, consider a configuration management system such as (in no order of preference) bcfg2, cfengine, lcfg, or puppet. You can start small, managing admin accounts and keys as well as restricting ssh logins with the AllowUsers and AllowGroups directives in sshd_config. Usually there's never enough time to learn to configure or deploy a CM system but if you target a test group of 2-3 machines and work at managing simple things at first (static files like authorized_keys, resolv.conf, etc.), you'll be surprised at how quickly you can get it implemented and ready to deploy to more systems. As more systems are brought under central management you'll also be surprised at the subtle misconfigurations which will turn up. What does this have to do with setting root access? Simple: Once you manage configurations from a central system, it's much faster and easier to manage keys and access. Auditing is simplified which may or may not be important to you. Further, if you couple a CM system with an automated build system like Kickstart or Jumpstart, you can seriously reduce backup overhead and server deployment time. Access control is an important facet of configuration management; sometimes it helps to have a glimpse of a bigger picture.