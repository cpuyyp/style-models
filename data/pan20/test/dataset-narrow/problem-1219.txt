Computing with trits instead of bits is like computing with Turing machines that have an alphabet size of 3 instead of an alphabet size of 2. Increasing the alphabet size like that (not necessarily 2 to 3, but 2 to larger) can permit a speedup in running time, and a compression in the use of space. (See $URL$ for some initial discussion.) Note that these improvements are fairly "minimal" -- they won't make an intractable problem tractable. (If you use an intractably large alphabet instead of computing over bits, that's "cheating," because then you are just transferring the hardness from one part of the computation to another.) Qubits at least appear to have a fundamentally different "character" from bits or trits. The state of a qubit (or, more generally, a quDit, "quantum digit") is a collection of probability amplitudes that associates the qubit to some extent with each possible state. Bits, trits, etc., are crisply in one state, the end. In particular, if Factoring is not in polynomial time, then there exists no ptime algorithm over trits that factor integers, while there does exist a quantum algorithm over qudits that factors integers. $URL$ 

My take is that the abstractly-motivated Turing machine model of computation was a good approximation of technology until very recently, whereas models of distributed computing, from the get-go, have been motivated by the real world, which is always messier than abstractions. From, say, 1940-1995, the size of problem instances, the relative "unimportance" of parallelism and concurrency, and the macro-scale of computing devices, all "conspired" to keep Turing machines an excellent approximation of real-world computers. However, once you start dealing with massive datasets, ubiquitous need for concurrency, biology through the algorithmic lens, etc., it is much less clear if there is an "intuitive" model of computation. Perhaps problems hard in one model are not hard -- strictly less computationally complex -- in another. So I believe that mainstream computational complexity is finally catching up (!) with distributed computing, by starting to consider multiple models of computation and data structures, motivated by real-world considerations. 

I'm interested in finding the minimum number of rectangles, the "best" set of rectangles for any notion of best, whether this problem becomes easier or harder for different classes of shapes. Thank you. :-) 

I am (slowly) writing a review of the Handbook of Chemoinformatics Algorithms for SIGACT News. One chapter discusses current software implementations, and the database searches (and other applications) don't seem to take advantage of as much information about the graphs as they could. On the other hand, perhaps more theoretical algorithms would be too hard to implement. It seems like a potential open area, though. So here's my question: Is there an overview (or a small handful of references) that discusses theory and implementation (hopefully) of algorithms of databases of graphs with metric information? (Each edge is a distance, and each vertex has a volume.) A chemistry-free description of an example problem would be: given a database of graphs, find all of them that contain a particular subgraph. 

"My question is what is the importance/effect of amino acid A occuring at position I in the sequence with respect to the model?" I suggest Monotony and Surprise by Apostolico for the modeling of importance of words appearing at certain positions, or in certain patterns. Beyond that, I'm not sure what you're looking for. 

I think you want a Petri net with transition guards and a global clock. (Other formalisms would also work, but I'm going with this one because it relates to the discussion on your other question.) Draw a graph -- vertices and directed edges -- where the vertices are your nodes, and the edges are the communication channels from one node to another. It sounds as though there's an edge between every node, if all nodes are capable of broadcast, but perhaps I'm misreading. In any case, the important thing is that you decorate each edge with a rule or set of rules, and the edge only "opens" when the rules are satisfied. (These rules are called guard expressions.) For example, to make "vary the amount of time a host caches responses" rigorous, you could make the rule for an edge, "if I receive a message, I wait 3 seconds according to the global clock before I pass it on." I don't understand your dirty-cache issue well enough to offer anything yet, but I think the conceptual point you need first is this notion of enabling or disabling a transition according to the rules of the system. 

While I agree with David Eppstein's response in general (and I upvoted it), the emerging field of automata that define biological processes and other natural computing "things" is a vibrant area. Getting hired later is not something I can speak to, but you might be interested in taking a look at Artificial Biochemistry by Luca Cardelli, or Efficient Turing-universal computation with DNA polymers by Qian et al. The first paper is Cardelli's latest attempt to provide formal methods to biochemical processes; the second, a theoretical DNA implementation of a stack machine. 

It sounds as though your first problem is the computation of the maximum cycle mean. I am no expert, but I know the problem is well studied. One paper on the subject that is not behind a paywall can be found here. 

I received the above question this morning by email. It was sent to a SIAM list service I belong to. I don't know the asker, and I don't know a good answer either. I figured, though, if some people here had good suggestions, either theoretical or practical, I could send him the link to this question. Thanks. Edit: I sent the asker a copy of this question, with Artem K's answer visible. He responded, saying thanks. 

The information content of self-similar fractals is a research topic of current interest in computable analysis. In particular, by generalizing Hausdorff dimension to a concept of dimension-relative-to-computational-resource-bounds allows one to talk about the dimension (information content) of individual points, or sets of points, within the fractal, instead of just the overall geometric dimension of the fractal itself. For example: Dimension of points in self-similar fractals, Lutz and Mayordomo, SIAM Jnl Computing, 2008. 

This might be cheating, because I am reinterpreting your notion of "number" as "countable ordinal." However, there is an extensive theory about exactly what you are asking about in your question -- which ordinals can be defined, with which definition-strengths. I looked at Wikipedia, and the large countable ordinal page has a lot of information. In particular, the "smallest number that is interesting because the system can't even prove it's a number" corresponds to the unrecursable ordinals, if we require nothing other than computability as the resource bound. 

$(x,y) \in R_i$ implies $(y,x) \in R_i$ for all $x, y \in V$. $R_0 = \{ (x,x) \mid x \in V \}$ If $(x,y) \in R_k$, the number of $z \in V$ such that $(x,z) \in R_i$ and $(y,z) \in R_j$ is a constant $c_{ijk}$ depending on $i$, $j$ and $k$, but not on the choice of $x$ or $y$. 

MIXCOLUMNS prevents attacks that focus on only a few S-boxes, because the mixing of the columns requires all S-boxes to participate in the encryption. (The designers of Rijndael called this a "wide trail strategy.") The reason analysis of an S-box is hard is due to the use of the finite field inversion operation. The inversion "smooths out" the distribution tables of S-box entries, so the entries appear (almost) uniform, i.e., indistinguishable from a random distribution without the key. It's the combination of the two features that makes Rijndael provably secure against known attacks. As an aside, the book The Design of Rijndael is a very good read, and discusses the theory and philosophy of cryptography. 

The Lamarckian Genetic Algorithm is used in chemoinformatics to screen for potential new drug compounds that can bind with a particular receptor. The computational problem is to search through a chemical database for candidates that can orient correctly (wrt the possible orientations of the molecule containing the receptor), and to combine that with a conformational search (i.e., one that considers the possible rotatable torsions of the molecule, which can strongly affect the reaction). Previously, it was feasible to perform either an orientation search or a conformation search, but not both. LGA takes advantage of computer speedup, and combines the global search of a genetic algorithm with a local search. 

An example of the application of the theory of association schemes to complexity theory is the following result of Evdomikov from 1994: 

I imagine this must be an introductory computational geometry question, but I'm not sure of the best search phrases, and I'm interested in variations of the question, also, so I'm hoping for pointers to useful references. I'm interested in feasible algorithms for the following problem. 

There is (perhaps) a distinction to be made between "Turing computable" and "effectively computable" in order to answer your question. If one defines "random process" as "a process that cannot be predicted, no matter what resources we have," and one defines "deterministic process" as "predictable process, given the input and access to (maybe a lot of) resources," then no Turing computable function can be random, because if we knew the Turing machine and simulated it, we could always predict the outcome of the next "experiment" of the process. In this framework, a Martin-Lof test can be seen as a deterministic process, and the definition of a random sequence is precisely a sequence whose behavior is not predicted by any Martin-Lof test/Turing computable/deterministic process. This, however, begs the question: "Is a random sequence effectively calculable, in real life?" There is, in fact, an industry here. There are published CD's with billions of random (?) bits on them that are used to perform computer simulations of physical systems, etc. These CD's guarantee that their sequences of bits pass a bunch of Martin-Lof tests. The book The Drunkard's Walk: How Randomness Rules our Lives gives a pop-sci explanation of this issue, in greater detail. Irrelevant point: I enjoy your column. :-) 

Thanks in advance. I'll be offline for a day or two, I'm afraid. If there are issues with this question, I'll address them within 72 hours. 

Probably the best collection of links in one place is the Further Reading section of the wiki that was put together to help assess Deolalikar's claimed proof that $P \neq NP$. Good luck. The problem appears to be hard. :-)