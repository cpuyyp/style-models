So far I know that the answer is finite, and the best upper bound I can prove is $k^{(1.5 k!)^2}$ (see below). I'll also just point out that this is a different question than the one talked about by Gasarch frequently on his blog (and in this paper). He wants to avoid all monochromatic rectangles, whereas I don't mind monochromatic rectangles, it's just the "broken" ones that I want to avoid. What is the motivation? In cryptography, we consider the problem of Alice (who has $x$) and Bob (who has $y$) both learning $f(x,y)$ for an agreed-upon function $f$, in such a way that they learn no more than $f(x,y)$. You can associate $f$ naturally with a 2-dimensional table, hence, a grid coloring. There are characterizations for this kind of problem of the following form (but with different notation): "$f$ has some cryptographically interesting property if and only if $f$ contains a broken rectangle." For examples, see Kilian91 and BeimelMalkinMicali99. So this problem has come up in some setting of cryptography that I was investigating. For my purposes, it was enough to know that there are a finite number of grid colorings that avoid broken rectangles and duplicate rows/columns. But I thought the combinatorial problem itself is interesting and I believe better bounds should be possible. The best bound I can prove: Define $R(2)=3$ and $R(k) = k \cdot R(k-1)$; hence $R(k) = 1.5 k!$. First, one can prove that if $C$ is a $k$ coloring with at least $R(k)$ rows, then it either has a duplicate row or a broken rectangle. Symmetrically, one can show the same thing with respect to columns. (The proof is pretty basic, following from the pigeonhole principle on the # of colors.) From this, we know that the colorings we care about all have dimensions smaller than $R(k) \times R(k)$, and we can get a very loose upper bound of $k^{R(k)^2}$ such colorings. I think this can be improved in two ways: First, I think the optimal value of $R(k)$ is $2^{k-1}+1$. Below is a (recursively defined) family of colorings, where $C_k$ is a $k$-coloring of size $2^{k-1} \times 2^{k-1}$ that avoids these forbidden features: $ \qquad C_1 = [1]; \qquad C_k = \left[ \begin{array}{ccc|ccc} k & \cdots & k & \\ \vdots & \ddots & \vdots & & C_{k-1} \\ k & \cdots & k & \\ \hline & & & k & \cdots & k \\ & C_{k-1} & & \vdots & \ddots & \vdots \\ & & & k & \cdots & k \end{array} \right]. $ I believe these to be the largest $k$-colorings that avoid these forbidden structures. Second, even if one could improve the bound on $R(k)$ described above, we still have the fact that $k^{R(k)^2}$ is a very coarse bound for the total number of colorings. This counts all possible $R(k) \times R(k)$ grid colorings, of which a large portion presumably have the forbidden features. 

There are plenty of results for muli-class learning with with fixed discrete labels: $$ \text{Standard multi-class classification:} \begin{cases} f: X \rightarrow Y_{index} = \{1, 2, 3, ..., k \}, \\ s.t. X \subset \mathbb{R}^{d_x} \end{cases} $$ I was wondering if there are any results which study multi-class learning with (countable, possibly finite) vector-valued outputs. $$ \text{Vector-output multi-class classification:} \begin{cases} f: X \rightarrow Y_{vector} = \{y_1, y_2, y_3, ..., y_k \}, \\ s.t. X \subset \mathbb{R}^{d_x}, y_i \in \mathbb{R}^{d_y} \end{cases} $$ One may suggest that this is another problem: metric-learning between $X$ and $Y$ space: $$ \text{Standard metric-learning problem:} \begin{cases} f: X, Y \rightarrow [0,1], \\ s.t. X \subset \mathbb{R}^{d_x}, Y \subset \mathbb{R}^{d_y} \end{cases} $$ However all the results I came across in the metric-learning study the functions on one single vector space (as in unsupervised metric learning). In addition, vector-output multi-class classification is much more restricted (easier to learn?) than metric learning, since in what I am asking, the number of possible output vectors is fixed (unlike metric learning). 

This is not my area, so apologizes if I am asking nonsense! I know that there are very good solver/theorem provers for solving 1st order logic. Now I have a problem, using 3-valued logic, but I am not sure if there are some practical ways of solving 3-valued logic (or converting it to a bigger 2-valued logic problem?) EDIT1: Is it possible to translated (~reduce?) any 3-valued logic to 2-valued logic? Can this be generalized to K-valued logic, where K is any arbitrary integer? (What about the case when K is infinity?) Any idea or reference is appreciated. 

I think this is a typo. In the Definition 2.1 and Definition 2.2, the estimation of $c_D$ is done by empirical summation of $\mathbf{1}(c(x_i) = y_i)$. Later in the Definition 3.1 it used the following notation : $$ ... = \Pr_{...} \left( \sum_i Z_i < k \right) = ... $$ In this notation $Z_i=1$ corresponds to correct decisions or $c(x_i) = y_i$, and $Z_i=0$ corresponds to the mistakes or $c(x_i) \neq y_i$. I suppose that in this binomial representation $Z_i = 1$ (correct decision) corresponds to tail"). That said, the right sentence seems to be "... having k or less errors is at least $\delta$" 

Here's another example of a tight bound having a log factor. (This is Theorem 6.17 from Boolean Function Complexity: Advances and Frontiers by Stasys Jukna.) 

and they show that this is the best possible relation between the two measures, since they exhibit a function $f$ which satisfies 

Edit: Since I haven't received any responses/comments in a week, I'd like to add that I'm happy to hear anything about the problem. I don't work in the area, so even if it's a simple observation, I may not know it. Even a comment like "I work in the area, but I haven't seen a characterization like this" would be helpful! Background: There are several well-studied models of learning in learning theory (e.g., PAC learning, online learning, exact learning with membership/equivalence queries). For example, in PAC learning, the sample complexity of a concept class has a nice combinatorial characterization in terms of the VC dimension of the class. So if we want to learn a class with constant accuracy and confidence, this can be done with $\Theta(d)$ samples, where $d$ is the VC dimension. (Note that we're talking about sample complexity, not time complexity.) There is also a more refined characterization in terms of the accuracy and confidence. Similarly, the mistake bound model of online learning has a nice combinatorial characterization. Question: I want to know if a similar result is known for the model of exact learning with membership queries. The model is defined as follows: We have access to a black box which on input $x$ gives you $f(x)$. We know $f$ comes from some concept class $C$. We want to determine $f$ with as few queries as possible. 

See Efficient Quantum Algorithms for Estimating Gauss Sums by Wim van Dam and Gadiel Seroussi. They have a nice introduction to the computational problem of estimating Gauss sums over finite fields and finite rings. They carefully explain how the computational problem is specified, how the characters are specified in the input and what the input size is. They show how to estimate the phase of a Gauss sum to inverse polynomial accuracy in polynomial time on a quantum computer. They then show that this problem is classically hard assuming discrete log is hard. 

Given the latest formulation of your question, this appears to be impossible. Consider the case where you have (families of) cyclic groups $\mathbb{G}$ and $\mathbb{H}$, where $\mathbb{G} \ne \mathbb{H}$ and we have a bilinear map $e: \mathbb{G} \times \mathbb{H} \to \mathbb{T}$. Under the XDH assumption we can suppose that DDH is hard in $\mathbb{G}$ and discrete log is hard in $\mathbb{H}$. Let $g$ be a generator of $\mathbb{G}$ and $h$ be a generator of $\mathbb{H}$. Then define $f : \mathbb{Z}_{|\mathbb{G}|} \to \mathbb{H}$ as $f(a) = h^a$. Now given $(g, g^a, g^b, g^X, z=f(a)=h^a)$, we can easily determine whether $X = ab$ by checking $e(g^b, z) \overset{?}= e(g^X,h)$. (You can also similarly verify the correctness of $z$ if you like.) Yet, it would seem unlikely that an extractor could extract $a$ or $b$ from such a tuple. Extracting $b$ is obviously equivalent to discrete log; if there is a distortion map from $\mathbb{H}$ to $\mathbb{G}$ (there cannot be one in the other direction) then extracting $a$ is equivalent to discrete log (in $\mathbb{H}$). 

The field of Resource-bounded measure applies Lebesgue measure to complexity classes. The idea is to obtain separations among complexity classes by talking about the relative "sizes" of these sets. 

Real-world cryptographic hash functions like MD5 & SHA are keyless. As such, it makes it quite difficult to apply techniques from theoretical cryptography to reason about their security. The simple reason why: for any keyless hash function, there exists a very small program/adversary which outputs a collision under that hash function; namely, a program that has such a collision -- which must exist! -- hard-coded. Phil Rogaway's paper Formalizing Human Ignorance: Collision-Resistant Hashing without the Keys deals with this problem. In it he shows that some very standard theorems for keyed hash functions (like the Merkle-Damgård construction & hash-then-sign paradigm) can be adapted and re-proven with "intuitionist-friendly" theorem statements applying to unkeyed hash functions. 

Without knowing any further restrictions on the parameters, the problem is NP-hard. Let $G = (V,E)$ be an undirected graph. For $v \in V$, define the set $S_v = \{ e \mid e \in E \mbox{ is incident to } v\}$. Then $| S_u \cap S_v | = 1$ if $u$ and $v$ are adjacent, and $|S_u \cap S_v| = 0$ otherwise. You can pad out each $S_v$ with distinct junk items until each set has the same size. Then an ordering of $\{ S_v \mid v \in V\}$ in which adjacent sets have "as large an intersection as possible" corresponds to a Hamiltonian path in $G$ (for sensible measures like maximizing $\sum_i |S_i \cap S_{i+1}|$ or maximizing $\min_i |S_i \cap S_{i+1}|$). If you want adjacent sets to have "as small an intersection as possible", simply complement each of these sets and the same reduction also works. Your question suggests that you think this problem can be done in $O(m^2)$, and now I'm curious why. 

The famous FTPL algorithm [1] is analyzing linear cost function. Is there any generalized proof for nonlinear functions known? Note that in the last paragraph of [1] it says "It would be great to generalize FPL to nonlinear problems". In [2] (page 69, 70, 71) there is analysis of FTPL for general functions, but the resulting bound $O(T)$ does not attain the optimal bound (i.e. $\sqrt{T}$). Are you aware of better analysis for FTPL (with general nonlinear functions) that attain the optimal regret? Update: The algorithm in [2] is indeed $O(\sqrt{T})$ (by choosing $\eta = 1/\sqrt{T}$) but it is calling the linear optimiazation oracle many times (not just once as in [1]), which results in having a different bound. [1] $URL$ [2] $URL$ 

Suppose I have a collection of (hidden) first-order rules: $$ \mathcal{R}: \{ Q_i(x) => P_i(x) \}_{i=1}^{k} $$ all defined over $x \in \mathcal{X}$. I can use these rules and (automatically) generate a large collection of (training) data for my supervised system: $ \mathcal{D}: \{(x_i, y_i)\}_{i=1}^{n} $, say for $y_i \in \{-1, +1\}$, and run a supervised system on this sampled data, and test on a heldout set. If my rules are compatible (not contradictory) a rule-based system should be able to get a perfect score on the sampled set. However, I am not sure how would a supervised do on this. Are there any possibility/impossibility on the ability of supervised systems for learning first-order rules (possibly with some assumptions) and based on finite samples? This is basically the reverse of rule-learning, in which the goal is to learn some rules $\mathcal{R}$, given a training data $\mathcal{D}$. I did a little bit of Googling but didn't get anything directly relevant (all I found was algorithms for first-order rule induction or training supervised systems that use 1st order rules as features). That said, it's possible that I am missing some results on this. Would appreciate any thoughts on this. 

Is there a direct conversion between gradient descent ([1], Alg 1 ) and any of the following algorithms? 1) Weighted Majority: $URL$ 2) Follow the Leader: $URL$ I keep seeing these together, but I don't see the direct conversion. [1] $URL$ 

The big open problem is to show an oracle separation between BQP and PH. But we don't even have a separation between BQP and AM (since AM is in PH, this should be easier). Even worse, make BQP considerably more powerful by allowing 1 round interactions with Merlin, giving you the class QAM or QIP(2) (depending on public coins or private coins) and we still don't have a separation. Known: 

I guess there are at least 3 questions in your question. I don't have a satisfactory answer to all of them, so this isn't a complete answer. Hopefully there will be more answers that answer all your questions. The question in the title: Can quantum algorithms with exponential speed-up be rederived using span-programs? As you noted, the general adversary bound characterizes the quantum query complexity of all decision problems, including promise problems for which we have exponential speedups. So, in principle, there is a span program that solves the Abelian hidden subgroup problem, which is the query problem used in Simon's and Shor's algorithms. But whether there is an explicit span program for this is your next question. Is there any work re-deriving Simon's or Shor's algorithms in the span-program framework? I haven't heard of any such results. I don't know of a span program for Simon's problem or any other AHSP. Is there a way to translate polynomial-method lower-bounds to general adversary lower bounds? Yes, I believe there is. I can't seem to find the paper that has this result, but I can give you a link to a talk given by Jérémie Roland. In the abstract of the talk, he says the following: 

Suresh's answer about the AKR conjecture got me thinking about the same conjecture for hereditary properties. I think (unless I've made a mistake) I can show that all non-trivial hereditary properties have (randomized and deterministic) decision tree complexity $\Theta(n^2)$, which settles the AKR conjecture for such properties (up to constants). I tried to search the literature to see if this has been shown somewhere, but I couldn't find a reference. So either I couldn't find it but it exists, or the theorem is uninteresting, or I've made an error. So, this is another example of a global property of all hereditary graph properties.