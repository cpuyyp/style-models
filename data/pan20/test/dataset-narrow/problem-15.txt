Before every production release, you should run a full suite test on all projects in your infrastructure. This is the only way to be certain that all project dependencies are non-breaking. Tests/provisionings must run in parallel. By doing this, your full repository testing speed will be the length of the longest test/provisioning in your repository. (As a side effect, this puts an emphasis on speeding up your slowest test) 

I pulled this from the Firefox Chef cookbook (in particular this .rb file). This an example of the request they use to get the latest version based on OS and language. 

For your own personal use, I think a GitHub repository with all of your bootstrap and installation scripts is a great idea. It serves as a portable way to download and distribute any scripts you need where firewalls and restrictions aren't an issue. As for within your organization, one suggestion that your IT might be open to as a compromise is to create a machine (barebones, VM, etc.) that is solely dedicated to downloading and storing the artifacts that you need. The criteria for this machine would be: 

We faced the same problem in the infrastructure which we're building. So, we had style blocks to set the bid price, depending on the on-demand price of the instance. AWS has an API for getting the on-demand price of an instance. We used this Python wrapper for the purpose. So, once we got the on-demand price (let's say ), we plugged in style blocks, which are , , , , which means we are trying for a bid price in the range 40%, 60%, 80% of the on-demand price. If all fails, then we are falling back to creating on-demand instances. Also, as this is independent of AWS's current spot pricing, we never pay a price above the on-demand price. But, if you're looking for a way to do it on-the-fly, then Hashfyre's solution should be the way to go. 

With this setup, you could download something like the latest version of Python onto the server (via logging in with credentials, API, etc.) and have it store Python in a central location for your other servers to download and install. You could store your artifacts in a multitude of ways (artifact repository, cloud storage, file server, etc.), but only that server should be able to upload to your storage. Your IT team could even set alerts when a new artifact is downloaded on this server or uploaded to your storage so that they can inspect it. I think this approach demonstrates a few things to your IT department: 

If testing locally isn't an option, then the most straight forward approach would be to use disk volume snapshots/backups to your advantage. These will still cost $$$, but will save you time in the long run. You should then separate your bash script into different working segments/scripts that can be tested individually. Once your server is provisioned, run a script, then take a snapshot. If it was successful, run the next script, take a snapshot, then rinse and repeat. If your script fails, modify the script, revert to the last successful snapshot, then try again. NOTE: I'm not sure if you can take snapshots of virtual machine disks in IBM Cloud/Softlayer, but it looks like you can create a VM image pretty easily. 

I have a set of rules in my AWS Config dashboard. And, I want to set a AWS CloudWatch alarm to be triggered whenever Config detects non-compliant resource(s). [The plan is to link that alarm to an SNS topic for sending out emails to the team for any non-compliant event/resource] Is there a straightforward way to do that? Or is there a workaround for the same? 

Note: This might not be the best way to handle your problem. But, this was a hack which helped me do my logging and monitoring. 

I went through the available metrics in the Cloudwatch dashboard, but haven't found anything related to AWS Config. 

where is any of the previous tasks. So, how do I set here? cause, Any of my 3 tasks can run. So, it can be or or . So, how do I dynamically write that in the adding task after them? 

I searched around for official api documentation, but couldn't find anything. If I do, I will post a link to it in this answer. 

I think a key difference between Terraform Modules and Workspaces is that modules can be inherited and used by other modules and configurations. Terraform Workspaces are intended to be a collection of configurations that represent a single environment, whereas Modules are components that can be utilized by one or more modules/configurations. I think you could use Modules in a similar way to how you are using Workspaces, but you would be contradicting the intent of the Modules. For example, say that you want to create a base configuration file for all your company's EC2 instances that allows traffic over SSH. You would create a module that spins up a VM, modifies your firewall to allow SSH, and any other configuration settings that you would want all of your instances to have. You would then have every VM Terraform config inherit this file. This would minimize the amount of code that you would have to write and allow for any changes that you would want across all of your TF configs to be made in one central module. 

I have multiple servers, each having a script polling an SQS queue [all polling the same queue]. So, is there any way I can ensure an equitable distribution of messages to all those clients [i.e. my worker servers here]. Like for example, if there are 100 messages in the queue, then if there are 5 workers, and so on. Can AWS ELB (Elastic Load Balancer) help me do it? If yes, then how? If not, then is there an alternate service in the AWS ecosystem which can help me do it? Or am I overthinking this? I mean, can this be solved straightforwardly in the polling script? [Please keep in mind the race conditions involved due to multiple clients polling a single queue] 

Is there an elegant way or a best practice when it comes to deleting old images from the Docker registry? I see a lot of requests/issues here: $URL$ but didn't find a good/popular solution for it. So, is there a tool or a technique which would help me do that? Also, is there any best practices which you follow while doing it? 

Monorepos are nice because it eliminates the technical constraints between multiple projects. This does however open the door to other complications within your repository (naming conventions, cross-team dependencies, merge conflict increases, etc.). I do not have any experience with CircleCI, but I will provide some input based on other CI tools I have used. 

I am creating a design proposal for a data lake hosted in S3. My goal is to create a flat data lake that will host a variety objects within it. The majority of these objects will be made available to the general public, but some will be for internal use only. My proposal is to have a bucket policy that allows only public access for items with a public=true tag implemented. However, this tag should only be able to be changed by users with the correct privileges. Is this something that is possible in S3 with IAM, and if it isn't, what other options that can be utilized to control private/public objects in an S3 data lake? 

Velocity Conference is one of the most popular Devops conferences, and also is an O'Reilly conference. From it's website: 

Also, having failover mechanism would help too, like how the answers in this post describe. However, I can't afford to have failover queues in a distributed architecture, as it would increase the complexity. So, the above workaround was a better idea for my team. 

Also, would the routing mesh act as an application load balancer? Like for example, if I am having a Docker cluster in AWS, how does the ELB added to it, work? And how would that be different from what the internal load balancer is doing? Or, did I get the concept of the routing mesh completely wrong? 

I have also asked the AWS Support folks for help too, as Lambda's security was crucial for us [HIPAA compliancy]. This was their response: