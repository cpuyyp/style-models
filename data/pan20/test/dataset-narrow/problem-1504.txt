I have never read (or tried) subsampling techniques on each node of the tree. I do not say that they might not exist, but it looks odd the statement "like is a common practice with random forests". Other than that, subsampling or bootstrapping the sample for each tree from random forests gives stochastic gradient boosting Friedman, 1999. They give better results with subsampling than without and more details are given also in Elements of Statistical Learning, page 358, 10.12.2 Subsampling. 

It is completely normal in some circumstances. If you consider the learning problem from a statistical perspective, learning is done by trying to estimate the conditional estimate of your output variable given input variables. When you are doing learning you basically have to main components: sample data from training and the learning algorithm, and you use both to build an estimation of the aforementioned conditional probability. Now consider the relation of each component with the desired estimated probability. Training sample Training data is only a sample from all the possible data. You do not have all the possible data since if you would have it you would not need to do learning, right? So a sample is just a fraction of the data. This sample could cover more or less the whole domain of the possible data. An immediate thought is that a bigger sample is better than a smaller one. As an extreme case to illustrate the idea is to have only one observation. But the size of the sample is not the whole story. It is possible that the probability you want to estimate is very complex. The complexity of that space requires also more data than a simple probability space. To give an example consider the problem of learning the mean of the heights of males from USA. Having a small random sample often would be enough if you assume a normal distribution. One the other hand take the example of learning to recognize speech. Since pronouncing is not the same against individuals you would need a pretty big sample to unveil the useful relation needed to predict and throw away things which are not useful in the data. Model/learning algorithm Not any model is appropriate to express any true model of the signal you learn. For example if your true signal resembles a second degree polynomial would be impossible to fit properly with a linear model, other than some particular cases. So there is a complexity in the model too, which could be compared with the complexity of the conditional probability you want to estimate during learning. Even if you know the true model and you learning model is able to describe it, sometimes you need more data to make the model able to fit. As another example consider a random forest which estimates a simple line. Random forest is very flexible since the underlying statistical model is basically a constant given a local region in the input space. Considering that, even if the model would be capable of estimating the true model, since the model is a local approximation, you need plenty of data in every useful region of the space, to learn a good approximation. Conclusion We considered the individual component comparisons with probability you want to estimate. But you have both. This makes even harder to understand what is happening behind the walls of learning. Thus there are no universal ways of telling when data is enough for a given model. So this is why is often the case that one algorithm can have different accuracies for different training set size and for different algorithms the required training set size to have approximate equal accuracies. One thing I tried often in practice is given a learning model to check how accuracy evolves as the learning sample size increase. If your training sample is large enough you have typically a region in a small sample size where error is large due to not enough data. After that you would have a region where errors stays approximately the same and this perhaps is where your model do it's best. Also is possible for even larger sample sizes to have a decrease in performance for some models, maybe do to inability of the model to describe what is there or due to overfitting or some other cases. You can trace this kind of behavior using some sort of bootstrap validation, where the sample used for training is taken repeatedly for increasing lengths. Something like 20 samples with 5% size, 20 samples with 10% size, and so on. This kind of information can give you an idea between the relation of the sample size with your learning problem for a given algorithm. 

Decision trees has one big quality and one big drawback. Their big quality is that they are what is known as glass-box models. What I mean by that is that they expose what they learn in a very clear and intuitive way. The name comes from the fact that you can see through a glass box. So, because of that, decision trees are very useful for analysis, they are a nice support to understand the relations between variables, which variables are important, in which way they are important. And even if they does not provide crystal clear information, they can bring you ideas about that. This might be very helpful especially if you have domain expert knowledge and you can put things together in a meaningful manner. Their main drawback is their high variability. This problem is mainly caused by their greedy approach. Each decision in the first level nodes shape differently the tree. You can even go further and see that a single additional data point is enough in many cases to get a totally different tree, especially if the sample is small or if the data is noisy. There are two types of approaches to solve this issue. The first type of approach tries to improve the single tree you built. This kind of approaches are known as pruning. A simple example would be reduced error pruning. This approach is simple and produce good results. You train your tree on a sample of data. Then you take another sample of data, you fit the new data on the tree, and then evaluate again the nodes of the tree from the perspective of the new data. If a non-leaf node get at least the same error if would not be split than if it would be split, then you can decide to cut the child nodes and transform that node into a leaf node. There are however much nicer pruning strategies, which are stronger, perhaps based on cross validation or some other criteria, mostly statistical based. Notice however that for reduced error pruning you need additional data, or to split your original sample in two, one for training, the other for pruning. If you go further to estimate the prediction error you need a third sample. The second approach would be either to build multiple times some trees and chose some based on cross validation, bootstrapping or whatever method you you, or use a tree ensembles like bagging or boosting algorithms. Note that for boosting and bagging you loose glass box property. Ultimately you have to choose between understanding and performance, having as a decent compromise the pruning procedure. 

I think there is no answer to your question since there is no absolute universal "good". Everything depends on the question you ask and the tools you use. This is why there are a lot of imputation techniques. There is no replacement for a missing value. However, in the constrains given by your question and used tools, you can think of imputation which does not alter your answer or at least measure some effects of the missing values. I will give some simple examples. One widespread technique is to replace missing values with a marginal central estimator for each variable. This might work for a classifier like decision tree, if there are no missing patterns in your data. However using this imputation would alter your confidence intervals if you study linear relations between variables using regression. This is simply because it will alter sample variance. Another much sophisticated imputation method is to use EM algorithm to fit the maximum likelihood estimator of the variance-covariance matrix. This estimator is unbiased and using this variance covariance matrix you can recover the linear model in an unbiased way. You then can go to analyze linear relations. But this works only for linear and log linear models and requires a lot of data and also requires to have missing data at random, which is not always the case. Another one is multiple imputation. What you actually do is to draw data at random for missing values according with a supposed distribution. You do that multiple times, let's say at least 30 times. You analyze each data set with traditional tools and later you aggregate those results into a single set of results. This works well mostly when data is missing at random, but is tedious, sometimes inconsistent and duo to randomness it can produce different results. Ultimately, sometimes is very hard to find proper ways to aggregate the results, it depends on the analysis you use. 

There is one way but you have to make some assumptions. In general a time series can be thought as a signal plus additional noise. The signal is an explicit function which models some phenomenon and the noise is some alteration of the output which is random by nature. Using this model you split $x_t$ and $y_{t-1}$ into something like: $$x_t = s^{(x)}_t + w^{(x)}_t$$ $$y_{t-1} = s^{(y)}_{t-1} + w^{(y)}_{t-1}$$ where $s^{(x)}_t$ is the signal of $x$, $w^{(x)}_t$ is the noise of $x$, respectively $s^{(y)}_{t-1}$ is the signal of $y$, and $w^{(y)}_{t-1}$ is the noise in $y$. Notice that $s^{(x)}_t$ and $s^{(y)}_{t-1}$ are well determined and noises are random. Using this notation the idea of approximately equal in statistical sense means that the signal is the same and if there are difference in samples, than the differences are caused by random components (aka. noises). This means that if you assume that signals are the same and remains only the noise. So what your null hypothesis is $H_0 : x_t-y_{t-1}=0$ can be written as: $$H_0 : s^{(x)}_t + w^{(x)}_t - s^{(y)}_{t-1} - w^{(y)}_{t-1} = 0$$ $$H_0 : w^{(x)}_t - w^{(y)}_{t-1} = 0$$ Now here comes the assumptions. You have two sorts of random variables (noises) which pose some difficulties when you are trying to apply tools based on central limit theorem. One problem is that the random noises depends on time. You have to assume that the noise does not change with time, in both series. This is translated as independent and identically distributed or ($w^{(x)}_t = w^{(x)}$ and $w^{(y)}_{t-1} = w^{(y)}$). So, if you assume that noises are independent and identically distributed your question can be worked out with a paired difference test. This means you can test if the differences depart significantly from $0$. You can use a t-test dependent t-test for paired samples if you additionally assume that the difference is normally distributed. You cannot use z-test on paired differences because you do not know the variance of the difference's distribution (when you use sample variance for variance in a z-test, you fall into a t-test). If you do not want to assume that the differences are normally distributed (that can be checked graphically at least with histograms or qq plots for skewness) you can use Wilcoxon signed-rank test, which should be fine for the number of samples you have. I would pick this test, for the same reason. Attention: I restate again that those tests are valid if an only if the noises in both series are independent of time. Also you can test only if they different. You cannot prove that they are equal, you can only state that you do not have evidence that they are different, which for most cases should be enough.