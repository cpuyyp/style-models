Your problem is NP-hard. I show this by a reduction from the shuffle problem: given words $w, w_1, \ldots, w_k$ over the alphabet $\{a, b, c\}$, decide whether $w$ can be obtained as an interleaving (aka "shuffle") of $w_1, \ldots, w_n$. This problem is NP-hard: see Warmuth & Haussler, On the complexity of iterated shuffle, JCSS, 1984, Theorem 3.1. Given an instance $w, w_1, \ldots, w_n$ of this problem, and writing $l_i := |w_i|$ for all $1 \leq i \leq n$, we build the DAG $G$ as a union of path graphs $L_1, \ldots, L_n$, where each $L_i$ for $1 \leq i \leq n$ has $l_i$ vertices written $v^i_1, \ldots, v^i_{l_i}$. Now, we define the cost function $f$ as follows: for each $1 \leq i \leq n$ and $1 \leq j \leq l_i$, for each $1 \leq k \leq |w|$, we set $f(v^i_j, k)$ to be $0$ if the $j$-th character of $w_i$ is the same as the $k$-th character of $w$, and $1$ otherwise. This reduction is clearly in PTIME, and it is clear that the minimum cost of a topological sort is 0 iff there is an interleaving of the path graphs realizing exactly the word $w$, showing that the reduction is correct. (Shameless ad: If you're interested in NP-hard variants of topological sorting, you may be interested about a recent preprint of mine which studies the complexity of finding topological sorts that fall into fixed regular languages.) 

One can talk of the treewidth of a Boolean circuit, defining it as the treewidth of the "moralized" graph on wires (vertices) obtained as follows: connect wires $a$ and $b$ whenever $b$ is the output of a gate having $a$ as input (or vice-versa); connect wires $a$ and $b$ whenever they are used as inputs to the same gate. Edit: one can equivalently define the treewidth of the circuit as that of the graph representing it; if we use associativity to rewite all AND and OR gates to have fan-in at most two, the treewidth according to either definition is the same up to a factor $3$. There is at least one problem that is known to be untractable in general but tractable on Boolean circuits of bounded treewidth: given a probability for each of the input wires to be set to 0 or 1 (independently from the others), compute the probability that a certain output gate is 0 or 1. This is generally #P-hard by a reduction from e.g. #2SAT, but it can be solved in PTIME on circuits whose treewidth is assumed to be less than a constant, using the junction tree algorithm. My question is to know whether there are other problems, beyond probabilistic computation, that are known to be intractable in general but tractable for bounded-treewidth circuts, or whose complexity can be described as a function of the circuit size and also of its treewidth. My question is not specific to the Boolean case; I am also interested in arithmetic circuits over other semirings. Do you see any such problems? Edit: An additional thing that can be done with bounded treewidth circuits (not arithmetic circuits) is to construct a constant-width OBDD representing them; see Jha and Suciu, 2012. This implies that tasks tractable for OBDDs (probability evaluation, satisfiability, etc.) are tractable for bounded-treewidth circuits. 

In their paper Every Poset Has a Central Element, Linial and Saks show (Theorem 1) that the number of queries required to solve the ideal identification problem in a poset $X$ is at most $K_0 \log_2 i(X)$, where $K_0 = 1/(2 - \log_2(1 + \log_2 5))$ and $i(X)$ is the number of ideals of $X$. What they call an "ideal" is actually a lower set and there is an obvious one to one correspondance between monotonic predicates and the lower set of the points at which they don't hold, besides their "identification problem" is to identify by querying nodes just like in my setting, so I think they are dealing with the problem I'm interested in and that $i(X) = N_X$. So, according to their result, the information-theoretic lower bound is tight up to a relatively small multiplicative constant. So this basically settles the question of the number of questions required, as a function of $N_X$ and up to a multiplicative constant: it is between $\log_2 N_X$ and $K_0 \log_2 N_X$. Linial and Saks quote a personal communication by Shearer to say that there are known orders for which we can prove a lower bound of $K_1 \log_2 N_X$ for some $K_1$ which is just slightly less than $K_0$ (this is in the spirit of Yoshio Okamoto's answer who tried this approach for a smaller value of $K_1$). This does not fully answer my question of computing the number of questions required from $X$, however, since computing $N_X$ from $X$ is #P-complete, I have a feeling that there is little hope. (Comments about this point are welcome.) Still, this result by Linial and Saks is enlightening. 

Does there exist a stable comparison sort using $O(1)$ auxiliary memory and achieving $O(n \log n)$ average run time? Context: There are comparison sorts with any two of those three desirable characteristics (see this list). Is there a reason why the three cannot be achieved simultaneously? [Edit: I added the algorithm from the accepted answer to the list on Wikipedia, obviously it wasn't there when I asked the question.] 

Tree decompositions and treewidth are a standard way to measure how close an undirected graph is to a tree. I am studying decompositions of directed acyclic graphs (DAGs), and have come to define them as follows: Given a DAG $G = (V, E)$, letting $G'$ be the undirected graph obtained by forgetting about edge orientations, a tree decomposition of $G$ is a tree decomposition $T$ of $G'$ as a rooted, directed tree, such that for any bag $b \in T$ and any vertex $v$ of $V$ in $b$, if $v$ occurs in none of the children of $b$, then for any directed edge $(u, v) \in E$, we have $u \in b$. The width of $G$ under this definition is then, as usual, the minimum across all decompositions $T$ of the maximal cardinality of a bag in $T$ minus one. My general question is: Is such a notion of tree decomposition of a DAG known? I know that there are existing definitions of tree decompositions for directed graphs, such as D-width, DAG-width, and directed treewidth. However, I don't think they are related to this definition, because, according to all of them, DAGs have low treewidth. Indeed, these definitions consider acyclic graphs as "simple". By contrast my definition only applies to DAGs, and its extension to general directed graphs is not interesting: unless I'm wrong, it implies that all elements of each strongly connected component must co-occur in a bag. Further, in my case, the width of a DAG may be more than that of the corresponding undirected graph. In fact, a tree decomposition of a DAG $G$ in this sense is a standard tree decomposition of the moral graph of $G$, with additional conditions enforcing it to have a certain "directed" shape. I think the treewidth of the DAG in this sense can still be arbitrarily larger than that of the moral graph, but I'm not sure how to characterize the DAGs that would have "bounded treewidth" in the sense I proposed. Motivation. In my context, the DAG is a circuit, and I use the tree decomposition to reason about valuations. The property that I require is designed to ensure the following: when processing the tree decomposition bottom-up, when we reach a new bag and we see a new element, we can examine its valuations based on that of its children, which are known because the children must be in the bag: we do not need to guess the valuation (as we would have to if it depended on nodes we haven't seen yet). I suspect that there may also be a relationship to inference in graphical models, where message passing needs to be done in only one direction, but I was unable to find references. 

It's easy to show the following observation (ignoring $L'$ for now): the possible words that we can form with these intervals are exactly $u \# \tilde{u}$ for $u \in \{0, 1\}^k$. This is shown essentially like the Lemma in @MikhailRudoy's answer, by induction from the shortest intervals to the longest ones: the center position must contain $\#$, the two neighboring positions must contain one $0$ and one $1$, etc. We have seen how to make a guess, now let's see how to duplicate it. For this, we will rely on $L'$, and add more intervals. Here's an illustration for $k = 3$: 

Let's consider a polytree $P$. The linear preprocessing can insert intermediate nodes and collapse nodes with only one child to get a polytree which we can use to answer queries on the original polytree, so that we can assume without loss of generality that all internal nodes of $P$ have degree exactly 2. We will do the linear-time preprocessing bottom-up following any topological order. For every node of $P$ that we process (whose children have all already been processed), we will construct an index structure for the node, which we call an "infix tree", and which may also include pointers to other previously defined such structures (but not modify them, of course: one can think of this as a purely functional data structure supporting constant-delay enumeration and constant-time union.) The construction will work in linear time, and for any node we will be able to enumerate its descendants in constant delay by considering its infix tree. There are three kinds of nodes in infix trees: 

@MikhailRudoy was the first to show NP-hardness, but Louis and I had a different idea, which I thought I could outline here since it works somewhat differently. We reduce directly from CNF-SAT, the Boolean satisfiability problem for CNFs. In exchange for this, the regular language $L$ that we use is more complicated. The key to show hardness is to design a language $L'$ that allows us to guess a word and repeat it multiple times. Specifically, for any number $k$ of variables and number $m$ of clauses, we will build intervals that ensure that all words $w$ of $L'$ that we can form must start with an arbitrary word $u$ of length $k$ on alphabet $\{0, 1\}$ (intuitively encoding a guess of the valuation of variables), and then this word $u$ is repeated $m$ times (which we will later use to test that each clause is satisfied by the guessed valuation). To achieve this, we will fix the alphabet $A = \{0, 1, \#, 0', 1'\}$ and the language: $L' := (0|1)^* (\# (00'|11')^*)^* \# (0|1)^*$. The formal claim is a bit more complicated: Claim: For any numbers $k, m \in \mathbb{N}$, we can build in PTIME a set of intervals such that the words in $L'$ that can be formed with these intervals are precisely: $$\left\{ u (\# (\tilde{u} \bowtie \tilde{u}') \# (u \bowtie u'))^m \# \tilde{u} \mid u \in \{0, 1\}^k\right\}$$ where $\tilde{u}$ denotes the result of reversing the order of $u$ and swapping $0$'s and $1$'s, where $u'$ denotes the result of adding a prime to all letters in $u$, and where $x \bowtie y$ for two words $x$ of $y$ of length $p$ is the word of length $2p$ formed by taking alternatively one letter from $x$ and one letter from $y$. Here's an intuitive explanation of the construction that we use to prove this. We start with intervals that encode the initial guess of $u$. Here is the gadget for $n = 4$ (left), and a possible solution (right):