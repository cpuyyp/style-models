If you have an index on then the planner will need to choose between an using that index or the PK (or a full table scan) - it will not benefit from both at the same time. --EDIT As pointed out by @jug in the comments below, this is not accurate at least since 8.1: the planner may choose to build two in-memory bitmaps and combine them to get the result set. This gets more expensive as the tables get bigger, so the planner may choose not to do this depending on the size of the table and the estimated cost of using one index and then filtering. --END EDIT The new index will only be helpful if in some cases using it is more efficient than access via the PK. The kind of things that could make this likely include: 

The SAMPLE BLOCK is operating on the index — which has far fewer blocks and is distorting the sample: 

All three options you are considering will impact performance on the Oracle DB. If you don't want to impact performance at all, apply archivelog files from Oracle to a standby DB and query that (you could try Linked Servers first to find out if it's fast enough for you). 

This assumption is not true - either kind can be indexed. The computed column must be deterministic in either case but when the computed column is persisted, the requirement that the computation is also precise is relaxed (ie it can involve floating point operations). 

More out of academic interest than as a practical solution, you can also achieve this with a user-defined aggregate. Like the other solutions, this will work even on Postgres 8.4, but as others have commented, please upgrade if you can. The aggregate handles as if it is a different , so runs of nulls would be given the same — that may or may not be what you want. 

turn off autovacuum for the table check each block to determine the degree of clustering delete and re-insert all the rows from blocks below a clustering threshold manually vacuum to free those (complete) blocks repeat steps 2-4 as regularly as necessary 

You may be able to achieve better performance by searching first in rows with higher frequencies. This can be achieved by 'granulating' the frequencies and then stepping through them procedurally, for example as follows: --testbed and dummy data: 

To do this without the risk of blocking another user trying to update the same profile at the same time, you need to lock the row in first, then use a transaction (as Rolando points out in the comments to your question) : 

This probably doesn't matter if "less than 3% of the rows are canceled." - let them be scanned anyway? Oracle: For the unique constraint, if you include (from a sequence) in the index then cancelled rows will never cause the constraint to fail 

Similar to what Sathya has already suggested, but I like to avoid completely if possible - an unhandled exception is usually the correct outcome for exceptions you aren't specifically handling: 

Can I rely on being executed before the context is read inside the view, as it has been in this test case run on 10.2? 

Of these, it is very likely that the last option is the best one - is the 'correct' type for most data that refers to a fixed point in time. (As opposed to 'noon' which is always 12:00 no matter which time zone you are in.) 

I use the system column to implement a form of optimistic locking, and sometimes need to 'touch' the row to bump without actually updating the row. I currently just do a 'dummy' update: 

If you are open to a different approach, Have you considered adding a 'deleted' Boolean flag to the table, or a 'deleted_at' timestamp instead. Or better still, deny CRUD access to your database tables and handle the audit trail in your transactional API :) 

If you look at the plan for the query you are using, you can see it never touches the table, instead it is doing a fast full scan of the primary key: 

But I discovered it would still depend on how you are connecting, eg from the JDBC Thin Driver you aren't using OCI and so the information isn't pushed 

No, 'Desktop Class' just means you only get basic choices from the installer1, according to the docs: 

It might help to define terms up-front. The term latency is used in (at least) two ways regarding I/O. It can mean: 

Oracle is not like MySQL in this. Like most other RDBMSs it comes with it's own built-in storage engine that cannot be exchanged for another. 

It is certainly true that spreading your simultaneous I/O between different drives will increase performance - that is no myth. It is a myth is that doing it twice will improve performance again. If you SAME, then splitting your array up into two partitions and putting indexes on one and tables on another is a waste of time. 

There is no 'demote' option for pg_ctl - if the primary was cold when the standby was promoted perhaps it could be converted to a standby manually, but otherwise it will need a new base backup from the new primary. The normal reason for switchover would be if the primary has failed so this would not be necessary. 

You may be able to work around the problem with a view, but it seems that in this case at least, adding a dummy is enough to prevent "flattening": 

Unless one or both is likely to happen, you should not create the secondary index - if they might, it would be best to test each scenario with and without the index to see if any performance benefit is worth the cost (eg increased storage and overhead for and operation) 

You can get the SCN for a row with the ORA_ROWSCN pseudocolumn Unless you have set row-level dependency tracking for the table, this will report the SCN of the last change to the block the row is in, which may not be much use. You can turn on row-level dependency tracking at time only, so you may need to drop and re-create your table. Note that row-level tracking increases the size of each row by 6 bytes. 

Your DB Buffer Pool should always be full unless you have recently restarted your server - Oracle will not purge blocks from the cache unless it has to in order to make room for other blocks. This in itself is not an indication that changing the block size will improve anything. You need to know more about the cache misses. If the OLTP side of your workload can query any part of your 5TB data, optimizing the buffer pool a little for it will probably not make much difference (but may unnecessarily slow down your big sequential scans that it was presumably optimized for) On the other hand, if there is a small subset of the data hit regularly, and cache misses for the OLTP workloads can be substantially reduced by reducing the block size (which will depend on whether the reads are really scattered across blocks or not), then perhaps the benefits would be significant. In the end you will have to test and find out :) 

@kevinsky is correct, you cannot use as a variable name in PL/SQL, however that is not the only issue. You also cannot assign values to IN variables like , , and . My best guess is that you meant those to be local variables seeing as the first thing you do with then is SELECT INTO them. test table: 

From digging around the postgres docs and newsgroups: will give you a new - if you call the function again at a later date, if you get a one higher, you know that no transactions committed between the two calls. You may get false positives though - eg if someone else calls 

Tables with billions of rows in them are common — don't fixate on the size of the table, it is much more important to know how many blocks you scan to query it, which depends on the query and on indexes. You do not want to consider partitioning, or anything exotic, just because you have 10 million rows in a table. 

then it is possible to insert a row into that links a Ford to a "Toyota" model. Adding to only makes the problem worse. On the other hand if we keep the foreign key as part of the primary key as follows: 

The result of the function is 'persisted' in the index in either case - the only difference is whether it is persisted in the table. 

build up a list of all possible values for the 'missing' leading columns (this can be done fairly efficiently from the index structure itself) iteratively perform range scans for each combination of missing columns and the column provided union the whole lot together in one result set 

Various sources (eg Wikipedia, PSOUG) state that Oracle's does not have a type. Is this true? What about other RDBMSs? 

It is pretty easy to be a bad DBA Seriously though, a DBA usually has special responsibility for something that is often critical to the success or failure of a business: its data If you run a company then you may well be keen to employ competent experienced people in that role I don't think it is a question of 'easier' or 'harder' - just a question of how valuable your data is: It isn't inherently harder to put a satellite in space than a person, but you would check your sums a good deal more for the latter 

No locks are held while the transaction is rolling back It is handled by a low priority background process 

I highly recommended reading the blog posts he links, they describe in detail the limitation of histograms you are running in to, eg: 

This also explains why locking the table outside the function in a wrapping plpgsql block (as suggested by @ypercube) prevents the deadlocks. 

The other side of the warning refers to 'set' operations not 'SET' operations - that looks like a message bug to me - for example it is also produced with windowing functions: 

To be precise, the type stores timestamps without any timezone information rather than "in local time zone". 

A normal table with or without indexes: New rows are dumped at the top of the heap. The RDBMS probably only looks at 1 block, so not just O(1) but very small O(1). If the table has indexes, a pointer to the row will be added to each. This will usually be an O(log(n)) operation. A table with some sort of clustering going on, eg an Index Organized Table or cluster for Oracle, or a Clustered Index for SQL Server and others: New rows are inserted into a particular block, which may cause the block to split or overflow, but whatever happens it is still O(log(n)) or better, caused by the b-tree or similar structure used to find the block. 

In other words "transactions that were in the past appear to be in the future" and "data loss" are entirely theoretical and will not be caused by transaction ID wraparound in practice. 

I stumbled on some research for sqllite that may be applicable. They reported increased performance of 20-70X for some applications It goes without saying that your mileage may vary. 

I do not think you will be able to mock up fake history with Flashback Data Archive - as Tom indicates in the link you provide. Other than that it really does sound purpose-built for your data. Perhaps you could consider a migration that leaves the current history data in a frozen state and gives access to a 'union' of the two histories for your historical data (through a set returning function for example)? --edit 

On it's own I wouldn't let this be the reason for choosing a quirky and fragile design. If you go the route there will be no way to take advantage of the database features for ensuring referential integrity, for example. A traditional normalised way of achieving the same thing would have benefits beyond RI: 

If you can run 6.06 in a VM on top of 11.04 server, it looks like MySQL 4.1 can be installed on that 

If the cost would not be too high, you could consider moving the flag into the table and index it? This would slow your transactions but improve query performance as the (presumably) large table would be accessed in one big 

But is this timezone safe—if the current date is in Daylight Saving Time and the start of the month is not or vice versa will the correct date result? 

What Oracle has had for a long time is an excellent implementation of MVCC - read about it in the very useful Concepts guide. I'll just add that Oracle owning InnoDB is unlikely to make much difference to how InnoDB works deep down at the level of concurrency control - although as I understand it the way they both implement MVCC achieves basically the same thing (as long as you are not mixing tables using other storage engines in your MySQL queries - though you couldn't fault InnoDB in that case) 

Yes it's safe. It's not exactly clear in the documentation but can return either or depending on what you pass it. If you pass a , it will cast the input to and returns so there can be no tz issue when casting back to . If you pass a it does the truncation the way you'd expect and returns a : 

If I understand correctly (and I'm not sure I do), you can add the extra field but use for the "non import-related items" - the constraint will then effectively ignore the rows with a but enforce uniqueness for the others 

If you plan on porting integer data back to SQL Server from Oracle from those fields at some point: if integers out of the range -2,147,483,648 to 2,147,483,647 have been inserted on the Oracle side, these values will overflow on the return journey If some part of an application is relying on integers being in the aforementioned range. Values outside this range might therefore cause an error 

Depending on your real-world data, you will probably want to vary the number of granules and the function used for putting rows into them. The actual distribution of frequencies is key here, as is the expected values for the clause and size of ranges sought. 

Have you considered Robocopy? It is able to copy NTFS ACLs. I guess you might need to have postgres running under the same Domain Account on source and target if possible but I'm probably out of my depth talking about such things. 

I've assumed you need to partition by , and but maybe not all are necessary depending on the PK of I've ignored the complexity you mention but don't specify completely - hopefully you'll be able to work it into the query in a similar manner to the rest You'll probably have to fix the CTE with your rules for specificity/generality I tested on 2008R2 - YMMV if you are on an earlier version