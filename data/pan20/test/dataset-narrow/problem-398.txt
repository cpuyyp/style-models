Watching queries execute with Live Query Statistics I noticed that it seems SQL Server is lazily constructing a hash table from the build input of a hash join. This is a meaningful difference in the case of 0 probe rows. It potentially saves the entire build side tree. I always thought a hash ran like this: 

Pull first probe row. Complete the operation if no row available (short-circuit). Match all probe rows against it. 

This sometimes causes the query to abort. How can I disable IO timeouts on my dev box? Also, in case the query does not abort I sometimes get dump files. How can I disable those? 

On my dev box I sometimes need to run very IO intensive queries such as index builds and . This can put so much load on the disk that it hardly can process anything else. This causes enormous lagging in other programs. For that reason I sometimes need to suspend using Process Explorer. If I do that for longer periods of time I sometimes get IO timeouts such as 

However, generally speaking relying on the side effect of a function to do real work from inside a SQL query is not a good idea. The optimizer is always looking for a reason to do as little work as possible, and you might be surprised sometimes to learn that the optimizer sees the necessity of your function differently than you do. 

An example of a major difference between SYS (or any other SYSDBA connection) and every other user: SYS can't do consistent read. One implication of this (there are others) is that you can't do a CONSISTENT=Y export as SYS using the old exp utility. Oracle employee and expert Tom Kyte is of the opinion that you should rarely ever use either one. Regarding SYS, he points out that it works differently as the example above indicates, but more generally he considers them to be "owned" by Oracle Corporation. If you make a change or add something in either schema and a problem occurs (e.g., a database upgrade fails), I suspect Oracle Support's answer would be, "You shouldn't have done that." 

Tonight our server decided to install SQL Server 2014 CU7 for SP1. The update appears in the Windows Update history and I found downtime in our logs for it. The previous patch level was CU3. I was surprised by this because I am not used to Windows or SQL Server automatically installing CU's. I also never witnessed a service pack being installed automatically. What has changed? According to sqlserverbuilds.blogspot.de the CU7 was released today. It was installed right away after release. I am not categorically against this but I would like to understand what policies are driving the automatic update choices. Strangely, tonight was not a "patch Tuesday". No other updates were processed. The server did not reboot, either. I'm certain this was not a manual action. So far my plan was to keep the server at the CU3 level and upgrade to 2016 eventually. The new CU being installed caused unnecessary downtime and introduced risk which I normally was not willing to take. In particular I do not wish to install patches at a random time. So what's the automatic update policy and should I do something about that? 

In #1 and #2, something uses the output of test_function, but in #3 nothing uses it. The optimizer knows this and does not call test_function. Since test_function isn't called, the side effect of updating the table does not happen. This is definitely intended behavior. I rewrote your #3 to the following, and test_function got called (the UPDATE happened). The reason is that the result of test_function was needed for output. 

This sounds like a job for Change Data Capture (CDC), which allows you to (among other possibilities) ship your archivelogs from the OLTP database to the reporting one, mine them for the changes, then query the changes out, ignoring any you don't want (e.g., changes of type 'D' for DELETE), and using whatever process you might devise apply those changes to your reporting tables. I have no idea how well CDC would do with a ruleset encompassing 4700 source tables from another database. I've never used it for more than about 50 tables myself. FYI, there are licensing-related limits on CDC. The full feature set is only available on Enterprise Edition. 

I'd like to keep rows with the same together in the result set. The groups themselves should be ordered randomly, though. is supposed to be the secondary sort criterion. Like this: 

I then waited at least 10 seconds to let the ghost cleanup task run. But the ghost records do not disappear. I also tried restarting the server. No ghost related trace flags are in use. This is not an actual problem that I'm having. I'm trying to understand ghost cleanup in general. Why do the counters not drop to zero in tables that have no writes? 

Row versioning maintains 14 bytes of internal versioning information for each row. But is this really a cost per row or is this a cost that also applies to each index on the table? It seems the 14 bytes must be added to all index records as well so that index-only scans (and other index-only accesses) can see the versioning information and perceive a point in time snapshot of the data. All information that I could find on the web only talks about a per row overhead of 14 bytes, though.