In my experience, when people claim to have an automated approach to feature engineering, they really mean "feature generation", and what they're actually talking about is that they've built a deep neural network of some sort. To be fair, in a limited sense, this could be a true claim. Properly trained deep neural networks can handle any number of pairwise correlations between individual features or groups of features. That said, without a great deal of up-front data pre-processing tools that know how to intelligently handle different types of input data (e.g., free text, images, etc.), none of this would be possible. Bottom line, it takes a great deal of manual effort to do something automatically. 

Certainly. A part of topic modeling output is typically some vector for each document describing it as a distribution over the topics in your model. You can use these as continuous-valued predictors for a classifier trained with the labels you have. I've done this before, but, depending on the domain characteristics of your problem, it may not be the best approach, but it is certainly a testable one! 

I think the tagging approach has some merit here. The frequency drop you're observing as a consequence of using this is to be expected, I think. After all, keywords are the words that help differentiate a document from others in a corpus. If you have access to an ontology related to the subject matter of your corpus, you could try to map your rare keyword tags to the ontology, and use parent-level information for each to get a slightly more general set of keyword tags! If this is an approach that interests you, Stanford's open source protégé system is a good framework for working on this. 

Many have shown the effectiveness of using neural networks for modeling time series data, and described the transformations required and limitations of such an approach. R's package even implements one approach to this in the function. Based on my reading, all of these approaches are for modeling a single outcome variable based on its past observations, but I'm having trouble finding a description of a neural-network-based approach that also incorporates independent predictor variables (a sort of ARIMAx analogue for neural networks). I've found references to Nonlinear autoregressive exogenous models (NARX), which seem like they should be what I'm looking for, but all the reading I've been able to find talks more about using this approach for multi-step-ahead prediction of a univariate series. Can anyone point me in the right direction on this? For bonus points, does anyone know of an implementation of what I'm looking for in R? 

This is an interesting problem! You're right in thinking the easiest approach to this is to use a look-up table for identifying locations. Of course, this approach is going to be only as good as your data set and is still going to be somewhat prone to misclassification. One resource I've found is the Free World Cities Database. One caveat—this isn't still actively maintained, and the countries are listed as country code, which may require further resolution on your part. Another possibility is the geonames data set. There, it looks like you'd want their dataset. 

Great question, @gsamaras! The way you've set up this experiment makes a lot of sense to me, from a design point of view, but I think there are a couple aspects you can still examine. First, it's possible that uninformative features are distracting your classifier, leading to poorer results. In text analytics, we often talk about stop word filtering, which is just the process of removing such text (e.g., the, and, or, etc.). There are standard stop word lists you can easily find online (e.g., this one), but they can sometimes be heavy-handed. The best approach is to build a table relating feature frequency to class, as this will get at domain-specific features that you won't likely find in such look-up tables. There is varying evidence as to the efficacy of stop word removal in the literature, but I think these findings mostly have to do with classifier-specific (for example, support vector machines tend to be less affected by uninformative features than does a naive bayes classifier. I suspect k-means falls into the latter category). Second, you might consider a different feature modeling approach, rather than tf-idf. Nothing against tf-idf--it works fine for many problems--but I like to start with binary feature modeling, unless I have experimental evidence showing a more complex approach leads to better results. That said, it's possible that k-means could respond strangely to the switch from a floating-point feature space to a binary one. It's certainly an easily-testable hypothesis! Finally, you might look at the expected class distribution in your data set. Are all classes equally likely? If not, you may get better results from either a sampling approach, or using a different distance metric. k-means is known to respond poorly in skewed class situations, so this is something to consider as well! There is probably research available in your specific domain describing how others have handled this situation. 

It's a slight variation on your problem as described, but if your goal is to build a good model for predicting documents labeled $L$, I would initially formulate this as a recommender system problem, until you've reached a desirable point in the learning curve of your system. I did exactly this in a publication a few years back. In my approach, all labeled documents are used to train the SVM, while all those unlabeled are used for classifying. I use the signed margin distance of each classified document for ranking, and take the top-$n$ as the next $n$ documents the reviewers should assign a label $\in(L, L_{not})$. A side effect of this process is that you get to iteratively evaluate the performance of your classifier as you add new labeled documents to the model, which should get you what you need. If this approach sounds useful to you, I recommend looking over my paper, as I describe some performance metrics useful for this, in addition to outlining the specific procedure to use that will ensure you're not biasing your model. 

In theory, if you have a large enough random sample of your data set, it should be representative of the characteristics in the larger population of data that will affect the relationship between parameter values and performance. It seems to me that a $5\%$ sample of your data might be too small for what you're trying to do. When I'm performing cross-validation on a test/dev data set, I usually set aside somewhere in the neighborhood of $10\%$ for the hold-out test/evaluation data, and use $90\%$ for my development work, but, as you pointed out, this may be too big given the computational constraints you're up against. Depending on the type of application you're working on, you could look at feature distribution in your sampled data and compare it to that in the larger population. If the two sets are comparable, then you might be ok to do your dev. work on the smaller data set. I say it depends on the type of application you're developing, because many statisticians would consider this cheating, in that it's generally not good model evaluation practice to look at hold-out data at all. Here's the procedure I'd recommend: