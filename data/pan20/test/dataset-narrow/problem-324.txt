A couple of things that you should note, DBCC DBREINDEX is deprecated, you should really be using ALTER INDEX REBUILD instead. As an additional item, rather than trying to reinvent the wheel there are free solutions out there that have been written by folks with many years of experience, such as Minionware Reindex. These are designed to be quickly and easily deployed and to provide simple management. 

The only significant memory that would be held would be if SSMS was being run on the server itself (and then you'd still be limited to the 2GB of memory space that SSMS can allocate). There will be a very small amount of memory allocated to maintaining the connection in SQL, but that's truly trivial. Of course, if you leave an open transaction, that's a whole other level of pain, but it doesn't sound as if that's the trouble your developers are trying to avoid (unless this is what they are trying to avoid, but came up with a PC way of telling them that). 

The trouble here is that the script evaluates the IF statement, and then the logic applies to the first statement afterwards. To handle multiple statement you need to put them within a BEGIN...END block. Something like this should work for you 

You can break down the vendors using a CTE, prior to using the COALESCE function to get a list of emails, with one per vendor. 

Yes, this is absolutely possible, and a great use of resources. You would get your OLTP databases added to an Availability Group, and then add the second machine as your secondary replica. For both replicas be sure to allow reads from secondary. Then do the same with your DW databases. You will end up with two AGs, and will probably want to run with each as the primary on different servers, so be sure that you include monitoring to let you know when the AGs failover, and what AG is running as the primary on each server. 

Your issue, rather than a corruption problem, is a sparse file limitation caused by the command taking an internal snapshot of the database (for consistency reasons) and checking that. There are really only two ways to resolve this: 

Per the information you provided from you are running the x86 version of SQL Server 2012. It would appear that the OS and SQL Server architecture must match in order for Availability Groups to function. This means that you would either need to change your SQL Server installation to the x64, or go to a prior version of Windows (2008) and install the x86 version of that. My recommendation would be to adjust your existing install and make it x64. This provides many enhancements such as not needing to mess with things like PAE and AWE when using more than 4GB of memory. 

Personal observation from experience with both FCI and AG failovers, with reasonable high volume transactional system (40k trx/sec). For each consider 6 databases ranging in size from 500MB to 4TB in size. Times listed for failover are what it takes for the database to up and in a writeable state on the new node. Your mileage can, and will vary, but this is at least a data point for you. Cluster failover: 47 seconds (avg) AG Failover: 10 seconds 

Enabling the remote admin connection allows you to access the DAC from another machine on your network, otherwise you would only be able to access it from the local machine where SQL Server is running (as the DAC would only listen on the 127.0.0.1 loopback address). This could be a problem if, for example, there is a high CPU condition that prevents you from being able to access Windows and launch a sqlcmd/PS/SSMS session. With the remote DAC enabled you can access it from a remote machine using the admin:server/instance logic. This can be highly useful in bad scenarios, and save a lot of time. It's something I have setup by default on all of my instances. Further reading for this at $URL$ 

The reason that localhost is acceptable would be because it is using a default instance of SQL Server. The default instance, by name, would be MSSQLServer, however you do not need to provide that. Previously the express was a named instance, which requires you to pass along the instance name (or port) in order to connect. The database name is MyDatabase. The assignment of MyDatabase_Data is for the data file inside the database. In this example a data and log file are designated and named. You can name these however you like (although you cannot have duplicate names within a database). You can also rename them after the database has been created should you so wish using an ALTER DATABASE statement. Filegrowth is dependant upon data being written to the database. Log files will continue to grow if the database is in full or bulk-logged recovery unless you take log backups. However if in simple recovery then the log will grow as large as is required to support open transactions that happen, or to prevent a loss of data with CDC or replication (if a log continues to grow you can check for reasons using the log_reuse_desc column of sys.databases). For the data file, that will grow as you insert data, as an when needed. It is generally recommended to not use percentage based filegrowths as this can lead to unexpectedly large file growths, not to mention performance problems. 

You can create additional nonclustered indexes on tables with clustered columnstore indexes, the following script shows this... 

I would expect you to see the default endpoint, as well as the Dedicated Admin endpoint. You will need to add another endpoint: 

First things first, get rid of the differential backup. If you are performing full, copy only backups off of a secondary these will do nothing for you. The differential backup works to backup changes from a full backup (not copy only). When a normal full backup runs it sets all of the pages in the database as being backed up. Going forward from that point any change to a page gets marked, and then the differential would come along and backup any page marked as changed. As time goes along the differential will get larger and larger. A copy only backup does not reset those flags, so things will just get worse. At this point decide how you want to proceed with your backup strategy. If you want to continue to take differential backups, then also take full backups on your primary. If you no longer want to take the differentials for the database that are in the AG then set the copy_only flag to N in the Ola script. This will allow the differential job to run and take backups of databases not in the AG, but will ignore those that are in the AG (so long as the backup preference marks a secondary replica as the machine to backup from). As regards the benefit of offloading backups to a secondary, this can help reduce resource contention on your primary replica, which is where most of your traffic can be. It can also be useful as a way to take database backups in a DR site, where you may have a secondary replica or two. 

Per Microsoft documentation changing the max server memory will clear the plan cache, as will changing: 

Failover partner is for when you are using database mirroring, you should not use it for Availability Groups. While it, strictly speaking, will work, it will only do so when the primary server is offline, and is not designed for this kind of scenario. Using a listener within an Availability Group is the way to go. This is a virtual network resource that will point you to the correct primary (and is required in the event that you want to use read-routing at any point in the future). Using a listener you should not have to perform an app restart to have client redirection, it should just work. 

It returns: 1 - if database is writable (includes databases not in AGs) 0 - if database is secondary in an AG -1 - if the database does not exist 

In this instance if you had the same table in multiple schemas that you would get incorrect results. You could fix that by also joining to the sys.schemas table with the schema of the table name. *Updated SQL to be dynamic as requested in updated question 

db_executor is not a default role (MS has a list of default roles), it would have been created by somebody. There's also no SQL Server 2012 R2. Potentially you are getting confused with Windows Server 2012 R2. 

I'm not sure what "Microsoft and Microsoft MVP standards" are, but my recommendation would be to find yourself some extra storage and to grow those logs out, and let them be at the larger size. Shrinking the logs is going to significantly slow down your month end processing, as each growth of the log is going to cause a stall in the processing while the new VLFs are zeroed out. Depending on the performance of your storage a 1GB log file growth could take 1-20 seconds (or maybe even longer). I'm pretty sure everyone would be super happy if it meant that the processing was completed earlier than it is now. There's the additional concern that your log shrinking routine would make the log smaller than it needs to be for general (not month end) usage. That would mean that you would have impact during regular work at those times when the log needs to grow. 

You need to use dynamic SQL for this. The following example will work (as does a quick check to see that the database exists, you could add additional checks as well). 

Per Microsoft's documentation, there is no requirment to set a database to single_user mode when running . If you are finding yourself setting single user mode I would recommend taking a look at the locks that are blocking that change for you. An alternative would be to grow the data files themselves rather than the filegroup and considering usage of trace flag 1117. 

The chances are the the table you are attempting to reindex exists in a different schema than the default, and as such you will not find it. The following update should allow you to obtain that table. 

As a workaround what you could do in this instance is change the default location for the log files to a new folder, and attach the data file with the rebuild log. Once that is done set the default location back to it's prior setting. Then you can run and ATLER DATABASE command to set the log file to the path/filename that you desire, set the database offline, copy over the log file to that new location, and set the database online again. After that just clean up the old file and folder. 

It sounds like you are using Availability Groups and either one of your replicas is not syncing correctly, or it is getting so far behind that it is causing the log on your primary to grow until is reaches its maximum size (or you run out of disk). 

You have to have a read-routing list, even with only two instances, if you want to offload reads to a secondary replica. The following script is an example of setting this up (you would change the AG name to whatever your AG is, and the replicas to your servernames (using the fully qualified domain name, and adjusting the port if required). 

This is a difficult scenario to truly handle. I would probably start with running a job on your secondary replica that checks the state of all the databases, and should it become primary for one of them then performs the ALTER AVAILABILITY GROUP [AG] FAILOVER; command for all of the others. The biggest challenge with this is also running jobs on the primary, and taking care to not accidentally try to fail one of those databases back. You could manage this by checking the AlwaysOn extended event session and looking for failovers there, and do not do work if the database was failed away (as the most recent captured event), this way you don't end up bouncing the databases around all over the place. You can use sys.fn_hadr_is_primary_replica to check and see if any particular database is the primary on your system. 

This is pretty ugly, but should do what you need (could always turn it into a function). Basically we convert to a numeric type (gets rid of the scientific notation elements and limits the number after the decimal point), then we do a bunch of replacements for 0's and for the . so that you don't end up with the trailing zeroes or a funky looking number. 

Check to see the amount of space actually used in your database. You probably have a lot of free space available. A quick way to do this would be to connect to your database and use exec sp_spaceused which will give you the database size and unallocated (unused) space. When SQL Server backs up the database it only needs to backup the pages that contain data (and the log), which means that the backup can easily be smaller than the files themselves. This question has also been answered in Why is a .bak so much smaller than the database it's a backup of? 

Which will give you the relevant information (note the bold, italicised SP1 from the example results) 

I would confirm that you are connecting using the correct login, and that you have not incorrectly assigned some other permission. I executed the script that you provided above, and then used to test the select permissions and received errors for both (as I would expect). Adjusted comments slightly, but the following I received what I expected (SQL Server 2012) 

This is by no means a clean looking solution, but it seems to provide the results that you are looking for (I'm sure that others will have nice, clean, fully optimized queries for you). 

I would recommend using sp_whoisactive by Adam Machanic. It's highly powerful, can give you all the information you need, and can run frequently, loading its results to a table. 

You can use SQLCMD mode in SSMS (or SQLCMD itself) to do this. The following query would connect to an instance, check for the database in question, create the login if it does not exist, and then connect to the database and add a user for that login. 

If you are stuck running SQL Server 2012 then SqlWorldWide will unfortunately not work for you (although it you are 2014 or higher it is the way to go). I ended up writing my own function to handle this on the lower version... 

You do not need to add any server roles to a new user, those are optional (although it is best to leave the public role checked). The sysadmin role encompasses all other roles, and is deity level access within SQL Server, granting you the rights and permissions to do whatever you want with the instance. 

You can absolutely setup transactional replication on mirrored publishing databases. It is fully supported, and Microsoft has some good content on getting it up and running. The only caveats are the ones that already exist for transactional replication, and the recommendation to have the distributor database on another server outside of the mirrored pair. 

There are no big concerns with taking it offline as a part of moving it. I've performed this action a couple of times in the past and encountered no issues.