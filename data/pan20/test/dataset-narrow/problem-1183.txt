Sometimes measuring quantumness in algorithms gets conflated with trying to measure the amount of entanglement produced by an algorithm, but we now think that a noisy quantum computer could have computational advantages over classical computer even with so much noise that its qubits are never in an entangled state (e.g. the one clean qubit model). So the consensus is now more on the side of thinking of the quantumness in quantum algorithms as related to the dynamics rather than the states generated along the way. This can help explain why 'dequantizing' is not likely to be generally possible. 

It depends on the context. For quantum algorithms, the situation is tricky, since for all we know, P=BPP=BQP. So we can never say that a quantum algorithm does something that no classical algorithm can do; only something that a naive simulation would have trouble with. For example, if a quantum circuit is drawn as a graph, then there is a classical simulation that runs in time exponential in the treewidth of the graph). So treewidth can be thought of as an upper bound to 'quantumness', although not a precise measure. 

For bipartite quantum states, where the context is two-party correlations, we have many many good measures of quantumness. Many have flaws, like being NP-hard, or not additive, but nevertheless we have a pretty sophisticated understanding of this situation. Here is a recent review. 

Like many complexity-class separations, our best guess is that the answer is that BPP^{HSP} != BQP, but we can only prove this rigorously relative to oracles. This separation was observed by Scott Aaronson in this blog post where he observed that the welded-tree speedup of Childs, Cleve, Deotto, Farhi, Gutmann and Spielman was not contained in SZK. On the other hand, BPP^{HSP} is contained in SZK, at least if the goal is to determine the size of the hidden subgroup. This includes even the abelian HSP, although I'm not sure how exactly to find the generators of an arbitrary hidden subgroup in SZK. The reason we can decide the size of the hidden subgroup is that if f:G->S has hidden subgroup H, and we choose g uniformly at random from G, then f(g) is uniformly random over a set of size |G|/|H|. In particular, f(g) has entropy log|G| - log|H|. And entropy estimation is in SZK. 

There are other contexts, such as when we have a quantum state and would like to choose between different incompatible measurements. In this setting, there are uncertainty principles that tell us things about how incompatible the measurements are. The more incompatible the measurements are, the more 'quantum' a situation we have. This is related to cryptography and zero-error capacities of noisy channels, among many other things. 

From the comment: in the mathematical field of graph theory, the Rado graph [1], also known as the random graph or the Erdős–Rényi graph, is the unique (up to isomorphism) countable graph $R$ such that for every finite graph $G$ and every vertex $v$ of $G$, every embedding of $G − v$ as an induced subgraph of $R$ can be extended to an embedding of $G$ into $R$. As a result, the Rado graph contains all finite and countably infinite graphs as induced subgraphs. In this note you can find other details, in particular: ... if we produce two random infinite graphs, then, with probability 1, they both have the extension property. Hence they are isomorphic (and also isomorphic to the Rado graph); in other words, any two random countably infinite graphs are isomorphic ... [1] Rado, Richard (1964), "Universal graphs and universal functions", Acta Arith. 9: 331–340. 

Suppose that a path in the first enumeration is $MINUS \rightarrow A_{I_i} \rightarrow A_{I_j} \rightarrow B_{I_k} \rightarrow B_{I_h} \rightarrow PLUS$, then the path is a valid solution iif there is a path from $I_i \rightarrow I_j$ and from $I_k \rightarrow I_h$ in the original maze (graph $G$). So we must expand the $A_{I_i} \rightarrow A_{I_j}$ and $B_{I_k} \rightarrow B_{I_h}$ traversals enumerating all the paths from $I_i$ to $I_k$ and from $I_k$ to $I_h$ in $G$. Infinite loops are detected when we are enumerating all paths from $I_i$ to $I_k$ in an expansion of a path that in a previous stage already contained $... \rightarrow M_{I_i} \rightarrow M_{I_k} \rightarrow ...$ for some submaze $M$ (there are only $n^2$ possible expansions). A solution is found if we find a path expansion that contains only inputs/outputs $I_i$; the maze has no solution if we cannot further expand the paths without loops. 

I've spent a lot of time on problems related to the computational complexity of (puzzle) games and I think there are many orthogonal aspects that can make a two-players or a one-player (puzzle) game attractive and fun: 

Some attention should be paid in order to prove that $M$ halts on all inputs (just note that it rejects on blank input and all non-halting symbols "cycle" through $( , S , Z$ or $<, >$ which cannot lead to an infinite loop). The language $L(M)$ is a superset of $L_Y$ ($L_Y \subset L(M)$) and it doesn't contain strings from $L_N$ ($L(M) \cap L_N = \emptyset$). Suppose that $L(M)$ is Context Free, then - by closure properties of CFLs, intersecting it with the regular language $\{r 0^* 1^* A\}$ produce a CF language: $L(M) \cap \{r 0^* 1^* A\} = \{ a \; 0^n \; 1^m \; R \mid m \geq 2^n\} = L_Y$ But by a simple application of the Ogden's Lemma for CFL we can prove that $L_Y \notin CFL$: just pick a long enough $s \in L_Y$ and mark all the $0$s; at least one zero can be pumped and whatever is the number of $1s$ in the pumping string the exponential growth of $2^n$ leads to a string $\notin L_Y$). So $L(M)$ is not Context Free. ... now I'm wondering if this is another "reinventing the wheel" answer, or it is a new (small) result. 

There is a discussion of this paper in section 8.1.2, Types as sets, in Cardone & Hindley, 2006 History of Lambda-calculus and Combinatory Logic; additionally section 10.1, Domain theory, traces back to this manuscript some crucial order-theoretic insights. 

The most comprehensive survey of the relationship between constructive proof theory (which is tied closely to the theory of constructive ordinals) and second-order impredicative arithmetic (which as Ulrik points out is equivalent in strength to System F) is Girard (1989). There he builds on his theory of dilators (1981), which I don't really follow, but I think essentially provides a nonconstructive theory of higher-order Skolemisation. My understanding is that you can't express $\Sigma^1_2$ formulae constructively in the Bishop—Martin-Löf sense, because they are impredicative in a way you can't eliminate by adding any sort of first-order induction scheme. I remember suggesting to an ordinal theorist that one could simply stipulate that you can ground an impredicative constructivism in a type theory based on the polymorphic lambda calculus, and use the reduction candidate technique from Girard's SN proof for System F to impose a reasonable total order on the universe of constructions, calling the equivalence classes you get from this the ordinals; he said something intelligent which I took away as saying you might get that to work, but it would have all the advantages of theft over honest toil. To get it to work, it is not good enough that you can prove in set theory the existence of such ordinals, you would need a constructive proof of trichotomy for the order. To sum up, with the regular notion of intuitionistic construction due to Bishop—Martin-Löf, the literature I know of strongly suggests no. If you are averse to honest toil and would embrace an impredicative constructivism, then my guess is that it can probably be done. You would, naturally, need a stronger theory that System F to constructively prove the required trichotomy, but the Calculus of Inductive Constructions provides an obvious candidate. References 

Disproofs of hypercomputation generally assume the validity of Bekenstein's bound, which asserts a particular limit on the amount of information that a finite amount of space can contain. There is controversy over this bound, but I think most physicists accept it. If Bekenstein's bound is badly violated, and there is no bound on the amount of information contained in a particular region (say, a black hole, or an infinitely fine and robust engraving), and there are arbitrarily refinable mechanisms to examine the contents of that region (say, by carefully examining the radiation emitted as a carefully constructed object falls into the black hole, or by running a stylus over the grooves of the engraving), one can suppose that an artefact just happens to already exist that codes a halting oracle. All very unlikely, but it does show that the claim that hypercomputation is impossible is not a mathematical truth, but based in physics. Which is to say that Andrej is right when he says we can imagine what it would mean to disprove [the Church-Turing thesis]. Namely, if someone built a device which (reliably) computed a function that cannot be computed by any Turing machine. 

Informally: the $a^3$ parts of $S$ force to match $3$ of the $S_i$s (otherwise some $S_i$ will not be included); the $c^3$ parts force to "close" $3$ of the $S_i$s. And the $b^B$ blocks (there are $m$ of them) must be filled exactly with the $b^{x_i}$ substrings of $3$ distinct $S_i$. Small example: 3-PARTITION instance: $A = \{ 3, 2, 1, 5, 4, 3\}, B=9, (m=2)$ 

$F$ needs only a limited look-ahead buffer (4 symbols) to make the rewrite and also to perform the $w+1$ operation; the internal states of $F$ embeds the internal states of $M$ so it can simulate the transition function $\sigma$; 

But there are many other puzzles/videogames that are directly inspired by the Hamiltonian circuit/path problem: Inertia, Pearl, Rolling Cube Puzzles, Slither,... ... and the "hardness" of HC makes them addictive: even small instances can be very hard to solve for our brain!!! 

NOTE: as noted in the comments, the trailing $Rx1$ (resp. $Rz1$) of the open (resp. close) sequences are unuseful and they can be removed (and $P_{c1}, P_{c3}$ shortened to $3m$ elements). 

After a failed polynomial-time quick attempt, here it is an idea to prove that it is NP-complete using a reduction from 3SAT. Given a 3SAT formula with $x_1,...,x_n$ variables and $C_1,...,C_m$ clauses, first build a variable assignment gadget like in the figure below (thanks to @Jukka for the clarifications, the graph drawing style, and the hint for the gadget with two configurations and same cost!). 

You can use the same argument used to prove the $\Omega(n^2)$ time bound on single tape. Suppose that you have a TM with $S(n)$ space that recognize palindromes $\{x\,0^{\frac{n}{3}} x^R \mid |x|=n/3 \}$ (where $x^R$ is the reverse of $x$) in time $T(n)$. When the (input) head crosses the middle $0^{n/3}$ it can carry only $S(n)$ bits of information. So it needs to make $\Omega(n / S(n))$ crosses, and each cross requires $n/3$ time. So $T(n)S(n)=\Omega(n^2)$. 

Just an extended comment: You can take a look to the approach used by Steinbach and Posthoff to find the 4-coloring of a 18x18 (and 12x21) grid without monochromatic rectangles: Bernd Steinbach and Christian Posthoff, Solution of the Last Open Four-Colored Rectangle-free Grid an Extremely Complex Multiple-Valued Problem. In Proceedings of the 2013 IEEE 43rd International Symposium on Multiple-Valued Logic (ISMVL '13) As proved by Gasarch et al. given a partial $c$-coloring of an arbitrary $n \times m$ rectangle, it is NP-complete to decide if the coloring can be extended to the whole rectangle without monochromatic rectangles: Daniel Apon, William Gasarch, Kevin Lawler, An NP-Complete Problem in Grid Coloring. So there are high chances that the problem is NP-complete even for equilateral triangles .... I think it would be a nice result to prove it. Just a side note: I spent weeks of CPU cycles on the monochromatic rectangle-free 4-coloring problem but I started from a wrong partial result (a wrong previous analysis that restricted the number of possible 1-color sub-configurations) and I used the STP constraint solver; you can achieve great improvements if you add constraints that break symmetries (e.g. an ordering on the coloring of a side of the triangle) and try to make an analysis of the possible configurations using only 1-color. EDIT: this is the result of an STP program for n=19 (~ 1 min.) 

Some years back, the rumour grapevine told me that more volumes of the series were planned, but the series was stopped at the fourth because preprints of nearly all the chapters were made informally available by the authors from their websites. The series is very classy, and it's a shame that it didn't continue. The rumour, if true, would support both sides of the debate over preprint culture: either it shows just how successful the preprint practice has been at making valuable content widely available at no cost, particularly to graduate students who might otherwise not afford more than limited access to these texts. Or it has undermined the effort of top-notch publishers like OUP to make first-rate, well-edited and well-typeset research publications available. Nobody needs these volumes, as supercooldave says. But if you have the money, and value the physical product, why not? It supports the best part of what looks to be a dying industry, and you will like holding it each time you take it off your bookshelf. 

Since supercooldave mentioned intuitionistic logic: without the rule of the excluded middle, model theory becomes much more complicated, and analytic proof theories become more important, typically the semantics of choice. Algebraic techniques, such as category theory, become preferred for abstracting away from syntactic complexity. 

giving the basic form ${\cal A} [P]$. More complex syntaxes are possible if we allow multi-arity propositional judgements. Usual formulations of first-order logic need only one propositional attitude, which is usually either "$[P]$ is a theorem" or the binary judgement "$[P]$ is a consequence of $[T]$". In the two-sided sequent calculus, we have a more complex theory of judgements, most commonly $H_1, \dots, H_n \vdash A_1, \dots, A_n$, where some logics have such judgements that are not trivially equivalent to any proposition of the logic's language. So different kinds of propositional are seen in fairly elementary classical logic. Martin-Löf's type theory resorts to a more complex family of judgements for three reasons: First, it is dependently typed, meaning that the propositions occur as syntactic entities inside terms. Second, he dispensed using a grammar to define which strings of symbols are valid terms and propositions, but used the inferential system to do so – a reasonable thing to do since propositions in such typed theories are generally not context free. Third, he devised a novel theory of equality, often called propositional equality, which leverages the beta-eta theory (or in some variants, just the beta theory), and the judgements that two terms share the same normal form are expressed using judgements expressing the beta/eta equivalence of two terms – again reasonable, given that beta/eta equivalence is computationally expensive and non-obvious in the proof theory. The judgements expressing beta/eta equivalence can be eliminated with not too much difficulty - have as the grounds for the introduction rule for propositional equality being that the two terms are beta equivalent (beta-eta equivalence is slightly more problematic) – but eliminating the judgement that terms inhabit types is much trickier; the least bad way I can think of for doing this is to reconstruct type inference in the term grammar, which leads to a more complex and less intuitive theory overall.