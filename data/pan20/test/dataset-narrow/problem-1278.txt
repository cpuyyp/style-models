In general, the solution here is "don't make it global," and to pass instances needed to functions that need them. In this simple program you probably don't need to worry about it too much. Think of more as something you should think about for the future. One way to refer to this concept is "dependency injection," which may give you something useful to search for (be wary though, as the phrase is also used to describe hideously over-complex "enterprise" engineering patterns as well; all I'm really talking about is the straightforward idea of passing instances of dependencies to objects or functions that require them). In the case of GLFW you can't modify the input callback signature, so consider looking into attaching data to the user pointer of the window to remove the global variable reference. 

That really depends what you mean by "correct." If you take the term quite literally (and ignore any concept of correctness of the implied design) then yes, it is perfectly acceptable. Your program will compile and run fine. It may perform sub-optimally, but it still may also perform well enough to be a shippable, fun game. 

This effect is particularly common in games that make use of cel shading effects, but is actually something that can be applied independently of the cel shading style. What you are describing is called "feature edge rendering," and is in general process of highlighting the various contours and outlines of a model. There are many techniques available and many papers on the subject. A simple technique is to render only the silhouette edge, the outmost outline. This can be done as simply as rendering the original model with a stencil write, and then rendering it again in thick wireframe mode, only where there was no stencil value. See here for an example implementation. That will not highlight the interior contour and crease edges, though (as shown in your pictures). Generally, to do that effectively, you need to extract information about the mesh's edges (based on discontinuities in the face normals on either side of the edge, and build up a data structure representing each edge. You can then write shaders to extrude or otherwise render those edges as regular geometry overtop your base model (or in conjunction with it). The position of an edge, and normals of the adjacent faces relative to the view vector, are used to determine if a specific edge can be drawn. You can find further discussion, details and papers with various examples on the internet. For example: 

As above, I would strive to avoid inheritance for this, and consider using established third-party solutions instead of rolling your own. Those steps alone should remove much of the maintenance complexity you're discovering (or will shortly discover). That said, another approach is to look at it this way: is a band-aid for an implementation detail problem. It itself doesn't represent a real "object" in any typical sense. So think about what might be a better analogy that also solves your problem of having a place to put the automatic and calls. Most inventories allow (some) items to stack, do they not? The "stackability" of an item is a property of that item, but the item itself should not be responsible for maintaining the stack. If you have a class representing an with methods to query the size of the stack, split the stack, et cetera then you can put your management of the lifetime of the item pointer (which, I should note, may become slightly more complicated with stacks, thus providing another argument to use something like Boost) in the stack class along with the implementation of the rest of its behavior. Items that cannot stack are represented by a stack that always contains a single item, and your inventory itself still deals with homogeneous instances of . 

Due to the nature of the Windows platform and the way it's evolved over the years and focused on retaining backwards compatibility, there are a lot of ways to do things. Each will have various pros and cons but often any option will work fine. You may be able to accomplish what you want in slightly fewer lines with DXGI, or slightly more (without knowing in detail what you are doing, it's hard to say). The DXGI approach is "more modern," if that matters to you. However, if your code is functional and (appears) bug-free, I wouldn't rush to change it. 

Yes, you can just create the HUD as a series of 3D objects that are always rendered at a fixed position in view space in a separate rendering pass (so they don't compete with depth buffer tests and the like with "world" objects). Fixing the objects' position to the camera in some fashion is one way to do this. You can achieve the "floating" effect by tracking the camera's current and previous view vector and interpolating between that over time, making sure to do your HUD elements aligned to the interpolated vector's view-space orientation at the not the true view vector's view-space orientation. 

The default constructor is only available when building with hot-reload constructors enabled; if you were to disable that option you'll get a regular old compile error, so you shouldn't rely on that constructors existence. 

The general technique is for the the server to maintain authority, and send the game state updates to the client as rapidly as needed. The client maintains a local copy of the game state which it simulates itself, predicting that most of the time the server will confirm the input it sends. When the server game state and client game state differ, you adjust towards the server state, usually interpolating to hide sudden transitions or rubber-banding effects. It's not clear from your post what the underlying transmission medium is (TCP versus UDP). Because you have a fast-paced game, you probably want to use UDP and simply send the client state updates as fast as possible. This is what is usually done in other fast-paced games, like first-person shooters. You can look up client-side prediction resources on Google. 

I would recommend wandering down to your local Large Chain Bookstore (Borders, Barnes & Noble, et cetera) and see if they have a copy of any of the books you are interested in. That way you can peruse them for yourself and hopefully get a better idea of whether or not a particular book would be good for you and your particular learning style. Amazon's "Look Inside" feature, if available for the books in question, may also allow you to do this to some extent. Personally, I'd be very cautious about buying a book that focuses on a specific API, largely because such books tend to go out of date relatively quickly and thus I don't see them to be as strong of an investment. I would thus suggest that you take a good look at all the freely-available online reference and educational material for cocos2d and game development in general (if you have not already, of course) and see if that is suitable enough for you. All of that having been said, I'm generally a fan of the O'Rielly books, however both of their iPhone books seem relatively old at this point. The Apress books have looked promising when I browsed them in stores, so that would be my secondary suggestion (the caveat being that I have not reviewed any of those books in detail and am basing this recommendation on my experience with the quality of the publisher's other material and quick flip-throughs of the books). 

A very simple stream provider would simply look in a specified asset root directory for a subdirectory named and load the raw bytes of the file named into a stream and return it. In short, what you have here is a system where: 

What you've described (authoring all your content to be normalized within a unit sphere) is not a common workflow. Generally, a reference frame is established within your modelling program (for example one modelling world unit is assumed to be one meter) and all objects are modeled against that reference frame. A building that is fifty times the height of a one-meter-tall character would be 50 units tall in your modelling program. Keeping the relative size of objects intact in their source form is useful if your asset pipeline ever involves working with multiple assets as references within a larger scene, as is typical with many large games. For example, many cinematic sequences are authored by importing asset references for the various characters into a scene and manipulating them within that scene. This becomes more cumbersome if you then have to apply scaling factors to bring everything into correct relative frames. It also means you have to store the appropriate scale for an asset somewhere, which is generally illogical if you could just store it implicitly in the asset data itself by modelling it at the actual size. So basically, it is a far less confusing workflow to simply build assets at the proper size originally. That said, it's entirely doable to build everything normalized and apply some scaling factor at runtime. It just introduces a lot of complexity for very little benefit (you're already transforming and rotating the geometry every frame, so scaling incurs no additional cost... but it also doesn't speed anything up). You can simply scale the pre-computed bounding volumes and other related shapes at runtime by the same factor you're scaling the render model by, if you want. It's just not very natural. 

The first option seems overwhelmingly preferable: it allows for the particles owned by a given emitted to be processed in bulk, and it allows you to do better broad-phase culling of objects. It provides locality of reference to your particles and implies far less per-frame maintenance of the same. The second "global particle list" option is a bad design because it relies on a global unnecessarily and implies that culling non-visible clusters of particles involves inspecting every particle, or at least re-associating every particle with an emitter every frame, which will be inefficient. 

A hash of the state is the right general idea, but the wrong specific implementation; two different states could hash to the same value, and while that might be a rare occurrence (depending on your hash algorithm), it's still a case you want to handle. Instead, consider a sequence ID. A sequence ID starts at some initial value and is updated by the server after every validated change to the game state. When the server sends an update to all clients, part of that update includes the new state sequence ID. When the client sends a request to change something to the server, it includes the sequence ID that goes along with the state it currently has. When the server gets a request from the client, if the included client sequence ID is not the same as the current sequence ID, the update is rejected because the client had a stale state; the server should inform the client (along with the new state and sequence ID). Otherwise the server accepts the move, updates its state and sequence ID, and broadcasts the change to all clients. Obvious this update on the server has be synchronized, but that's true anyway since you're modifying the game state anyhow. The downside to this approach is that the client cannot dispatch multiple state change requests to the server; after sending one, it has to wait to get confirmation and a sequence ID update. There are several solutions to this, but I think the ideal solution is to simulate the successive moves locally as if the server said "yes" to everything, but keep them queued in memory. As the server confirms each move, send the next queued action until the server state catches up with the client. If at any point you get a response from the server that wasn't a confirmation of that client's own action (that is, an update from another client), discard the remaining queued up actions and resynchronize the game state, displaying an appropriate error message to the user if needed. This system can be used as the basis for fault-tolerate (that is, crash-tolerant) persistence systems as well -- if you have a storage backend with transactional guarantees, like a database, you can persist update requests and sequence IDs and be able to resume the same sequence of transactions after a crash. It can allow for idempotent operations, also; what I've described is a simplified view of the basis for item transfer systems in the MMOs we build where I work. 

It's probably the case that you're only going to find what you want as part of a larger 3D art production suite. For example, Blender should be able to do this, as should any other 3D modelling package (such as Max or Maya, although those are non-free and in fact quite expensive). Blender and the other tools of its class are very powerful and can do all that you need (and much more), but they do have a bit of a learning curve associated with them. Fortunately Blender has a lot of documentation and tutorials available. 

which uses the def instruction to define a constant vector in register and uses that as the second multiplicand to . You could also use swizzling on the register, so you could put different constants you may need to multiply by in 's components. You can also just write the multiplication in terms of and assign it values via the APIs. 

When rendering with multisampled anti-aliasing, a coverage value is computed for each fragment; this coverage value is based on the fraction of the pixel that would be covered by the fragment based on the triangle that created the fragment. The net result is that the edges of the triangle are anti-aliased. Because the coverage is based ultimately on what the originating primitive covers, only the edges are anti-aliased. Fragments on the interior of the primitive are rendered normally. Blending is applied to those interior fragments, but to do blending properly one must normally sort one's primitives from back-to-front to achieve correct results. Sprites or other billboarded textures can be sorted (although it's sometimes expensive), but non-billboarded quads are sometimes used for rendering large volumes of grass, foliage or hair and cannot always be effectively sorted back-to-front. Alpha testing can be used in such a scenario (which essentially allows the alpha channel to determine if the fragment is visible or not, without blending it) but creates aliasing. Alpha-to-coverage attempts to solve the problem by allowing the alpha value of a fragment to be used as its coverage value (the value that would normally be computed based only on the primitive). This allows you to have varying, appropriate coverage values for the fragments within the source primitive and achieve nicer looking anti-aliased transparency when doing MSAA.