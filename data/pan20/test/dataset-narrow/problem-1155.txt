Is there a publicly available testbed of perfect graphs? Ideally, the number of vertices should be 100 or more. 

The answer is "yes" if and only if the graph is bipartite. (counterexample: the 5-cycle) The answer is "yes" if and only if the graph is triangle-free (counterexample: the cartesian product of an edge with the 5-cycle) 

If the graph is bipartite, then the answer is "yes." If the graph has a triangle, then the answer is "no." 

If we consider the minimization problem $\min_y \{c^T y : Ay \ge b, y \in \{ 0,1\}^n \}$, then the following reduction shows that an algorithm running in time $O(2^{\delta n/2})$ for $\delta <1$ would disprove the SETH. A reformulation proves the same result for the intended problem (the maximization version). Given an instance $\Phi = \wedge_{i=1}^m C_i$ of CNF-SAT with variables $\{x_j \}_{j=1}^n$, formulate a 0-1 IP with two variables $y_j, \overline{y}_j$ for each variable $x_j$ in the SAT instance. As usual, the clause $(x_1 \vee \overline{x}_2 \vee x_3)$ would be represented as $y_1 + \overline{y}_2 + y_3 \ge 1$. Then for every variable $x_j$ in the SAT instance, add a constraint $y_j + \overline{y}_j \ge 1$. The objective is to minimize $\sum_{j=1}^n (y_j + \overline{y}_j)$. The objective of the IP will be $n$ iff the SAT instance is satisfiable. Thanks to Stefan Schneider for the correction. Update: in On Problems as Hard as CNF-Sat the authors conjecture that SET COVER cannot be solved in time $O(2^{\delta n})$, $\delta <1$, where $n$ refers to the number of sets. If true, this would show that my problem cannot be solved in time $O(2^{\delta n})$ as well. Update 2. As far as I can tell, assuming SETH, my problem cannot be solved in time $O(2^{\delta n})$, since it has been shown that Hitting Set (with a ground set of size $n$) cannot be solved in time $O(2^{\delta n})$. 

This problem is NP-hard. David S. Johnson mentions on pg. 149 of this column that the problem remains hard in planar graphs of maximum degree three with all weights either $+1$ or $-1$. I cannot find the cited paper -- A. Vergis, manuscript (1983) Any ideas as to where to find the paper? Or what the reduction was? 

Edit Actually, the part of the model about initialization and measurement does matter a bit. Even if we assume that we know the global phase differences between the modules after initialization, we certainly have to assume that measurement is restricted to each module alone, and that coherence with other modules is lost after measurement. I still don't think that this will destroy the quantum speedup, since the global state can be (quantum/unitarily) transformed suitably before the measurement, hence one should still be able to perform any desired single measurement at the end of the quantum computation. 

Minimum Dominating Set is not an isomorphism problem, hence there is no reason why it should be expected to be reducible to GI. 

Here $P(Q)$ is the power set of $Q$ and $ssM(Q)$ is the space of substochatic matrices on $Q$. A right substochastic matrix is a nonnegative real matrix, with each row summing to at most 1. There are many different reasonable acceptance conditions The transitions are only one part of a machine, initial and final states, possible output and acceptance conditions are also important. However, there are only very few non-eqivalent acceptance conditions for deterministic machines, a number of reasonable acceptance conditions for non-deterministic machines (NP, coNP, #P, ...), and many possible acceptance conditions for probabilistic machines. Hence this answer focuses primarily on the transitions. Reversibility is non-trivial for probabilistic machines A partial function is reversible iff it is injective. A relation is always reversible in a certain sense, by taking the opposite relation (i.e. reversing the direction of the arrows). For a substochastic matrix, taking the transposed matrix is analogous to taking the opposite relation. In general, the transposed matrix is not a substochastic matrix. If it is, then the matrix is said to be doubly substochastic. In general $P P^T P\neq P$, even for a doubly substochastic matrix $P$, so one can wonder whether this is a reasonable notion of reversibility at all. It is reasonable, because the probability to reach state $B$ from state $A$ in $k$ forward steps is identical to the probability to reach state $A$ from state $B$ in $k$ backward steps. Each path from A to B has the same probability forward and backward. If suitable acceptance conditions (and other boundary conditions) are selected, then doubly substochastic matrices are an appropriate notion of reversibility for probabilistic machines. Reversibility is tricky even for non-deterministic machines Just like in general $P P^T P\neq P$, in general $R\circ R^{op}\circ R \neq R$ for a binary relation $R$. If $R$ describes a partial function, then $R\circ R^{op}\circ R = R$ and $R^{op}\circ R\circ R^{op} = R^{op}$. Even if relations $P$ and $Q$ should be strictly reversible in this sense, this doesn't imply that $P\circ Q$ will be strictly reversible too. So let's ignore strict reversibility now (even so it feels interesting), and focus on reversal by taking the opposite relation. A similar explanation like for the probabilistic case shows that this reversal works fine if suitable acceptance conditions are used. These considerations also make sense for pushdown automata This post suggests that one motivation for non-determinism is to remove that asymmetry between forward steps and backward steps. Is this symmetry of non-determinism limited to finite automata? Here are corresponding symmetric definitions for pushdown automata 

It seems that many people believe that $P \ne NP \cap coNP$, in part because they believe that factoring is not polytime solvable. (Shiva Kintali has listed a few other candidate problems here). On the other hand, Grötschel, Lovász, and Schrijver have written that "many people believe that $P=NP\cap coNP$." This quote can be found in Geometric Algorithms and Combinatorial Optimization, and Schrijver makes similar statements in Combinatorial Optimization: Polyhedra and Efficiency. This picture makes it clear where Jack Edmonds stands on the issue. What evidence supports a belief in $P\ne NP\cap coNP$? Or to support $P=NP\cap coNP$? 

An answer to your problem tells you whether there exists a clique of size k. So, any such algorithm for your problem must be at least as slow as clique algorithms. See the wikipedia section on FPT and clique for more. 

/* My response no longer applies after the question was edited. */ The unweighted versions are trivial. The answer to "How many vertices can be removed so that $H$ is still a subgraph?" is $|V(G)|-|V(H)|$. The answer to "How many edges can be removed so that $H$ is still a subgraph" is $|E(G)|-|E(H)|$. The complexity may be different when the graphs are weighted or if you don't assume a priori that $H \subseteq G$. 

My first observation: we know how to solve your LPs in strongly polynomial time, but this is an open question for general LPs. 

The case $diam(G)=3$ is due to Brandstädt and Kratsch, and the case $diam(G)=2$ is noted in a recent paper of mine. 

Yes. You could find the actual cut using $n$ calls to the oracle. Let the vertices be labeled $1, \dots, n$. Add a dummy vertex $0$ with outgoing edge weights equal to zero. For each vertex $j=2,\dots, n$ tentatively merge $j$ with vertex $1$. If the max cut objective remains the same, leave $j$ and 1 merged. Otherwise, merge $j$ with $0$ instead. 

You might need to be more specific as to what "slightly super-polynomial" means. But, this might work: you can take an NP-complete problem, and pad it. For example: for any constant $k\ge 1$, $n$-vertex instances of CLIQUE remain hard when only $n^{1/k}$ of the vertices have positive degree (e.g., take an instance $G=(V,E)$ of CLIQUE and add $|V|^{k}-|V|$ isolated vertices to it). These instances of CLIQUE can be solved in time $2^{n^{1/k}}\text{poly}(n)$. 

It might be possible to keep the "$\mathbb{N} \to \mathbb{N}$" part, and replace the "(partial) functions" part with something else (I am thinking here of an analogy Fermions <-> "(partial) functions", Bosons <-> "something else"), but the Church-Turing thesis would probably still hold in such modified settings. 

The Church-Turing thesis is about (partial) functions $\mathbb{N} \to \mathbb{N}$ (or $\Sigma^* \to \Sigma^*$ for a finite alphabet $\Sigma$). How do you define a definite value based on some random output (for a given input)? There is at most one (random) output occurring with probability $>\frac{1}{2}$, so taking that output if it exists (and undefined otherwise) as definite value could be a reasonable definition. (One might argue that $\frac{1}{2}$ is too small for physical implementation and one should take $\frac{2}{3}$ instead, but I guess that is not really important for this answer.) The Church-Turing thesis now asserts that even this extended notion of computability for (partial) functions $\mathbb{N} \to \mathbb{N}$ still leads to the same set of computable functions. 

If a massively online collaboration is set up, then it should try to focus on problems with a reasonable chance of success. The three classical construction problems of antiquity are known as "squaring the circle", "trisecting an angle", and "doubling a cube". Modern mathematics resolved all three, but much more important was the earlier Descartes revolution, which enabled mathematics to free itself from the mental prison of compass and straightedge constructions. Notice that the Greeks used compass and straightedge as a practical computational device, as witnessed by the efficient epicycle approximation scheme for celestial mechanics computations. Many conjectures and generalizations of solved conjectures from graph theory should be amenable to solutions by collaboration. However, typical experience with collaborations suggest that teams of 2-4 members are far more effective than significantly larger teams. An example of a very successful team in this area are N. Robertson, P.D. Seymour and R. Thomas, which attacked problems like the strong perfect graph conjecture, generalizations of the four color theorem, and graph minor related conjectures. The elapsed time between announcement of new results and their actual publication has been notoriously long, also for other teams of researchers in the same area, indicating that pure workload volume here is slowing things down, so that collaboration (which already happens) could be beneficial to speed things up. (I'm still curious when some of the announced generalizations of graph minor related results for directed graphs will be published, and whether the people who will finally publish them will be the ones which initially announced the results.) I currently try to understand the role of completeness of intuitionistic logic in practical applications of computer assisted proof refutation. But if you really plan to do proofs by massively online collaborations, then having a solid computer assisted proof refutation system in place might really be important. After all, if you don't know your collaborators sufficiently well, how will you be able to judge whether you can trust their contributions, without wasting a significant amount of time checking everything they did? (I have the impression that mathematicians are more used to proof refutation and enjoy its positive sides like direct personal feedback, while computer scientists show less routine with this sort of feedback.) Anyway, establishing that proof refutation is a natural part of collaboration when tackling difficult problems seems like a good idea to me.