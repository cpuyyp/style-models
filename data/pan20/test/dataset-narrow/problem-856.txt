The short answer is, always use whats provided by the system repositories. Be very careful what repositories you do install too. Some are just plain bad. You shouldn't ovewrite the systems packages with newer versions, Redhat is designed and orchestrated very carefully and you might end up with strange bugs or problems if you do. Some things to consider and look out for which can cause issues include. 

GSSAPI authentication is not handled by PAM. The PAM module for kerberos is used for password authentication of a user, using the kerberos protocol to obtain a valid ticket. There are 3 outcomes of the GSSAPI authentication. 

Freeing space due to inconsistencies is making a huge flawed assumption -- that the corruption is to do with files, not the actual structure of the filesystem. Some corruption simply cannot be fixed by merely freeing the associated space. What if the space it frees is a base directory for all the tree of directories you keep your data? Just use tmpfs. Create a big swap file or partition. will be swapped out if necessary in favour of other items. If you do this you'll want to set the kernel knob to 100 to make cache and anonymous memory values equal to one another. Edit: I prefer @zordache answer. Just rectreate the filesystem on startup on the associated disk that uses it. 

Well, no it doesnt, compiling and running it indicates its not flushing this data at all, so somethings amiss. I get the following from strace using the options you specified and the environment variables you used.. 

Another problem can be that bacula has ran out of maximum connections, but you can just restart the agent in this case to fix that. 

No, but the kernel fills up a buffer only up to a maximum size then will discard data. It does not pose a performance impact in terms of using up too much memory having rules not listened to. 

OS corruption is quite unlikely (system calls being screwed somehow I guess). hdparm does not utilize the filesystem to do its tests which eliminates any slowdown from that area, including degragmentation issues. If your using LVM, theres a risk that you happen to be reading from extents that are fragmented though. Your examples dont indicate this however. VM corruption, well anythings game I guess and thats down to a number of factors I can think of off hand, but probably would include more: 

I have 14292208 Kb free. About 300M of memory has been used up. But, if I go by what RSS is telling me I've actually used 10GB of memory! Finally, if you take a look at the process mappings, you can see that the virtual memory addresses are the same as one another. 

There is no clearly defined way of determining in any tool I know of which processes share which maps without iterating through all the mappings and comparing addresses. However, linux does offer reasonable estimate known as the Proportional set size. This is reported in /proc/[pid]>/maps. This value is the size of the mapping divided by the number of siblings/parent processes with the same mapping open. So, with a program that has a 1MiB mapping open, plus a 1MiB shared with 4 other processes, the proportional set size is 1MiB + (1Mib / 4) or 1.250 MiB. The RSS in this case would be 2MiB. There is a patch for htop floating around which will use the PSS to calculate a 'good estimate' of actual memory in use. 

When you run with setuid binaries you only change your effective UID not your real one. So the call will always fail. You should remove the . Its redundant in this case since you use fopen to open the file anyway, its also racey to perform an access check like that and then perform a read on it. 

The answer to your question is you are using more memory than 72%. You can calculate it via the OOM Report. The bit that tells you what the state of the memory was is this bit: 

My first suggestion is ask yourself why you need to encrypt the SSL certificate for, and what losses this produces on your operation if service is down for, say - 2 hours until it is discovered. What are the potential losses to you if the certificate is breached, and how much time would it take to rectify? If it costs more than your SSL certificate itself and the time to recover in the event of a breach is too high, then perhaps it is right to keep it encrypted. To be blunt, you cant have your cake and eat it. If you want to encrypt the SSL key, then you do so at the cost of non-graceful restarts and delays to your operation. If you encrypt it but provide a way to automate decryption, its probably just as worthless, or potentially more dangerous than having the key decrypted in the first place. If you make it your VPS providers responsibility, then your just shifting the problem onto them and it will likely either cost you a lot to give you the guarantees you want, or you'll have to accept that there will be times where the VPS provider doesnt meet your expectations when they reboot the VPS. My suggestion, is to make reboots timely and with forewarning, I dont think it would be unreasonable to ask your VPS provider for this. That way there shouldn't be an incident you cannot at least put under some control. *The problem of using SSLPassPhraseDialog is a similar, more common and problematic mistake people do with cron jobs. That is, deploy cronjob to run as root, then make the files ownership of whats run a non-root (say FTP) user who can potentially modify the application to escalate privileges. If you write a program to spit out the passphrase, make steps to ensure that the file is not readily readable nor is the file modifyable with anything but root (this includes making sure its parent directory its kept in is not owned by someone else). 

Yes. The most succinct way I can describe what a sysadmin does is to balance access, and availability to resources. All IT infrastructures need someone who can do this no matter what platform you sit on. 

You've got your accept rule in SMTP after your reject rule before the rest of the unmatched traffic. Basically, your and port will never match due to the order of which you've added them to the IPtables chain. 

mtr can really only give you a ballpark figure. Many routers will drop ICMP packets as part of a quality of service regime (icmp being less important to it than tcp/udp traffic). Others may delay the traffic, or do both. All you can really say is that sending ICMP traffic which that router should respond to may result in unreliable performance, but that you cannot say the same holds true for other types of traffic like TCP. To summarize, if you have genuine loss of packets to a particular destination caused by a router mid-hop, you will see <= loss % all the way down the future hops. If your destination hop responds with 0% loss, you are not dropping packets. Some routers deliberately drop ICMP traffic they are responsible for responding to, thus you may get 'additional loss' confined to just that hop. If that hop is BOTH performing some form of traffic shaping AND really losing traffic things get horribly muddled because you cant tell how much loss you really have. Instead the best you can do is take the lowest loss % from a future hop and state that its probably around that %age of loss you are seeing. 

Posix ACLs are the only clear-cut elegant way to do this, this is how I deal with shared read/write resource conflicts particularly on web-based systems. Here is a running example. In my example I have a directory called . In addition I have the users , , and First, I have created a group called and then added the users to this group. 

Which will alter the routing rules out of the host. You might need to clear your arp cache after doing that. 

This happens because RSS is not an authoritative value that tells you how much of that memory being used by that program. Its an authoritative value as to how much resident memory is mapped by that program. And there is a difference. RSS can at best be only used as a hint to how much memory you are utilizing. The kernel has a lot of tricks up its sleeve to save memory. Processes may share lots of memory, especially processes that fork. If you have a parent that allocates 100M of memory, then spawns a child both of these processes will share that area of memory, both the parent and the child will claim to have an RSS value of >= 100M, because they both map to the same region of memory. Technically this is correct, the RSS for the parent process is >= 100M as thats how much memory the process has mapped, and the child process also has RSS >= 100M because that process also has that much mapped, just it happens to be that both processes share (mostly) the same mappings. You can demonstrate this with some simple python. 

I noticed you tagged fastcgi. I suspect thats your problem. PHP (assuming thats what your doing this with) only terminates the connection once the interpreter has exited and cleanup starts, meaning the connection remains open as idle until garbage collection. FastCGI typically doesn't 'terminate' as it runs as a service which is why your experiencing these long sleep times. The best way to fix this it to explicitly declare a mysql_close() at the end of your code. Anyway, its best practice to clean up after yourself! But as a kludge workaround you could set a wait_timeout in mysql to destroy the connections. 

Next, in the document root for each host, relabel their document roots to the same category as the ones labelled in the httpd config. 

EDIT Thinking about it, is practically doing what the webserver does. Try setting and to and respectively, then see how that holds out. If you rewrite a policy for it you need to consider a number of things: 

Now you are duly notified of these dire warnings. This will clear the output displayed in . It will not clear your history, nor will it clear the bash job history (such as running the process like ). But will no longer show the arguments. 

Change the network description for the libvirt network to not do nat. From the VM host run . Then the network that the VM lives in to remove the natting. 

Cant you just put all your data in another directory like or ? Frankly is it really that much of an effort to backup multiple paths? 

It is most likely the 'hidepid' option to the mounted /proc filesystem. Check . If or , remove the mount option and try again. 

That works out as being only between 34816 and 32768 bytes away but we cant say which sector is damaged out of the four that comprise the block. If I had to hazard a guess, I'd say that theres probably a whole slew of blocks around the same address which will report I/O errors (assuming the raid striping is say 32k in size or whatever). Additionally a read may not pick up the problem if the RAID is fetching the block chunk from a different disk. A write must propagate to all the disks in a RAID1 setup anyhow so this could make writes fail but reads succeed. Additionally, if we assume that the chunk size of the RAID card is 32k we can also assume that the damaged block plus the one reported by SMART are both damaged by whatever happened on that platter. Its just the SMART test read from the good disk for the first 32k and the bad disk for the next 32k. Modern hard disks keep 'reserve sectors' to replace damaged sectors like this with a new sector location. Seeing as you are now getting this, and the that message from smart I'd say a disk has ran out. In terms of doing something about it; thats a bit more trickier. The LBA addressing is an abstraction against the real disk underneath. You'd need to identify which disk it is that is causing this issue, fail it in the RAID array and replace it. In any case, you have a bad disk and you should look to replace it ASAP.