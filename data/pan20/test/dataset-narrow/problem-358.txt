I'm not very clear on what you mean here, if what you mean is to have an Access frontend where you collect data and you want it to 'send' this data also to an SQL Server database (which sounds very eclectic, btw) you can use VBA for that. Heres a discussion on the matter that would do just that: VBA to Append Access data to SQL Server Table If this is not the case then you'll need to update your question with more info. 

So I'm trying to get some data from an SQL Server 2008 instance, the query is quite simple in structure but it has an extremely poor performance, it takes just under a minute to retrieve a bit over 1,000,000 rows (from SSMS). Here you can see the execution plan. 

The data is intended for statistical purposes and the query is being used from R-Studio (through RODBC), when querying from this environment its 'breaks' somewhere near the 450,000 - 500,000 rows. I figure RODBC is timing out somewhere near the 30 Sec mark... The query 

This is not appealing to me since I tend not to trust the user input, I want more control thus being able to trigger an error and not importing any part of the file would be better. I'm not sure if this effectively wipes the possibility of using for this process? I also tried reading line by line and inserting using a cycle, this is effective but unfortunately its takes to long even after optimizing my code I just went from 15 minutes to 4, and that's only for 850 records. I'm a bit mad that the process worked so smoothly when running from the Access frontend. That was because of the VBA method. This allowed for fast XLS import into a temp table and from that one the execution carried on updating tables. I would like to know what other options have guys tried when importing data from flat file sources into SQL Server. Admittedly I'm a bit confused at this point. Any help would be appreciated. 

I just came across a somewhat old database (and frontend) and it has funny way of dealing with the aspect of unique ids. I've got one table with a single column and row storing an Integer (currently 31448). This number is used on an table as 'InvoiceNo', the table also has a unique auto-inc id (currently 2847) It looks a bit like this: 

So I went for a cycle that loops through the rows adding up correlatives by two and storing the result in a variable, then, and depending on the result of the result variable (Positive, Negative) it would either progress by using the last number (positive) or retain the current one (negative) for the next operation. 

You could deal with such scenario by using Descriptive Metadata tables, have a look at the Wordpress database diagram. As you can see it makes use of three metadata tables that can store any number of parameter per Post, Comment and User. So in your case a bare-bones implementation could work like: 

I just want to shadow @BradC a little. I tested his code on Visual Studio 2015 (Update 3) on a Windows 10 Pro x64 machine against an SQL Server 2014 instance and I can confirm you can get Extended Properties of a field without issues. To replicate I use one of my local databases. I used the following code snip to define some extended properties. 

Right now I'm stuck in trying to get the same output as the original query, since my its doing the trick but returning output for 3 different queries. I also cant manage to get to "gain" a new value in each round, sort of like a kind of name. Any hint is appreciated 

This will create a temporary table to store all the table names and their respective columns, it will then perform an update (in this case) per every column on every table only when datatype is VARCHAR. 

Am I missing something radically obvious? I saw something simillar by searching on a forum and the suggestion was to add 2 new tables, but that look overkill in the long run. Suggestion goes about creating a table holding the 'Action' groups: 

This other one when down a bit and then further down in size and finally got bigger than the first time it backed up with compression. 

This will output: , and again, the final number would be 1 instead of -1 unlike described in the question. Hopefully the OP will pop along to clarify a bit more. 

Is really up to you if you want to normalize or not, depends largely on the requirements of your implementation and how are you displaying (or not) the data... I think an alternative approach to the dataset you present would benefit a lot of a structure such as: 

Which is nice and easy to use for generating a chart, for example. But as I mentioned earlier I started to look into this sort of query and I believe they can turn into something more compact as in a cycle. I'm aware of the old time If its not broken don't fix it so any input in the matter would be appretiated. So far I'm stepping into something around these lines: 

Unless I completely destroy the question in my head this is what you meant by two rows out of each one: 

Of course you can now access the properties and use their values. The Count value that wasn't working on your example: 

So basically since we have 4 Action groups it would store 4 records per every original record up in Records table. In a nutshell the problem is to continue with this multiple references and cope with it when joining the tables or look for a different approach, whatever that is. 

Troubleshooting: Timeout Expired A list of Typical Causes of this problem can be found here Additional tips here Hard to tell what can be causing this with no details, there's a number of approaches you can try each one depending on a different scenario. 

SSIS does comes with SQL Server evaluation. Additionally if you're a small company or a start up you could take advantage of the Bizspark program and test the full version throughout and/or develop with it for 3 years. And to spice it up a bit this article can give you an additional input on the subject before you jump in: The Hidden Costs of SSIS 

No actual relationship between the two tables other than the InvoiceNo which is not actually an enforced relationship. The frontend then, when the user adds a new invoice performs the following: 

Im having some 'perception' problem, maybe... I got this table linked multiple times with another small table and maybe is more of a personal problem with the way it looks but I feel there might be a more 'minimal (?)' way of doing this, or its just the ugliness of the diagram that upsets me, Anyway, here's what it looks: 

I've got a new SQL Server 2014 (12.0.2000) instance with about 50 databases, it was recently upgraded from SQL Server 2005. They were using a lot of backup space so the old maintenance plan was replaced to a different approach: Check Db Integrity -> Back up Database (Full) (with Compression) -> Rebuild Index -> Maintenance Cleanup That happens everyday while databases aren't being used and it preserves backups for 5 days. At the beginning it worked OK and a lot of backup drive was saved but then some of the databases started backing up with erratic file sizes. A few images that display the file size change: This one went down a whole lot, that was expected. Thing is, it then when down another two times (?) 

Use this version of NW instead. Its very light (2.6 MB in disk) and you can download the scripted version. Should be just fine. 

I tried for a bit to replicate your scenario and finally I managed to effectively send an email to myself using SMTP, but I had to create an SQL CLR stored procedure as follows: 

In some cases as bad as over 20 different versions of the same street... The most beautiful part is that the data from this database is being referenced from at least 5 other databases in the same server, making every change a very risky process. So I'm thinking, what steps can be taken to get rid of the duplicates?, What alternatives there is for avoiding the address table accumulating such an amount of data inconsistency?. Maybe even to ask, is there any salvation for such a big mess? It really is a nightmare. 

Finally this one went down in size, further down next day and then up the that place it was the first day. 

Regardless of the way the OP formulated the question and the lack of background behind the problem I think a possible solution would be to use a WHILE cycle. First I got a consideration: The OP mentions that for the last row the result should be -1 which does not makes sense if you follow the original pattern. That in my head would go as: 

From MS Access: Select 'Database Tools' from the Ribbon, inside there's the 'SQL Server' option (also called Upsize Wizard). Once in the wizard you can choose between creating a new table on the SQL Server database of your choice or using an existent table (if you choose to use an existent table you are to make sure the field types on your Access database match the ones on your SQL Server one). In the next steps you must select your SQL Server instance and also which fields you want to upsize along with indexes, table relationships, etc. 

(Forget about merge replications and all other fancy methods). So after all kinds of test including Azure and other cloud services I realized I was gonna have to go with something far less orthodox. I created a new custom TCP/IP service (basically a silent console application that runs in the background on a separate thread than the users frontend) using C# that basically queries the database on either end and finds modified, deleted and new records every 15 minutes (using GUID to make sure records on both sides have same identifiers) it will then compress and encrypt this data and send it to the other location where a queue table with timestamps will determine whether or not the records are to be committed. If a record on the receiving instance has a newer timestamp that the record being brought in (accounting for a 4 hour difference) then they go into a conflict resolution routine. Then some modifications to the users frontend where necessarily mainly so they're aware of data changing on one side or new records added to the database. All of this was possible because they only needed 3 tables to be doing this cycle, I will not recommend it for drastically more complex scenarios. 

Completely forget about the "user-centric" model and think to a lower level, Individuals. Individuals can have any number of Accounts and Accounts can have any number of Roles. Individuals can be represented on any number of abstractions around the database or even across databases. What do you think? 

I've read a few blog posts like this or this (there's quite a few) after I inherited my first SQL Server Instance and noticed many objects where prefixed, I wanted to start developing new ones with a less verbose approach and singular naming convention (lot easier for me to [and possibly other developers] work on Object Relational Mapping). One of the most widely used prefixes was or , but then I had and and even 

Here's everything you need to know about Microsoft Support Lifecycle Policy and from here you can search for specific products lifecycle information. Specific for 2008 family here. Interestingly enough it was announced on the 18th of July this year that SQL Server 2008 Service Pack 4 update is to be supported until 7/9/2019. Source 

Probably better to work with a metadata table. This, because the combination between different rooms and facilities can be quite hefty. Have a look at this approach. I created an entity called Establishment so you can have Hotels, Hostels, B&B, etc under the same table, the difference comed from the type defined on your EstablishmentType. Specific details of addressing, for example, can be stored on another table that can be shared with the Individuals Entity (not represented in this example). Anyway, test schema: 

So they do restore the way they're supposed to but I'm here wondering, how come they vary in size, I can totally understand the change after the compression but after (?). Why would the backup behave in this way? Any known reasons? Additionally: Nothing really going on in the logs. 

About the images: You should probably keep them stored on a file server and reference them from within and additional media table. 

I feel that you could add an additional Entity in there, one that represents people as a part of this (school-like?) institution, call it "People" (yeah...) So , , , , etc are all People. That table will hold Attributes that the sub classes/entities will share (because they're all people, right?): Firstname, Lastname, DoB, etc. You then refer to this table on your other entities. Eg the by using the FK (or whatever you call it =) A student is then able to become a tutor and even a Teacher or a Head of Department at some point without being added to simultaneous tables within the database. To, then, define a relationship between your , your and your you'll use a junction table in between that could look like: 

A way of knowing why is this error happening would be to check the SQL Server log and look for details into why is this happening: Try to login again with SQL Server Authentication, after getting your Error 18456 change to Windows Authentication and in your Object Explorer go to: Management -> SQL Server Logs -> Current - XX/XX/XXXX XX:XX:XX Now search for your newly logged error and identify the error State With this State number assest the nature of the error using the following table: 

Install instructions on GitHub Sample usage: Generate DDL scripts for all database objects and DML scripts (INSERT statements) for all tables in the Adventureworks database and save the script to a file 

I would use another table with the key values (also suggested by @McNets) feel its less verbose and more scalable. 

What's wrong its trying to copy such amount of data (straight from a table, mind you) to the Clipboard. But since this is Access we're talking it wouldn't shock me that's suggested over Exporting the data safely into a format like .CSV or Even better .SQL Is funny that you mentioned the system architecture because on first sight I would check that as possible cause. I would highly suggest changing the approach into something more around this lines: $URL$ 

On trying to query and work from Visual Studio this is just hell, I wouldn’t matter having a whole set of table prefixed but please just keep it that way for the whole Database, at least. So in the end I would definitely say is a matter of personal preference but it also has a lot to do with consistency and if you go down that road, a lot of people will be happy at least you keep it the same along your development. ...On the other hand I just wanted to say, if you take other approach and go for non-prefix, singular-name table you will make a developer happy in the future, and what’s more important than happiness?