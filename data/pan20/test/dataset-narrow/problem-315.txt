You'll get that error when one of the columns in the select won't fit into the destination table. Tracking it down can take some time, but it's just like kindergaten - try and fit the shapes in the holes. If you compare #Variable_info to #stored_proc_info, you will probably find that one of the columns in #variable_info is larger than its corresponding column in #stored_proc_info. I can't find #variable_info in my version of the sp_blitzcache, so I can't tell which columns don't match. Often, I track it down by running a query like this: 

Question: if I'm getting duplicates, could it be the fetches? do they both need to be fetch next, or neither? 

Hit next and you should be on the page to Specify Table Copy or Query. In this example, hit the button to Write a query to specify the data to transfer, but you can also create a view or table, if you prefer. On the Provide a Source Query page of the wizard, paste all your sql. In this example, I need to paste: 

I'm not a replication guru, but record may be marked as deleted only in case when an update is generating Page Split. For the case of replication you can test it by looking at DBCC PAGE and look through transaction log. 

Can see it only on my SQL2014 on Win8.1 VM. Error persists even when run query locally as local admin 

You did not get the point of SQL programming. You can have 1000 copies on multiple machines of a script to create/alter an object, but until one of those scripts is executed against a server, that object on that server will be unchanged. Yes, you can have multiple tabs/windows with the same procedure, but for SQL Server all of those are different/competing sessions and whoever is last - wins.On another hand: SSMS. You can have multiple copies of a script of the same object in different tabs, which is very handy - you can try different scenarios just by switching tabs. It is not job of SSMS to track your changes and keep records which script is a "Master Copy". That is YOUR responsibility. However, if you are in a project development, you and your boss can't completely rely only on your discipline. Project manager have to choose which way to do a source control. Here is a not full list of things, which can be done: - Implement Change Data Capture on the server; - Restrict permissions on Dev/Test and allow "playing" only on Dev or personal machine; - Use change tracking/deployment/source control tools, which will keep records of any changes. And do not be surprised with SQL. All developers are experiencing that issue with source control. See it here: $URL$ 

Before we go to the next step, save a copy of the file that will be re-created each month, get filled with data then renamed. SQL needs to see it in the wizard, or it will argue with you. This is also the folder that your end-users will need permissions to access. In my example, this file is: 

But the query itself takes 03:31. Question: is current_timestamp the timestamp of the beginning of the query? here's the entire big, bad query: 

A few things: You are going to need some data types in there. the drugstore_main is larger than the drugstore_details table, which could make things slow down the road. How are you getting the data? That will play a large role in how you want to structure your database. It might be easier to build your database to reflect the flat files you're using to populate it. Oh, and save yourself a lot of trouble by using integer ID's, don't use newid(). 

I set up a checkDB job, and I used , thinking I was really clever. Looking at the job in detail, however, it only seems to do the Database then stops, reporting success. I can grow my own like this: 

At first I'd suggest to have group_id and visit_id in INT or at least BIGINT. If that table has only 3 columns it might worth to create not-unique clustered index by group_id and unique constraint on visit_id: 

You create clustered index on only one column. Technically, you can do it on multiple, but the main goal to have it as short as possible. If you created clustered index on you DO NOT NEED any other indexes on that column. I can guess that your performance improved because before there was no Clustered index and indexes were so bad that SQL decided to do full table scan instead. Suggestion: read a book about Indexes, their differences and how they work. 

Yes you can. Use SQL $URL$ By doing that you'll have a full control behind the curtains, but please fully document it and be consistent. It is a nightmare to troubleshoot databases with synonyms. As an alternative you can use dynamic SQL, it might be more transparent. 

Might happen you still have some opened transactions, which hold you from shrinking individual files. See who hols them, close them. Reboot server if necessary. Shrink individual files while nobody accessing the database. switch it temporarily to Single User if necessary: 

Maybe you want a full outer join instead of a left, to make it look exactly like your desired output. I'd try it with both and see which you prefer. As a few have mentioned, without the isnull fuction, you may as well have inner joined. There are a few other places you can apply isnull() to potentially change the output, too. In the join, for example. 

I didn't build it, but we've got a 32-bit Windows Server 2008, with SQL server 2008 and 4GB of RAM. I need to upgrade it to SQL Server 2016 in order to log ship to a new server, so we can put this thing out of its misery. The problem - it looks like SQL Server 2016 requires, at minimum, Windows Server 2012, which is only 64-bit. Question: how can I migrate the data to a SQL 2016 server with <1 day of downtime? Is there a way to get around it with compatibility levels? 

2. Create your excel file: Paste the results into an excel sheet. Format it the way you want it, Create your pivot tables and charts 'n stuff, then delete all the rows with data (keep the headers). Important: Save the excel file in a location where your SQL Server Agent can access the file. You may need to edit the security properties of the windows folder and add SQL's service account to the Group or User Names. I keep a template file in a place where no end users can open it up, change a column and break everything. It adds a few steps, but makes it a bit more end-user-proof. In this example, my template is: 

Try both of them for both of your data sets to see the difference in query cost and amount of I/O using . For instance, when you force for the second data set I/O for table jumps 24 to 215 reads. So, be VERY CAREFUL using any kind of these hints. 

There are 2 options: 1. Create clustered index by account_id. Delete all other indexes. 2. Add new column and create clustered index on that column. Create another non-clustered index on ONLY one column . The second option is more preferable because it is much faster. You join only by 4 bytes. While your current queries join by 40 bytes. That means your current operation is 10 times more expensive. Also, ask yourself couple of questions: - Do you really need have Account_ID as Unicode? - Can you convert Account_ID to INT or BIGINT? Hopefully, you've got my point. ADDITION: 

Get the same error message. In addition to SQL 2014 installed SQL 2016. With SS2016 behavior is little different: It produces an error in ANY case, even if I set SQL port to 1433. I guess because it is "Named" instance. So far see only that extra evidence within the file that SQL Server loads during the request (I assume it is normal): 

As for country codes... ideally they would be in their own column, but you could add them back in afterwards with something like this: 

I'd let everything stew in your brain for a while. sleep on it and start making your changes tomorrow. 

I've got a cursor sending out pager messages, and occasionally it sends out duplicates. The syntax looks like this: 

3. Set up a .dtsx package: You can do it directly in SSIS, but I find it easier to use the SQL Server Import and Export Wizard by right click on the database, hitting Tasks and Export Data. Set the Data source to by SQL Server Native Client, set up your server and database, then hit Next. Set the Destination to be Microsoft Excel, and the excel file path to your shared folder (not your blank template): 

but I think I'm barking up entirely the wrong tree. Also, the performance is horrendous (not a deal breaker, since it will be automated, but still something to be mindful of). Question: How would you break this up by a foreign key? Some sort of pivot? The number of diagnoses is quite variable... Ideally, it would be something I can easily smush into a larger query, but there's obviously ways to work around that if needed. Here's my current solution, but I'm pretty sure it's not a good one: 

However, there are plenty of objects in the database, which were created with credentials of that account. I've tried to run one of these procedures and it was executed successfully. When I've tried to recreate that scenario in my test database it returned me an error: 

In your sample case SQL Server made very good choice. Here is what it was actually doing: - In the first set of data it extracts 10 rows from table based on the column. Then it extracts 10 corresponding IDs from table using kind of operation for each ID. - In the second scenario, when there are 100 matching rows in table SQL decided not to do operation, but use instead, which is much cheaper from the I/O perspective. Yes, you can use hints for your queries ($URL$ 

So, the question is: How it is possible that procedure created with an option can be successfully executed while SQL Server login does not exist for that user? 

Items with big values in and are red flags, such as Table scans and Key Lookups. There can be because of bad indexing, statistics, etc. Wild guess: when you use Temp Table Variable SQL comes up with better query plan. 

The Problem: This query will only return patients whose last 4 visits happened within 1 year. Question: How would I find all the patients with any 4 visits that happened within 1 year of each other? Credit to Jonathan Fite, I used his suggestion to rewrite the query like so: 

How often do I need to run it? It needs to run at the same interval that I keep my backups, right? If I keep the backups for 1 day, I need to run it every day? Can I restore the databases to my test environment and checkDB there? If I've got lots of sql servers, but they all use the same SAN, am I going to want to stagger my CheckDB's? 

Here's the James May method. Looking for feedback & ways to do it smarter. 1. Write your query in SSMS. To get dynamic date ranges that always fall to the first of the month, I do it this way (there is almost certainly an easier way): 

this can be a doozy. I'd start with using the replace function to get rid of all the junk. You can also add it in universally, although that will be harder. It would look something like this: 

You have to be very careful. Assuming your query is not restarting and you have a for that SPID. If you restart SQL Server it won't help, because transaction still would have to be rolled back. The problem with that is following: When you run a transaction in multi-CPU environment with not restricted degree of parallelism it most probably will generate parallel execution plan. Would say your transaction run for 10 minutes on 8 CPUs. When you Killed it, will be processed ONLY by ONE CPU. That means it might take up to 8 times longer to recover. 

That depends on a type of . If it is the impact is minimal, but it can be tremendous amount of I/O associated to a process and enormous amount of time. If you plan to do a then you need at least the same space in your data file and in a log file as the size of your index. In other words just triple size of your index. For instance: You have 1TB database with only one table which is also 1TB in size. If you want to build/rebuild clustered index on it you will need additional 1TB+ in data file and 1TB+ in a log file. Impact on TempDB is minimal. Be aware that there might be completely different results depending on current average page usage and on a new fillfactor. For instance if you have 60% average page usage and planning to fill pages 100% then for 1TB table you'll need only about 600GB of new space. Also, can have significantly lower impact on Log file when you use Simple logging with 0% fragmentation. If you don't like my answer, why wouldn't you test all your scenarios with at least 100GB table and post results here?