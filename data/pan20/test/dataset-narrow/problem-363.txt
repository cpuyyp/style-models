I have a user which is supposed to have privileges on several databases. I thought I had set it up right: 

First critique: I don't think a should consist of so many fields. A primary key is just meant to be a simple identifier of uniqueness. Arguably you could have two fields in your PK for many-to-many intersection tables, though I personally always have a single-field primary key, even for these. So I'd say your PK really should be redefined as a unique index. Then I would suggest adding to the fields of that index: 

I have been given a DMP data pump export file to import into my local Oracle instance. I've tried running this command line: 

Thanks to all who answered - really appreciate the points you've given me to think about. The general feeling I got was that a single database is preferable, but I would like to add some countervailing points in favor of the sharded architecture, and addressing some of the concerns that other people have mentioned. Motivation for sharding As mentioned in the (updated) question, we're aiming for massive sales worldwide, with literally millions of users. With the best hardware and indexing in the world, a single DB server won't take the load, so we have to be able to distribute across multiple servers. And once you have to look up which server any given customer's data is on, it's not much more work to give them a dedicated database, which makes things simpler in terms of keeping people's data neatly segregated. Response to Concerns 

Checking our production logs from yesterday, we discovered a period of about 5 minutes when a whole bunch of really simple queries were timing out. Further investigation on the server logs showed a huge spike in disk activity, which led me to the conclusion that an automatic CHECKPOINT was being run on the DB at that time. That's something I really don't want to happen during peak hours. So I was thinking of scheduling a daily CHECKPOINT every day during off-peak hours. Is that a good idea? Bad idea? Waste of time? If not that, then what? 

I believe the mystery is cleared up. The documentation only says that the SQL Server account needs full access to the filestream folder, but when I checked who had access to the main DATA folder, I saw that the SYSTEM account also had access. I gave full rights to the FS folder to the SYSTEM user, and now I don't get this error anymore. 

And no data is imported. From what I've Googled, one possible cause of this is that I need to specify . But I have no idea what the name is of the schema in the dmp file. Any easy way to find out? EDIT: I didn't find a solution to this question, but I did find a workaround... I tracked down the guy who made the DMP, and beat got the schema name out of him. Specified according to his definition, and Hey Presto! 

This might be dumb and feel like I am going back trying to understanding basics. So I create a test table like below and create a clustered index on it 

On the other server though, I can get the high throughput numbers from the diskio tool but using SQL server method I am not able to get the throughput numbers. the SQL numbers are close to what I get if I run diskspd in single thread, even though the plan is running in parallel. So I was wondering what are the things I can check to see why SQL Server's IO is slow or why SQL Server is not able to push more IO's through. 

How is your application connecting to the SQL Server? If it is not a direct connection and involve two hops(for example lets say you have an web tier that goes through an App tier and then to SQL Server) it might be related to kerberos authentication. You can check if if the connection that were coming into the principle server for the application were NTLM or Kerberos. Please check the below link Using Kerberos Authentication with SQL Server If it is, that means that your principle server was configured correctly to facilitate kerberos authentication and you would have to do the same thing for secondary server. Register a Service Principal Name for Kerberos Connections Another not so ideal situation could be that someone has added as login on the primary server, you could technically make it work by adding the same thing to the secondary server, but it is not recommended. 

From my testing I do not see an option to encrypt a symmetric key with Database Master key. Isn't the Database master key a Symmetric key. What do they mean by public key of th database master key? 

and got around 1400MB/s I was also able to get comparable throughput using a T-SQL query as below and calculating the throughput from the number of reads and time. I got this from Glenn Berry SQL Course on PluralSight "Improving Storage Subsystem Performance ". 

So as you can see from the estimated number of rows, the first one is 3.2511 which came from the density vector and for the second one the estimated number of rows of 7 comes from the histogram. So is it true that SQL Server can sniff the variable when we recompile adhoc query or is it something that I do not understand? 

But when I execute the same thing with the it does not return any results. Second case - If I replace the with and no also I dont get any results out. eg 

What I am trying to understand are the key locks on the non-clustered index(indid 2). Why are there two key lock on non-clustered index? If I check dbcc page on page id 248, I could locate the obvious one((1bfceb831cd9)) which is the lock for the entry for the record 6 which got changed to 7. Output of DBCC PAGE below 

Now all these different cases for different values of @calcType seem to be wasting a lot of space and causing me to copy and paste all over, which always sends shivers down my spine. Is there some way of declaring a function for CalcField, similar to lambda notation in C#, to make my code more compact and maintainable? I would want to do something like this: 

I have a business requirement to have a FK field on referencing . If there is a value in , it should be unique, but it's not mandatory, so there could be multiple records with null values. Is it possible somehow to enforce this type of uniqueness on the DB level, using either an index or a constraint? 

In other words, for all records where is 1, I want to see the and , alongside the values of and that are returned from for an input of . What's the syntax to do this? 

Now I want to create an index on LongValue, so that I can easily look up on serialized values that represent numbers. 

I have a query that was generated by C#/Linq-To-Entities code. As with this kind of query, it looks seriously ugly and I doubt it would help to include it here. But I have run it through a query profiler and found that 50.5% of the processing time is happening in a "Nested Loops (Inner Join)" step, which is joining a clustered index seek on (0.3%) and a clustered index scan on the same table (13.1%). I confess I don't truly understand what this "nested loops join" is doing, but why would the query be doing a scan on the table right after doing an index seek, when the fields it's looking up are available directly through another index on exactly the required fields? And yes, I have run , to no effect. Not being an expert in understanding the query plan, I'd appreciate if you could prod me for answers to relevant questions; I might just not know enough to have supplied you with enough useful info. Thanks! 

Both tables have an index on their date field. Now, I want to create a view that has a full outer join between the two tables on the Date field, to show me the value for that date (if there is one) along with the for that date. 

Indeed, that was the answer. I set the db recovery model to "Simple", waited a minute for the filestream data to clear up, and then I could remove the filestream file and filegroup. 

I have two machines, each with a default instance of SQL Server 2008 installed; each exhibiting a different behavior. 

This might be a permission issue. I am assuming that the monitor server is not on the primary and the issue could be that the primary server is not able to update the values in the monitor server. Can you check where your monitor server is and make sure primary server service account has permission to update the monitor server msdb. 

So how does an estimate of 404986 going into the sort operator comes out as an estimate of 100? Is it just an arbitrary number since it cant sniff the variable in the TOP operator? 

So for the first query the behavior is as expected, the estimated number of rows is calculated from density vector. Seek Predicates - Seek Keys1: Prefix: [db].[dbo].[test].c1 = Scalar Operator([@Min]) 

So just trying to understand what the industry standard is around DBA's being able to see card number. As per PCI document 

I have the below query and I am trying to understand the sort estimate when I use TOP with a variable. PLAN 

I have a transaction replication and the subscription was synced from snapshot years ago. Now I have a new table that needs to be added to the replication. Problem is the table is close to a TB and I do not want to generate a snapshot for that table instead just want the new data to be synced, kinda like when you sync from backup. Its inserts only to the table and I can sync the old data manually later. How do I make replication to start transferring the data without generating the snapshot. 

All of the scenarios work if I . So does XACT_ABORT has any kind of restriction when it comes to linked server? Edit - Did some more testing and looks like it has to do something to do with the number of rows also Possible Repo On ServerA 

I think its the same as this But I was upgrading to SP2 CU7. Anyway I disabled and re-enable the CDC and the log reader agent resumed. 

So the first one is querying from a source database and inserting into target staging table. Staging table is truncated every day before loading. Below is the plan. $URL$ Now the target table and the source table have the same primary key which is an identity column. About half of the time the SSIS package fails with primary key violation on the target server. I am not sure why this is happening. What are the scenarios when a query would read duplicate primary key. Or is it some issue in the way SSIS is loading data? Initially I was thinking that a scan combined with page split is causing the duplicate read but the table which is scanning has an identity as primary key and its mostly inserts. 

What am I missing? UPDATE: here's a clue. If I delete as a user from under the database's security context, then I go to the user under the server's security context and under "User Mapping" I grant access to the database, then it starts working. It could be that the database was originally restored from a different server, which also has a user. I guess then that the user identification is not based on the username? 

I have an intermittent problem with a software installation package that installs our product (written using InstallShield/InstallScript). During the process of the installation, we restart the SQL Browser Service. Most of the time this works fine. But occasionally - and I have not worked out how to reproduce this predictably - the service fails to restart, and I find in my "Services" manager that the service status is set to "Disabled". Any ideas what would be causing the service to be disabled, and how to prevent it happening? 

I tried to include the execution plan, but that sent me over the 30,000 character limit. The crazy thing is that I'm supplying AssessmentID = 1538, which is the most useful limiting information in the query, but the execution plan is all but ignoring this fact, and scanning almost every other joined table, only filtering down by AssessmentID kind of as an afterthought... 

I have often found that when I am having performance problems on my MSSQL database, I can resolve them by running . I learned this because that's always the first line of defense from tech support, right after, "Is your computer turned on?" My understanding is that SQL Server is supposed to keep stats automatically; I shouldn't need to nanny it. So I'd like to understand: what circumstances might lead to the stats falling out of date so badly that I need to update them manually? 

These all qualify as nuisances, but if I really want to work around them in order to gain the performance benefits, I can make a plan. The real kicker is the fact that you can't run an statement, and you have to go through this rigmarole every time you so much as add a field to the list of an index. Moreover, it appears that you have to shut users out of the system in order to make any schema changes to MO tables on the live DB. I find this totally outrageous, to the extent that I actually cannot believe that Microsoft could have invested so much development capital into this feature, and left it so impractical to maintain. This leads me to the conclusion that I must have gotten the wrong end of the stick; I must have misunderstood something about memory-optimized tables that has led me to believe that it is far more difficult to maintain them than it actually is. So, what have I misunderstood? Have you used MO tables? Is there some kind of secret switch or process that makes them practical to use and maintain?