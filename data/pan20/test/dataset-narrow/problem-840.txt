You have only installed the PHP language bindings, which allow PHP to communicate with memcached. You haven't actually installed memcached. If you want to run memcached, you will need to install it. (And there is no such daemon as memcache.) 

Note that this doesn't install input methods or change the keyboard layout, but those don't really apply to a container. 

The correct way to handle this is to have the application listen on the correct interface and/or IP address, not 127.0.0.1, and use iptables only to allow traffic, not to play weird NAT tricks. 

The node did redirect you. As the documentation explains, the client is expected to connect to the specified node to retry the request. The server does not do this. If you're using , then you must use the option if you want it to follow these redirects. 

It is a 32-bit unsigned integer, converted into four 8-bit unsigned integers so that humans can read it more easily. The numbers are represented most-significant-bit-first. 

You need to use to set the default file context for your non-standard location for the MySQL databases. 

This SMTP exchange is generated by Copfilter, an addon to the IPCop firewall. Its presence here indicates that your VPS service provider is intercepting outbound SMTP traffic in order to filter it. Typically Copfilter is used to run anti-virus and anti-spam checks on email, though it could be used for other purposes. In other words, instead of reaching Gmail's servers, you have connected to the firewall, and the firewall will decide whether to pass your email along. It could also do other things such as keep a copy of the email you send. 

It looks like you've obtained PHP 5.5 from Red Hat Software Collections. Unfortunately this SCL doesn't include the PECL geoip extension, so you will need to install it yourself. 

Yes, you have correctly determined what hardware is in your server. Windows Server really is not significantly different from the client OS. 

The only reason for actual device nodes in /dev nowadays is for the boot environment, before udev has started. Typically only and are needed in the actual filesystem, which is sufficient to get to the point where udev can be started. It will then provide everything else. 

After changing the system time zone, you need to restart the system logger (or any other service that uses the timezone). 

Your is wrong. Though, it's not your fault; the OVH guide is wrong too. The should be set to and the and lines should not exist. 

Many EC2 AMIs, especially Ubuntu's, have SSH password authentication disabled by default. So if you don't have a key in place, you can't log in, at all. If you only just created the instance, then delete it and start over. 

First, use bridging. If you were following the howto you linked, you should already have bridging set up. Second, assign the IP addresses for the VMs within each domU, not within the dom0. The dom0 should only have its own IP address configured when using network bridging. 

Sorry, RHEL (and by extension CentOS) does not yet support using key material on a USB drive to unlock the boot drive. 

Later versions will provide even more data; for instance in EL 7 the exact mirror and URL from which the package was downloaded is stored here. 

Why do you want to? There is absolutely no need for what you're trying to do. The reason that appears in the is that on Red Hat, PHP and its modules are generally built separately; PHP actually gets built three times (or maybe it's four) during the RPM build process to account for each SAPI. Once the resulting binaries are installed on your system, though, there is no functional difference. 

Your host doesn't have an active firewall blocking any connections, so the iptables rules you added effectively do nothing. Your system accepts all connections anyway. If your connection is firewalled, then the firewall is outside the server (e.g. Amazon EC2 security groups, GCE firewall, etc.). 

Add a static route to your external mail server's IP address which is explicitly routed via the Ethernet interface instead of the default route. Send out your outgoing mail to the external mail server on the submission port (587). 

You've already redirected the request to the remote site; you have no further control over it. The browser will contact the remote site and get whatever response it gets. 

The thing I see most commonly done in this circumstance is using jwilder/nginx-proxy to handle all incoming connections and then proxy them to the correct container. More complete PaaS solutions like Kubernetes and OpenShift have their own means of handling this (e.g. by running an internal haproxy container which does the same thing). 

For completeness... Your hardware is more than ten years old; you should expect it to perform like hardware of ten years ago! While you should definitely tune your squid server, there's not much that will help when it's running on that ancient monstrosity, except to use more current hardware. 

The typical way this is done is to have the reverse proxy/load balancer set an HTTP header that specifies the original protocol that was used by the connection from the client. That header is known as . 

The and directives that you're trying to use were added in Apache 2.4 and are in the module. They are not available in Apache 2.2. 

You don't need to - and shouldn't - use to change users in an init script. Use or better, instead. The command should be available from RHEL 5 and will do the right thing with respect to SELinux contexts, etc. 

I think that the metadata your system has for the updates repository is out of date. The update is failing because of not being able to find the corresponding package update. However, I checked my local CentOS mirror, and the correct update package is there. Therefore I suspect you have slightly old metadata for the updates repo. I suggest you clean out your metadata and then try the update again. 

Your problem is not . Your problem is that you have specified to connections back to the very same nginx thus creating an infinite loop. When a connection comes in, nginx immediately reconnects to itself 2048 times, throws that error, and gives up. To solve the problem, you need to to the correct web application, wherever it is, or remove it entirely. 

Looks like just another random connection attempt from part of a botnet. Assuming your mail server is properly secured, you can ignore it. Of course, this is a good time to check and ensure that your mail server is properly secured. 

RFC 6125 ยง 4.4 is what you're looking for. It specifies that the common name (CN) is not required to be checked when subject alternate names are present, though clients are allowed to do so. In practice many clients now ignore CN completely. Contact whoever issued your certificate and ask them to fix the mistake. 

Keep in mind that SELinux will never allow the web server to write in user home directories. If you need this, you will need to place your web content elsewhere and make the appropriate directories writable. 

It looks like is not allowed to login directly via ssh on an MLS system. You will need to ssh in as a user and then to root. 

Not enough to worry about. An idle server, of course, will consume no bandwidth. But you seem to be concerned about the traffic from Internet background noise, that is, automated attacks which every IPv4 address receives. Fortunately for you I happen to have an idle server around, which has been doing nothing but responding to Prometheus scrapes for a while. It has the ssh port open, but nothing else open globally. It really hasn't used much bandwidth. 

You were assigned a netblock, not a . But you specified in your and scripts. Try fixing that first. Better yet, use the new way (well, new in the last several releases) of specifying your addresses. 

If you insist on putting PostgreSQL's data directory in an unexpected location, you can make SELinux work correctly by setting a new file context for the new directory structure that is equivalent to the existing one. For example: 

But, that is some terribly nasty hackery and probably not necessary, so before you do that, try having systemd restart nginx automatically if it stops. Do that with a systemd drop-in: 

Download the 64-bit version of WebSphere, if available. Install the 32-bit compatibility libraries on Ubuntu: 

While your host resolves to your server's IP address in the DNS, your host does not. It is being answered by another server entirely. It appears that it is being answered by some server hosted by Namecheap, which is returning a redirect to www. but dropping the path and query string. It does not appear to be capable of preserving them. You can fix this by fixing the DNS record so that it is an A record pointing to your server, instead of a record pointed at Namecheap's broken redirector. 

You have configured your system with repositories for both MySQL 5.7 and MariaDB 10.0. This is not going to work; these packages cannot be run side-by-side on the same system. You will need to choose one or the other. Obviously since CentOS is already expecting to use MariaDB and for various other reasons, you should probably choose MariaDB rather than MySQL. 

The warning message shows the current value, not the suggested value. In case you get this warning, you should set the value to double its existing value, which should be a power of 2. And as explained in the documentation, if you are warned about two values, you should increase hash max size first. 

The second rule allows the initial connection attempt, and the first rule allows all of the rest of the traffic, as long as the connection remains open. It is first because it will match most frequently, and having it first makes things faster. Your ICMP rule is fine and should be left as-is. 

Looks like you ran out of memory. This is typically what Killed means in this context. You can confirm it by checking . The setup you linked to will not fit into a 512MB RAM (or EC2 micro) virtual machine. You will have to cut a few things out, or use a larger virtual machine. 

OpenStack will run fine on a single machine, and this is indeed a useful setup in scenarios like evaluation or developing OpenStack itself. A tool called PackStack, which is available on RHEL variants, makes it pretty easy to deploy. But OpenStack is probably overkill for your scenario, unless you plan to expand to multiple physical servers in the future. If this is the case, you should take the time to get OpenStack up now, which will make expanding easier in the future. If you don't plan to expand, then just using straight up KVM (with libvirt?) is fine. You can run virt-manager on your desktop and manage the remote machine with it. It automatically sets up an ssh tunnel to the remote machine to do this. Note that you have to place your ssh public key in the for root on the remote server in order to do this, and set instead of on the server's ssh daemon. And if circumstances change later, it's pretty easy to import KVM virtual machines into OpenStack. 

You're missing the block which passes requests upstream to php-fpm. Copy and paste it from your block which serves the HTTP web site. Except for the SSL-specific settings, the two blocks should probably be identical. 

With Red Hat and derived distributions, I recommend using the remi repository to get PHP packages (and symfony) to ensure a stable system (though with necessary bugfix updates as well). It currently contains PHP 5.3.19, symfony 1.4.20 and symfony2 2.1.3. 

We build backup systems for one purpose: To enable restores. Nobody cares about backups; they care about restores. There are three reasons one might need to restore file(s): Accidental file deletion, hardware failure, or archival/legal reasons. A "complete" backup system would enable you to restore files in all of these scenarios. For accidental file deletion, things like Dropbox and RAID fail because they simply reflect all changes made to the filesystem, and a deleted file is gone in these scenarios. Your backup system should be able to restore a file to a recent point in time fairly quickly; preferably the restore would complete within seconds to minutes. For hardware failure, you should use solutions such as RAID and other high-availability approaches when possible to ensure that your service remains up and running, as a full restore of a system can take hours or possibly days due to the necessity of reading and writing to (relatively) slow media. Finally archives, or full backups (or equivalent) of the systems at a specific point in time, can serve restores in both legal and disaster recovery scenarios. These would typically be stored off-site, in case a stray meteor turns your data center into a smoking crater... Your complete backup system should be able to support restores for any of these three types, with varying levels of service (SLA). For instance, you may decide that a deleted file may be restored with one business day granularity for the last six months and one month granularity for the last three years; and that a disk failure should be capable of being restored within four hours with no more than two business days of data loss. The backup system must be able to implement the SLA in a backup schedule. Your backup system must be fully automated. This cannot be stressed enough. If the backups aren't fully automated, they simply won't happen. Your backup system must be capable of fully automated backups, out of the box, with little or no special configuration or scripting required. You must periodically test restores. Any backup system is utterly useless if restoring from backup fails to work. I think most of us have horror stories along these lines. Your backup system must be able to restore single files or whole systems within the SLA you're implementing. You must purchase backup media on an ongoing basis. Whether you're just doing on-site tape backup or going whole hog with off-site cloud backup, make sure you have it in the budget to pay for the gigabytes (or terabytes!) of space you will need.