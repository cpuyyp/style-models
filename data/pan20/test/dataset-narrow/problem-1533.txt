I'm now learning YOLO but I don't understand how the number of grid cells is determined. Some article like this one uses 13 x 13 while others use 19 x 19 (e.g. Andrew Ng's course in Coursera). It seems that the height and width should always be the same, but how is it determined? Is there any general guideline regarding how many grid cells should be picked up over the others? 

I now read a book titled "Hands-On Machine Learning with Scikit-Learn and TensorFlow" and on the chapter 11, the author writes the following explanation on batch normalization: 

I now read a book titled "Hands-on Machine Learning with Scikit-Learn and TensorFlow" and on the chapter 11, it has the following description on the explanation of ELU (Exponential ReLU). 

As far as I know, the width and the height should be a figure divisible by 2, in order to use a pooling layer. However, I don't understand why the depth must be a figure divisible by 2. The pooling layer just operates on a 2-dimensional screen based on width and height, and it operates on each filter (depth) separately, right? Why should the depth also be set to a figure divisible by 2? 

I have read quite a lot about discretization techinques, measuring WoE and IV etc., but the basic question - when is it worth to use binning and when isn't - is not entirely clear. On the one hand, the result of discretization seems quite "pretty" and more easy to handle for me, but on the other hand it is always emphasized that binning always causes information loss. Could you provide some clues when should one use this technique? 

I have studied the usual preprocessing methods for Machine Learning but I couldn't cope the following specific problem. I apply the "usual" preparation for modeling (dummy variables, normalization, PCA etc., of course in the necessary cases) to the training data. So far, so good. But when I get the to-be-classified new data to make prediction the model constructed above, it's evident that I must apply these preparatory steps to this new data set as well. And the problem arises here, because if I simply apply the preparatory steps for my new data in turn again, these doesn't take into consideration the characteristics of the training data. So, if I convert the new data factors into dummies, then it takes only the existing factor levels in the new data into account; if I min-max normalize the new data, it will be normalized according its own min-max values, disregarding the values in the training data; and if I use PCA, then the resulting components from the new data will be totally independent of the training data. So essentially my point is that applying the same conversions separately to the train set and the new data set(s) (which could be only one observation as well), then the two resulting transformed sets will have nothing in common, so the prediction will be baseless. I found some traces that in some cases there is some "learning" step in the training phase in these transformations as well and apply this "knowledge" to new data (caret and Sklearn, for instance, with "predict" could transform to the new data with characteristics learned from the training data), but generally speaking this inconsistency remains unmentioned otherwise. What is the correct practice here? 

In order to converge to the optimum properly, there have been invented different algorithms that use adaptive learning rate, such as AdaGrad, Adam, and RMSProp. On the other hand, there is a learning rate scheduler such as power scheduling and exponential scheduling. However, I don't understand at what kind of situations you should use one over the other. I feel that using adaptive learning rate optimization algorithm such as Adam is simpler and easier to implement than using learning rate scheduler. So how can you use it apart properly, depending on what kind of problems? 

In many cases an activation function is notated as (e.g. Andrew Ng's Course courses), especially if it doesn't refer to any specific activation function such as sigmoid. However, where does this convention come from? And for what reason did start to be used? 

It is not a pre-requisite, and you can learn it easily once you encounter something you are not familiar with. Statistics is pretty old and there are many learning resources on the Web, which you can get to whenever you hit the wall while learning about deep learning. As to which field is pre-requisite, I think it is enough to first learn about Gaussian (normal) distribution, linear regression, and logistic regression. Then when you encounter something you don't understand, it is time to invest your time on statistics. The more requisite fields I believe are calculus and linear algebra. If you haven't learned about them (such as partial derivative, matrix transpose, etc), it is very difficult to start to learn deep learning. Also, the prior exposure to some of machine learning algorithms would make your learning faster. But I'm sure you already got it given that you finished Andrew Ng's course on Coursera. Andrew Ng starts deep learning course on Coursera from August 15th, so you can join it and get a grasp of what is required. The book you linked sounds more like focused on machine learning than on statistics, BTW. 

I have a factor variable in my data frame with values where in the original CSV "NA" was intended to mean simply "None", not missing data. Hence I want replace every value in the given column with "None" factor value. I tried this: 

I want to apply Connectionist Temporal Classification for an OCR task where I have a bunch of images with text of variable length and the output is the text itself: 

while in other cases there is no such an external output but users just leave to the Embedding layer to decide the representation vectors. I don't understand what is the real difference between these approaches regarding the desired outcome? Maybe the internal-only solution is not a semantic representation? What is the point of applying embedding layer to an external matrix of which the rows already have fix length? Moreover, what is the purpose/effect of the parameter of the Embedding layer? Am I correct guessing that this set to True let the Embedding layer fine-tune the imported word2vec weights to take the actual training examples into consideration? Further, how to instruct Embedding layer to properly encode "metacharacters"? Setting the parameter True it can incorporate padding zeroes but what about UNK (unknown), EOS (End of Sentence)? (By the way, I cannot understand what is the point to explicitly sign the end of sentence in a sentence based input...) And finally: how could a model predict the translation of a word which is not represented in the training set? Is it tries to approximate it with the "closest" one in the vocabulary? 

What you want to do in order to deal with unbalanced data is: 1) just use another metric, as ROC-AUC. This can be sufficient if you have a dataset not so much unbalanced, say up to 1:20. This metric is not officially supported in Keras, but you can find a custom callback which calculate the AUC score every end of epoch here. 2) if the imbalance is very big, say 1:1000, you need to balance the dataset. There are lots of techniques, the simplest one is to make copies of the minority class samples, as many as needed to reach the balance. Other smarter techniques exist, and some of then are included in imbalanced-learn for SKLearn. If you want to have a better overview of techniques and metrics used in such cases, I suggest you this paper. 

No. Any dimensionality reduction technique suppose you are dealing with algebraic values, and categorical variables do not have such meaning. If you have a categorical variable which indicate - as example - families' pets in one of (cat, dog, red fish) mapped to (0, 1, 2), you simply cannot say that the distance between cat and dog is equal to one, because those categories do not lie in an algebraic space. So, in those cases, you do the one-hot encoding of the variables, a.k.a. dummyfication. Then you can safely apply PCA, or any other dimensionality reduction technique. 

The function is from TensorFlow, FYI. The author explains that the γ parameter should not be set on ReLU activation function. However, I don't understand why on ReLU, the next layer's weights can take care of scaling... I understand that the next layer takes input from the ReLU output, which is . Why does it take care of scaling and thus no need to set γ parameter on ReLU? 

I just started to learn Convolutional Neural Network, and like to predict a Pokémon type by its apperance (the input is image). However, while many Pokémon has only one type, some Pokémon have two types (no Pokémon has more than 2 types). I'm not sure the proportion, but it is something like 60% for only one type vs 40% for two types, I guess. In this case, how should I classify it? Should I mark all the output probability that has > 33% as predicted types, or is there anything better way? Also, is the convolutional neural network suitable in these cases? 

How could I compute similarity taking semantic distance into account? Shall I use word2vec representation instead of TFIDF? 

I have discovered that Amazon has a dedicated Deep Learning AMI with TensorFlow, Keras etc. preinstalled (not to mention other prebuilt custom AMIs). I tried out this with a typical job on several GPU-based instances to see the performances. There are five such in the Ireland region (maybe in other regions exist even more, I don't know, this variance is a bit confusing): 

create_model is a function that builds the Neural Network Model. The fitting (last row) gives a long error message: 

I face with a special NLP (?) problem. I have a sequence of program development steps with code (actually a Git repository with multiple commits of build sources) and a full list of test cases (which I haven't any more specific closer description about apart from some quite general ones like id, timestamp etc.) of which subset that have been run against in a specific build test runs. Based on this historic data I have to build a machine learning model which predicts/recommends relevant test cases for any future build of the given software. Apart from the extremely large amount of data points (several hundreds of program files with several hundred megabytes of gross length and several thousands of potential test cases) I cannot see any promising way to extract features from the code base for the prediction. My first idea is to trace the changes in respective code files per build runs and the executed test cases (so the independent variables are the IDs of the files with binary values if they are modified or not in the current development stage, and the goal is a multilabel classification with as many labels as the possible test cases are) but it's evident that the source file change is a very poor guess for a necessary test case to involve. Is there any practice to a proper feature extraction for this problem? I'm aware that program code handling with ML/DL does exist but usually in a form of code generation and syntax checking which are not my cup of tea. 

I wonder whether one epoch using mini-batch gradient descent is slower than one epoch using just batch gradient descent. At least I understand that one iteration of mini-batch gradient descent should be faster than one iteration of batch gradient descent. However, if I understand it correctly, since the mini-batch gradient descent must update the weights by the number of the batch size in one epoch, the training would be slower than the batch gradient descent, which computes and updates the weights only once in one epoch. Is this correct? In that case, is it worth worrying about the loss of the overall training time? 

The means the x-axis on the graph above. I understand the derivative is smooth since the line has a curve and in that realm the derivative is no longer equal to . However, why is it the case that if the function is "smooth everywhere, including around z=0", it speeds up Gradient Descent?