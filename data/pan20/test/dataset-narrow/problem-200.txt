You will receive a list of SQL Server logins for your SQL Server instance. Note: These are the SQL Server Logins and not the database users. Database Users Database Users can be queried by querying the system catalog view of each user database by issuing the following query: 

In the Oracle Support Document I referenced, in the section for the installed components, there are a list of items that get installed for example for the Administrator client installation: 

Based on the quoted article MySQL should run both statements using the same execution plan and the supplied index. 

I would search the system tables in your restored master database directly, that contain the data you are looking for. You can then access for example the sys.sysxlgns table without having to rely on the sys.server_principals view which actually accesses the master.sys.sysxlgns system base table. sysxlgns = server_principals For a list of system base tables read the following MSDN article: System Base Tables Microsoft states that ... 

Perform a once a week (e.g. Sunday) Perform a every other day, except e.g. Sunday Perform a every other hour Store data in a different location other that your main database(s), or even in an off-site location (geographical redundancy) 

This is an issue that wasn't accounted for in the "Requirements Engineering" phase of the project. It shouldn't be looked at as an issue of the database system, because the database is performing as it should do. The mail is sent because it is not (yet) part of the correct business logic. It is called a Business Logic Flaw or possibly even a Business Logic Issue. Business Logic 

We have various installations at our shop and we tend to start low with four tempdb files and then add additional files if there is tempdb contention as pointed out by @Kevin3nfs comment, where he references an SQLSkills.com search. Microsoft has a knowledge base article: Recommendations to reduce allocation contention in SQL Server tempdb database 

Reference: My personal script Run it and see if any objects are displayed containing your obsolete filegroup. Go with the rather than with the name. The joins are intentionally to catch any "orphaned" references. Alternatively use this smaller script to quick check for items in the obsolete filegroup: 

In this case the SID of the SQL Login () will match the SID of the database user (). Restore Behaviour When you restore a database from a source SQL Server the database users (Native SQL Server Logins and local Windows Accounts only) in the source database will have different SIDs than the SQL Server Logins on the target server. This is because the SID is unique for the source and target Native SQL Server Login or Local Windows Account. The SQL Server Logins that are based on Windows Domain Accounts will always have the same SID, because SQL Server will retrieve these values from Active Directory. When you restore the database from the source to the target SQL Server the SIDs of the Native SQL Server Logins will be mismatched, even though a user might be listed in the system management catalog of the SQL Server instance and in the system management catalog of the restored database. Solution To rectify this and allow you to navigate the "SQL Server Login | permissions" and/or the "Database Properties | Permissions" you can relink these orphaned database users to the SQL Server Login. Switch to your user database and query the orphaned database users: 

should catch most cases. However, unlike the column, columns store JSON in its original form, not in a canonical form. So this method doesn't exclude the possibility that there will be some weird JSON, for example starting with another space character, which won't be matched. 

(This query by itself doesn't look too bad, but you are going to use it everywhere you would use a single table name in JOINs and so on). 

I am not sure if this is a very clean solution, but it might work. Suppose that you have a table and a table. Create a third table, . This should have three columns, , , and . and are both foreign keys to the respective tables. for the column, restricting the integer values that it can take, for example from 1 to 8. Now impose two constraints on the table. First a constraint on , which ensures that a member can only have one leader. Then a constraint on the combination of columns . This will mean that each leader can only have as many members as there are distinct values possible for . I can see a couple of drawbacks with this - first of all inserting a new is now a little tricky, as you have to find a value of which has not been used. Bear in mind you have to make a check at this point anyway - to see if the in question already has enough members. Secondly the type for the might suggest that there is some difference between the members which have the different values - that the numbers are meaningful. However, I think it is a reasonably clean and normalized design. 

that one day some other part number such as 9 would be acceptable. You can quickly and transparently add it to the table, rather than changing a bunch of constraints on different tables. that you might want to store some extra data alongside each part number, for example the name of the part. You can add a column to the table which you already have. 

We are using Postgres 9.3 and making use of the JSON column which was new to that version. Version 9.4 added a LOT of utility functions for handling JSON, as well as the new JSONB column. You can see this by comparing $URL$ with $URL$ 9.4 has a function, which returns the type of a JSON object (as a string: 'string', 'number', 'array', 'object', etc.) I would like to know if there is any practical way in 9.3 of retrieving all rows which are scalars (or, equally useful for us, which are not objects). We would like the column of a given table to always be a JSON object. This means that we can do queries like: 

Solution (only if the above mentioned matches) The solution is apparently to turn on the trace flag 1224 which will turn off lock escalation: SQL Server Lock Escalation and Blocking 

Advanced SSMS Connection Properties window In the Network section of the Connection Properties change the Network Protocol to TCP/IP. Advanced SSMS Connection Properties window recommendations Your connection window should look like this: 

Public database role There is also a database role, which is somehow linked to the server role. If you query the (view) in the master database, then you will see that there is a principal with the same id as the server role which is 0. I am assuming that this is the missing link, between the database_role public and the server_role public. Edit: Added some information regarding the database role "public" 

Or you could read up on the configuration options on Technet: Server Configuration Options (SQL Server 2012) 

Addendum regarding primary key indexes On the assumption that you are using the InnoDB engine and that the PK on the tables is based on the , then all other secondary indexes will be based on this primary key as stated in the article 14.11.9 Clustered and Secondary Indexes in the official documentation. Even if you didn't create a primary clustered index yourself, MySQL will do it for you to ensure speedy queries. This is documented as: 

SQL Server Agent Jobs Not Being Logged in Msdb There are cases then the job steps of jobs executing are unable to enter the details into the msdb job tables due to contention (locks, blocks, long running transactions). The tables required by the backup job are being locked by another process (another maintenance plan / other 3-rd party tools / cleanup jobs / a long running transaction) and might be determined as the victim of a deadlock and rolled back. The data is missing in the msdb database and because no FULL or DIFF backups exist, nothing gets deleted. Ola checks this with the following part in the stored procedures: 

Security depends on the requirements of your business. Ask the head of IT for SLAs or OLAs and any company-wide security policies, etc. One question you haven't yet asked is: Availability. Will you be required to have Always on Availability Groups? You asked a lot of questions, I hope you have a general starting point. 

Adequate TLog file sizing Don't try and shrink the TLog file too much. It will grow again. Instead try sizing the TLog file according to your observations, allow for a little growth and monitor how full the TLog file is. Frequent TLog Backups If you have a look at my simplistic model, you may have noted that the TLog didn't have a lot of CHECKPOINTS logged. If the database has to handle a lot of transactions modifying data, then shortening the interval between TLog backups can help keep the TLog from growing, because a TLog backup creates a CHECKPOINT which allows the data to be written to the database. Let's add an example using my over simplistic model. Additonal Backup A backup is performed and logged in the TLog and during the backup a user modifies data: 

If for some row the JSON in the comment column is an object, but which does not have any such key, you simply get back a NULL. However, if there is even one scalar in the table, the query fails with the error 

One approach is to check the string form of the JSON column. An object should start with '{', or maybe by some spaces followed by that character. So 

I would certainly recommend B in most cases. Both A and C leave open lots of room for inconsistency. Enforcing that exactly one of two columns is (for case A) is quite a pain. If you need uniqueness, enforcing that one column is either a unique value, or , and the other one is also either a unique value, or , with exactly one of them being , is a huge pain. C also misses some consistency checks as you recognized. I don't exactly understand what the information is that you are trying to represent. But it usually makes sense to me that if you are trying to make one table have references to one of two different other tables, then the information in the first table actually consists of two types of different thing. It should also be considered that it's much easier and more readable to do a , than to have lots of queries of the form: 

This is a pain, as we have had strings written to this column some of the time, mainly due to errors. Is there any reasonable way of either filtering these out in a query, or identifying them all so that they can be removed in one go? 

The only situation where I would not do it this way, was if 1 to 8 is something which could never ever change. For example, you might be sure that there will only ever by 7 days of the week (but who knows?). In this case I would create a TYPE with this constraint. Then you can use the type in multiple tables. If the days of the week (or whatever) DOES change, you can change the TYPE. 

@CoderAbsolute's answer gives you a good design for your tables. Since he or she did not go into detail about why this approach is better, I thought it was worth adding another answer. First of all, design your table structure in accordance with how your data fits together. Don't try to smoosh different types of things into one table, don't add several tables for the same kind of records. Try to 'normalize' - if a table will have the same info repeated many times, then move that info into a different table, and link it from the first table using a foreign key. You should be aware of what first normal form, second normal form and third normal form are. Many real-world databases do not match these standards completely, but being aware of what you should aim for will help you make a much cleaner design. I would say, don't worry about optimization until you've already got a correct design. First of all, you don't yet know how many entries you will have in your tables. Secondly, database engines are designed to make queries as fast as possible, even if you have a lot of entries. Don't second guess the developers of your database software. When you've figured out that you really do have a bottleneck, you should look first at indexing. We think of a database table as being something like an array. In reality, it could be stored as a bunch of 'blobs', one for each row, which might all have different locations on a disk, not necessarily in order. (This is not a super accurate technical description of DB internals. But it should give you a clearer picture of how the pieces fit together.) How does the database find all the blobs? It has a somewhere, a list of 'pointers' which tell it where to find each blob. So typically, finding every single row of a table is an efficient process. It goes through the list and records each one. Now, suppose that you most commonly retrieve all the photos for a given user. This might be a slow process, since it has to go through every single row of the table, and look at the field. In this case, you should add an Index on that field. An index is something like a lookup table, which allows the software to quickly find the location of all the rows with a given . So it doesn't have to check every row in turn. There are different kinds of indexes. Some are optimized for matching a particular value. This is probably the type you want for the column. Some are optimized for finding things greater or smaller than some value. This might make sense for timestamps. (Give me all the photos from the last month.) You can have an index on multiple columns if this is what you regularly query on, or even on some function of one or more columns. Some indexes mean that similar items are stored close to each other on disk, to take advantage of caching to retrieve them more quickly. You should familiarize yourself with the possibilities for your DBMS. Experimentation can also be very valuable here, since the exact speedups depend on your settings and also your hardware configuration.