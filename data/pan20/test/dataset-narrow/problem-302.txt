If you want to be able to view the exact backup history for longer time than those four weeks that you keep the backup files on disk, make sure that you set the cleanup task not to delete the backup history from the msdb database, you can then run the backup report from SSMS (right click database, select reports, standard reports, backup and restore events) or run the following to view the backup history: First create this function - I'll just have it in msdb - which is not a good idea. 

No ambiguity there. If you set 'Auto Update Statistics' to 'False' statistics will not be updated automatically. If you set 'Auto Update Statistics' to 'True' they will be updated according synchronously or asynchronously according to to the 'Auto Update Statistics Asynchronously' settings. 

Just making sure - Is the log file set to a fixed size? If so, try adding a secondary log file to the database and then do a checkpoint. If all else fails rebuild the log. 

Not as it stands, you will have to add an index for it and that will never be able to scan only the 2011 column from the pivoted view - adding index onto that view is not possible and will give an error . Then you need to create an index for the underlying data to get the Pivot operator to run faster. Let's look at if the Optimizer has something to say. Your sample data is really small so run your query like this. 

So if you are not on the stated service pack level you will need to upgrade to at least that. It is not neccessary, but start by running the upgrade advisor. Thomas LaRock has a checklist on his page both for the tasks needed before an inplace upgrade and what to do afterwards. 

This will mean that you can restore to the point in time before the upgrade (last full backup and log backups until the last one) and if you restore the last full backup and the differential made after the database was put back into the full recovery model and any log backups made after that. And finally the best solution: Schedule your log backups to run every 10-15 minutes during the upgrade and dont change the recovery model to simple. 

You can setup a test database and trace the application activity using the SQL Server profiler. By tracing the event "Security Audit\Audit Schema Object Access Event" you can effectively trace all the SQL the client is sending to the server. But that will not necessarily help you in discover the application logic. 

If the first query returns standard or web edition you are stuck and need to upgrade to the enterprise edition. The second query returns the set maximum size of memory that the engine is allowed to allocate, if you are using the enterprise edition you need to set the limit higher by right clicking the server in the SSMS, select properties, memory and change that or execute this which will set the limits to 124 GB leaving some for the os 

The Always on logs are written in the same location as the errorlog. By moving the errorlog you can change this location. You will need to change the service startup parameter by opening the SQL Server configuration manager and changing the -e parameter and restarting the service. 

First make sure that replication is not causing this, as stated in the connect item the "log_wait_reuse_desc=XTP_CHECKPOINT does not necessarily mean that the XTP checkpoint worker is holding up log truncation." so start by running and make certain that all data has been distributed. Then there is this little snippet here: 

First things first, I hope you have a backup, this is a serious error and you should do a restore, even if you lose some data as that way you will end up with a consistent database but the second best option would be this. You can peek into the data pages to see what is stored there and maybe, just maybe you can get most of the data from the non damaged tables. Now before we start you should at least read Paul Randal's Inside the Storage Engine: Anatomy of a page and How to use DBCC PAGE. and you should really watch his video on Advanced Data Recovery Techniques First to make sure what is on the damaged page. 

TDE will add a little extra CPU cost to the server but that's marginal on current CPUs. It will however only protect the data at rest, but that's including backups of the databases. It's fully compatible with failover clustering and AlwaysOn but you have to make sure that all nodes in AlwaysOn are configured correctly ($URL$ I would say that it's more practical than trying to use Bitlocker or similar techonologies. 

I cant offer you to dump this to the errorlog. For that you need to do some serious research and both these options are lightweight and can leave you with a simple file to look at. In SQL Server 2005 you need to create a server side trace on transaction filtering on eventsubclass 2. In SQL Server 2008R2 and later you are better off creating an extended event session on the SQLSERVER.SQL.TRANSACTION event filtering on transaction_state = 2 But since you are using SQL Server 2005 you need a trace, here is a script that will create a server side trace capturing all rollback events: 

You will need to expand the filegroup, either by adding space to one of its files by extending them or by adding a new file. Right click the 'Reporting' database and select properties and under files extend a datafile by changing the initial size, check that you have enough space on the disk drive the file resides on and then add at least 10% of the size of the file. Then change the the autogrowth settings and set those to more than 1MB, find a limit that can happen fast on your storage, 256MB is usually safe 512MB or larger if your storage is fast and if you have instant file allocation active $URL$ 

If MSDB is on c: master is on c: as well and I'm almost willing to bet that tempdb is still in the default location as the average I/O stall on c is 19 seconds! First, check if tempdb is on c:\ or has any file on the c:\ partition and if so move it away Check if the master database has some free space - Usually you dont need to tune it as very little is written to the master database, unless if someone has created objects in it Then make space in the MSDB database is way to big which can cause a lot of troubles, please remove as much backup and job history from it database as you can. You can do all these checks very simply by downloading and running sp_blitz 

or a primary key on VeryRandomText you would get a scan of that index. See books online or here: $URL$ 

You have to change the collation settings either in your queries or in SSAS. There are several different options so the important question is do you use SSAS for other databases than the Croatian_CI_AS one. If so you can pepper all your queries with collate Latin1_General_CI_AS to make sure that you get the data in the Latin1 collation into the cubes. If you are only using SSAS for the Croatian data, then change the SSAS collation to the same. 

Best practises state that the optimal storage configuration is to have at least three volumes: One for the OS, another for the Database files and the third for transaction logs and to that you should add a fourth one for tempdb. Sometimes you just don't live in an optional world though. If you cant possibly get a third pair of disks, I would recommend that you benchmark different configuration using SQLIO. In your case you might be able to squeeze more I/O from the raid controller, without sacrificing redundancy by setting all four disks up as a single volume using RAID 1+0 and create four partitions on that for the os, data, log and tempdb. Only a benchmark will tell. 

The software installer is reading the or environment variables to create the user database, you will have to change the installation script or temporary change the environment variables while running the installer. This has nothing to do with Express itself as it is creating the databases where it's asked to do so by the installation script. Now if you want to have the database in the home directory you are better off by using SQL Server Express LocalDB. I would consider this software a failure. 

You start by making a tail of the log backup of the damaged database, . You then restore the database from your last full backup backup, with no recovery and then restore your all your log backups ending with the one you made with no_truncate see this kb article: $URL$ 

Run the setup.exe for express again and you can find the option to repair the installation on the from the installation screen instead of doing it through add and remove programs. You can also install 7-zip and use that to extract the setup file to get the msi files for the repair 

Yes a partial backup is a backup of the primary datafile and all read/write filegroups in the database This is an online restore, you will need the enterprise edition to do this. But this will function on the dev edition as well You can use the simple recovery model for this and you will not need to do any exercises changing the recovery model, just change your current backups to be partial and go though all the possible scenarios. 

_BIN will order in the binary (ASCII order) of characters while CI_AS will use dictionary order with case insensitivity and accent sensitivity so BIN will treat "A" and "a" as different characters and order them accordingly and sort the alphabet in this order. 

And the update the lowest date_2 value with the date_1 value from the second lowest seq_id - This is update with a join with the strange looking SQL-Server syntax and is