my main question is this: Is the security filtering for a GPO calculated using AND logic or with OR logic to apply the filters? Here is the context of my situation for a background on why I am asking: OU Structure: 

At this point the image:tag is completely deleted from disk and is purged from the registry. The blobs are deleted and you'll see the manifests gone from NOTE: If this was the last image in your repo, you still have to manually delete the repo listing from disk () - however this is just metadata. The actual image data has already been removed. I believe this might be a bug in the garbage collector. I have a question about it here: Docker Private Registry - Deleted all images, but still showing in catalog MORE DETAILS: $URL$ $URL$ 

What you think of for UNC paths are typically for SMB/CIFS aka "Windows file shares", not FTP. You can use a free Samba server to host the files you need and they'll be accessible by a UNC path: Some links for you: $URL$ $URL$ Here is Samba file server setup for Ubuntu: $URL$ 

I can connect and authenticate without issue over clear text (unencrypted) but cannot seem to communicate with the server over TLS. I connect to this server over TLS through various other systems (our code repo, jenkins, etc all authenticate over Encrypted ldaps protocol to it over port 10686 so I know the server is responding fine over TLS. It uses self-signed certs but that hasn't been an issue so far with other services connecting to it. Based on the log files below, it seems the directive triggers the StartTLS function which I do not want. Regardless I've still experimented with using it... Different configs I've tried: WORKS: (unencrypted) 

I currently have a WORKING SFTP login, using a private key for login and the user is chroot'ed into their home directory. Goal: Keep the user chroot but allow WRITE access to the relative chroot directory, without having to specific any path or cd anywhere. In other words, when the sftp user logs in, I don't want them to have to cd to another path in order to upload a file. 

I've used flock a lot in the past to ensure that a process only spawns once (in case it gets hung up/etc) but I've never used it for two different processes. In this scenario two different scripts affect the same "temp" directory (the dir is clobbered at the start of each run). So I need to ensure the temp dir is not clobbered by one script while another one is still needing it. My goal: 

Yea they didn't make this easy and it's still not perfect, but the v2 registry API now has the ability to delete images. 

Any time I authenticate a system with LDAP/AD I like to have at least one "local" admin that can get in even if Directory Services go down/have an issue. Essentially a "backdoor" just in case the directory service has an issue. Is this possible in Jenkins? I can only see the option to enable 1 authentication method at a time. 

Gather image digest: with added to the header on the call The call returns header key with a value such as Using that value from step 2, run the delete call: Registry API returns Run garbage collection manually: Garbage collector deletes the associated blobs from disk (log omitted here, but it successfully deletes the blobs) 

Trying to get active checks of a Windows host from my nagios server. At this point I'm not even using nagios, and just running check_nt from the command line. 

I have a tricky situation. I need a user to execute a specific command as another user -- However this command only works if the environment is fully loaded. Otherwise the command I'm running won't work, so I need to do a type command. Command works fine when ran from root: 

Following the official documentation ($URL$ I have been able to successfully delete an image. As expected, after deleting, the image can no longer be pulled nor its manifest called via API. I feel like I've got the hard part done, however the problem is that the repo is still listed under after the deletion is finished. I'm trying to fully purge the registry. Here is my registry compose file: 

I have a docker container running web server thatâ€™s listening on port 80 (jenkins/jenkins:lts) I have 2x interfaces on the host, one is intended to be dedicated to the container only (once this is working it will be locked down only to allow HTTP/HTTPS Here is the NIC Config on the instance (This is AWS so they are Elastic Network Interfaces): 

It looks like you're missing the wildcard in the object declaration. It should be instead of when declaring bucket objects 

The actual image data is stored in the blobs directory on disk but they are shared between different manifests so it's not safe to just purge that directory out unless you've considered all images that may share the blobs. 

My git server is Gitlab and it's locally (private) hosted and it has a trusted cert from comodo that browsers trust without issue. If I'm in a situation that I need to use HTTPS instead of SSH (From an Ubuntu 16 box in this case), then I always have to bypass SSL verification by using on my git commands. If I don't disable the verification, then I get: 

I have an odd scenario that I'm not sure how to get around. Normally this is doable with subnet ACL's - However they are not stateful. I need "reply" established packets to allow a return (like a typical firewall) I have a public and private subnet. Public needs to reach out to the internet so I have outbound allowed to 0.0.0.0/0 but I want to restrict outbound for specific subnets (10.100.1.0/24 and 10.150.2.0/24 for example) I could of course set this easily in ACL since it allows 'deny' but it won't allow reply packets since it's not stateful. Is the only option to control this with 'inbound' rules on the other internal subnets? That makes a lot more rules for our different networks when it would be much cleaner to just restrict it on the outbound. Any ideas are welcome, including re-architecting the whole thing (this is greenfield) The comment below asked to further clarify the environment needs, so here is the environment as it's being requested for me to build out: Think of it like a local and DMZ: 

I have the "OperationsGPO" applied to the "Biz-Users/Operations" OU This GPO is a User based GPO but it only applies to a single group of users within the Operations OU, and I ONLY want to apply the GPO to the user if they are logging into a computer in "Site2-Computers" OU Therefore I have added the required Operations users (not all of them) to a group "OpsUsers" and added the Site2 computers to a group called "Site2Comps" and I added those groups to the "Security Filtering" field of the GPO Since there are 2 groups defined in the Security filtering ("OpsUsers" and "Site2Comps") I am not clear on how they are applied -- do BOTH parameters need to match (AND logic) or does ANY of the objects need to match (OR logic)? Thanks! 

Restore the image like normal with clonezilla. Ignore the error about GPT needed. Boot into the restored system, the disk is currently MBR so it's 2TB max Run example: gdisk will give you a warning that it's MBR and will automatically conver to GPT. Type and press enter. Confirm the write operation Go back into gdisk (like in step 3 above) Type to print your partition table, the first partition should start on sector 2048 (this is default setup, if yours is different the rest of the steps won't be exact for you) Type for a new partition. When prompted for first sector, choose the lowest number you can (which is probably 34) last sector is 2047 in my case. (right before the start of primary partition) -- Essentially start: 34, end: 2047. When prompted for the type, choose to verify the new partition layout to write changes At the shell: ignore the warning about the new partition created At the shell: example: Reboot, you should be good to go. Use gparted or any other partition utility to expand over the now available free space. I personally booted to Ubuntu Live CD and used gparted for easy partition growth. Now that the partition is resized, use LVM to extend the LV/FS across the partition like normal. 

This call returns the header key with a value like this: Using that value from step 2, run the http call: