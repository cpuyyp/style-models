Yes, by default a standard user can get to a bunch of stuff. Most, if not all, of this can be fixed with policies, but I can absolutely understand why people end up using a deep freeze type solution and, indeed, non persistant images is a selling point of VDI solutions. Depending on whether UAC is enabled users may be able to access and modify files on the local drives, install applications (though generally to their own profile), run viruses or malware etc etc. To clarify, by default, the standard user is nowhere near secure enough for use in an environment where users need to be heavily controlled such as schools, public stations and so on. Correct me if I'm wrong, but I didn't think Deep Freeze was about securing a particular session, but to prevent changes from sticking? 

A very crude way to do this would be to save the running configuration, make the change and then set the router to reboot.* Many engineers use this method when working remotely to ensure that they can get back to the box is they accidentally break the way they're connected. Use the following command to set a reboot in 5 minutes (I don't know what the max is, but it's always in minutes). And then use the second command to subsequently cancel it when you're happy. 

I found a way using robocopy which comes with Windows Server 2003 Resource Kit Tools It has an option that only logs the differences without copying them. The command was: 

If your system is vulnerable, the time and date will display and /tmp/echo will be created. If your system is not vulnerable, you will see output similar to: 

I am not getting an IP in the subnet where the DHCP is working and should serve an IP, I am getting instead only an IP in the subnet (link local address) that is probably given by Windows which runs on the laptop I am using to test this, which makes me think it might be a problem of the network firewall blocking the DHCP Discovery service. And if that is caused by the firewall, which ports I should be opening for DHCP Discovery to work? 

I encountered the following situation: The network I administer was blocked, and this was traced down to a specific rack, after I reseted both switches, everything worked. I supposed it was the core switch so I replaced the core switch. It happened again and this time I reset the secondary switch which practically connects all the stations in that building to the network and it work. The topography simplified is like in the image below. Can this happen, can a faulty secondary switch block an entire network, and how can I detect when there are similar problems and I have two cascaded switches, which one is the culprit? 

I respect you for thinking critically and trying to learn, but honestly, you should work on your research skills. A simple Google would have taken you to Wikipedia which has this excellent explanation of why it's correct, and how it happens. Source: $URL$ 

The redirect from the root to the Citrix site is performed on Web Interface server, rather than through the Secure Gateway. You should check the Web Interface directly, bypassing CSG. 

For a start, this computer will no longer have a trust relationship with the domain due to, well everything. Computer password will have expired, certificates will be wrong and so on. You'd be better rebuilding the server and importing your mailboxes I'd have thought. 

The profile gets created locally using the default profile of the machine that you're logging on to. It's not until log off that it becomes a roaming profile as such. C:\Users\Default is still there, and it will still be used when the profile is first created. 

Your question is a little sparse, but a look at the 'KEYNAME' makes me think you're referring to this issue: $URL$ As described in the article, a policy is only considered "Fully managed" if the registry key is one of these locations: 

ShellShock is practically a conjunction of more than one vulnerabilities of bash, and at this moment there is also malaware that exploits this vulnerability, so ShellShock can be an issue that is still open, there is a thread with updates from RedHat about this issues. Redhat recommeds the following: Run command: 

There are news that an unpatchable and unfixable USB firmware exploit is "out there". This could seem unrelated to servers but since the affected hardware from Phison is not disclosed there is a possibility of devices that are connected to an internal network could be infected or potentially vulnerable. This could be the worst nightmare for a network & system administrator, an ICT consultant, etc. Is there a way to check if a USB devices has the potential BadUSB vulnerability, so that USB devices can be "vetted by the IT departments"? 

The SyncBack job reported successful when finished. But I had history where some rights were not copied or some files were missing. How can I audit if the copy/mirroring was successful, check all the files and the security permissions against its original copy? 

This question is a bit open ended. A Router by its strictest definition simply routes packets at Layer 3. So, essentially, no. I don't know of any normal off the shelf pure router that have this sort of behaviour built in. However, there are most certainly other devices that could. Now, the confusing thing is that these could be bundled with Routing technology and may be even be badged as such. So, if the argument is "IP traffic is insecure by default" then your colleague is absolutely right. However, it's unlikely be simply house keeping - just think how big this cache would have to be. Routing is just one part of getting Internet traffic from A - B and you can bet your e-mail goes through all sorts of other devices inbetween. 

Personally, I don't see as a security risk so I would be inclined to leave it as is on a relatively unknown system. I don't see the point in being there. If you do remove them, just follow good procedures - i.e., document everything and remove them from a few shares and see how things go for a couple of weeks, checking backups and access etc. 

I am using terminal-service server with Windows Server 2012 and we have two terminal servers share trough NLB, users connect to a terminal in the domain with Remote Desktop Connection (see picture below). I need to map a network drive for the users in terminal to a share on another domain ( which has the domain controller Windows Server 2003). I try creating a GPO in through (domain controller for with Windows Server 2012) to map the network drive. and are physically connected in the same network. How to map a network drive to a share in the other domain () on terminals in the main domain ()? 

Sometimes the other uptime command display a time that is not correct so I use this command to display the 

I have a print server running Windows Server 2012 which is shared in a domain and is used by terminal services clients. The domain network spans between countries using tunnels and in some situations the print server and the printers are not in the same physical network and in this situations when the network is busy big documents fail to print. If I check the Print Queue Window I don't get very much data on what happens the only available fields displayed are Sometimes Size is empty and Status doesn't give too much information. The only solution remaining is to select from that same Print Queue window and when some print job is stuck that usually doesn't work. 

This is a classic case of trying to solve a people problem with a technical solution. Is it possible? Not as standard, no. Is there a workaround? Almost certainly, you could run a script or service to monitor the folder and rename things. Should you? In my opinion, no. Any attempt to come up with some hacky solution will only lead to excruciating pain down the line, probably won't ever work properly anyway, and you'll then become responsible for the entire naming scheme. Conclusion: It'a a management problem. The offenders need to be told to work properly and adhere to policy, just like any other workplace rules. That said, it seems like you need a proper document management system. Some of them are very powerful, though they still require the user to use it properly! 

Local Group Policy (Based on the client machine - this is not connected to your AD Group Policy) Site Level Policies Domain Level Policies OU Level Policies 

No, because you're not meant to be using static ICA files - that's a huge huge hack. You should be brokering connections on demand using Citrix Web Interface or StoreFront. Yes, your users will prompted to login before hand but this ensures a proper distribution and control of where users end up. I suppose in principle it might be possible to shove a load balancer in front of it but, well, don't. 

Directories need x bit set (for directory that bit is seen as search bit) to open. So I use tree so I can get only the folder set and avoid the nightmare of having all the files set as executables ( the option for tree is ): 

you are not vulnerable. The other part of ShellShock check is the CVE-2014-7169 vulnerability check ensures that the system is protected from the file creation issue. To test if your version of Bash is vulnerable to CVE-2014-7169, run the following command: 

Then a series of multiple command are passed using switch to strips all the trailin leaves only what is between and replace each with the rest two command add a few spaces and a The result is something like: 

I am administering a network with many locations and work with multiple providers I checked out with my DNS provider to change the records for one of the IPs, they replied: 

I knew that I can log with that account and the old password if I just disable the network connection. So I logged in with the network disabled and the old password and it work. If I try to refresh the trust by locking and unlocking this time with the network enabled I get the same: 

For all intents and purposes, it doesn't matter. Your server got hacked - what you need to do is restore from a last good known backup and ensure you're fully patched, have a sufficient firewall and IPS in place and reset all passwords. 

Oh gosh - don't even think about it, please. If it's still running, with no errors then you have no choice but to wait. Doing anything else stands a high chance of killing the task and leaving you with a inconsistent file system. 

I don't understand this question - you've disabled the page file (Which, to my knowledge, isn't recommended practice even in a virtual environment) and now you're asking how you can provide a page file? Why don't you just re-enable the page file on this one server, and set the size manually? 

If you're configuring a "Tagged" Network on XenServer, then this implies that it corresponds to a physical NIC which is plugged into a trunk port. Is this case? If the port on the switch is an access port with only a single VLAN, then you shouldn't be tagging anything on XenServer. Posting screenshots of your NIC and Networking pages from XenServer would be great, along with some more detail on your network topology. Also, there is no XenServer 6.5! 

If there is not a gateway on the traced path, only Layer 2 devices, the IP will not be reported. But Cisco has a utility that works on Layer 2. But this utility is dependent on CDP protocol which : 

Is this general practice and PTR records can be added only by the ISP companies that manage the IPs, or is just the practice for this DNS provider? 

I have some virtual machines servers with some micro-services and do some load tests with JMeter. All the servers including the JMeter machine are sharing the same virtual network. The virtual network has no adapters: 

But this has the great problem that many user can stop the script and that means they end up not having all the printers available. Can I connect all the available network printers without using the logonscript or the command line app? 

In our organisation we have around 30 printers an we are currently connecting for the user all the printers using a logon script that calls the command line app con2prt for a series of printers. 

There's no built in mechanism for Citrix to do this, however you don't state: What Citrix product you're using? What version? What version and type of the Citrix Client (Online Plugin, Reciever tc) you're running? You may have better luck by looking into redirecting the client device or client drives. 

Computer config gives you full configuration options similar to manually changing it in Folder Options on the machine. User config just gives you an "Execute" option which you can set as default. Computer config would be the preferred - just go onto a working machine and duplicate the settings. I think this would work for you, but you'll need to experiment: 

DHCP works using broadcasts which will travel between collision domains (Layer 2, switches) but not broadcast domains (Subnets, routers). You can, however configure ip forwarders / ip helpers on the routers to do this. Even better, the DHCP sever will know which subnet the request originated from, meaning a single server can server for multiple subnets.