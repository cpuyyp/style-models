To test your setup, you can use a tool like fiddler (or simply wireshark) from a client that is outside your network. 

First, I would like to point out that you're probably taking the problem from the wrong end: in theory, you should write down a business case for a better backup solution, have your management submit that to the central IT and then let them sort it out themselves. Now, this being out of the way, you're going to face a number of issues with that proposal. The first one being that Mercurial isn't built for what you want to do and neither is robocopy. You can find ways around you problem by deslaring the backup location itself as being a repository but that's not really efficient. I would instead investigate two different tracks. The first one is to use Volume Shadow Copy to create a snapshot history that you store on a different disk array on the server (you seem to be running windows on your file server and I'm assuming you have a recent version 2008 or higer). That will give you an easy way to recover user files (actually users will be able to recover their files themselves) while keeping the backup size close to the data size. The downside of this is that you will need admin access on the server to set this up so there is no way around involving your IT administrators (which I personally think is a good think, but that's only me). The second option is to bit the bullet and get a "real" backup software. If your data is important enough, you can probably make a business case for that investment. If not, you can always use Cobian backup to create incremental copies of the files as a user-level task or, if you really want to use mercurial, use Cobian backup instead of Robocopy to create the repository copy. 

Another option is to setup a source control repository somewhere and have the server automatically pull new revisions in is production tree. Mercurial seems well fitted for that. For the security side of things, you can setup HTTPS as transport and require logon for accessing the source control. 

You can enable user logon logging by following the instructions in KB221833. That will create a log file in which will contains timing information about each step of the logging process. 

What FTP program are you using ? Some cheap ones can't handle files larger than 2/4 gigs without rolling over. Also, grab a simple CRC program and run it against your local archive and the remote one. use CRC32 since it's much faster than even MD5 and you're not interested in it's cryptographic properties anyway. 

I woudl suggest that you check your path MTU and adjust your NIC accordingly. I've seen this happen when routers would drop oversized packets without sending back the proper ICMP messages (or when these messages would be dropped by a firewall on the path and not acted upon). 

The command line works in the same way. You can use it without argument to get a list of open files (sadly, the tool is hard-coded to truncate the output so it might be a bit touch and go if you have long file path). You can then use the command to disconnect a specific file. Edit: you can do better with powershell. 

It's a bit hard to answer that without knowing exactly what you want to monitor and why: bandwidth use ? number of transactions ? Number of connections ? Debugging an application ? Security check ? All these can be done but requires different approaches. Anyway, if I where you, I'd start by going to the DB server and checking the SQL performance counters to see if you don't have what you're looking for in there. 

The state sever, which is used to save session state in memory, has been stopped. There should be another log entry telling you why. 

To answer your question directly, the thing that can best improve your system's tolerance to hard drive failure is to use your extra hard drives to hot spare parts. With such a low disk count, the chances of having two simultaneous drive failure is low enough that you can rely on backup. Having a hot spare, however, will allow the system to recover much more quickly (and safely) from a single drive failure. You'll need two spares, though: one of each size. I have to add, however, that it sounds like your heading in the wrong direction here. First, you need to understand that RAID is not a backup system. There are many things that can go wrong with your RAID that will cause total data lose: manipulation error, controller failure, viruses, fire/water on the server itself. You need to use a real backup system. Second, the best way to increase redundancy for an AD is not to improve the disk subsystem or have better backup but to add a second domain controller, if possible at a different physical location. It will improve uptime much more than anything you will do for a single server setup. 

I think you need to have a better understanding of what SSL is for and how it works. SSL implements several security functions among which these are the ones that matter to you: 

The issue is that the SSL negotiation happens before the HTTP header is transmitted. This means that, at the time the server needs to pick a certificate and key pair for certifying its authority, the only information it has about what host the client is attempting to reach is the IP address. There is no viable solution around this problem for IIS 7. In IIS 8, you'll be able to use Server Name Indication (an extension to TLS) to work around it but even then it will only work with newest clients. ($URL$ 

The certificate common name is incorrect. It shouldn't be an URL () but just a host name (in your case trish0002). Also, you should use a fully qualified host name, not just the server name. Otherwiese, the same certificate could be used to identify different hosts belonging to different networks. In other words, you apparently have important issues with your certificate authority: it does not do its job properly. 

Limiting a program to just one instance isn't an OS function: it has to be coded right inside the code. This means the correct answer is: it depends on how the program performs the single instance detection. There are several ways of doing this: 

This does yield the expected result: instead of getting all users who are directly or indirectly members of the group I'm searching for, I get all direct members of that group, plus a random selection of indirect members (members of groups that are member of the searched for group). The list I'm getting seems arbitrary: I can't find any difference in group membership between two users who should be present in the result set but one is there and the other isn't. (I need to solve this issue with LDAP search because the result will be used in an application, not through powershell. But using powershell in this way, I can reproduce the original problem in the way described).