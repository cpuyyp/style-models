We're introducing some aspects of parallel processing quite early on in Scratch. Each sprite has its own script which appears to execute in parallel with those of the others. Scratch has a broadcast and receive message protocol, and support for shared as well as private variables and lists. Children might encounter this in a maze game, perhaps programming a number of similar ghosts to chase the player's avatar. It's also useful for agent-based modelling, e.g. the spread of an epidemic through a population. Of course, it's not true multi-threading, as all of Scratch runs inside Flash, inside the browser, on just the one core, but I doubt those using Scratch will be aware of, or care about, the distinction. This does, though, lead to potential difficulties in 'graduating' from Scratch to a text-based language such as Python - young Scratchers who've been used to programming in parallel in Scratch can find it hard to adjust to doing just one thing at a time in introductory Python programming. 

First up, the logistics. I had a detailed discussion with the HOD and also with the students, and convinced all of them to come for additional classes (I called them Bootcamps) The Bootcamp was a 2 day affair ( 6 hours each ) and was held on a holiday. No attendance or anything like that. Students who are serious can come (and 80 % of the class showed up) Taught the basics of OOPS. I essentially summarised the entire semester worth of contents into 12 hours. 

I kind of like this question because I have faced this situation many times, and I have in fact, told my students, that yes, they should change their specialisation or at least pursue something else. Most of the time, the students hate me for saying that. Some have accused me of interfering with their lives. Others, think I am overstepping my boundaries. However, as is always the case, some of them are happy to sit and discuss why I said that. These sessions can go on for long hours, but eventually, we are able to figure out what is right for them. For instance, I had this student who was just terrible at programming. If not for decent repetition and memory, the student would have failed the class. I found out that the student had extraordinary artistic skills. I was able to sit and map out a career path which used these skills. Now the student is working hard refocusing career goals, away from computer science. Like this, there have been many cases where I was able to realign people towards their goals. All said and done, I tell students that they are bad at this (by providing historical data for the same) but I also tell them what they should be doing instead. This seems to work. Over the years, I have learnt that some students are more open to such suggestions than others, and have gotten good at identifying such students. So, the process works something like this. 

Some interesting projects for those working with block-based languages (such as Scratch, Snap! and Blockly): For Scratch, check out Dr Scratch, which takes a rubric approach to evaluating how much 'computational thinking' is evidenced by a project. Whilst the analysis might seem a bit reductive, it can be used independently by learners and includes some useful guidance on how to progress. The developers describe their approach in this paper. Dr Scratch is built on Hairball, a Python module which does static analysis of Scratch projects. A more conventional autograder, lambda, is being developed by Michael Ball for Snap! It's already integrated into UCB's Beauty and Joy of Computing MOOC, and I think there are plans to make this more widely available. Michael wrote about this for Hello World #3. Chris Roffey has developed an autograder for Blockly used in the initial round of the TCS Oxford Computing Challenge programming challenge, although I don't think the code for this is shared publicly. 

I haven't taught about analog television but thanks to my gaming related experience (both as a gamer and game development trainer) I know that the topic of horizontal and vertical sync is very important for understanding one of the more core things of computer science, the display. Further, I am all about the cutting edge stuff. Yet, when it comes to getting the concepts of display, I am all for going to the roots, and CRT is by far the easiest way to explain these things. So, to answer your question, yes, the analogy is still relevant. And going further, It will and continue to be the starting point for studying this display stuff. Eventually, you will have to upgrade to modern screen but CRT is definitely the go point on the table. 

Date everything Everything should be dated; ideally, you'd prefix all filenames with a time stamp of when they were created (originally - not just when the actual file was transferred/copied/downloaded/etc.). As always, use ISO 8601 for time stamps and other dates. 

Absorb non-electronic notes If you do have non-electronic note materials; e.g., hand-written notes, images, lab book notes, or handouts from a class; if they're potentially important, try to get them on the computer. Scanning them or snapping a picture with your phone can be better-than-nothing. Optical character recognition (OCR) can help make some hand-written notes into electronic copies. It's not particularly reliable or stress-free just yet, but as long as you have something for future OCR to work on, it remains an increasingly useful tool. 

From a systems approach, capacity issues do make some sense for explaining a peak like the one here. The rapid inflow of students could be allowed since they were filling available capacity; then, once that capacity was taxed and admissions was allowing too many students into the classroom, they'd have had to cut back on allowing new students in. Helps explain the difference in interest-vs.-achievement The mid-1980's peak is the basically one place where Freshman interest is higher than degree completion. 

I'm sure there are plenty of others. Not sure I'd use them in every lesson, but they might be a useful incentive towards more purposeful use of the devices they bring. I know one school where in free time pupils are only allowed to play games they've coded themselves... 

There seems little value in students copying code off a display, but much in watching the teacher model how they think about the task, talking through the problem solving process of coding, as well as debugging (in the case of, ahem, deliberate, mistakes), iterative development and refactoring. I think this works if it's editing a longer program or writing short examples. I'd say good practice would include sharing the code produced, via Github or elsewhere, as well as screencasting the talking through of the development process itself. 

Seems like a developing field is a discussion that a community of researchers engages in. To participate, you'd need to: 

This answer's based on "A History of Capacity Challenges in Computer Science" (2016). As we can see in the plot, there's a rapid rise in Computer Science majors until the mid 1980's. It seems generally agreeable that this followed from computers being an interesting field of study. 

They help you trace through your notes to find something that you recall but need to reread. Especially in research, understandings constantly change. 

Avoid relying on third-party solutions whenever you can't easily-and-reliably convert away from them at any time without losing the prior content. 

This can make sense in the capacity explanation, because interested students would've been less able to achieve a CS degree due to rising entrance requirements. However, it's still strange that student interest drops back below obtainment after that. If it really was a capacity issue, then maybe students reasoned "CS is too hard" or "I don't have the grades/background to get into the CS department"? 

I am currently using Minecraft (especially Minecraft realms) to teach my developers (not starters but folks who have already covered some distance) how to work in team and also to improve their spatial and logical thinking. Also math, to some extent. So, first up, I am using Minecraft to teach, but not the basics of programming. If you wish to introduce someone to programming though, I would say Minecraft is useful but not with the whole MineCraft + NetBeans + all that jazz route. If you are going to all that trouble of configuring that for a freshers batch, you might as well just setup the real developer environment which would be simpler. When in comes to newcomers, I have found that Minecraft is more useful in bringing people into the world of computer based thinking. For instance, a lot of girls where I live are afraid and/or hesitant to get into programming. Many would find the concept of using their computers for anything other than watching a movie or checking email, extremely alien. In such scenarios, Minecraft would help. If you were to check the Minecraft Education site, that is also what Microsoft is doing with the program. 

As shown, women chose CS relatively more often until the peaks, then relatively chose other majors after the peaks. Not a bad thing So women like being medical professionals more than programmers - awesome! As long as people are freely choosing what they want to do, then that's exactly what we should be striving for. 

Definitely, Philosophy is exactly where you want to end up! Anyone who can't trace their field back to Philosophy hasn't understood it so much as memorized it. If you know exactly how to get from basic philosophical principles to the current state-of-the-art, then you can really claim to understand the field. Questions 

They never are. Whenever you read a textbook - even the best-known ones in well-developed fields, e.g. introductory Calculus - never just trust it. Put each word on trial, and absorb only the ones that you can't tear down. You often need to memorize large amounts of content without first thoroughly scrutinizing it. All of this content should carry a red flag in your head that labels it as unscrutinized claims. If there should come a time where you really need that information, or want to build upon it, you should really scrutinize it first. 

I have no recommendations for the theory part but since you are asking for implementation advice as well, here we go. If you are looking to use C sharp (you haven't specified a specific language of choice) then, as a dot net guy myself, I would recommend you start your self learning on concurrent programming with .NET implementation of same at Threads and Threading. The link provides with what you can (most of the things you have discussed can be done in C sharp Threads) and what you cannot do. If you wish to dig deep, there are no specific books on threads in C sharp, but you can use the reference book I used which contains detailed implementation notes on the same at C sharp 70 483 

By default, I approach every training I conduct ( I mostly conduct sessions for folks who are in their final year, or finished graduation but looking to upskill for a better career) with a simple underlying goal. If you learn xyz technology, will help you (eventually, if not right away) to make a little more money than what you do now. I have noticed that there are a lot of students (defined above) are pretty good in what they do (coding stuff). However, they lack certain essential skills that are really warranted in a work atmosphere. Taking my own example, at best, I am a below average developer at best, but somehow somewhere, I end up working in some high level projects. When I self analyse, I notice that my communication skills, presentation skills and simply the ability to discuss without fear with the key stakeholders (like the VPs, CTOs and CEOs) allows me get assignments that otherwise would not. So, over the years, I have made it a point to provide feedback about non-technical skills to my students. Now, after so many years of training, I want to take this to the next level. instead of just giving feedback, I want to make it part of my curriculum. There lies my challenge. In a session I conducted last year, I noticed that an entire team of developers were just straight up bad with their English. This may seem strange to other community members, but English, the preferred language of communication itself is a huge challenge for many of us folks here in India. When I finished my gentle tirade by concluding that the reason why all of them being billed lowered rates despite being technically competent (and I was one of the trainers who made that happen) is because you guys cannot speak and communicate in general. Eventually, despite maintaining excellent ratings for all the previous ratings, for the last day, I got 1 out of 10 ratings for every point in the review sheet. So, a very long story short, like the English example above, there are so many other qualities that are lacking in developers in general. I want to make it part of my training, but at the same, I don't want to end up with negative reviews either. You folks are in the education domain. Have you faced this issue? How can I handle this? Right now, the only solution I have is, just give up on that. My pay is linked to my reviews. At the same time, I feel strongly about this. As always, if this is too broad, I hope the mods will flag it, or I can break it up into individual questions. 

Speak the language, e.g. know about quantum computing terminology. Know the background, e.g. quantum physics and computational theory. 

Lots of profiling. For all anyone knows, that random gibberish full of typos and conspiracy theories on Reddit is the deepest, most correct theory of physics ever. But, since it's probably not and there's only so much time in the day, your bets are better hedged by focusing on content that appears to be of higher quality. To that end, you should develop a bunch of prejudices over what's likely to be better content, then selectively focus on it. Sure, if you're truly out of stuff to read, then maybe you'll end up going through that weird Reddit post; or, if you've got too much good stuff to read, you might not even be able to make time for Einstein's words. It's all relative. Examples of common prejudices: