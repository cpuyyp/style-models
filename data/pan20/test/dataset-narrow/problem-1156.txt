performing a two-qubit measurement with respect to some orthonormal basis $\{ |00\rangle, |\mu\rangle, |\nu\rangle, |11\rangle \}$, where $|\mu\rangle,|\nu\rangle$ span the subspace of Hamming weight 1, and producing as output some state $\rho \in \{ \rho_{00}, \rho_\mu, \rho_\nu, \rho_{11} \}$ corresponding to the outcome it measured, where $\langle 1 | \rho_{00} | 1 \rangle \leqslant \langle 1 | \rho_\lambda | 1 \rangle \leqslant \langle 1 | \rho_{11} | 1 \rangle$ for each $\lambda \in \{ \mu, \nu \}$. 

I'm not sure if you're going to come closer to a natural graph product formulation of than this. I rather suspect that if this does play a deep role in computational complexity that someone has studied something like this; but without knowing what areas of complexity theory it is pertinent to, I would not be able to tell you where to look, beyond perhaps investigating tensor products of hypergraphs. 

One reason why it might seem strange to you, that we seem to think there is more apparent (or conjectured) power in the randomized reductions from $\mathsf{NP}$ to $\mathsf{UP}$ than the comparable one from $\mathsf{BPP}$ to $\mathsf P$, is because you may be tempted to think of randomness as something which is either powerful (or not powerful) independently of what "machine" you add it to (if we caricature these complexity classes as classes arising from machine models). And yet, these reductions of different power exist. In fact, a computational resource such as randomness does not necessarily have a fixed amount of computational power, which is either "significant" or "not significant". We may consider any complexity class which is low for itself — for instance, $\mathsf L$, $\mathsf P$, $\mathsf{BPP}$, $\mathsf{BQP}$, $\mathsf{\oplus P}$, or $\mathsf{PSPACE}$ — to be amenable to a machine model of sorts in which the machine always has a well-defined state about which you can ask questions at any point in time, while also allowing for the computation to continue beyond the question that you ask: in essence, exactly that the machine can simulate one algorithm as a subroutine for another. The machine which performs the computation may not be particularly realistic if we restrict ourselves to practical constraints on resources (e.g. physically realisable and able to produce answers in low-degree polynomial time for problems of interest), but unlike classes such as $\mathsf{NP}$ — for which we have no idea how a nondeterministic machine could produce the answer to another problem in $\mathsf{NP}$ and use the answer in any way aside from (iterated) conjunctive and disjunctive truth-table reductions — imagining such a class as being embodied by a machine with a well-defined state which we can enquire into does not lead us badly astray. If we take this position, we can ask what happens if we provide these computational models $\mathsf M$ with extra facilities such as randomness or nondeterminism. (These extra facilities don't necessarily preserve the property of being interpretable by a machine model, especially in the case of nondeterminism, but they do give rise to 'new' classes.) If this extra facility gives the model more power, giving rise to a class $\mathsf C$, this is in effect equivalent to saying that there is a reduction from $\mathsf C$ to $\mathsf M$ using that facility, e.g. a randomized reduction in the case of randomness. The reason why I'm describing this in terms of classes which are low for themselves is that if we take seriously that they are "possible models of computation in another world", your question about randomized reductions corresponds to the fact that it seems that randomness dramatically increases the power of some models but not others. In place of randomized reductions from $\mathsf{NP}$ to $\mathsf{UP}$, we can observe that there is a randomized reduction from all of $\mathsf{PH}$ to the class $\mathsf{BP\cdot\oplus P}$ — which is obtained if you add bounded-error randomness to $\mathsf{\oplus P}$ — by Toda's Theorem. And your question can then be posed as: why does this happen? Why should some machines gain so much from randomness, and others so little? In the case of $\mathsf{PH} \subseteq \mathsf{BP\cdot\oplus P}$, it seems as though the modulo-2 nondeterminism entailed in the definition of $\mathsf{\oplus P}$ (essentially a counting quantifier modulo 2) catalyses the randomness entailed in bounded error (essentially a counting quantifier with a promise gap) to give us the equivalent of an entire unbounded hierarchy of existential and universal quantifiers. But this does not mean that we suppose that $\mathsf{\oplus P}$ is itself approximately as powerful as the entire polynomial hierarchy, does it? Neither the resources of bounded-error randomness nor modulo-2 counting are thought to be nearly that powerful. What we observe is that together, these two quantifiers are that powerful. There's also a question of whether we can really say that randomness is weak in absolute terms, compared say to nondeterminism: if randomness is so weak, and if we're so convinced that $\mathsf{BPP} = \mathsf{P}$, why can we only bound $\mathsf{BPP} \subseteq \Sigma^{\mathsf p}_2 \cap \Delta^{\mathsf p}_2$ in the polynomial hierarchy, using two levels of indeterminism, let alone one? But this may just be a result that, while we suspect that randomness added to simple polynomial-time computation doesn't give much power, we have no idea of how to simulate that additional power using only a small amount of nondeterminism of the sort involved in $\mathsf{NP}$ and $\mathsf{coNP}$. (Of course, it's difficult to prove anything nontrivial in complexity theory; but that again is just the statement that these different sorts of resources are difficult to compare on a scale!) There is no strong argument that I can give to defend why this should be the case, other than to observe that so far it simply is the case; and that if you think that $\mathsf{PH}$ doesn't collapse, is different from $\mathsf{\oplus P}$, and that $\mathsf{BPP} \approx \mathsf{P}$, then you should consider the possibility that facilities such as randomness and nondeterminism can have powers which are not easily comparable to one another, and which can synergize or catalyse one another to give computational power that neither one would plausibly have on its own. The hypothesis that $\mathsf{BPP} = \mathsf{P}$ is not that "randomness has no power", but that randomness alone (or rather, supplemented only by polynomial time computation and provided to an otherwise deterministic computational model) is not powerful. But this does not mean that there can be no power in randomness, which may be catalysed by other computational resources. 

Relevant definitions. In the following, I am broadly following the definitions of "Flows on hypergraphs" [free PDF link] by Cambini, Gallo, and Scutellà. 

How does one map binary strings to pure states? Why did you associate $0011 \mapsto \frac1{\sqrt2}(|0\rangle + |1\rangle)$ and $0110 \mapsto \frac1{\sqrt2}(|0\rangle + \mathrm e^{i\pi/3}|1\rangle)$ rather than the other way around? What specific mapping does one use for the boolean strings of length 4, with Hamming weight 1? What is the significance of the overlaps between those states, and the states you give for strings of Hamming weight 2? None of these details are provided, or even implied in any way that I can tell. What transformations of states are allowed depends crucially on the answers to the previous questions, but also is likely to be somewhat artificial from a physical point of view. Consider an operation mapping $|0\rangle $ to $ \frac1{\sqrt2}(|0\rangle + |1\rangle)$, and $|1\rangle $ to $ \frac1{\sqrt2}(|0\rangle - |1\rangle)$. In quantum computation, there is exactly one operation which does this — the Hadamard gate — and it's self-inverse. What transformation of bit-strings does it correspond to? Unfortunately, none. The strings of Hamming-weight 2 correspond to equally spaced states on the equator of the Bloch sphere (at 60° intervals), so their images in the Hadamard operation must include states at every 60° interval in latitude on the intersection of the XZ plane and the sphere — so your states for 4-bit strings must include $\frac12(\sqrt3|0\rangle \pm |1\rangle)$. This is not a real problem, but then what other states are there associated to strings of Hamming weight 1, and what does the Hadamard gate map them to? Unfortunately there is no solution for four-bit strings. In fact, it's not clear that any operations other than the identity operation and bit-flips can be represented, so it isn't clear how states involving the sub-bits would actually be prepared. One could respond that perhaps only the "continuum limit" actually describes the state-space of a non-trivial quantum computation, but then one should also not describe it as a "limit" of these discrete "models". Related to the above problem: How does one represent an entangled state? Consider the state $$ |\Psi^-\rangle = \mathrm{CNOT} \Bigl[\tfrac1{\sqrt2}(|0\rangle -|1\rangle)\otimes |1\rangle\Bigr].$$ How would you represent this state? The representation has to somehow include the fact that 

Initialise a list of all of the elements of $S$, representing those which are potentially compatible with $P$. Iterate through your forbidden relations, and remove from the list of potentially compatible relations, any element of $S$ which violates any of the constraints. (That is, if $a \leqslant b$ is forbidden, remove any element of $S$ for which that relation holds; and similarly for any element of $S$ fo which $a \cong b$, if that relation is forbidden.) Perform a breadth-first search of $R$: and at each link $(i,j) \in R$ traversed, remove from the list of potentially-compatible orders any order $P$ for which $j < i$, by checking for each order on the list whether this is the case. 

Passing to the dual hypergraphs to look for products. As you mention in the comments, we may interpret the bitstrings contained in $A$, $B$, and $C$ as edges in hypergraphs. 

The complexity of this is essentially the sum of the in-degree of $i$ and the out-degree of $j$. I imagine that you would also like to remove the prohibition on $i \leqslant j$ and/or $j \leqslant i$ at the same time; we can achieve this simply by removing any restrictions which are stored at $F[i,j]$ or $F[j,i]$ as appropriate. Representing subsets of $S \subseteq P_r$ Assuming that all you want to use the subsets $S \subseteq P_r$ for is to check compatibility of individual pre-orders with the elements of $S$, I think the best approach is to store an $r \times r$ array where each element $(i,j)$ contains a collection which indicates the labels of all pre-orders $P \in S$ such that $(i,j) \in P$. (That is: you do not need to store any forbidden relationships.) The collection may as well be a simple list, for small sets $S$ or if the orders which it contains don't have too many common relations (e.g. minimal and maximal elements in common); otherwise use your set-implementation of choice (e.g. balanced search trees) instead. This can be updated simply by adding a new relation $P'$ to the entries for all pairs $(i,j)$ which are contained in $P'$. If $P'$ is represented by a reduction $R'$, one may do this by a depth-first search from its minimal elements (which one may store a list of in the representation), and performing depth-first search to find all descendants of each element. 

I'm interested in operational ways of demonstrating (with high probability of confidence, in an error-free setting) that a POVM operator on n-qubit states is a projector. Specifically, I'm interested in ways in which one can do so using product states. (I'm more interested in describing it as an analytic characterization of projectors, but I find it easier to describe roughly what sort of characterization I'm looking for in terms of a protocol.) I'm not interested in poly-time approaches per se; just ones which are much better than process tomography. Preamble. One way to characterize a projector is process tomography. Given a POVM P, represented as a CPTP map of the form $$ P(\rho) \;=\; \sum_{j = 1}^m \;\;\mathrm{Tr}(\rho E_j) \;\;|j\rangle\!\langle j|\;, $$ one may determine whether any particular Er is a projector by collecting statistics on measurement results, e.g. on all n-fold tensor products of the eigenstates of the Pauli X, Y, and Z operators, using these to compute matrix coefficients for Er, and then determine whether that matrix is idempotent. If we are given a promise that Er is either a projector or bounded away from being a projector in some suitable norm, the probability of correctly deducing whether it is a projector then amounts to the confidence in the estimates of the matrix coefficients. However, if you know in advance that Er is a projector, and you would like to convince someone else that it is, you can certify this somewhat more efficiently if you are able to prepare a complete basis of eigenstates of Er. The outcome Er will never arise for 0-eigenstates, and always will for +1-eigenstates. These eigenstates aren't necessarily product states, of course, but having them as a resource makes it in principle less laborious to give evidence that Er is a projector: it suffices to collect enough statistics to show that it is quite likely that any one eigenstate is indeed an eigenstate (made easier by the extremality of the measurement probabilities). There are only 2n such eigenstates, and the confidence in your estimates converges (and quickly) despite "sampling" fewer states, because of the extremality of the eigenvalues. Question. Is there a characterization of projectors, which one may (statistically, as with the eigenstates) verify more succinctly than tomography — and which can be performed with only (input-dependent) product states? If this cannot be done for all projectors in a way which is clearly better than tomography, what are the classes of operators (apart from the class of product projectors) for which there are better approaches? For instance, this is easy for rank-1 projectors in the case n = 2 (even if we consider qudits of arbitrary dimension rather than qubits), just by using the Schmidt decomposition; we can find a (not necessarily orthogonal) basis for the kernel, and then complete this basis to allow us to estimate the trace of Er and show that its other eigenvalue must be close to 1. But it's not clear how to extend even this simple case to n = 3. 

But assuming, for instance, that ZF set theory (or an essentially equivalent foundation for mathematical subjects such as computer science) is consistent, we don't need Gödel's Theorems anymore to tell us that there are statements which are neither provable nor disprovable from our foundations: interesting specific examples, such as the Axiom of Choice have already been demonstrated. So even without Gödel's Theorem, it is certainly concievable that a statement such as "P = NP" is indepdendent of set theory. For this question, Scott Aaronson has written a very accessible article on the subject, which also goes over the basics of formal logic, what it could mean to have a model of set theory which included an axiom which is equivalent to asserting its own inconsistency, and the subject of logical independence results in general. 

I'm happy to say that I think we can answer this question in the affirmative: that is, deciding whether a linear congruence is feasible modulo k is coModkL-complete. We can actually reduce this problem to the special case of prime powers. One may show that: 

So, that's likely why these ideas are not widely studied. Generally, whenever there is an article which claims to generalize quantum computation in some bold new manner, one should check whether probability is conserved (and what significance the authors attribute to it not being conserved), and whether the algebra is otherwise sound. 

I'm not sure who would suggest that qubits can meaningfully be described this way, or why anyone would do so. There are simply too many missing details, and it falls afoul of no-go theorems for local hidden-variable theories in quantum mechanics. This isn't "an exhaustive description of a digital quantum computer that uses a finite set of pure states to perform its operation" — even for single qubits — and extensions to multiple qubits immediately presents problems. 

Thus, for any other gate we may supplement $\pi_1$ (or $\pi_2$) with, we obtain either a tautologous set, obtain no additional accepting power over just $\pi_1$ (or $\pi_2$), or may reduce to an earlier easy case of OPSAT. Then any instance of OPSAT with $\pi_1 \in \mathcal G$ or $\pi_2 \in \mathcal G$ is easy. Delta-function gates. Consider the two-bit gates for which there is only one input which satisfies them: $\AND$, $\NOR$, $G_{10}(x,y) = x \et \neg y$, and $G_{01}(x,y) = \neg x \et y$. Circuits made only with $\AND$ gates can only accept the string $1^\ast$: supplementing them with any other delta-function gate allows them to simulate either $\EQUAL$, $\pi_1$, or $\pi_2$, which are solved cases; similar remarks apply to $\NOR$. As well, the gate-set $\{G_{01}, G_{10}\}$ can be used to also simulate the $\PARITY$ gate. We may thus focus on either the gate $G_{10}$ or $G_{01}$, possibly supplemented with the gate $Z(x,y) = 0$. We focus on $G_{10}$, with the case of $G_{01}$ being similar.Circuits made of $G_{10}$ alone can be built to accept $1(0|1)^{n-1}$, except for the string $11$, by applying an arbitrary circuit to the final $n-2$ bits and then applying the circuit $G_{10}(x_1, G_{10}(x_2, x_3))$. Clearly, the string $11$ can't be accepted by $G_{10}$ or by $Z$; and we can show by induction that any $G_{10}$ circuit which accepts a string must have intermediate outcomes of the gates in the left-most branch all yielding $1$, up to the left-most input itself. No additional benefit is obtained by adding $Z$ gates. Therefore, $G_{10}$ circuits can only accept $x \in 1(0|10|11)(0|1)^\ast$. Finally, circuits composed only of $Z$ gates accept no inputs.