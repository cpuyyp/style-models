If you want to port forward, then you should assign the virtual machine a fixed IP address. For Ubuntu this can be done in the file . Consult the documentation presented by the command Choose an IP address on your subnet not served by DHCP. Alternatively, you can configure your DHCP to serve the virtual machine a static IP address. Depending on your virtualization container, you may be able to configure it in the virtual machine definition. 

Normally passwords would go into a password file and be read as need by the transport that needs it. The example configuration should contain auth examples for both server and client connections. It is possible to configure multiple authentication methods for server authentication and/or client authentication. Exim uses two passwords files by default: for users connecting to exim, and when exim is connecting to a remote server. 

It is a good idea to use different (sub)domains for email and web servers. For example and This allows you to resolve this issue fairly easily. Typically rDNS is typically only required for outgoing SMTP traffic. You would configure your domain to use the subdomain as its MX, and configure that domain in the PTR record. Try checking your domain with to see other common domains. 

Check your logs for messages containing . If there are problems setting up logging, then it should should be locked. Is running? It is needed for locking to work on NFS versions before 4. Alternatively, you may try mounting with the option for local lock management. This is not recommended if you have remote clients for the file systems. 

EDIT: I ran some queries on my database of email data. Of 2500 connections that used a local address in the MAIL FROM command, only 28 retried with a non-local address. This is about a 1% fail rate on just rejecting the use of local addresses in the MAIL FROM command. I found no cases where a server reconnected and tried a non-local address. Assuming the Envelope_from header is added before spamassassin filters the message both conditions can be combined in a single blacklist entry in local.cf. Again this assumes you do not run spamassassin on outgoing email. 

Try changing the path in /etc/environment. Put /opt/jdk6.0/bin in front of /usr/bin. However, /usr/bin should be a symbolic link to an appropriate Java version. On Debain or Ubuntu you can use update-alternatives to modify then entry. However, in that case you should consider installing the sun-java package and not run Java from /opt. 

Run fsck first to ensure the data is consistent. You may have to use sudo to gain the require privileges to repair. Then you can use to remove the file. You may want to use less or more to read the file first to see if you want to keep it first. There are a number of tools which allow you to browse and delete files. I use mc and emacs from the command line. If you use a graphical interface, the file browser will also handle the cleanup. 

Wombie has given the same answer I would use for setting up radv and routing. Inet6 is designed to do self configuration without the need for a DHCP server. I don't use DHCPv6 as it can be handled by radvd and zeroconf. radvd can be configured to supply name servers, and can advertise servers from /etc/resolv.conf. New versions of radvd have also implemented distributing search lists. I configured avahi-daemon to distribute name services and servers to clients. You will need clients are able to do service discovery. I haven't done much testing of service discovery as I run a dual stack network across the board. 

As has been noted, your domain registrar likely can forward email for you. If you are sending email from your domain, you will want some basic email setup including an MX. Adding SPF, and DKIM signatures may increase your delivery success. You will want your MX to accept (and possibly forward) email sent to the sending address, as well as mail sent to postmaster of each sending domain (which you do want forwarded). You may want to use a address to get responses directly to your address. Although, if you have set up forwarding, you do have that option as well. If you already have setup an email server on one of your Linux servers, it can act as the MX for all your domains. You will need to update your configuration to include the additional domains. You have two options for aliasing addresses in this case (both can be active): 

I would like to use DKIM to validate, but a high percentage of senders don't properly publish their public key in DNS. If you can't run these tests, while the connection is still open, do not bounce the message unless you can verify that the sender was not spoofed. (I do appreciate all the offers of money from the FBI, UN, banks, etc., but they still haven't delivered.) 

The default apache configuration restricts access to most locations to secure your system. Certain standard locations have less restrictive access. Review you configuration for lines containing , or . These will be inside a specification indicating what they apply to. Check the directory tree containing the resources you can't reach for a files as these could contain additional restrictions. Try using a tool which allows you to see the headers of the failing requests. The first line should include an status code and a short explanation of the status code. Check your apache errorlog which should log any failures based on security restrictions. 

To have it work for all programs you will need to use a transparent proxy, with firewall rules or DNS configuration to get your traffic to the proxy. I setup a Transparent Squid Proxy using Shorewall to build the firewall rules. It also run a standard proxy on the standard port which is more reliable. To use DNS you will need to limit access to upstream servers. This will require firewall rules. However, it can be done for a single system with an entry in the file. Your proxy will need to be configured to forward traffic as required. In your case it should proxy to a specified address, and pass the rest of the traffic normally. Unless you are sure of the IP address for is fixed and stable, you should likely proxy all traffic. Large sites often use special DNS handling to provide IP addresses to the fastest available server for a particular user. These tend to move over time, sometimes quite frequently. 

Note: This will not force word.php to be HTTPS To do that you would need an equivalent rule on the HTTPS site to redirect back to HTTP. Be careful that you don't leak secure information in that case. 

Outlook is very forgiving of formatting errors, and has been very good at introducing them. Try delivering them to a store where you can look at them in raw format. A local mailbox or Maildir store on the server would be ideal. ContentHeaders may be checked by both desktop clients and webmail clients. Don't assume because it is webmail it supports HTML. Several webmail applications default to text mode due to security issues with HTML. Ideally you should generate a multi-part MIME with both text and HTML format. This is the most flexible option. Properly done it should display well everywhere. 

The simple answer is No. I have seen a site that claimed to do this, but it didn't serve all the A records for cases where I knew some of the answer. The PTR records used for reverse DNS records belong to different zones than the related A records. They may note be administered in the same DNS servers or by the same organizaations. It is possible that several DNS administrations have A records pointing to the same IP address. The DNS admimistrator for the PTR records may well be unaware of at least some of them. There are a number of services offering Dynamic DNS. It would be extremely rare for the PTR to match A record. In many case there is no PTR record for the IP address. For properly configured mail servers there will be a PTR record matching the A record for the mail service. This may not be the only A record for the server. It has been my experience that most spam comes from IP addresses where the PTR record does not match the A record, and in many case at least one record is missing. If an address is also used for virtual Web servers, there will be more than one A record. These may be spread across a number of domains, and the various DNS aministrators may be unware that others are using the same IP address. 

Other factors normally constrain the data rate. Data transfer rates will be the slowest of the rate the source can provide the data and the rate the target can consume the data. Switches may not be able to transfer data between all ports at the full rate. In most configurations many ports transfer data at rates well below the speed of the link. Faster links may still be desirable as the link latency will be lower. A 1200 byte packet will take roughly 1 millisecond per hop on 10 Mbs links, 0.1 millisecond on a 100 Mbs link and, only 0.01 millisecond on a 1 Gbs link. There will be additional latency in transfer due to buffering, distance, and speed of transfers within devices. 

This will run the commands in a sub-shell and run those in the background. If you run into problems with zombie processes, the spawned process with the command after the backgrounded commands. This assumes you are using as the scripting shell. 

Normally, you want your load-balancer and web servers in a DMZ (De-Militarized Zone). Access to the DMZ from the internal and external networks should be controlled by the firewall. If the load-balancer is in front of the firewall, it is unable to balance both these loads. As other have posted, both the firewall and load-balancer will be a single point of failure unless you have redundant equipment. 

Most of your content should be owned by a user other than apache. Assuming your use FTP to update your content, you can set all the files to be owned by ftp:apache. I would suggest using SCP for SFTP instead of ftp. Files should have permissions 644 or 640 and directories permissions 755 or 750. For files and directories the application needs to write to add group write access. 

Mail servers are located by an record. Mailgun should provide a name for the mail server to target. I've included SPF records which you may want to include. If they provide multiple host names add more records. Some servers don't support the SPF record type, in which case SPF will use the record. 

There is no DNS records you can create that will do this. You will need to allow time for DNS change to propagate. Setting the TTL down to an hour or so for twice the old TTL (usually a couple days) before the change will speed the propagation significantly. You could proxy the new server from the old server. Until the DNS changes propagate fully you will have some traffic on the old server. You could also look at doing DNAT on the firewall for the old server if all domains moved. 

Are they using a proxy service on the firewall? It may be possible that facebook related pages containing session cookies are getting cached. Depending on the order pages are loaded it could replace the session cookie with someone else's cookie. It should be less of a problem with connections which shouldn't be cached. 

While it may require some site modifications, you may want to consider moving static content to a separate domain/server. This would include content such as javascript, css, and images. This server should receive simpler requests devoid of cookies and authentication. As your site grows this site should be easy to scale up by cloning the server and adding additional IP addresses for the domain. Dynamic content tends to be more difficult to scale out. This gets more difficult if you have sessions to track. Unless you tie users with sessions to a particular server, you will need to look at session replication. As your site grows you will eventually need to look at scaling your database. Replication is one approach and works well for read-only or mostly read access. 

If you have control of the application, it should not matter if they could see other application paths. If the apps run with different UIDs, you only need to set permissions on the application directory to 700 so that other users can't see their files. Even if the user can enter paths to resources, you can sanitized the paths they enter. Limit the accepted paths to appropriate directories. There are a number of methods of providing configuration data securely. You may want to consider using a service repository/registry. If you have passwords or other sensitive data, it should be store in an encrypted format.