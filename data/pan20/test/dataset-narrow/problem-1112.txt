Let a finite alphabet $\Sigma$. Let $\mathcal{Reord_\Sigma}$ be the family of computable partial functions between the strings of this alphabet $r\,:\, \Sigma^*\, \rightarrow \Sigma^*$ with the restriction that the value must be a permutation of the argument. I'm interested in a class of deterministic abstract machines with finite control that compute all and only the functions of this family. That is, given a string $s \in \Sigma^*$ as input, the machine either doesn't halt or halts returning a permutation of $s$, and every possible mapping of those is realized by some machine in the class. All the transitions should be local (no random access on a tape), and possibly they should not explicitely name the symbols to be written (they should be 'copy' or 'swap' operations, not 'write symbol "A" '). Does this class of transducers exist? Has it been researched? 

I have a large dataset of trees and I would like to search it by specifying a treelet (connected subgraph). The query should return all the occourrences of the treelet in the dataset. Are there efficient algorithms to do so? I was thinking of something like suffix arrays, however, naively encoding the trees as strings (by a fixed traversal ordering of their nodes) won't work, since the search treelet can be of any arbitrary shape. UPDATE: Some details about the typical instances I expect: The dataset will consist in at least tens of thousands trees, each consisting in about twenty to thirty nodes. The trees will not be not binary, but the typical children number per node will be small (usually no larger than four or five, although in some degenerate cases it can reach about thirty). The number of labels will be in the tens of thousands. I need that for NLP applications: each tree will be the dependency parse of a sentence, each node representing a word occourrence and each label a dictionary word (with some decoration). 

Historically, scientific articles had to be printed on paper, and journals were shipped internationally. Each extra page used to add a significant cost, thus articles were subject to length limitations, and even simple working code usually takes way much space than an informal description. Today there is no good reason not to include code in any kind of article that references an algorithm. It may also useful to abandon print-oriented formats like pdf and postscript in favor of more semantically-aware formats (HTML with MathML or perhaps an opensource variation of Mathematica). 

There are two main gaps in our understanding of neural networks: optimization hardness and generalization performance. Training a neural network requires solving a highly non-convex optimization problem in high dimensions. Current training algorithms are all based on gradient descent, which only guarantees convergence to a critical point (local minimum or saddle). In fact, Anandkumar & Ge 2016 recently proved that finding even a local minimum is NP-hard, which means that (assuming P != NP) there exist "bad", hard to escape, saddle points in the in the error surface. Yet, these training algorithms are empirically effective for many practical problems, and we don't know why. There have been theoretical papers such as Choromanska et al. 2016 and Kawaguchi 2016 which prove that, under certain assumptions, the local minima are essentially as good as the global minima, but the assumptions they make are somewhat unrealistic and they don't address the issue of the bad saddle points. The other main gap in our understanding is generalization performance: how well does the model perform on novel examples not seen during training? It's easy to show that in the limit of an infinite number of training examples (sampled i.i.d. from a stationary distribution), the training error converges to the expected error on novel examples (provided that you could train to the global optimum), but since we don't have infinite training examples, we are interested in how many examples are needed to achieve a given difference between training and generalization error. Statistical learning theory studies these generalization bounds. Empirically, training a large modern neural network requires a large number of training examples (Big Data, if you like buzzwords), but not that monumentally large to be practically unfeasible. But if you apply the best known bounds from statistical learning theory (for instance Gao & Zhou 2014) you typically get these unfeasibly huge numbers. Therefore these bounds are very far from being tight, at least for practical problems. One of the reason might be that these bounds tend to assume very little about the data generating distribution, hence they reflect the worst-case performance against adversarial environments, while "natural" environments tend to be more "learnable". It is possible to write distribution-dependent generalization bounds, but we don't know how to formally characterize a distribution over "natural" environments. Approaches such as algorithmic information theory are still unsatisfactory. Therefore we still don't know why neural networks can be trained without overfitting. Furthermore, it should be noted that these two main issues seem to be related in a still poorly understood way: the generalization bounds from statistical learning theory assume that the model is trained to the global optimum on the training set, but in a practical setting you would never train a neural network until convergence even to a saddle point, as to do so would typically cause overfitting. Instead you stop training when the error on a held-out validation set (which is a proxy for the generalization error) stops improving. This is known as "early stopping". So in a sense all this theoretical research on bounding the generalization error of the global optimum may be quite irrelevant: not only we can't efficiently find it, but even if we could, we would not want to, since it would perform worse on novel examples than many "sub-optimal" solutions. It may be the case that optimization hardness is not a flaw of neural network, on the contrary, maybe neural networks can work at all precisely because they are hard to optimize. All these observations are empirical and there is no good theory that explains them. There is also no theory that explains how to set the hyperparameters of neural networks (hidden layer width and depth, learning rates, architectural details, etc.). Practitioners use their intuition honed by experience and lots of trial and error to come up with effective values, while a theory could allow us to design neural networks in a more systematic way. 

I'm a bit skeptical about the possibility of explaining that problem to a 10 years old, or even to a lay person, without incurring in misrepresentation of the key concepts. All explanations formulated in terms of "easiness" vs "hardness" of finding vs checking solutions assume the Cobham's thesis, which is arguably false in the general case, and can be considered little more than a rule of thumb at best. 

Complexity classes are defined in terms of asymptotic complexity, hence they don't map well to the cognitive abilities of humans, which are necessarily limited to bounded problem sizes. The rule of the thumb is: if something is easy for a computer, then it may be hard for a human, vice versa, if it is hard for a computer it may be easy for a human. Here "easy/hard for a computer" refers to practical tractability, not an abstract complexity class. For instance, adding up a list of 1 billion integers is easy for a modern computer and difficult for a human, while producing a verbal description of a picture is easy for a human but difficult (currently impossible in the general case) for a computer. Artificial Intelligence research showed that many cognitive tasks that humans and animals carry out easily, in some cases even subconsciously, can be modeled as NP-hard problems. Humans are not able to find optimal solutions to these problems for all sizes, but they are able to find heuristic solutions for practical sizes much better than the best known AI algorithms. Also note that the left-brain vs right-brain distinction you mention is too simplistic and obsolete. Lateralization of brain functions is much more subtle, and may even vary from one individual to another. 

You might also want to look into Distance measures to compare real and ideal quantum processes arXiv:quant-ph/0408063 which gives an overview of distance measures for quantum channels and their relationships. They use the term S distance for the diamond distance and J distance for the trace distance of the Jamio≈Çkowski operators associated to the channels. 

Some models of computation are universal in the sense they can compute any arbitrary computable function $f:\mathbb{N} \rightarrow \mathbb{N}$. Other models are universal only as far as the input and output are encoded: $\exists$ a set $A$ such that $\forall$ computable functions $f:\mathbb{N} \rightarrow \mathbb{N}$ the model can compute a function $f':A \rightarrow A$, such that $\forall n \in \mathbb{N},\ f(n)\ = \ d(f'(e(n)))$, for some encoding and decoding functions: $e:\mathbb{N} \rightarrow A$ $d:A \rightarrow \mathbb{N}$ with suitable constraints on the computability (and possibly the complexity) of these functions. 

With regards to question 1, knowning the distance between the unknown target vertex and some known origin vertex on the hypercube can help the search process. However, the value of the distance itself determines how much helpful this information is. Typical quantum walk algorithms are usually variations/approximations of Grover search: they involve an approximate rotation of the state vector in a 2-d subspace of the total Hilbert space. You can use these algorithms to efficently prepare an approximately uniform superposition of all vertices at a given distance from the origin. Then you can search your target vertex inside this superposition using quantum or classical (Monte Carlo) search: For classical search just prepare the superposition and measure it in the vertex basis and repeat until you find the target. For quantum search, the superposition preparation procedure (and its inverse) becomes a subroutine that replaces the Hadamard transform in the Grover iteration. The usefulness of this depends on the value of the distance: in the $n$-dimensional hypercube the number of vertices at distance $d$ from a given origin is the binomial coefficient $\binom{n}{d}$. Hence the majority of vertices ($\approx \frac{2^n}{\sqrt{\frac{\pi}{2}n}}$) are at $\approx n/2$ distance: while you can efficiently prepare the superposition of these vertices, searching the target inside it still takes exponential time. 

There is a one-to-one correspondence between Chomsky Type-0 grammars and Turing machines. This is exploied in the Thue programing language which allows you to write Turing-complete programs specified by an initial string and a set of string-rewriting rules (a semi-Thue grammar, which is equivalent to a type-0 grammar). UPDATE: Other than esoteric "Turing tar-pit" languages like Thue, various general purpose languages that allow the programmer to extend their own syntax can be used to perform Turing-complete computation during the parsing-compilation stage. Languages in the Lisp family, in particular Common Lisp, are probably the most obvious examples, but also, broadly speaking, languages with static type checking that doesn't always need to halt, such as C++ with templates, Scala and Qi. 

If I understand correctly, if you want your circuit to be able to generate all permutations, you need at least $\lceil \log_2(n!) \rceil$ probabilistic gates, though I'm not sure how the minimal circuit can be constructed. UPDATE: I think that if you take the Mergesort algorithm and replace all comparisons with random choices with appropriate probabilities you'll get the circuit you are looking for. 

I think that the description is a bit too vague. You have to assume that $w(s)$ is sufficiently smooth to make efficient optimization feasible. Anyway, I don't know about any particular theory for this kind of problems, but the obvious algorithm that comes into my mind is to expand the successors of each element starting from the one computed from the heuristic key and then use some scheme to consider the other key in order of increasing distance form the heuristic one (either exhaustive or random sampling). More generally, you could convert the problem into a more conventional optimization problem by considering an extended search space: $\forall s$ consider a set $\left \{ s_d \right \}$ elements. $s_0$ corresponds to $s$ and has as successors only $s_1$ and the element reached by the heuristic key. $s_1$ has as successors only $s_2$ and the elements reached by keys at distance $1$ from the heuristic key, and so on. (Note however that $s_{d_{MAX}/2}$ has exponentially many successors). You could assign the weight $w(s_d)$ equal to $w(s)$ plus some penalty (additive or multiplicative) based on distance $d$. This will drive the optimization algorithm to expand low-distance nodes first. 

I think it's improper to call this measure 'complexity'. A more apt name would be 'input to output size relation'. Note that, strictly speaking, this is a property of the problem you are solving, not of the particular algorithm you use to solve the problem. In the case of the compression problem, if you mean lossless compression of arbitrary binary data, then the average compression ratio is never less than 1 (and it's typically strictly greater than 1). 

Let a quantum channel $\Phi(\cdot)$ between two Hilbert spaces $\mathcal{H}_{in}$ and $\mathcal{H}_{out}$. What is the quantum channel $\Phi_{inv}(\cdot)$ that best reverses $\Phi(\cdot)$ ? $\forall $ states $\rho$, the state $\widetilde{\rho} \equiv \Phi_{inv}(\Phi(\rho))$ should be the best approximation of $\rho$ (according to some appropriate distance measure). If we assume that the input state comes from a known probability distribution, according to the quantum data processing inequality there are cases, depending on the source entropy and the channel properties that allow for perfect decoding. However, how do you perform decoding in cases where perfect decoding is not possible, and in particular if no source distribution is known (so you can't assume that the state has been encoded with an error-correcting code)? I presume somebody has already thought about this, but I can't find it in the literature. 

If I understand correctly, there are proofs of universality of systems, such as the Wolfram's 2-state 3-symbol Turing machine which are controversial because of subtleties regarding enconding. 

If you just consider consecutive samples then, for most kinds of PRNGs the answer is that you can't distinguish them from true RNGs. If you consider large input samples, then, in principle (if you don't take complexity issues into account), you can always distinguish a PRNG from true randomness: For each PRNG $r$, seed length $L$ and starting point $x$ there exist a sample sequence length $k$ such that $f([r(seed, x),\ r(seed, x+1),\ \dots,\ r(seed, x+k-1)])\ =\ 1$ with certainty, and $f(\dots)\ =\ 0$ for any other sequence, where $r(seed, n)$ is the n-th sample generated by the PRNG initialized with $seed$. If the PRNG has finitely many states, as it is the case with all the PRNGs commonly in use, then the sample sequence length $k$ is independent of the seed length and starting point. The distinguishing algorithm $f(\cdot,\ \cdot)$ can be just a brute-force search that tries in order all possible initial states to check whether they produce the sample sequence. Of course, for large state spaces, this is unpractical. Moreover, the output sequence of a finite-state PRNG is periodic or eventually becomes periodic, although the period can be very large. The design goal of cryptographically-secure PRNGs is to ensure that no method substantially faster than brute-force search can be used to distinguish them from true RNGs. Currently, various PRNGs are believed to be cryptographically-secure, though no mathematical proof exists. Cryptographically-insecure PRNGs like the Mersenne Twister can be easily distinguished from true RNGs by observing a relatively short sample sequence.