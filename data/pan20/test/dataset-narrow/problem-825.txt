There are two basic scenarios- 1.) End host / stub network (i.e. edge of the internet) - These entities know that a packet is either delivered locally or is sent to a default gateway. This determination is made by the IP address of the transmitting device and its subnet mask. 2.) Router with full view/default-free - A router receiving full routes (generally via BGP) has a database of publicly routed subnets. Your 64.34.119.x network may be part of a larger summary route. The router in question finds the shortest match (i.e. longest subnet masks) between the destination of the packet and these locally received subnets. This route will have a corresponding next-hop - the next router to which the packet must be sent. This next-hop router performs the same look-up and forwarding, and so on, until the packet reaches a router with said subnet locally connected (most likely something like #1). Note that there is little or no knowledge of the intervening network(s) between the transmitter and the receiver. Each routing device knows only the next-hop to reach a given destination. 

I think you answered your own question. Removing the HP switch allows everything to work. Putting the HP switch back breaks it. Start by testing the connection between the Cisco and HP switches and work your way back. We don't have enough information to debug the HP switch (L2? L3? model number? configuration?) but it seems like your issue is most likely there. 

In some circumstances having those SSD drives as local scratch drives - for particular latency-intensive tasks, caching or even swap - can be a win. The SSD solution is quite a bit more expensive than the USB. If all you're going to do with the SSD is boot, then go with the USB. Frankly, though, there are several ways to netboot VMW that work well and can relieve you of the need for any boot media. 

Multiple IP addresses is not the same thing as multiple interfaces. Typically a single interface represents a single L2 network but can have multiple IP addresses (sometimes known as aliases or secondaries). Multiple interfaces, in turn, generally correspond to distinct L2 networks. 

In short you're blocking the wrong port. SIP registration happens on port 5060 (TCP or UDP). The 10000+ ports are going to be for actual RTP bearer traffic, not call setup. Adjust your firewall to block 5060 and 5061 inbound and you should stop seeing the messages. While you're at it you might also consider whether you even want or need your system to be listening for SIP registrations on all interfaces. Remember - you likely connect to your provider, not vice-versa. 

More buffer doesn't necessarily imply more speed. More buffer simply implies more buffer. Below a certain value you'll see overflow as applications can't necessarily service received data quickly enough. This is bad, but at the point where there is sufficient buffer for the app to service at a reasonable rate even in the event of the occasional traffic spike then anything else is likely wasted. If you go -too- large then you're placing a much larger burden on the kernel to find and allocate memory which, ironically, can lead to packet loss. My hunch would be that this may be what you're seeing, but that some other metrics would be required to confirm. It's likely that the 2.5M number may come from recommendations around setting rmem and wmem values for TCP - where the relationship between window sizing and buffer settings can have significant effects under certain circumstances. That said, TCP != UDP - but some folks assume that if it helps TCP that it will also help UDP. You've got the right empirical information. If I were you, I'd stick at the 256K value and call it even. 

There's a big difference between swap being used and the machine actively swapping. The kernel will preemptively move more and more onto disk as it sees calls for large amounts of memory. If the stuff moved out to disk isn't that frequently used then it's not necessarily a huge problem. If data is constantly being swapped in and out then there's usually a big problem. The real measure here is to look at something like iostat to observe how much data is actually being passed back and forth in a given interval. That said, a 15G VM on a 16G box is probably not going to play out well. The OS itself requires a certain amount and there's always a percentage of overhead for the VM. If you are, in fact, actively swapping then you may see substantial improvements just backing off to, say, 12 or 13G. 

RDMA over Ethernet actually shares some of the same requirements as Fibre Channel over Ethernet (FCoE) as far as lossless delivery and bandwidth guarantees are concerned. Many of the switches that support elements of the various Data Center Bridging (DCB) standards are also appropriate for RDMA. At a minimum you'll likely need Priority Flow Control (PFC) but depending on the stack in use you may require jumbo frame support. The other mechanisms in DCB (bandwidth reservation, congestion notification, etc) are also hugely helpful if supported. These features tend to be found in higher end 10GE switches - specifically those that can be had with enterprise storage features. There's nothing theoretically to stop you from running RDMA over Ethernet on a generic switch, of course, but any kind of glitch (i.e. the sorts of things the gear above is supposed to stop) is going to get pretty bad pretty quick.. 

Unless something has changed since I last looked at them, Dell switches don't actually support spanning tree on a per-VLAN basis. If there are other switches connected to your stack (on any VLAN) that don't support (or aren't configured for) RPVST then the whole setup will drop back to standard 802.1d timers (...which would account for long convergence times). If the root bridge moves then you'll hit the worst-case scenario. It's hard to say precisely where you should be setting the priority for the root switch without knowing about the rest of the topology, but generally it should be at a point that is stable and is the destination for most of the L2 traffic. In practice this is typically going to be the L3 default gateway lives. Wherever you choose to put it, set the priorities for the root and secondary root explicitly and unambiguously. If you -do- want to be configuring spanning tree priorities on a more fine-grained basis then you might want to take a look at configuring multi-instance spanning tree (MST) which will allow you to set up several topologies, each one or more VLAN's assigned. 

You have a series of VLAN's that the PVLAN feature is using - the primary, the auxiliary and possibly some number of community VLAN's. The basic mechanism of PVLAN is to map traffic received on isolated ports into the auxiliary VLAN. Traffic found in the aux VLAN is transmitted on the primary when sending to promiscuous hosts. Community VLAN's operate in much the same way. The interesting part about all of this is that the PVLAN behavior is actually locally implemented on the switch. As such, as long as the primary/aux/community VLAN numbers are consistent, making it work between switches basically consists of making sure all of the appropriate VLAN's are allowed across the trunk. The trunk doesn't need to be specially configured if it's running between switches. As to PVLAN traffic hitting a non PVLAN switch? If the VLAN's are just switched through to another PVLAN-aware device then it works. If not, then hosts in the primary VLAN will be able to communicate with the promiscuous devices on the PVLAN-aware switch and will be able to send traffic -to- isolated ports, but traffic back from an isolated device won't be delivered. 

HSRP by itself is going to be limited in terms of what it can achieve. It's nominally just a mechanism to allow a pair of routers to share a virtual MAC/IP in active/passive configuration. In the event that the active dies, the passive router takes over. By definition it's only handling outbound traffic. It's possible to layer other solutions onto HSRP - so, for example, link tracking can be used so that the primary router drops its priority and allow the secondary to take over in the event that an upstream link (or route) is lost while both routers are still alive. To get to something approaching an HA solution you'd need to have some kind of mechanism on both routers that was actively checking the availability of the path between the router. Getting BGP routes from the upstream is certainly a start but ultimately if you can't demonstrate that traffic is actually passing you really don't know if you have connectivity. This would get you into scripting on the router, mechanisms of validating connectivity, etc. There are mechanisms within IOS / XE to do this (PfR and such) to allow you to put it together. Or, alternately, take a look at what's available with iWAN - which puts it all together (and some other useful stuff) to run over any combination of link types, etc. 

The VPN head-end would need a route back to your Mac's IP (10.20.4.x) which the Centos box would need to send via the Mac's 192.168.141.x address - unless you have some kind of masquerading / NAT setup on the Centos side. Generally Cisco has good support on the native Mac side for VPN access, however - particularly the OS X native IPSEC client (at least since 10.6). If you're not trying for some specific lab setup I'd suggest simplifying the whole thing. 

I know this is late (putting it mildly), but this might be handy as a pointer for someone in the future. On almost any reasonably modern Cisco switch there is some version of private VLAN (PVLAN). The idea of PVLAN is to keep hosts within a given VLAN from being able to talk to one another unless explicitly allowed to. There are three types of ports in PVLAN: 1.) Promiscuous - A port configured as promiscuous can send and receive to any port in the VLAN. Your router's port would likely be promiscuous. 2.) Isolated - Can only send traffic to promiscuous ports. 3.) Community - Can send traffic to other ports in the same community and to promiscuous ports. In your scenario you'd have all of the hosts mentioned in the same VLAN. The externally managed boxes would be set up as isolated while the remainder would be set up in a common community. Your router/gateway would be a promiscuous port. The actual implementation of this is going to vary based on which switch platform you have in use, but the principles remain the same...