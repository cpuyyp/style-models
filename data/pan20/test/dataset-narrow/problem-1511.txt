I cannot recommend highly enough this online book on neural networks. The tricky part about neural networks is the stuff you mentioned -- what value to use for the learning rate, what topology to use for the network, etc etc -- we call these things hyperparameters to distinguish from parameters which are estimated by the optimization process. This big section of the aforementioned book covers exactly that. I don't know if you'll find what you want though. The reason is because there is no silver bullet. 

In R, does not come with a plot function, but code for that is provided here. Alternatively, you can use the more recent and IMHO better package called which features a function, so you can just do: 

You start by a dummy model $f$. Then you create a new model $g$ based on the errors of the existing ensemble $L(f(x),y)$. The code should be pretty straight-forward. Check the algorithm in the wikipedia page for a more formal presentation. 

Summary: Each is stochastic, data such as yours, which is small and comes from the same distribution, are bound to produce slightly different trees, even if the random forest itself is deterministic. You can fix this by passing the same seed to each which you can do using . also has a parameter which it passes along each . EDIT: a previous version of this answer said that was not passing along to each . This was wrong, and was removed. 

Anyway, I would avoid such hacks. Most models can be made somehow interpretable. I have seen people even making recurrent neural networks interpretable. And, if interpretability is a concern, just use a simpler model that is easier to interpret and explain. 

Feed-forward neural networks are discriminative models, i.e. they model $P(y|x)$. Something like restricted Boltzmann machines look a lot like neural networks, and share some of the proprieties, and are generative models, i.e. they model $P(y,x)$ --- which, of course, you can always turn into a $P(y|x)$ using Bayes' rule, but the opposite is not true. Libraries like Theano have support for RBMs. EDIT: since the writing of my reply, generative adversarial networks have been invented. (Actually, they were invented in 2014, but only in 2016 were they taken to their full potential.) In these networks, you feed noise to the neural network and it generates data samples. This is achieved by co-training it against a regular discriminative neural network. Both networks are trained at the same time: the discriminative one must classify each sample as being either true or synthetically created by the other neural network. And the generative neural network must fool this one, so they get better and better, and, in the end, you have both a discriminative and a generative model! 

filter words that appear in less than 3 documents if that's not enough or it is not a possibility for your use-case, use a dictionary 

It sounds like you have a lot of data, so probably a simple train-test split is enough. No need for cross validation. I would just use something like 75-25. That is in fact the default value in sklearn. I would use less data in the training only if your algorithm is too slow and cannot cope with the extra data. In that case, instead of throwing away the data, you might as well use it for validation or for testing hyperparameters. All this being said, more important than all is how you split the data. You should make sure that customers in the testing data are not in the training data to make sure your algorithm is generalizing, and not merely memorizing customers. This is standard procedure in medical data mining, and it is very important. Make sure you do not have customer overlap. You might also want to make sure that the distribution of the variable you want to predict is similar in the training and the testing data. 

It has been observed that averaging human predictions gives better predictions than any individual prediction. This is known as the wisdom of the crowd. Now, you could argue that it is because some people have different information, so you are effectively averaging information. But no, this is true even for tasks such as guessing the number of beans in a jar. I hypothesize it has to do with some of the reasons given above about data mining models. Some techniques such as the dropout method in neural networks (where in each iteration during training you use only a chunk of your neural network) give results similar to an ensemble of neural networks. The rationale is that you are effectively forcing nodes to do the same predictor work as the other nodes, effectively creating a meta-ensemble. I am saying this to make the point that we may be able to introduce some of the advantages of ensembles in traditional models. 

Similar to the previous one, except you train with and without the feature, and see the impact it has on the model accuracy. This has the added benefit that you don't have to think about hyperparameters such as how much noise you'll add to each feature like the previous approach. Furthermore, you can better understand the impact of several features in the output by trying with and without those. I haven't seen this approach being used, but apparently it's also being used, as another person replied. 

I thought of throwing some historical perspective into this... The initial algorithm is called AdaBoost. Unlike most of machine learning, this algorithm has its genesis in hard theory. The authors were trying to answer the following theoretical question: 

But all this depends very much on what you are doing. For instance, if you have performed clustering analysis and you know your data is made up of clusters, you could use the median within each cluster. Possibly other solutions could include things like multimodal or multiview models. These are recent techniques that can cope with missing modalities, and you can see a feature, or subset of features, as a modality. For instance, you could build a different classifier for various subsets of your features (using the complete cases in each of those subsets) and then build another classifier on top of that to merge those probabilities. I would only try these techniques if most of your data is missing. There are more advanced deep learning versions of this using autoencoders. 

Unfortunately, AFAIK no sklearn model supports categorical variables. For instance, sklearn decision trees only support rules such as , not which would be desirable here. Also, the decision tree algorithm they implement only produces local one-look-ahead optimization. What this means is that, it may not produce rules such as followed by , even if such rule would be highly desirable. In the end you will end up with non-sense things like: and then where 1 is Volkswagen and 6 is Ferrari. The typical workaround is to use one-hot encoding, which might be a pain in the ass in your case. And then, it might not: sklearn decision tree supports sparse matrices, so the memory penalty would be low. You can use scipy for this. (Sparse matrices stores data differently than regular matrices. Instead of requiring $n\times m$ size, the memory requirement is proportional to the number of non-zeros in your matrix.) In terms of speed, it should not be any different than if the algorithm natively supported categorical variables. This being said, your data may not allow for use of sparse, if the rest of the features are non-sparse. I don't think scipy has support for a sparse-dense hybrid matrix. Another workaround I can think of would be to produce an Euclidean distance matrix between observations of the various categories (you may want to normalize first). Then group categories that are close together. Then build a hierarchical model, where you predict the final category for each category. In python, this is easier than it seems. You can create a class for your model using sklearn base classes. I love Python and sklearn. But I believe in using the right tool for each job. I would use R which has native support for categorial variables (they call them ) and has a plethora of decision tree packages. (Note: xgboost for R does not support categorial variables, it ignores the factor class-type.) Weka could also be a good tool, which also has very powerful decision tree algorithms. 

Why am I using residuals and gradients as interchangeable words? You have discrete mathematics and continuous mathematics. In neural networks, we use the definition of gradient from continuous mathematics. You have a loss function that is $C_1$ continuous (meaning that the loss must be differentiable at least once). $$f'(x) = \lim_{h\to0} \frac{f(x+h)-f(x)}{h}$$ In gradient boosting, we use the definition of gradient from discrete mathematics. It's usually called fininite differences, instead of derivative or gradient. $$f'(x) = f(x+1)-f(x)$$ Personally, I think gradient is a misnomer. But it's marketing. Gradient boosting sounds more mathematical and sophisticated than "differences boosting" or "residuals boosting". By the way, the term boosting already existed when gradient boosting was invented. It was used for AdaBoost, which changes the weights of the observations based on the loss, rather than training in the residuals. AdaBoost only works for weak models (each model must commit errors), while gradient boosting can do resample and work for strong models. 

Have you considered using a quantile forecast as your ARIMA? Quantile forecast is when, instead of predicting the average, you predict quantiles. In your case, you could predict for a given day the probability that an observation is below the 0.5%-percentile and the 99.5%-percentile. That range would define the 1% of "abnormal" values for the next days. Recapitulating, usually in regression you find the average value by minimizing $\min_\beta \sum_i||X_i\cdot\beta -y_i||^2$. You could find the median value by minimizing $\min_\beta \sum_i|X_i\cdot\beta -y_i|$. You can also find any given quantile $\tau$ by minimizing $\min_\beta \sum_{i|y_i\geq X_i\cdot\beta}\tau|X_i\cdot\beta-y_i| + \sum_{i|y_i< X_i\cdot\beta}(1-\tau)|X_i\cdot\beta-y_i|$. Of course, these minimizations problems become more and more complicated. You can only estimate your $\beta$ for this last one using ADMM or gradient descent. I don't think you'll find python tools for this. You'll find plenty of packages in R though. For instance, see this page: 

SVM solves an optimization problem of quadratic order. I do not have anything to add that has not been said here. I just want to post a link the sklearn page about SVC which clarifies what is going on: 

What happens if you do not normalize your data is that your data is multiplied by the random weights and you get things like $s'(9999)=0$. The derivative of the sigmoid is (approximately) zero and the training process does not move along. The neural network that you end up with is just a neural network with random weights (there is no training). Does this help us to know what the best normalization function is? But of course! First of all, it is crucial to use a normalization that centers your data because most implementation initialize bias at zero. I would normalize between -0.5 and 0.5, $\frac{X-\min{X}}{\max{X}-\min{X}}-0.5$. But standard score is also good. The actual normalization is not very crucial because it only influences the initial iterations of the optimization process. As long as it is centered and most of your data is below 1, then it might mean you have to use slightly less or more iterations to get the same result. But the result will be the same, as long as you avoid the saturation problem I mentioned. There is something not here discussed which is regularization. If you use regularization in your objective function, the way you normalize your data will affect the resulting model. I'm assuming your are already familiar with this. If you know that one variable is more prone to cause overfitting, your normalization of the data should take this into account. This is of course completely independent of neural networks being used. 

There is a data mining competition website called numer.ai. Presumably, behind the website is a hedge fund which makes use of the predictions that people send. People within the 100th top places continuously make money, until the next dataset is revealed, and the competition resets. What I don't understand is that websites like Kaggle avoid overfitting by having a public and a private leaderboard. The private leaderboard is only revealed at the end of the competition and only then are prizes handed out. Numerai says under rules that it uses the same approach. Quoting: 

Unsupervised feature selection are things like clustering or PCA where you select the least redundant range of features (or create features with little redundancy). Supervised feature selection are things like Lasso where you select the features with most predictive power. I personally usually prefer what I call supervised feature selection. So, when using a linear regression, I would select features based on Lasso. Similar methods exist to induce sparseness in neural networks. But indeed, I don't see how I would go about doing that in a method using kernels, so you are probably better off using what I call unsupervised feature selection. EDIT: you also asked about regularization. I see regularization as helping mostly because we work with finite samples and so the training and testing distribution will always differ somewhat, and you want your model to not overfit. I am not sure it removes the need to avoid selecting features (if you indeed have too many). I think that selecting features (or creating a smaller subset of them) helps by making the features you do have more robust and avoid the model to learn from spurious correlations. So, regularization does help, but not sure that it is a complete alternative. But I haven't thought thoroughly enough about this. 

Important point on gradient boosting One important difference between gradient boosting (discrete optimization) and neural networks (continuous optimization) is that gradient boosting allows you to work with functions whose derivative is constant. In gradient boosting, you can use "weird" functions like MAE or the Pinball function. In my code, you can see that you can choose between MSE, MAE and quantile (which is the pinball loss). These latter two do not work very well for neural networks. The pinball loss is used for quantile predictions, like wind forecasts in energy systems. This is the reason why gradient boosting is much used in these systems. Small point: many people think that the reason why MAE and Pinball functions work bad for neural networks is because they are not continuous. While it is true they are not continuous when $y=\hat y$, it's very easy to work-around that. This is not the reason why results are so poor with those losses. The reason why they don't work well is because the optimization steps use the gradient (in gradient descent), and the gradient is constant for the entire function $L'(y,\hat y)=1$ or $L'(y,\hat y)=-1$, so the magnitude of the gradient is always 1. The optimization will depend solely on the learning rate heuristic that you are using, which usually will not work out very well. 

Both of these works are old and, from what I read, do not delve much into it (but I have not read them fully). I wonder if anyone knows if there are features for which only some kind of lookahead search would work, or whether regular the regular local search algorithm is good even for those cases, and whatever thoughts you have about local vs lookahead searches.