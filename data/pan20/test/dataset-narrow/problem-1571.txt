Since our cross-validation scheme was resistant to over-fitting, the training and test accuracy are close on the practice set, and our practice training accuracy is close to our test training accuracy. Now we can safely assume that accuracy on the unseen test set for the real data is about worse than training accuracy. However, imagine you a single validation set or no validation at all on the practice data. Now your model is more likely to overfit and you might see results like this: 

Contrary to what others are suggesting, trying to extract data on square footage and number of rooms from apartment ads is not a problem for machine learning - especially not when you don't have any training data. You will spend 10 times as long trying to build a machine learning system for this task than you would manually extracting the data yourself. If you really want an automated system, consider building some simple regex rules based on ads you have seen. There are only so many ways people describe these features, and you should be able to get most of them when they are present in the ad. You won't get 100% accuracy, but you're not going to do any better another way, in my opinion. You can start with some simple rules, and improve them as you look at more and more ads. In Python, for example: 

Is there a standard way to do this? I am most interested in neural networks but also open to advice relevant to any other type of model. Note that I also have other categorical and numeric features such as and that also need to be fed in to my model. 

This is not correct. Every connection between neurons has its own weight. In a fully connected network each neuron will be associated with many different weights. If there are inputs (i.e. neurons in the previous layer) to a layer with neurons in a fully connected network, that layer will have weights, not counting any bias term. You should be able to see this clearly in this diagram of a fully connected network from CS231n. Every edge you see represents a different trainable weight: 

Clearly, the optimization of your loss function led to it only predicting the most common words in English. The probability of other words are obviously small. I am no expert, but I would check three things. 1. How long does it take for the loss to converge? With the output you show it is likely it does not take a long time. 2. Maybe try to filter out so you have different words in the sentences. 3. Check if it can help to optimize another loss function which would have a bias towards more diverse sentences. I hope this can give you a few ideas at least. 

I have a similar dataset atm and have been doing some research, not yet finished. What needs to be kept in mind is the need for generalization given that there is such few samples and many dimensions. The linear regression model is not robust enough given the dataset, it does not handle a high number of dimensions, compared to samples, well. Even if you could tune in order to get a good result, you are still probably overfitting. Stepwise feature/model selectors such as Cp-statistics does not seem to do well with so many dimensions vs samples. Ridge, Lasso, and Elastic net seems to be the way to go for feature selection. These penalized linear regression models handles the high number of dimensions vs samples much better. The linear regression is penalized in order to generalize better and not overfit, creating a less complex solution. R: GLMNET - Penalized regression Depending on alpha, you may receive a different feature set, coefficient that are not zero. E.g Lasso is biased for not returning features that are very correlated with each other while ridge might return a set of variables/dimensions/predictors/features that are correlated with each other. K-Fold Cross-validation could be used in order to check the features selected are selected over and over again. This could be used in order to make sure that you don't overfit. I hope this gives you a few ideas. 

These are in comparison to a simpler model like a 1D conv net, for example. The first three items are because LSTMs have more parameters. 

Encode each as the number of times it appears in the training set Encode each as the mean of the target value in the training set. See "Target-based encoding" here. If the target is categorical you can use a vector of frequencies for each target category, eg `[0.4, 0.1, 0.3, 0.2]. 

The class probabilities are the normalized weighted average of indicators for the k-nearest classes, weighted by the inverse distance. For example: Say we have 6 classes, and the 5 nearest examples to our test input have class labels 'F', 'B', 'D', 'A', and 'B', with distances 2, 3, 4, 5, and 6, respectively. Then the unnormalized class probabilities can by computed by: 

What definition of BLEU is the Google Brain paper using? I could not find a separate definition in the paper itself. 

For multiclass classification where you want to assign one class from multiple possibilities you can use : 

Unfortunately none of these three variables can go directly into linear regression. looks like a numerical variable, but it is actually categorical. For example is probably orthogonal to , and should not be interpreted as twice as significant. The correct way to handle this is to create a boolean dummy indicator variable for each possible site code. You can also use this method for some of your other variables which also appear to be categorical. and will not work well in a linear regression because their relationship is highly non-linear. For example two points can have the same latitude/longitude but be very far apart. One typical method is to convert pairs into predefined zones, and treat the zone as a categorical variable. The final type of variable you have is a . You could convert this directly to a categorical variable, but it might be better to take only the month to reduce the number of categories and generalize seasonal effects better. 

Some ideas:1. Could you assign features for jobs and assignments? Performance would be the labels. Then you could simply use an SVM for example. 2. Given a user representation matrix X, each row i is an agent, and a job representation matrix Y, each j is a job representation, and a matrix of performance P, where every row stands for a user and every column how well the agent performed on that job, you could learn X and Y through optimization. Check this: $URL$ I hope this helps. 

With a higher learning rate, you take bigger steps towards the solution. However, when you are close, you might jump over the solution and then the next step, you jump over it again causing an oscillation around the solution. Now, if you lower the learning rate correctly, you will stop the oscillation and continue towards the solution once again. That is, until you start oscillating again. To keep in mind is that a larger learning rate can jump over smaller local minima and help you find a better minima, which it can't jump over. Also, it is generally the training error that becomes better, and the validation error becomes worse as you start to overfit on the training data. 

I just had another idea. Why don't you uniformly randomize the assignments. For each randomization of assignments, calculate the cost. Run this, for I don't know how many times. Pick the configuration with the least cost. Because of linearity of expectation, this should give you a good enough answer, might not be the optimal, but more efficient. 

Take a few words you know are linked with e.g republicans and with democrats. Extract their word embedding. That is the vector representation of the word which will be high dimensional. This will assume you have trained word embedding, either pretrained, or even better trained them yourself on your data. The word embeddings could have been trained/extracted through e.g LSTM training or matrix factorisations. On the word embeddings, do dimension reduction(PCA as an example) to 2d. Plot! Those who are close together will be "similar" for the objective they are trained for. Pretrained word embeddings exist to download. However, they might not be the optimal embeddings for your objective, but better than nothing still.