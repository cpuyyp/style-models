Imagine that the hyperparameter is a L2 penalty or a dropout rate -- something that we think that should have a single sweet spot -- too high and you're underfit and too low and you're overfit. I keep getting nonconvex plots like the one above when doing cross-validation. I guess this just points to a lot of noise during training -- I've got a lot of variables for a modest sample size, and I need to regularize heavily to get a good model. But still I'm a little bit unsure whether this sort of thing might point to a bug in some aspect of my implementation. Has anyone come across this sort of thing before? And did you just shrug off the nonconvexity and go with the model that minimized the prediction error? If so, that begs the question: why not just compute a prediction error at each update during training, saving any set of weights that minimizes prediction error -- even if the model is nowhere near converged. Basically letting the noise work in your favor. This seems appealing, because sometimes I get really low prediction errors early on, only to have them evaporate as the loss function declines. This seems horribly unprincipled, but I ask myself "why should I care if it is"? And "is it unprincipled anyway?" 

Take the following tensor: $$ \left[\begin{array}{cc} a & b & c\\ d & e & f\\ g & h & i\\ \end{array}\right] $$ $$ \left[\begin{array}{cc} j & k & l\\ o & n & m\\ p & q & r\\ \end{array}\right] $$ Where each matrix represents a channel. This could be reshaped fairly easily into a vector: $$ [a,d,g,b,e,h,c,f,i,j,o,p,k,n,q,l,m,r] $$ And then concatenated row-wise with other vectorized tensors to form a typical flat-file dataset of dimension $N \times P$, where $N$ is the number of training samples and $P$ is the product of the tensor dimensions. Rather than futzing with a convolutional layer, one could simply constrain ones weights to be zero in the subsequent layer. If $X$ is a flat $N\times P$ dataset of concatenated vectorized tensors, then the convolutional weights would form a sparse matrix, with the first two columns of a $P \times 4$ convolutional "layer" being $2\times 2\times 2$ filter being $$ \left[\begin{array}{c} w 0\\ w0\\ 00\\ ww\\ ww\\ 00\\ 0w\\0w\\00\\w0\\ w0\\00\\ww\\ww\\0w\\0w\\0w\\00 \end{array}\right] $$ This seems to me more intuitive than the tensor formulation, and could be computed fairly simply using sparse matrix packages. Perhaps it is partly matter of taste. But I'm curious: is there anything special about the tensor paradigm -- either mathematically or computationally -- that is superior to the flattened representation? I understand that computers convert matrix algebra to for-loops "under the hood", but doesn't the advent of the GPU make such explicit looping irrelevant? 

Not sure if this question is OT here. If it is, perhaps move it to meta? If not: I'm trained in classical statistics, work in research, and have learned much (and taught) machine learning. I've got a doctorate, but not in statistics or computer science. I'm not a software engineer and am only really fluent in R. Maybe I know a bit of C++ and can write a SQL query, but I wouldn't be the guy you'd hire to set up a data pipeline. There are plenty of engineers who know maybe a thing or two about statistics and machine learning, but who can build apps in Java, manage a spark cluster, set up a distributed database, etc. But they wouldn't be the person you'd hire to do any sort of analysis that isn't already canned in python. Browsing jobs in the private sector, I see a lot of demand for the second type of data scientist. Is there much demand for the first type? What is the pay differential in the private sector between the two types? Did you start as the first type and become more like the second type "on the job?" 

I'm not sure how to correct this behavior, and I'd appreciate suggestions. It would seem like the adaptive stepsize algorithm would need to know when it is going to hit a wall in a given direction, before it hits that wall. Is that possible? Would something like adagrad improve over RMSprop? Alternatively, I haven't yet implemented batch normalization -- not sure how that would help, but, would it for some reason? Edit: here's an update, a few hundred iterations later. Same behavior. 

Not significantly better. It looks like we'll have to change the architecture of the network (more layers, more neurons, and/or different activation functions) to improve beyond this point. To inform this architecture change, let's take a look at the sigmoid activation function: 

This should allow you to use all cores of all CPUs. This can, of course, also be done in Tensorflow: 

No- not if your regex is actually generating the data. If this is the case, the regex (or a complementary one) should be able to classify the resulting text perfectly. That is, you know what the rules for creating a 'happy' or 'sad' piece of text are within the regex, and so you simply apply those rules backwards to classify it. So, if you take what the regex says when it creates the data as ground truth, then there's no reason to create a machine learning classifier to classify it- the creation mechanism itself already does so. On the other hand, if you have created a regex that will produce "happy" or "sad" tweets, and you want to know if the regex itself or a machine learning algorithm will better conform to the opinions of, say, human observers, you would have a very interesting question to evaluate and a relatively straightforward test case for sentiment analysis. 

For this example specifically, I would suggest visualizing the data using a Chord Diagram. Chord diagram from Delimited.io: 

Generally, the fact that your training and validation performance are improving at the same rate is a good thing- this (usually) means that the algorithm is learning generalizable features of your problem space rather than overfitting to the noise of your training set. Reaching a plateau in performance is also to be expected- it's very rare that a real-world machine learning problem can be perfectly solved, and a perfectly solved problem would be the only type that didn't reach a plateau in testing and validation performance (before it eventually did reach a plateau at 100% accuracy). Think of the plateau as the maximum performance that can be achieved given the particular parameter values, features, and architecture of the solution. In order to achieve performance beyond your plateau values, one of these considerations will need to be adjusted. 

This is, of course, very interesting. After 1000 epochs, our networks is able to roughly approximate the downward curve from 1:0 ($\pi/2$: $\pi$) of the sine response, but not the initial upward curve 0:1 (0:$\pi/2$) or the region in which the function is negative ($\pi$:$2\pi$). This result begs the question- what will it look like after 10000 epochs? 

Generally, I would refer to this as transfer learning or network adaptation. That is, taking a network that has learned useful features from one domain and adapting that network and its developed features to another domain. That said, there appear to be many sources that closely conflate fine tuning with transfer learning. Therefore, I would say the difference in terminology is primarily opinion-based and suggest closure of this question on those grounds. 

Please note that this function does not necessarily define the approach you should take, but only one possible approach. To create a custom loss function perfectly tailored to your use case, you'd want to know the relative cost of a false positive and a false negative, and customize the function accordingly. 

Obeservation predictions falling into the green segment could be classified as positive examples, and predictions falling into the red segment would be unclassified. This approach would be sufficient for Naive Bayes. Some other approaches (SVM, gradient boosting machines etc) may benefit from a custom loss function definition, in which you define a function that disproportionately punishes false positive predictions. Something like: $(y_i = 0) \rightarrow (d = s)\wedge (y_i = 1) \rightarrow (d=1)$ $L(p) = \frac {\sum_i\frac{(p_i-y_i)^2}{d}}{n_i}$ $L(p)$ = loss function $p_i$ = predicted class likelihood $y_i$ = actual class (0 for negative example, 1 for positive) $n_i$ = number of observations $s$ = Penalty parameter for false positives. Would heavily punish false positives relative to false negatives. It can also be adjusted to your needs, to more or less heavily punish false positives. For $s = .5$, the function looks something like: 

Keep in mind that the Accuracy measure is measuring whether the values are Exactly The Same. I.e. it is a classification measure, whereas approximating a sin curve is much better suited for measurement as a regression problem. That said: In evaluating the network's performance, what is the network actually doing? Let's take the network and perform a little visual analysis on its performance: 

When fitting neural nets and getting close to the bottom, I consistently get a very distinct pattern in the loss function (and the mse). See below: 

What sort of models do they use? Presumably some flavor of neural net. Do they do a lot of feature engineering? Or do they throw in huge raw matrices of oscilloscope output? Given that it must be supervised learning, what is their target? A thumbs up? How do they deal with heterogeneity in the population? We're not independent nor are we identically distributed. It seems like it works better lately. Curious how it works. 

Self-driving cars, arms that can learn ow to pick up objects, machines that can have conversations, etc. I understand how neural nets work. But I really don't know much about how neural nets are combined to form intelligent systems. Is it all rule-based above a few base nets for tasks like e.g. image recognition? Or are there meta-nets that do deep learning on the outputs from base nets? Things to read/references would be super-helpful! 

I'm implementing batch normalization for the first time, and I've figured out a tentative solution for the gradients. Given the gradients, how should I do the updates? Does it make sense to use parameter-adaptive update rules like ADAM and RMSprop with batchnorm parameters? Their value is intuitive with regular layer weights, but less so with $\gamma$ and $\beta$. What is commonly done, and why? 

I like to start with an overfitting set of hyperparameters, train to convergence - usually zero training error - then start increasing the regularization until the test error starts declining. Time is saved by the fact that I never re-initialize, but jack up the regularization parameters in an already-fit network. My regularization parameters are dropout rate and weight decay. I usually leave one of them fixed and move along a grid of the other. When trying for a given set of hyperparameters, I usually start with large batches (faster) and then move to small batches (deeper). I also rely on ensembling, combining the prediction of 4 - 64 models trained analogously, maybe with different architectures Ymmv, perhaps. 

The final step in dropout regularization is to multiply the weights by the dropout probability. This is motivated by analogy to bagging: averaging the weights of multiple nets. But it isn't truly that because of Jensen's inequality. The effect of the averaging is basically just to shrink the weights by some factor. Why average at all? Has anyone tried simply not averaging, and controlling the magnitude of the parameters with e.g. L2 regularization? One would still get the benefits of preventing codependence, but one would need to regularize some other way. 

Interesting question. I haven't worked with this sort of data much, but it seems to me that the bulk of the job is likely to be feature engineering. Every "supervised" statistical method that I know of requires that you shoe-horn your data into "outcomes" and "covariates." $\mathbf{Y}$ and $\mathbf{X}$. Once your data is in this form you simply find an appropriate algorithm that can estimate $$ \mathbf{Y} = f(\mathbf{X}) $$ But taking data on how people play a video game and turning it into a 2-dimensional matrix isn't necessarily obvious. One of the most difficult things is that you'll need to make observations of different users comparable. Say you're playing quake 3 and you're trying to predict whether the user is going to go for the BFG. How do you define $\mathbf{Y}$? Is it ? Or should it be ? (maybe people messed up the rocket jump, for example). What if they got fragged on the way to the BFG? The point is that defining the outcome involves choices. Likewise what are the covariates? Say I get spawned in some location, and I know the map, and immediately move toward the BFG. I'll have a different set of , , , and keystrokes than someone who got spawned elsewhere on the map. So is it n-grams that you want to look at? Maybe rather it is change in euclidian distance between spawning location and BFG location over some time interval. if so, what is the appropriate time interval? In general, what you're doing is taking raw data and turning it into abstractions of that data. This is actually what a neural net does. It takes raw data $\mathbf{X}$ and forms a lower-dimensional representation of that data, $\mathbf{V}$, where $\mathbf{V}$ has fewer columns than $\mathbf{X}$. When neural nets work well, those columns of $\mathbf{V}$ can be uncannily similar to things that a human would pick out of a stream of raw data. Akin to this: $URL$ Those $\mathbf{V}$ are then just related to your outcome $\mathbf{Y}$ in a linear model or a logistic regression. But you don't turn raw keystrokes into , and without a lot of data and long training times. It'll go faster if you can prespecify functions of your raw data, which requires domain knowledge.