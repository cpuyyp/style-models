Your transform looks correct. To transform from world to eye coordinates, I I always use a "lookat" transform, defined by 3 vectors: $\bf{e}$, $\bf{a}$ and $\bf{u}$; in english, the eye position, the point it's looking at, and an up vector, which must not be in the same direction as $\bf{a} - \bf{e}$ (more specifically, not a multiple of it). The space is defined using $\bf{z} = {{(\bf{e} - \bf{a})}\over{|\bf{e} - \bf{a}}|}$, which means the negative z axis points in the direction of what I'm looking at (this works well for OpenGL); $\bf{x} = {{\bf{u} \times \bf{z}}\over{|\bf{u} \times \bf{z}|}}$ and $\bf{y} = \bf{z} \times \bf{x}$, which, again, for OpenGL, makes $\bf{x}$ rightward on the screen and $\bf{y}$ upward. To transform into eye coordinates is a matter of subtracting they eye's position, then projecting into the space defined by the vectors above: $$\begin{bmatrix} \bf{x}_x & \bf{x}_y & \bf{x}_z & 0 \\ \bf{y}_x & \bf{y}_y & \bf{y}_z & 0 \\ \bf{z}_x & \bf{z}_y & \bf{z}_z & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & 0 & 0 & -\bf{e}_x \\ 0 & 1 & 0 & -\bf{e}_y \\ 0 & 0 & 1 & -\bf{e}_z \\ 0 & 0 & 0 & 1 \end{bmatrix} $$ Which is exactly what you have to begin with. 

So I been thinking about this for a while and tried to google for an answer but without any success. If all your textures are 8bit LDR images, like JPEGs, couldn't that potentially cause conflicts with exposure control/tone mapping when rendering. That is if you adjust the rendering exposure of your image that should expose detail in the textures that aren't really there, since they been clamped out by the low dynamic range. So wouldn't it make sense to also have the textures as HDR images, saved as .exr, in linear colour space with 16bit half-float to get a good colour representation (32bit "full" float might be overkill?). To have more detailed and correct colour values might also, I figure, have an effect on GI and how colour bleed is calculated? Or is it simply not necessary since the end result of the rendering we want is probably going to be similar to the exposure level of the texture when it was photographed any way? And since camera mostly shoot in 12-14bit you would have to take multiple exposures of the texture and do all that extra work to piece them all together to one HDRI. Edit: To clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines. 

Assuming you are not going to filter the buffer using HW texture filtering: since you only need 4 values for the material index, pack it in the sign bits of uv x and y. 

GPUs can efficiently scale an image by an arbitrary amount (within limits - display options fall within those limits by design) either using a 3D rendering operation or as the signal is sent from GPU to the display. Both of these paths have fully dedicated hardware for the arbitrary resizing and are not likely to be optimized for doubling or halving. Both support a technique called "bilinear filtering" though advances in display hardware may provide higher quality. If a game is run at non native resolution on a laptop (TVs and desktop displays have their own scalers, but laptop displays generally rely on the GPU), one of these two methods will be used. Display scaling is essentially "free", so it is preferred and the lower the resolution the game is originally rendered the faster the frame rate (or the lower power consumption) - with no special performance benefit for, say, doubling. If a 3D operation is required (say, due to a limitation or issue in the display scaler), the scale operation is not free, but is going to go up with the source and destination sizes smoothly, with no special casing at doubling/halving. The game may look far better at certain scaling factors, but that's up to the user to decide. Edit: The intuition behind the question is correct: if GPUs didn't have dedicated scaling hardware, doubling/halving could implemented more efficiently than arbitrary scaling, either in shader code or via a slightly simpler hardware design - though the 3D rendering operation is a requirement of graphics APIs and display scaling is a requirement from systems vendors. 

A Transform structure can now be allocated on the heap with 16 byte alignment, since our custom will now be invoked: 

No MakeFile or Visual Studio Solution file is provided. So you will need to do the setup yourself in the Visual Studio IDE. Note that you could just drag and drop the files (.h and .cc are known extensions to Visual Studio) in a new console application if you are not/less familiar with Visual Studio. The code itself is just plain C++ code. No special stuff. So it does not matter which platform or IDE was used during development. In fact the code is pretty self containing except for 

Here, the hit position in light view space coordinates is calculated as follows from the hit position in camera view space coordinates (shading space): 

For completeness (and in addition to Nathan Reed's answer), I explicitly add the inverse projection matrices for perspective and orthographic cameras. Perspective Camera $$\begin{align} \mathrm{T}_{\mathrm{view \rightarrow projection}} &= \begin{bmatrix} \mathrm{T}_{00} &0 &0 &0 \\ 0 &\mathrm{T}_{11} &0 &0 \\ 0 &0 &\mathrm{T}_{22} &1 \\ 0 &0 &\mathrm{T}_{32} &0 \end{bmatrix} \! , \\ \mathrm{T}_{\mathrm{projection \rightarrow view}} &=\begin{bmatrix} 1/\mathrm{T}_{00} &0 &0 &0 \\ 0 &1/\mathrm{T}_{11} &0 &0 \\ 0 &0 &0 &1/\mathrm{T}_{32} \\ 0 &0 &1 &-\mathrm{T}_{22}/\mathrm{T}_{32}\end{bmatrix} \! . \end{align}$$ 

Does this look like "typical" projective aliasing? Could dynamic rendering of the shadow map, using the view frustum clipped to the scene, possibly with cascading, address the issue? Edit: Adding a closeup of bilinear filtering, post shadow comparison, just to show what I get. Bad shadow acne shows up because of interior edges; I'm modeling with stacked virtual blocks and not doing a proper union operation. From my reading, implementing boolean operations on polyhedra is not trivial, but will also allow me to implement static shadow volumes as well as clean up for some 3D printing software. 

Instead of a screen plane in front of the eye, it describes a film plane, where the image is projected, to explicitly model camera optics. You don't need to compute the focal point in doing ray tracing - it's just a way to find the plane of focus for depth of field effects. For depth of field effects I use a standard perspective projection but jitter the position of the eye on a circle parallel to the focal plane - and I make sure all my rays for a given pixel go through the same spot on the focal plane making it sharp while stuff in front of or behind that plane ends up blurry. It's a simple model of something like an aperture, and gives pretty good results. 

So I'm trying to wrap my head around this from a fairly technical point of view. When you add a Bump or Normal or Displacement map in your shader they should not be gamma corrected. But diffuse textures should be. But why? When you save a file in 8bit (or 16bit Integer) with a format like JPEG, PNG or TIFF they get the gamma of 0.4545 (1/2.2) burnt in. So a camera captures the light from the real world (that is linear) and when saving the photo as a JPEG a gamma value of 1/2.2 is added, as a way to compress the information into 8bit, and then when you view the image on a monitor that adds a gamma of 2.2 making the luminance linear again. So when using a diffuse textures (like a photo) you need to a "remove" that 1/2.2 gamma to make it linear (called linearizing) so now the texture looks darker but these values are correct from a mathematical standpoint so the renderer will make the correct calculations. Side note: the software used a display gamma of 2.2 to make sure the textures looks correct it's just for the internal calculation that they are linearized. But when I create a Bump map in Photoshop and save that as a JPG or PNG then that 1/2.2 gamma is burnt into that image as well, so why doesn't this Bump map need to be linearized as well? If I load a Bump map in 3ds Max with a input gamma of 2.2 just like a would do with a Diffuse texture, the diffuse looks right but the bump looks wrong, so I have to set the input gamma to 1.0 for the bump in order to make it look right, but as I said: why, both image files have the 1/2.2 gamma burnt in, right? 

Open GL and other graphics APIs support floating point formats smaller than 32 bits (e.g. see $URL$ While GPUs seem to handle these formats natively, CPUs have limited support (x86 architecture chips can convert to/from 16 bit floats using the AVX instructions). It is sometimes convenient to pack/unpack these formats on the CPU , but it is not obvious how to do so efficiently. What are the fastest techniques to convert between 32 bit floating point and the small float formats on the CPU? Are libraries or sample implementations available? 

I've got bilinear filtering on in the texture (without it, I get serious shadow acne). Sadly, my attempt at PCF failed too - it looks just as ragged: 

My experience working with shader compiler stacks a few years back is that they are extremely aggressive, and I doubt you will see any perf difference, but I would suggest testing as much as you can. I would generally recommend providing the compiler (and human readers) with more information where the language allows it, marking parameters according to their usage. Treating an in-only parameter as in/out is more error prone for humans than compilers. Some detail: shaders run almost entirely using registers for variables (I worked with architectures that supported up to 256 32 bit registers) - spilling is hugely expensive. Physical registers are shared between shader invocations - think of this as threads in a HyperThread sense sharing a register pool - and if the shader can be complied to use fewer registers there's greater parallelism. The result is shader compilers work very hard to minimize the number of registers used - without spilling, of course. Thus inlining is common since it's desirable to optimize register allocation across the whole shader anyway. 

This is clearly wrong. All six shadow maps seem kind of stretched. Without the stretching, the curtain's shadow would still be in the close vicinity of the curtain itself and the leaves' shadow would be much smaller (equal to the leaves' shadow for the spotlight). Furthermore, the circular shadow at the end of the pillar's shadow is associated to the buckets in front of the curtains. Any ideas what goes or could go wrong? For clarity, the following two images show the shadow factor of the cube map face corresponding to the spotlight, for the omni light by adding: 

Even if I use a value equal to 100000.0, I'll notice lit areas? Depth Biasing I use (to prevent shadow acne; note that I use 16bit depth maps) for all my Rasterizer states. PCF filtering I use the following sampler comparison state for PCF filtering (my shadow maps have no mipmaps) for both spotlights and omni lights. 

But if your Visual Studio is installed and configured correctly, every console application can just include these headers without any problems. (Just try a basic console application, since the problem has in my opinion nothing to do with the code you want to use.) Note that you need a Win32 Console application and NOT a Win32 Project. It could be possible that the pre-compiled header file is enabled ( or ) in which case you could disable this or include the header as the first included header in every file.