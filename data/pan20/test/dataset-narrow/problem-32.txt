I don't think it matters much, which API you want to use when leaning to "program graphics". The more important things to learn are the typical concepts you use when working with 3D scenes. For example you want to learn how to write a simple (and later more sophisticated) Scene Graph. These concepts are much more important than the specific API method names you have to call. Compare it when learning how to write programs. If you are quite good at writing programs in e.g. Java, you won't have that much trouble learning C++, Objective-C, Swift, or some other object-oriented language. Because it's the concepts and the thinking which you learn. The choice between OpenGL, Direct3D and Metal is primarily the choice, which operating system you target. Direct3D is primarily available on Windows, Metal on OS X and iOS, and OpenGL is supported on most systems including OS X, iOS, Windows and Linux. The graphics card probably has little to no influence on this decision as I don't know a card that supports only one or two. If you have no dedicated graphics card, then rendering in real time will be a problem soon. Although a Intel HD Graphics and the AMD companion can already do quite much. So, choose your platform. 

The issue was that I declared as an array in my geometry shader. It should have only been declared as a (i.e. ) 

When is it a good idea to declare the image format in a shader? i.e. Wouldn't this shader work just as well if I did not specify the ? 

I am trying to pass vertex attributes from my vertex shader -> geometry shader and then to the fragment shader. Pretty simple program, here is my vertex shader: 

If I just comment out the last line in the fragment shader, and say it works fine. And I clearly am declaring gModelPosition as an input from the previous stage. Am I doing something wrong here? 

Originally mistmatching image formats ( instead of ) were used in the creation of textures and the call to . This error was hard to track down because the format (without alpha) only mattered (in this case) in the compute shader. Using everywhere fixed the issue. 

1 Having an angle of exactly 90 degree means that the line between $E$ and $P$ just touches the respective circle/sphere in point $P$ as a tangent. 

Given that I didn't miss anything, you can probably cut this down to a problem in the 2D space. Viewing onto the plane defined by the center points of the spheres and your camera origin, the scene looks like this: 

The spheres become circles with the center points $C_1$ and $C_2$, and the intersection circle is now only 2 points with only the closer one $P$ being interesting. The camera/eye is arbitrarily set to the point $E$. Calculating if one point on the spheres is visible or not is easy: Simply check whether or not the angles at point $P$ between $E$ and $C_1$ respectively $E$ and $C_2$ are both greater (or equal to) 90 degree1. If $P$ is visible, some part (e.g. at least that point) of the intersection circle is visible. Otherwise the whole intersection circle must be occluded by one of your spheres, namely the one which creates an angle of less than 90 degree. Here is how it looks if $P$ is not visible from $E$: 

I am reading the OpenGL SuperBible 7th edition which covers atomic operations on memory specifically within shader storage blocks. It says (about atomic operations): 

So, as I interpret it, if I had one large buffer (containing say, multiple meshes' data) and I was planning to call if I instead specify the minimum and maximum index values that this call will use, I could achieve a speedup. Is this a correct use case for ? It seems like I would almost always know the minimum and maximum index values (because I would know ahead of time what I am going to draw) so it seems like should be just a drop in replacement for . However, this answer implies that might actually be slower. So, what are the pros and cons of using glDrawRangeElements? 

I realize here that is typedef'd to void but I am wondering if there is any case where using the GL types could cause issues when it it is not explictly clear that the values are coming from OpenGL. Another example would be writing an image to a file (from ). If I am setting fields in the header which normally takes things in the form should I change that to a ? 

Transformations, and especially rotations, may have different visual effects, depending on the order they are applied to the target object. For example, using a rotation $M_R$ and a translation $M_T$ to the object's vertices $v_i$, $M_R \cdot M_T \cdot v_i$ will usually1 result in a different pose than $M_T \cdot M_R \cdot v_i$ does. 

You can clearly see how that point is occluded by the circle around $C_2$ and that the angle between $E$ and $C_2$ in $P$ is less than 90 degree. 

Rendering the scene usually involves more than one shader program which, in my case, all use the same attributes and share at least some of the uniforms. To have them working properly, I currently play safe, meaning I rebind the attributes and get the appropriate uniform locations every time I switch between shader programs. So basically multiple times in every frame, which is probably not the best approach. So, is it (in general) necessary to rebind attributes and uniforms after switching shader programs? And why? If so, is there a way to do this once at start of the program and never have to touch them again (except for setting the uniform values)? 

However, new, DSA OpenGL eschews the binding points. Is there a DSA way to bind a buffer to the transform feedback binding point? 

So I am a little confused: If I have an nVidia graphics card and install the nVidia drivers, do I need to install Mesa? What if I have the card but do not install nVidia's drivers? What, exactly, is meant by "implementation" in this context? 

If I just swap with (which is the input image ) in the call to then I see the image, so I know it is not a problem with the frag shader. (Here is is though anyway): 

The docs for list several optional flags that can be used when a buffer is mapped such as . The docs say: 

If I am writing my own function to check shader compile errors, or do anything that interacts with the OpenGL API but ultimately returns as a result of my application code, should I use the C++ or ? I.E. 

Note: When creating the above images, I simply drew two filled boxes with rounded corners. So first a filled red box and then a smaller filled blue box above it. This also avoids some pixels being left white due to rounding errors (no pun intended). 

As of now, when rendering my scene and while iterating through the scene graph, for each node its model matrix is calculated from the parent's model matrix and the scene node's pose. This is quite simple and already reduces the number of matrix multiplications to one multiplication per node and frame. But all these matrix multiplications have to be done on the CPU and in every frame to be able to perform (multiple consecutive) movements of scene nodes very fast. The GPU, however, is probably much better suited to perform lots of matrix multiplications, so I'm thinking about sending multiple partial model matrices to the vertex shader instead of computing everything on the CPU. Sending every single part (so the pose of every node) to the GPU probably does not make much sense, as in this case all the computations are done for every vertex instead of every node, which would actually decrease performance. But maybe scene nodes with lots of children or only non-moving children (relative to its parent) could be a place to split the model matrix and shift the multiplications to the shader. So, when is it better to send partial model matrices to the shader and move the multiplication to the GPU? Or is it simply a bad idea to to this? 

My GPU supports OpenGL 4.4 but looking at the OpenGL extensions viewer the only part of 4.5 that it does not support is the GLSL version. I am able to use all DSA functions without specifying any sort of Extension Prefix. Similarly, I have been able to run code that uses DSA on GPUs that only support OpenGL 4.2 How is this possible? I thought that, if my GPU does not support a version of OpenGL I have to use extension prefixes to use those features. 

Is there some reason why my compute shader is not being called? Even if I just write in the compute shader, the screen is all black. I do not know what I am missing. does not return anything. EDIT: I originally failed to include the code for the texture creation (the first two lines). This affects the qustion. Please see the answer below. 

However, in the older we use to specify a flag which would include things such as to specify expected usage patterns. Where do we include that with the newer OpenGL?