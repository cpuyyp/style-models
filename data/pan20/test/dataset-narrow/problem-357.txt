If all these check out you may want to create a fresh connection line to make sure you don't have a typo. 

Your belief that this should not be necessary is correct based on your assumptions. Normally this means the assumptions should be rechecked (unless there is data corruption or a bug). If you run the following you should get the same results from the first query as the second: 

My goal with this question was to determine how mkstore could be scripted. Here is the method I came up with using Powershell. Here are the requirements: 

BenV is right.+1  Since it sounds like you don't want your pool size to change, you should set the minsize the same as the maxsize. Here are some additional Oracle entries on DRCP: 

This statement does not make sense. Either you are selecting all entities in set 1 or you are not. If you are only selecting collection 1 2 3, then you are not selecting all entities in set 1. You can select all entities in set one that are also in collection 1 2 3, let us know if this is what you meant. It looks like you have different patterns of ways you would like to transform the data. In Oracle, if you give each pattern a function returning a pipelined result set, you can then pass the appropriate collection identifiers to the appropriate pattern function and have it return the requested results. So, for you first example you could call: 

It seems that something went wrong on the step. Since this is a new install, I would follow the instructions for uninstalling and then the instructions for installing. 

If you'd rather do more work on the Excel side, then you could create the following table in Oracle: 

Here are three methods of solving this in Oracle. The first is an Analytic SQL solution that only has to do one full table scan (assuming no indexes). The second is Nick Chammas's CTE solution converted to Oracle syntax and the third is a solution using a PL/SQL cursor FOR loop inside a Pipelined function. The solutions all produce the results expected. For these few rows the analytic solution performs best followed by the recursive CTE. Which is easiest to work with will be in the eye of the beholder. Analytic Solution 

In addition to the impdp/expdb option already mentioned, you could restore/recover the database from your backups to a new location and then enable flashback database to revert the restored database anytime you want. The advantages of this method are that it will make sure your backups are good, verify and improve your recovery procedures, and allow for faster reverting. The downside is that it will take longer when you need a fresh copy. Here is an overview of Flashback Database from the documentation. 

I agree with Phil+1 that this is not readily possible. Furthermore, it is very unclear how this could even work. Imagine the following simple table is being replicated. 

A cursor can be explicit or implicit, and either type can be used in a FOR loop. There are really two aspects to your question. 

The problem is that when is changed to the Index_Stats view does not get populated. Is there an online way to determine the benefit of compressing an index and/or the number of columns that will produce optimal compression? Update: $URL$ indicates that if Distinct_Keys from DBA_Indexes is "a lot smaller" than num_rows then the index is a good candidate for compression. This helps some, but isn't definitive and doesn't help determine the number of columns. He does give some guidelines for that, but nothing that can be determined programatically without a bunch of dynamic SQL. 

Alex Poole is correct, the call is in an SQL context which can only handle 4000 characters. You could solve this by calling the method in a block like this: 

A user has a table they would like to update. Depending on the data in other columns they want a particular column to be updated with an ascending value starting from 1 that is gap-less. The rows that will not contain this ascending value also need to be updated. Is there a way to do this with a single UPDATE statement? Create sample data: 

As far as the database is concerned, the most powerful user is the one connected as sysdba. As you have seen and by default, you can connect as sysdba locally using . While it is advisable to create another user for your work, a new user only granted the DBA role will have fewer privileges than . 

For Oracle Look at Change Data Capture or Workspace Manager's table versioning. They each have their pros and cons, but both limit the additional data that needs to be saved to only what changes. 

I have some XML schema definitions that have been registered using DBMS_XMLSCHEMA. I have created tables using the XMLTypes generated. I can create XML using XMLQuery and if I understand correctly, the query results can be inserted into the table if the definition is correct. Ultimately the XML needs to be written to a file, but the part I am unsure about is if writing XMLQueries from scratch is the best way to generate the XML. Is there any way to have the XML schema generate an XML stub for a guide or generate template XMLQuery? The data is currently all in regular Oracle tables, but arranged quite differently, so any information on easing the export to XML based on a specific set of registered schema would be useful. 

With Oracle's Advanced Queuing is there a way to dequeue messages in a LIFO (last in first out) order? There is an indication through a lack of information that this is not an option, but perhaps there is a way to do this such as queuing in a different order. 

Since the app knows the user, can you just have it do the search differently for HR Administrators? If you need to prevent access on the database side you could call a package that enables a role, but only call it when the user is an HR Administrator. 

The flip side of this coin is that you could define a cursor with only the rows you need, but this is leads to even more code that would be nice to avoid. 

You didn't indicate whether the year is part of the candidate key or not, but I'm not sure it matters, because in either case 2NF would be satisfied as far as the year is concerned. On a practical level it is a bad idea to separate the year for all the reasons you listed. 

To count the rows deleted just after the delete statement use SQL%ROWCOUNT. Here is a demonstration. 

If this returns 200 rows then you might not be looking at the correct rows. If it returns zero rows then the function is not changing the data. 

Since the WeekCoefficient can change every week for any different building, building unit, or category and you want the design to be flexible down to days, you should store ranges in a Rates table. 

We have multiple Oracle 11.2.0.2 databases with og4odbc database links to a SQL Server 2008 database. A particular table in the SQL Server database has two Varchar(36) columns. The following statement returns 1292 rows: 

Think about how what kind of queries and reports you want to run against your data. Lets say for example you want to know how many students were absent in the month of 2013-10. Here is how that would look with each table: 

Depending on what your requirements are about how it meets your criteria, an Oracle database would apply. If your buffer cache memory area is as large as the database itself then as the data is accessed it would be copied into memory eventually causing most reads to be done in memory. Writes would still go to disk. You could also consider a RamSan type device that allows memory to behave as a disk. They use disks for their persistent store, which would give you some of the same benefits and allow you to use any database you like. 

You are not doing anything wrong and your understanding is correct. Materialized views that are created or altered to have a refresh schedule use rather than . This would be unexpected given the following wording from the 11.2 Administrators Guide (emphasis mine): 

You might want to verify this in SQLPlus. If it still doesn't disconnect you then verify your assumptions by running the following after the trigger finishes: