Compute a constant degree spanning tree $T$ of your graph, root it, and now greedily find subtrees of roughly size $r$, extract them, and repeat. Naturally, if there is no constant degree spanning tree, then the star example shown above demonstrates that this algorithm can fail. 

This problem is solved in the following paper, where more general problems are mostly addressed: $URL$ . 

Well, you can apply the planar separator theorem together with dynamic programming and get running time $2^{O(\sqrt{n})}$, where $n$ is the number of vertices in the graph. The idea being that you try all possible assignments for the variable vertices on the separator, and all variables mentioned in clauses in the separator (assuming each clause has a constnat number of variables). If a clause node is large, then you have to be a bit more clever - you have to guess whether to assign it to the left side or right side subproblem. The details for such things tends to be messy and not immediate, so I am not going to give more details. I think the original papers by Lipton and Tarjan solved similar problems using similar ideas, if my memory serves me right. 

The eps-net theorem is an easy consequence of the eps-sample/approximation theorem. The easiest way to prove the eps-sample theorem is probably via discrepancy. In this case, you can bound the number of ranges using direct geometric argument, so you do not need the VC dimension argument at all... For a description of the eps-net theorem via discrepancy, see the book by Bernard Chazelle: $URL$ Or the book by Jirka Matousek: $URL$ It is also described in my book $URL$ (this is an early draft). 

Here is a simple trick that might be useful. Consider a random sample that picks every point with probability 1/k. It is easy to verify that with good probability exactly one of your k nearest neighbor would be in the sample. Compute the nearest-neighbor in the sample. Repeat this O( k log n) times. With high probability the k nearest points in the $O(k \log n)$ points computed are the k nearest neighbors to your query. Thus, finding the k nearest neighbor, is equivalent to doing $O( k \log n)$ nearest neighbor queries. In short, give me a fast data-structure for answering nearest neighbor queries, and I would be happy to give you a fast data-structure of k-nearest neighbor. 

Here I am explaining how to get $O(n *\mathrm{polylog} n)$ randomized running time. We need a sequence of observations: 

Well, there are cases where LP gives you no useful information. Consider a graph $G$ with $n$ vertices, and the problem of finding a maximum independent set in $G$. The LP gives you a solution of value at least $n/2$ (give every vertex a value of $1/2$). But the optimal independent set might be of size between $1$ and $n$. On the other hand, the greedy algorithm does give useful results in some cases. For example, if the number of edges is $m \ll n^2$, then the greedy algorithm would give you an independent set of size $\Omega(n^2/m)$ (always pick the vertex that has the smallest numbeer of neighbors). 

If you insist on precise partition, then you need to compute all the balanced partitions of a set of points in the plane by a line (the optimal partition is a Voronoi partition, so the two point sets are separated by a line). Such partitions are known as $k$-sets. The fastest algorithm currently known for this work in $O(n^{4/3} \log n)$ for computing these partitions in the dual [i.e., the $k$-level of a set of $n$ lines, for $k=n/2$]. Once you have all the possible partitions, you just need to check each one of them. Using standard tricks, this can be done in constant time for each partition. (Update: Proving that the optimal partition is realized by a $k$-set, for $k=n/2$, is not completely trivial. I would leave it as a cute exercise for the interested reader. Hint: Consider the line passing through the two optimal centers, and the direction perpendicular to it.) If you do not care about the exact solution, then an easier approach would be to use a coreset for $k$-means clustering. This would result in $O( \epsilon^{-2} \log n)$ weighted points in this case, with total weight $n$. Then, you just need to solve the problem on the weighted point-set. The easiest solution would be to generate then a set of candidate locations for centers, and trying all pairs on the weighted points. The coreset construction, and generating the candidate centers is described in this paper: $URL$ 

The intersection graph of interior disjoint balls in $\mathbb{R}^d$, should have treewidth $O(n^{1-1/d})$, if there is justice in the universe (let me think about it - yep - there is). The treewidth should be $\Theta(n^{1-1/d})$. This family of graphs is contained in the family of graphs of low density graphs in $\mathbb{R}^d$. I have a paper (with Kent Quanrud) on low density graphs: $URL$ Sorry for the shameless self promotion, etc. 

Yes. See for example the stronger concentration for the occupancy problem in the following note: $URL$ Theorems 10.3.1 and 10.3.2. (This is also covered in the Randomized Algorithm book by Motwani and Raghavan). The version stated here is the martingale version, but it is the same thing... 

P.S. I think by being more careful about the parameters, the first lower bound can be improved to $\Omega(1/\epsilon^2)$. 

Lattices and linear subspaces are closed under Minkowski sum. Thats more or less immediate from their definition. Lattices+linear subspaces are closed under Minkowski sum (i.e., a member of this set is for example a set of parallel lines in distance 1 from each other). Connected polygons with holes are closed under Minkowski sum. Rings [the set differences of two concentric disks] are closed under Minkowski sum (a disk is considered to be a ring, naturally). The set of line segments parallel to a certain direction are closed under Minkowski sum. Mushed potatoes are closed under Minkowski sum, but only if they are well cooked (or maybe not, it is too late)... Also, the family of finite union of concentric rings is closed under Minkowski sum. 

Get the book research problems in discrete geometry. Read through it, see which problems you find interesting, read the literature, solve, and publish. Warnning: The problems in this book are hard. However, it is an excellent introduction to open problems in the field, and a good way to learn about the field. 

I find the fact that network flow is polynomial time counter intuitive. It is seems much harder on first look than many NP-Hard problems. Or putting it differently, there are many results in CS where the running time to solve them is way better than what you would expect it to be. 

This is a set packing problem under the constraint that for the solution $\mathcal{A}$, for any subset $\mathcal{B} \subseteq \mathcal{A}$, we have that there is always an element in $\bigcup_{X \in \mathcal{B}} X$, which is covered exactly once. Proof: Given a solution to your problem, it immediately has this property. Indeed, if $E_1, \ldots, E_m$ is the optimal solution to your problem, then consider a subset $\mathcal{B}$ of these sets, and assume $E_i$ is the last set in this sequence appearing in $\mathcal{B}$. By the required property that the solution is incremental, it follows that $E_i$ covers an element that no prior set covers, which implies the above property. As for the other direction, it also easy. Start from the solution $\mathcal{A}$, find the element that is covered exactly once, set it as the last set in the sequence, remove this set, and repeat. QED. 

I think little is known about such questions because they are immediately in the NP-hard domain. As a concrete example, consider the problem of deciding given a set in the plane (with piecewise linear boundaries), what is the minimal number of triangles needed to cover it. However, after appropriate twiddling, this is minimum set cover of points in the plane by triangles, which is hard. And this is maybe the simplest variant of such a problem. Another way of thinking about this problem (which is hand-wavy and just intuition, but nevertheless...), is that you are trying to find the simplest description of a decision surface that separates some set of points from another. And that is, in general, a very hard problem... 

All of the other answers +... Arguably the most useful thing for you to try and do is engage in research. Following stackexchange, reading some background material/papers and figuring out what you might find interesting might be the most effective way to prepare yourself to grad school. 

Set cover by half-spaces. Given a set of points in the plane, and a set of halfplanes computing the minimum number of halfplanes covering the point sets can be solved in polynomial time in the plane. The problem however is NP hard in 3d (it is harder than finding a min cover by subset of disks of points in 2d). In 3d you are given a subset of halfspaces and points, and you are looking for min number of halfspaces covering the points. The polytime algorithm in 2d is described here: $URL$ 

First step let assume the graph has even number of vertices. In the second stage, we will extend the construction, so that if k is even, then we will show how to turn the graph into having odd number of vertices. The solution is a refinement of the idea suggested in the other answer. First part Claim: Given a $k$-regular graph $G$ with even number of vertices, one can compute a graph $H$ which is $(k+1)$-regular, and $H$ is Hamiltonian iff $G$ is Hamiltonian. Proof: Take two copies of the $k$-regular graph $G$, let call them $G_1$ and $G_2$. For a vertex $v \in V(G)$, let $v_1$ and $v_2$ be the corresponding copies. Create a clique with $k+2$ vertices for $v$. Pick two vertices $v'$ and $v''$ in this clique, and remove the edge between them. Next, connect $v_1$ to $v'$ and $v_2$ to $v''$. Let $C(v)$ denote this component for $v$. Repeat this for all the vertices of $G$, and let $H$ denote the resulting graph. Clearly, the graph $H$ is $k+1$ regular. We claim that $H$ is Hamiltonian if and only if $G$ is Hamiltonian. One direction is clear. Given a Hambiltonian cycle in $G$, we can translate it into a cycle in $H$. Indeed, whenever the cycle visits a vertex $v$, we interpret it as moving from $v_1$ to $v_2$ (or vice versa) while visiting all the vertices in $C(v)$. As such, this results in a Hamiltonian cycle in $H$. (Note, that this is where we are using the fact that the original number of vertices is even - if the cycle is odd this breaks down.) As for the other direction, consider a Hamiltonian cycle in $H$. It must be that $C(v)$ is visited by a portion of the cycle that starts in $v_1$, visits all the vertices of $C(v)$ and leaves from $v_2$ (or the symmetric option). Indeed, the Hamiltonian cycle can not enter and leave from the same $v_i$. As such, a Hamiltonian cycle in $H$ as a natural interpretation as a Hamiltonian cycle in $G$. QED. Second part As noted below by Tsuyoshi any 3-regular graph has even number of vertices. As such, the problem is hard for a $3$-regular graph with even number of vertices. Namely, the above reduction shows the problem is hard for any $k$-regular graph, although the resulting graph has an even number of vertices. We observe, that this implies that the following problem is NP-hard. Problem A: Deciding if a k-regular graph $G$ with even number of vertices has an Hamiltonian cycle going through a specific edge $e$. However, if $k$ is even then given an instance $(G, e)$ we can reduce it to desired problem. Indeed, we replace the edge $e$ by a clique of $k+1$ vertices, as before deleting one edge in the clique, and connecting its two endpoints to the endpoints of $e$, and removing $e$ from the graph. Clearly, for the new graph $H$: 

This is a variant of geometric set cover. Depending on the exact settings, you might be able to do some good approximation. The problem is of course NP-Hard. The natural huersitics are to use greedy algorithm (always pick the rectangle/strip that covers the most area not covered yet. The alternative technique is to use reweighing. There are some interesting theoretical results, but frankly, nothing that should be too useful in practice. One interesting hueristic you might want to try, is to first decompose your polygon into minimal number of convex shapes (using Keil dynamic programming algorithm), and then covering each convex polygon separately... 

My favorite is Rabin's linear time algorithm for computing the closest pair of points in the plane (or more precisely its simplification). It get across the importance of the computation model, the power of randomized algorithms, and some elegant way to think about randomized algorithms. This said, CS is still far from achieving the level of elegance one encounters in mathematics (well, they had 5000 years head start), from basic definitions/results in calculus, topology (fixed point theorems), combinatorics, geometry (Pythagorean theorem $URL$ etc. If you look for beauty, look for it everywhere... 

Samples provides you only with statistical guarantees. For a fixed $\epsilon$ whatever you compute would hold for everything "except" $(1-\epsilon)$ fraction (I am being very informal here). Thus, taking an $\epsilon$-net sample of a set of points in the plane, computing its smallest enclosing disk, results in a disk $D$ that contains $(1-\epsilon)$ fraction of the whole point set (or $(1-\epsilon)$ of the measure). Coresets on the other hand, provide geometric guarantees. For example, for a point set $P$ in the plane, an $\epsilon$-coreset for smallest enclosing disk is a subset $S$, such that any disk $D$ that contains $S$, if you slightly expand it (formally, scale it up by a factor of $(1+\epsilon)$, the it must contain all the points of $P$. The beauty of samples is that you can generate them without a priori knowing what you are going to use them for. Coresets on the other hand are application dependent - you need different algorithms to compute them depending on the function you are trying to capture. As for characterization - we have many positive results (see the survey here: $URL$ We also have counter examples which are somewhat mysterious - $URL$ (no coreset no cry [after working for months on trying to compute a coreset for this problem, I definitely wanted to cry]). The rule of thumb is that you are not going to have a coreset if: (A) The underlying function change dramatically if you can remove a single point, and there are many such points (say, closest pair, or k-center clustering, etc). Solving such problems in linear time requires different techniques. (B) Or putting (A) differently - the underlying function is not very stable if you move points around. It is unlikely to have a better characterization without assuming something significantly stronger on the underlying function that one is trying to compute. 

Somehow doing better than $O(n^d)$ looks hard. If the cell is significantly larger than its average expected size, one can use sampling, to find it. Formally, assume the bounded cells (in the plane) form a polygon of area $1$ (this polygon $Q$ can be computed in near linear time in the plane). Assume the largest bounded cell $C$ in the arrangement of lines has area $\alpha \gg 1/n^2$. Sample, $m = (\log n)/\alpha$ points from $Q$ - and let $P$ be the resulting point set. With high probability one of the points falls inside $C$, and computing all the faces in the arrangement containing points of $P$ takes $O(( n^{2/3} m^{2/3} + n + m)*\mathrm{polylog})$ time using standard magic (i.e., algorithms for computing many faces in the arrangement of lines). It is now straightforward to apply binary search on $\alpha$, to get an algorithm that computes the largest cell, in time $O\Bigl( \bigl(n + 1/\alpha + n^{2/3}/\alpha^{2/3}\bigr) \mathrm{polylog n}\Bigr)$, where $\alpha$ is the fraction of area of the largest cell out of the total area of the bounded cells (i.e., the area of $Q$). 

The solution is slightly more complicated because these rays open up. By adding the two rays on the sides, one guarentees that only the region of interest is really relevant. 

One can think about skip quadtree as a skip-list implementation of a data-structure storing the points according to their z-order. It is (arguably) at least conceptually simpler... See chapter 2 here: $URL$ . And yes, assuming you can perform some basic z-order operations in constant time, you can definitely do ANN in logarithmic time. The aforementioned chapter also shows that there is no way to avoid having bizarre operations if one wants compressed quadtrees. Note, that LCA operation is not necessary...