If this works, try to take a mysqldump and keep it safe side. And recreate a fresh installation with correct settings and remember to remove the entry innodb_force_recovery. If this doesn't work, increase the value and see if it works. Since I'm not sure about the history of this server. You may need to verify your innodb_log_file_size set correctly as per recommended values. 

Have you tried mysqldump using --single-transaction ? In this way it doesn't lock the table for threads doing SQL,DML operations provided no DDL statements should be issued and you will get the backups for conistent states only. This will be efficient only for innodb tables. Below url might give some insights about the option. $URL$ 

Why do you think you need to compare the data? Anyways it has to go to DB I guess. You can have a current_timestamp as one of the columns to DB and value can be derived when the Device generates its local data using device NTP. 

You may switch the table from innodb to memory, but ensure the size of the table isn't that huge so that it doesn't eat up entire of its memory. Yes, you may safe guard or recover the table during system crash. Have master/slave replication setup and once the table is created on master with memory as engine type and change the table engine type to innodb (on slave only). So that even at the instance of master crash/restart you will get the data from slave. 

I would like to understand the differences between innodb_autoinc_lock_mode options 0,1 and 2 when parallel load data infiles are given. I see in "0" option, it locks the entire table and does the First transaction requested for N number of records say TX1. So when next transaction say TX2 is raised in meantime when first transaction is still getting uploaded using "load data", then it has to wait in the queue for the first one TX1 to complete. And then it sets the max(AI_column)+1 value from the table and does upload for the next set of load data. In this case it doesn't jump the Auto Increment numbers. Also I see in "1" option, it locks the entire table and does the First transaction requested for N number of records say TX1. So when next transaction say TX2 is raised in meantime when first transaction is still getting uploaded using "load data", then it has to wait in the queue for the first one TX1 to complete. And then it sets the max(AI_column)+1 value from the table and does upload for the next set of load data. And then it sets the max(AI_column)+some_creepy_jump. But I see in "2" option, it doesn't lock the entire table. Instead it keeps inserts for each process simultaneously and inserting records for which ever request falls in randomly and ends all threads with average time around (1.21 sec for 7 parellel threads using load data having 1000000 record each). In this case it has multiple transactions in mixed order. And then it sets the max(AI_column)+some_creepy_jump atlast. I'm using mysql 5.1.61 . 

An approach I follow for normal mysqldump consistency Write a shell script in below mentioned flow and save it as "mysqlbackup.sh" 

Here are some of my inputs to stand "FOR OUTSIDE DB" It purely depends upon your need. If your business wants to keep them always available and recoverable with no down time then you might have to think a proper option to keep them available 24/7, but keeping them in Database the capacity management of DB server will be in question. If no priority for your need then I would say better to keep raw files in appserver, just the location of the file in DB server in terms of would help and adding index to it would benefit to get the right file location quicker even the number of rows are more. Imagine you have each field consisting of 1GB text and for 100 rows the size of the table would go more than 100GB. Sounds weird for me when I need to take care of everything like how DB performs when it comes to time taken for recovering the Database in-case of shutdown / backup / High Availability and over all performance of DB server. I haven't got an opportunity to design such a requirement. But if given 

Not sure if this is some default behaviour of Mariadb or lack of our knowledge to figure how Mariadb picks its my.cnf. I have worked to resolve similar issue and made mysqld_safe to point to the my.cnf that we want to pick it, in below fashion. 

First lets understand that to take an incremental backup you need to keep full backup on the server where you are taking incremental. Though you take a full backup on SERVER(A) you may push to AWSSERVER(B). It doesn't matter, but inorder to take an incremental you need to keep the full backup copy as source backup. UPDATE Below is the approach for incremental, you will be able to get your answers whether the idea you have is a feasible one or not. 

And then drains down within 5 minutes. And when I check the show processlist I see queries for DML and SQL are halted for some minutes. And it processes very slowly. Whereas each query are indexed appropriately and there will be no delay most of the time it returns less than 1 second for any query that are being executed to server the application. 

Firstly there is no need of DB side changes. From device application end you need to have a logic of retry to update in cloud once internet is back. Until it finds the connectivity, log the entries serially into local of that device. Once it finds the connectivity, re-spawn the logged entries from device and let it go as similar as cloud normal update. 

Lets say user_id is indexed. It should be effective according to the extended explain plan. Please verify this before you push the code. 

From Mysql 5.6 onwards you can explicitly mention the partition to be queried and it can go on further querying the data based index within the partition. I.E. 

I should group all phone numbers, and its total number of attempts and mention its no:of time answered and no:of time not answered. For this the condition is answer_time='0000-00-00 00:00:00' for not answered and answer_time > '0000-00-00 00:00:00' for answered. There can be any number of times the call has been made to a particular number. Any suggestions or approach would be helpful. I'm terribly stuck up on this. The result should be : 

I need to install two different versions of Mysql in one server OS RHEL6 and it will be separated by user called mysqlprod and mysqlpreprod. I need to separate their binaries, softwares,data directory and mysqldaemons. I assume tar ball installation is a best way to deviate the installations. But I don't find tar ball for Mysql 5.6.17 in $URL$ downloads. Its just rpms and I cannot do "rpm --basedir=" because it says rpm cannot be relocatable. Where can I find tar ball binaries that is provided in $URL$ ? or anyother way to achieve my above setup in Mysql 5.6.17 latest GA? Please advise. 

Alert if any query running beyond X secs or any breaching value to relevant teams. The team could see if slow query log / show processlist contains unusual entries, if confirmed as SQL injection kill the query and remove that application user as a preventive measure. 

I have a setup master/slave in which applications are pointed only to master. Yesterday Master had got crashed due to "multi bit error on dimm detected" in Front indication panel in orange color. After reboot I see the master went for recovery then it came up and application started to use the Master normally but still the err msg exist in Front indication panel. But now the slave had stuck out due to primary key constraint for a table. The problem I face is master has a table data until what exist in binlog of master. But slave has the table data what is not in master binlog. Below is the table details when compared to Master table and Slave table. 

Note: I have used SPLIT_STR function to exactly cut for the said occurence position by said delimiter value. In unix it is like -> Returns "Dick". Try it on your application requirement and see if this suits, I guess I dragged it long. I wish you to get someone with a decent fix. Use my fiddle -> $URL$ for quick testing purposes. All the best!! 

By this way you can collate details when your usage goes peak. And you may check when you are awake in the day time. 

This ensures your backup data consistency and can avoid situations invalid backup data at the time of urgent restore. 

In this, at the first attempt the call is answered at the second it is not. The attempts may range max to 16 times. FOR MAX 

crontab -l 00 3 * * * /usr/scripts/mysqlbackup.sh 2> /usr/scripts/log_mysqlbackup..err Monitor your nagios or netcool to point to the file /usr/scripts/log_mysqlbackup..err 

By this way DB performance will look good, quick recovery and data availability 24/7 and less time spent in-case of any maintenance activities. 

You have to take decision based on below two questions. a). Do you expect to get more records in each type (Bank,Applicaition..)? Ans). Go for separate tables. Lets say Bank might get 200000 records, application might get 50000 records. And if their rate of growth is expected to grow higher its better to keep them in seperate table names like bank_documents, application_documents etc. By this you might avoid issues when you alter the table or add index you can go table by table. Ofcourse it is back ache to DBA but no stress. b). If there is not much of records? Or not much bigger rate of growth? Ans). Then go for single table for all types and you may figure later as and when data grows you may prepare a road-map for application team to split types to different tables. 

Could there be any load spiking in cpu processors or IO, that which can slow down Master Perf? If any medium that I can trfr data that uses less cpu/IO utilization ? 

Can Elasticsearch automatically remove data? I'm using ES 2.3. We have 26nodes in a cluster with three Master nodes and rest are data nodes. There will huge writes happening through out the day. We see in one of the nodes the OLD GC was going for a very long time more than 19Hrs and after it is completed. Applications started to timeout. I see the data from that node has gradually reduced from 1.4T to 8GB. Immediately I stopped the ES in that node. I have two questions. 

I have been facing problem with my database server quite a month, Below are the observations that I see when it hits the top. 

I have got into slave lag gradually say every minute it lags around 5secs on slave. So it started morning 10AM the Seconds_behind_master was 13. Now at 4:30 PM it has reached to 560. No variable changed. Not much selects are running in there, its all regular ones. In fact we have stopped few applications thinking those could cause slowness due to selects. This seems to be strange for me. This had never happened before. I have below variables already set to minimize IO load on disk.