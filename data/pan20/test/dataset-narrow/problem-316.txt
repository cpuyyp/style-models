You need a tally table or calendar table that includes all years (that you want to report on). Call the table Allyears or something like that. As a field, you could put something like yearstart datetime and populate it with January 1 of each year (consider yearend as well). Then simply join the Allyears table to TestDates and display the years you want. Note, you have to decide if they were a customer on January 1, do you count them for the year? Or did they have to be a customer on Dec 31st to be counted. That will determine how you should create the table and join it. Apparently, I'm the only one who has trouble posting Sql code, the site or my company's security doesn't allow the edits when they contain code (sometimes). 

I think source1 has the answer. He (Aaron) states that even when you get an "accurate" count, you could be blocking users trying to write to the table. As soon as you put your eyes on the result, it could be immediately inaccurate as writers were trying to put data in your table while you blocked them for the accurate count. So, I believe using partition_stats works fine for what you want. I wouldn't be afraid to test it though and it might be an interesting test at that. You could create a db of 1000 empty tables. Then a routine that constantly inserts and deletes data in random tables. Then run your partition query and run your count query and compare. I think what might work best is to use partitions and accumulate your data in a table and over periods of days or weeks, any table with consistently zero results is unused or abandoned. 

The general approach of the SQL standard is that many things not imposed by chosen semantics are left to the vendor to optimize or specify. Under the chosen policy users choose names when they care what they are. SELECT without FROM isn't standard SQL either. MySQL chooses to use the expression that evaluated to a column's value as its name. Those PostgreSQL "?column?"s aren't the column names, they are strings output instead of the column names. If the system defaults to internal unique names then clashes are avoided compared to defaulting to patterned names. In your example you could have selected "column1" but with your expected behaviour you'd need "t2.column1". Ultimately, they just chose a certain design for the language. And even where people leave a rationale, engineering and ergonomics are about pragmatic timely tradeoffs where one decision cannot necessarily be justified as "best". 

I don't know PHP or MySql but the answer is definitely not creating new tables for each room. I'd have one table for rooms and one for messages and have the messages refer to the rooms (via foreign key). You probably want to consider archiving and/or purging as the data will probably grow quickly. Look into MySql's ability to partition tables and if there is an ability to SWITCH (this is possible in Sql Server) data in a partition from one table to another VERY quickly - it's just a meta data change. This could move Gbs in seconds vs having to physically copy and delete data from one table to the other. 

I'm no huge fan of Microsoft Access, but I'd be curious to know why you were unsuccessful at it. It should easily be able to hold that information and is probably easier to learn than other RDBMS systems. Having said that, I think either MySql or Sql Server Express would work great. You would use a text import process (Sql Server has a wizard that makes it easy), bring all the data into one table and then dedup and organize anyway you see fit. 1 million rows with 10 columns wont be a very large table, my guess is it will be somewhere near 500Mb. 

There should be a table for every box, ie entity type/class, and every diamond, ie relationship type/class. There should be a column for every oval, ie attribute/property. Each line from a diamond to a box is indicates a foreign key. Relationship PKs are composite and consist of FK columns to associated/participant/referenced entity types. 

(As I commented there, "The four boldface sentences for FD, holds, superkey and CK would have sufficed.") (A table with an empty CK is constrained to contain at most one row. A column set determined by the empty set is constrained to have the same subrow value in every row.) 

That asks for tuples P where there is an offer S1 where [P's sid is S1's sid and for all Items S1's iid is that item's iid]. I hope you can see that this involves a single iid being the same as all the ones in Item, not what you want. Your answer 1 

There is an excellent blog post $URL$ that explains what's happening. SQL Server allows for a set number of compilations based on their complexity. It groups them into small, medium, and large. For large compilations, there can be onlyi one compiled at a time, so let's say all of your procs are considered large, then each one has to be compiled serially. That could account for the blocking. I think there may be several approaches to the problem - consider more resources (more CPUs will allow more small and medium queries to be concurrent or may up the threshold for what is considered medium). Also, more memory may solve the problem. If you're like most of us, that might not be possible. Another option might be to review the ADO calls and see if the number of calls can be reduced or spread out so that not all calls happen at the same time. Reducing the number at any given time should reduce your wait time. If that doesn't work, consider fixing the 'compilability' of the stored procs. Maybe break them down into smaller chunks which might reduce them to the small or medium buckets and allow more parallel compilations. Or determine why the procs need to be recompiled each time. See if they can be rewritten such that they don't need to be recompiled. Finally, I'd consider using Plan Guides. These will allow the procs to be precompiled and may save some time. Hope that helps 

A FD is a MVD "in disguise". If an FD holds then a certain MVD also holds. With MVDs 1 is a special case of multi(ple). This can be seen from the rules of inference for MVDs at the definition link: RA → B implies A ↠ B. Hint: t1 & t2 can be t3 & t4. Hint: In the example there are only two tuples, so t1 & t2 must be those, and you know the MVD so you know A & B, and there are only two tuples, so if t3 & t4 exist they must be those. So fill in the equations and see whether given those two tuples (t1 & t2) there are also two tuples with the properties needed (t3 & t4). 

That has a "for-all for which there-exists", just like my solution. But your proposed answer 2 has a "there-exists for which for-all": 

You can look in the system dmv for os performance counters. With many of the counters, you have to collect the value, wait some time, then collect it again to get the number of transactions that have happened during that time frame. Something like this will work: 

You feel like they should be separated because that makes perfect sense in the relational world. But as you said in the NoSql world and in MongoDB in particular, you want to group like-items together. I've not done extensive research on Mongo, but have spent some time with the online classes and I believe the answer is to store them together. You probably realize there is no such thing as a join in Mongo and therefore if you wanted to get 100 rows and get their corresponding images, you'd have to get the IDs for the 100 rows and then get 100 rows by their identifier (object_id or whatever). In other words, you have to do manual joins. 

Relational calculus is a notation for expressing relation values in terms of given named relations and sometimes constants. So is relational algebra. A constraint is an expression making a statement about one or more relation values. But such a statement is not relation-valued, so you can't express it in relation-valued notation alone. (So you need to ask what exactly is wanted.) You have to have operators returning equality of relations, or whether a relation is empty, etc. (Either of those two is sufficient.) 

If you do want to involve the previous Payment meaning in a query then you use JOIN to get the table whose meaning is the AND of the simpler meanings. PS You don't need to declare a FK because it will be enforced by the combination of FKs in Payment and Enrollment. 

You must decide exactly what these mean and what situations can arise. Together those determines the constraints (PKs, FKs etc). You must give the meanings to users. In the case of payments, purchases, orders, prices etc finalized transactions are usually recorded with the data from that time, even if a customer ceases to be a customer or prices changed. Does this matter to you? Is there is a FK ? I gave Payment a meaning that would lead to that FK. The integrity problem is that your design allows Enrollment and Payment to disagree about EnrollID and StudentID even when the DBMS is enforcing the constraints. That can never arise, and the database would describe an impossible situation, so it's an error, but the DBMS doesn't know that, so it can't reject it. That problem is gone with FK . Then the DBMS catches those errors. But then of you update one of those tables then you have to update the other. That is called an update anomaly. (This one doesn't lead to errors when you have the 2-column FK; it's just inconvenient.) You can see that the table meanings are simpler when you just have 

You are correct, the only way to "rearrange" the order of the columns is to create a table with the new structure and push the old data into it, drop the old table and then rename the new table (or some variation of that). It requires copying all the data in the table and some drops and renames. 

The stored procedure sp_addsubscription allows for you to specify that you want all articles (i.e. tables) or just one. If you want a subset of articles in a publication, you just need to run the sp_addsubscription once for each article that you want. It looks like the sp_addsubscription defaults to @article = 'all' My suggestion is to go through the New Subscription Wizard and have it generate the script only. Then you can go in and modify the script as you need it. You would need to add multiple sp_addsubscription lines. 

Natural join "automatically" joins on equality of common columns, but you should only write that if that's what you want based on table meanings and your desied result. There's no "automatically" knowing how two tables "should" be joined or in any other way any table "should" appear in a query. We do not need to know constraints to query. Their presence just means the inputs may be limited and, consequently, the output may be too. You could define some kind of join_on_fk_to_pk operator that "automatically" joins per declared constraints; but if you want the meaning of the query to stay the same if only constraints change but not table meanings then you'd have to change that query to not use the new declared constaints. Just giving the query you want using join & conditions already leaves the meaning the same despite any constraint changes. What constraints hold (including PKs, FKs, UNIQUE & CHECK) don't affect what tables mean. Of course, if the table meanings change then the contraints might change. But if the constraints change it doesn't mean that queries should change. One does not need to know constraints to query. Knowing about constraints means we can use further expressions that without the constraint holding wouldn't return the same answer. Eg expecting via UNIQUE that a table has one row, so we can use it as a scalar. These queries can break if the constraint was assumed but not declared. But declaring a constraint that the query didn't assume cannot break it. Is there any rule of thumb to construct SQL query from a human-readable description?