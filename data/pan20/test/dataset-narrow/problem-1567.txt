You still need to know $s$ is A1, because the representation of A1 (and whichever action is taken) will be the input to your network. 

A RNN-based generative adversarial network (GAN) might also be able to achieve what you want. However, please note that GANs are notoriously fiddly to train. A GAN is actually 2 networks. You create a discriminator and a generator and train them in parallel. The generator takes a small completely random vector (e.g. 10 numbers sampled from N(0,1)), plus the rating you want to achieve. Then it generates a text sequence output. The discriminator would take a text input, and a rating, and outputs 1 if it is real, or 0 if it is fake. You present either real training data or output form the generator to the discriminator, and use these to train it. You train the generator based on whether it fools the discriminator. The tricky part is to maintain balance between the two components - if either becomes too good relative to the other, training will stall. However, if you can get it to work, you will have a true generative model, which samples from a population space (the noise vector that you input) plus is conditioned on the rating. The most relevant Keras example for this I could find is mnist_acgan.py which generates an image, not text sequence, but hopefully should give a start. 

Whilst it is still possible to estimate the value of a state/action pair in a continuous action space, this does not help you choose an action. Consider how you might implement an $\epsilon$-greedy policy using action value approximation: It would require performing an optimisation over the action space for each and every action choice, in order to find the estimated optimal action. This is possible, but likely to be very slow/inefficient (also there is a risk of finding local maximum). Working directly with policies that emit probability distributions can avoid this problem, provided those distributions are easy to sample from. Hence you will often see things like policies that control parameters of the Normal distribution or similar, because it is known how to easily sample from those distributions. 

The sigmoid range $(0,1)$ is technically open, because no input value maps to $0$ or $1$. You can get arbitrarily close to $0$ or $1$ but never equal them. Taking an exponent followed by a log can cause overflow issues during computation, because exponents grow quickly. Sigmoid also gets close to zero quickly and this can underflow (potentially rounding to 0). However, the end result of the combined function is likely to be something within normal calculation range, because of the log (this is similar to - but even more extreme - multiplying and dividing by a very large number). 

It is possible to get a perceptron to predict the correct output values by crafting features as follows: 

In summary your three big errors were the wrong dimension for the bias vectors in setup, incorrect derivatives for sigmoid function (using correct form, but with wrong variable) and forgetting to use bias at all when predicting at the end. Other details are still worth noting, but would not have prevented you getting something working. 

Because the old output layer has a simple form, this is quite easy to achieve. Each output neuron should have a positive weight between itself and output neurons which should be on to represent it, and a negative weight between itself and output neurons that should be off. The values should combine to be large enough to cleanly switch on or off, so I would use largish weights, such as +10 and -10. If you have sigmoid activations here, the bias is not that relevant. You just want to simply saturate each neuron towards on or off. The question has allowed you to assume very clear signals in the old output layer. So taking example of representing a 3 and using zero-indexing for the neurons in the order I am showing them (these options are not set in the question), I might have weights going from activation of old output $i=3$, $A_3^{Old}$ to logit of new outputs $Z_j^{New}$, where $Z_j^{New} = \Sigma_{i=0}^{i=9} W_{ij} * A_i^{Old}$ as follows: $$W_{3,0} = -10$$ $$W_{3,1} = -10$$ $$W_{3,2} = +10$$ $$W_{3,3} = +10$$ This should clearly produce close to output when only the old output layer's neuron representing a "3" is active. In the question, you can assume 0.99 activation of one neuron and <0.01 for competing ones in the old layer. So, if you use the same magnitude of weights throughout, then relatively small values coming from +-0.1 (0.01 * 10) from the other old layer activation values will not seriously affect the +-9.9 value, and the outputs in the new layer will be saturated at very close to either 0 or 1. 

You do not need to provide values before training, although you may want to decide things such as how many parameters there should be (in neural networks that is controlled by the size of each layer). TensorFlow calculates the values automatically, during training. When you have an already-trained model and want to re-use it, then you will want to set the values directly e.g. by loading them from file. The specific code that handles changes to weights and biases from the tutorial is this: 

It seems standard in many neural network packages to pair up the objective function to be minimised with the activation function in the output layer. For instance, for a linear output layer used for regression it is standard (and often only choice) to have a squared error objective function. Another usual pairing is logistic output and log loss (or cross-entropy). And yet another is softmax and multi log loss. Using notation, $z$ for pre-activation value (sum of weights times activations from previous layer), $a$ for activation, $y$ for ground truth used for training, $i$ for index of output neuron. 

My understanding of the question is that you are using a basic MLP, feed-forward neural network, to work with data that is a 3-dimensional vector which you are calling $(x,y,z)$ - although note that most notation you read would call the whole observation $x$ which would be a vector $(x_1, x_2, x_3)$. You want to accept an input of 1st, 2nd, 3rd observations of this data (a window on some possibly longer series), and predict the 4th observation. For the given architecture, the prediction is made immediately on the supplied input - a MLP has no "memory" for previous inputs - so you will need 3 x 3 = 9 inputs to represent each (x,y,z) in the series. For a MLP it doesn't really matter how you arrange this input - e.g. you could have $(x^{(1)}, y^{(1)}, z^{(1)},x^{(2)}, y^{(2)}, z^{(2)},x^{(3)}, y^{(3)}, z^{(3)})$ or $(x^{(1)}, x^{(2)}, x^{(3)},y^{(1)}, y^{(2)}, y^{(3)},z^{(1)}, z^{(2)}, z^{(3)})$ or any other arrangement as long as you are consistent for every training example and prediction. You don't usually want or need to somehow combine the x, y and z values to create a pre-processed layer with 3 inputs, or create a special layer that combines $x^{(1)}, x^{(2)}, x^{(3)}$. You might do that if your understanding of how the sequence worked allowed you to come up with features that modelled something important. However, the more usual approach is to allow the neural network to figure out how the multiple inputs combine from the training data. You may want to look at alternative architecture of Recurrent Neural Network (RNN), which has more options for dealing with predictions of series. For a RNN, you would have 3 input neurons, and you would run the network 3 times in sequence and read the prediction after running it the third time. You have noticed correctly that a MLP has no "understanding" internally that your x in (x,y,z) at step 1 is the same type of variable as the x in steps 2 and 3. The MLP would have 9 separate inputs and would ignore any direct relations, or even that there is a sequence being predicted. Whilst in a RNN, both those details are built into the model as core assumptions. In some cases that could be an important advantage to the RNN. But it is still worth trying the simpler MLP first and testing whether its predictions are good enough for purpose. 

Gradient exchange occurs in distributed learning systems that perform gradient descent, when one part of the distributed system need to use the gradient values from another part in order to complete a task. For example, you may distribute a large data set between multiple nodes, and want to calculate a gradient descent step as part of optimisation. One way to do so is calculate a subset of batch gradients on each node and collate them at a single node in order to alter parameters synchronously. This means it is necessary to fetch gradients from all nodes into a single node so that a combined gradient for some weight parameters can be calculated and the parameters updated consistently in the update step. Gradient exchange is just a term to describe that event - node A needs some gradients that node B has calculated, so they are requested (or pushed) and have to travel between the nodes. This is a relatively slow I/O process - it is necessary for distributed system to work, but for high performance you want to minimise time spent moving the data. Other data (such as the parameters) also needs to be shared between nodes. This particular piece of data regarding the gradients is singled out for the paper as the authors have discovered a way to compress it significantly without losing performance of the learning algorithm. This is partly because gradients can be treated approximately in the first place. Many learning algorithms further adjust or normalise gradients after they have been calculated, so using super-precise values is not as important as you might think. There may also be clever ways of splitting the update work so that each node only needs some of the gradients and only updates some of the parameters at each step. That will keep node CPU busy, possibly at the expense of more complicated communications. I do not know the details of any optimised distributed learning system in order to tell you the precise data exchanges and optimisations taking place. There are likely to be a few variations possible, depending on the framework and which algorithm is being implemented.