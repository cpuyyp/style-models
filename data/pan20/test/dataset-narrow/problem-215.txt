For reporting purposes splitting the field out into date and time has some benefits. Some possible benefits you could realise include: 

Probably not. Violating a declarative constraint will always raise an error and prevent the transaction from committing writes that would violate the constraints. The point of database constraints - checks, foreign keys, uniques etc. - is that they prevent invalid data from being recorded in the database by aborting writes that will violate the constraints. The constraint violation raises an error and rolls back the transaction. If you want the transaction to fail gracefully you will either have to deal with the exception or pre-validate the data before attempting to write it. In the latter case you will have to replace the validations in the stored procedures or implement validation in the application. 

Recent versions of SSAS (IIRC 2008+) support subqueries in MDX in much the same way that SQL does. What you see here is a subquery being used to produce a subcube that is then queried by the outer query. There's an article on it here. 

If you could meaningfully slice the data by key1, key2 or key3 then you could make dimensions out of them. In that case you would have separate dimension tables that had all the values of each key, and any relevant attributes (even just a human-readable description of what the value represents) or groupings (roll-ups of the key values). Each dimension table would have the key value and its attributes, and would link to the fact table on its key. If your values (Var X) are additive across rows (i.e. all the values added together for Key1='Foo' mean something) then you can use the keys to slice the data. I don't think that relationships between the Var columns are really relevant to dimensional modelling unless you want to unpivot them and indentify them by a 'variable type' dimension rather than separate columns Beyond that, the question is a bit vague to really get a clear view of what you are trying to achieve and why you want to make a dimensional model of your data in the first place. Maybe if you can clarify your goals a bit we can give you a more meaningful answer. EDIT: Dimensional modelling is just identifying the axes by which you want to slice data in aggregate. Sometimes a dimension might be very simple - just a code. Sometimes it may be complex with lots of sub-attributes - such as an insurance policy. A dimension lets you slice data in aggregate, either by its key or by rolling up the data by some attribute of the dimension. If the VarX columns are additive between rows - i.e it makes sense to aggregate (for example) var2 for two or more rows - then a dimensional structure might make sense for reporting. One key point of a dimensional model is that it tends to facilitate efficient queries in aggregate, and plays nicely with OLAP tools. So, if I had values 'Foo', 'Bar', 'Wibble', 'Blarg' for key1 and values 'A', 'B', 'C' for key2 then I might have a few rows that looked a bit like: 

Assuming that you are talking about tables containing the same kinds of entities, you typically want to have one table. You would not have any performance differences and a whole lot of management differences between the two approaches, with the single table being easier to manage. Typically large tables do not have performance overheads compared to smaller versions of the same when properly indexed, since the index seek time grows slowly compared to the row growth. In a normalized design, you will have different tables for different entities or relations, but as far as partitioning (either supported with a DBMS feature or manually with separate tables), that usually is necessary when you have certain requirements, like granularity of backup or data loading/unloading on a partition basis. 

The relational model is indeed very simple. Columns in a row in a relation are all related to a key, thereby defining a table/relation. Everything else follows from those principles. Typically the goal of object databases is to abstract some of the modeling that would be done in a traditional database to determine what object attributes are related to a key and how the design should be created, and how things will be accessed and how things should be indexed. And to handle higher-level concepts like inheritance or polymorphism or object variation, all of which is effectively hand-designed in a relational model. Similarly with network databases, moving some concepts of these higher-level constructs to first-order features within the database. I think it's a little unfair to relational databases which are very much standardized and researched and documented that all these other "databases" get to use that name (like other databases that existed before relational databases and normalization was researched and defined), but so be it - how many times have you been told someone's database is in Excel? There's nothing wrong with these approaches, but they do tend to not be good general approaches, platforms are not that standardized and performance is still typically what it is. A well indexed and normalized data model is still going to perform as well as its design. And a system with a lot of specialization like an object or network database may or may not outperform it depending on whether the design is really using the features to their best advantage. It's still bits on a disk and references to other bits on a disk and if you don't do that smart, it will still be slow... 

In a data warehouse with a date dimension it's often common practice to annotate the dimension with this sort of thing. You can use a TVF to generate the initial data but perisisting it in a table will be faster for most purposes. There's no particular reason that this technique could not be applied to a transactional application. 

Worth noting is that oracle dump files are a stream format with snippets of SQL embedded. You can also use sed to do substitutions on it, such as schema names. 

If the relationship is strictly hierarchical (i.e. a portfolio can have several sub-portfolios or projects, but a project cannot appear in more than one sub-portfolio) then you can model portfolio and project using a subtype pattern, e.g. 

Wrap it with a batch file that sets the environment variables and then kicks off the job. This lets you set up the wrapper file on a per-environment basis and hold as many environments on the machine as you want. 

You can install SSAS, SSIS and SSRS2012 and use a SQL Server 2008 R2 database as the backing data store. Assuming that the vendor will let you set up other databases on the server there is nothing that would prevent you from doing this. However, you would have to Licence SQL Server 2012 in order to do this. If (a) you can do this and (b) the vendor will let you set up your B.I. suite and associated databases on the server then there is no technical obstacle that would prevent you from doing this. If you wanted to put it on a separate server then you would have to license that separately. 

If your ITVFs are very complex and expensive, you run into the danger of people building on them and incurring costs they don't need to incur because it's the only way to get some data - i.e. something which might be less expensive as a persisted computed column or something which is being computed in the function but most users don't really need - expensive things which few people need should obviously be in the last layer or a separate function. 

From my point of view, you need to be careful that you really are talking about only ITVFs. Inline table-valued functions are basically parameterized views, so they are better optimizable compared to multi-statement table-valued functions. However, this limits some of their functionality. I find it unlikely that all your logic for frontend #1 can be managed with ITVFs, but maybe. It's just as likely that they will duplicate logic in forms in Access unless all they are using is the linked tables and not trying to do any forms at all. It's an OK strategy, but I still expect you might have some duplication of code. As a strategy, it is probably sound because ITVFs are intrinsically pretty low on the complexity list - I tend to use the constructs in SQL in increasing order of complexity - trying to solve problems using the least complex structure possible: 

This scheme is designed to partition on an accounting period. Dates are also frequently used for this, although any key could be used. You can create a table on the partition scheme as if it was a filegroup, e.g. 

If you want to spread your partitions over different physical volumes, you can still stick with 8 volumes. You can stagger the index partitions so that an index partition resides on a different physical volume to its corresponding data partition, e.g. 

Your decomposition to 2NF is correct. Decomposition to 3NF requires taking the non-key attributes that have their own dependencies into separate relations. The relation in 3NF would look like: 

(note not tested, just off the top of my head, but you can fiddle with it). Then you can re-load the stored procedures. Note that if you're frigging with the code base you should really test what you're doing rather than doing a blind search/replace on production code. What could possibly go wrong? 

Unfortunately you're not working for the customer, but if you were you could use an argument along the lines of enquiring just what architectural flaws the software has so that exposing the database schema might be a security risk. 

Yes, the benefit comes when you want to query on a part of the index. If you put the part-used predicates first the index can be used for queries that involve those predicates but not all of the columns in the index. Also, unless you have other requirements it can help to put the most selective predicates first, as this can trim down index seek operations faster. In your case is not necessarily redundant depending on the nature of the queries on the table. However, it may not be necessary to include all of the columns. If, for example, you do a lot of queries by and then may be useful to help resolve those queries as is not in the right order to be useful for that. You might, however, find that is redundant on . From 9i, Oracle introduced a 'skip scan' operator where trailing index columns can be queried more efficiently, which can reduce the need for supplementary indexes of this sort. In a more specific case, if you are querying by and and don't need any other columns then the query could be completely resolved though the index without having to read anything from the table. You can also build covering indexes that have non-indexed columns attached. If all of the needed columns can be resolved from the covering index the query doesn't need to touch the main table at all. Finally, in answer to your last question: If you have a set of regularly used queries that are using up a lot of resources and could be tuned using an index then it's certainly worth considering. However, maintaining indexes comes with an overhead on inserts, so you will have to trade query performance off against the overhead that the indexes place on insert or update operations. 

You should post the query, given that it filters on some types, I would expect an index involving that column would help. I expect an NCI on the transaction type and date would indeed help a lot based on the query in your comment. And INCLUDE the PlayerID and PointDelta columns - that index would be covering - also if it's just one (or a few) types for basically just this query - make a filtered index: $URL$ 

Based on everything you have said in the question and the comment, I don't think you need to worry about hardware yet unless you are trying to just get a ballpark price estimate for feasibility. Since you are a software person, I would build the prototype on commodity hardware like your ordinary laptop, analyze and understand the problem and then spend money once you know more about the profile of the software and the problem space. If your laptop is not up to it, then pick up a refurbished server for a few hundred dollars to get it closer to being able to test. It sounds like a small amount of data and there is no indication of the large amount of ongoing data analysis (which can greatly be affected by the data model - some data models can make analysis many orders of magnitude faster). If you are trying to test at production loads, you will need a production spec machine - so the problem is a chicken and egg one. To have a production machine, by your own admission, you would need an expert. And I know this is relatively off topic from your question, but: The things you need to be looking at on your software design are the rate at which the data is coming in, the amount of processing (parsing, de-duping) you will need to do, the model for the data, the size estimates, the way the reads are going to work (whether you have multiple models - one for writing and one for reading, like data warehousing), and the complexities of the analysis and whether this will be performed by SQL Server in your architecture or by client code (and whether you intend the client code to run on the same server or whether you will also have an application server). This will tell you a lot more about what you want out of your SQL Server than your point 4 (the only thing you have given us which has anything to do with determining the nature of the server configuration you will want in addition to the $2000-$6000 budget). And this would be information that your expert would need to be useful to you. In my experience, the right time to make these kind of decisions is as late as possible - and a decision on this hardware can be deferred. 

Misalignment does not create a greater data integrity problem than would otherwise be present. The database and file system have mechanisms in place to ensure file system opearations are atomic. Generally a disk crash will result in data loss but not data integrity issues. 

By joining this dimension table to the fact table you can then roll up by Key1Grouping and see stats for (say) 'Group A.' You can calculate ratios of Vars after the base additive measures have been aggregated. Again - I'm still a bit unsure of quite what you want to know. 

. . . and so on. For a query using a small number of partitions you will at least get the benefit of spreading your data and index disk I/O. Depending on the nature of your I/O you may be better off having all of the partitions on a single large RAID-10 as mentioned elsewhere. Of course if your controller won't let you do this (e.g. an IBM Shark) or you have multiple controllers then you will have to use multiple volumes. If you have the option I'd suggest benchmarking it before the system gets to production. Another point to note is that some systems (DS8000s come to mind) put their physical volumes into a pool and abstract away a lot of control you have over physical disk layout. For 1B rows you might want to look into direct attach storage if this is an option. 

You can query the INFORMATION_SCHEMA: $URL$ In the INFORMATION_SCHEMA.COLUMNS table there is a IS_NULLABLE column. You could turn it into a function, I imagine, but I would probably put this logic in an outer part. 

You are far better off simply inserting into a mail queue table in the trigger and having a separate email process handling the mail - either in SQL Server Agent or an external program. This gives you the benefit of using the trigger as you desire, relatively low latency as the other job can see the mail once it's committed, scalability of managing the mail load independently etc. You can wrap the mail queueing into the transaction properly. Then the queueing app is reliable if the mail server is temporarily unavailable or if you have email address issues. If mail needs to be resent, it's simply a matter of resetting the sent flag. You can make expiration on mail, so that mail which couldn't be sent to the SMTP won't be retried after a certain time. You can quickly route all mail to a test mailbox. You can do quite interesting things with attachments and other things based on data in the queue without tying up the trigger for extended periods - things like generating reports etc. So the app offloads the mail responsibility to another component and is free to complete the database transaction and return success to the users, while the mail subsystem gets on with its work ensuring all you want to do with the mail gets done. 

Messing with the application is the last thing you should do, particularly if relations with the vendor are getting tense. You need to make sure your management knows that the performance issue is down to the architecture of the system. Get that in writing somehow; write up a report describing the issue and the reasons you can't fix it unilaterally. Make sure it's emailed to all of your internal stakeholders and you have a record of it being sent. Arrange a meeting to follow that up and explain it. That covers your arse - this is quite important if the vendor (or the party that authorised the purchase) tries to get political. Now, escalate the issue with your I.T. management function, making sure it's well documented. As you understand the root cause of the problem, this shouldn't be an issue. The vendor has sold you a system that is not fit for purpose. It's now a management issue. Light the blue touch paper and stand back. If you've already done this, then you've done everything that you can do without interfering with the application itself. Again, I can't say this strongly enough: do not do anything to the application that could void the SLA. 

Comparing dmp files like that is probably quite difficult to do, as the format is quite complex, with snippets of embedded SQL and all sorts. Having said that, although it is a binary format, it is a stream format (i.e. no internal pointers), so it's quite amenable to processing with sed. I think loading the contents of he dmp files into two schemas and comparing the two is by far the easiest way you're going to do this, barring the existence of some tool that I'm not aware of.