$\def\F{\mathbb F_2}\let\ET\bigwedge$Let me convert my old comment above to a proper answer, as it is a much simpler construction than in the other answer. XOR-SAT can be computed in polynomial time by Gaussian elimination. Thus, it is reducible to Horn-SAT. The reduction amounts to describing the local conditions for correctness of a transcript of a run of the Gaussian elimination algorithm, and with a reasonable representation of the output, it can be made a DLogTime reduction. In fact, Gaussian elimination is simple enough that a (slightly optimized) reduction can be described completely explicitly. Assume that we are given a matrix $A=(a_i^j)\in\F^{m\times(n+1)}$, representing the linear system $$\begin{align*} a_1^1x_1+\dots+a_n^1x_n&=a_{n+1}^1\\ \tag{L}\vdots\\ a_1^mx_1+\dots+a_n^mx_n&=a_{n+1}^m \end{align*}$$ over $\F$. We will produce a system (H) of Horn clauses such that (L) is solvable iff (H) is satisfiable. The definition of (H) will simulate the following version of Gaussian elimination over $\F$: 

The problem is decidable in polynomial time (in fact, in uniform $\mathrm{TC}_0$). We can recast the question as follows: given positive integers $a,b,c,d,e$, find the sign of $$\Lambda=e\log(a/b)-\log(c/d).$$ First, note that we can test whether $\Lambda=0$: if we assume w.l.o.g. that $\gcd(a,b)=\gcd(c,d)=1$, then $da^e=cb^e$ implies $a^e\mid c$ and $b^e\mid d$; in particular, apart from the trivial case $a=b=1$, equality can hold only if $e\le\max\{\log c,\log d\}$, in which case we can evaluate $a^e,b^e$ in polynomial time. Likewise, we can rule out the case when $(a/b)^{e_1}=(c/d)^{e_2}$ for some positive integers $e_1,e_2$. The argument above shows that if this is the case, then $e_1,e_2\le\max\{\log a,\log b,\log c,\log d\}$, hence we can find $e_1,e_2$ by brute force in polynomial time, and then it suffices to compare $e$ with $e_1/e_2$. Thus, suppose $a/b$ is not a rational power of $c/d$, which implies $\Lambda\ne0$. Write $\alpha_1=a/b$, $\alpha_2=c/d$, $\lambda_1=\log\alpha_1$, $\lambda_2=\log\alpha_2$, $\beta_1=e$, $\beta_2=-1$. Note that the logarithmic height $h(\alpha_1)=\max\{\log a,\log b\}$, $h(\alpha_2)=\max\{\log c,\log d\}$, and $\lambda_1,\lambda_2$ are linearly independent over $\mathbb Q$. By some version of Baker’s theorem (e.g., the “explicit result by Baker and Wüstholz” in $URL$ we have $$\log\lvert\Lambda\rvert>-Ch(\alpha_1)h(\alpha_2)\log\beta_1$$ for some constant $C$ (note that in the statement given in the link, we have $n=2$ and $d=1$, hence $C$ is really independent of the input data). The number on the right-hand side is polynomial in the length of the input, and it thus suffices to approximately evaluate $\Lambda$ with polynomially many bits of precision, which can be done by standard numerical algorithms in polynomial time. (If you want to look for more information, the keyword is “linear forms in logarithms”.) 

You can subscribe to the RSS of ArXiv, and you can keep track of which papers are getting accepted to the relevant conferences. 

It is known that if $E = DTIME(2^{O(n)})$ is not contained in $SIZE(2^{\varepsilon \cdot n})$ for some $\varepsilon>0$ then $BPP = P$ ($URL$ (Actually, a slightly stronger assumption is needed, namely, the separation between $E$ and $SIZE(2^{\varepsilon \cdot n})$ should hold for almost all input lengths - thanks to Ryan O'Donnel for pointing it out.) More generally, this paper gives a direct optimal translation of any circuit lower bound into a corresponding derandomization result: $URL$ 

We cannot hope to prove a general impossibility result since if one-way functions exist (and we believe they do), then in particular it follows that the statement "If $P \ne NP$ then one-way functions exist" is true. However, we can prove that certain proof techniques are too weak to prove that statement. In particular, the following paper of Akavia, Goldreich, Goldwasser and Moshkovitz proves that this statement cannot be proved by certain black-box reductions (conditioned on plausible assumptions): $URL$ 

No, $\mathrm{NP}\subseteq\mathrm{BQP}$ is not known to imply $\mathrm P=\mathrm{NP}$. Even the stronger assumption $\mathrm{NP}\subseteq\mathrm{BPP}$ is not known to yield a deeper collapse than $\mathrm{NP}=\mathrm{RP}$ and $\mathrm{PH}=\mathrm{ZPP^{RP}}=\mathrm{BPP}$; in particular, it is not even known to imply $\mathrm{NP}=\mathrm{coNP}$. (However, all these implications are likely true by virtue of their premises being false.) 

$\let\mr\mathrm$There are several results in the literature stating that a certain class $C$ satisfies $C\nsubseteq\mr{SIZE}(n^k)$ for any $k$, and usually it is straightforward to pad them to show that any barely superpolynomially expanded version of $C$ is not in $\mr{P/poly}$. Let me say that $f\colon\mathbb N\to\mathbb N$ is a superpolynomial bound if it is time-constructible, and $f(n)=n^{\omega(1)}$. For example, $n^{\log\log\log\log n}$ is a superpolynomial bound. In fact, an instructive exercise shows that if $g(n)$ is any unbounded monotone computable function, there is a superpolynomial bound $f$ such that $f(n)\le n^{g(n)}$. First, direct diagonalization shows that $\Sigma_4^P\nsubseteq\mr{SIZE}(n^k)$ for any $k$. The same argument gives: 

$\def\mc{M_\mathit{const}}\def\mp{M_\mathit{paradox}}$Let me for the record write up the answer to Q1, so that it doesn’t live only in the comments. The reasoning given in steps 1–5 in the question is correct in the real world. Thus, $\mc(\mp)$ outputs NO, and $\mp(x)$ halts in constant time, but there is no short enough proof of this in ZFC. When trying to formalize this argument in ZFC, the problematic step is 2: here, we need to assert, in ZFC itself, the implication 

Let $X$ and $Y$ be sets, and $\mathcal{B}$ be a partition of $X \times Y$. I would like to prove that there exists a distribution $\mathcal{D}$ over $X \times Y$ whose marginal is uniform over $X$, and such that the distribution over $\mathcal{B}$ induced by $\mathcal{D}$ has large entropy(the latter distribution is defined by assigning each $B∈\mathcal{B}$ the total probability mass of the elements of $B$ under $\mathcal{D}$). We can use the following condition: Consider the bipartite graph $G$ whose sides are $X$ and $\mathcal{B}$, such that for each $(x,y) \in B$ there is an edge $(x,B)$ in $G$ (multiple edges possible). Then, every set of $x$'s of size at least $\frac{3}{4}|X|$ has at least $\frac{1}{100}|B|$ neighbors in G. I would appreciate it if someone could refer me to a relevant theorem. This question can be viewed in a sense as a generalization of Hall's theorem, where the above condition is a relaxation Hall's condition, and where instead of getting a perfect matching, we get a set of edges whose corresponding subgraph is roughly regular. Background: The motivation for this questions comes from communication complexity. In the setting of communication complexity, two players, Alice and Bob, get inputs $x$ and $y$ respectively, and interact in order to compute some function $f(x,y)$. Here, each set $B \in \mathcal{B}$ consists of pairs $(x,y)$ that yield the same transcript of communication between Alice and Bob, and I would like to prove that under some condition, one can find a distribution over $X \times Y$ such that Alice gets a uniformly distributed input, and such that the entropy of the transcript under the distribution is large. 

(This translates into a uniform family of depth-3 circuits of size $2^{O((\log n)^2)}$, where the bottom disjunctions have fan-in only $O\bigl((\log n)^2\bigr)$; this is often called “depth $2\frac12$”.) Proposition 3 is again a consequence of the structure theorem for finite abelian groups: any such group can be written as a direct sum of $O(\log n)$ cyclic subgroups, thus groups $A$ and $B$ are isomorphic iff they can be written as direct sums of cyclic subgroups with matching orders: that is, if $|A|=|B|=n$, then $A\simeq B$ iff 

The answer to all three questions is negative. Consider e.g. the prefix version (3), and assume for contradiction that it is decidable by a (wlog deterministic) automaton $A$ with $m$ states. Consider the strings $$\begin{align} u_k&={+}^{m-k}{*}{+}^k,\\ v_l&=1^{l+1}0^{m-l+1}. \end{align}$$ Notice that $u_kv_l$ is a well-formed Boolean sentence for every $0\le k\le m$, $0\le l\le m+1$, and it is easy to see that $u_kv_l$ evaluates to $1$ iff $k<l$. By the pigeonhole principle, there are $0\le k<l\le m$ such that $u_k$ and $u_l$ land in the same state of $A$, thus $u_kv_l$ and $u_lv_l$ are both accepted or both rejected. However, the former evaluates to $1$, and the latter to $0$. For infix formulas (questions 1,2), one can use in a similar way formulas of the form $$(0+((0+\cdots((0+((1+((0+\cdots((0+0)*1))*1))\dots*1))*0))*1))\dots*1)).$$ The argument in fact shows that any deterministic automaton evaluating well-formed Boolean sentences of length $n$ must have size $\Omega(n)$, or in other words, any online algorithm for evaluation of such sentences needs space at least $\Omega(\log n)$. This bound is tight: as shown by Buss, it is possible to evaluate Boolean sentences in infix notation in online space $O(\log d)$, where $d$ is the depth of the formula (i.e., maximum nesting of brackets). (The paper advertises it as an online $O(\log\log n)$-space algorithm for evaluation of balanced formulas, using the fact that these have depth $d=\log n$.) On the other hand, as also shown by Buss, Boolean sentence evaluation (in either prefix or infix notation) can be done in ALOGTIME, aka uniform $\mathrm{NC}^1$, and it is in fact complete for this class under DLOGTIME many-one reductions. Since regular languages with unsolvable syntactic monoid (for example, $\bigl((a|b)^3(ab^*a|b)\bigr)^*$) are also $\mathrm{NC}^1$-complete, it follows that one can reduce Boolean sentence evaluation in DLOGTIME to a regular language. 

There exist no boolean one way functions, since for a boolean function, you can always guess a preimage of the output, and with high probability, you'll be right. 

The lines oracle is used to decrease the query complexity of the test from $d+1$ to $2$, at the expense of using a larger alphabet. If you don't mind making $d+1$ queries, then the lines oracle is indeed unnecessary. However, it is usually better to make two queries over a large alphabet than $d+1$ queries over a small alphabet. One reason is that, very roughly speaking, we have a technique, called PCP composition, which can be used to reduce the alphabet size of PCPs, but cannot be used to reduce the query complexity. Hence, it is preferable to have small query complexity and large alphabet size rather than the other way around. A second reason, which is very related to the first one, is that many hardness-of-approximation results are proved based on $2$-query PCPs, but can not be proved using based on a PCP with more queries. It should be mentioned that there are also generic techniques for reducing the query complexity at the expense of the alphabet size, so one could avoid the lines oracle and apply those techniques instead. However, using the lines oracle gives a more direct proof. 

I’ll comment on why a relation as in the question $$ (2^n)! = \sum_{k=0}^{m-1} a_k b_k^{c_k} $$ (for every $n$) helps factoring. I can’t quite finish the argument, but maybe someone can. The first observation is that a relation as above (and more generally, the existence of poly-size arithmetic circuits for $(2^n)!$) gives a poly-size circuit for computing $(2^n)!\bmod x$ for $x$ given in binary: simply evaluate the sum modulo $x$, using exponentiation by repeated squaring. Now, if we could compute $y!\bmod x$ for arbitrary $y$, we could factor $x$: using binary search, find the smallest $y$ such that $\gcd(x,y!)\ne1$ (which we can compute using $\gcd(x,(y!\bmod x))$). Then $y$ must be the smallest prime divisor of $x$. If we only can do powers of $2$ for $y$, we can still try to compute $\gcd(x,(2^n)!)$ for every $n\le\log x$. One of these will be a nontrivial divisor of $x$, except for the unfortunate case when there is an $n$ such that $x$ is coprime to $(2^n)!$, and divides $(2^{n+1})!$. This is equivalent to saying that $x$ is square-free, and all its prime factors have the same bit-length. I don’t know what to do in this (rather important, cf. Blum integers) case. 

Regarding the last eigenvalue: The last eigenvalue $\lambda_n$ measures (roughly) how close is the graph to be bipartite. For example, $\lambda_n = -d$ if and only if the graph is bipartite (this is a fairly easy exercise). You can read more about it in Luca Trevisan's blog: $URL$ $URL$ 

Let $X$ be a random variable taking values in $\Sigma^n$ (for some large alphabet $\Sigma$), which has very high entropy - say, $H(X) \ge (n- \delta)\cdot\log|\Sigma|$ for an arbitrarily small constant $\delta$. Let $E \subseteq \rm{Supp}(X)$ be an event in the support of $X$ such that $\Pr[X \in E] \ge 1 - \varepsilon$, where $\varepsilon$ is an arbitrarily small constant. We say that a pair $(i,\sigma)$ is a low probability coordinate of $E$ if $\Pr[X \in E | X_i = \sigma] \le \varepsilon$. We say that a string $x \in \Sigma^n$ contains a low probability coordinate of $E$ if $(i, x_i)$ is a low probability coordinate of $E$ for some $i$. In general, some strings in $E$ may contain low probability coordinates of $E$. The question is can we always find a high probability event $E' \subseteq E$ such that no string in $E'$ contains a low probability coordinate of $E'$ (and not of $E$). Thanks!