I think there are restrictions and you should probably use a workaround such as a local view that references the remote table or a local synonym. In any case this should remain an exception, obviously the best place to store a procedure is in the database where the data is stored. If you find that a program frequently needs access to several databases, it might be a good idea to merge those databases. 

Unfortunately it would seem that your customer is running its DB in NOARCHIVELOG mode since the view V$ARCHIVED_LOG is empty. V$LOG_HISTORY only records the switchs of the online redo logs which all DBs have. Only databases that are setup in ARCHIVELOG mode will archive redo logs. I'm afraid your customer is out of luck. At this point you should probably consider contacting Oracle support. 

The record is still there (with a flag set). If we look at the actual binary data (just before the section, we see that the data has not yet been overwritten: 

Your database character set is , it doesn't have the sign. Either use or change your database character set (export, reinstall with a character set that supports the euro sign like or , and import). Here's an example of : 

Note that standard statements, even though they are destined to be used only once, will occupy a space in the shared pool (until they are aged away according to an LRU algorithm). Prepared statements are therefore a client optimization. They allow the client to limit the number of hard parses by using bind variables and reduce further the number of soft parses by reusing the same cursor multiple times when possible. Not using prepared statements usually leads to a shared pool full of slightly different statements that are used only once each. The number of parses increases dramatically, which can be a source of contention and therefore bad database performance. You shouldn't worry too much about the size of the shared pool: use prepared statements and Oracle will keep the statements that are used the most at the top, they shouldn't age away and their plans should be available at all time. 

There is no difference in treatment in Oracle between a java and a standard : both are treated as cursors. They are both parsed (="compiled"), executed and (for queries only) fetched. The difference between the two kinds of statement is that : 

If for some reason this grant has been removed, you would need to regrant the rights to either or a specific user (with a DBA account): 

If you're locked on inserts, it usually means that you're trying to insert rows that have the same values for a set of columns that are UNIQUE, for example: 

If the client itself has already left or the network communication has been severed, the killed session may stay indefinitely until you restart the instance. I don't think these sessions take resources. 

Your joins every row to every other (including itself) N times. This is a multi-auto-cross join. I can see how it could behave poorly performance-wise when the number of records increases. I suggest a query like this instead: 

The statement is initially parsed. If the statement has already been parsed by the database before (and is still in the share pool), the previous execution plan will be re-used. This is called a soft parse. If the statement is new or too much dissimilar to previous statements, a new plan is computed. This is called a hard parse. The statement is then executed (once the client asks for it). A fetchable result set is returned for queries. The client can re-use the cursor to run the same query, possibly with different variable values (jump to 4.) 

The queries are an order of magnitude different ! The access paths themselves shouldn't have such an impact but you're comparing the simplest aggregation function () to one of the most complex () ! Furthermore, you have specified an clause in your , this will force Oracle to sort, which explains the optimizer decision to ignore your hint (this hint is also undocumented as far as I can tell). If you wish to compare the different access paths, use the exact same query with different hints, eg: 

In Oracle, a binary tree index on a NOT NULL column can be used to answer a COUNT(*). It will be faster in most cases than a FULL TABLE SCAN because indexes are usually smaller than their base table. However, a regular binary tree index will still be huge with 157 Mrows. If your table is not updated concurrently (ie. only batch load process), then you might want to use a bitmap index instead. The smallest bitmap index would be something like this: 

This means that you won't be able to use plain SQL to overcome this limitation: you will need some PL/SQL. Either a trigger that populates both fields or a function that wraps the sequence call. Here's an example with such a function with 11g: 

Both actions (binding and re-executing) are client behaviours. To Oracle a client that uses a cursor only once or another client that uses a cursor a thousand times are treated exactly the same way: 

Null entries are taken into account by a bitmap index. The resulting index will be tiny (20-30 8k blocks per million row) compared to either a regular binary tree index or the base table. The resulting plan should show the following operations: 

If you grant an object type privilege (without ) such as to a role, all users of this role will be able to assert this privilege on their own schema. Consequently you can grant these privileges to a role and the users will inherit such rights (on their own schema) when they are granted the role. 

You can enable it at the table level. You can specify an undo retention ; although I can't find the limit it is probably larger than you need since the goal is to keep records for legal purposes. There is an example of a scenario that might apply to your specific case: Using Flashback Data Archive to Recover Data. Oracle Total Recall is part of the Advanced Compression Option, available on the enterprise edition. 

When you do DML on a table, Oracle may make the changes to the base table almost immediately, even if you don't commit. The other sessions don't see these changes yet thanks to multiversion read consistency but the actual physical block may already be overwritten with your uncommited work. This is done because Oracle is optimized for commit. Most sessions should rarely rollback so Oracle anticipates by writing preemptively the changes. This is also one of the reasons that allows Oracle to have arbitrarily large transactions (limited only by the size of the undo tablespace). The changes are not delayed (much). This is also why you almost never wait for a commit in Oracle, even for multimillion-row DML. One of the consequences is that rollback needs to undo the changes. The uncommited transaction is read back from the undo tablespace and each and every change is undone in reverse order. This can take longer than the initial DML for several reasons: 

The reason your index isn't being used is that the two following SELECT are fundamentally different to the optimizer: 

A function-based index adds a virtual column to the table (This column is then indexed). Dropping the index removes the virtual column, which leads to a cleanup that takes time (same amount of work as the removal of a non-virtual column). 

This index will only ever index a single row at most. Knowing this index fact, you can also implement the bit column slightly differently: 

You're not seeing the benefits of parallel execution because both insert methods use single-row inserts, hence parallel does not kick in ! Parallel execution is only available for bulk operations. Lots of tiny operations doesn't qualify as a set operation. You're inserting rows one by one here, you should try it with a bulk operation (, ...) and see if you can achieve some performance improvement. You can tell when Oracle does a parallel execution because the explain plan will display parallel operations. For further reading: 

If the row is locked, you will receive an ORA-00054 which is in most cases preferable to indefinite waiting. 

Your requirements look like a good case for Oracle's Flashback Data Archive (Total Recall) feature. Disclaimer: I haven't used this feature yet. The description reads: 

This will delete the rows from sale that are present in split and replace them with their appropriate split products. You could also write it: 

Most likely your tables have up-to-date statistics but sometimes the optimizer is baffled because it oversimplifies the cardinality estimation. This seems to be a good candidate for dynamic sampling. In its default value (2 in 10g and 11g), dynamic sampling will only be used if one of the table has no statistics. In your case you would need to change its value to be able to let the optimizer collect statistics to build a better plan. I suggest you use the hint that will let you modify the optimizer behaviour for a single query. I tested with a subquery and you need to use one of the following syntax: 

I'm pretty sure you're using the MS SQL Server terminology for "clustered" here. This is quite different from the cluster concept in Oracle. If I'm right then you're just wondering why your regular B*Tree index is not used to answer your query. It is probably because your column is nullable, therefore some rows may not be indexed and the optimizer will have to go to the table to be sure to retrieve all rows. You should be able to get an index (fast-full) scan with a query like this: 

So cleanout (full with redo) will be performed during commit if the blocks are still in the SGA. In active systems, it is common for blocks with uncommited transactions to be written to disk and flushed from the SGA. In this case, the block is left as is and the next query that touches the block will perform delayed block cleanout (your point 5 doesn't happen in all cases). 

Both functions and take three arguments. The two last arguments take a default value if they are unspecified. The second argument is the format. It will default to your session parameter value. Since you will potentially lose information during the conversion, there is no reason to assume that the two functions are the inverse of one another. Let's take an example with the default poor choice of format : 

This is an expected and normal behaviour, Oracle wants to protect you from yourself since Oracle guarantees: 

you select only all date up to the at midnight. All rows later that day won't be selected. What is happening is that when you use on a date, it will be converted to VARCHAR2 and then converted back to a date. The net effect will be a truncation to your date format as shown: 

Some session, after being killed, will remain in the view, even after all their changes have been entirely rolled back. This is often because Oracle is waiting for the client to return a relevant error message, such as: 

you insert a row in table A a trigger on table A (for each row) executes a query on table A, for example to compute a summary column Oracle throws an ORA-04091: table A is mutating, trigger/function may not see it 

If is unique, you can use referential integrity with a virtual column (11g+) to make it conditional: 

I suppose you want to create an APEX administrator account. They are workspace administrator of the special workspace "Administration" that you can create following the standard APEX account creation. 

You can use the legacy (but still supported) utility to export a table/schema/DB to your local computer, and then use to import the data on another instance. 

You can't really list all rows that are being locked by a session. However, once a session is being blocked by another, you can find which session/row is blocking it. Oracle doesn't maintain a list of individual row locks. Rather, locks are registered directly inside the rows themselves -- think of it as an extra column. You can find which session has acquired a lock on an object through the view, but this will only list general information, not at the row level. With this view you can also find if a session is being blocked by another. In that case, if a session is blocked by another session, the row information is displayed in the information. You can retrieve the rowid, let's build an example with 2 sessions: