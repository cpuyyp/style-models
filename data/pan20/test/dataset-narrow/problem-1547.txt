Let LASSO pick the best ones. If the features are highly correlated and you want them picked as a group, add some L2 regularization too. This is called Elastic Net regularization, and it is a generalization of L1 and L2 regularization. Other than that, do not feel obliged to artificially group features. 

I would recommend a Restricted Boltzmann machine, which is a type of neural network that can model probability distributions. What you are trying to do is to estimate the marginal distribution of the missing features. In a probabilistic model, this is done through marginalization. The beauty of this method is that it provides the full distribution over the missing features rather than mere point estimates. Here are two tutorials. 

The 1/2 coefficient is merely for convenience; it makes the derivative, which is the function actually being optimized, look nicer. The 1/m is more fundamental; it suggests that we are interested in the mean squared error. This allows you to make fair comparisons when changing the sample size, and prevents overflow. So called "stochastic" optimizers use a subset of the data set (m' < m). When you introduce a regularizer (an additive term to the objective function), using the 1/m factor allows you to use the same coefficient for the regularizer regardless of the sample size. As for the question of why the square and not simply the difference: don't you want underestimates to be penalized similarly to overestimates? Squaring eliminates the effect of the sign of the error. Taking the absolute value (L1 norm) does too, but its derivative is undefined at the origin, so it requires more sophistication to use. The L1 norm has its uses, so keep it in mind, and perhaps ask the teacher if (s)he's going to cover it. 

Hyperparameters and parameters are often used interchangeably but there is a difference between them. You call something a 'hyperparameter' if it cannot be learned within the estimator directly. However, 'parameters' is more general term. When you say 'passing the parameters to the model', it generally means a combination of hyperparameters along with some other paramaters that are not directly related to your estimator but are required for your model. For example, suppose your are building a SVM classifier in sklearn as shown below: 

There are many but some of the resources are awesome. One such resource is Udacity. This is probably the best resource I have encountered yet. 

There is no preference for stacked generalization. You can use any algorithm whether it be a , etc. The only thing that you need to take care of is the fact that the algorithm which you are applying to your model is useful to your data or not. Also, model stacked at level 1 may not be a very good model for level 2. For example, suppose at level 1 you stacked an XGB model with , but at second level it might happen that the same depth is just an overkill and hence you should discard using that model at second level. So, in short there is no preference for stacked models, the preference is for the data on which you are going to train your model. 

There's a recent paper on this subject (On-line Random Forests), coming from computer vision. Here's an implementation, and a presentation: Online random forests in 10 minutes 

Welcome to Datascience.SE! Like you said, you can extract the day of the week. Also extract the hour of the day, then encode these two variables using sines and cosines with their respective periodicities (7 and 24). Also create a column for the UNIX/epoch time. If there are "special days", such as holidays or sales, create boolean columns for them too. 

Clustering I think a tree is a perfectly appropriate data structure in this case. You don't need an embedding to do clustering; there are similarity-based approaches too, and defining a similarity function in your case is straightforward: I'd say it's a function of the common depth between the two items in consideration. You will probably want to define a root to "normalize" the scores. For example, given the root "a/b/" and the paths: 

As told by @smci, this technique is called Data Imputation. There are several techniques which can be used to deal with the missing data. Some of these are: 

are used to control the output of a neural network. They tend to make the output smaller. Suppose the loss function is give as : 

In general, more training examples means improvement in learning but you can also get a very good (and nearby to the optimal score) if you just fit a good algorithm on a subset of your data set that has enough training examples. Here are a few things you can do in your current case : 

I will assume that the dataset here is being split into training and validation sets. When you split up your dataset into training and validation sets, you need to take care of that you don't throw away too much data for validation because it is generally seen that with more training data, we get better results in most of the cases. So 50:50 is too bad, 60:40 is fine but not that good. You can make it 80:20 or 75:25 for getting better results. 

Alternatively, replace the new line characters with something else before parsing, and back again afterwards. 

Yes, this is a well-studied problem: rank aggregation. Here is a solution with code. The problem is that the quantity you are trying to estimate, the "score" of the item, is subject to noise. The fewer votes you have the greater the noise. Therefore you want to consider the variance of your estimates when ranking them. 

A simple solution is store the words in a dictionary (since you have to store them in some data structure) whose key is the character distribution; e.g., Counter("hello") = {h: 1, e: 1, l: 2, o: 1}. Querying the dictionary would give you the anagrams. For storing the immutable key (character distribution) can either use a tuple list, incurring a sort, or you can use a vector the length of your alphabet (26). So you can make a speed-space trade-off in the preparation stage; the look-ups are constant time, not counting the time it takes to calculate the character distribution of the query word. If you go the latter, fixed-width route, you can make another trade-off by hashing the key, since you know the cardinality of the input (number of unique words). 

There is not any single good answer to this question as this thing depends on many factors and one of them is the . As per my experience, I prefer not to remove features as even little information can be very useful. 

Comparing two libraries or tools in terms of these things is somewhat that is opinion dependent. Some people prefer for doing almost all the tasks. has also gained quite reputation. But what is better for you depends on what you want do. In my personal experience, I have found that along with libraries is all that I need to do all the Natural Language Processing tasks. 

If you don't define the validation set/ validation split for your model, then it has no way to check for it's performance because you have not provided anything to the model on which it can validate its performance. In this case, the model will run through training examples, will learn a way to minimize the cost function as per your code and that's it. You get a model which has learned a hypothesis from your data but how good the model is, it can only be checked by making predictions for the test set. 

As you've explained it, you want to detect communities, which is a common problem in social network analysis. If your network had been directed and temporal, I would have suggested you look into influence analysis; e.g., as in Detecting Communities of Authority and Analyzing Their Influence in Dynamic Social Networks. 

It's an SFrame, not a CSV; you need GraphLab, not pandas. They show you how to load it in the attendant notebook: 

Let $x, y \in \mathbb R^N, \mathbf \alpha \equiv (\alpha_0, \dots, \alpha_N) \equiv (\alpha_0, \mathbf \alpha'_0), \; \mathbf z^\alpha \equiv z_0^{\alpha_0} \dots z_N^{\alpha_N}$ and ${d \choose \alpha}$ be the multinomial coefficient. Then $$\left(\left<\mathbf x, \mathbf y \right> + c\right)^d = \sum_{|\mathbf \alpha| = d} {d \choose \alpha} c^{\alpha_0} (\mathbf x \mathbf y)^{\mathbf \alpha'_0}$$ If you redistribute the coefficient ${d \choose \alpha} c^{\alpha_0}$ between $\Phi(\mathbf x)$ and $\Phi(\mathbf y)$, it follows that $$\Phi(\mathbf z) = \left(\sqrt{ {d \choose \alpha} c^{\alpha_0}} \mathbf z^{\alpha'_0} \right)_{\forall |\mathbf \alpha| = d}$$ References 

I have a dataframe consisting of some continuous data features. I did a kde plot of the features using seaborn kdeplot functionality which gave me a plot as shown below : 

So this vector y = [0,0,0,1] is your target to these input types that you feed into the network as labels corresponding to your above input data. 

I have a huge data set with one of the columns named 'mail_id'. The mail_id is given in a very creepy format as shown below: 

After a detailed analysis of my data I figured out that I cannot drop these two columns from my dataframe as they are too importanct for prediction. I can hash these two features but there is one more interesting thing. There are only 2,000 types of user_ids and mail_ids. So doing one hot encoding can help a lot. My question is that if I convert this into one hot encoding using 'get_dummies' method in pandas with sparse=True, will it be memory efficient or is there any other efficient way to do it? 

This is the log-likelihood: $\log P(x; w) \equiv \log \prod_i P(x_i | w) = \sum_i \log P(x_i | w)$, where $P(x_i | w) \equiv \left\{ \begin{array}{rl}\sigma(x_i), & y_i =1 \\ 1 - \sigma(x_i), &y_i = 0\end{array} \right.$ Why the log-likelihood? When you have a probabilistic model, such as logistic regression, it's one way (the MLE) of finding the parameters that fit best. Recall that in logistic regression we are, contrary to the name, trying to classify rather than regress, and the MSE is a regression loss; it seeks to minimize the distance from a point, while we wish to penalize being in the wrong subspace (the parts that don't correspond to the correct class). If you squint a bit, you can see that the negative log-likelihood minimizes the cross entropy. 

By the definition of the logistic regression model $\mathrm P(y = 1 | x,w) = \sigma\left(\left<w, x\right>\right)$ thus $\mathrm P(y = 0 | x,w) = 1- \sigma\left(\left<w, x\right>\right)$ You should be able to verify (by setting y=0/y=1) that this is equivalent to $\mathrm P(y | x,w) = \sigma\left(\left<w, x\right>\right)^y \left[ 1- \sigma \left( \left<w, x \right> \right) \right]^{1-y}$ I think you understand the rest? 

Welcome to the community!! There can be a lot of answers to this question but I would suggest you the approach I took when I shifted from software development to the data science field. 1) Refresh your statistics and probability concepts. You should not go into too much details but you must understand basic things like Gaussian Distribution, Mean, Variance, Probability,etc. 2) Go through the basics of Machine Learning concepts. I prefer Andrew Ng's machine learning course on Coursera. That will help you build a strong foundation and will give you a great start in the field 3) Choose a particular language Python/R for building models. Though it's totally upto you but I prefer python as it has great libraries for machine learning as well as Deep learning. 4) Take part in competitions. We learn by doing not by taking only lectures. I suggest you should join Kaggle and the slack community out there namely, 'KaggleNoobs'. It's a great community. I learn everyday a new thing from there. P.S: Data Science is a vast field. It demands from you various skill set like Data Analysis, Data Visualisation, Machine Learning,etc. So sometimes it can become frustrating too. But once you start enjoying it, you will become a master eventually.