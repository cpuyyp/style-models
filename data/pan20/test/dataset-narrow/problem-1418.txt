returns a for the number of milliseconds since Jan 7, 1970 (~1,425,579,434,000 at the time of this post!), not an since program start, so that might be one problem. Also, minor point of clarity, I'd suggest using something like: 

where is incremented according to the above ranges - ie whether the beat was hit, how much time has passed, etc. 

This is a fairly complex question, so please bear with me. I'm attempting to calculate an edge loop based on the intersection of a plane with a mesh, bounded by a sphere. Here is a mockup of what the expected result might look like: 

Or, as an associative array, such as the .NET Dictionary where the keys encode the XY location and the values are objects containing tile information. Finally, you can use trees such as Quadtrees for arbitrary tile sizes and/or densities. If your map is totally unordered, then a simple graph where each tile has references to its neighbors will suffice. Mostly this depends on what your API provides. If you expect to have a lot of empty space then the associative array is going to be the most straightforward to use, since it can be made to act like a regular 2d matrix, without allocating space for unused elements, as well as having an insert, delete, and remove complexity of O(1). Otherwise a plain 2d array is fine. In general, always pick the least complex data structure for your needs. 

Mismapped UV coordinates. Incorrect image import settings. An image format Unity doesn't recognize. Something else I haven't thought of. 

As the book says, a geometry shader can create new geometry in the pipeline. In this case, quads - which are just four vertices, or triangle pairs, that make up a 3D square. We often use quads to render sprites, which are just images with a position either in 2D or 3D coordinates. In fact, most 2D game engines are actually rendering everything with three dimensions, but using quads facing an orthographic camera. Put another way, since every particle in a system is more or less identical, we just need to feed our image, its size in the world, and all the particle's positions into the shader effect, and it can tell the hardware to come up with all the little billboards with the image properly mapped onto each one. 

The word you're looking for is "ambient light," and is the common term for most game engines. You can find more information about Unity's ambient light, fog, and other global scene settings here: Unity Manual - Render Settings 

Harmonizing the soundscape is another level of polish you can apply to your game, and it works like this: If you're going to be playing a lot of the same sounds over and over, and most action games need to, you don't want it to get annoying. So you fine tune the sounds so that they don't clash but still sound pleasant, or even musical, when played together. Conversely, you can emphasize sounds by using harmony followed by sudden dissonance or vice versa. Horror games often use ambiance in harmonized tones, then make the parts that are supposed to be scary dissonant. Just like we can use music to signal changes in tone, so can we use contrasting harmony. This way we can get quick changes in mood that wouldn't make sense to try with music. So, when the sound isn't clipping, all the recorded audio is high quality and noise free, and they aren't playing the same voice samples over and over, but the game just sounds off, it might just be the harmony. You'll usually find it in low budget games that make heavy use of clips from libraries or found on the internet, with no attention to tweaking those sounds to work together. It's not that bad (compared to some of the other flaws mentioned), but it can make the game sound "amateurish." 

I can't remember where I first heard it, but that's an old rule of thumb that doesn't hold up to much critical thought. Truly, it comes down to business objectives. If you don't plan on selling the game, it doesn't matter. The fact is, the more hardware you can support the bigger your potential market is. However, most independent developers aren't going to have the resources to support anywhere near cutting edge hardware anyway, because in order to take advantage of said hardware they're going to have to expend a fair amount on content. Any independent developer that needs to worry about that usually has the resources to find out what system specs they need to target, and that's likely information you can buy. Steam, for example, collects those statistics and likely resells it to developers. 

You could define how these would affect the character and add flavor text. If you group them into something like biomes, you can randomly select a few at a time for each battle depending on where the battle takes place. For example, if you has just two attributes - Offense and Defense: 

Think of your frustum as a set of planes defined by three vectors each. In your case, 5 planes (left, right, top, bottom, far). A typical frustum has 6 (near, far, left, right, top, bottom) Take the dot product of each plane's normal with the point's location to get the distance of that point from that plane. If the distance is greater than zero, it's in front of the plane. If it's less, it's in back. If all the plane's normals are oriented toward the centre of the frustum, then the point should be in front of all of them. That would indicate the point is inside. Otherwise it's outside the frustum. If the normals are pointed out, then it would be in back. So if it isn't in back of all the planes, then it would be outside the frustum. The normals of the planes will be determined by the winding order of your frustum's vertices. ABC -> normal "out," CBA -> normal "in." Just be sure that you always use the same winding order for plane vertices and everything will be okay. 

You can do this using AssetBundles, although the games would essentially end up merged into one game, and you would host each inside. So imagine building all of your games into one game project, with a special selection stage being the first thing loaded. Then, you'd break up the games into asset bundles. You would then trigger the asset bundle to download when the game was selected, and then you'd load the included Stage asset when it was finished. I'm not sure if this is a good idea, though, because I don't think you can remove those bundles without removing the entire application. 

You can use Blend Shapes for this, which would allow you to account for possibly animation problems in your modeling tool rather than further down the workflow, but if you want Finer Control Identify the regions you want to morph and assign weights to each vertex within those regions so that your modifications can taper off. That part is no different from regular vertex weighting for animation. 

It doesn't particularly matter which you pick, although working with 2D colliders will create fewer potential problems if you don't plan to use that extra axis of movement. Your main problem right now seems to be picking one or the other physics model. Generally, if you're using an image as a game element it isn't part of the UI. You would normally use a sprite instead. But, if your heart is set on using a UI image, you could match a collider's position and shape to it in the place where it appears to be. Or, you could move the world in line with the UI (assuming you're using the new Unity UI, not the old IMHUI). Really though, it will make more sense to use a sprite. 

Optional: Scan from the polygon's center toward the far edge of the image to find the new edge of the pixels, then trace the edge (eg using marching squares) to adjust its bounding box. Note: This doesn't seem work with pre-multiplied alpha images, although I'm probably doing that wrong. 

As far as I know, you don't need a license. But, to be on the safe side... you can actually call the army (as I understand it they assist with other branches as well) and ask questions like these, for free! They can also assist with other things, like where insignia are supposed to go, how they're arranged, uniform standards, etc. Department of the Army's Office of the Chief of Public Affairs 

You can use a model. Often, HUD elements, sometimes including weapons, are rendered seperately from the environment. In the simplest example you'd set up the animation so that the ironsights line up with the zero-axis, and align the model's zero axis with the camera. Using Unity as an example, you'd put the weapon in the HUD layer so that it's drawn with a camera that only draws the HUD layer. Alternatively, you can set up the player avatar so that it lines up the same way, using the same camera. This has the added benefit of displaying to other players what you're doing without having to make a separate model, but re-positioning the gun with respect to the camera can be fiddly. Of course, it's possible to use an image as well. If memory serves, TF2 uses a combination of these - with the sniper rifle animation leading into a sprite overlay with the sights dead center of the screen. 

This is entirely an issue of shader configuration. Without completely reiterating the workflow in the Unity manual, you'll probably want to apply the new Standard Shader and then, using the material charts, and adjust the material's parameters accordingly. 

Since you're using Unity's terrain, you'll have to make some sacrifices. Unity's terrain is semi-procedural and stuck in the Z-Y plane, so you'll either have to: 

Here's my possibly naive way of doing it: I. Create a render target the same size as the image that will be partially erased (should be transparent). I'll call this render target brush. II. Render the polygon, offset relative to the image's origin and size, in white to brush, here shown with checkers and border to indicate transparent section. 

Here's another trick that can be helpful: If you plan to set up a large group of these, you can first test a rectangle that encompasses the component rectangles, then test the interior rectangles individually. 

If you're trying to get a component named "name of object", then that will return a null because there cannot be a component named such. As above you need to Find the GameObject with that name. Then, to access their components, you'll call 

While you're right to use the quaternion as the underlying type for rotation, I'm concerned that you're exposing other fields in C++ which should be handled using getters/setters. The convention for C++ is to expose get and set methods, and leave the particulars of how the field is handled to the class. That is, excepting TDA since this is a component, something like this: 

For we check whether it intersects . Since it doesn't we don't have to check or . intersects , so we have to check and . intersects , so we check and and find intersects it. If contained a lot of rectangles, we could save a lot of time by checking the union first. 

If your heart is set on using Unity 5, then you'll have to take care of these issues on a case by case basis. I suspect that static methods such as have been moved to the applicable object, ie , in order to align with OOP principles. However, I don't have the Unity 5 beta to check, and it seems the documentation hasn't been updated for 5 yet. Update: According to this, Spriter2Unity has been updated to be compatible with Unity 5. 

The rhythm starts immediately Never drops out and comes back off-measure And the tempo never changes 

Note that this is by no means an exhaustive list, since many games use multiple stats to arrive at a final "defensive" result (agility, current speed, movement relative to attacker, flanking) that's applied to any potential damage. What drives the selection of one relationship over another? 

Thus, your component container should be the abstract "Component" type. Behavior specific to each component should be contained entirely within the specific component class. The more important thing to cache friendliness is combining components that need each other's data. Position and speed will usually be accessed very close to each other. Same with position and, say, rotation. So put them all that data in a physics component with the physics behavior. So rather than trying to optimize all the components in aggregate, focus on the actual function of each component. Basically, assume the cache will be wiped between each component update and structure your data accordingly. 

When the player approaches a surface they can take cover behind... A prompt appears indicating they can enter cover, and what to press to do so. If the player presses the prompted button, their avatar is locked into a path following the scenery they're taking cover behind. If the cover is a half wall, the avatar should crouch. If the player approaches a gap in cover the camera should peek around the corner and ...they should be prompted to jump to the next cover ...otherwise, the player can exit cover by pressing another action button. 

which is equal to (10,10) I'm guessing that the problem is a misunderstanding of order of operation. Adding parenthesis to show what happens first, we would have: 

Employing an artist would give your game a specific aesthetic that will help it stand out, but it's perfectly acceptable to use whatever tools available to make your game. Usually the assets produced by the more user friendly tools out there look sortof bland. You can usually tell when someone has been using the out-of-the-box assets from Poser, for example. Unless the class is about 3d modelling, the professor isn't going to care how you make your assets, as long as you don't steal another game's assets and call them your own, or use assets commercially that require a license. Don't be afraid to dig into Blender, either. While you say you're not an artist, you don't have to have amazing realism right away, or ever. Start by making simple props, and keep looking at tutorials to learn new techniques. Your models don't have to be realistic, they just need to be different. Show us things we've never seen before instead of trying to beat people with more experience at their own game. And don't make another zombie/survival/simulator game. 

The earliest, broadest definition of artificial intelligence included (what we now call) calculators. This was refined to exclude calculators because they could not be "taught" how to perform new calculations. Then, (what we now call) computers were invented, which could be reprogrammed to perform any conceivable algorithm. Again, we excluded computers on the basis that they could not program themselves but had to be instructed. We therefore assume that once a system is created which can learn to perform a task on its own (like this program which can learn to play certain classic NES games), we will have conquered AI. Machine learning is its own highly developed domain, but it's usually hard to implement or is very slow. For games where it can be easily run in real time, it often makes games so difficult that they become unbeatable. So to protect our own ego, let's call it a problem of definition.