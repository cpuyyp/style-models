This is a 2-layer network because it has a single hidden layer and an output layer. We don't count the first layer. When we say 3 layers, we actually mean 2 hidden layers and an output layer. Perhaps this helps you? EDIT: We don't count the input layer because there's no parameter (bias + weights). In actual implementation, it's not implemented. Netural network framework simply "connects" the input features to the first adjustable layer (eg: hidden layer). 

It's fine to do a t-test on unequal sample size, however, the power wouldn't be as good as equal sample size. 1:) Yes or no. Impossible to say without plotting the outliers. What's more important, can you assume your data be normally distributed? Have you checked the QQ-plot? Have you checked the histogram? Do they look like close to a normal distribution? While the t-test is robust against non-normal data as long as the sample size is sufficient large, your data shouldn't behave too far away from a normal. When you think about outliers, ask yourself the following questions: 

For instance, 96.42193513 is positive and thus A is the label for AB. Now we have three C, C would be your prediction. If you repeat my procedure for the other two examples, you will get Python's prediction. Try it! 

While I'm not an expert in neural network (experts, please add an answer), I'm sure professional implementation for neural network is tricky and highly optimized. If you simply implement a vanilla text-book-styled neural network, you won't be able to use it to train a large network. Keep it small and you'll be fine. 

You should specify what exactly the model you're thinking. Of course there're differences. For example: 

Playing with the second policy network Our second policy network will need less iterations to pick move 2 because it's prior probability given by the policy network is correct in the first place. Remarks Everything here is very similar to analysis. We start off with some prior probability (given by the policy network), then we generate data to move the probability distirubtion (given by the value network). Summaries 

In an next-generation sequencing, your sequencing machine always give you errors. How reliable is your pipeline to detect those errors? How good are your reads for your problem? For example, if you're detecting disease related to structural variant, you might need more samples to compsenate for the short reads. What's your sequencing depth? In the paper I quote, they have like 30x depth. It'd be pointless to talk about the training size unless you know well about your depth. Does your sequencing cover well your region of the genome you're interested in? Sensitivity and F1 score. You can have good idea by drawing an F1 curve something like as in the paper: 

Learning rate gives the rate of speed where the gradient moves during gradient descent. Setting it too high would make your path instable, too low would make convergence slow. Put it to zero means your model isn't learning anything from the gradients. 

GitHub has built in support for showing a notebook. You will just need to run the notebook yourself, then upload the file to Github like all other file types. Your viewer will be able to see your notebook without any installation. 

Simple google search on "stats why regression not absolute difference" would give you good answers. Try it yourself! I can quickly summarise: 

It's possible. Consider KNN. When you train a model, it essentially "remembers" the training set. Now, when you use the model to predict an unbalanced data set, the model simply read off from the memory, so there's no problem. For example, imagine you train a model with an uniformly distributed age group from 10 to 80. Now you have a test set where everybody is 70-80 years old. KNN simply find out the neighbors for your old people (say 60-80 old people in your training data) and then compute a weighted average. Using a balance training set to predict a imbalanced test set is not super challenging. The other way around is much more challenging. 

Note that our first policy network gives incorrect prior probability for our example. It gives 0.9 for move 1, which is a losing move. This is fine because not even Google could train a perfect policy network. Playing with the first policy network AlphaGo needs to generate a simulation with Monte-Carlo, and it needs to choose move 1 or 2. Now, AlphaGo draws a uniform-distributed random variable, and it'll pick: 

Your problem sounds like the classical top-N personal recommendation to me. There're lots of possibilities in the literature, for example: 

I'd want to add @Neil_Slater's answer by sharing my application. In my application, I want to train a model that can automatically load a chess position from a chess book like this: 

is a good reference. 2. EM algorithm is a technique for estimating the most likely parameters and is not limited to mixed Gaussian. The algorithm is useful when there's no closed form formula for the solving maximum likelihood problem with one or more latent variables. 

Data scientist won't actually make you better paid. You might have seen 150k+ for some data scientists, but those people usually have a Phd and they can get similar salary even they do something else. Data science is a very big field; you can work in the field as a software engineer. But don't expect training a gradient boosting model; you'll more likely be asked to process Hadloop big data application, optimization Python workflow etc. In my experience, a data science certificate is useless. If your next position is a software engineering in data science, you don't need a certificate; you'll just need have a good proven record that you can deal with data. If you next position will be mathematical modelling, you'll need something like a Phd. 

If you want to train on the digit-level, you should define a fixed rectangle size, large enough to fit a single digit in your data set. If your convolutional neural network is good, it should be able to detect your digit. If you haven't you should take a look at the Google paper: $URL$ 

This is a classical example of collaborative filtering, in particular you're talking about user-based collaborative filtering. I don't think Amazon ML supports collaborative filtering ($URL$ but you can do it in R and run your R code on the cloud. It's easy in R, take a look at the package. The vignette has an example with the MovieLens movie rating data set. You may also use the Microsoft Azure ML framework as it has better support for recommender systems. Google "Azure recommender systems" will give you useful links. 

In your question, you have a binary classification problem. I understand what you're asking - you want to know how exactly a network classifies the inputs. Does the network only give (-1, +1) or something else? Most neural networks don't just say "this is upper or this is lower". The networks would give you a probability distribution [0..1] for each possible class. The most common implementation is the popular softmax layer. You'd just choose the class with the highest probability as the prediction. It's also possible to encode your network such that it outputs +1 or -1. In fact, we can always add a layer just after the softmax layer to do exactly that. If you haven't I recommend you to study logistic regression before tackling neural network. 

The correlation matrix is not a reliable measurement for multicollinearity because it only considers the pairwise effects. Unfortunately, multicollinearity is defined as: 

Most modern computers can fit the entire 2.5GB file into the memory. No, virtual machine won't help you, in fact, your RAM memory will just get eaten by the VM. A single 2.5GB file is not generally considered "large" in 2016 standards. 

It looks like counting data to me. Without further information in the question, I'd keep it as a categorical data and model it with discrete techniques (e.g. Poisson GLM) 

In convolutional neutral network, the weights are shared within a feature map. What about two different feature map? How to make them different (so that we don't learn the same thing again). Q: What exactly in the training algorithm to make it so that the weights are different across different feature maps. For example, if I define 2 feature maps, does the network guarantee that the weights are different in feature map A and feature map B? 

To me, this is a crazy idea because there is absolutely no guarantee your "team" can win anything. You'll need to place yourself like in the top-three, but remember you don't have access to the private leaderboard until the end of the competition. Furthermore, the prize money is minimal. The prize money is designed to offer to an individual, not enough for you to make a living. 

Now if I try the easy phase , I get a confidence score 0.535. But now if I try the phase , I also get a confidence score 0.535. Question: How does the Facebook wit.ai work? I don't understand why I would get the same confidence score in both cases. 

This is the scatter plot you have. Your scales are quite large, the most common strategy is to log transform them and thus fit a log-normal regression model. If you do this, your model will improve slightly. My $R^2$ improved from 0.1171 to 0.1873. While it's not a very strong linear relationship, you have some outliers pulled away the OLS fitting at the end of the x-scale. You might want to remove them or fit a robust regression (e.g. L1 cost function). Your data definitely exhibit positive linear relationship, but you'll need to look at outliers. 

Generating a training set requires an expert-domain knowledge, it can be very hard or it can be very easy. Example 1: Web document classification If you're interested to classify a web-document, you'd have billions web pages on the Internet for you to download. The problem is not the amount of data (you just need a web crawler, therefore cheap), but how you process them into something representation that is more manageable. Example 2: Disease classification Collecting disease data could be very expensive. Not only there could be legislation issues, you'd need a team of Phd specialists to analyze the data (very expensive). The experiment must also be sound statistically, for example, you'd have to consider covariate variables. 

You should not use PCA if you only have categorical variables, and thus the distance function in PCA is invalid. Correspondence analysis is a common alternative. 

It was not a hard-problem because it was like training the MINST digits. I collected enough samples, randomly add some noise to those samples. My model was a 2-layer convolutional deep-learning. 

While the other answers aren't wrong, they don't touch anything about bioinformatics. I'll go into details. In bioinformatic, simply asking for the size of the training set makes no sense. You'll need to understand deeper about bioinformatic to answer this question. There is a good reference: An ensemble approach to accurately detect somatic mutations using SomaticSeq for you. The goal of the project is to build a gradient-boosting machine for classifying somatic mutation given an alignment file. This is very close to what you're doing. Now, you want to know the training size? You should ask yourself the following: