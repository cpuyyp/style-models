Example 1: You shouldn't care. The process is much more involved than it seems, and I don't think you have any guarantee. The 3 commands go into the queue in order. The first one is pulled and executed, when the second is attempted, the engine may choose to put it back in queue if it waits for more than some threshold, and the third may be pulled and attempted. if it happens that the resources are now free, the 3rd will be executed first. if not, it may go back in queue again. Example 2: I think the above explains it, and the answer to both your questions is no AFAIK, unless you develop an execution buffer of your own. Don't. Rationale: Now you got to the core challenge, which should really have been the title - "How to deal with deadlocks". You assumed that your 'global lock table' solution is the only way to make things right, but I think the price of losing all concurrency is not worth it. It would be easier just to have the procedures executed serially using a scheduler of some sort. You will open a Pandora box with this approach that will be hard to close. Queuing and syncing is a very complicated challenge. If I were in your place, I would first investigate the root cause of the deadlocks. Sometimes, something as simple as adding or modifying an index can solve it. If you post your deadlock graphs here, along with the involved queries with their execution plans, people will help you pin point the root issue. HTH 

Condsiderations for security options in SQL Server (Two things to mention for typical simple configurations) 

Consider granting VIEW DEFINITION permission as defined in the below quoted reference; this still applies to SQL Server 2012 as well. 

Per the below site and quoted text it does state minimal logging is more effecient than full logging, but it does NOT state that NO logging occurs at all in recovery mode. 

Note, sometimes with the SQL Server and Visual Studio and other components, software, etc. as such, you have to install them in the order based on their version or things in the Windows registry, DLL component, etc. just get out of wack. For example, if you have some component (or software) of the 2005 and 2008 version you need, it's best to install the oldest first (2005) and then once that is complete install the newest (2008). I've seen this too many times where someone installs some old component or software as such on top of newer software, etc. that's already installed then the system start having strange problems. If all else fails, you can backup your important data, and then do a fresh wipe of your HD and install a fresh copy of Windows. Microsoft has no way of emulating all software on everyone computers in the whole world, so perhaps when you upgraded and then downgraded, something just got corrupt and there was something on your system that they didn't test with, etc. I typically always do fresh system upgrades rather than in-place upgrades but I'm in a business environment mainly too. 

All the proposed solutions using COUNT may prove to be inefficient as the aggregates will most likely need to be computed for the entire set, unless the optimizer will be able to push the predicate down. There is a more natural solution which IMHO is much clearer, and typically more efficient. It involves wording the question in negative form: 

Depends on what data you store in it. If you store a null or an empty string, it will take around 2 bytes. If you use all 50 characters, it will take around 102 bytes. See the documentation here. 

The COUNT technique that was suggested here should work, but the problem is that your table structure and data mess it up. You did not define any keys, and you have duplicates in the demo data - for example, course 101 enrollment by student 4 appears twice, on the same date, with different grades. Proper declaration of keys would have not allowed it. Changing the predicate to would also give you the result that you were looking for, taking into account that there may be multiple registrations. The generalized problem you are facing is called 'relational division'. Search here and you will find many discussions on the topic. Typically, the most efficient way to solve relational division problems is to use a negative formulation of the question. It seems weird at first, but once you get the hang of it, you can't go back... In your case the question is: 

Below is a script of the T-SQL part of this process which I run after doing the above 4 steps, and it always works in my environment to accomplish what seems to be similar to what you've explained that you're trying to accomplish. I also have to ensure the AD account has a strong/complex password, and set it to never expire. 

It would seem the explanation would be that your your committed transaction sizes with the operations are HUGE. Simply make the commit\batch size parameters of your logic in SSIS or your execute TSQL in the package of a smaller size. Try testing with 100, 1000, 10000, 100000, and so on to see what gives you the best result and prevents the issue from occurring. If the transactions are smaller, once the committed transactions are committed in recovery mode, then the log space of the committed transactions can be reused by subsequent (or other) transactions. 

Give something like the below a try... You'll obviously need to plug in your variables for your environment, check the data types (may need to add logic to keep leading zeros?), change from the final temp tables to your regular table(s), etc. Works fine for me for import from XML files to temp tables without deleting the files afterwards but adding logic to delete files from the UNC path shouldn't be too difficult with another xp_cmdshell command. 

You concern yourself for nothing. "it's a bottleneck since of each row processing" is an assumption you make, and not necessarily what the optimizer will choose to do, even if it looks to you that you write it that way. SQL is a declarative language, and as such you tell the engine what you want done, and not how you want it done, like you do in imperative languages. The query optimizer, especially in SQL Server, is a mind blowing wizard in moving things around, considering alternatives, and understanding what it is that you want, regardless of how you write it. It doesn't get it 100% right, but it is damn good. Write your query, use several syntax like you did is never a bad idea, and test it against a representative data set. If you see performance challenges, examine the execution plan, or post it here so people can help you. HTH 

You need to be looking at SET Operators. Something along the lines of the following query should do what you are looking for: 

Look at the and you will see the difference. Oracle uses a cost based optimizer that takes into account the actual values that are used in the query to come up with the optimal execution plan. I have not tested it in Oracle, but it is most likely that is evaluated at compile/optimize time, and the optimizer is aware that your predicate only covers one day, and therefor it will make sense to use an index for example. When you 'hide' the value in a function, the optimizer has no way to tell what value you use, as the function is only evaluated at execute time. If the general density of the column suggests that the average value will return many rows, and will benefit from a full table scan for example, you will get a full scan even for 1 day's worth of data, which may be less optimal for the value you are using. As a rule of thumb - don't 'hide' predicate values from the optimizer by placing them in functions. 

Here is what you guys need to do: 1) Launch SQL Server Configuration Manager - you should be able to find this program on your: Start menu -> All Programs -> Microsoft SQL Server 2008 -> Configuration Tools -> SQL Server Configuration Manager 2) Expand tree view "SQL Server Services". On right pane, youl will see the list of Services installed. In my case, I will have to look at the SQL Server 2008 R2 instance. Previously, it was set to run under "NT AUTHORITY\NETWORK SERVICE" 3) Click on this SQL Server instance -> Properties and change the "Log on" to LOCAL SYSTEM. The system will prompt you that it needs to restart the SQL Service. Please do so. 4) Once this is done, you can then backup your database without any hassles! Hope this helps! 

I have a server that is reporting NTFS errors; one of the error messages implicates d:\$MFT as being possibly corrupt. My SQL data files reside on this drive. The Master File Table holds information about all files on the disk, so I imagine if the corruption was to occur SQL may not be able to read data from one of the MDFs or NDFs? I imagine CHECKDB will not be a relevant consistency check, and the only way to resolve this error would be a chkdsk /r? Any help would be appreciated. We haven't had any corruption reports in the SQL error log yet. I imagine it could get serious depending on what parts of the MFT are corrupted (as previously stated it could affect SQL's ability to read MDFs or NDFs)? Thanks 

Continued Trouble If you continue to have trouble you might set the Use SSL field to a value of No and also ensure all the SSL fields below that are blank and then test the connection again. 

Do a FULL backup of the primary DB (now or whenever), copy that backup file over to the secondary server, then restore that to the secondary DB in standby mode. Afterwards, see if your LS jobs run successfully on secondary once you get enough fresh logs to apply to it. 

Check the Event Viewer on the Windows Server which SQL Server is installed for more detail of issues as well Restart SQL Agent Service Check Service Broker if applicable Check DB mail settings, etc. $URL$ $URL$ $URL$ 

Give this a shot since you state brings up a NULL value, despite restarting the server. 1. Run the below with the local as the second argument. . . TSQL 

Below is a TSQL method to find all explicit DB object permissions and members of fixed or custom DB roles per each DB you run it against. Just search the results for the value of the role name you need to confirm the user is a member of beforehand and that should suffice for your need. 

I think it depends on how well you want to lock it down really, and how trustworthy they are for what you are allowing them to have access to. 

I am not sure if this has been addressed already. I have been a long time reader but this is my fifth post. I need assistance with a database restore solution. I receive a full BAK and the transaction logs 30 minutes from a supplier. I would like to restore the DB and process the TRN files nightly. The database has to be in a mode where I can query and insert the new information into a reporting database after the TRN files have been processed and then repeat the process the next night. It seems no matter what I try I get the "Different Recovery Path" error or I can process everything only for that day. I am running SQL Server 2008 R2 standard and have been banging my head on this one for about a week now. Any help or pointers would be appreciated. 

Well the one more reason for this Error i.e. SQL Server VSS Writer. When disk volume is backed up via Windows Server; a replica also known as “Volume Shadow Copy Snapshot” is developed for the same volume. Further, it calls upon the associated VSS Writer and if an error is encountered via any of corresponding VSS Writer, it leads to the failure of the backup operation. Here, the error is encountered by the VSS Writer of the SQL Server environment that causes the failure of entire backup process. 

A few more suggestions—adding as an answer to accept more characters so I can include more. . . BE SURE TO RUN EVERYTHING AS ADMINISTRATOR 

Wouldn't you want to ensure the secondary logs are restored to the secondary server at certain intervals? With log shipping, I thought you ship the logs over to your secondary DB and then restore from those to keep the secondary up-to-date as close as possible to your primary. Do you see any reason why you would not want to automate the log shipping restore on your secondary DB in the log shipping configuration? Once those log files are applied to secondary, then the job will purge the logs that are not needed any longer since those are reflected in the secondary DB at that time. Test it out and see how it goes. EDIT: @nulldotzero Okay, since you cannot run restore operations on primary or secondary in an AlwaysOn configuration, and because you are also copying the TRN log files to the seconary as well as the DR for redundancy, then you could just setup a SQL Agent job on the secondary instance to do the below for whatever hours you feel need to be purged. 

So just as dwjv suggested, set the PATH OS system environmental variable to include the path on the OS where the mysqldump.exe exists ($URL$ otherwise, change the directory in your command window to that path where it is and then run the command.