I took a few days and wrote something to do this. It was not really as straightforward as I was expecting so I'll share what I learned. What was hard Here are the major headaches I encountered. Dealing with JPEG Compression Artifacts When sampling the values for the legend, I encountered many JPEG compression artifacts. This required me to apply a median filter. The media filter width needed to be a parameter of the function (needs different values depending on how big the legend is.) Dealing with "bands" in the legend The legend in your first example is divided into discrete sections (bands) and the numeric quantity desired is probably the middle of the range that the colour spans. For example, the left-most box covers values from -6500 to -6000 and should probably return "-6250" when trying to sample a single colour of that value. Also, consequently, no pixel will return the value "0". There is no colour representation for "0" (in that first map anyway.) Dealing with differences in tone / value. Sometimes the exact colour in the map doesn't exist in the legend, especially due to the text labels, so you have to do a "closest" matching algorithm and the hue seems to be more important than the value (black&white-ness) so it was better to evaluate "distance" between colours in a colourspace other than RGB. Text overwrites data There are plenty of black pixels in the map due to the labels, but these need to be ignored somehow. It makes sense to try to infer what the value behind the text is, and only colours that are not in the legend qualify for this kind of inference. Manually selecting points on the legend is not precise Each time I eye-balled it and manually found a point that looked like approximately the end-point, I'd choose a slightly different pixel, resulting in slightly different values between attempts. Perhaps a snapping technique would be better where I could "hint" where the legend is, but it would find the high-contrast edge markings and snap the hint point to the exact point. Most maps don't have continuous legends. Most other images that I went searching for with test data had discontinuities in their legend (discrete boxes of colour) as opposed to a nice continuous linear gradient like you provided. This makes it extra difficult to automatically come up with the mapping from colour to value just by analyzing the positions of the colours in the legend. I was disappointed when trying to apply it to most maps. Visualization tips It was very useful (especially while debugging) to visually show the point on the legend that it thought best corresponded to the colour being picked in the map. Persistence It's useful to persist the location of the endpoints of the linear gradient portion of the legend and the values that are associated with them between runs of the application. Don't underestimate the value of this. It's annoying to have to specify all that stuff every time. What worked well Represent the legend as all the pixels along a line segment Specify two (x,y) locations of the start and end of a linear gradient (the legend) and manually define the values that are associated with them. build up a discrete sample set for the gradient Sample all the pixels in between the start and end points to build a linear gradient dataset. Median filter Apply a median filter to the linear gradient dataset to filter out JPEG noise. Quantize the dataset to locate centroids Optionally filter out bands (long runs of samples in the linear gradient dataset that have approximately the same colour) and replace them with a single sample at the centroid (average position). Use bilinear sampling when picking a pixel Pick a pixel in the map and perform a bilinear sample to smooth out pixellation. This seems to help improve the smoothness and resolution of the map. Linear search is fine Linearly search for the data point in the linear gradient for the closest colour. For an interactive picker working from mouse clicks, you have lots of time to perform a linear search. No fancy hashing, indexing or optimized searching was required. Hue is important in the colour distance function Compare distance between colours using a distance function in a colourspace that tolerates brightness difference more than hue difference. (like YUV, for example) 

Most normal distribution functions (NDFs) are parametrized by some variable (tipically $m$ or $\alpha$) that determines the "roughness" or "spikiness" of the NDF (this is often meant to be the rms slope of the surface). Here is an example of how a roughness parameter effects an NDF: $URL$ we can think of the NDF as a function of the direction as well as of the roughness value (as in $D(\theta,\phi,m)$. Here, $\phi$ and $\theta$ were used to represent spherical coordinates for the microsurface normal direction). A roughness map can also be thought of as a function: one that returns a value for $m$ for every point $p$ on a surface. Hence, $m(p)$. Since $m$ is an argument of the NDF we can write the previous expression like this: $D(\theta,\phi,m(p))$ And, because $m(p)$ may be different for every $p$ but stays fixed when we only vary $\phi$ and $\theta$, we effectively get a whole different NDF on every point on the surface. 

Now, when we generate samples for Phong shading wo do not generate them in this space. Rather, we use a different reference system where the Specular reflection direction is the 'z axis', thus, our angle $\theta$ is the angle between the reflected direction and the sample direction, and the angle $\phi$ is the angle between some $S_{ix}$ that is the projection of our sample direction onto the plane perpendicular to $R$ and some arbitrary but constant vector perpendicular to our reflected direction 

When applying various types of attenuation you are just multiplying by some number smaller than 1. This is akin to every atom of your material having some probability of absorbing the light that hits it. Something to note is that these calculations should be performed ( in the context of gamma correction and colour spaces ) in a linear space. If you don't perform the calculations in linear space you might end up with overly bright, overly dark, overly saturated or under saturated colours. 

I'm just going to add a few more details to the paper that Stefan linked to. First of all, the matrix $H_1$ displayed in the paper is incorrect; every element should be multiplied by a factor of $\frac{1}{8}$. $$ H_1 = \frac{1}{8} \begin{bmatrix} 4 & 4 & 0 & 0 \\ 1 & 6 & 1 & 0 \\ 0 & 4 & 4 & 0 \\ 0 & 1 & 6 & 1 \end{bmatrix}$$ Second, the derivation of the new face point formula for quads in the first half of the paper is confusing, so I will fill in some of the details. The matrix $G_1 = HGH^T$ contains the 16 control points for one sub-quad of the original quad with control points $G$. The entries of $G_1$ are referred to with $q_{i,j}$. Of most interest are the elements $q_{1,1}$, $q_{1,2}$ and $q_{2,2}$, which represent control points for a corner, edge, and interior of the $4\times 4$ control point grid for the sub-quad. All the other elements have similar formulas due to the symmetry of the subdivision process. By carrying out the matrix multiplication, we get: $q_{2,2} = \frac{P_{1,1} +P_{3,1} +P_{3,1} +P_{3,3}}{64} + \frac{6(P_{1,2} +P_{2,1} +P_{3,2} +P_{2,3})}{64} + \frac{36P_{2,2}}{64}$ We can see from this formula that, when relocating a control point, its new location is determined by the $3\times 3$ grid of points surrounding it. Here we see that in this grid, corner points have $\frac{1}{64}$ influence, edge points have $\frac{6}{64} = \frac{3}{32}$ influence, and the center ("old") point has $\frac{36}{64} = \frac{9}{16}$ influence. These weights are consistent with the ones given at the bottom of this blog post. With a bit of rearranging, we get: $q_{2,2} = \frac{1}{4} \big[ \frac{1}{4}(\frac{P_{1,1} + P_{1,2} + P_{2,1} + P_{2,2}}{4} + \frac{P_{1,2} + P_{2,2} + P_{1,3} + P_{2,3}}{4} + \frac{P_{2,1} + P_{2,2} + P_{3,2} + P_{3,1}}{4} + \frac{P_{2,2} + P_{2,3} + P_{3,2} + P_{3,3}}{4}) + 2\frac{1}{4}(\frac{P_{1,2} + P_{2,2}}{2} + \frac{P_{2,1} + P_{2,2}}{2} + \frac{P_{2,3} + P_{2,2}}{2} + \frac{P_{3,2} + P_{2,2}}{2}) + P_{2,2}\big]$ The first term is merely the average of the four surrounding face points; in other words, it is $F$. The second term is the average of the four surrounding edge midpoints multiplied by $2$; in other words, it is $2R$. Finally, $P_{2,2}$ is simply the original control point $P$. And so we see this is the same as the original formula for quads: $\frac{1}{4}(F + 2R + P)$. The paper does not provide a derivation of the general case for an $n$-sided polygon, but it seems like a reasonable extension based on intuition.