Before I start, I will say that I am not familiar with wampserver but I am familiar with MySQL and LAMP. First things first, are your tables within your databases MyISAM, InnoDB or a mixture of both? - You can tell on the file level by looking at the suffixes. MyISAM tables have the table name with the following suffixes: 

Your service can remain the same apart from changing it to look at . Once they are all up you will need to issue the command in order to tell the pods they are now working as one replica set, details of this can be found in the mongodb documentation: $URL$ ** Note, I have copied the example yaml's from my sharded cluster that has been deployed through kubernetes which means I have removed some parameters so there could potentially be a typo, also you might want to scale down the persistent volumes and memory limits before proceeding. 

I have a Sharded Cluster and I am trying to access a specific member of a replica set within a shard. When connecting to the mongos it is fine, but when using the same credentials on a member of the replica set I am unable to gain access even though my user has the role of root. The error is: 

Review indexing on cours table as you're getting a full table scan there. $URL$ Great indexing tips there ^ 

You can write a simple script externally or use the MySQL event scheduler. Both will work just fine. The benefit of the MySQL event scheduler is that it will be part of your database backups. 

Joel, there is a lot of unknowns. If you post your my.cnf, queries, table definitions and queries, EXPLAIN plan output, then you might get more precise answers. You say "disabled query caching and other caching" what 'other caching' are you referring to? What steps did you take to disable QC? Did you restart your MySQL server between tests to flatten any buffers? If no and the KVP table was smaller and fit into RAM then InnoDB might have been able to serve queries fitting a certain criteria from data in the innodb buffer pool. You also refer to memcache. With MySQL 5.6 there is a memcache api. this means you can use memcache calls to access data stored in innodb. This will not be incidentally speeding up your queries. There is also another method of bypassing the SQL layer within MySQL called HandlerSocket. These are a couple of subjects that I would point you at should you continue to desire this type of access to your data with the added bonus of having MySQL persist the data to disk and also be crash safe through InnoDB's crash recovery process. 

On the topic of blocking processes, below is a quick and dirty way ofrecording blocking - --CREATE A TABLE TO HOLD PROCESSES WHICH BLOCKED 

I guess the answer boils down to whether or not makes sense for your situation, it depends. For your situation, does it make sense for the rows in the 'child' table to remain if their corresponding 'primary' rows go. Would the data in the child table be meaningless without the parent? If so then you a cascading delete would enforce referential integrity. You might want to keep child rows as a record, an archive of past activity (though potentially you could write these rows to another tables specially for this purpose. A example I used to illustrate this point was the doctor/patient relation. One doc can have many patients. A patient can only be a patient if they have a doctor. If the doc goes (leaves the practice) then something has to happen to the remaining patients. One possibilty is that they get purged another is that a default value replaces the doc reference or they could be removed from the main table and placed somewhere else. Alternatively no activity occurs and the patients remain as they were, untouched, as though the doc was still present. It depends what you want to do. From personal experience think carefully about the design of the database. This week I had to run a purge of orphaned records in a table which pointed to nowhere and was literally taking up space. Hope this helps. 

Now try pinging and seeing if you have connection, if so then MySQL should work, although depending on setup you may have to comment out the from the config. 

I recommend looking at the documentation on how to do a rolling upgrade across the board, but one step which caught me out when doing the rolling upgrade was the addition of the parameter which I used, or you can use parameter that is in the upgrade documentation. You need to use one of these parameters for MongoDB 3.6, this is required for configs, mongos and mongods. Would be good to see and example of the errors you are getting. 

I believe this is because you have more than one pod trying to use the same using the same data directory which you can not do if the pod is running mongo, one data directory has to be exclusive to one mongo instance. If the goal is to create a group of pods with the same data then you are looking at creating a replica set. In order to do this first you will need to create multiple persistent volumes as so: 

Then you will want to create a statefulset binding to the persistent volumes which means that when the pods go down and come back up they attach to the correct persistent volume, the yaml will look something like this: 

Slave_SQL_Running: No The above indicates that although the replica is connected to the master it is no longer applying SQL statements to the dataset. This is due to the problems exposed in the 'Last_Error' field. You should resolve these issues either by aligning the data or skipping the transaction and syncing the data thereafter. You've likely arrived at this scenario because of data or database object inconsistencies between the nodes. You should explore monitoring the replication health using any of the popular methods for example using Nagios and Percona's Monitoring Plugins for MySQL. There is a broad choice in this space. 

Slave_SQL_Running_State: Reading event... Suggests that it's working on a large event. If you have MIXED or Row based binary logging and have changed a large amount of data on the master you might end up with a large event or series of large events. Use the mysqlbinlog tool to see what's going on with your relaylog. The master might show a concentration of binary logs over a small period of time (on the filesystem) 

It could be related to indexes. Think about how a constraint could affect inserts. PK is unique by design and you have three columns and with your 3 col PK MySQL needs to check each time a new row is inserted whether that PK is unique. Why can you not simply use the 1st column as your PK? Secondary keys need to be updated when you add new data, ensure that you have expand_fast_index_creation turned on. FK relationships need to be maintained based on their inclusion in the table. What rippling effect does your INSERT have on related tables? On the other hand it might be due to your settings. Check and tune the following: innodb_buffer_pool_size innodb_log_file_size innodb_flush_log_at_trx_commit Some tweaks to these could spin you up a few notches. 

Straightforward this one - when failing over from one sql server 2008 node to another, should the browser service be running on the passive node ? At the moment, on attempting to failover, a an error is produced saying that certain clustering related dlls can't be found. The dlls are present, I just wondered whether this browser service was stopping access. 

Add the stored procedure to a job, run every 5 minutes or however long you want t check. Not perfect gives you a quick overview. 

I assume you're using SQL Server (T-SQL reference there). When you execute the T-SQL you can choose to include the actual execution plan, this will allow to see how SQL went about its execution, including whether a scan took place. Within this execution plan, SQL may even give hints on how to improve the performance which you may or may not decide to take up. $URL$ $URL$ Hope this helps! 

A node was evicted from a windows cluster without first uninstalling the sql first. When the node was put back, an error was produced when we tried to failover. It was reporting a missing dll. Feels like removing the node before uninstalling sql from it has corrupted or deleted part of the build. Has anyone else encountered this situation? Can this be repaired or is it best to uninstall the sql from the node and start again installing a new sql node. Thanks! 

Get advice from the vendor. If you're using the commercial product you're paying for expert advice from Oracle themselves. 

There are multiple audit logging products available. If you are using Percona or MariaDB flavour of MySQL you have the option of their plugin. If you are using Oracle MySQL you can pay for their enterprise version of audit plugin (as part of Enterprise Edition). There is also an audit plugin from McAfee that will fill this requirement and is generally available cross-alternative and from 5.1+. These products permit you to log both logins and queries. Finally there's a plugin to track logins only from a community contributor. Links to all below. $URL$ $URL$ $URL$ $URL$ 

This all depends on the size of the table. You're doing a full table scan without a where clause so an index isn't going to help. The use of the rand() function for sorting is going to produce a temp table and this will harm performance. If you have a primary key you could generate a random number in the app and perform a query where a single row is selected and can make use of the clustered index. You're likely to be serving this all from memory if you're using the InnoDB storage engine.