The traditional approach to dependent types, however, allows for arbitrarily complex constraints on types, with the following caveat: the onus of verification is placed on the programmer rather than on the compiler. In this setting, an element of is not an integer less than 2, but a pair where is an integer and is evidence that holds. Therefore the compiler only has to check the evidence at compile time, rather than building it herself. Note however that some computational inferences can be performed, typically $1+1$ can be seen to be identical to $2$ (but $x<2$ can not be seen to imply $x\leq 2$ without some contribution from the user). The obvious advantage is that arbitrarily complex constraints on the code can be verified, and the programmer may have access to automatic tools to assist the production of the required evidence (the famous "tactic language" of Coq is an example of this). This is the main difference between refinement types and dependent types. The Haskell extensions tend to be in the second category, where evidence has to be provided by the programmer. I do need to qualify all this: there is no hard line which differentiates refinement types from dependent types; in dependent types there may be a significant amount of information inferred by the compiler, and systems with refinement types may require information to be supplied by the developer. In fact the interaction between the communities contains IMHO some of the most interesting developments of type theory applied to practical software engineering. 

You can get a better ratio between $c$ and $s$ if you assume the Unique Games conjecture: Khot, Kindler, O'Donnel, and Mossel showed that for $\Pi$ the max cut constraints $x_i \neq x_j$, solving GapCSP$_\Pi$($\frac{1}{2}(1 - \rho)$, $\frac{1}{\pi}\arccos(\rho) - \delta$) is as hard as Unique Games for any $\rho \in (-1, 1)$ and any constant $\delta > 0$. Optimizing over $\rho$ gives $\frac{s}{c}$ arbitrarily close to the Goemans-Williamson constant. 

Associate with $S$ and $S'$ the posets $P = (\leq, S)$ and $P' = (\leq, S')$, where $\leq$ is the dominance order in $\mathbb{R}^d$. The function you describe is an isomorphism between $P$ and $P'$. 

Just to add to Domotor's answer, any hyperplane whose normal vector has polynomially bounded coefficients (in fact bounded by $2^{o(n)}$) contains an entire face (i.e. subcube) of the cube of dimension $n - o(n)$. This follows from the Sauer-Shelah lemma. To be precise, say each $|a_i| \leq n^c$. Then for any $I \subseteq [n]$, $\sum_{i \in I}{a_i} \in \mathbb{Z} \cap [-n^{c+1}, n^{c+1}]$. So by averaging there exists some integer $b: |b| \leq n^{c+1}$, for which $\mathcal{I}_b = |\{I: \sum_{i \in I}{a_i} = b\}| \geq 2^{n-1} n^{-c - 1} = 2^{n - o(n)}$. Then according to Sauer-Shelah, there exists a set $S \subseteq [n]$ of size $n - o(n)$ shattered by $\mathcal{I}_b$, i.e. for each $T \subseteq S$ there exists $I \in \mathcal{I}_b$ such that $I \cap S = T$. This means that for any $\epsilon \in \{-1, 0, 1\}^S$, there exists a $\delta \in \{-1, 0, 1\}^n$ such that $\sum{\delta_i a_i} = 0$ and $\forall i\in S: \delta_i = \epsilon_i$. In other words, the hyperplane normal to $a_1, \ldots, a_n$ contains the entire subcube corresponding to $S$ (i.e. the projection of the full dimensional cube onto the coordinate subspace corresponding to $S$). 

The domain and model theories of PTSes hasn't been explored all that much I'm afraid. One detailed source is Thomas Streicher's PhD work: Semantics of Type Theory. He gives category theoretic semantics for all PTS, though I'm not sure he addresses $\eta$-conversion. 

It is not provable without additional axioms. In fact, it implies double negation elimination (take $B=\top$), which in turn is equivalent to the excluded middle. 

To my knowledge, no machine checked proof of a complex mathematical development has ever been retracted. As Andrej points out though, it occasionally happens that soundness-breaking bugs do crop up in these systems (though usually not silently, as Andrej suggests), and the fix to that bug involves some changes to existing proofs, or, more likely, of the standard library of the proof system involved. Some examples of such library breaking proofs in Coq: $URL$ $URL$ It's hard to say whether the established proofs depended on the inconsistency, since after the fix, they required minor tweaks to be accepted by the proof checker. But this happens at each non-trivial update! My personal opinion is that such mistakes are unlikely to happen, since the paper proof needs to be well polished before machine formalization can even be attempted. Inconsistencies in proof frameworks usually require the heavy use of strange combinations of esoteric features, and so very rarely crop up "by accident". 

any set system of $m$ sets has an $\epsilon$-net of size $O(\frac{1}{\epsilon} \log m)$; an $\epsilon/2$ net of an $\epsilon/2$-sample for $\mathcal{S}$ is an $\epsilon$-net for $\mathcal{S}$; 

Yes, there is a connection between the spectrum of the graph and the size of the maximum cut. It might be easiest to see this with the normalized graph Laplacian, $L = I - D^{-1/2}AD^{-1/2}$, where $I$ is the identity, $A$ is the adjacency matrix of the graph $G = ([n], E)$ and $D$ is the diagonal matrix defined by $D_{ii} = d_i$, the degree of vertex $i$. The largest eigenvalue of $L$ is $$\lambda_{n-1} = \max_x\frac{x^TLx}{x^Tx} = \max_{x}\frac{\sum_{(i, j) \in E}{(x_i - x_j)^2}}{\sum_{i}{d_i x_i^2}}.$$ Because $(x_i - x_j)^2 \leq 2x_i^2 + 2x_j^2$ by Cauchy-Schwarz, $\lambda_{n-1} \leq 2$. Equality is achieved if and only if for each edge $(i, j)$, $x_i = -x_j$, which means that $\lambda_{n-1}= 2$ only if the graph has a bipartite connected component, i.e. a connected component whose maximum cut is equal to the number of edges in the component. For a robust version of this result, see Luca Trevisan's paper. He shows that $$ \lambda_{n-1} \geq 2\frac{\text{MaxCut}}{m}, $$ (where $m$ is the number of edges of the graph) and if $\lambda_{n-1} \geq 2(1-\epsilon)$, then there exits a set of vertices $S$ and a partition $(S_1, S_2)$ of $S$ such that $$ |e(S_1, S_2)| \geq \frac{1}{2}(1-\sqrt{16\epsilon})\text{vol}(S), $$ where $\text{vol}(S) = \sum_{i \in S}{d_i}$ and $e(S_1, S_2)$ is the set of edges with one endpoint in $S_1$ and the other in $S_2$. Note that the second bound does not necessarily lower bound the max cut in a useful way: it only lower bounds the maximum cut of the set $S$ in terms of $\text{vol}(S)$. This is unavoidable: think of a graph that's the disjoint union of an edge and a large graph with small maximum cut, say a clique. The maximum cut of the graph is very close to $m/2$, which is the minimum possible value of the max cut of a graph with $m$ edges. However, $\lambda_{n-1} = 2$, the maximum possible value of $\lambda_{n-1}$. In other words, while $\lambda_{n-1}$ is a relaxation of the maximum cut (up to appropriate rescaling), the gap is arbitrarily close to 2, and a 2-approximation to max cut is trivial. Nevertheless, in the paper Luca uses the spectral result to design a recursive approximation algorithm for max cut. This is one cool thing about the paper: it uses a relaxation that, unlike the SDP relaxation of Goemans and Wiliamson, has trivial gap, but despite that an optimal solution to the relaxation actually provides enough guidance to achieve a non-trivial approximation factor. 

Edit: Elaboration on the 4th point: Wadler defines, among other things, the Reynolds embedding from $\lambda_2$ to $\lambda\mathrm{PRED}_2$ with higher-order functions (Figure 4). This embeding sends a type to a proposition (the parametricity theorem) and sends a well tyepd term to a proof of parametricity of that same term. The theorem you get implies "type correctness" of the defined function rather trivially. In fact I think Wadler treats this in detail in Section 5, and treats the $\mathrm{add}$ example in the appendix. The only subtle point is that the function is not directly defined over the inductive type, rather it takes the type and constructors as arguments. At this point I really have to suggest you work through some examples. 

This is useful for expressing pre and post-conditions on your code. Again, a trivial example would be the function, which only makes sense on non-empty lists. 

Andrej answer covers uses of extraction, but as far as expressiveness goes, I believe that having impredicative Prop leads to a system that is strictly stronger than Agda. In fact "Martin-Löf type theory with universes" is sometimes called "Luo's predicative UTT" One subtle issue is induction-recursion, which gives Agda significant power and seems to be absent in Coq. However there is a trick by V. Capretta (It's likely been independently discovered) which allows expressing such definitions in Coq. 

The Berry-Esseen theorem can give tail probability lower bounds, as long as they are higher than $n^{-1/2}$. Another tool you can use is the Paley-Zygmund inequality. It implies that for any even integer $k$, and any real-valued random variable $X$, $$ \Pr[|X| >= \frac{1}{2}(\mathbb{E}[X^k])^{1/k}] \geq \frac{\mathbb{E}[X^k]^2}{4\mathbb{E}[X^{2k}]} $$ Together with the multinomial theorem, for $X$ a sum of $n$ rademacher random variables Paley-Zygmund can get you pretty strong lower bounds. Also it works with bounded-independence random variables. For example you easily get that the sum of $n$ 4-wise independent $\pm 1$ random variables is $\Omega(\sqrt{n})$ with constant probability. 

Indeed, if the above is true, $\mathsf{NP} \subseteq \mathsf{P}^{\oplus \mathsf{P}}$ (a comment by Joro suggests that this is what he actually meant). Since $\mathsf{P}^{\oplus \mathsf{P}}$ is closed under complement, it follows that $\mathsf{coNP} \subseteq \mathsf{P}^{\oplus \mathsf{P}}$ holds as well. If a derandomized Valiant-Vazirani theorem holds relative to $\oplus P$, i.e. with the 3CNF formulas augmented by $\oplus P$ predicates, then, using Fortnow's argument in his simplified proof of Toda's theorem, we would get $\mathsf{PH} \subseteq \mathsf{P}^{\oplus \mathsf{P}}$. The usual randomized Valiant-Vazirani theorem implies a randomized version of this: $\mathsf{PH} \subseteq \mathsf{BPP}^{\oplus \mathsf{P}}$. This is one of the lemmas used by Toda. Note: My original answer had a bug, thanks to Emil Jeřábek for pointing it out. 

You can try the Coq user manual, in particular this section is pretty nice (the server seems to be down at the moment though). For meta-theory you can try some recent work of Benjamin Werner et al., see Proof-irrelevant model of CC with predicative induction and judgmental equality and On the strength of proof-irrelevant type theories for the most salient work. 

Here's a quick sketch to show that there is no Turing machine to decide whether an arbitrary class of problems is decidable. I should clarify what I mean by class of problems: a class of problems $T$ is a Turing machine which enumerates the elements (natural numbers, say) of a recursively enumerable set one after the other, such that each element in the set is eventually printed. The problem intuitively captured by $T(n)$ is: "is the number $n$ in this set?". This captures the usual problems in the field of computability, such as "is i the index of a Turing machine that halts on empty input?". Suppose there was machine $M$ which, given as input a class of problems $T$ answered $\mathit{true}$ if that class is decidable and $\mathit{false}$ otherwise. Now take an arbitrary Turing machine $T$. We build the following class of problems $T'$ in the following manner: 

The second condition can be read as: if $a$ is not a normal form, then there is some rewrite step from $a$ which conforms to strategy $S$. TERESE (9.1.12): If $A$ is a superset of the set of normal forms for $\rightarrow$, a strategy $S$ is $A$-normalizing if the restriction of $S$ to terms not in $A$ is normalizing. This means that a strategy is $A$-normalizing if every reduction sequence for $\rightarrow_S$ either 

If I am understanding your problem correctly, this is the problem of computing the face containing a given point in an arrangement of line segments. There is a randomized algorithm running in expected time $O(n\alpha(n)\log n)$, where $n$ is the number of line segments, and $\alpha(n)$ is the inverse Ackermann function. The algorithm computes the boundary of the face, and from that you can list the grid points if you want using a sweep algorithm, but the time to do that will of course depend on the number of grid points inside the face. 

Field theory and algrebraic geometry would be useful in topics related to error correcting codes, both in the classical setting as well as in studying locally decodable codes and list decoding. I believe this goes back to work on the Reed-Solomon and Reed-Muller codes, which was then generalized to algebraic geometric codes. See for example, this book chapter on the classical coding theory view of algebraic geometric codes, this brief survey on locally decodable codes, and this famous paper about list-decoding Reed-Solomon and, more generally, algebraic-geometry codes. 

As in my comment, the configuration graph (Chapter 4.1.1. of Arora-Barak) is a simple way to view a Nondeterministic Turing Machine (NTM) as a graph. The vertices of the graph are labeled by configurations: the internal state of the NTM, the contents of the tape, and head positions. There is a directed edge from vertex $c_1$ to vertex $c_2$ if the NTM can move from $c_1$ to $c_2$ in one step. This is especially useful for arguing about space-bounded computation. If the space used by the NTM is bounded by $s(n)$, then on input size $n$ you only need to consider a configuration graph with $O(2^{s(n)})$ vertices. Then a lot of complexity results follow from algorithms for directed $s$-$t$ connectivity: Savitch's theorem (NPSPACE = PSPACE) follows from an algorithm that uses $O(\log^2 |V|)$ space, PSPACE in EXP (or, scaling down, L in P) follows from DFS, etc. Similarly, you can show that directed connectivity is complete for NL under L reductions, so NL is equal to L if and only if there is a logspace algorithm for directed $s$-$t$ connectivity.