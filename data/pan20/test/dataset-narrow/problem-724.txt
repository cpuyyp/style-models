But the code I've written above isn't quite what you want—we still need to fix a couple more things! The first issue is that it will display a message box containing the numeric ID of the problematic column, rather than the alphabetic ID displayed in the Excel UI. For your user's sake, you probably want to change this. Since you're using a loop with the numeric column ID as the loop counter, you need an algorithm to systematically convert that numeric ID to the alphabetic ID. I would define a reusable function for this. The following function is adapted from one appearing in a Microsoft support article: 

Notice that each column follows the truth table for an XOR operation, which basically just says that the output is 1 (true) whenever the inputs differ. That's the "exclusive" part of the OR. Eventually, while doing this preliminary research, I suspect you'd come across someone talking about how XOR operations are used to implement parity checks. Mast's answer here already spilled the beans. Parity indicates whether an integer is even or odd, and can be calculated simply by a XOR sum of the bits. Judging from your comments in the code, you already know that for a binary number, the lowest (least significant) bit determines whether it is odd or even. So the good news is that your code is almost entirely correct. If you change the s to s, it will follow the rules of the assignment and produce the correct result. Thus, you would have: 

The Fundamentals: Style & Readability Before we get into optimizing, there are a couple of glaring problems with your coding style—that is, the way the code is written and the way it looks. 

When I see this question, I immediately understand what you are trying to do as a bitwise rotation (also known as a circular shift). You need to either rotate the bits in the value left by 24 or right by 8 (they yield equivalent results). As the bits are shifted off one end, they're inserted back in the other end. Ironically, this is one of those rare cases where the code is conceptually simpler in assembly language (!) than it is in C#. The x86 processor family has two very simple instructions that accomplish precisely this feat— and —which rotate left and right, respectively. That means the code in assembly language is almost as simple as: 

Critically missing here is the word interpreted. The language's level of abstraction does not matter as long as it is compiled to machine code, preferably by an optimizing compiler. Theoretically, you could write this in any high-level language of your choosing and it would be translated to assembly code that is equivalent if not even more optimal than what you've written by hand. In fact, to demonstrate that, here is a C++ translation of your QBasic code for : 

Note that I've explicitly used the type here, instead of the short name . This is because I've hard-coded assumptions about the type's width, and I like things to be obvious. is obviously always a 32-bit type. At least for me, since I bounce back and forth between languages, that's not always so obvious with . I've also thrown in an assertion to check that we're rotating by a reasonable . That's just an assertion, though, not an exception, because the C# language standard guarantees that the shift amount will be masked, such that only the lower 5 bits will be used (resulting in a value from 0 to 31). This means that the language protects you from the undefined behavior you would get in C by trying to shift by too many places. (Again, ironically, so does x86 assembly language.) So this is basically the generalized form of the code that you have. A call like: 

The code can be improved by writing simple C++ wrapper classes to implement automatic resource management, as was discussed above. You should also add error-checking. To really squeeze speed out of it, you'll need to resort to some ugly tactics. As written, this does much the same thing as the Win32 function, and like I mentioned above, your setup is such that you need to call this function repeatedly. The obvious way to make code execute faster is to reduce the amount of code that you have to execute. Here, that means factoring out common tasks and doing them only once: 

Although the other answers made some good stylistic observations and suggestions for improvement, many have missed a critical bug lurking in this code. Here's the offending line—see if you can spot it: 

And here is the simpler code that doesn't use exceptions. Notice that it is exactly the same logic as before, explicitly detecting the error and handling it, but it doesn't use exceptions for flow control—just normal flow control structures like loops (which we're using anyway!): 

The Main Event: Optimizing At the core of your code's logic is a call to to retrieve the current color value of a particular pixel on the screen. That is unfortunate if you're concerned about speed, because is a very slow function, indeed. Why is it so slow? The above-linked documentation doesn't give much of a hint. In fact, this API has a dirty little secret: individual pixels cannot be directly manipulated by the graphics subsystem, so (and its brother, ) have to simulate it, which makes an ostensibly simple, straightforward function call into an extremely slow operation. In addition to the usual overhead of a function call, parameter validation, and switching to and from kernel mode, the function also has to create a temporary bitmap, copy the contents of the DC into that temporary bitmap, perform all necessary color-mapping, map the coordinates and locate the pixel, retrieve its color value, and then destroy the temporary bitmap. A lot of work for a single pixel! The throughput of is on the order of 100,000 pixels per second, which is usually fast enough. If you want to speed up drawing operations that involve the manipulation of multiple pixels, the general strategy is to minimize overhead by copying the DC's contents into a temporary device-independent bitmap (DIB) and then using pointer arithmetic on this DIB to access individual pixels. This gives you direct access to each pixel, and allows a throughput of millions or more pixels per second. The problem for you is that you cannot simply use a cached snapshot of the screen—you really do need to update it each time through the loop, which means that this strategy doesn't save you a whole lot in terms of overhead. But it'll still be slightly faster. Here's an approximation of the code that would be required: 

This works, but aside from the readability concerns (which could be addressed by a comment), it is slower than the original. Even an optimizing compiler has little choice but to transform this into a nearly-literal sequence of machine instructions. A better solution is to do the computation with an unsigned integer value (in an unchecked context, of course). That way, you'll get the wrap-around behavior as desired, and the integer being treated as unsigned ensures that the result won't be interpreted as a negative value. The code remains essentially what you had, except with the addition of some ugly casts to force the intermediate arithmetic operations to be done on an unsigned integer, and then to cast the result back to a signed integer: 

Simpler, clearer, faster. Rarely is it possible to say this about any piece of code, but I believe this really is the most optimal way to write this. 

Now we've XOR-summed all of the bits from the three inputs. All that's left is figuring out the parity of the result—i.e., whether the XOR-sum is odd or even. You've already implemented one way of doing that, but as Quuxplusone pointed out, it is an unnecessarily complicated way. While it doesn't always hold when you start getting into more advanced things, for simple arithmetic operations, fewer instructions means faster code. Arguably more importantly, it means simpler code, which is more likely to be correct code. Moreover, fewer branches virtually always mean faster code, and certainly code whose flow of execution is easier to follow, and thus easier to debug. I'd disagree slightly with Quuxplusone here and say that clever is totally fine, as long as your cleverness has some notable advantage. You don't always have to write code the "normal" way, because the "normal" way might be sub-optimal. Generally, if we're dropping down to write in assembly, it's because we want to write the best code we possibly can (either fastest, shortest, or whatever metric we are using to judge "best"), which means that "normal" isn't necessarily an important goal. Sometimes, "readable" isn't even an important goal. But, by the same token, I do agree there's no point in deviating from what is normal if your deviation is inferior, and that's certainly the case here. Your XOR-sum is in the register. You know that the XOR-sum tells you the parity in the least-significant bit. So what is the obvious thing to do? Mask off everything but the least-significant bit, and that'll be your answer! How do we mask off bits? Use a logical AND operation: 

One thing you are doing well, though, is following a consistent style throughout. While K&R is not my preferred brace style, you are using it consistently, and that is really what's most important. Taking these points into account, here is how I would revise your current implementation: 

(We could have just as easily done the first, before the . It doesn't matter.) Go through your current code, and prove to yourself that this gives exactly the same results, without needing to flip the bit with the , without needing to involve the carry flag (CF) via , and without needing to do any conditional branching (). Putting it all together, the code is essentially: 

The standard (and recommended) way to solve this is for each module to provide its own exported and functions (or whatever you want to call them—the names are not important). This is what Windows DLLs do (for example, the Network API functions provide and to handle memory management), and you should strongly consider following the same model in your own project. Regardless of their names, the implementation of these functions is downright trivial. just needs to call (or if you're using C), and just needs to call (or if you're using C). They are just wrappers. Header: 

You then follow the simple rule that every call to is matched with a corresponding call to from the same module. This ensures that the block of memory is always deallocated by the same module that allocated it. For additional robustness, you might want to add error checking and even modify the function signatures to make extra information available to the callee for this purpose and/or return a status code to the caller. Along similar lines lines, if you don't actually need general-purpose memory allocation, you might modify the function signatures to allocate only a certain type of memory and thus make them harder to use incorrectly. For example, could always allocate an array of integers, taking as its only parameter the length of the array, and would free a block of memory allocated by . These wrappers actually give you a lot of power. If diagnostics indicate that you have memory-fragmentation issues, you can add fixed-size allocation logic or switch over to the low fragmentation heap without introducing breaking behavior that would affect your clients. As far as your goal of making it easy for the client to use, no competent programmer is going to have a hard time understanding how to use and functions. Personally, I make it a rule that all memory a caller needs to be freed must be explicitly allocated by the caller. This makes memory-management responsibilities significantly easier to reason about. But if you don't have such a rule (and I can't encourage you to adopt one, because you care more about simplicity than correctness), you could just have the DLL allocate the memory internally by calling the function and return that block. Then, the client will just follow the rule that any memory it gets from the DLL, whether implicitly or explicitly, must be freed by calling . There are Win32 functions that follow this model, too. Alternatively, if you don't want to add additional exported functions to your libraries, you can just ensure that all of your code is standardized to use a single external memory allocator. For example, if you are targeting the Windows API, you can use either and , or and . This is safe because these allocators are universal and don't depend on the calling module. I should also point out that if you are writing strictly C++ code and don't have any need for a lowest-common-denominator "C" API, then you can just return a shared smart pointer object from any function that needs to allocate memory. In addition to the obvious benefit of preventing the possibility of memory leaks and vastly simplifying the client code, smart pointer objects remember their associated deleter, thus guaranteeing that the memory will be correctly deallocated, even across module boundaries. If you're C++11, use ; otherwise, you can use Boost's . 

Lots of bad practices exhibited here… First of all, you probably shouldn't be using exception handling here at all, as Incomputable already pointed out in a comment. Exceptions should be for truly exceptional situations. If you expect a zero input as a possibility and are going to explicitly handle it, then I would argue that it is not exceptional at all. Thus, a simple loop and conditional test would suffice: just keep looping until the user enters a valid input. Second, your variable names leave a lot to be desired. What are and ? It's okay to use terse, single-letter variable names like when their purpose is obvious, as for a loop index, but that's not the case here. Therefore, you should use descriptive variable names. Since we're doing division, maybe and would be good choices? Third, is just terrible and should never be used in real code. I guess you're using this to work around the fact that your development environment closes the console environment as soon as the program finishes execution, instead of leaving the window on the screen so you can see its output. Don't use code as a hitch to work around your IDE's bad behavior—learn to configure your IDE so it does what you want. If you absolutely must rely on such a hitch, use . But there's really no "good" way to do this, so prefer not doing it. Fourth, if you're going to throw exceptions: 

I don't think that's a valid conclusion. That's not to say it isn't true that compound expressions execute more quickly in an interpreted language, but I don't think you have "clearly demonstrated" that here. Nor do I think you have proven that method 1 is superior to method 2 in interpreted languages. All you've proven is that you managed to translate method 1 into code that runs more efficiently in QBasic, whereas you weren't able to do the same for method 2. I can trivially write method 1 using the same number of lines and the same looping construct as method 2—do you think it'll run more slowly now? 

Noteworthy, perhaps, but not surprising. If you don't use the integer divide operator, you get a floating-point division. Semantically, then, this is really just a bug: you're dividing two integers using the floating-point division operator. Why is it so much slower to do the division in floating-point? Well, there are a couple of reasons. First, you have to pay for a conversion from the integer value into a floating-point value, and then a conversion from the floating-point result back into an integer value. Aside from the fact that this pointlessly injects a bunch of extra instructions, it also vastly increases the latency. Second, there's the fact that the QBasic system is so old that it can only emit instructions that use the x87 FPU, which is less efficient than SSE/SSE2 instructions and slower to move values into and out of because they have to round-trip through memory. In fact, it is probably even worse than that. Given its age, QBasic probably doesn't even use FPU instructions. Back in the day, not all systems had FPUs installed, and floating-point operations had to be simulated using a routines provided by a runtime library, where you not only had to pay the cost of a function call, but the simulated code was much slower than a simple . 

You know that is 0, and you know that is less than the maximum positive value representable by an , so there is no possibility for overflow. In fact, the compiler sees this as simply . You are not so lucky inside of the loop, though. There, if the value you seek is in the second half of the array, you'll end up assigning to , and then if the array is sufficiently large (i.e., if is sufficiently large), then the calculation of will overflow as described. Perhaps not terribly likely that you'll have an array large enough (famous last words!), but still a bug waiting to strike when you least expect it.