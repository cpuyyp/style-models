You can find the proof of NP-completeness of binary puzzle on my blog (soon or later I'll find the time to submit it somewhere). A3) Furthermore if you add the constraint that each color can be used only once, then the problem is still NPC even on $n \times 1$ grids with only horizontal constraints. The reduction is from 3-PARTITION which is strongly NPC: given a set $X = \{x_1, ..., x_{3q}\}$ and a target sum $B = (\sum x_i)/3$; use (new) colors $c^i_1,...,c^i_i$ for each $x_i$. Start with a grid $G$ of length $Bq+q + 1$ with the following sets of allowed colors: $S E^B T_1 E^B T...E^B T_q$ Where 

Case k fixed The same approach above can be used for any fixed $k >2$ (the table size grows polynomially). Case k not fixed If $k$ strings are given as input, and $k$ is NOT FIXED: in that case there is a quick reduction from the 3-PARTITION problem which is strongly NP-complete, so it remains NPC even when the input numbers are represented in unary. Given a set of $A = \{x_1,x_2,...,x_{3m}\}$ of $3m$ positive integers (that can be specified in unary), and a target sum $B$ we must find $m$ disjoint subsets $A_1,A_2,...,A_m \subseteq A$ of $3$ elements ($|A_i|=3$), and the sum of the elements of each subset must be $B$. Over alphabet $\{a,b,c\}$, set: 

I'm not an expert, but if you need a practical complexity measure for strings, you can take a look to the Titchener T-complexity measure. See Titchener's web site for a quick introduction; his papers can be downloaded in pdf format. Abstract - A new measure of string complexity for finite strings is presented based on a specific recursive hierarchical string production process. From the maximal bound we deduce a relationship between complexity and total information content. ..full article... I found some papers on practical implementations, too (see for example "A Fast T-decomposition algorithm") 

is proved and the authors say that it has been derived in a slightly different form in: I.M. Barzdin, Ob odnom klasse machin Turinga (machiny Minskogo), Russian, Algebra i Logika 1 (1963) 42-51 but don't cite Rich Schroeppel's paper (1972) in which the theorem is also derived ... :-) 

The Rule 110 cellular automaton (often simply Rule 110) is an elementary cellular automaton with interesting behavior on the boundary between stability and chaos. In this respect it is similar to Game of Life. Rule 110 is known to be Turing complete. This implies that, in principle, any calculation or computer program can be simulated using this automaton. Another simple example is a Bitwise Cyclic Tag System: The "program" is a sequence of instructions $0, 01, 11$ (i.e. $C = p_{n} p_{n-1} ... p_2 p_1; \; p_i \in (0|01|11)^+$) ; the "data" is a binary string (i.e. $w \in \{0,1\}^+$). If $p_1$ is the "current instruction" and the data is $w = a_1 a_2 ... a_m$; you can put the program and the data side by side ($p_{n}...p_2 p_1 \leftrightarrow a_1 a_2 ...a_m$), and the "execution" of $p_1$ is: 

(just an extension of my comment) (As pointed out by Joe in his answer) Shannon - in his 1948 paper, "A Mathematical Theory of Communication" formulated the theory of data compression and established that there is a fundamental limit to lossless data compression. This limit, called the entropy rate, is denoted by H. The exact value of H depends on the information source --- more specifically, the statistical nature of the source. It is possible to compress the source, in a lossless manner, with compression rate close to H. It is mathematically impossible to do better than H. However some class of images (for example medical grayscale images) without high-contrast edges and with smooth level transitions can be compressed (not so efficiently). JPEG-LS and JPEG2000 seem to be the standards for lossless storage of medical images. See this table for a comparison of compression ratios (the JPEG-LS achieves a slightly better compression). Using "lossless medical image compression" I found the following articles that may help you: A recent (2011) survey on medical image compression techniques: Two Dimensional Medical Image Compression Techniques - A Survey ... This paper presents overview of various compression techniques based on DCT, DWT, ROI and Neural Networks for two dimensional (2D) still medical images. A detailed presentation of two standard lossless compression algorithms: JPEG-LS and JPG2000 in lossless mode: Lossless Compression of Grayscale Medical Images - Effectiveness of Traditional and State of the Art Approaches ... Three thousand, six hundred and seventy-nine (3,679) single frame grayscale images from multiple anatomical regions,modalities and vendors, were tested. ... Another survey: A Survey of Contemporary Medical Image Compression Techniques EDIT Perhaps you are still wondering "What the hell is the entropy of an image?" ... OK, it's the amount of information contained in the image ... but to better understand it, you should read something about the 3 phases usually used in image compression: 

I found a good candidate while reading some papers about quadratic diophantine equations: J. C. Lagarias, Succinct certificates for solutions to binary quadratic Diophantine equations (2006) From the abstract: ... Let $L(F)$ denote the length of the binary encoding of the binary quadratic Diophantine equation $F$ given by $a x_1^2 +b x_1 x_2+ c x_2^2 +d x_1+e x_2 + f = 0$. Suppose $F$ is such an equation having a nonnegative integer solution. This paper shows that there is a proof (i.e., “certificate”) that $F$ has such a solution which can be checked in $O(L(F)^5 \log L(F) \log \log L(F))$ bit operations. A corollary of this result is that the set $\Sigma = \{F : F \text{ has a nonnegative integer solution}\}$ is in the complexity class NP ... ... but - to be honest - the only evidence that I have that it is nontrivial is the number of the pages of the paper ... 62! :-) 

My favourite is Minesweeper (which is NP-complete) Then comes Sokoban (which is PSPACE-complete) ... and last but not least One dimensional peg which is nothing but a regular language (DSPACE(O(1))) But tons of games have been studied from the the point of view of Complexity Theory ... you can find a big list on Wikipedia: Game Theory (with some good references at the bottom, if you want to go into more depth). For a more formal (deeper) approach, you can download Robert Aubrey Hearn's thesis "Games, Puzzles, and Computation" (which - in 2009 - became a book with the same title). 

Perhaps you can build a language in DPSACE(n) that cannot be recognized by a MPA with $k=1$ using a diagonalization argument (probably the idea is similar to the one in Ben's answer, but I didn't dig into it): Suppose that over alphabet $\Sigma = \{0,1\}$ you encode a MPA using a list of transitions: $s,a,p \rightarrow s',p',L|R;...\#$ where $s$ is the current state, $a$ is the current symbol, $p$ is the pebble status, $s'$ is the new state, $p'$ is the new pebble state, $L|R$ is the move direction, $\#$ is an endmarker). A Turing machine $M$ on input $x$ can check if it is a valid description of a $MPA_x$ and simulate it on input $x$ for $4^{|x|}$ steps using $6|x|+\log|x|$ cells, stretching the input in this way: 

The previous reduction doesn't work with the current reformulation of your problem (19 July 2014); however I leave it below, because it is correct for that particular maximum k-set packing problem variant (which is defined in the reduction). Here it is a fix for the current version: CURRENT VERSION Again your problem is NP-hard for $l \geq 3$. NOTE that $l$ should not be part of the input otherwise it is simply a generalization of the MAXIMUM $k$-SET PACKING problem (MSP): given an instance of MSP, feed it to your algorithm setting $l = k$; so the NP-hardness proof is immediate. If $l,k$ are not part of the input, I give you the idea of the reduction for the case $l = k-1$, but it can be easily extended to arbitrary $3 \leq l < k$. Given an instance of MAXIMUM $l$-SET PACKING PROBLEM (MSP), i.e. a collection of sets $A_1, A_2, ..., A_n$ and an integer $p$ that represents the minimum number of sets required for the solution, build an instance of your problem in the following way: 

If you mean "the best" computational model to make your life more complicated, then you can use Wolfram's 2-state, 3-symbol universal turing machine. PROS: none except the sensation of walking the fine line between reason and craziness; CONS: tons ... :-D (only a joke, I basically agree with the previous answers ...) 

There is a lot of research on the boundary between decidability and undecidability of the halting problem for small models of computation: Turing machines, tag systems, CAs, ... This boundary is clearly linked to the (non-)universality of such models. I'm wondering if there are similar works on the "logic-side": 

From Papadimitriou's paper "On total functions, existence, theorems and computational complexity": Nondeterministic multivalued functions with values that are polynomially verifiable and guranteed to exist form an interesting class (called $TFNP$) between P and NP ... .... It is quite interesting, in view of Theorem 2.1, that in some cases of problems in $TFNP$, if together with the input we are also given one of the solutions that are guaranteed to exist, then the problem indeed becomes NP-complete ... For example the TRICHROMATIC TRIANGLE problem is in $TFNP$, but the SECOND TRICHROMATIC TRIANGLE is $NPC$ (the paper contains a sketch of the proof). 

For all the other pairs, set $e(T_i, T_j) = d+1$ The $n+2$ tasks can be scheduled before deadline $d$ if and only if the original graph has an Hamiltonian path from $s$ to $t$ So your problem is NP-complete. 

From the comment: the undecidable questions are related to initial configurations with infinite support, whereas the conjecture refers to the behaviour of the ant when starting from a configuration with finite support (all but a finite number of cells are in the same initial state). If the conjecture is true then there are no undecidable problems starting from a finite support: just stop when the ant starts building the highway towards empty space. 

A fairly natural and studied variation is the Tape-Reversal Bounded Turing machine (the number of tape-reversals are bounded); see for example: Juris Hartmanis: Tape-Reversal Bounded Turing Machine Computations. J. Comput. Syst. Sci. 2(2): 117-135 (1968) Edit: [this variation is more artificial] the halting problem is decidable for a Non-erasing Turing machine that has at most two left instructions on alphabet $\{0,1\}$; see Maurice Margenstern: Nonerasing Turing Machines: A Frontier Between a Decidable Halting Problem and Universality. Theor. Comput. Sci. 129(2): 419-424 (1994) 

Only an extended comment: I only heard about it (and I know almost nothing about it :), but there is a computational model based on Graph Rewriting From Wikipedia: "... The basic idea is that the state of a computation can be represented as a graph, further steps in that computation can then be represented as transformation rules on that graph. Such rules consist of an original graph, which is to be matched to a subgraph in the complete state, and a replacing graph, which will replace the matched subgraph. ..." You can take a look at: D. L. McBurney and M. R. Sleep. 1991. Graph Rewriting as a computational model. In Proceedings of the UK/Japan workshop on Concurrency : theory, language, and architecture: theory, language, and architecture, A. Yonezawa and T. Lto (Eds.). Springer-Verlag New York, Inc., New York, NY, USA, 235-256. or this thesis for an introduction: König, Barbara (December 2004). Analysis and Verification of Systems with Dynamically Evolving Structure. Habilitation thesis, Universität Stuttgart, pp. 65–180. But I don't know if it is still an active field of research. 

I read your last comment in the Joshua's correct answer; if you need to transform EQ-GI to colored GI (i.e. you are in trouble with the colors assigned to the equivalence classes) you can use the following reduction: Suppose that the starting graphs are $G_1 = (V_1, E_1)$, $G_2 = (V_2, E_2)$ and there are $q$ equivalence classes; then you can add to each graph a "permutator", i.e. a complete graph on $|V_1|+1=|V_2|+1$ nodes ($K'_{|V_1|+1}$,$K''_{|V_2|+1}$) and use $q+1$ colors $c_1,...,c_q,c_{q+1}$. In both $K'$ and $K''$, $q$ nodes are distinguished and colored with $c_1,...,c_q$ the remaining nodes are colored with $c_{q+1}$. The nodes of $G_1$ are colored with color $c_{q+1}$ and nodes in the same equivalence class are linked to the corresponding color in $K'$; the nodes of $G_2$ are colored with color $q+1$ and nodes in the same equivalence class are linked to the corresponding color in $K''$. Also note that you can drop the colors and get an equivalent GI instance :-) 

I think that declaration before use of variables and the function polymorphism of the OOP languages are other examples of programming languages specifications that cannot be handled by context free grammars: