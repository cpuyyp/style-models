For example, due to the complexity of the images in the ImageNet database. Algorithms will often use hundreds or thousands of output nodes to be capable of classifying a large array of different things. Researchers also relax the cost function and allow the $k$ highest outputs to be considered. If one of these is correct then the example is considered to have been correctly classified. Furthermore with the existence of such complex data, often times a certain object might be a subset of another object. For example, consider the picture of a human being. If the algorithm detects a human with the highest activation at the output node, but also has high activations for hands, eyes, face. This is not wrong! The image does in fact contain these objects as well and them being detected by the model is encouraging. 

Furthermore, if you also want to reject all excess points in your calculation that are found once they have arrived at their location you can add this. 

My suggestions I would not use a NN for such a case. Most of the frameworks allow you to add information throughout your model. Such that if you have images you can run a CNN over them, then when you are ready to convert your layers to a densely connected layer you can in additional information, such as vectorized text. You can thus use a random forests approach or something even simpler to get your 100% classification even faster. Then you can feed the output of this model to your deep learning framework which has already "extracted the features" from the images and concatenate these additional features to that tensor. Then you will pass this larger tensor through the subsequent Dense layers to get your final output. 

Applied to neural networks The first neural networks only had a single neuron which took in some inputs $x$ and then provide an output $\hat{y}$. A common function used is the sigmoid function $\sigma(z) = \frac{1}{1+exp(z)}$ $\hat{y}(w^Tx) = \frac{1}{1+exp(w^Tx + b)}$ where $w$ is the associated weight for each input $x$ and we have a bias $b$. We then want to minimize our cost function $C = \frac{1}{2N} \sum_{i=0}^{N}(\hat{y} - y)^2$. How to train the neural network? We will use gradient descent to train the weights based on the output of the sigmoid function and we will use some cost function $C$ and train on batches of data of size $N$. $C = \frac{1}{2N} \sum_i^N (\hat{y} - y)^2$ $\hat{y}$ is the predicted class obtained from the sigmoid function and $y$ is the ground truth label. We will use gradient descent to minimize the cost function with respect to the weights $w$. To make life easier we will split the derivative as follows $\frac{\partial C}{\partial w} = \frac{\partial C}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial w}$. $\frac{\partial C}{\partial \hat{y}} = \hat{y} - y$ and we have that $\hat{y} = \sigma(w^Tx)$ and the derivative of the sigmoid function is $\frac{\partial \sigma(z)}{\partial z} = \sigma(z)(1-\sigma(z))$ thus we have, $\frac{\partial \hat{y}}{\partial w} = \frac{1}{1+exp(w^Tx + b)} (1 - \frac{1}{1+exp(w^Tx + b)})$. So we can then update the weights through gradient descent as $w^{new} = w^{old} - \eta \frac{\partial C}{\partial w}$ where $\eta$ is the learning rate. 

This is where the problems start. The next two lines do not include the probabilities but only the ground truth labels. 

A generative model is able to generate instances from a given distribution. So let's try to get our model to generate instance from the distribution of all hand written digits. This includes numbers 0 to 9 and for each of these numbers there are a very large set of possible way of writing the same number. Take the number 4 from the MNIST dataset, which are comprised of handwritten numbers. 

Easy Solution The easiest way that i would suggest. Assume you are using a single statistic to define what is happening throughout the 3 sample window. From the collected data get the maximum $s$ of your nominal points ($y=0$) and the minimum $s$ of your anomalous points ($y=1$). Then take the halfway mark between these two and use that as your threshold. If a new test sample $\hat{s}$ is larger than the threshold then assign $y=1$. You can extend this by calculating the mean $s$ for all of your nominal samples $y=0$. Then calculate the mean for your anomalous samples $y=1$. If a new sample falls closer to the mean of the anomalous samples then classify it as $y=1$. But I want to get fancy! There are a number of other techniques you can use to do this exact task. 

Imagine the matrix as a 2D table. You have 100 rows and 32 columns. Then the for loop acts as an iterator which will return values along the first dimensional axis. This dimension thus disappears and returns the remaining dimensions. When there is a single dimension the default in Python is $(n,)$. A matrix $(100, 32)$ iterates through $(32,)$ A matrix $(100,28,28)$ iterated through $(28,28)$ A matrix $(100,2,2,2)$ iterated through $(2,2,2)$ 

This should yield about 98.75% on the validation set. This is pretty good and should be enough to test some new data using this set. 

Unbalanced class distributions First, unbalanced datasets will cause your model to have a bias towards the over-represented classes. If the distribution of the classes is not very drastic then this should not cause a significant problem with any algorithm you will employ. However, as the difference between the class distribution becomes more severe you should expect to get higher false negatives for that class. Consider this, you are trying to have the model adequately identify what it means for a specific example to belong to a class. If you do not provide sufficient examples, then the model will not be able to understand the extent of the variation which exists among the examples. If the class distribution is very different, then I would suggest anomaly detection techniques. These techniques allow you to learn the distribution of a single classification and then identify when novel examples fall within this distribution or not. 

You can look into the MICCAI dataset. This contains 35 3D MRI scans of brains. The brains have been parcelated into 134 distinct brain regions by professionals. 

Bag-of-Words builds a dictionary of the words it has seen during the training phase. Then using the word the frequency of each word in the example a vector is created. This can then be used with any standard machine learning technique. 

Ok let's go part by part. There's quite a few parts here where you do not take into consideration the bias in your network. Choosing your inputs and output If the vector 0-6 is determined there really is no need to output 1-7. The 1-6 is already known and adding additional outputs will only add complexity to your model. Unless you have substantial amounts of data you want to keep your model as simple as possible in order to get good performance. Thus, I would output a simple neuron with a continuous value. You can use RMSE as your loss function with a regression output from your neural network. Additionally, you should supplement the samples you put into your input space with some additional information that you might think would contain information about the trend line. For example, if I had 2 different products, bitcoin and gold, and their input vector was the same, I might expect the gold to have very little fluctuation but the bitcoin to have very high fluctuation. Your input features to your network contain all the information from which your network will learn. Thus, you want to make sure that you are supplying sufficient information to have a meaningful prediction. Deep learning is data hungry You will need approximately 100,000+ instances. Each instance is a set of features. These should be drawn independently and such that they are identically distributed. In other words, you want to get multiple trendlines from a varied source of data that you wish to use your network with and then you will randomly draw 0-6 points, that is your features, and 7 which will be your label. Consider the data distribution you are trying to learn. If you want your network to classify cats/dogs, you need to give a wide range of different looking cats and dogs such that the network can identify the variance which exists in both of these classes. If you restrict the data source too much it will have high bias and will not generalize to novel data that you will later feed into it. 

Alternatively, you can get the probability distribution function for each variable and get the overlapping area between two groups. 

You are correct! The array you see between those for your weights are the bias of the node in the starting layer. Notice how there are 5 elements in that array, this coincides with the 5 nodes in your first hidden layer. 

Genetic algorithm This consists in 4 crucial steps: initialization, evaluation, selection and combination. Initialization Each individual in the population is encoded by some genes. In our case the genes represent our $[x, y]$ values. We will then set our search range to [0, 1000] for this specific problem. Usually you will know what is naturally possible based on your problem. For example, you should know the range of possible soil densities in nature. We will create 100 individuals in our population. Evaluation of the fitness This step simply asks you to put the $[x,y]$ values into your function and get its result. Pretty standard stuff. Selection There are many ways with which you can select parents. I will always keep the alpha male. The best individual in the population, he will be cloned to the next. Then I will use tournament selection. We will repeat the following until the next generation population is full. Pick four parents at random, take the best individual from the first two and the best from the last two. These will be the two parents which will gives us our next offspring. Combination From the two parents we will build the new genome for the child using the binary values of the $[x,y]$ values of the parents. The resulting binary value for each codon in the genome of the child is selected from the two parent genes by uniform random. The code 

Your input matrix should have dimensions (# of instances, length of history of events, 1). For example, I have text data that is sequential and I will cut it up into sequences which are sequentially dependent. Each row in the matrix is the sequence, the history of events. My label for each of these is the next sequential letter, or event in your case. The input $X$ has the following shape 

This gives the following plot. The purple and yellow points are the 2 different classes from the training set. Then the light blue points are from the testing set. You can see that the points in the testing set after being transformed by PCA will line up alongside the training set. 

K-means attempts to group distributions into $k$ similar categories based on some metric of nearness. If your dataset contains repeated values this is not a problem. It simply means that you have 2 distributions from which you are drawing instances, and the variance of these distributions is essentially zero. For example, if you have a dataset from which you want to cluster two species of fish with 2 features: color and length. However, the fish of each species are all clones (ignore other biology stuff i know nothing about like environmental factors). Thus they will all have exactly the same color and the same length in either group. Each measurement of each individual will be exactly the same. This is fine. We simply have 0 variance in each distribution. Clustering is very well suited for very tight distributions because they are easily separable. However, distributions with high variance will have significant overlap and will thus cause many instances to be wrongfully classified as being a part of the other cluster. The ideal number of clusters This value can be calculated by determining the average distance between each instance and its nearest cluster. You will see that once you surpass 2, the average distance will not decrease in any further in this case, which means a single distribution is being split into 2 groups. Thus 2 is the ideal number of clusters. 

You state that some of the words may occasionally be truncated or concatenated. Thus I would extract n-grams from your Strings and then use that into a bag-of-words vector. How does this work? 

To consider the fact that you have a list of $K$ categories for each node you can add a weighting when calculating the normalized frequencies. For example let's say we have 3 classifications and the following webpages (nodes). 

This means that if we had all class 1, with probability 1. We would get an AUC of 0.5. This does not make any sense. For example, if I force it by doing this probas[100:100000] = 1 truth[100:100000] = 1 There will only be a few 0 values left. Then after the ordering and cumulative sum we get the plot as follow, which is obviously wrong. 

Using the notation from the wikipedia page, the convolution in a CNN is going to be the kernel $g$ of which we will learn some weights in order to extract the information we need and then perhaps apply an activation function. Discrete convolutions From the wikipedia page the convolution is described as $(f * g)[n] = \sum_{m=-\inf}^{\inf} f[m]g[n-m]$ For example assuming $a$ is the function $f$ and $b$ is the convolution function $g$, 

In code Assuming matrix $X$ contains the data where rows are the instances and columns are the features, and matrix $Y$ contains the labels. First we split our data into a training and testing set 

Next we need to reshape the data such that it matches with the Tensorflow framework which we will be using under the hood of Keras. This requires that the instances be the first dimension and it also requires a channels dimension as the last one. thus for the MNIST data we need to have $(6000, 28, 28, 1)$. 

We will make a list which will hold the radius of each circle for each point. For each point we will iterate over the other points by their relative nearness. If an associated label is not yet seen, add it to the temporary list. Otherwise, we end the function and take the average of the last distance and the current illegal point. 

Of course the more classes you will choose to have then the harder it will be to train the model. That means the resolution of your results is limited by the amount of the data you have available to you. The other option is to use a regression algorithm. This will learn the tendency of your curve in higher dimensional space and then estimates where along that line a new example would fall. Think of it in 1-dimension. I give you a bunch of heights $x$ and running speeds $y$. The model will learn a function $y(x)$. Then if I give you just a height, you will be able to estimate the running speed of the individual. You will be doing the same thing but with more variables. There are really a ton of methods that can do this. You can look through some literature reviews on the subject to get a hold of all of them. But, I warn you there is A LOT. I usually start with kernel-Support Vector Regression (k-SVR). To test your model. Separate your dataset into three parts (training, validating, testing) if you have sufficient data. Otherwise two parts (training, testing) is also fine. Only, train your model on the training set and then evaluate it using the example it has not seen yet which are reserved in the testing set. Post-processing This is the step where you can further model your output $y(x)$. In your case that might not be needed. 

We can then see that the discriminator is trained exactly the same way as a basic classification CNN with 2 output nodes. I will describe the training process of a CNN below. Training the generator When we train the generator we will keep the discriminator fixed, this is necessary as to not saturate our parameters and make the discriminator too strong to ever beat. So what we essentially have is a CNN (the generator) connected to another CNN (the discriminator). The connecting nodes between these two models is going to be output which will generate the desired images once trained. Do note that the generator wants the instances it will produce in this case to be classified by the discriminator as being from the real distribution, so they will have labels $y = 1$. 

Validating results using MNIST Let's pass through a few random values from the MNIST dataset to see if they get rightfully classified by our model. Note how I set the dimensionality of the data. The instance dimension must exist even if we only have a single instance to predict as well as the channel dimension 

When evaluating your algorithms, especially when your dataset is unbalanced, you should use more metrics than just accuracy. The accuracy is how many examples you have correctly identified in total. As you have seen if you have an unbalanced dataset where 0.5% of your instances are 1's then this will result in 99.5% accuracy if you blindly set all your outputs as zeroes. This is obviously wrong albeit the high accuracy. The accuracy is calculated as $Accuracy = \frac{\sum{TP} + \sum{TN}}{\sum{TP} + \sum{TN} + \sum{FP} + \sum{FN}}$ where TP is true positive, TN is true negatives, FP is false positives and FN is false negatives. If you want to capture the performance of your unbalanced dataset you should look into the percentage of FP and FN you are calculating. You can do this using the sensitivity and the specificity. Calculate the sensitivity as $Sensitivity = \frac{\sum{TP} }{\sum{TP} + \sum{FN} }$ and the specificity as $Specificity = \frac{\sum{TN} }{\sum{TN} + \sum{FP} }$. An ideal classifier should have the accuracy, specificity and sensitivity all be 1. This would mean every sample is correctly classified. In your case where you are getting very high false negatives, you will see that your sensitivity will be very low. This is a measure with which you can state that your algorithm is performing poorly. It is good form to always include these metrics in any statistical study you are doing. Accuracy alone is not sufficient to prove that you are obtaining good results. Moreover, there is the receiver-operator curve (ROC). This will tell you your false positive rate for any true positive rate. You can then calculate the area under this curve (AUC) to get a comparable metric of performance. All of these should be used together when exclaiming the performance of your algorithm. The ROC and AUC can be omitted however leaving out the sensitivity and specificity of your algorithm is unwise.