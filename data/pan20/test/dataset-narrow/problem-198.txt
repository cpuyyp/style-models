where is the more up-to-date rate with EUR for the money choosen by the user. The efficiency comes from the fact that the range of rates is supposed to be small, the values being close together. 

Is it really the case that only the owner a large object is ever able to delete it, or I'm missing something? My use case is a db that stores mailboxes that are shared between different db users. All the tables with mail contents and large objects with mail attachments are owned by a "master" user, and other users may or may not delete messages depending on their database role. Deleting a message means deleting everything related to it, including attachments stored in large objects. So it would make sense that a non-owner should be able to delete large objects, just like he can delete from other tables if he has been granted the DELETE privilege on these tables. 

For the 2nd comparison, it's evaluated to false because some of the rules explained in 3. Look for the best match lead apparently to a cast to . But you may convince PostgreSQL to evaluate the way you want by adding a function/operator couple with an exact type match: 

Another interpretation of the question could be that in , A is a table-type with certain columns, and is a arbitrarily different table-type with different columns but that happens to contain 0 row. In that case there will be no operation in SQL that satisfies the question, because the result type of a query cannot depend on the contents of relations. It must always depend only on the definitions of the relations involved, so that it can be determined without actually running the query. 

Don't use at all. In your case, it's just confusing. Also, you need to know that the fact that the server asks for a password or not is not driven by the existence of this password. It's driven by the server-side file that you need to study and possibly modify according to your needs (don't forget to reload the server after modifying it). EDIT: reviewing your pg_hba.conf. The relevant lines are: 

If it's not satisfactory, check the and flags, also tunable through and . They're defined as (from ): 

Yes, dropping the primary key constraint will drop the index. But it should be noted that serial is not a real type, it's a shortcut for , where is auto-generated. You don't say what is the current column type, but since you expected to be able to reuse the index for the serial column, let's assume it's INT. It's not obvious in the question if you plan to ultimately drop the old column, but if you just want to transform it into a serial column, dropping and recreating the associated constraints is not necessary. Just create a sequence, initialize it to the next value of the primary key, and let its nextval be the default value of the column, as in: 

Yes, cursors are not shared across sessions, and their namespace is private to their session as well. Moreover, if the cursor is not declared "holdable" trough the clause, it's automatically closed at the end of the transaction. The relationship between cursor lifespan and session is detailed in the docpage for : 

Just call to know the size of the database. (without the clause) does not free any space, it only marks it as reusable, and thus will not change the database's size (except in a rare boundary case, see Routine Vacuuming). does statistical sampling and would be useful if you needed the row counts, but for the global db size, it's not necessary. 

Even though plpgsql event triggers are pretty limited in current versions, you can achieve the particular result of disallowing only with them. Consider this: 

With a generated pivot query In the case of the question, the generated query should look like this: 

Run to check the actual contents of the latest log file. If these warnings are the reason why the log files are huge, they can be muted by setting to OFF in See the compatibility notes in postgresql 8.4 docs for why this warning kicks in. The other default logging settings of PostgreSQL on Ubuntu are: 

Assuming is used as the pager, you can turn off this screen-clearing feature which indeed is annoying within psql. Use the option: 

This assumes than any simplified version of a word is already part of the table, as seems to be the case in your sample data. Otherwise they should be inserted. 

If you can't use , as an alternative still based on encrypting the ID, you may look at the plpgsql implementation of SKIP32, or XTEA for 64-bit (bigint) which won't require a compilation or being superuser. The drawback is that you cannot fine-tune the size of the output by changing the sequence's range. 

or to get a single byte as an integer. See Binary String Functions and Operators in the doc for all functions. 

A bit of research reveals that this question is known as the subset sum problem in computer science. On stackoverflow, Lukas Eder provides an Oracle solution to a similar question, and a longer analysis on jooq's blog. Here's a postgres version derived from his work: 

From a design perspective, why would the table be needed at all? The information that a specific student follows a certain type of education is implied by the existence of the corresponding row in the table. If 's reason for being is for SQL joins, it could be a view returning the union of all the tables. As for the foreign keys to , to be sure that each entry in can only refer to its corresponding type, I can see two options: 

If you replace auto by manual, you could start this PostgreSQL instance only when desired with the command: 

So there is a configuration file that can be tuned to rotate log files when they grow over a size given by the keyword , among a lot of other options. On the other hand, should you want to use the PostgreSQL feature to achieve this, as opposed to the pre-configured method of the OS, you need to consider all logging parameters in , not just . First a Debian/Ubuntu packaged PostgreSQL will turn off to not interfere with , but most of options depend on it to be effective, as mentioned in the doc, for instance: $URL$ 

The 1st line concerns the user. It's irrelevant for your pg_dump command since you're using the user with The 2nd line concerns any other connection through Unix domain sockets (TYPE column is ). From the client, it means when you do not use . It says that if the OS user is the same name than the db user, he doesn't need a password. The 3rd line says that if is used (IPv4 TCP connection), a password will always be asked to the client. The 4th line says the same with IPv6. Based on this, this command run by the OS user should not ask or need a password: 

It's also consistent with $URL$ You may try on your system and check if is part of the results, and if yes, to what package it belongs, and then what is the origin of that package. Maybe for some reason it's the same as the sample postgresql.conf, except it doesn't have the suffix. 

The field separator can also be set with or Once the results are in the file, use alone to redirect the output back to the terminal. 

When called from psql with the program refers to a client-side command to launch by . When is used, it's a server-side command run by the backend. But the trimming itself is non-trivial. For instance, see How to manipulate a CSV file with sed or awk? and Trim leading and trailing spaces from a string in awk. 

The command tests that the process (corresponding to ) does exist from the point of view of the operating system. In the positive case, it reports that it's running (note that the word is not used). Essentially that's implemented by reading and checking the result of . It does not mean that this PostgreSQL instance does accept connections or is able to execute queries. To that purpose, better use or a script like Nagios check_postgres plugin. 

This means that I have to wait until all of my files are copied back from the new master the old one, which means a lot of data traffic When using , yes. But, if you're confident that the existing files at the destination are almost identical to the source, may be used. The rsync remote-update protocol is able to identify what parts of files changed to minimize the amount of data to transfer. See examples of uses on the postgres wiki: Binary Replication Tutorial / Starting Replication with only a Quick Master Restart My personal experience with versus is that is good if the "new master" has not drifted that much from the state when it became master, like a few minutes of transactions on a moderately busy system. Otherwise it still works but appears to be faster. 

Concerning the second issue, a DBA can create a symlink from inside the directory to any directory, and will follow it, so the real upload directory can be anywhere on the file system. If the system admin agrees to this setup, as a non-priviledged user you could eventually run a simple plpgsql function matching the functionality of the shell script: 

The index is probably corrupted. Hash indexes are only WAL-logged since PostgreSQL 10. In previous versions, they don't have that mechanism that makes them persist correctly across unclean shutdowns. The doc on CREATE INDEX for 9.5 has this warning: 

To remove duplicate combinations of adjacent columns, the structure of the resultset should be changed so that each output row has only one couple of adjacent columns along with their corresponding dimensions in the parallel coordinates graph. Well, except that the dimension for the 2nd column is not necessary since it's always the dimension for the other column plus one. In one single query, this could be written like this: 

Instead of running , use pg_createcluster in the first place. The problems you're trying to solve are already taken care of when using the tools provided by the debian/ubuntu packages to handle multiple clusters and multiple PG server versions simultaneously: See also: pg_wrapper pg_dropcluster pg_ctlcluster pg_lsclusters 

In the first step, is not in the table yet so the INSERT is executed (or if it's in the table already, then it's not executed, but that's not the case you're asking about). In the second step, is now in the table so the SELECT is executed too. See Rules on INSERT, UPDATE, and DELETE in PostgreSQL documentation for more details. The (wrong) idea that the SELECT should not be executed when the INSERT gets executed is a common misinterpretation of how RULEs work, because intuition is misleading here. When looking at the rule, we're tempted to think that it does this: 

As itself requires to be superuser, this function will also need to be validated and owned and checked as by a superuser. 

the two decimal constants are implicitly casted to double precision numbers, since it corresponds to the closest matching types for the function: 

This outputs only one line and column at a time. Extracting another column at the same time (like an ID) would be significantly more difficult for the post-processing, so this simple example assumes some kind of outer loop going through the IDs that would have been obtained previously. 

Ideally, it should be done immediately after and the database should shrink to its lowest possible size at this point in time. 

is a pseudo-column indicating the physical location of a row in the form of , so this will dump the contents in their order in data files. This COPY command should error out when it reaches the offending row, but at this point it should have streamed some output (to be confirmed in practice). The end of this output should indicate the up to which the data is not corrupted. Starting from this , it should be possible to pinpoint the offending row with a dichotomic approach, running queries such as 

In a programming language, if you execute this query, fetch the result (a single-row, single-column string) and execute that result as an SQL query, it will produce the expected pivoted data with the 150+ columns. In psql, this can be done through \gset and a variable 

Now if several sessions issue the same in parallel, there is indeed a potential race condition that is dealt with by relying on the unique index on , and comes back to the caller as an error. The error mentioned in the question happens if, for instance: 

does not apply to rows, but to objects. The fact that rows do not count is made explicit in the documentation: From $URL$ 

You're not asked for a password because the configuration file in CentOS says that a connection through a Unix domain socket implies the authentication. succeeds only if your OS user is the same as your database user. It's a reasonable default. You may edit to indicate that anyone can connect locally to to certain databases or all databases, or more generally whatever policy is best suitable in your case. The file is documented at: $URL$ Alternatively, keep the default policy and try to add to psql invocation. This should trigger a different authentication rule () in the default that will ask for the password. 

As for looking at the console, what you should want instead is having this run in a terminal when you work with the database: 

This error message is new in postgres and is indeed related to the replication bug fixed in 9.3.3 As mentioned in the git commit from Mon, 17 Mar 2014: 

has Unicode non-ASCII quotes and instead of the normal ASCII double-quote. These fancy quotes are not delimiters for the SQL syntax, so they get interpreted as being part of the locale's name. Try with: 

Assuming the default as shipped with Ubuntu's PostgreSQL package, a plausible reason for these huge log files would be that your data import would generate tons on warnings like this: 

In PostgreSQL, an UPDATE is implemented as marking the current row version as dead, followed by inserting a new version of the row with the new values. The entire row is recreated no matter how many columns are updated. Additionally, if it leads to an index change, a new row version is also created in each concerned index. On the whole, it's pretty heavy (compared to overwriting bytes in a file). By contrast, DELETE is implemented as marking the row version as dead, both in data and indexes. Since dead in this context means no longer visible to this transaction and the future transactions (unless it's rolled back), this is a bit similar to your flag, except it's more efficient than an UPDATE since there's no new version of the row. At some point, VACUUM will kick in, presumaby by way of , and mark the dead row as reusable space. Quoting the documentation: