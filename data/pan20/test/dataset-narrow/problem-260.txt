You need to talk to your network admin to see if Kerberos delegation is enabled, and whether it's constrained or not (if it's enabled and unconstrained that is a massive potential security risk, just by the by). If it is constrained then you need to register a SPN (Service Principal Name) for your SQL Server and bind your SQL Server account service to that SPN in your AD configuration. $URL$ has an explanation of it from the point of view of a web server that wants to pass user credentials to a SQL Server (whereas what we're doing here is passing user credentials from SQL Server to a second SQL Server, but the principle holds). If Kerberos delegation is not enabled and your network admin refuses to budge on that (and you can't create SQL logins), then you're out of luck and need to escalate via your management (or think of an alternative to using linked servers), since NTLM authentication (which is the fallback if Kerberos isn't enabled) doesn't work with linked servers. 

No it wouldn't (that is, it's not logically equivalent. Performance would probably be better though). Two reasons: 

That said, based on what I've seen the query plan would look something like this: (this is vastly simplified, the actual output from sp_showplan is rather more detailed) 

(I've used ANSI SQL syntax, I'm not 100% sure if MySQL supports all of ANSI SQL - but it should be close enough.) 

If a has multiple and an has multiple then yes you need a link table between the two (as per the answer in your comment). However, if, for example, you only ever want to attach a single to any given , then you could just have a field for genre inside the table. 

The potential downside of that is data inconsistency (i.e. staff members having overlapping targets) - you can fairly easily write a script to check that though. I'd recommend making an stored procedure and wrapping all the checking/updating logic in that, also. (A couple of style notes - I rewrote your query slightly to seperate the join condition from the where clause. Also, I know it's only an example, but is a bad habit to get into - you land up transferring far more data than you need.) 

That is going to be horribly messy, since you'll need to find the single pair that's the newest prior to the . What I'd do is add another field to go with , and then when you add a new target for a staff member, before you add it check and see if there are any current targets () and if there are then set their end date to the new target start date (optionally: throw an error if there's more than one current target). So, the table turns out looking like: 

1: If the column you're changing is part of the clustered index definition, then yes. If not, no. Any non-clustered indexes involving that column will have to be rebuilt when you change the column type, also. 2: A heap just means no clustered index, so nothing to rebuild. Same as answer #1 for non-clustered indexes. 

First: This isn't easy to implement and it's going to break very easily (by which I mean performance is going to be horrible, and it's a nice little route for SQL injection attacks if you're not very very careful). I strongly advise you to re-think what you're doing because there has to be a better way. Second: The question as it stands doesn't actually make sense - what condition are you checking on the columns? ? Also, do you want to or the conditions together? To actually answer the question, though, to do this you'll need to write some dynamic SQL: 

Example below is, for each column in the table that starts with either 'A' or 'B', return all rows in the table that have a non-null value in any of those columns. Note: I don't have a Sybase install handy to test this. Chances of it actually working are very low, but it should hopefully be enough to make you realise what a bad idea this is. 

(* I'm aware this would probably fail any security audit going, but to my mind if we've let an intruder into the server room that knows to look in the third drawer down for the unlabelled 'sa' password post-it, then we're screwed anyway.) 

You use when a column isn't being filtered on (isn't in the clause) but is being selected. means that the column will only be included at the leaf level of the index (it's a small efficiency saving, essentially). Also as noted, if you have a statement without a where clause, indexes will never be used (faster to just scan the table rather than muck about with an index). Index seek is only used when you're returning a comparatively small number of rows, for your sample query if most of your and rows are marked as Active then the optimiser may well decide to index scan instead of seek. A query that would be more likely to get an index seek instead of scan would be something like: 

(* An extent = 8 pages. will log/remove extents if they're all from that one table, otherwise it'll log/remove pages from mixed extents. ** One side effect of this is that can potentially leave empty pages allocated to the table, depending on whether the operation can get an exclusive table lock or not.) So (back to the original question), is conclusively better than if you're emptying the table out but want to keep the structure (NB: can't be used on a table that's referenced by a foreign key from another table). As noted in @Tullo's comment, also check your database's recovery model - if it's full, then you either need to start taking log backups, or change your recovery model to simple. Once you've done either of those, you'll probably want to shrink your log file as a once-off operation (NB: log file only) in order to reclaim all that free space. Finally, another thing to be aware of - table statistics. run TRUNCATEDELETE` so the query optimiser doesn't get tripped up by old statistics. 

There's no server-level 'read any database' permission and server-level roles can't be granted database-level permissions So yes, you will have to map users to databases individually. If you're using Active Directory, you can create a windows group, then give that group a login to SQL Server, then apply db_datareader in all databases for that group (you'll need to create users in each database though). 

Firstly, the correct term for this is an audit table (or maybe audit history). Transaction logging is something entirely different (that's a core part of the DBMS which logs all active transactions to guarantee the ACID properties) - the DBMS transaction log (which is in a binary, very-hard-to-read format) is reused once it's no longer needed ("once it's no longer needed" is a bit vague, but a full dissertation on transaction log semantics is a bit beyond the scope of this answer), where an audit history stays forever, as long as you don't clear the table. To answer the actual question - it should be done inside the database (triggers being the easiest way). The basic reason for that is you want anything that's integral to the data (i.e. constraints, security rules, audit history etc.) to happen, no matter which application is using the database. Additionally: 

First point is that in any situation where data is changing rapidly and extended-duration table locks are taken, you're going to have induced latency in updates to those locked tables. Unless you use snapshot isolation that is. Caveat: snapshot isolation may not be appropriate for a reporting server (it increases memory usage and can hit tempdb hard), it depends on how hard you're pushing that server. Second point - have you looked into replication, specifically transactional replication? It should provide near-real-time updates without constantly pushing the database into recovery like log shipping does. Note that it may have a performance impact on your primary database, though. Third point - Are your indexes appropriate to the workload your database is undertaking? If you're not sure, profile it. The Database Tuning Advisor is a useful guideline, but don't believe what it says ipso facto (like carpentry's "measure twice, cut once", I'd like to propose a similar rule for database tuning - "profile twice, index once"). Use the Index, Luke! is a good site to start with if you thought an index went in the back of a reference book. Fourth point - if you've got reports that are doing massive rollups (i.e. months of data or whatever), is it okay to have some lag time in those reports? If so, you can schedule a task to create some summary tables (weekly, daily, hourly - depends on your requirements) and have your report hit those instead of summing ten million rows or whatever. Generally if you have a quarterly rolling income report (or whatever), then up-to-the-minute accuracy isn't necessary. 

Much simpler: You can find a full list of styles in Sybase BOL, although the explanation of each style isn't particularly clear (and is actually flat wrong for style 12 & 112). The Complete Sybase ASE Reference Guide includes a full list of styles with examples. (Please note that the complete guide is externally published, not from Sybase. Also I'm not affiliated with the book or the publisher, just mentioning it because everyone working with ASE should have a copy of it.) 

From your description, you'll need the following tables ("..." means you might need additional fields in that entity): 

The last time I played with Access was when 2003 was the hot new thing, so this may not be entirely accurate to every detail. However, what you need to do is go to the query designer, change the view to "SQL" (i.e. raw text entry) and then you want to your two left-join queries together, e.g. 

If you're the dbo (or aliased to dbo) or sa (which is implicit dbo on all databases), then you automatically have all permissions. Otherwise, a anywhere on the permissions chain overrides any . I'm not 100% certain on this, but I think if you have a user in two roles, it will apply all the granted permissions from the roles, then layer any over the top. 

My guess is that it's a legacy support thing (i.e back in version 1 of their software, they had date and time in seperate fields and they've been forced to support it because it always worked that way). That said, there's nothing brilliant about seperating date and time, so there's nothing you're missing. (The other option is that your ERP system developers didn't know that a datetime field can store a date and a time, but far more likely is that they had a business requirement to make it work the same as prior versions.) 

The SessionProductLink is a Junction table, which is the 'bit of the design' you were missing. The field names in italics are foreign keys (it should be obvious which fields they link to ...). the ID fields are the clustered key fields in the first three entities, for SessionProductLink you'd want a compound key of (ProductID, SessionID). (You can use a surrogate key there if you want, that's a style decision you can make) Other things you might want to think about: 

So no it doesn't lock tables, rows or pages. However, any (even with set) will issue a (schema stability) lock, which basically means no schema changes can happen while the is executing (schema changes = add/drop column, change datatype of a column, change nullability of a column and a couple other operations I can't think of off the top of my head). A lock should not interfere with DML statements (///). 

If you need to redesign your database, chances are you'll need to rework some of the associated code too (that's entirely normal), but you should be able to redesign your database without losing any data. (Obligatory note: take a full backup first.) 

You can do this (assuming I'm understanding you correctly) via having foreign keys in your child tables referring to the parent table. 

(This was originally a comment to @DaveE's answer, but I've put it into its own answer because it got long) is a logged operation. It has to be otherwise it's not ACID-compliant. However, differences between and : 

Put an expiry date into your user table, then every week shift all old expired users to an archive table. You don't have to break your data model for temporary users, and you keep a reasonably small users table. 

Edited after a couple of comments by the question asker!: Better idea for your design: (If you're not familar with these diagrams, that's seven tables, with foreign key links as pictured [the three tables all have two foreign keys referencing the data tables]. It probably looks like I'm overcomplicating things, but trust me, "tokenise once and store in the database" is so much more efficient than "tokenise data each time you retrieve it".) Sample data, using a simple tweet (ignoring Users for now, it's the same concept though): "test tweet! #howdoesthiswork #newbie" 

Essentially, the backup will be of the state of the database when it finishes the data-reading portion of the backup (so all of the data will be backed up), plus whatever amount of transaction log is required to ensure transactional consistency (the start time of the included log is ). Paul Randal covers this here (with aid of a diagram, which makes it all so much easier). In your example, would be committed (or rolled back if a was issued instead of a ) and would be rolled back (regardless of the end result of that transaction). (The other reason you try and do backups at a quiet time, aside from I/O contention, is that all of the transaction log generated during a backup normally has to be included with the backup.) The recovery phase of a database restore takes all the committed transactions from the log included in the backup and applies them to the database, and rolls back all the un-committed transactions. (This is why / is important. and you can use the database, but you can't apply any further log backups, you need to restore it in order to roll in log backups. Recovery breaks the log chain by rolling back uncommitted transactions.) Further reading: 

It's called Entity-Attribute-Value (also sometimes 'name-value pairs') and it's a classic case of "a round peg in a square hole" when people use the EAV pattern in a relational database. Here's a list of why you shouldn't use EAV: 

First things first: MS Access was not designed for multi-user access. Every version of Access I've used had a disturbing habit of corrupting tables at a vastly increased frequency if there were >1 users using it. If the two users are connected to the Internet all the time, I'd recommend shifting your table storage to SQL Server and having the users connect to that (use a VPN or some other form of security! If they're on a company LAN it's even better, you shouldn't need a VPN then). It's a fairly straightforward process to convert to SQL Server. The users will still use the Access front end, but instead of having the tables stored inside the .accdb file and having to merge them, the Access tables are converted to linked tables to the SQL Server tables. This is possibly a bit more up-front work, but it'll save you hassle down the road (how often do you need to merge? who's going to do the merging?). Also, if the application ever gets more widely used, you can easily build another front end (in C#, Java, ASP.NET, whatever) and connect it to your SQL Server back end. 

Access is a perfectly fine database system for small scale individual-user apps. Here are some criteria for shifting: 

Short version: It depends. Generally spoken Sybase SQL Server is smart enough to do things the fastest way, though. Long version: Sybase's query processor is, at it's core, very similar to the one used in MS SQL Server. It will create worktables (internal temporary tables; not visible to the user) if the result set is sufficiently large to overflow available memory (similar to a table spill in SQL Server). Otherwise, it'll do a pair of index scans (you do have indexes on and in both tables, right?), then a join and output, all in memory. Caveats: 

DBCC uses snapshots internally. Snapshots are then implemented as sparse files in Windows. So this is actually a problem with Windows' handling of sparse files, which causes the snapshots used by DBCC to occasionally break on large and very active databases. It's reasonably well documented with a few recommended fixes in $URL$ However, if you can DBCC on the dev server I'd recommend that instead. (I've personally only seen this problem with databases that were still serving traffic while being DBCC'd/snapshotted. Backup/restore to an idle server, DBCC there => no more 665 errors.)