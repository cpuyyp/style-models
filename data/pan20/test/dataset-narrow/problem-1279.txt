There are many ways to achieve this. Here's one way, which should be easy to make work with your existing code. Create a Transform and make it a child of your player character. Position it where the attack lands and then add it as a public Transform of the PlayerAttack script. Let's call it attackPositionTransform. Now in your MeleeAttack function, you can compare the position of the attackPositionTransform with the target. 

All files in the Resources folder exist in your built application (regardless of if they are used). This allows you to load them as needed with . I believe Unity made the decision to prevent files in Resources being built into sprite-sheets because there would be two copies of the same sprite in the built app (which is wasteful). This answer on the Unity forums uses the same explanation: $URL$ 

The boards representation in memory will be probably one of the first things you will consider. This link presents one way to represent the board and may be useful in general. In most cases it is not so important as to the specific board storage mechanism used as it can be abstracted away. One way to do AI would be to use the minimax algorithm to look a few moves ahead. You'll benefit here if you kept your worldstate separate to rendering, input etc. 

To reduce the amount of statements you have you need to make your solution data-driven instead of manually adding logic for each UI component that needs to interact. You could do this by storing your buttons data (such as each buttons bounding box and state) in an array of structs or a NSArray (or NSMutableArray) of classes. Then when you receive a touch you just need to iterate over the list to see if any touches are inside the bounding box, if it is then update that objects texture. To keep track of touches across their lifetime I use a single NSMutableDictionary with the touches pointer as its key and a custom class as its value. This allows me to store information per touch (such as if I've already handled it) and react to touches that are held and not moving. 

A visualisation of a minimax decision tree (from the wikipedia article). Down the left hand side is the turn number. The leaf nodes each have an evaluated score which is fed back up the tree to make a decision. however draughts is the game with the highest branching factor that has been completely solved. 

Render your scene to floating point texture (with a format such as A16B16G16R16F) using other floating point textures on your models, and/or lights that may have a brightness greater than 1.0f. To display this texture the range of visible colors needs to be converted to something displayable on your screen - this process is called tone mapping, and a variety of different tone mapping equations can be used to get different effects. This is a must since monitors can't display the full range of colors or luminescence that we can store in floating point textures (it would be cool if it was possible, but it would also be a blinding hazard...). Bloom and other after effects are added to further exaggerate the difference in luminescence of the things rendered. The bloom is calculated from the floating point buffer and combined with the tone mapped image. 

You can go two roads with game development as you have described it. The Hard Way - OpenGL or Direct3D You use the LWJGL or JOGL for Java or OpenGL with Wiggle or GLFW in C++. This method is notoriously difficult and will consume a lot of your time (upwards of 50 hours probably). However, it will pay off in the long run and you will have learned a lot. You will spend more time with the graphics, and less time with the game. The Easy Way - Slick2D Here you use a high-level library that abstracts from all of the intricacies of game development. It's much easier to learn than plain OpenGL or, even worse, Direct3D. Since Slick2D is a Java library, you can generally distribute it easily to Windows, Mac, and Linux. The downside is that it isn't as flexible as OpenGL or Direct3D and you don't have as much control. You will spend less time with the graphics and more with the game. 

I would suggest using the fixed function pipeline as a backbone of your program. I find it to be easier to have something show up using it instead of purely shaders. Then, when you know you haven't messed up your vertices or normals, you can slowly but surely inch your way into the shading world. 

There will only be a slight performance hit if you calculate the orthographic and perspective projection matrix each time you render to the frame buffer. I suggest caching them in a variable, calculating and storing them once in the initialization code, and then using to load the matrix. The rendering method would look like this in pseudo-code: 

It's more clear if you do it the way you described. For me, clarity always wins over performance. It will be easier to trace bugs if you enclose only the code that absolutely needs the shaders with . 

It seems like you are trying to do things backwards. Here is my preferred way of creating an FPS camera: Store the position and orientation in client code (Euler Angles) Define the position and orientations along the x, y, and z axis in the client code. This would would something like this in C++: 

You should most definitely calculate the matrices in client cod, since the matrices do not change on a vertex-to-vertex basis. You are essentially repeating one calculation - of which you know the answer will be the same - for every time the vertex program is run. 

In the input handling code I make sure that the angles are never higher than 360 degrees or lower than 0 degrees to prevent precision loss, but this is up to you. This method is unfortunately prone to the gimbal lock issue. I myself have never experienced this problem with FPS cameras, but you should be wary of it nonetheless. If you wish to avoid this problem, try quaternions. Be warned - they are notoriously difficult to understand. 

OpenGL can be used in C. Some libraries can't, though. For example if you want to use GLM you will have to use C++ instead of C. 

When this happens in my experience it's usually due to a combination of using texture wrapping/repeat and bilinear filtering. The bilinear filtering can sometimes sample from coordinates outside the 0-1 range and because it's set to repeat it will sample pixels from the other side of the image. Try using GL_CLAMP_TO_EDGE (example of usage) to confirm this is the case. There's a few different ways to fix it: 

Agent Knowledge When the agent first enters the map it only knows the amount of doors in the room, and the id of the room it's currently in. I needed to create a structure that would store information the agent had learned such as which doors it had not been through, and where the doors lead to that is had been through. This class represents the information about a single room. I chose to store the unvisited doors as a and the visited doors as a , where the key is the door id and the value is the id of the room it leads to. 

Not only does C# have a future in games development, it has a present - Check out Magicka, a game recently released (and selling well) that was developed using XNA. Also take a look at the winning Dream.Build.Play entries. C# is a viable way to make games (although right now I don't believe it's the right choice for all games). As time goes by we'll see it becoming more popular in the computer gaming industry, however I don't expect a sudden shift to C# at any point. 

It looks like you're missing a separator character in your dependency options, glut32.lib and kernel32.lib are separate libraries. If you open the additional dependencies window, every entry should be on a new line. 

Assuming I were to develop a graphics engine for a modern PC game that used only DirectX or OpenGL, which techniques could I use to make sure it ran quickly? Edit: I'm looking for any approaches that I could take that would result in a speed increase. By graphics engine I mean general purpose graphics engine - such as the one used in Unity or Ogre3D. 

PNG is lossless, but the color depth can be changed. Use jpg for the remaining cases - photographic images, paintings, pictures with a lot of small details, colors and gradients. If you try to use PNG to store these images you'll notice the filesize is a lot larger than the jpg. Tools like adobe image ready will give you a preview of your image along with the filesize in a variety of formats and quality settings. 

Here is an example program that draws a line to a window using the cross platform library Qt. Includes have been omitted. 

This is all highly theoretical but you may be able to use categories (Mac Developer Tips) to override NSObjects (Apple Developer) alloc and dealloc methods. 

In addition to using the code you have above you need to configure the axis in the input manager and name it correctly. The accepted answer here may be of help: $URL$ 

The human vision system includes some (pretty sophisticated) feedback control to allow the eyes to track the relative motion of objects. The ultimate effect of this is that, when you focus on a particular point, the image of that point will continue to be projected to roughly the same location on the retina despite motion of the point and your head. (As a simple demonstration: focus on a word on this page. Move your head around. Note how violently you have to move your head before the word begins to blur and displace from the centre of your field of view.) To emulate this effect, when you displace the camera, you should also adjust the view direction so that whatever the player's eye is focusing on stays in the same location on the screen. Since you don't have any information about where the player is looking, the best you can do is to make the not-unreasonable guess that they're probably looking at the centre of the screen. This will produce an effect more similar to what you 'expect', which is subtly changing parallax as the viewpoint moves. It's probably impossible to completely eliminate nausea. Motion sickness is typically a response to inconsistent sensory input. In a vehicle, the inconsistency is that the inertial information from the inner ear indicates motion, while visual cues indicate that you are stationary. With head bob, the converse will be true. 

The general problem: determine which of all the possible combinations of objects has a nonzero intersect volume. The naive, general approach is simple: For each possible pair of objects, compute the volume of intersect. This is usually not practical, since it requires O(n^2) relatively expensive intersect operations. Hence, practical implementations are often specialized, making certain assumptions to allow the avoidance of intersect checks, or reduction of their cost. Spatial partitioning takes advantage of the fact that objects are typically small relative to the total volume, and will typically reduce the number of comparisons to O(n log n). Axis-aligned bounding boxes and bounding spheres provide inexpensive coarse intersect checks, as long as objects obey certain compactness assumptions. And so on. 

Personally, my preference would be to not use gluLookAt. Instead, I'd recommend using gluPerspective to set your perspective transform. Then, push a translation and rotation to define the global co-ordinate space relative to the camera co-ordinates. (That is, don't move the camera, move the world). Draw things whose positions are fixed relative to the camera either at the begining, before you push the global co-ordinate transform, or at the end, after popping the transform. In this manner, to get a camera at (10,10,0) which is rotated down 30deg about the x axis, you could write: 

Space partitioning would be useless for A* in an established graph. Spatial partitioning speeds collision checking, which is useful when constructing a graph that you navigate with A*. In a static environment, you should be pre-calculating the graph. In a dynamic environment, you will need to do some collision-checking on-the-fly to, at the very least, discover when edges have been broken by changes, and to find new paths.