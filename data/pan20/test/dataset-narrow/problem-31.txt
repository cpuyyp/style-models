Here, the sets of opaque and transparent geometry are mutually exclusive. The benefit of this approach is that geometry is only drawn once and the rasterization state does not need to change. Unfortunately, it could be possible to observe, through the semi-transparent holes of some double-sided object, that object itself. In this case there is no guarantee that fragments of this object would be blended with other fragments of this object, since the order in which the fragments are resolved is and should not be known. Another approach: 

Transparency can be achieved (or actually approximated) by using alpha-to-coverage (in case of MSAA) or basic alpha/transparency blending using the following approach: 

Here, all transparent geometry is drawn twice. First, opaque fragments are processed by the pixel shader whereas transparent fragments are clipped inside the pixel shader. Second, opaque fragments will be discarded by the early-z-test of the rasterizer whereas transparent fragments are processed by the pixel shader (and blended afterwards). Both approaches have some differences, but the difference that concerns me the most is the latter approach having two variants of the pixel shader: one that clips transparent fragments and one that does not clip transparent fragments. In case of multiple pixel shaders, this approach will always result in a doubling of the number of pixel shaders. Is it possible to have the benefits of the second approach while having only one variant of the pixel shader? 

Solution 3 Ensure the 16 byte alignment by overwriting /, /. We declare a template class which does allow heap allocation with arbitrary alignment restrictions: 

Bind a rasterizer state with a Less-Equal comparison function. Bind no blend state. Render opaque geometry. Bind alpha-to-coverage or alpha/transparency blend state. Render transparent geometry. 

This seems to result in the correct behavior (see the shadows of the leaves, pillars and curtains). The bright border on the first image starts at the position of the spotlight and is due to the near plane distance of 0.1. This border will completely vanish after multiplying the shadow factor with the spotlight's contribution. Omni light My omni light's intensity is cut off at a distance of 3. The corresponding light camera has a near plane at a distance of 0.1, a far plane at a distance of 3, an aspect ratio of 1 and a vertical/horizontal FOV of $\pi/2$. This means that one of the six light cameras (after applying some rotations), is completely identical to the spotlight camera above. I double checked this in the code: Frame 1: (spotlight): 

Spotlight My spotlight's intensity is cut off at a distance of 3 and at an angle of $\pi/4$ radians (umbra angle). The corresponding light camera has a near plane at a distance of 0.1, a far plane at a distance of 3, an aspect ratio of 1 and a vertical/horizontal FOV of $\pi/2$. The spotlight is positioned somewhere above the tree and faces downward to the floor. The shadow map of my spotlight has a resolution of 512x512. I use the following DXGI formats: 

I finally found the cause of the problem. There appears to be a problem with the shadow map of both the omni light and spotlight. While debugging, I noticed by accident that the light-view-to-light-projection () and matrix entries were not equal. Due to the aspect ratio of 1, both matrix entries must be equal. Furthermore, due to the FOV of $\pi/2$, these matrix entries must be equal to 1 (assuming no floating-point precision issues). The constructor of my class expects a , , , , order or , , , order and correctly handles the creation of the view-to-projection transformation matrix via redirecting to expecting a , , , order. Since, I did not want to create a corresponding for each light per frame, I bypassed that class by directly providing a member method in my and classes redirecting to using the wrong order of arguments. This explains why the above images are wrong. The generated shadow map of the spotlight is still equal to one of the six shadow maps of the omni light, although both use the same but wrong transformation matrix (which contains the wrong transformation matrix). The HLSL code for the spotlight uses the transformation matrix, which includes , and thus will perform the "right" mapping. The HLSL code for the omni light uses the transformation matrix, which does not include , and thus will not take the non-uniform scaling of the 00 and 11 entries into account, resulting in some non-linear stretching and thus the wrong mapping. This explains why the above images differ for the omni light and spotlight. After fixing the bug: Omni Light (6 faces) shadow factor: 

This code snippet is based on the DirectXTK and is also included in my own codebase (including documentation). This data structure can now be used as follows: 

Bind a rasterizer state with a Less-Equal comparison function. Bind no blend state. Render all geometry and clip transparent fragments Bind a rasterizer state with a Less comparison function. Bind alpha-to-coverage or alpha/transparency blend state. Render transparent geometry. 

A more recent paper (2005 at least ;) ), has a more concise notation while comparing multiple BRDFs including the Cook-Torrance BRDF. Their formula does not include the division by 4. Addy Ngan, Frédo Durand, Wojciech Matusik: Experimental Analysis of BRDF Models, Proceedings of the Eurographics Symposium on Rendering 2005. Project Page, Supplemental (Take a look at the supplemental!) Note, however that the Cook-Torrance BRDF is not equal and thus not a synonym for the Torrance-Sparrow BRDF. The latter includes your division by 4. An interesting reference overview can be found in: Rosana Montes, Carlos Ureña: An Overview of BRDF Models, Technical Report, 2012. The same Cook-Torrance BRDF formula is also present in: Philip Dutré, Kavita Bala, Philippe Bekaert: Advanced Global Illumination, 2nd Edition, 2006. Edit: I looked at some (isotropic) implementations of the F, G (or V depending if you factor the foreshortening in the denominator into G) and D: 

I read the MTL specification for materials and noticed that the specular exponent Ns must be a scalar value instead of a spectrum. Would a specular exponent with three color channels (i.e. RGB, XYZ, ...) make sense (i.e. could represent (the specular exponent is of course not physically based at all) something physically plausible)? 

The maximum allowed number of threads per compute shader group is 1024 for Shader Model 5.0. Is it advisable with regard to performance to stay close to this maximum number? In order to resolve SSAA and MSAA (down-scaling with appropriate tone mapping), I wrote some compute shaders. One general compute shader works in all cases and performs one task without synchronization for each output texel and uses [numthreads(16, 16, 1)] (= 256 threads/group which is a multiple of NVidia's warp size of 32 and AMD's wavefront size of 64). Algorithmically, this does not result in the best performance, since for MSAA $T$x, each task performs $T$ texture loads from the radiance, normal and depth textures, and for SSAA $T$x, each task performs $T \times T$ texture loads from the radiance, normal and depth textures. So instead of dispatching jobs in terms of output texels, I have some specialized compute shaders performing tasks with synchronization for each input texel (for SSAA) or for each subsample of an input texel (for MSAA). Here, each task performs $1$ texture load from the radiance, normal and depth textures and stores the results in group shared memory. Next, a takes place and only one job per output texel continues to finish the work. This is not ideal in case of many SSAA and MSAA samples, but for few samples such as MSAA 2x, one expects to see some gains. In practice, however, the general compute shader outperforms all the specialized compute shaders for both small and large numbers of samples/ouput texel? SSAA 2x [numthreads(4, 16, 16)] = 1024 (multiple of 64): ~655 FPS vs ~577 FPS SSAA 3x [numthreads(9, 8, 8)] = 576 (multiple of 64): ~265 FPS vs ~200 FPS SSAA 4x [numthreads(16, 8, 8)] = 1024 (multiple of 64): ~127 FPS vs ~75 FPS Can this be solely due to dispatching more threads/group? Can this be due to the synchronization of all threads per group (close to the maximum allowed number of threads)? Or is there some performance one needs to sacrifice in case of using a 3D instead of 2D block of threads per group? 

If you use instances of the struct as member variables in other structs of classes, the alignment restrictions remain of course. So instead of having only alignment restrictions for and , you will have now alignment restrictions for , and . Smart Pointer Pitfalls Be careful if you use and especially . The latter allocates the data block together with the control block without using customly defined allocators such as our custom . This means that this method will not work: 

and especially will work as intended since the latter explicitly calls (due to the absence of a control block). RasterTek RasterTek's second series of D3D11 tutorials is a subset of RasterTek's first series of D3D11 tutorials with the minor difference that the latter uses the obsolete . The transition is from D3DXMath to DirectXMath is actually pretty straightforward. Therefore, it can be interesting to take a look at the first series as well (which I ported to , run fine in both and configurations, and can be founded as well in this repository). If you want to stick to , you can also take a look at the D3D11 tutorials of Microsoft itself (explanation + code). (I also created a repository with some code refactors of these tutorials). 

Assume that we have voxelized the surroundings of the camera viewpoint in a 3D scene into a regular 3D grid of cubes, storing the average normal, average (diffuse) radiance and some flag indicating whether or not a voxel is empty. Next, the finer levels of the 3D texture (voxels) are downsampled to get coarser levels ("pre-integrated") that will be stored as MIP levels for that texture. (I know that some sparser structures can be used instead, but a regular grid is conceptually the most easiest to master first.) Now, given that we have our surface position (and thus also the voxel we are in) and our surface normal, how does voxel cone tracing work and how is it applied? 

Problem and (which use under the hood) require 16 byte alignment. The C++ compiler can automatically ensure this for and data allocated on the stack, but can't ensure this for and data allocated on the heap. In your specific situation (referring to the RasterTek tutorial), three member variables of your class are declared to be of type and you construct one instance of this class on the heap by using the default (due to the absence of a custom one). Using the default does not guarantee this 16 byte alignment. Even if your code would have run fine using the default , you could still face this issue after adding some extra member variables in the future. Solution 1 The most easy fix in this specific case, is to allocate your class instances on the stack instead of on the heap. This, however, does not prevent you or future users of your code to allocate an instance on the stack instead. Solution2 The programmers of the DirectXMath library advise the general programmer audience to use and for calculations only, and to not use them as containers for storing persistent data (i.e. class member variables). For the latter, they advise to use , , etc. The latter structs do not impose the 16 byte alignment restriction and their instances can simply be loaded in SIMD registers by using for , for , etc. 

represents the world-to-light-projection transformation matrix (world-to-camera-view-to-world-to-light-view-to-light-projection). I go to camera view first to mimic the same multiplications between my depth passes and shading passes to reduce z-fighting (although, this does not result in visual differences so far). So apart from the fact that a spotlight corresponds to one DSV (and is part of one SRV to a for all spotlight shadow maps) and an omni light corresponds to six DSVs (and is part of one SRV to a for all omni light shadow cube maps), the shadow map generation is pretty much the same for both types of lights. The shadow map of my omni light has a resolution of 512x512. I use the following DXGI formats: