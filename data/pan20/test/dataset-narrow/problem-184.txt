You might want to look at hash partitioning. You just define how many "buckets" you want and a column to hash by, mysql takes care of distributing the rows for you. 

Before any percona script it will give you TONS of debug info that can help point you in the direction of what's going wrong. Also remember these are all perl script so you can inspect the source yourself to (maybe) see what's going wrong. 

where you enumerate all the individual dates you are interested in. I have come across situations where it is clear MySQL is not smart enough to effecitvely use a range query (i.e. between x and y) verses enumerating specific values for small sets. Which ever the use case you'll want to make sure that column is indexed if you're making regular reporting queries based on its values. 

Innodb slave side only seems odd to me. The main reason you should look toward that engine is data consistency and disaster recovery. This means you want it on the master if you have to choose one. Further, row level locking innodb provides will better enable you to potentially avoid table locks blocking master selects while writes are happening. This of course depends on your access patterns. I've had mixed results with compressed rows. In my experience they can definitely save space on things such as log records, but obviously you're taking a cpu hit. If you're resource constrained anywhere carefully weigh and bench mark whether it's better for you to be CPU bound or I/O. You stated you are high I/O but i'm not sure what else is going on, if this is a dedicated server, etc. If you're running percona look at the *information_schema.INNODB_CMP* table for compression stats on time spent doing those operations. Play around w/ differen key_block sizes when setting compressed rows if you go that route to see what works best for you. Further, be sure your file_format is set to Barracuda. If it's set to antelope the alter will succeed leaving you just wasted time as your rows are not really compressed then. Partitioning by date could be wise for logs as you'll likely be querying by that dimension. Keep in mind to partition by range that axis has to be part of your primary key. If you'll be querying by more than just date you'll probably want a composite key. If you're doing that and innodb remember large PKs quickly bloat your index requirments as a copy of the full PK is stored for every secondary index. If you have secondary indexes keep in mind partitioning by range can actually hurt your performance as it now has to scan every partition for matches and you loose the benefits of partition pruning. 

Some additional information could be found here as well: Considerations for Installing Integration Services. You might want to take under consideration the licensing as well and still validate the standalone server is licensed properly. 

Any account enclosed in "##" are certificate based accounts and you will not be able to simply create them as you would other logins/users. If you have a backup of the master database prior to the accounts disappearing you could might try restoring the database to get them back. Other than that I suspect you will need to reinstall or try rebuilding the master database. Read through these two articles (here, here) for rebuilding the system databases. I would suggest taking a backup of the current state of the system databases. Script out all the logins you have in master as you will loose these. 

So I would necessarily say it the transactions to the database on the secondary but it them to the secondary. Then depending on which mode you are in would be when the transaction is committed to the database on the primary (synchronous or asynchronous). So I would probably state it as: 

There is no defined max number of logins and users in SQL Server. Although, based on how your application is configured you can hit the max number of user connections you can have to a given database (32,767) rather quickly. 

The documentation in BOL shows the process to create the SSISDB (SSIS catalog). You will have to have SSIS already installed on the given SQL Server instance before trying to create it. The only reason this catalog would be required is if you are using the project-deployment model for your SSIS environment. If you are using the file system or SQL Server store/msdb then you do not have to create it. 

We've encountered a couple use cases where having federated tables prove useful, despite their limitations and caveats. As best I can tell this must be enabled with a federated declaration in the .cnf upon startup. Our use cases are limited and certainly don't warrant restarting every instance just to have it. At the same time I'm thinking of adding it as a default to our standard config so it is available on instances as they get bounced during more pressing maintenance. Is there any reason not to have this option available unless decided absolutely necessary? For a final clarification: This question is not about the disadvantages of actually using a federated engine; rather it is just about just enabling as an available option. Perhaps to put it another way: Is there good reason it is disabled by default other than making sure you don't shoot yourself in the foot if you understand the consequences? 

I was going through compressing a bunch of archival myisam tables using myisampack It was making decent progress until it got to the most recent tables. The overall database is a set of archives for a single production table. Each archive version represents 1 quarter of a year going back several years. Nothing has drastically changed in the data being stored or the average row length. The individual archives have been growing with time as there simply are more on more rows each quarter over the previous one. Up until the most recent 4 quarters the rate of compressing/rebuilding the index (which is only a single int PK) was around 14-16k rows/seconds. The last ones were crawling along only doing about 1k rows/second. Even right from the start; it wasn't like it started of as fast as the others and then suddenly slowed down after getting so far into it. There wasn't any spike in general system load or i/o contention from other processes when it started slowing down. Has anyone experienced this? Is there something myisampack where it's overall rate of progress it should be expected to tank when compressing a table over a certain size? Edit: Since these are myisam tables I should point out they are all in the same datadir and all stored on the same physical array. 

You can find the logins on an instance by querying and checking the column to know if they are still useable. 

As long as you have given any account that needs explicit permissions to those objects (views/procedures) you should see no issues. The issue I usually see after altering PUBLIC role permissions is applications that assume the permissions are there instead of just giving explicit permissions to them. You will find that some Microsoft documentation states to not alter the PUBLIC role permissions. I work with the IASE database checklist and the permissions given to the PUBLIC role is one of the first things we have to remove/revoke. I always keep the script handy to restore the permissions if needed. EDIT Query to find public permissions that I use sometimes: 

Additional: If you add this as a step to your job just make sure the previous step is set to only "execute next step" on successful action. On failure for your previous step should be set to stop job with as failure, based on your requirements. 

A database shrink is likely what you performed. Although ill-advised, if you need to regain disk space or just bring the data file (MDF) of your database under control you will need to perform a file shrink. Again there are plenty of articles on MSDN and blog post that provide the way of doing this. 

As stated in the documentation you do not pass the UID and PWD to utilize Windows Authentication. You can see an example here. From remarks within documentation link: 

You have many state changes you want to happen in order for your interaction to be considered a success. This pretty much defines the "atomic" goal of a transactions. 

You can tweak this value in your cnf to be a bit lower. Changes to this will require a server restart. In a perfect world, though, you'd want your buffer pool size to be large enough to hold all your innodb tables in memory for fast performance. Of course this isn't always feasible. 

I've been able to find some stored procedures that do this for other brands of SQL than I use. I realize all the required information is in information_schema and I could use that to build the create statement myself. It just seems like this is a problem that's been solved in other flavors and there's likely already a stored proc out there to do what I want w/o reinventing it myself for mysql. Why I'm trying to set some dynamic schema introspection as part of documentation. The idea is I'd like to have the doc pages illustrate a current representation of the schema in the various environment it may exist in (dev, qa, prod). The "obvious" thing to do is just query the dbs and "show create table blah". However doing that requires select access on the table. I don't really care to create an account with global read access to every table in every DB. I'm looking to either have a restricted script on a cron that runs/collects the output of show create table; or have a mysql event that constructs the output I want in an event. Either way this output would ultimately be persisted to a documentation db w/ just the schemas stored so the documentation app can pull this information without its db account having select access to the actual tables. I'm tempted to lean toward an internal event that populates this just so there's not an account out there with wide read access. Why the why I'm explaining my ultimate goal in case someone can point out a different implementation path or solution that matches my requirements (or convince me my requirements are off to begin with). 

Underlying SSIS task that do not require SSIS to be installed to run, nor require you to edit via BIDS or SSDT-BI. They are there more or less for easy deployment of basic database maintenance that does not require much knowledge to build. Is it something that fits any environment? I would say no, it is mostly for the one man/woman shops that just need something up and running quickly and is sufficient. Now that is not to say in large environments maintenance plans don't have a purpose, just find they may not used as often. You might consider them a stepping stone to SSIS packages. 

The only thing I am getting right now to figure out is an error on . Which I do have the March Preview of SSMS 2016 on my laptop, not sure if that has something to do with this or not. 

If your current database is made up of one data file and one log file then that is how it will be restored. What purpose does the network admin in doing this if the current database is one single data file already? Does network admin understand how file structure works in a database? If you just wanted to split the tables and indexes between multiple files you would need to manually create each in their own file group, assigned to its own physical file. Then you could import the data into those tables. The other way is to do that same thing after you restore it. Outside of that if you involve partitioning tables between files it becomes more cumbersome and involved. 

After upgrading to Percona 5.6.20 from 5.5.34 I noticed counts on larger tables take a massively long time to execute. The table is about 27G and 300M rows. It is not compressed and was innodb before the upgrade as well. Before a count(*) on that table would take maybe 3 minutes. Now, it's consistently taking over a day to run. The last one I actually let spin until it finished took 36 hours. The DB is on the same hardware as before the upgrade. Buffer pool size remained the same (20G). One config change was increasing the number of buffer pools from 1 to 8. Has anyone ran into this before? Are there any new 5.6 cnf settings that might be related to this? Table schema in question 

You're using mysql 5.1, This means the only engines you'll have available are MyISAM or innodb. in both cases adding a new column, regardless of nullability will require a complete, blocking, table rebuild during alter table. You maybe be able to use the pt-online-schema change tool that does some tricks to rebuild the table in a less blocking manner but may not work out if you have complicated composite primary keys. If you have a more recent version (5.6.22 Percona Build) you could use a tokudb storage engine that allows you to to "instantly" add columns. The way it's indexing works it propagates changes down as they're accessed. 

So to be clear the old slave (oldS) and the first new slave (newS1) share the same server ID. It's not circular replication so I'm hoping things will turn out okay. I wouldn't have expected the fallout though. Alarms started going off b/c oldS1 started falling farther and farther behind. Looking at the logdir it was making thousands and thousands of empty relay logs. I stopped slaving on newS1 and that seemed to clear things up in that oldS1 stopped making empty relay logs and caught back up. Both slaves seem to be in a consistent state up to the point I stopped slaving on newS1. 

I would suggest taking a look at a free e-book Red Gate published here: Performance Tuning with SQL Server Dynamic Management Views. It is free and a good read. It will point out those DMVs that provide cumulative data versus point-in-time (what is currently in the buffer). As long as the server has not been rebooted you are in good shape to capture some good information. 

Brent noted on it in his answer but just to expand a bit more...I'll just note around learning the BI side of things that using Azure (or other cloud providers) may get a bit expensive. You will need a minimum server size to truly work with those components and running those (or forgetting to turn them off) can drive your bill up rather quickly. [e.g I've forgot to turn one server off and it ran for a day or so, had a bill for about $40.] Now that is not to say that you should shy away from doing anything in the cloud. With Azure you can get a $25 credit each month from Visual Studio Online, same place where you download free Dev Edition of SQL Server. While it is not much to do things for days at a time, it will at least get you familiar with that side of the DBA/BI world. I actually do most of my learning with BI locally on my laptop, just have local SQL Server instance installed and then Visual Studio 2015 + SSDT. (VS 2015 is current version of this writing.). I play around with data sets I download from Internet and just load them into a database...then you can play with designing SSRS report around it. You also have the data dump from StackExchange to use as well, which was just updated a few months ago. 

Promoting a slave would probably be my preferred route. As you pointing out any selects on MyISAM tables would require table level locks. There is one tool that might be able to help, pt-table-sync. It's primary purpose is to find gaps and differences in existing master slave relationships. A nice thing about it is it does this in nice "chunks". Think of it kind of like antilock breaks. The chunk size is configurable but you could, for example go through doing 1000 rows at a time, minimizing lock times and letting things flow through. I haven't used it to fully repopulate a slave from scratch though although I'd give that a look. Once you have a full copy, do another run to catch new stuff, updates that have come in. Do a flush table with read locks, do a final table sync. run show master status to get the binary log position to start slaving from, unlock tables. Oh, if you don't have binary logging you'll at least need a master bounce and cnf change unfortunately. Another approach that might be much simpler depending your write downtime tolerance. You're all myisam, you gave the size in rows by what's the disk footprint in MB or GB? Figure out how long it would take to transfer that size between your machines (hopefully there both in the same local network). You could do flush table with readlock, again still show master status, then just rsync the .MY* files over to your new DB's datadir. One final alternative, depending on how you're setup: Can you do an LVM or other kind of filesystem snapshot? This would be the best way to minimize downtime. You flush tables with readlock, show master status, start snapshot, then unlock the tables to allow full read write activity to flow through. The difference here is you'll copy the snapshot you started. You'll just need to feel confident the write activity won't exceed the snapshot size you allocate before the copy finishes. What ever method, before promotion I would verify the character set conversions went as desired. They can be a real pain to reverse. I would also recommend upgrading to innodb if possible to make it possible to use xtrabackup in a non blocking fashion in the future.