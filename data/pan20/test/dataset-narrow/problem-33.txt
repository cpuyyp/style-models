I want to rotate a 3D object. On this website $URL$ the first step says: (1) translate space so that the rotation axis passes through the origin What is meant by "space"? Is it every point on my object? If so, the matrix he provides would just mean that all the points would end up being 0,0,0 when I multiply it by that matrix. That would mean the rotation matrices would have no effect. 

This is what my lecturer told me in the coursework: We only consider 4*4 matrices. These are used to rotate, scale or translate objects (or any combination of these operations). Matrices are also used later in the implementation of the virtual camera model. If you do not know the difference between a vector transformation and point transformation, look it up. I can't seem to find an answer and made an account for this website just for this question. 

The above image could not be created using unassociated alpha, and is symptomatic as to why ray tracing renderers cannot generate any alpha model other than associated alpha. The fundamental difference between the two alpha formats is defined by the two different over operations, as described by Porter Duff. Associated alpha over: 

In an associated alpha image, the alpha channel could be considered to represent a distillation of the geometry of occlusion covering a given pixel coordinate. 0.5 would represent a baked down 50% occlusion, while the RGB values would represent the degree of emission. This also means that an RGBA triplet such as 0.0,12.4,10282.9,0.0 is entirely valid, and represents a pixel that is emitting and unoccluding. Naturally occuring things such as candles represent such a combination. Consider the following image: 

Disclaimer: I have no idea what is the state of the art in the environmental map sampling. In fact, I have very little knowledge about this topic. So this will not be complete answer but I will formulate the problem mathematically and analyze it. I do this mainly for myself, so I make it clear for my self but I hope that OP and others will find it useful. 

$$ \newcommand{\w}{\omega} $$ We want to calculate direct illumination at a point i.e. we want to know the value of the integral $$ I =\int_{S^2} f(\omega_i,\omega_o,n) \, L(\omega_i) \,(\omega_i\cdot n)^+ d\omega_i $$ where $f(\omega_i,\omega_o,n)$ is BSDF function(I explicitly state dependance on the normal which will be useful later), $L(\omega_i)$ is radiance of environmental map and $(\omega_i \cdot n)^+$ is the cosine term together with the visibility(that what the $+$ is for) i.e. $(\omega_i \cdot n)^+=0$ if $(\omega_i \cdot n)<0$ We estimate this integral by generating $N$ samples $\omega_i^1,\dots,\omega_i^N$ with the respect to the probability density function $p(\omega_i)$, the estimator is $$ I \approx \frac1N \sum_{k=1}^N \frac{f(\omega_i^k,\omega_o,n) \, L(\omega_i^k) \,(\omega_i^k\cdot n)^+ }{p(\omega_i^k)} $$ The question is: How do we choose the pdf $p$ such that we are able to generate the samples in acceptable time and the variance of the above estimator is reasonably small. 

If you look closely, you will see that along the edges of the green red axis, and the red blue, that there is some "escape" of the values where the primaries mixed cannot replicate the range of visible lights that the experimental observers were attempting to replicate. What we learn from this is that any combination of tristimulus physical lamps can never encompass the entire spectral locus shape. In order to fully enclose the unique shape of the spectral locus, it would require using primaries that consist of imaginary visual lamps that would form a triangle well beyond the spectral locus tips. Examples include the imaginary XYZ lamps, or the ACES lamps. Those lamps of course do not exist in the physical sense. So with all of that said, is it plausible that such a display could exist in the future? Absolutely. If we switch to a 40 lamp display, or something close to that, we could replicate the various wavelengths that enclose the entire shape of the spectral locus and fully encapsulate the CIE standard observer response. 

If I have a point in world space: (wx, wy, wz) and I have a centre of projection: (cx, cy, cz) and I wanted to project that point using perspective projection would my point on the screen be calculated by: x = (cz * wx) / wz y = (cz * wy) / wz I got this from this lecturer $URL$ I don't understand what he means by the variable 'd'. What would the variable 'd' be? 

Consider a triangular face of three vertices A(0,2,1), B(3,0,1) and the origin, and the normal vectors at the vertices are nA=(0,0,1), nB=(1,1,0) and nO=(-1,-1,1), respectively. The incident light is white and directional in the direction of L=(0,3,4) and the intensity is 1, the background ambient light intensity is 0.1, and the diffuse reflection coefficients for (red, green, blue) are (0.6,0.7,0.8). No specular light contribution need be considered. How do I find the (red, green, blue) intensity values at the centre of the face using flat shading? Flat Shading: I = Ia x Ka + Fatt x Il x (Kd x (N x L)) Final intensity = ambient reflection x ambient reflection coefficient + (attenuation x incident light intensity x (diffusive reflection coefficient x (surface normal x Light intensity)) My lecturer told me that because ambient reflection coefficient and attenuation are not given, I can forget about it. Also specular light contribution need not be considered. The surface normal is: A = (0, 2, 1) B = (3, 0, 1) O = (0, 0, 0) U = A - O = (0, 2, 1) V = B - O = (3, 0, 1) Surface normal = U x V (cross-product) = (2, 3, -6) I = 0.1 + (1, 1, 1) x ((0.6, 0.7, 0.8) x ((2, 3, -6) x (0, 3, 4))) I = 0.1 + (1, 1, 1) x ((0.6, 0.7, 0.8) x (0, 9, -2.4)) I = 0.1 + (1, 1, 1) x (0, 6.3, -1.92) I = (0.1, 6.4, -1.82) = RGB intensity values Is this correct? 

In an EXR, the convention is to store associated alpha, also known as premultiplied. The latter term however doesn't do justice to the nuances of the alpha format as compared to its evil distant cousin known as unassociated alpha, aka straight or key alpha. In an associated alpha image, two facets are represented via the RGB triplet and its associated alpha channel: 

The answer should be a theoretical no for any tri-light system, or even four light. The reason is, if you examine the original CIE RGB primaries, you will see that the positions of the three CIE 1931 lights correspond to the tips of the triangle in the spectral locus image here: 

Based on the names of mentioned papers I can partially guess what they do(unfortunately I do not have the time and energy to read them right now). But before discussing what they most probably do, let's talk about power series a little bit :D 

If we have a function of one real variable e.g. $f(x)$. Then if it is well behaved then it can be expanded into a power series $$ f(x) = \sum_{k=0}^\infty a_k x^k $$ Where $a_k$ are constants. This can be used to approximate $f$ by truncating the sum at some step $n$ $$ f(x) \approx \sum_{k=0}^n a_k x^k $$ If $n$ is sufficiently high then the error is really small. Now if we have function in two variables e.g. $f(x,y)$ we can expand it only in the first argument $$ f(x,y) = \sum_{k=0}^\infty b_k(y) \, x^k $$ where $b_k(y)$ are functions only in $y$. It can be also expanded in both arguments $$ f(x,y) = \sum_{k,l=0}^\infty c_{kl} x^k y^l $$ where $c_{kl}$ are constants. So function with real arguments can be expanded as sum of powers of that argument. Something similar can be done for functions defined on sphere. Now, let's have a function defined on sphere e.g. $f(\omega)$. Such a function can be also expanded in similar fashion as function of one real parameter $$ f(\omega) =\sum_{k=0}^\infty \alpha_k S_k(\omega) $$ where $\alpha_k$ are constants and $S_k(\omega)$ are spherical harmonics. Spherical harmonics are normally indexed by two indices and are written as function in spherical coordinates but that is not important here. The important thing is that $f$ can be written as a sum of some known functions. Now function which takes two points on sphere e.g. $f(\omega,\omega')$ can be expanded only in its first arguments $$ f(\omega,\omega') = \sum_{k=0}^\infty \beta_k(\omega') \, S_k(\omega) $$ or in both its arguments $$ f(\omega,\omega') = \sum_{k,l=0}^\infty \gamma_{kl} \, S_k(\omega)S_l(\omega') $$ 

There has been quite some research into this using Barten contrast sensitivity function. It is the current formula behind the Dolby Perceptual Quantizer as featured in SMPTE 2084 and HDR10. This, coupled with a colour appearance model such as the work behind Dr. Mark Fairchild's CIECAM02, can result in very accurate predictions of quantization depth. 

The nuances of the difference mean that care should be taken when dealing with alpha and the RGB channels. 

Offset. While any transform between two RGB colour spaces can be accomplished with a pure 3x3 matrix, some alternate transforms, such as to YCbCr with given weights, requires an offset. 

Best method Pick $p$ proportional to the integrand $$ p(\omega_i) \sim f(\omega_i,\omega_o,n) \, L(\omega_i) \,(\omega_i\cdot n)^+ $$ But most of the times it is very expensive to generate a sample according to this pdf, so it is not useful in practice. Methods suggested by OP: Method one: Choose $p$ proportional to the cosine term $$ p(\omega_i) \sim (\omega_i\cdot n)^+ $$ Method two: Choose $p$ proportional to the EM $$ p(\omega_i) \sim L(\omega_i) $$ 

So the idea is not to expand in all arguments. One method which might be worth investigating would be to ignore BSDF and expand only the environmental map i.e. $$ L(\omega_i) \approx \sum_{n=0}^K \beta_n S_n(\omega_i) $$ this would lead to pdf: $$ p(\omega_i) \sim \sum_{n=0}^K \beta_n S_n(\omega_i) (\omega \cdot n)^+ $$ We already know how to do this for $K=0$, this is nothing but the method one. My guess is, it is done in one of the papers for higher $K$.