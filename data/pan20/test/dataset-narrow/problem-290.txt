This is not an answer, but instead a (hopefully) better explanation of my comment left on David Browne's (much better and hopefully accepted) answer. First, David's answer is the best approach in my opinion, and he does a great job of quickly touching on the major concerns with multi-tenant database design. Use partitioning! Use it! Partitioning is the right feature here and works with NCCIs as well. My comment about further improving NCCI performance by using a filtered NCCI was not meant as an alternative to partitioning, rather (depending on your data behavior) you may want to use a filtered NCCI in addition to partitioning. A filtered NCCI is a good option when the following is true: 

One potential cause could be that the default database listed for the user account being used to connect to SQL Server is pointed to the wrong database. Adjust this on the SQL Server side and see if that resolves your issue. 

Your trigger can then update the PKEY2 column on the Parent table as part of the updates made after the child table: 

If you're using the option found in the Server Properties -> Security window (screenshot below), you can view the logs captured by this in the SQL Server ERRORLOGs, which are found under the Management -> SQL Server Logs folder. If you want to filter your service accounts out of this, you'll ned to setup a SQL Server Audit as you are unable to customize the basic Security auditing to anything other than None, Failed, Successful, or both Failed and Successful. If you are interested in setting up an Audit, be sure your instance is patched to SP1 CU1 or later if you're not using Enterprise edition as this feature was exclusive to Enterprise prior to this patch. Here's a blog post by Eralper Yilmaz that runs through how to set one of these up. The Option: 

First and foremost, look at your SAN vendor's documentation for recommendations regarding storage for SQL Server. This really should be your first step as the vendor hopefully has done a lot of this general analysis for you. If the documentation doesn't mention hosting databases, do some more digging before you choose to go with Tiered Storage. Understand how data migration between tiers works on your SAN and how frequently it occurs. Beware of anything that will run a scheduled job that tracks how active sectors may be during a time frame (often a 24 hour period, but this if often configurable). I've found that in this scenario when it comes time to migrate the data, that data is often slow to access during the operation. You'll want to insure this process runs during a period of low db activity (which means don't run this during the same time you're backing up your database). If you have a database that's active all day, I'd recommend you NOT use tiered storage at all if this is how tiering works on your SAN. Another problem with migrating data between tiers when this type of analysis is performed is that often times data access patterns change and are not consistent throughout the day. This can result in data living on the faster tiers that really shouldn't be there. For instance, when you run Index Maintenance overnight the SAN may flag that data as hot and migrate it to a higher tier. If those indexes don't get accessed during the next migration analysis window, you're now wasting I/Os on idle data. Depending on your database usage patterns, this could happen quite often where your data is getting migrated to the faster tier only to sit there idle. Again, look at your vendor documentation. I would hope they clearly outline the recommended approach needed for their hardware. Also, give SAN Storage Best Practices for SQL Server, from Brent Ozar a read. That goes into much more depth on some best practices and is well worth the read. 

Quite simply, is there a way to adjust the default value of the FailPackageOnFailure task property to True instead of the regular default (of False) for new Control Flow Tasks? Here's an image of what I'm talking about: 

Some notes, DBCC CHECKIDENT requires db_ddladmin privileges or higher to use. Because of this, I couldn't include a DB Fiddle example. Also, and this may be something you want to dig into as well, but I believe that pushing data from a "test" environment qualifies said environment as "production worthy" which may affect your licensing. Licensing is out of scope, but this could bite you depending on how you have your server licensing setup, so just be wary of promoting data up your environment stack. 

For most typical implementations of SQL Server (e.g. all objects are stored within the schema), I cannot think of a permission set that will allow you to do this without the inclusion of some DDL trigger that prevents certain operations against a user, just as @dbaduck alludes to. While permissions are always prioritized over permissions (ref), this scenario is a bit of a paradox because the moment you grant a user the ability to control permissions on schema objects is the same moment that user can now any restrictions you would have imposed upon him/her as is related to said schema objects. Convoluted, right? If you love confusing graphs, I'm going to point you to MS's Permissions (Database Engine) article. Within it, you'll see the way that permission inheritance is laid out. What this shed's light on is that permissions on a user are not the same as permissions on the a schema, and because of this there's really no clean way of doing what you want without some DDL trigger tomfoolery. 

Stop inserting new records into the old table, and instead insert them into the new table. We're not allowed to insert records into the Partitioned View because we're using values in the underlying tables. Partitioned Views don't allow for this, so you have to insert records directly into the new table in this scenario. Adjust any queries (that point to the old table) to point to the Partitioned View. Alternatively, you can rename the old table (e.g. TableName_Old) and create the view as the old tables' name (e.g. TableName); how fancy you want to get here is up to you. 

Precision and Scale are based on the mathematical concept of Significant Figures. You're not alone in that the current definition/usage for these terms is confusing. 

Sure, you can use PGP, but your encryption and decryption routines will need to be handled at either the Application layer or by custom CLR routines. In either case, a rewrite to something is likely needed. There are also native alternatives as already discussed that I think should be used over PGP, but really your criteria will dictate what will work better. 

Conceptually, yes, SSMA is well equipped to handle large Oracle DB migrations. However, every environment is different and depending on what idiosyncrasies your Oracle DB holds, this may or may not be a simple process. Don't expect to see a success screen after a few clicks with any approach. First, I suggest you give a quick read of Michelle Li's answer to a similar question here. She outlines some key items you need to be aware of before you start. I will also draw your attention to the Migration Engine option. You'll want to ensure this is set to Server Side Data Migration Engine otherwise your data will go through whatever client SSMA is being executed on, and this is just a waste of bandwidth. Most importantly, plan for failure. It will likely take a few iterations to work out all the bugs, and you may even have to roll in some custom processes to accommodate things SSMA just can't handle, such as External C libraries/functions, SQL Loader Tasks, complex PL/SQL statements, any external scripts being called via cron, etc. I think SSMA will take a stab at conversion of many of these things, but that doesn't mean it will work 100%. Testing is going to be critical, so plan accordingly. I will also suggest you try migrating a copy of your Prod environment (e.g. Dev or some other pre-prod area) first to work out the bugs. This will ensure you're not killing your production Oracle environment while you're fine tuning things. In closing, the complexity of your end solution will likely reflect the complexity of the Oracle database. If it's just data, this could go swimmingly well, but if you've got a lot of logic tied into PL/SQL routines, etc. this can be time consuming. Use SSMA if possible as it'll hopefully cover 80% or more of the work, and then manage the remaining components how best you see fit, be it SSIS or otherwise. 

Try adjusting your input column type to . I've seen this fix similarly annoying Date conversion issues. Here's a full list of what those input types mean. 

Since Precision is a measure of how exact the number you wish to represent is and we know we're limited in how much we can store, you have to sacrifice precision for scale. So basically the smaller the scale the larger the final value can become. Scale may best be explained with how we look at maps. A map of a state or province shows a much larger scale of area with less detail, whereas a map of a city shows a smaller area at greater detail. Scale can therefore be thought of as the amount of detail. The more detail we want to convey comes with the cost of showing less actual stuff. I don't know if that helps you at all, but hopefully that helps provide some context behind the definitions of these terms. 

After following the above steps, the OU container in question is now configured such that any account added to it will be able to register and delete SPNs for itself and itself alone.  This is exactly the right amount of permissions as these accounts will be unable to trample over SPNs registered by other accounts. The purpose of restarting SQL Server in step 16 is to ensure the SPNs are registered as expected.  SQL will try to remove any registered SPNs on shutdown and add them on Startup, so the restart is really only needed if no SPNs currently exist for said SQL Server service. The final note on this approach is that if you are running SQL Server in a traditional Failover Clustered Instance (FCI) configuration, it is NOT Recommended to add this instance's service account to this OU, per KB 2443457. I really need to post Part 2 of my Kerberos series... 

Why is it necessary to utilize Cascading RI? I've always found it to be much more troublesome than it's worth. Since you're already customizing a solution, I think the easiest approach would be adding a second PKEY column that you can reference in the and tables within the trigger. So your Parent Table DDL would now look like: 

Here's the SQLFiddle to validate the output. As for the second question, it looks like an entirely different problem: 

The first TLog backup simply forces a CHECKPOINT operation and flushes the log to disk. The first DBCC Shrinkfile operation shrinks the physical TLOG down to where the current logical log is located The second Log Backup issues another checkpoint and will likely backup/clear any resident log operations located in front of the logical log. These were never flushed in the past for whatever reason, but should be cleared at this point. This step is critical to fixing your issue and sometimes this backup can take a bit of time depending on how much data needs to be written down to disk. The second DBCC SHRINKFILE operation moves the logical log file to the head of the physical log file, eliminates excessive VLFs that are likely there now, and reduces the TLOG file back down to a very small size, in this case 1MB. It is imperative to manually grow the file in step 5 so you don't run into vlf issues again down the road Manually increasing the size of the tlog will allow you to control the size and number of VLFs within your physical log file. This is an opportune time to configure this properly, so take advantage. 

Max's answer provides some good context about how you can perform cell-level encryption, so I'm not going to re-hash that; rather, I'm going to add additional context I think you may be looking for. 

Reorganizing an Index should be done when you have elevated amounts of white space within your index (i.e. column in the sys.dm_db_index_physical_stats DMV). Many people (and even solutions) in the SQL Server community say you need to look at Index Fragmentation levels (e.g. the in that same DMV), but this is not as much a concern depending on the type of underlying storage (e.g. anything that's RAIDed fragments your data for redundancy purposes regardless). Others also have come to the same conclusion. The downsides to Reorganizing your Indexes when not needed is that this operation will purge portions of your cache so it can have sufficient room to perform the reorg within memory. Reorgs aren't as destructive to your cache as Rebuild operations, but they will reduce your Page Life Expectancy (PLE) and potentially purge useful data that you will have to re-read from disk (e.g. increased I/O operations). Additionally, Reorg operations do NOT update Index Statistics. Updated Statistics do more to produce optimal executions plans than defragmented indexes, so if you're going to mindlessly run a nightly operation I would say Updating Stats is the better choice over Reorging instances. The right answer though is to have a solution check your White Space thresholds and reorg/rebuild indexes when needed. Have another process check your statistics and update when needed. There are a few solutions out there, such as Ola's maintenance solution, Minionware's Reindex solution, etc. But I often find myself customizing solutions to fit my needs. 

Click Ok (on permissions entry window) Click Ok (on Advanced Security Setings window) Click Ok (on OU properties window) Add service accounts running SQL Server services to the OU (Optionally) Restart SQL Server Services running under said account(s) Enjoy Treats