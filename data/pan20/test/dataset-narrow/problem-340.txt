This afternoon, another developer reported the same errors on another SQL Server instance that uses this same storage server in the same manner. Restarting that SQL Server instance cleared up the errors for him, as well. In that case, the SQL Server log does not mention transactions being rolled forward, which suggests to me that that may not be related to the issues that I was seeing. The network was functioning, since I was able to pull data from some of the partitions. Given that restarting the instances corrected the problem, there clearly was not any corruption on the remote files causing the issues. So it appears that the data was being written successfully, and it could be read successfully. What caused these errors to occur (i.e. how do I prevent them from recurring), and if they should recur, is there a more graceful way to correct the problem? 

I've seen so many times that we use numbers like 5,6 or 10 at most. What is this concurrency? Doesn't it refer to number of simultaneous users in bench? EDIT I'm not talking about any specific benchmarking tools, I mean in general. In benchmarks for databases OR for apache servers we use low concurrent numbers. why is that the case? 

I've read in a forum that if your database capacity is more than 1G you should buy its license. Is this true? How much will it cost? 

EDIT 2: The output of in order to remove extra information based on read and write: $URL$ Another result which reached around 1000: $URL$ 

I have used to generate SSL certification. All the private key, chain and certificate is generated by `let's encrypt. Now to use it in I first merged and into a file called : 

Using: SQL Server 2008 R2 I am currently stepping through a query execution plan, and have come across an instance of a clustered index update on a table. The issue here is that the columns that are being updated are NOT part of the clustered index. Table: 

Currently running on SQL Server 2008 R2 I am attempting to increase performance of an UPDATE statement. I notice an Eager Spool operation in the showplan popping up. My understanding of spooling operations is pretty basic - they create temporary storage for the table during the update. I also know that, while they are preventing much worse execution times, eager spools are often indicative of underlying problems with table structure and/or query statements. My question is pretty simple: When you see an Eager Spool in your query plan, what problems do you first look to address? I will be analyzing every part of our system to increase performance - I'm just looking for guidance as to where I should start. 

I have a database running under the simple recovery model. The database contains three narrow tables. For two of the tables, data is populated by insert statements. The third table is initially populated by an insert statement, and then the record is completed by an update statement. On average, the database sees 4000 inserts per minute and 2000 updates per minute. It's active, but not exceedingly so. The database has 10GB of space allocated to the transaction log. According to the Disk Usage report / DBCC SQLPERF(LOGSPACE), the log space usage is steadily increasing. Yesterday, usage was at 33% and growing. Overnight, it has increased to 48% and continues to rise. DBCC OPENTRAN returns no open transactions on the database. Nothing stands out to me when I run sp_whoisactive. Another database on the same server, with similar activity, a slightly larger transaction log size, and simple recovery model, is cycling between 0% and 10% usage, which is the behavior that I am used to seeing for the first database. What other steps can I take to determine the cause of this behavior and ensure that the transaction log will not fill up entirely? *Given that I cannot justify the growth in log usage, please assume that "allocating more space to the transaction log" is not a viable solution. 

When there is a unique key in a table (not ) and procedure is running, on duplicate key error the whole process will be halted. I want to resume on error and call the procedure again. The procedure: 

Now users in the name of who has been added by uid=60 should not have be shown! The result of this query is empty. I can't figure that out, what I'm doing wrong? 

As option separates table files instead of putting all data and indexes of DBs into one ibdata file, is using this option improve speed of alter table? I have a table of 40M rows and when I alter a specific table it takes about 5 to 6 hours. Does this solution help? Is there other ways around to improve alter table speed on heavy tables? 

The database is now online on the second server, and I can query data from filegroups PRIMARY and C. The database is aware of filegroups A and B, but they are marked as offline. Is there a way to restore the remaining two read only filegroups directly from the .NDF files? 

If they are not, then you will need to add some logic to the interior select statement to convert Ageing from whatever it is into those string values (e.g. '0-30', '365+'). As an example to check this, I did the following: 

I think that you will need to reassess your figures using compressed file sizes, and then make a decision based off of that. 

I believe that the problem you're running into is that you are only specifying a minimum date. The way your query is written, you will get everything from five years prior to the date through the end of time. You need to specify both a start and end point for your range to eliminate the data occurring after the date specified. One way to write this would be: 

Using SQL Server 2008 R2: I'm currently attempting to track down the cause of a deadlock issue we have. Not sure where to turn. Here's the setup. 

A multi-threaded application processes messages, then calls one of two stored procedures for every message. If the application has not processed the user, it will call a stored procedure that inserts a row into the table. If the application has processed the user, it will update any or all of , , , based on and does not change. There will be only one row in the table per . The thread that the application processes messages is chosen based on the player_id, so two threads will NOT process messages for the same player at the same time. The INSERT and UPDATE statements are using the directive. Here's the issue: The process is causing deadlocks with thread 1 locking the primary clustered index and thread 2 locking the non-clustered index. I do not understand why the locks are occurring - the UPDATE and INSERT statements are using row locks, and it's been tested and proven that threads are not accessing the same rows. What could I do to fix this issue? Caveats: There must exist two stored procedures, one updating and one inserting. As much as I would like to switch to a single MERGE, that's not gonna happen. : / 

I'm making the assumption that 00 values only appear at the end of a classification and that they can be ignored (i.e. they are placeholders for null, or something along those lines). An explanation of how the query works is in the comments of the code. This will work whether or not the graph entries in classification are in order by sno. 

The code is going to check to see if there is a collision on If there is, it will scan the table to find the next gap, update all of the values in that range, and leave you a gap to insert the new record into. If no collision is found, the update is skipped. I can't make any guarantee on performance for this, though. I'm sure there are more optimal ways to do it. Here's the dbfiddle - you can see the before and after the update occurs creating a gap for the insert to take place. 

I want to insert about 14000 records in a junction table, but the problem arise when there is a duplicate key for unique(iq_id,q_id)? what do do? 

is just 4,688! It is not much compared to the total data of the collection which is 30M documents. When Mongo gets slow when it has domino effect, CPU usage and Memory is not high. Mongo just uses 40% of the memory. Disk partition is if that helps. Another example of a very slow query in full details: 

Why is this happening? P.S.: I should note that Horde_groupware database have innoDB tables, when everything is messed up and I I get the error says bad information in .frm file. 

I want to fill this table with lots of data. Let's say millions of records. How to do this? I need to produce unique emails. Different data for emails. I know I can use but don't know how to produce random data 

I have a database with its primary file and log file stored locally. One table is split into 101 partitions which are stored on a Windows share on another server accessed by a UNC path. Each partition filegroup file is 1GB in size. Yesterday, I ran a bulk process to sequentially load data into partitions 2 through 58. The partition schema ensured that none of the files were filled to the 1GB capacity. The load did not report any errors. Today, when I attempted to retrieve data, I began to see three distinct errors: 

Okay, I think that I'm beginning to understand the nuance of your question. Given that space efficiency is a high priority, I think that you can strike the varchar(max) option from your list. Since your minimum file size is 11K, nothing is going to fit on a single page. All the files will be stored as BLOBs. SQL Server won't compress that (aside from compressing the backup), so you're not going to save any space with that approach. Next, I'm going to make the assumption that processing time can be traded for space efficiency. In that case, I would recommend that you compress the files yourself before storing them and decompress before displaying them. This goes for any solution you end up going with. You should end up with better compression than letting the file system handle it (which applies if you use a Filestream). So I think that your question boils down to Filestream (and possibly FileTable) or VARBINARY(MAX). The standard answer provided by Microsoft is: