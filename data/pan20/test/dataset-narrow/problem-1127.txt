If solving this problem is indeed of great interest to many people, ask one of them ! If you're afraid of your work being scooped, time stamp it on the arxiv. 

A related pointer (though not a complete answer) is the book by Luc Devroye on generating random draws from different kinds of distributions. While the book doesn't have detailed complexity analysis, it outlines a number of very specific algorithms for sampling from different densities, and provides good pointers to do a proper running time analysis. 

Observe that by mirroring across the line $x = y$, we can ignore all points in the upper triangle of the grid (at the cost of increasing the partition size by a factor of 2). Now consider the lower triangle. If we draw the vertical line $x = n/2$, and "reflect it" off the diagonal $x = y$, we get a square with endpoints $(n/2, 0)$ and $(n, n/2-1)$. all of these points can be placed in a single set. This leaves two similar copies of the lower triangle with half its size. Any construction for these two triangles can be "paired up" since sets in these partitions don't conflict. 

Firstly, I need to know which reference to cite, and it does seem like this is the most recent work on this. Secondly, it is true that the result is quite tight in the r = 1 range. I'm interested in the m >> n range, and specifically in the realm where r might be poly log n, or even n^c. I'm trying to slot this result into a lemma I'm proving, and the specific bound on r controls other parts of the overall algorithm. I think (but am not sure) that the range on r provided by this paper might suffice, but I just wanted to make sure there wasn't a tighter bound (that would yield a better result). 

I'm not sure whether this is what you're looking, but the phase transition in random SAT is an example. Let $\rho$ be the ratio of number of clauses to number of variables. Then a random SAT instance with parameter $\rho$ is very likely to be satisfiable if $\rho$ is less than a fixed constant (near 4.2) and is very likely to be unsatisfiable if $\rho$ is a little bit more than this constant. The "black hole" is the phase transition. 

I was originally inclined to close this question, but then felt it's worthy of an answer. In this situation, since you aren't comfortable releasing your work into the wild, the best solution is to email the questioner privately, if you really want to give the answer. Alternatively, you could wait, write up the results, put them on the arxiv, and then point the questioner to the answer. 

In other words, provide all the background they need, and more. But don't prove teeny little lemmas from scratch. 

Probably the most useful application of computational geometry within computer vision is in the realm of shape analysis/matching and image registration. There's a lot to google here, but you can start with Kendall's book on shape space, and also look at problems of measuring the distance between shapes. 

I am by no means an expert on this, but a key component of the definition of pseudorandomness (as opposed to attempts to define randomness) is that the goal of something "pseudorandom" is to fool a circuit. In other words, the motivation is to think of the pseudorandom string being supplied to the circuit instead of the truly random string. In that sense, it's not really that you're trying to pretend that $x$ and $G(y)$ "look the same". It's that they "look the same" to a circuit (of necessarily bounded complexity). So the role of the circuit is crucial, as opposed to merely being a "test function". 

Update: The description below is for a different problem (in which you have all pairwise distances in a set rather than pairwise distances between two distinct sets). I'll leave it up anyway since it is closely related. This problem is called the beltway problem, and is a special case of the general $d$-torus embedding problem. It is also closely related to the turnpike problem, in which the distances differences are absolute (not modulo some number). It is not known whether the beltway problem admits a poly-time algorithm. There are various pseudo-poly-time algorithms for related questions. The best reference (alas an old one) is the paper by Lemke, Skiena and Smith. 

It's possible that what the OP wants is as follows: Fix an ordering. By the standard trick using hyperplane arrangements, each such ordering represents one of the n! congruent simplices that make up the unit hypercube in $n$ dimensions. Now the goal is to define a way of sampling from this simplex with independent samples from the data (rather than trying to sample from the simplex directly). In fact this can be done. Luc Devroye's book describes the procedure (see V.3), which essentially boils down to letting $x_i$ be an exponentially distributed random variable, normalizing them, and then setting $\rho(\pi(i))$ to be the partial sum $\sum_{j \le i} x_i$ (I'm over simplifying a little, so what I said isn't entirely precise, but it's the general idea). 

If the distance function is a metric, then you can employ either $k$-center clustering (where the maximum radius of a ball is minimized) or $k$-median clustering (which minimizes the sum of distances to cluster centers). $k$-center clustering is easy: merely pick the $k$-farthest points, and you're guaranteed to get a 2-approximation via triangle inequality (this is an old result due to Gonzalez). For $k$-median clustering, there's been a ton of work, too much to review here. Michael Shindler at UCLA has a nice survey of the main ideas. Both these problems are NP-hard in general, and are hard to approximate to within an arbitrary factor. Note that if you drop the condition of being a metric, things get a lot worse in terms of approximability. Another, more heuristic approach which might be ok for your application is to use a technique like MDS (multidimensional scaling) to embed your distance matrix in a Euclidean space, and then use one of many different Euclidean clustering methods (or even $k$-means clustering). If you are sure that your distance function is a metric, then you can do a slightly more intelligent embedding into Euclidean space and get a provable (albeit weak) guarantee on the quality of your answer. Ultimately, as with most clustering problems, your final choice depends on the application, your data size, and so on. 

For the question as stated, you're going to have problems because the space of all metrics is an unbounded cone. In particular, suppose such a bounded $\epsilon$-net did exist and had elements $d_1, \ldots, d_r$. I can always take some metric that lies within one of the $\epsilon$-balls defined by this net, and scale its values so it pops out of all balls. So you need either some constraint on the metrics themselves, or a different way of defining the set $S$ (maybe $S$ is not a finite set but some set defined by other constraints on $M$) 

The SODA 2008 Ailon-Liberty paper on fast Johnson-Lindenstrauss transforms uses a "dual BCH code of design distance 5" as part of the construction. They cite the MacWilliams-Sloane book on error-correcting codes as the source for how to construct these codes, but my initial wading through the book doesn't appear to yield the answer (there are dual codes and BCH codes, but...) Is there an easy way to describe how to construct such a matrix, for general matrix dimensions $m \times d$ ? 

In other words, if I can reconstruct, there's a lot of mutual information in the system. Is there a "converse" to Fano's inequality: something of the form 

This example is from social choice theory, and elections in particular. We know that Arrow's theorem (and the Gibbard-Satterthwaite theorem in general) rule out the possibility of elections that are fair, non-manipulatable and without other bizarre consequences. But a seminal paper by Bartholdi, Tovey and Trick showed that finding the desired 'hack' to break a voting scheme was NP-hard, and there's been a large body of work by many researchers on the complexity of problems in the realm of election design. There's a nice survey by Faliszewski, Hemaspaandra, and Hemaspaandra on this topic. 

(warning - self promotion alert: I have a paper that describes different implementations of Johnson-Lindenstrauss and how they compare) 

Any algorithm that uses the Robertson-Seymour results to infer a "polytime" algorithm for things involving graphs that exclude a fixed minor is asking for trouble. The constant hidden in their result is "galactic". 

The book by Dubhashi and Panconesi collects together many such bounds, more numerous than can be listed here. If you find that hard to access immediately, there's an online survey of Chernoff-like bounds by Chung and Lu 

You're looking for resource bounded Kolmogorov complexity. You can start with this paper and branch out. 

Since the term is overloaded, a brief definition first. A poset is a set $X$ endowed with a partial order $\le$. Given two elements $a,b \in X$, we can define $x \vee y$ (join) as their least upper bound in $X$, and similarly define $x \wedge y$ (meet)(join)as a greatest lower bound. 

In the geometric setting, where $C(x,y) = \|x - y \|$, this formulation is called the bottleneck matching problem. It's possible that this is the generic term for it (I've seen this formulation used in the Kleinberg-Tardos algorithms book for MSTs). 

Branch and bound is an effective heuristic for search problems, and Wikipedia lists a number of hard problems where branch-and-bound has been used. However, I haven't been able to find references to suggest that it's more than just "one method" for solving these problems. Anecdotally, I've heard that some of the best heuristics for SAT and integer programming come from branch and bound, so my question is: 

Another example is estimating the volume of a polyhedron in high dimensions. There's an unconditional lower bound on deterministic strategies to approximate the volume to even an exponential factor, but there's an FPRAS for the problem. Update: the relevant paper is (link to PDF): I. Barany and Z. Furedi. Computing the volume is difficult, Discrete and Computational Geometry 2 (1987), 319-326. 

Suppose I'm given a connected polygon in the plane with holes. I can "remove" a hole by drawing a straight line from the boundary of a hole to another boundary (either of another hole, or the boundary separating the polygon from the exterior). 

Stream algorithms require randomization for the most part to do anything nontrivial, and because of the small-space constraint, need PRGs that use little space. I know of two methods that have been cited for use in stream algorithms thus far: