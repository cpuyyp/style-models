EveryDNS used to have this, but I switched to DynDNS because I found out my router has support for DynDNS. 

Intel AMT is a motherboard feature found in certain chipsets such as Q67. So if you are looking for a superMicro board with this, then just find out which intel chipsets support Intel AMT, and then look for supermicro boards with that intel chipset. There are some various other requirements as far as what CPU you use, etc. In a nutshell the motherboard keeps one of the ram chips powered, the northbridge powered(which runs the management interface), the onboard ethernet connection, and an extra flash chip to store settings. So you can connect to the system over ethernet and access the remote managmeent features/power-on/power-off etc. I am not sure though if this is available in any chipsets targeting servers though, but it is really powerful and would seem to be well suited for servers. 

If my understanding is correct (please correct me if not), the first line of this indicates that the process with PID 1884 is asking netlogon to log in to a domain named "DEECEE". It literally thinks the domain name is DEECEE. Of course, the previous snippet (and others) show that this process, pid=1884, is shotgunning out requests, some of which are legit, and some aren't. Checking the process list on that machine tells me it's a process. So I found out the application pool: 

So I spent some time enabling and disabling these sharepoint services and watching the DNS queries go out. It appears that the User Profile Service is causing the queries for at least _ldap._tcp.deecee. I know the whole thing isn't sharepoint's fault; as I said earlier these queries are coming from all over the place. The ones for just _ldap._tcp.deecee, though, are coming only from our sharepoint hosts. So that adds another question. What is the user profile service doing that's causing the lookups to _ldap._tcp.deecee? It still leaves the question for the rest of our servers, though. 

I am running pfsense 2.0.3 nanobsd 4g i386 on virtualbox. VM configured with 4gb ram, there's 8 gb total on host system, with two net interfaces configured as host only. This will go on an SSD mini atx box, but for now I am just running on VM for learning pfsense. I assigned interfaces, em0 to WAN, and em1 to LAN. From the windows host(hosting the VM) I brought up the browser and tried to connect to the LAN IP. I was intermettently getting timeouts and I would reboot the server or use the reboot web configurator option, and sometimes I could get the login screen but after logging in with default user/pass, I'd get a blank page. Absolutely no error messages or feedback of any kind. I typed password carefully, thinking maybe it was doing anonymous authentication, since according to their documentation provides a blank page by-design. After many tries and reboots I finally got the wizard screen. I completed the wizard and the final page indicated it was going to redirect after a few moments, after a few minutes it redirected but failed to retrieve the next page. From there the web configurator again was not responsive, timing out. I rebooted and still same thing. How do you troubleshoot something that gives you absolutely no feedback or error messages? Any ideas about what might be wrong would be welcome, but primarily: How do I troubleshoot failures in the web configurator? Is there logs specific to the web configurator, or do I need to poke around in the web server logs, pfsense logs, etc.? Is there any documentation on directory structure that would help me find these? I've found from distribution to distribution, that each has it's own idea of where user programs, logs, etc are stored. 

This seems to do the trick (with perhaps a caveat), to find all folders that user "someuser" has access to, in this example on the C drive, using the built-in Windows icacls command: 

The /t is needed to tell it to recurse directories. The /c is needed to tell it to keep going even if it encounters errors. The /l makes it work with symbolic links (if any). (That last one is an L, and these flags can be upper or lower-case.) The will be recognized by DOS old-timers as the way to say "look for directories, not files". Of course, if you DO want to find files and not folders, change it to , and of course you can point it to any drive, or run it from any folder and leave off the drive/folder path and let it search relative to that folder only. I sought the same answer as the OP, and found this entry, but was bummed to see only an offer based on a downloadable tool. Like others, I preferred to use something built-in, and I found it, in this icacls tool. And I have confirmed it works on Windows Server 2012, 2008, and Windows 7, so I suspect it will work as well in Server 2003, Windows 8, and so on. The resulting list will be folders indicated line after line, such as: SID Found: c:\somedir\somesubdir. Note that if you run this as a user who does not itself have permissions to some directories being traversed, you will get errors interleaved in the results such as: c:\System Volume Information: Access is denied. And if you may be searching an entire drive, that could result in hundreds of such errors, making it hard to find within them the results. Some may think the answer is to run the command line as administrator, but that will simply cause far more such errors to appear, as you will now be traversing folders that were previously hidden. Now, if you were interested in hiding those errors, you won't be able to use a find command to pipe only the results which DO succeed (those which DO refer to "SID found"), because the errors will NOT be filtered out by the pipe to the find command. Instead, if you want to remove all the errors, you need to use the rather obscure trick of redirecting the error stream (stderr) to the "bit bucket" by using . So the example above would become: 

Scoped to a specific filesystem name :No header so that first line is a snapshot name : List snapshots (list can list other things like pools and volumes) : Display the snapshot name property. : Capital denotes descending sort, based on creation time. This puts most recent snapshot as the first line. : Says include children, which seems confusing but its because as far as this command is concerned, snapshots of TestOne are children. This will NOT list snapshots of volumes within TestOne such as . : Pipe to head and only return first line. 

So my understanding of one scenario that ZFS addresses is where a RAID5 drive fails, and then during a rebuild it encountered some corrupt blocks of data and thus cannot restore that data. From Googling around I don't see this failure scenario demonstrated; either articles on a disk failure, or articles on healing data corruption, but not both. 1) Is ZFS using 3 drive raidz1 susceptible to this problem? I.e. if one drive is lost, replaced, and data corruption is encountered when reading/rebuilding, then there is no redundancy to repair this data. My understanding is that the corrupted data will be lost, correct? (I do understand that periodic scrubbing will minimize the risk, but lets assume some tiny amount of corruption occurred on one disk since the last scrubbing, and a different disk also failed, and thus the corruption is detected during the rebuild) 2) Does raidz2 4 drive setup protect against this scenario? 3) Does a two drive mirrored setup with copies=2 would protect against this scenario? I.e. one drive fails, but the other drive contains 2 copies of all data, so if corruption is encountered during rebuild, there is a redundant copy on that disk to restore from? It's appealing to me because it uses half as many disks as the raidz2 setup, even though I'd need larger disks. I am not committed to ZFS, but it is what I've read the most about off and on for a couple years now. It would be really nice if there were something similar to par archive/reed-solomon that generates some amount of parity that protects up to 10% data corruption and only uses an amount of space proportional to how much x% corruption protection you want. Then I'd just use a mirror setup and each disk in the mirror would contain a copy of that parity, which would be relatively small when compared to option #3 above. Unfortunately I don't think reed-solomon fits this scenario very well. I've been reading an old NASA document on implementing reed-solomon(the only comprehensive explanation I could find that didn't require buying a journal articular) and as far as I my understanding goes, the set of parity data would need to be completely regenerated for each incremental change to the source data. I.e. there's not an easy way to do incremental changes to the reed-solomon parity data in response to small incremental changes to the source data. I'm wondering though if there's something similar in concept(proportionally small amount of parity data protecting X% corruption ANYWHERE in the source data) out there that someone is aware of, but I think that's probably a pipe dream. 

Did you ever solve this (since posting back in 2012)? Either way, I'll offer some thoughts in case others may find the question and still seek an answer. Thanks for the link to my blog post, btw. Sorry it didn't help then. You refer to other comments reflecting the problem with no solution. Did you happen to revisit the comments since then? You may find new info. In addition, did you happen to have more than one browser open looking at the Admin? That could cause the problem (as each browser creates different cookies to track the "user" who is logged in). Or did you happen to have more than once instance of CF against which you were logging in, but using perhaps the same IP address (but different ports) for their CF Admin? That would cause even one browser to send the same cookies to the different instances, which may cause the instance to reject/rotate them to protect against session fixation. Indeed, to that point, did you ever try using the coldfusion.session.protectfixation jvm flag to see if that helped ($URL$ though it may be overkill to solve this one problem. FInally, did you happen to see the later post I did, which showed how some problems are due to duplicate cookies? ($URL$ 

Where can the pfsense log files be located and viewed? I have searched the documentation and it doesn't indicate the log files location for the various components of pfsense. 

I have seen people recommend RAID 10 over RAID 5 for databases due to RAID 10 giving better performance and a better chance of recovering from a hardware failure. This confuses me as I thought the purpose of using RAID 5 was more a matter of the parity allowing the detecting and correcting write errors to ensure the integrity of the data. My understanding was that RAID 10 can not recover from write errors. I.e. if a bit has an error, it will be the opposite of the bit in the mirrored drive, and thus it will be impossible to tell which bit is the one with the error, and which is the correct one. However, I tried googling along the lines of detect "write error" with raid 5 vs raid 10 to see if anyone covered this point, and came up empty handed. Am I making this all up in my head? Can a RAID 5 array detect and recover from write errors using the 3 parity bit? Or does the detection not occur until much later when the data is read and the parity indicates an error? If a RAID 10 array has a write error, will it be able to determine which of the mirrored bits is the one in error? I.e. the drive indicates a read failure for that particulor bit, or does it just see the bits do not match and since there is no parity it can't determine which is in error? I see some discussion of rebuilds being triggered by a read error. Do write errors not get detected until later when the data is being read? In other words, does the writer error occur, but the erroneous data just sits there until possibly much later when the data is read and the parity indicates an error. Is that why you are at risk of getting additional read errors during rebuild, in that you could be writing a large amount of data with errors but the errors will not be detected until the next time the data is read? I would like to clarify that tape backups do not address the above question. If you have a scenario where data integrity is very important, and you can't detect write errors, then all the tape backups in the world won't help you if the data you are backing up already has errors. 

We have all of the logging turned on for MS DNS Server and see plenty of NXDOMAINs for requests of this form: Note that I am not talking about Those are working fine. Here is an error entry from the log: 

Note that the client is requesting According to Microsoft's documentation, the proper request should be The requests come in from all of our AD joined machines. They include Windows 7, Server 2008, 2008 R2, 2012, and 2012 R2. Our DNS servers do have the appropriate SRV entries for and they do resolve correctly. So that's not the issue. A coworker opened a case with Microsoft and the tech finally claimed after a few days that this is normal. I don't buy it. Why is there no mention of this behavior at all in any documentation? So, Does anyone else see this behavior? Clients looking up SRV records for ? If so, are they getting NXDOMAIN results? Any ideas how to fix this? Thanks in advance. Update A - There's more In my domain I'm seeing these invalid queries in order of most common: 

Just do beware that some of the folders which generated such errors, which errors are now hidden, may well be folders that the named "someuser" DOES have access to but which YOU do not. So you may want to think twice about simply ignoring these errors. but if you want to, that's how you can do it. That possibility does potentially limit the value of this answer, I realize. If anyone with more familiarity with things would like to expand on or correct my answer I'd welcome it. 

I found this problem happening on a newly installed system, with a newly installed IIS, and a default app pool with pages in the default site. We found that the only (seeming) solution (in IIS) was to change the app pool's identity (in the app pool's "Advanced Settings") from the default of applicationpoolidentity to either of the 3 other options: localservice, localsystem, or networkservice. (Don't forget to start the app pool after making the change.) As for why this worked, and what the implications are (running the app pool that way, or why the default did not work), we did not get a chance to explore. I do think that this server in question was indeed join to an AD, as Doug notes in his original post (long time, no talk, Doug). So that seems to be where the problem is. And I suspect there may be a better solution than what we did. I'll look forward to seeing if others ever elaborate on this. But since others are raising this issue here and elsewhere, and this is among the first google search results where the thread is not closed and I can offer this reply, I hope at least this workaround may help someone. 

We are doing nightly full backups and noon differential backups. We use Full recovery model with SQL Server 2005, but logs are never backed up and are truncated(TRUNCATE_ONLY) after the full backup. Restoring to a point in time is not a requirement, restoring to one of the nightly or noon backups is sufficient (Not my decision). So the question at hand is, since they are throwing away the logs every night, is there any reason to not use Simple Recovery model? Is there any benefit to using Full Recovery model if we are throwing out the logs every night? Thanks. 

I installed SQL Server 2008 and typed in an instance name of but it seems to have been isntalled as a default isntance. Trying to connect using fails but using just the computer name succeeds. The service is listed with as if it is the default instance, but the data directories all have the .sql2008 instance name suffix. So not only is it not what I wanted, but the data directories and service names have inconsistent suffixes. Was there something else I needed to do besides specifying an instance name during installation? Maybe a checkbox I missed? Is there a way to change from a default instance to a named instance?