I use school colors and principal's name. If someone comes in tomorrow and says your principal is no longer Mr. Smith, it's now Ms. Jones then that change is made for every object (student) in the school. You don't have to go to each individual student and tell them that their principal has changed. It worked really well this year because we did get a new principal. 

I post solutions after the due date of an assignment. And usually, I'll go over them in class as well. Two birds... 

I have 3 tiers of labs. First are the required labs. They're worth 100 points each and every one must do these. If they don't do one, it goes in the gradebook as a zero. These are also the labs that I think are the best of each topic for practicing what they need to work on. My calendar is based on how much time I expect 90 plus percent of the students need to finish this set. Second and third tiers are bonus and challenge labs. They're worth 110 and 125 points respectively. If a student does these they go in the gradebook. If not, the grade is omitted so there's no penalty for not getting them done. These tend to be extensions of the required labs, but might be a little more difficult. Generally the students that are the fast finishers are also the ones that like doing the labs, so throwing a few more their direction will keep them working. Plus, it's an AP class so the 10 or 25 extra points on a lab grade will be a motivator. I also have a page with a list of websites they can go to if they're finished with all 3 sets of labs. It links out to sites like code.org, codecademy, Code Combat, and CodingBat. Sites that are related, but a little more fun than just cranking out labs. And I'll also let students come up with independent projects to work on. 

In my opinion the report correctly characterizes it as having longevity and being wholly technology-independent. 

I don't see how this follows in general. It depends, of course, on what the test is testing: if the test is about finding the one incorrect value in a 2000-entry trig table I would expect a computer would be of help, but that's not what a CS test usually looks like, is it? I expect a good test would in general measure the student's understanding of a topic, not their ability to mechanically carry out repetitive tasks. 

The way I understand the report and especially the bit you pasted in your own answer, the key to IT is in the T. It's IT if it has a clearly technological approach (and by necessity the treatment is at theoretical level rather than practical). 

Grades. Works especially well around final exam time. You can find the average. Find the highest and lowest. Find the average with the lowest dropped. Find the most common grade. Count how many are in the range 90-100%. Given an array of grades assigned for each class, calculate GPA. Given 3 arrays that represent labs, quizzes, and tests calculate the final average. Each array can be weighted differently to match whatever your grading scale is. Ex: I would have labs weighted at 10%, quizzes at 20% and tests at 70%. 

Wish I had a good answer, but I can sympathize with your Scratch example. What I started doing was just giving up on the first day we did Scratch. I would show them the drawing tools before we did any coding. Spent about 5 minutes showing how to make sprites and backgrounds and then gave them the rest of the period to play, with the understanding that was the only day that they had to just play around and doodle. Day two, we start coding. Worked fairly well. It was at least better after I started doing this than before. I think the novelty of it wore off when they were allowed to goof off for a day. Haven't taught Scratch in a few years, but I noticed the same thing happening with Jeroo. Kids love painting the island with flowers and water. The last lab in the set is one where they get to build their own lab, and make their own island. I think having that to look forward to helps. 

Lots of profiling. For all anyone knows, that random gibberish full of typos and conspiracy theories on Reddit is the deepest, most correct theory of physics ever. But, since it's probably not and there's only so much time in the day, your bets are better hedged by focusing on content that appears to be of higher quality. To that end, you should develop a bunch of prejudices over what's likely to be better content, then selectively focus on it. Sure, if you're truly out of stuff to read, then maybe you'll end up going through that weird Reddit post; or, if you've got too much good stuff to read, you might not even be able to make time for Einstein's words. It's all relative. Examples of common prejudices: 

Students get to see at least one working solution, although I try to come up with several. Posting the solution serves as a good block to students asking to turn in assignments late. They'll understand pretty well that they can't turn it in late after I've already posted solutions. 

You said he refuses to work, but has mastered the material. How do you know he's mastered the material? I've had students like this in the past. They'll finish 2 weeks worth of assignments in a day and a half. Talk to them. See what interests them about programming. The few times that this has come up, the student had something specific they wanted to build. It was well above what was expected in class, but it was still programming, so still related. We came up with a plan for them to work on their pet project, which always required that they finish the assignments I gave first. Then, they're free to build their project. 

Further rule of thumb: if there is a big $O$ somewhere, or a proof by induction, or a graph, or an inductively generated structure, it's probably CS. If kids who are good at the IT and DL parts fail miserably at it, there you have the conclusive proof. 

Rule of thumb: if your aunt (who is a stereotypical aunt), knows about it, it's digital literacy, otherwise it's IT. If it's on Slashdot, it's IT. 

As for CS, in your original question you say that you "have no problem understanding what Computer Science is and what it includes", but let's talk about CS as well. The definition from the report you pasted in your own answer is good enough for me: 

I semi jokingly tell my students at the beginning of the year they can play any game they want on the computers, as long as they write it first. It at least gets a laugh. For me it takes two things. One, I need to be able to see their screens. Given, this is probably much easier with school provided devices. When I moved into the classroom I'm in all of the student seats were facing forward with the monitors facing away from the teacher. First thing I did was push all the desks to the wall so that the monitors are facing the center of the room. I can see almost every monitor from almost any spot in the room. This also helps getting to students to help them since I don't have to squeeze through rows. It does help that I have a pretty big classroom though. And two, I need to keep them busy. What I've found is that once there's a student playing games it spreads. And the first one usually isn't playing because they're screwing around. They're probably done with their assignments. I throw more assignments at a class than most students will be able to finish - some are extra credit. I also have a list of enrichment activities that they work on when done with labs - Coding Bat, Code Combat, Project Euler, making their own game, that sort of thing. That way there's always something CompSci related to work on. We are partially BYOD in that kids are free to bring their devices, but aren't required to. There's always a handful of students that prefer to work from their own notebooks. My rule on that is that they're free to do so, until I catch them screwing around on something else. Then, they have to use the school computers. For most, that's enough incentive to keep on task. 

The first peak, around 1985, is before my time. However it's my impression that there was a major loss of momentum around those years. The term "AI winter" was coined in 1984, suggesting that pessimism was prevalent enough to assign it a name in discussion. Presumably this pessimism dissuaded incoming college students from selecting CS, causing the drop a few years later when they graduated. That second peak looks like it's centered around 2004/2005, suggesting that students were really optimistic about Computer Science around 2000/2001. And those years were the peak of the dot-com bubble, which started its clear burst in late-2000. It seems probable that the dot-com bubble bursting dissuaded incoming students from going into CS then, too. Females in specific also followed this trend, and had the same peaks: 

I don't think you can just by using an auto grader. I use an auto grader to tell me if the code solves the problem it's supposed to solve. And I tell my students that any code that solves the given problem is correct. Some people write a 2 line solution, some write a 50 line solution. But I don't consider either one more correct than the other. Generally the shorter solution takes more mental work and the longer solution takes more physical work (typing). But I will go back and look at the submissions. Since the auto grader has checked it, I don't have to worry about whether the code builds or gets the right answer. That takes out the time consuming, boring part for me and lets me focus on style and any tips that I can give. I guess, especially early in the year, I don't want to penalize students for a working solution that might not be as clean or elegant as another student's. There's enough frustration in a first year course without worrying too much about style and efficiency.