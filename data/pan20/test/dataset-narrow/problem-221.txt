If you're using READ UNCOMMITTED isolation level or the WITH (NOLOCK) in your queries, your queries can skip rows or see rows twice. Kendra Little explains it in more detail in her 30-minute webcast There's Something About Nolock. 

Because a UNIQUEIDENTIFIER column can store whatever UNIQUEIDENTIFIER you put in it. Take this code: 

If you restore a database as a new database, the dbi_crdate field is the restore date/time. Here's how to reproduce it: 

We've been working together on this error over here: $URL$ Turns out that the problem is contained databases. If you have a contained database, sp_BlitzIndex fails. (I've never actually seen a contained database in the wild before, so that's kinda amazing - wonder how long this has been broken.) 

Actually, you did have to drop the snapshot before the restore. Here's the Books Online post about it: $URL$ In particular, "The database cannot be dropped, detached, or restored." You might have been using a restore script or tool that automatically dropped the snapshots for you. (My own restore database script does that - checks for snapshots first and deletes 'em.) 

First, inserting data into a table variable is single-threaded (not parallel). You can learn more about that in Paul White's epic post Fording a Parallel Execution Plan. Second, when you're running multiple queries, you can't predict which queries will end up on which cores. An incoming query gets assigned to a worker, which is pinned to a scheduler, which runs on a CPU core. SQL Server doesn't know in advance how complex the query (or queries) will be, so it can't predictively load balance. If you truly want to get the CPU to 100%, you'll want to run a lot more than 3 queries, or you'll want to check DMVs to abort your queries if they end up on the same scheduler. Or do what I would do - stop using table variables and switch to a temp table instead. 

The database is corrupt. Setting it to offline sealed your fate. If it's mission-critical data that you need to recover, contact Microsoft Support. It's going to cost $500, but it's worth a shot if you need that data. Otherwise, restore from backup. 

In Task Manager, is the CPU consumer actually sqlserver.exe, or something else? If it's something else, troubleshoot that first. If it's actually sqlserver.exe, check what queries are running right now. The easiest way is with Adam Machanic's excellent sp_WhoIsActive. Here's how to run it. Check the server's wait stats. My favorite way to do this is with sp_AskBrent, my stored proc that takes a 5-second sample of wait stats plus checks some diagnostic queries. 

Boiling down the question to the basics: Q: "is there some way to set a policy or something on the Microsoft SQL Server instance itself that will cause any SSMS clients to be colorized?" No. SSMS query color-coding is part of a client-side connection and that data is stored in the registry. You could theoretically export that registry setting and apply it to anyone in your company with SSMS, but it's not a trivial task for us database people. I'd take that request to your sysadmins with Active Directory Group Policy administration duties and explain what you need. It's a fairly common type of request for companies that manage software deployments centrally. Unfortunately, it's not point-and-click easy, and you'll need to update the registry keys every time you add or rename a production server. These types of mass client deployments usually require change requests because modifying the Windows registry isn't something you want to be doing often. 

Zoom out a little and think about the bigger picture. In the real world, is your update statement really going to look like this: 

The DMV sys.dm_os_buffer_descriptors will help you break down which tables from the master database (or any database) are being cached in memory. This query from that Books Online page can be run from the master database and give you a list of which objects are involved: 

Typical causes: someone drops the login (thinking they're cleaning up bad logins) or restores one of the system databases. It's tough to guess what this was after the fact, though. In user databases, you can go through the transaction logs to reverse engineer it, but you can't do log backups for master, so you're out of luck. 

Express Edition (the free version of SQL Server) doesn't have maintenance plans because it has no SQL Server Agent. Agent is the constantly-running-scheduler that kicks off jobs like maintenance plans. 

Start with the Books Online page on estimating memory requirements for in-memory OLTP. It goes into deep detail, and here's just a snippet: 

Bad news: social security number isn't in the format of DDMMYYXXXX. (My own SSN starts with a number above 3, for example.) Take out the DATEDIFF and CONVERT functions, and just look at the source data. You'll likely see the offending rows just by eyeballing it: SELECT Users.Id, Users.FirstName + ' ' + Users.LastName AS Medarbajder, Users.SSN AS CPRNR, SupposedlyDate AS STUFF(STUFF(LEFT(Users.SSN, 6), 3, 0, '.'), 6, 0, '.'), FROM dbo.Paychecks, dbo.Users WHERE Users.CustomerId=214 AND Users.Id=Paychecks.UserId order by 4; I don't know where you got that formula from, but it doesn't work. (It certainly doesn't produce a valid date for my own SSN.) There are ways to guess approximate dates from social security numbers, but not produce valid birth dates. 

SQL Server databases start up, but then they need to analyze the transaction log to roll transactions forward & backward. This process can take anywhere from milliseconds to hours (even days!) in the event of a long-running transaction, many (think thousands) of user databases, or databases with lots (think tens of thousands) of virtual log files. If you only want the app to get in when recovery is done and the database is ready, have the application retry its connection. If you want the app to be able to get into SQL Server immediately, but maybe not be able to run any queries yet, set its default database to TempDB instead of a user database. It'll likely be online right away even when user databases are unavailable. 

This means the cluster network name (tcsdb) can't be registered with Active Directory at the moment. It could be that your newly primary node can't see a domain controller, or can't see a writeable domain controller, may have a network interface down, may be having routing problems, etc. 

Great job on troubleshooting, and yes, you are correct. Ola Hallengren's backup scripts need a change as described in this Github issue for his scripts: $URL$ Ola hasn't accepted the pull request yet (he's fairly new to Github), so if you want to use my fixed version with the new parameters I describe in that issue, here's my repo: $URL$ If all that sounds too scary - and I wouldn't blame you - then you'll need to seed the restores yourself manually for each new database. 

It gives you a prioritized list of server issues, and URLs to copy/paste into your browser to learn more about each problem. For example, in the screenshot above, the server's slowing down because some meatball is running a backup right now. You can click on the "ClickToSeeDetails" line to see, uh, details: 

To get the top wait, try running sp_BlitzFirst @SinceStartup =1. (Disclaimer: that's an open source script, and I'm one of the authors.) You can also use 3rd party monitoring tools like Idera SQL DM, Quest Spotlight, and SQL Sentry Performance Advisor. (Disclaimer: those cost money, and I didn't write any of those, but they're awesome.) The @SinceStartup = 1 switch gives you waits since startup, which isn't as cool as monitoring software, but it's a free start. If your top wait is PAGEIOLATCH, that means reading data pages from a data file. If that's the case, you want to find the queries reading the most data. For that, use sp_BlitzCache @SortOrder = 'reads' (again, disclaimer, open source script, and I'm one of the authors.) That'll give you the top 10 queries ordered by how much data they're reading. They're usually candidates for index tuning or query tuning. I'd always rather tune those rather than add memory, but if PAGEIOLATCH is your top wait, and you're not allowed to tune those top 10 queries, nor tune their indexes, then the next fix is to add memory to the VM. (But only after going down this route of troubleshooting.) 

On the Devices table, add two fields: LastStatus and LastDateTimeChecked. Update it whenever you insert records into the Connections table. The Connections is your history, but you shouldn't be hitting it for live reporting queries. 

There's a few different questions here. Q: Can I force GETUTCDATE() to stay the same during a transaction? No. If you have a long-running transaction, the time is going to change while you work. This has nothing to do with your isolation level, and more to do with the fact that life doesn't have a Pause button. (Yet. I've tried pointing Dr. Horrible's Freeze Ray at my computer, but that just made my solid state drives become even more solid.) Q: Can I force non-deterministic functions to return the same result every time? By definition, no. If you could, they would be deterministic. Q: If I need dates/times to stay static during a transaction, what do I do? If you only need a single value, create a variable, and set its value at the beginning of your transaction. In sp_BlitzFirst, for example, I set a @StartSampleTime variable, and then keep using that throughout the code. If you need multiple rows, select the data into a permanent object like user table or a temp table, and repeatedly refer to them during the life of the transaction. 

Then, consider adding network capacity. I'm guessing your servers have just one 1Gb Ethernet port each, and it's getting saturated during your index rebuilds. You could consider upgrading to teamed cards with load balancing, or to 10Gb cards, or to separate heartbeat network cards. I'm not usually a big fan of separate heartbeat networks because they increase complexity, but that might be the cheapest/easiest solution here. (Start with teaming first though.) The above steps will let you keep your automatic failover AND your index rebuilds. However, I'd be a bad database guy if I didn't suggest that maybe rebuilding every index, every night, is a little bit overboard. Not only is it causing your AG problems, but it's also inflating your tranaction logs, causing those backups to take longer too. For more on that, watch my GroupBy session, Why Defragmenting Your Indexes Isn't Helping. 

Disable antivirus. Man, I hate saying this, but I've seen issues where antivirus took too long to scan a file, and it caused installation problems. Re-download the installation media. I know you've seen other users run this same setup and it's worked, but I've seen enough corrupt downloads that I would try this anyway. It doesn't cost you anything. If D is a network share (a mapped drive), copy the installation files locally before you start. 

And you'll capture a 30-second sample of your server's waits during the test. Look at the wait stats section to identify what your server's waiting on - and feel free to post a picture of that portion of the results to get additional help. The bottleneck may not be storage or CPU. Wait stats are the key to finding it. 

Think of the right answer in terms of the COALESCE command, which returns the first non-null value from a list. The prioritized list of recommendations are: 

SQL Server Express is free, but doesn't include SQL Server Agent. Agent is the job scheduling piece of SQL Server. To fire off regularly scheduled jobs, you'll either need to use Windows Task Scheduler or a fully licensed SQL Server (Standard, Enterprise, Datacenter Edition, etc). Those have Agent built in, and you can set up jobs on those servers to back up your Express servers. 

When SQL Server creates the index on the computed field, the computed field is written to disk at that time - but only on the 8K pages of that index. SQL Server can compute the InvoiceStatusID as it reads through the clustered index - there's no need to write that data to the clustered index. As you delete/update/insert rows in dbo.Invoice, the data in the indexes is kept up to date. (When InvoiceStatus changes, SQL Server knows to also update IX_Invoice.) The best way you can see this for yourself is to actually do it: create these objects, and execute updates that touch the InvoiceStatusID field. Post the execution plan (PasteThePlan.com is helpful for this) if you want help seeing where the index updates are happening. 

In AWS EC2, you don't get the ability to add additional network ADAPTERS to your VM. You can add additional network INTERFACES, which are just different IPs/subnets/routes/etc, but they're using the same underlying network adapters (cards). You can choose to use instance types with faster networking ports, though. The EC2 instance configuration page lists which ones have 10 Gigabit ports, although I like ec2instances.info for easier sorting and filtering. When you're doing Always On Availability Groups, it's especially important to use this faster networking because high-network operations (like backups to another machine) can result in SQL Server timeouts (disclaimer: that's my blog post) if you've only got GbE networking. (That causes AG failovers.) 

That rule of 5 and 5 can be violated - the less write activity you have, and the faster your hardware is, and the better you tune your queries, the more you may be able to get away with more indexes. On the flip side, if you have crappy hardware and crappy queries, you might need to drop those numbers lower. The 5 and 5 rule stems from the fact that I've gotta start people somewhere, and I have 5 fingers on one hand, and 5 fingers on the other - so the rule is easy to communicate. 

I hate to say this, but there's a bug in Books Online. The Books Online page on partition switching says: 

Performance usually improves from version to version of SQL Server. SQL Server 2000's Analysis Services was nowhere near quick. As to why - Microsoft doesn't give us the source code so it's a little tough to analyze that. ;-) Like @ConcernedOfTunbridgeWells pointed out in his comment, though, the real reason you should get off SQL Server 2000 is that it's out of support. 

For sp_Blitz, there's an enhancement request filed at Github to add outputs for version, and if you'd like to influence how the work is done (or contribute code), you're welcome to leave comments over there. For sp_WhoIsActive, the author Adam Machanic is considering adding an output to the Messages tab each time it runs. Until then, Oreo's suggestion above of checking the code is the best one. We won't break that intentionally - we haven't changed those strings in years, and don't intend to start, heh. 

Yes, I've got clients with 1k-10k databases per server. (Fog Creek is actually one of them, and I can mention it here because I know Joel's talked about it on the podcast.) Usually you run into problems at this scale, and you start thinking about moving down to VMs instead - it's easier to manage more virtual SQL Servers with less databases per instance. Log shipping technically works, but you have to roll your own solution rather than use the built-in tools. The problem is that these types of businesses usually have new databases coming online all day long, at any time of day, and they need these new databases to be automatically protected by the log shipping. This means as soon as a database is created, you need a full backup taken, plus get it added into the log shipping setup. You'll be rolling your own scripts, and you can't just have one job. One job won't be able to keep up with the sheer number of databases if it backs up one database at a time - you end up with multiple jobs, each of which handles a range of databases or does something like the modulus of the database id. SAN replication is much easier because it automatically protects any new databases that get added in. For critical databases, I recommend using a combination of VSS-integrated SAN snapshots, dirty (non-VSS) SAN snapshots, and regular SQL Server transaction log backups, but you mentioned you've got SIMPLE recovery mode and a 24-hour RPO/RTO. You'll be able to stick with a much simpler solution: dirty SAN snapshots plus conventional full backups. The specifics will depend on your SAN vendor's replication options, but typically they can take a dirty snapshot without quiescing SQL Server's writes, then replicate that snapshot over to your DR site. Done correctly, this happens without impacting SQL Server performance. The problem is that without quiescing SQL Server's writes, you're rolling the dice on whether or not each database will come online. Odds are, when you fail over, you're going to have a handful of databases that don't recover correctly - no problem, that's where your full backups come in. Every night, you should be doing full database backups to another location (not the SAN). That other location should also be replicated to your other site. This way, when you fail over, most of your databases will come online instantly via the SAN-replicated copy. Some of them will not (usually a fairly small percentage). For those few databases, you restore the most recent full backup. End result: pretty quick recovery time objective at a relatively low complexity cost. I would NOT recommend this solution if you need every database to be at the exact same moment in time, or if you needed up-to-the-second recoverability. (Just making it clear because some reader is going to freak out at this design - it's certainly not for everybody, just for this one specific business case.)