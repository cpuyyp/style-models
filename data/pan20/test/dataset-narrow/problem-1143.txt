The best example I can think of is an algorithm (described below) to compute the $k$-level in an arrangements of $n$ lines in the plane, i.e. the polygonal line formed by the points that have exactly $k$ lines vertically above it. This is not the most efficient algorithm known for the problem. There are more efficient algorithms with simpler complexities, but I believe this one is more practical than most (if not all) of them. The analysis is probably not tight, because it uses the $k$-level complexity, which is a famous open problem (I think all other terms in the analysis are tight). Even still, I doubt improved bounds for $k$-level would make the running time much simpler. I'll assume $k=n/2$ to write the complexity as a function of $n$ alone. The algorithm is based on the line sweep paradigm and uses two $(\log n)$-ary kinetic tournaments as kinetic priority queues. Insertions and deletions are performed when a line goes above or below the $k$-level, moving a line from one kinetic tournament to the other. Therefore, there are $O(n^{4/3})$ insertions and deletions (using Dey's bound for the $k$-level complexity). Each event is processed in $O(\log n)$ time and there are $O(n^{4/3} \alpha(n) \log n / \log \log n)$ events (the $\alpha(n)$ comes from the complexity of the upper envelope of arrangements of line segments, while the $\log n / \log \log n$ comes from the height of a $(\log n)$-ary tree). The total running time is $$O(n^{4/3} \alpha(n) \log^2 n / \log \log n).$$ Please check Timothy Chan's manuscript $URL$ for more details and references. The $1/\log \log n$ factor can be removed by using a binary (intead of $(\log n)$-ary) kinetic tournament, but it actually speeds up the kinetic priority queue in the tests that I performed. The complexity should get a little uglier and worse (while the algorithm will still be practical) if a kinetic heap is used instead of a kinetic tournament (a $\log$ inside a square root should show up). 

A problem instance is a finite list of 4-tuples $(\alpha_1, u_1, v_1, \beta_1), ..., (\alpha_N, u_N, v_N, \beta_N)$, where $\alpha_i, \beta_i \in X$ come from a finite set, and each $u_i,v_i \in A^*$ are words over a finite alphabet $A$. A solution to the problem is a sequence of indices $i_1, i_2, ..., i_K$ such that 

For all $1 \leq k \leq K-1$, we have $\alpha_{i_k} = \beta_{i_{k+1}}$. The words $u' = u_{i_1} u_{i_2} \cdots u_{i_K}$ and $v' = v_{i_1} v_{i_2} \cdots v_{i_K}$ are not prefixes of each other 

The problem is in your assumption that rational relations are closed under intersection. The following counter-example is taken from Example 2.5 in Berstel's "Transductions and Context-Free Languages": Let $X, Y \subseteq \{a\}^* \times \{b,c\}^*$ be rational relations defined by \begin{align*} X ={}& \{ (a^n, b^n c^k) \mid n,k \geq 0 \} \\ Y ={}& \{ (a^n, b^k c^n) \mid n,k \geq 0 \} \end{align*} They are rational since $X = (a,b)^* (1,c)^*$ and $Y=(1,b)^*(a,c)^*$. But the intersection $$ Z = X \cap Y = \{ (a^n, b^n c^n) \} $$ is not rational. If there was a transduction $\tau : \{a\}^* \to \mathcal{P}(\{b,c\}^*)$ realizing $Z$, then since transductions preserve regular languages, the language $\tau(a^*) = \{b^n c^n\}$ would be regular, a contradiction. 

I recently became aware of the rather appealing characterization of deterministic word-to-word transductions as word functions with bounded variation (see e.g. [1]). This coincides with the set of functions that can be realized by subsequential transducers, which are deterministic finite state transducers with an output function defined on accepting states. Not every function defined by a non-deterministic FST is subsequential, that is, even if the FST is single-valued, it might not be determinizable. I wondered if there are any extensional characterizations of the set of functions definable by (single-valued) FSTs? Thanks! [1] Bruyère, Véronique and Reutenauer, Christophe. A proof of choffrut's theorem on subsequential functions. Theoretical Computer Science, 1999. 

That is, an instance is like an instance of PCP, but with restrictions on how "dominoes" can be composed, and instead of finding strings that are equal, we need strings that differ by at least one symbol. It is not immediately obvious to me how to decide this problem, and I have not been able to find an example of it in the literature. Update: Origin of the problem Given a finite state transducer over input/output alphabets $\Sigma, \Gamma$ and with initial state $p^{-}$, two states $q_1, q_2$ are said to be twinned iff for all $u,v \in \Sigma^*$ and $y_1,y_2,z_1,z_2 \in \Gamma^*$ if $$ p^{-} \stackrel{u/y_1}{\to} q_1 \stackrel{v/z_1}{\to} q_1 \text{ and } p^{-} \stackrel{u/y_2}{\to} q_2 \stackrel{v/z_2}{\to} q_2$$ then either $z_1=z_2=\varepsilon$ or $|z_1|=|z_2| \neq 0$ and one of $y_1, y_2$ is a prefix of the other. [Weber & Klemm, Economy of description for single-valued transducers, 1995]. This is equivalent to the "algebraic" definition given in [Berstel, Transductions and Context-free languages, 1979]. There are effective procedures for deciding whether all states are twinned or not. I am interested in computing all pairs of states that are not twinned. Applying the method of squaring transducers [Beal, Carton, Prieur, Sakarovitch, 2000], this reduces to deciding the problem in my question for all pairs of states. 

Assuming the given problem is feasible, the algorithm returns an $x$ such that $Px\le 1$ and $Cx\ge 1-O(\varepsilon)$. The number of iterations is $O(m\ln(m)/\varepsilon^2)$, because each iteration increases some constraint by $\varepsilon$, and this can happen for each constraint at most $N$ times. The proof of correctness is via the invariant $$\mbox{Lmax}(Px) \le 2\ln(m) + (1+O(\varepsilon)) \mbox{Lmin}(Cx).$$ The invariant implies $$\max Px \le 2\ln(m) + (1+O(\varepsilon)) \min Cx.$$ At termination the left-hand side is $\Omega(\log(m)/\varepsilon)$, proving the performance guarantee. In Step 2.1, the desired $j$ must exist as long as the original problem is feasible. (This is because, for any feasible $x^*$, and any $x$, if we were to choose a random $j'$ from the distribution $x^*/|x^*|$, the expected value of the partial derivative of Lmax$(Px)$ with respect to $x_{j'}$ would be at most $1/|x^*|$ (see the previous proof sketch for Set Cover). Likewise, the expected value of the partial derivative of Lmin$(Cx)$ with respect to $x_{j'}$ would be at least $1/|x^*|$. Thus, there is an $j$ such that the partial derivative of Lmax$(Px)$ with respect to $x_{j'}$ is at most the partial derivative of Lmin$(Cx)$.) Then the invariant is maintained in each iteration because, by the choice of $x_j$ and $\delta$, and the smoothness of Lmin and Lmax, increasing $x_j$ to $x_j+\delta$ increases Lmax$(Px)$ by at most $1+O(\varepsilon)$ times the increase in Lmin$(Cx)$. Learning (following experts / boosting) One reference for understanding this connection is Adaptive game playing using multiplicative weights, by Freund and Schapire. Here is a quick summary to give the technical idea. Consider the following repeated game. In each round $t$: 

Define time $t_k$ to be the number of steps before $\epsilon_t \approx 1/2^k$. By (2) above, $\epsilon$ decrease by a constant factor after about $\Theta(1/\epsilon^2)$ steps, so $t_{k+1} = t_k + \Theta(2^{2k}) = t_k + \Theta(4^k)$. Thus, $t_k = \Theta(4^k)$. That is, after $\Theta(4^k)$ steps, the distance from the bug to the common point $p$ will be about $1/2^k$. Changing variables, after $N$ steps, the distance from the bug to the common point will be $\epsilon = \Theta(1/\sqrt N)$. And, in the $N$th step, the bug travels $\Theta(\epsilon^2) = \Theta(1/N)$. So the total distance traveled in the first $N$ steps is $\Theta(1+1/2+1/3+...+1/N) = \Theta(\log N)$. This is the lower bound. It extends to proposed Variant 2 (as I understand it), as follows: Adding the restriction that the bug should move to the nearest point in the intersection of the two most recently placed circles does not help. That is, the $\Omega(\log N)$ lower bound above still applies. To see why, we will modify the example above by adding a single extraneous circle that allows the bug to meet the restriction while still traveling the same path: