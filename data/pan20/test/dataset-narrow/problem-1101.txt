This answers question (2): The greedy heuristic for Set Cover / Maximum Coverage always picks the set which contains the maximal number of uncovered elements. Assuming your modification for the heuristic is picking the set which greedily increases the solution profit, you might end up in a lousy approximation ratio. Consider the following example: $$A = \{a_1,a_2,\ldots a_{k}\}, B=\{b_1,b_2,\ldots b_k\}$$ $$C = \{C_1\} \cup C_2$$ $$C_1= A\cup B \setminus\{b_1\}$$ $$ C_2 = \{\{a_{i},b_1\}|i\in[k]\}$$ The greedy heuristic will output $C'=\{C_1\}$, so the profit is $1$, while the optimal solution is $C_2$ which gives profit $k-1$. 

We denote by $[t]$ the set $\{1,2,\ldots,t\}$. A $(n,k)$-perfect hashing family is a set of functions $H=\{h_i:[n]\to[k]\}$ such that for every set $S\subset [n], |S|\leq k$, there exists some $h_S \in H$ such that $h_S$ is injective on $S$. There are known (deterministic) builds of such family of size $O(e^{k+log^2k}log n)$, while there is a $\Omega(e^k \cdot \frac{log n}{\sqrt k})$ lower bound , which is almost tight. We define an extension of such families as follows: A family of functions $\mathcal F=\{f_i:[n]\to [k]\}$ is a $(n,r,k)$-perfect hashing family if for every set of sets, $\mathcal S=\{S_1,S_2,\ldots,S_r\subseteq [n]\}$, such that $$|\mathcal S|=r,\ \forall i:|S_i|=k,\ \forall j\neq i:S_i\cap S_j = \emptyset$$ There exists a function $f\in \mathcal F$ which is injective on all $S_i\in \mathcal S$. That is, every set of $r$ disjoint sets of indices of size $k$ there exists a single function which is injective on all of them. 

To see that this is a much more powerful adversary, consider the case of $3$ experts and $3$ days in which the outcome was always $1$. If $p_1=(1,1,-1), p_2 = (1,-1,1), p_3=(-1,1,1)$, then each expert had a mistake, but a weighted majority vector of $(1/3,1/3,1/3)$ had none. 

$DTIME(n^{polylogn})$ is known as $QP$ (quasi-polynomial). It is widely believed that $NP\not \subset QP$, although it is a stronger statement than $P\neq NP$. Some common conjectures, such as the Exponential Time Hypothesis imply $NP\not \subset QP$. 

A non-deterministic Xor automata (NXA) is syntactically an NFA, but a word is said to be accepted by NXA if it has an odd number of accepting paths (instead of at least one accepting path in the NFA case). It is easy to see that for a finite regular language $L$, there exists a minimal NFA which doesn't contain any cycles (if a cycle was both reachable from the initial state and you get from it to an accepting state - your language is not finite). This is not necessarily the case for NXAs. Denote by $xsc(L)$ the xor-state complexity of a language $L$, and by $axsc(L)$ the acyclic xor state complexity of $L$ (i.e. the size of a smallest acyclic NXA which accepts $L$). 

A chain is not considered an AVL tree, as after the second insert (a chain of length 3), the root's left son has height 1 while son on the right has height -1 (which is the definition of the height of an empty tree). 

Subset sum is known to be approximable in polynomial time for all $c>0$. Does the same hold for DAG-Subset-Sum? 

(all vertices has degree 5, except for the blue vertex). For every vertex $v$ in the 3-regular graph $G$, create two copies of $H$ and connect $v$ to their blue vertices. It is easy to see that regardless of whether $v$ was selected for the cover, you have to include at least $5$ vertices from $H_v$ in the cover (you can take the blue and green ones). Hence every vertex cover of size $m$ in $G$ corresponds to a $m+10|V|$ sized cover in the new 5-regular graph, and vice versa. 

And now simply find the shortest path with an algorithm of your choice (run time is linear in $|G_p|$, since it is a DAG). 

Let $G=(V,E)$ be an undirected, random $r$-regular graph. Let $s,t\in V$, and denote by $N(v)=\{u\in V\mid (u,v)\in E\}$ the neighborhood of $v$. I'm looking for the distribution of the number of neighbors of $t$ which are closest to $s$, that is, let $$C = \text{argmin}_{u\in N(t)}d(s,u)$$ ($d:V\times V\to \mathbb N$ is the unweighted distance function which counts the number of hops between the vertices, i.e., the length of the shortest path). 

For example, consider $X_0 = \{5,4,3,2,1\}$ and let $r=\frac{1}{2}$. The distribution of $x_3 = 3$ is as follows: 

The problem can be solved in $O(n^2)$: Denote $A=\{a_1,\ldots,a_n\}, B=\{b_1,\ldots,b_n\}$. We will build a graph in which every node $(i,j,x)$ will denote which $a$'s and $b$'s we have already executed and $x$ is a bit representing if the last task was $a$. 

We are given a directed acyclic graph $G=(V,E)$ with a number associated with each vertex ($g:V\to \mathbb{N}$), and a target number $T\in \mathbb{N}$. The DAG subset sum problem (might exist under a different name, a reference will be great) asks whether there are vertices $v_1,v_2,...,v_k$, such that $\Sigma_{v_i}g(v_i) = T$, and $v_1\to..\to v_k$ is a path in $G$. This problem is trivially NP-Complete, as the complete transitive graph yields the classical subset sum problem. An approximation algorithm for the DAG subset sum problem is an algorithm with the following properties: 

In particular, the Majority algorithm (see 1 and [2]) is able to find a majority item in the stream using a single counter. 

"Guess" (i.e. go over all possibilities) the number of elements in the minimal feasible disjoint cover, $x$. Since the sets are of size $\leq k-1$, the smallest feasible cover is of size $k\leq x \leq 2(k-1)$. Randomly color all items by $x$ colors. Denote the coloring by $C$. Use dynamic programming to figure if if a set $S\subseteq [x]$ of item colors can be obtained by selecting disjoint sets. Formally, each cell in our matrix $A$ will be some subset of $[x]$, our initialization will be $A[\emptyset]=true$, and our computation will be $A[t]=\vee_{t'\subset t}\{((A[t']) \wedge (\exists s\in \mathcal{S}:C(s)\cap A[t']=\emptyset)\wedge (t'\cup C(s)=t)\}$. Basically, $A$ is a boolean matrix denoting if some set $t\subset [x]$ of colors is coverable by disjoint sets. 

I'd like to add another approximation algorithm, a parametrized one: For a fixed $\delta>0$ (or more preciesly, $\delta =\Omega(\frac{1}{poly(k)})$ ), you can compute a $(1+\delta)$-approximation of the number of simple paths, in either undirected or directed graph, of length $k$ in time $O^*(2^{O(k)})$. 

An interesting case is when the graph is bipartite and the fairness only applies to one side, that is assume that $G=(L\cup R,L\times R)$, and we are given a profit function $p:L\times R\to \mathbb N$. A Fair Bipartite Matching is a matching in $G$ such that for any two vertices $u,v\in L$: $$(\forall w\in R:\ \ p(\{v,w\})\geq p(\{u,w\}))\to M(v)\geq M(u)$$ 

How bout the simplex algorithm for linear programming? In many occasions it is used in practice. Edited to add: I think it's more of a "worse-case exponential algorithm" which runs efficiently on practical instances/distributions rather than runs faster on practical sized adversarial instances. 

Assume now that we want sub-linear space, and are willing to settle for approximate Rank queries. That is, given $i$, we want to return an estimate $\widehat{R_i(S)}$. 

I'm looking for problems that are hard to solve in FPT time but has an approximation algorithm. That is, problems that are: R1. W[1]-hard. R2. Admit a (preferably constant) approximation algorithm in FPT time. The problem I'm familiar with is counting the number of simple paths of length $k$ in a graph. It is known to be #W[1]-hard, but admits a $(1+\epsilon)$-approximation in FPT time (for any constant $\epsilon$). Also interesting would be problems that satisfy R1 and R2, and also: R3. There exists $\epsilon>0$ such that the problem is not $(1+\epsilon)$ approximable in FPT time (unless W[1]=FPT). 

There are a few tools, either online or offline, that could solve (find equilibrium) in a game explicitly given as a real-valued matrix. Such tools are Game Theory Explorer and Gambit. However, as far as I know, these can not be used for finding equilibrium in a game with parameters. For example, consider a two player game symmetric game, parametrized by some $x\in[0,1]$, which is given by the following payoff matrix for the row player (and $A^t(x)$ for the column player): $$A(x)= \left( \begin{array}{ccc} \frac{x}{2} & x \\ 1-x & \frac{1-x}{2} \\ \end{array} \right) $$ Each equilibrium in this game can be represented as $<p_a,p_b>\in[0,1]^2$ where $p_a$ is the probability of the first player playing the first strategy and $p_b$ is the probability the second player plays the first strategy. Analytically we can find all the equilibriums for this game $$EQ(x)= \begin{cases} \{<1,1>\}&\mbox{if } x>\frac{2}{3}\\ \{<3x-1,3x-1>,<1,0>,<0,1>\}&\mbox{if } \frac{1}{3}\leq x\leq\frac{2}{3}\\ \{<0,0>\}& \mbox{else} \end{cases} $$ I'm looking for a solver which is able to understand inputs like $A(x)$ and produce $EQ(x)$. 

In the well studied problem of Hamiltonicity, several papers/theorems gave sufficient "degree conditions" for the existence of Hamiltonian path in a graph. These include: 

The special case in which the function may only increase the value of an element is also of interest. 

EDIT: I'm specifically interested in computing the number of distinct elements exactly, with high probability, and not in approximation algorithms. In the paper An Optimal Algorithm for the Distinct Elements Problem, the authors give a $O(\gamma^{-2}+\log n)$ bits algorithm for computing a $(1+\gamma)$ approximation with high probability, and claim that this is optimal. However, setting $\gamma<n^{-1}$ for getting exact count with high probability gives a $\Omega(n^2)$ bits algorithm, which seems worse than the $O(n\log n)$ proposed above. They do not assume that $n$ is known in advance, which may explain this difference.