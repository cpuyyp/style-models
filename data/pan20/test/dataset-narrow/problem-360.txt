I have an script that I used to use. this is to compare 2 databases, in the example I am comparing with . Now I only use the visual studio schema comparison tool as suggested in the comments. It is not even properly formatted, it is not a complete, ready, free from bugs solution, but it would at least give you some ideas. I still would like to suggest you use the visual studio community edition 

All that done - and to verify that it is all done alright I have the following script to check how many rows in each of the partitions: 

Thanks to all of the answers to question below I can retrieve my scripts from the table. How to retrieve a large text from inside a table? I am very thankful, that is working fine. Sometimes though, I need to re-run a script that I have run in the past, or I want to attach it to an email. It would be really helpful if I could save my script to a file. I have tried a few things, all using bcp and xp_cmdshell. Write to a text file from a stored procedure Exporting Data Programatically with bcp and xp_cmdshell How to create text file using sql script with text “|” so these above are all working as you can see the scripts below: Please not that you need to temporarily enable xp_cmdshell first. 

I am currently dealing with a bunch of servers and they have different Database Mail Configurations. I can see the Database Mail configuration for each of those servers. I use the following script: 

the below code, get the current session , gets an error when when converting to datetime, then sets the dateformat to and last but not least test the conversion (casting) again and it works 

question is, in the event of a , when this current server becomes the secondary, alongside its c:\ drive, would the certificate still be valid? I should be testing this instead of asking but I don't have a good testing environment in hand. 

the session 92 you can see on the above picture is a select and the session 75 is an update that I left the transaction open. session 92 

apparently this is because I need to download and install Microsoft SQL Server Data Tools - Business Intelligence for Visual Studio 2013 $URL$ I will get this done and if it all works I will accept this answer. 

I need to create functions in the master database, and I am developing a template for that, I want first to check if the function already exists, if not create a dummy function. then grant all the relevant permissions to this dummy function. the third step is to alter the function to the proper code. the code that I am using: 

I have a procedure that shows me all the permissions on the user objects. I can specify the name of the object too. the code is this: 

after trying out this script: Script out all alerts By Carolyn Richardson or even the traditional way of ssms -> right click -> script alert as none of these methods are fully comprehensive because they miss out information that is inside the alert, like the job it calls, etc. for example I have been Setting Up Failover Alerts and I get many actions set up as you can see on the script below: 

But still when I try to login to that SQL Server instance, using windows authentication, it says I don't have a login? Where to look at in order to troubleshoot this? 

I found what the problem was. I had not deleted the database from its original location, after I detached it. I copied the database to the new drive, which on my case is T:. After I deleted the database from its original location, in my case F:\ssas\data, then I could attach the database from T:. After attached, I checked on the F:\ssas\data folder and SSAS had created the following structure there, see below. 

I would like a script that would tell me at any point what transactions I have currently open on this and\or on the involved tables for example in the script above ,. I have been looking at the following interesting links: GUIDE FOR READING SQL SERVER TRANSACTION LOGS Questions About SQL Server Transaction Log You Were Too Shy To Ask so far I have: 

Are there other performance drawbacks than: 1 - slowing down some DBCC commands 2 - Queries that use operators such as TOP or MAX/MIN on columns other than the partitioning column may experience reduced performance with partitioning because all partitions must be evaluated. 3 - 

How can I find the code of the blocking session, in this case, the session 75 when the session's status is 'sleeping'? New version this new version shows also the blocking session, however, I could not find out how to get the database name and other data from a sleeping session. 

this procedure inserts values into a table variable from the following table: please note that the columns in the table are VARCHAR and for some reason the parameters of the stored procedure and inside the code as well, everything is converted to NVARCHAR. 

there are still errors on this replication monitor, but they are unrelated to this subscriptions as they are on a different server. I would like to include a link to the question below because it might be useful when you have problems deleting a publication SQL Agent still attempting replication for deleted publication 

Besides what @James raised on the comments regarding the SSAS settings of commit timeout and force commit timeout, It was the change that I did on SQL Server that make it work at the end. 

and after that I changed my Powershell script to the following: (which is a little bit different than @ShawnMelton's but it is working fine), any comments appreciated. 

while in the process of creating a publication in one of my servers I continuously get the following error message 

you can wrap the code inside a stored procedure with as a parameter, that is how I did it here, including the error handling stuff, which is important too. Another thing is to install the active directory module for windows powershell 

I have got something that I believe solves the problem, however I have not worked with dynamic truncation: 

How to find out how much i/o resources a task, session, request or transaction is currently using? by task I mean the DMV called DMV sys.dm_os_tasks. by session I mean the DMV sys.dm_exec_sessions Sometimes a session has a request, by request I mean the DMV sys.dm_exec_requests or a session might have a transaction instead - sys.dm_tran_database_transactions How can I link these things with I/O usage? I had a look at the DMV called DMV sys.dm_os_tasks . The DMV sys.dm_os_tasks has some interesting columns to find out more about processes currently running in sql server. sys.dm_os_tasks - not showing pending_io_byte_count However, when I try to identify how much I/O each of my tasks is dealing with the pending I/O in bytes is always zero. 

when I run my script it shows me the next running time for the job in general, but I would like to see it by schedule too. 

3) what if I created a single table with all the columns of all my 5-6 tables, they have a bunch of columns that are the same for all of them, but another set of columns that are different. this table would be way bigger and wider but I could partition it according to their different types. I could use this table from the start and avoid the inserting I am doing at the moment, but then on the selects I would have a much bigger table to deal with. would this be something worth considering testing? 

I have recently created a linked server to one of my SSAS servers. when I go ahead and open the catalogs, to see which ssas databases I have there, I use the following procedure: 

My question is: How can I use the above script that uses is_srvrolemember to find out the server roles of ? 

but when I use my own sp_foreachdb procedure, the source code is on this link below: A more reliable and more flexible sp_MSforeachdb 

The image below shows a transaction replication problem that I am currently investigating. The image comes from the Replication Monitor. How can I get this information using T-SQL? 

what should I monitor BEFORE and AFTER the changes in order to conclude whether these changes are worth applying in LIVE? For how long should I monitor if the purpose of the monitoring is ONLY to prove THESE changes above? I have put a simple question, expecting just a starting point. I have nothing in place at the moment, and have not started this task so far. I prefer not to rely on third party tools - for the purpose of this exercise. 

I am attaching a SSAS database that I have just moved to a different drive following the below link: Attach and Detach Analysis Services Databases First I had to grant permissions on the new drive to the ssas services account: 

if you want to know who wrote what and when, I would create 2 users, one for each of the procedures involved, and then create procedures with execute as user add an extra column on your testtable to keep the session_user or current_user so that you know who inserted/updated each record since insert_procedure will be executed as insert_user and update_procedure will be executed as update_user. this script can help you to find out the original login, and compares different ways of addressing the users. 

I basically saves the contents of DMVs into tables (tablebackups) so that I can compare the situation as it is now (straight from the DMVs) and the situation as it was (from the tablebackus tables). works fine. Only problem and hence this question: Sometimes the total_worker_time now is less that what it used to be, for a reason I haven't identified, and therefore it gives me negative values. I have been run this script as a stored procedure that is called by a job every 5 min. so in the last 5 min I know how many times each procedure has been executed, that has been accurate, but the CPU usage has not been, for the reason above. what can I do to tackle this? 

The question: How can I find out what I can remove from the plan cache? or at least how would I start this investigation? 

I am using the procedure to find out what groups a user belongs to. I have created an AD group called , and added myself to it, then created a login and granted to the group. 

maybe something else is failing. check what is failing by running the script below. that could be an start for your investigation. 

I create a procedure in order to create random values to be added to MYXML column I want to add one million records to that table 

I have publications with over hundreds of articles. I save the scripts, but I would like them to be properly formatted. How can I format my scripts in SSMS? I wanted to see my scripts in this way: 

OrderPayment and Order. Looking at it today(day 1 after the rollback) , it seems so clear that this indexed view - with an inner join using these 2 big tables - is not a good idea. Unless I had changed also the way data is written into these tables, alongside the replication. 

when I use the function fn_my_permissions, I get the exact permissions inside and outside the OPENQUERY. 

I have just installed a new sql server 2014 server, migrated some databases there and connected some applications to them. These servers have also SSAS and SSIS. I have a group of developers there were helping with the connections and doing all the last developments on packages, and data warehousing stuff. Now that everything is working, I want to remove the sysadmin permissions that I had previously granted them. Now, these guys they have some requirements, that I am willing to grant them, so that it is less weight on my shoulders: 

I have a test environment where the AD group is currently . Recently I have had some issues, specially regarding to people restoring databases. My concerns are: 

The only extra is that this indexed view is partitioned according to the PARTITION schema and PARTITION functions below: 

I am aware that I am using a linked server within a dynamic script, is there a way to work around this? 

and this one I would like to see all the operations for those tables that I am touching but this script returns nothing. so, the question is: 

SSD = solid state drive We have a specific question about tempDB performance on server CRM01 (Marketing CRM server - running large queries): On CRM01, tempDB data and log files reside on the solid state drive. The SSD total size is 400Gb. Tempdb data files take up 300 Gb and the log around 100 Gb and may need increasing but there is no space left. The question is theoretical. We do understand that: - if tempdb log files grow too much we should split our processes so that data should be processed in smaller chunks, or we should look at using possibly temporary physical tables instead of tempdb. 

We had great results on our queries that were using this indexed view, however, I had to drop it because of too many deadlocks caused by the replication. replication was deadlocking with itself when inserting into table dbo.tblBOrderPayment or table dbo.tblBOrder how could I have avoid this rollback? whilst I cannot re-create the view and put it in place in a busy production system, I have add below information from the log that shows me 2 replication procedures deadlocking each other. 

and this only happens in some of my servers, while in others it always works fine. It goes away if I drop and re-create the stored procedure. questions: 

I have a process(es) hungry for tempdb, but I am struggling to identify this process. any ways I could achieve this? 

from sp_addarticle (Transact-SQL) I get an example as how to add an article to a publication. this adds the table to the publication 

but when I select from that table, there is always 7 rows less. my count always tell me the table has 7 rows more that it actually has. one thing I thought might be the case, is this is actually not a table, it is a heap, because it does not have any indexes as you can see on the script below. 

This is the piece of code that I am mostly using to test the email sending. It shows how to send the result of a query by email, but in this case I made the query very simple because I need to get the email working first. 

we have a process that is called every season (4 seasons per year) we have new items for sale and there are price variations in many items. we have also many different markets (these are different countries i.e. USA, France, Germany, UK I need to update the test database with the contents of the live database for the new season coming up. I have many tables, but let's concentrate on one of them: So far I import all the table filtered by season\year from into a database called in the That will bring about 15 million rows accross. After this import finishes, I then use the Merge command, to refresh the data in live as per the script below (it is a big merge). 

I execute the stored procedure and on a separate window I use the script below to check if it is running. (if it shows up, then it is running) 

Run SQL queries from data server (you need to be remotely connected to the database server) Enable use of Kerberos on the database server Set proxy account for linked server, so that MDX queries are executed in its context instead of in context of the user that is issuing t-sql query: