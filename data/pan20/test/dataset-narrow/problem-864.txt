Below is how a large organization does it with Puppet and SSH keys. The puppetmaster distributes the user accounts, ssh settings directory for each user, and sets a password for each user. $URL$ This can be replicated in any config management system, not just Puppet. 

This is an area where I've been met with a great deal of frustration in Solaris. One unreasonable way I've found that could work for Solaris global zones would be to create differential flars (one from the beginning, then one afterwards), then extract the differential flar's cpio archive and pass it to a DIY monkeyscript that would create a Puppet module. When you have ZFS root pools you can just create a new BE, make your changes, lucompare and send that to DIY monkeyscripts which can spit out your Puppet modules. But again that doesn't help with non-global zones. 

(Modified from this config example on monit.com) This could play into the ulimit option mentioned earlier as well. Restarting the service is a bandaid. You should instead try to find out why it's leaking memory. 

In this situation you should probably try to duplicate the volume rather than trying to duplicate the individual files. Does your iSCSI target have a way to clone the volume? 

To connect directly to the desktop. This will automatically SSH to servername.domain.com, forward a port to the destination VNC server, then connect to that port. Replace desktop with the internal DNS name or IP of the desktop, and of course the rest should be obvious there. As an alternative there's also X forwarding, which you can bounce through a tunnel, but XLib does not make any attempts to optimize for bandwidth. 

Run a second instance of Monit with a different pid file, different check interval and a different config file. Wrap the expensive check in a script that will repeat the status of the last check until the next "expensive interval". Adjust the alert count to compensate. 

Re-posting from my LVS mailing list response: Usually for MASQ/NAT mode the real server would be in a different subnet with the LVS server set as the default gateway. If you want to do one-arm i.e. same subnet MASQ then the test client needs to be in a separate subnet OR you need to have special routing rules on the real (backend) server. $URL$ 

My first question would be why? If mod_security is on the actual server it will be transparent. If mod security is on a gateway then the client will only ever see the gateway server address. Just use x-forwarded-for to see the client IP in the server logs. It may be possible to use TPROXY in the linux kernel in a two subnet configuration where the servers default gateway is translated through the mod sec box (servers would NOT be able to have a public IP address). But I'm not sure if apache/mod_sec even supports TPROXY it needs code specifically to support it (like in HAProxy). 

DSR is implemented in the Linux Kernel (IPVS) which only works with LVS (Linux Virtual Server). You will also need a health checking daemon like keepalived or ldirectord. BTW DSR does not work in Amazon AWS or in Azure due the to the network virtualisation security that they use. It won't work in things like Docker either. $URL$ 

I know this is a fairly old post but I was searching for load balancing remote desktop on AWS and came across it. Personally I prefer NOT using RD Gateway and just sticking to plain old session broker with a load balancer in front of it for high-availability). You could always use a VPN for extra security. 

I'm glad that you seem to have solved the immediate problem with fail2ban, and it does make sense to block at the iptables level, but you can do the exact same thing in your HAProxy config: You can use acl's with src_http_req_rate() or even src_http_err_rate(Abuse): I've used them in some examples of haproxy configs for DDOS mitigation here. Longer term: It sounds like you might want to implement a double login for that page, if the users will accept it then you could just put an extra htaccess password in from of the login page, so that they need to separate logins (pretty hard to crack with a script). Or you could use mod_security to do the same thing or some kind of honey trap. 

Really the best solution here would be APCUPSD, you ought to get something that has monitoring capabilities (USB,Serial,Ethernet). 

Your "most preferred" package is handled by . The command can give you information on the alternative packages and their priority. will display all of the editor entries and their priority, in a machine-parseable format. There is also a utility called that accepts input and returns the first usable entry. 

A bandaid approach would be to listen non a non-privileged port, then redirect with xinetd from the privileged port. 

Have you tried mod_evasive? It may be exactly what you are looking for, depending on the block time you want. 

By far the simplest way to do this is to install VNC Server on the Desktop machine. Then you can do the following from your laptop with RealVNC Viewer installed: 

In your second scenario you could possibly NAT the entire second network to a 10.0.0.0/24, IE 10.119.0.10 would go out your natty router, be sent to their natty router, then NAT to 192.168.1.10 on their side. If it even works it will be as much work as just changing the network enumeration on either end really. 

In the USA there are OSHA standards with regards to ambient noise. If the ambient noise exceeds a certain threshold you are required to wear ear protection. I am sure that any EU countries have even stricter standards. Even if it's not immediately "ear-piercing," prolonged exposure to elevated noise levels can damage your hearing. $URL$ 

As it sounds like your company is running a captive portal this will be quite an issue. If you log on to the captive portal are you then able to use the proxy normally, or do you have to keep that portal session active? 

Mount the OS root partitions in /mnt/osA and /mnt/osB would give you some output similar to Solaris' You could then diff the more concerning files closely, like sysctl.conf, httpd.conf, etc. And how could I forget Blueprint! With Blueprint you can run against a system and get a recipe of what has changed from the default install. 

Your original attempt was correct. If you remove the port from the server line then it will use the same one as the incoming request. However you will need to add a checkport because it wont know which port to do the health check on! Check your status page to see if the health check is working. 

Yes, Traffic is configured to hit a Floating Virtual IP, when the master load balancer fails the slave detects this, brings up the Floating IP and sends a load of gratuitous ARPs to the network. You can achieve this with VRRP, Keepalived, CARP etc. The Loadbalancer.org appliances use HA-Linux & HAProxy. 

I see the confusion now, you actually want to know how the backend server can read the x-forwarded-for header from the logs? Try these instructions for Windows IIS XFF or Apache XFF. 

This should be fairly simple: a) You must configure the passive mode ip as x.135 (which you have done) b) You must restrict the port range on the ftp server (which you have done) c) You must bind haproxy to ALL relevant ports (your config doesn't show that) Be careful to remove the port from each real server when you do this i.e. 10.11.130.140:21 becomes 10.11.130.140 You almost certainly need to enable persistence/sticky in the haproxy config... and longer timeouts - 15mins? But more importantly when testing - Turn off the firewall stuff completely test if your haproxy config works from the local network - once you are happy that is all working. Then start configuring your firewall. Do you really need a local stateful inspection on each FTP server? Just configure your external firewall correctly. Hope you get it working. 

Karl, Call me crazy bu don't you just need to remove the OR? redirect scheme https code 301 if !host_mydomain or !{ ssl_fc } should be: redirect scheme https code 301 if !host_mydomain AND !{ ssl_fc } i.e. if NOT the domain I don't want AND NOT SSL 

By specifying you tell netstat to use the port number instead of the service name. If you look at the output of you will see that the port numbers are instead human-readable service names. These are mapped from , so if you are listening on port 80 you will see or , and if you are listening on port 8080 you will see or . From the netstat man page on Fedora 16: 

To completely block traffic you could do an iptables drop command for the outbound traffic to that dst address. ie 

So you could specify any number of check cycles between checks for your expensive check. Configure Monit to have the check interval you desire. 

I've been fairly happy with RackMonkey. It allows me to define the apps that run on the bare metal, what hosts do what functions, where Staging is for the app, where Development is for the app, etc. It's also just perl/sqlite3 so it's easy to extend the functionality. 

It's been my experience that things like these die as soon as your shell is terminated by idle activity. This is one of the circumstances where you should be using something like GNU Screen. With GNU Screen you log into your server, type , and then begin your work. You can spawn additional terminals inside that screen session with Ctrl+a c , and cycle between them with Ctrl+a n and Ctrl+a p. To disconnect from the GNU Screen you can type Ctrl+a d. To reconnect you can log in and run . A Howto from Kuro5hin is available here: $URL$ 

This is one of the areas where Puppet is helpful. Since Puppet is self-documenting, you can easily expand your network and recover from outages. Puppet collects certain information about the hosts it manages, the processors, RAM, disks, SSH keys, etc. The basic premise is to kickstart enough to get EPEL installed and onto the local PuppetCA, then your puppetmaster can take care of the rest of the configuration. Rolling endless crap into kickstart is not a solution. A software that you can use to identify changes in an environment is Blueprint, which can also play into Puppet configs. The end result is that your servers should be recipes. Predictable, reproducible recipes that you can test and deploy at will. The question here for you is the size of your environment. How many physical hosts are you managing? How many virtual? If it's not a lot, Puppet might not be worth the effort. 

First of all are you sure the connections are not just hitting the queue? i.e. you have reach maxconns? What does your stats page show? Also just disable conntrack (it sucks): 

It is simple enough to use HAProxy to load balance any TCP connection including telnet (most protocols are very similar to telnet anyway). But you should enable persistence by source IP and for long lived connections you should make sure you set long timeouts and keepalive i.e. 

I'm not sure that I would call DRBD easy, awesome maybe but not easy. As suggested by Jeroen: Round robin DNS or HAProxy can be used to route the traffic to both nodes, but yes your actual problem is that your database is only in one place. So hitting both servers may not be what you want. DRBD would ensure storage is replicated between the servers (solving the problem). Or the standard architecture is 2*HAProxy in front of N+1 Web Servers with the database on a separate and highly available platform (maybe master/slave maybe shared storage maybe DRBD or maybe memched). 

Try removing parts of the system to find the bottle neck. 15 test servers sounds an awful lot! you should be able to get 1000's of TPS out of a single test unit. You are not asking HAProxy to wait for a response are you? i.e. utilising the maxconns functionality and queueing functionality? Like I said try simplifying, but if you do think it is HAProxy then please post the configuration. 

At Loadbalancer.org we do this is a slightly complicated way :-). Before we restart HAProxy we check all of the current states and check for any servers that are DOWN or MAINT. Then we BLOCK SYN packets (handles the dropped traffic on reload issue) Then we restart HAProxy Then we manually set all the states to DOWN or MAINT (based on what they were 0.01 seconds ago..) Then we re-enable SYN packets... Giving us the required result of seamless restarts with no downtime or lost packets. If you can't be bothered with ALL the above then just set everything to DOWN state after a reload ..do it fast and cross your fingers :-). v1.6 has some new state-full restart code but I haven't researched it yet.