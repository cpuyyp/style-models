Note that instead of you can use which executes faster, but then you won't see any records that aren't s. 

I don't think MySQL makes a lot of sense if you're using it for data aggregating only, so yes, consider using Apache Kafka for that. Then, if you want a more fully-featured OLAP system than ClickHouse, consider e.g. MariaDB ColumnStore (more technical details here), also known as MariaDB AX, which supports bulk loading of data in addition to all the usual conveniences and flexibility of SQL, including some advanced features (like window functions and CTEs). Also note that ColumnStore has a Kafka adapter. 

I'm surprised that time_zone values and give different results for , given that my time zone is GMT, +0000. This looks like a bug to me, but maybe there's something I'm not understanding. 

I don't know if this is likely, but if we imagine an additional record, essentially a duplicate of the data in record with id 6: 

Looks like maybe a comment in one of the option files (such as /etc/my.cnf and any .cnf file included from that file) has become un-commented. Search through these files for "Disabling symbolic-links is recommended" and make sure there is a comment character (i.e. '#') in front of it, then save the file and try starting mysqld again. 

Yes, the server would still be accepting remote connections. And yes, it probably did what it was supposed to do. doesn't set neither in MariaDB nor in MySQL. It does however allow you to remove root accounts that are accessible from outside of localhost, as you have already done. If you don't have any database users with host != localhost, then remote login shouldn't be possible. You can of course also block the MariaDB server port in your firewall. 

One solution that could work is a Galera cluster with one node in read-only mode. (See e.g. here for info about geo-distributed Galera cluster.) Galera cluster is a (virtually) synchronous multi-master solution. MariaDB and Percona provide this technology, as does Galera themselves. There is one problem: Since you would be writing to more than one node, there is a higher likelihood of experiencing deadlocks / certification failures, including on commit. (See this article for an explanation.) However, this is something your application could handle if you take care when developing it. Be sure to review the Galera cluster limitations before you decide on this technology. Some of the most notable requirements are: A Linux OS, it only supports the InnoDB storage engine, and all tables must have an explicit primary key. MySQL Group Replication is a similar technology, but this wouldn't be suitable for geo-distributed database nodes as in your use-case. 

I believe the reason is that the information_scheme is the bottleneck here. Afaik, a known problem when having plenty of tables around. 

So, that got me thinking, You could do this with plain vanilla tables, temporary add partitions by hash and later remove it (or keep them, I can strongly recommend partitions). I'm using mariadb however, not mysql (so XtraDB) Perhaps this helps someone. I'm still running it, so far so good. Changing ENGINE seems to do the job as well, so I bring it back/forth between MyIsam and them back to InnoDB. It's fairly logical, if you change ENGINE, the table disappears from innodb, so it will not be a problem anymore. 

That will effectively drop each partition in the change and recreate it with a temp copy of the content of what was in there. You can do this per table if you want, my application allows for that to happen, so no need to worry about synced backups etc. Now for the rest of the table, since I haven't touched all partitions in the process some will be left with the log sequence warning, for those ones that are broken but and covered by this reorganize action I will probably run this: 

I managed to solve this -for the most part- by changing a few parameters. apparently it looks like the database needs to warm up by opening the tables at least once. This is behavior I am familiar with since mysql 5 with plenty of tables and databases. The query still runs slow when it's the first time it runs (TABLE CACHE IS OFF). By changing the parameters below, this only happens on the first run of the query. Any additional attempts to run it after that does not exhibit this slow down anymore. The following parameters helped 

I found out there is perhaps a cooler way to solve this problem working on partitioned tables. I needed to drop partitions from some years back, and had to add some for 2014. Almost all partitions report this error, so also old ones. Very nasty crash. So while DROPPING old and using REORGANISE of the MAXVALUE partition(the last one), It will create new files who are ok, so I get less and less warnings. In the mean time, it helps incrementing the log sequence counter, so I don't need to insert bogus data. I have this happening on a master server btw... So this: 

I used the last ones on several tables. The warnings happen when it's trying to open the files and there is one for every partition definition it opens with counter issues. Almost rolled over the counter today for the last tables. I think once it's all processed one needs to flush the binary logs. update: I can conclude a few things now I managed to sort this problem out. 

The following assumes that you feel the gaps in the sequence is a problem. This usually isn't a problem, unless these gaps become enormous and there is a risk that at some point you will exceed the maximum value. I'm further going to assume the table is defined similar to this: 

Additionally, it's possible that another software (such as cPanel) might change the variable dynamically after MariaDB has started. To see which options the MariaDB server will get from the option files, run: 

Even though only 0.1% of the articles have a duplicate, the cost (in terms of wasted space) of a NULL int value (for a column) in the table is small. (In fact, if using the InnoDB storage engine, you will waste no space at all if the value is NULL. (See e.g. Bill Karwin's SO answer here as well as the MySQL documentation about InnoDB field contents.) However, if the column needs to be indexed, then there is a performance penalty. A separate table () is the more normalized approach. This will add another join to your queries, but again the cost of that (in terms of extra memory and processing time) is small because the table is small, especially if best-practice indexing is used. You should therefore have a compound index (article_id, redirect_to) on the article_redirect table. For the separate table approach (), it's good practice to define an explicit primary key (in every table). Assuming an article can only be a duplicate of one other article (but many articles can duplicate the same article), then the primary key should be . As an aside, I will mention that it's usually not good practice to have columns ( is a type of ) together with other "meta data" columns such as title, timestamps and so forth that you might retrieve in queries. Instead, consider moving this to a separate table with a foreign key pointing back to your article table. See the MySQL documentation about optimizing for BLOB types for details. 

However, in your case, because of the at the end it will still create a temporary table, though it might not necessarily be a disk table. If you really want to avoid the temporary table, then you'll have to decide if you really need this ordering in the query. You will avoid the temporary table if you instead on each . (Note that you then need to wrap each in parenthesis.) And just a thought, given that these two tables are identical except for their names, why not just merge them into one table and avoid the -> temporary table problem? 

Is creating columns for all the combinations the correct way? Based on the little information you have given us: No, columns for all combinations is probably not the right solution. Imagine if you suddenly get more products, then you will need to add more columns. Or if some products are removed. Designing your schema so you have to alter a table to deal with something as trivial is bad practice. No, you should probably use records instead. Let's say you have a table, something like this: