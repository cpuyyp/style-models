IMO, an external table is much easier to manage and flexible than a SQLLoader script. So if you're already doing a recurring SQLLoader load, I see no possibilities of evil in switching to external tables. By doing the external table, even though the base of the table resides in a text file outside the database, the data can be accessed from within the database, no need for a separate SQLLoader tool/scripts. So you can write procedures/packages to manipulate the data straight from the external table. No need to load it anywhere...as long as it's in the directory you have set, it's already "loaded". As a DBA, I'd much prefer managing a DML script using an external table than having to manage a SQLLoader script. But SQLLoader has been around a while....if your DBA has too, it may be a tough sell. :) 

You can also do this by utilizing the v$parameter view. Assuming your spfile is saved under the oracle_home in the dbs/ folder, this is a rough query using functions within functions to get the oracle_home. 

Even though you are using SYS (which you really shouldn't be), the view is stored in a SCHEMA1, as I will refer to it. SCHEMA1 is trying to select from a table, via the view, in SCHEMA2. Therefore, you need to grant SELECT access to SCHEMA2.TABLE to SCHEMA1. And don't run things as SYS. :) 

Some of the mystery is solved - The reason that some sites could connect and some couldn't is that there was an additional connection string stored in the machine.config file that two of the sites were using. The site that worked when a port number was specified did not use this connection string, so worked. The other sites didn't work when port number was specified because I hadn't added the port number to this connection string in machine.config. Adding the port number to that connection string as well as the ones stored in web.config allowed all the sites to work. Additionally, the reason that iisreset had an effect is that once IIS knows the port number that SQL is using, it seems to 'remember' the port number, and so it no longer has to rely on using port 1434. Once I did an iisreset it 'forgot' what port number was used, so had to make a request on port 1434, which for some reason doesn't work. So my question has gone from a very confusing 'works sometimes' situation to a basic 'cannot communicate on port 1434' situation. 

To calculate TPS (transactions per second), run the query several times and calculate difference over time interval. There are ready made tools for that, one of them is $URL$ More info: $URL$ 

If you really want two queries, you can use special FOUND variable to test if previous query gave any result: 

If you want to see also query duration, you can set instead of . This is very useful for query tuning. Then reload config (restart or HUP) and collect enough log to estimate traffic. Note: neither method will include queries embedded in user-defined functions. 

You can always calculate higher-level information from lower-lever aggregate. For example if you had an aggregate on (app_id, day, collection_id) you could use it instead of (app_id, day). You can materialize your aggregates with feature. But this is not an only way. If old data is static, it could be enough to insert new rows daily, with something similar to 

The answer depends on what is really going on the wire (that is, in client session). First, what is a deadlock? It is a cycle in lock dependency graph. Whatever it means, to make a deadlock occur, you need to have some locks first. Locks normally do not live outside of transactions/queries. A connection itself cannot cause any deadlocks if it does not put locks on database objects. (Shared object access is main purpose of locks) If the connection opens a transaction, does some queries (effectively locking some database objects), and then still executes or sleeps in middle of the transaction (without commit/rollback), it COULD cause a deadlock - when sibling transactions access same resources. If it is just an idle connection - there's no risk of deadlock. 

First off, I'm using SQL Server 2008 R2. When I set up my maintenance plan and got to the "Select Report Options" step, I selected to email the report to the Operator I already have set up. This does not allow me to alter the subject, which I believe Gmail uses when it collapses emails into conversations. Is there a way to trick Gmail to separating them out based on the database? Or is there a better way from within SQL Server Management Studio to email the reports out? I have multiple instances, with their own Operator, each Operator is using the same email address (no_reply@noneofyourbusiness.com), but the name is different....but since the subject is the same, Gmail still collapses them into one big conversation. Does anyone else have to deal with this issue? 

Does anyone know what triggers the USER_ID field in the log.xml to be populated? The value also exists in the V$DIAG_ALERT_EXT view. I've found by observing the logs that if a temp tablespace fills up, it will log the USER_ID of the problematic SQL statement causing the issue. But other than that, it appears that value is always NULL. 

It's always possible that this table is populated by something external to the database instance that the table is situated on - from an application, via a linked server, from a script on the server. There are probably other possibilities but you get the idea. The best way I can think to track activity on the table is to run SQL Server Profiler (via the Tools menu of SSMS on the 2012 version). I'm not going to write a tutorial on that software here but you essentially want to filter the activity on the server by the name of the table that you are interested in. If anything updates that table then it should appear in the profiler results. 

I have a table that has a full text index defined on it. Change tracking is set to 'Automatic' but I can see that 'Table Full-Text Pending Changes' is set to '58429', and recently added rows (for the last few days) in the table are not being picked up. The index is enabled. I don't understand how Change Tracking can be 'Automatic' but the index is not updated ? 

You do not need any triggers / rules to maintain it. There are open ends here - that's just a draft... Some issues: 

This answer assumes that you want to connect via TCP to localhost. (for local socket connection see Erwin's answer) Two options to ease your pain: 

I understand that you want to go with single database (as it is good from management & maintenance point of view), but maybe it's too much integration. I am assuming that: 

After bringing the slave database cluster into standalone read-write mode (either by internal logic inside or by restarting it without ) it cannot be easily reverted to be a slave. If you can, use file system level snapshots. This would be the easiest approach as you can revert the cluster to a known state without copying whole data directory. More or less that would be the logic for testing slave: 

(...and so on - every client fails) Comments and ideas What's interesting - even if the clients work in "new connection for each transaction" mode (add "-C" option to pgbench), same error occurs. Not always, and not for every client, but it does. I asked this question some time ago on mailing list: $URL$ - this SO post is just a copy. For more audience: Same thing applies to rotating a partition. So it might be interesting for all people who use partitioning. This is all on PostgreSQL 9.0, Linux. Solution (thanks to Chris Travers)