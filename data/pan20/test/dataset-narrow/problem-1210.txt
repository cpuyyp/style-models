It is known that many logic problems (e.g. satisfiability problems of several modal logics) are not decidable. There are also many undecidable problems in algorithm theory, e.g. in combinatorial optimization. But in practice heuristcs and approximate algorithms works well for practical algorithms. So one can expect that approximate algorithms for logic problems can be suitable as well. However the only reseach trend along these lines I have managed to find is the max-SAT problem and its development was active in nineties. Are there some other active research trends, workshops, keywords, good references for the use and development of approximate methods for modal logics, logic programming and so on? If automated reasoning is expected to gain prominence in the future applications of computer science then one will have to be able to go beyond undecidability constraints of logics and approximate methods or heuristics can be natural path to follow, isn't it so? 

I would expect that the empty relation corresponds to the always-zero probability distribution. Disjoint union corresponds to addition of probability distributions. However, standard union would be more complicated. I don't really know if there is some way to encode the standard union for probability distributions. (I have heard people say that probability distributions form a boolean algebra. So there must be some operation that serves as the union.) 

Unfortunately, there are too many things are going on here. So, it is easy to mix things up. The use of "full" in "full completeness" and "full abstraction" refer to completely different ideas of fullness. But, there is also some vague connection between them. So, this is going to be a complicated answer. Full completeness: "Sound and complete" is a property you want for a traditional logic to have with respect to its semantics. Soundness means that whatever you can prove in the logic is true in the semantic model. Completeness means that whatever is true in the semantic model is provable in the logic. We say that a logic is sound and complete for a particular semantic model. When we come to constructive logic, such as Martin-Lof type theory or linear logic, we care not only about whether formulas provable, but also what their proofs are. A provable formula may have many proofs and a constructive logic wants to keep them apart. So, a semantics for a constructive logic involves specifying not only whether a formula is true, but also some abstract semantic notion of "proof" ("evidence") for its truth. Abramsky and colleagues coined the term "full completeness" to mean that the proofs in the logic can express all the semantic proofs in the model. So, "full" refers to proofs here. A "complete" logic can prove everything it needs to. A "fully complete" logic has all the proofs that it needs to have. So "full completeness" means "constructive completeness" or "proof completeness". This has nothing to do with full abstraction. Full abstraction: "Adequate and fully abstract" is a property you want for the semantic model of a programming language. (Note the first difference: we are now dealing with the properties of the semantic model, not the properties of the language!) Adequacy means that, whenever two terms have the same meaning in the semantic model, they are observationally equivalent in the programming language (with respect to some notion of execution). Full abstraction means that, if two terms are observationally equivalent, they have the same meaning in the semantic model. These ideas can be related to soundness and completeness, but in a somewhat contrived way. If we think of the semantic model of a programming language as a "logic" or a "proof method" to talk about observational equivalence, then adequacy means that this proof method is sound; full abstraction means that this proof method is complete. There is no notion of "full completeness" here because nobody has yet produced a semantic model that represents a constructive proof method. (But, such a thing is theoretically possible, and one of these days somebody might do so.) In your case, you are interested in translations rather than semantic models. The properties of adequacy and full abstraction can be extended to deal with translations as follows. You think of the target language as your "semantic model", i.e., a formalism that you fully understand somehow. If so, you have some notion of equivalence for it. Then, we say that the translation is adequate if, whenever the translations of two source programs are equivalent in the target language, they are observationally equivalent in the source language. We say that it is fully abstract if, whenever two source programs are observationally equivalent in the source language, their translations are equivalent in the target language. In reality, I don't know of any target languages that we really fully "understand". All we know is some other notion of observational equivalence for the target language. In that case, the translation is adequate if the observational equivalence of the translations in the target language implies observational equivalence in the source language. $$ \tau(M) \cong \tau(N) \Longrightarrow M \cong N$$ The translation is fully abstract if the observational equivalence of the terms in the source language implies the observational equivalence of the translations in the target language. $$M \cong N \Longrightarrow \tau(M) \cong \tau(N)$$ Some authors take "fully abstract translation" to mean the combination of these two properties: $$M \cong N \iff \tau(M) \cong \tau(N)$$ Egger et al seem to be similarly extending the idea of full completeness to translations. In their set-up, formulas are types and proofs are terms. Their translation translates types as well as terms. They call their translation fully complete if the translation of a type $A$ has only those terms that are obtained by translating the original terms of type $A$. $$\forall N : \tau(A).\; \exists M : A.\, \tau(M) = N$$ Now for the vague connection between full completeness and full abstraction. Proving that a semantic model or a translation is fully abstract often involves some of definability. This is because our languages are generally higher-order. So, if the semantic model or the target language has too many "contexts" then it will be able to poke our terms or semantic meanings in undesirable ways and spoil their equivalence. "Undesirable ways" means in ways that the programming language itself cannot poke them. So, to get full abstraction, we need to ensure that the "contexts" available in the semantic model or the target language do come from those in the source language in some form. Note that this relates to the full completeness property. Why do we want such properties? It has nothing to do with compilers! We want these properties in order to claim that the source language embeds into the target language. If we are happy with a particular target language (as being clean, understandable, somehow fundamental or God-given) then, if the source language embeds into it, then we can claim that there is nothing new in the source language. It is just a fragment of the target language that we know and love. It is just syntactic sugar. So, fully abstract translations are given by people to establish that particular target languages are great. They are also sometimes given by people who have a big or complicated language to deal with. So, instead of defining a semantics for it directly, they translate it to some core language and then give semantics to the core language. For instance, the Haskell report does this. But the full abstraction of these translations is rarely ever proved because the source languages are big and complicated. People take it on faith that the translation is good. Once again, this has nothing to do with compilers. Compilers are rarely ever adequate or fully abstract. And, they don't need to be! All that a compiler needs to do is to preserve the execution behavior of complete programs. The target language of a compiler is generally huge, which means that it has lots of contexts that can mess up equivalence. So, equivalent programs in the source language are almost never contextually equivalent when compiled. 

I am trying to translate code from one programming language into another (to be specific - from RuleML to Drools, but other pairs can be expected as well) and it would be nice to know - whether there exists more general workbenches for this. E.g. one can formally specify programming language, that is clear. But are there any tools that can be used for capturing the formal semantics for formally specified language. It is obvious that the operational semantics is the most suitable for industrial programming languages. And - if semantics is specified then the translation from one programming language into other can be done at the meta-level. I guess, that this maybe too hard for C-to-Java translation, but it should work for more modest, recent languages that are created by academia. I have heard about Maude, but I guess that there can be something more. My experience is that Google can not replace bibliographies or suggestions from community. Thanks. 

[Hopefully, this is my last answer to this question!] Ohad's original question was about how denotational semantics differs from structural operational semantics. He thought that both of them were compositional. Actually, that is untrue. Structural operational semantics is given as sequences of steps. Each step is expressed compositionally (and it is remarkable of Plotkin to make the discovery that this is possible!), but the entire behaviour is not defined compositionally. Here is what Plotkin says in his introduction to the SOS article [emphasis added]: 

A lot of the senior Computer Scientists in Britain have had industrial experience before they came to work in academics. Christopher Strachey, the founder of denotational semantics, was a consultant programmer before entering academics. Tony Hoare, the founder of axiomatic semantics, worked in industry (Eliott Computers) for several years. Samson Abramsky, who holds the Christopher Strachey Chair at Oxford, in fact developed his interest in Computer Science during his work in industry (GEC). Cliff Jones, a Fellow of RAEng, worked in IBM, Vienna for several years before coming to do his PhD, and did another stint at a start-up company called Harlequin even afterwards. I have to say that all of them probably did innovative R&D type of work while in industry, which might be necessary to keep your mind active in thinking about research directions. 

There is known connection between classical and modal logics and type theory (lambda calculus), but are there connections between nonmonotonic logics (e.g. defeasible logic) and type theory (lambda calculus)? Maybe HoTT provides some generatlization where such connections can be found? If such connections can not be established then widely available proof assistants (Isabelle, Coq) are not useful for nonmonotonic logics, aren't they? 

Hoare logic can be used for proving program correctness (e.g. for deriving correctness statements for the whole program from the statements for the individual commands or constructions; good summary is $URL$ The question is - is there any line of research where Hoare-style reasoning can be made for programs that creates and deletes objects. One thought can be that this reasoning can be simulated by defining large pool of objects which have (for the purposes of correctness analysis) additional state attribute with values from the enumeration {not-created, created, destroyed}. But I guess that better approach should exist. Are there any references or keywords for furher search into this matter. 

The published version of the paper ("Lambda-definability in the full type hierarchy" in To H. B. Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism) has this remark. It also has a remark that could be construed as an explanation of the term "logical relation": 

Is the "right" proof theoretic view of resolution just that it is a fragment of the sequent calculus that suffices for checking satisfiability of formulae in CNF? Indeed! 

The fact that each step is expressed compositionally doesn't mean that the entire behaviour is expressed compositionally. There is a nice article by Carl Gunter called Forms of Semantic Specification, where the different methods of specifying semantics are compared and contrasted. Much of this material is also reproduced in the first chapter of his "Semantics of Programming Languages" text. This should hopefully clarify the picture. Another word about "operational semantics". In the early days, the term "operational" was used to refer to any semantic definition that referred to detailed evaluation steps. Both denotational semanticists and axiomatic proponents looked down upon "operational" semantics, regarding it as being low-level and machine-oriented. I think this was really based on their belief that higher level descriptions were possible. These beliefs were shattered as soon as they considered concurrency. de Bakker and Zucker's Processes and denotational semantics of concurrency has these interesting passages: 

I am considering one optimization problem who is known to be NP hard in the general setting. But there is application of this problem on the cylces of graph. This problem involves several sets and in this setting each set is the set of nodes of cycle in connected unidrected graph. And all cycles of connected unidrected graphs are the all sets for this problem. This is quite specific setting for the problem and I wonder whether there can be improvements in this specific setting. I would like to work out all the details myself but the question is - is there the general theory of all cycles in grpahs (connected undirected). Like - what is the number of cycles, what is the minimum and maximum lenght of them, how much common nodes they can have and so on? Mybe there are connections with group theory - e.g. cycle could be some kind of orbit for a group element (in rude language). Any such information provides the constraint on the initial problem and therefore - the complexity improvements can be possible to achieve in this specific setting. Google gives a lot about Hamiltonian and similar specific cycles. My question is about all possible cycles in graph. Any references could be helpful. Any names for the problems and keywords in this are could be appreciated as well. Thanks. 

In typical object-oriented programming languages like Java, classes are used as types. On the other hand, type-theoretic approaches to object-oriented languages treat interfaces as types. Are there any theoretical frameworks that formalize the use of classes as types? 

I would divide the books on programming language semantics into two classes: those that focus on modelling programming language concepts and those that focus on the foundational aspects of semantics. There is no reason a book can't do both. But, usually, there is only so much you can put into a book, and the authors also have their own predispositions about what is important. Winskel's book, already mentioned, does a bit of both the aspects. And, it is a good beginner's book. An equally good, perhaps even better, book is the one I started with: Gordon's Denotational description of programming languages. This was my first book on semantics, which I read soon after I finished my undergraduate work. I have to say it gave me a firm grounding in semantics and I never had to wonder how denotational semantics differs from operational semantics or axiomatic semantics etc. This book will remain my all-time favourite on denotational semantics. Other books that focus on modeling aspects rather than foundational aspects are the following: 

There are lot of applications of Horn clauses (notable examples include use of rules in cognitive architectures and knowledge bases, as well as use of rules in business rules programs). Are there formal methods that can help specify and verify Horn clauses. Is there semantics of Horn clauses. One can perceive Horn clauses as something similar to programming code of traditional programming languages and therefore one can expect the denotation, operational and similar semantics as well for Horn clauses. 

Recently such themes as semantic web, modal logics, business rules have seen increased interest as research topics in computer science (alhough many of then have more than 80 years of history), but are there any open source projects (even better if they are actively used in commercial applications as well) that use these technologies and where one can participate and get new challenges for research? I have found a lot of research projects (e.g. satisfiability solvers, logic programming environments) but at present I don't know many succesfull applied applications. There have been few reports on some of them in the jornal of "Theory and Practice of Logic Programming" - mostly about solving scheduling problems with logic programming methods. But maybe there is some open source projects as well? Sometimes it seems to me that modal logics (semantic web is part of them) and logic programming gives false impression about their power and their generality has few applications, e.g. the concrete algorithms of integer programming or scheduling are more powerfull and ready for applications than general methods suggested by the former ones. 

In addition to Andreas's excellent answer, please keep in mind that type classes are meant to streamline overloading, which affects the global name space. There is no overloading in Haskell other than what you can obtain via type classes. In contrast, when you use object interfaces, only those functions that are declared to take arguments of that interface will need to worry about the function names in that interface. So, interfaces provide local name spaces. For example, you had in an object interface called "Functor". It would be perfectly ok to have another in another interface, say "Structor". Each object (or class) can pick and choose which interface it wants to implement. In contrast, in Haskell, you can have only one within a particular context. You cannot import both Functor and Structor type classes into the same context. Object interfaces are more similar to Standard ML signatures than to type classes. 

The answer to such question can have practical applications as well - e.g. it can help to define some kind of formal semantics of the "business rules" (declarative programming paradigm that is used more and more in business applications, e.g. IBM ILOG, JBoss Drools, Oracle Business Rules, PHP and Python also have their own rules engined) and going further - this can be useful for cognitive robotics as well. 

I am acquinted with the basics of such notions as logic programming, monotonic and non-monotonic reasoning, modal logic (especially dynamic logic) and now I am wondering - does logic programming provides anything new to any logic? As far as I understand, then (at least in dynamic logic) the logic programming refers to the formalization of state transitions (by actions, i.e. - logic programing can be understood just as logic about actions). But the same notion "logic programing" seems to be in use even in domains, where there is not state transition, just exploration of one state (or set of states - in case then the initial set of premises are vague enough to describe more than one state). It seems to me that some authors simply use the notions "logic programming" for describing the procedure how to evaluate the query (I am reading currently about defeasible logic programming). But such procedure (although practical indeed) does not add anything new to the underlying logic. E.g. there is notion of "rational closure" (e.g. used for adaptive logics; just the consequence set for some set of premises) which should contain all the possible knowledge about state and therefore all the possible results of "logic programing" (if it is indeed perceived just as state exploration, derivation). So - the question is - does logic programming provides anything new to the logic and does every logic (to be completely understood and readied for applications) need to have its own logic programming? Maybe I am just missing the point... Just for reference I find the following works interesting about this subject, if there are more along this line, then it would be great to hear!