Is my MULTI SVHN model too small for this problem/dataset? If not, how do I improve its performance? Is there a way to generate better candidate sliding windows, as my single digit SVHN seems to be the best model I have among the three. Is my reasoning correct that the MNIST model didn't work well on my images as the digits are somewhat different (handwritten vs computer generated) ? Are there other preprocessing techniques that I could use to improve my performance espeically for images like the 2nd one (18458882)? Is there any other approach that I could try to solve this? 

You can test with higher values of Dropout (0.5,0.7,0.9) and/or try L1/L2 Regularization to combat overfitting : Keras Regularizers. Update: You can play with a combination of l1/l2 regularization and dropout for your convolutional and FC layers. Start with low values of lambda (0.001) and increase it thereon. A common practice is to have dropout only in the FC layers, see if it helps your problem. Also, from the looks of your loss curve, your model probably hasn't converged.You can train it until your validation loss starts going up/becomes stable. 

Its possible that because this is a "kohonen" class object that there are functions that get these data matrices for you. You'll need to read the documentation to figure this out. What I've shown above is digging in the structure to find the data you want. 

that is, a file called in the current working directory at the time you ran . You can get the current working directory with in R. So for example if its something you can read with , you can do: 

Column names are case sensitive, so you have to use lower-case 'group' in your FOREACH. 'GROUP' in upper case is the grouping operator. You can't mix them. So don't do that. 

"Because its there". The data has a seasonal pattern. So you model it. The data has a trend. So you model it. Maybe the data is correlated with the number of sunspots. So you model that. Eventually you hope to get nothing left to model than uncorrelated random noise. But I think you've screwed up your STL computation here. Your residuals are clearly not serially uncorrelated. I rather suspect you've not told the function that your "seasonality" is a 24-hour cycle rather than an annual one. But hey you haven't given us any code or data so we don't really have a clue what you've done, do we? What do you think "seasonality" even means here? Do you have any idea? Your data seems the have three peaks every 24 hours. Really? Is this 'gas'='gasoline'='petrol' or gas in some heating/electric generating system? Either way if you know a priori there's an 8 hour cycle, or an 8 hour cycle on top of a 24 hour cycle on top of what looks like a very high frequency one or two hour cycle you put that in your model. Actually you don't even say what your x-axis is so maybe its days and then I'd fit a daily cycle, a weekly cycle, and then an annual cycle. But given how it all changes at time=85 or so I'd not expect a model to do well on both sides of that. With statistics (which is what this is, sorry to disappoint you but you're not a data scientist yet) you don't just robotically go "And.. Now.. I.. Fit.. An... S TL model....". You look at your data, try and get some understanding, then propose a model, fit it, test it, and use the parameters it make inferences about the data. Fitting cyclic seasonal patterns is part of that. 

The images are not of a fixed resolution but are mostly in the range (80*20 to 130 *40). Due to lack of enough labelled data(~3K rows), I had to go for an open source digit dataset. I tried both MNIST and SVHN with not much luck. I've detailed both my approaches below.. Note : The only preprocessing that I've used is converting to grayscale ,resizing the images to a fixed size and doing a mean subtraction on the image. 1. SINGLE DIGIT MNIST and SLIDING WINDOW FOR TESTING : I trained a Convnet on the MNIST set with ~99% on the validation set.Then I used a sliding window to move over my test image and predict for each. This approach had two problems : First, as my images are not of a fixed resolution often I couldn't get the right sliding window (one which contains just one digit completely). Second, I cropped out individual digits from my test images, and tested my MNIST model on these, the results were 'wildly off', out of the ~15 samples that I had cropped out, my MNIST model couldn't get even one right. Here's a cropped out image for reference. 

So I just wanna confirm that both methods are valid and then which one is better in random forest application? 

I trained a model with results as below. It is a stacking model with base learners of random forest and gradient boosting. The mega model is a GLM. The dataset is imbalanced in the target class as shown in the confusion matrix on the right top. The target class is a default status of a single loan (Positive: default; Negative: non-default). The AUC ROC score is quite high but the f1 score is still only 0.53. My concern is both of recall and precision were approximately 0.5, meaning that the model can only distinguish half of the bad cases and only half of bad cases it diagnosed were truly bad. If I adjust the probabilistic threshold, recall will increase but precision will also be sacrificed to some extent. Due to the highly imbalanced situation on the positive cases, a low precision may lead to a large proportion of False Negative among the predicted positive cases. e.g. under a lower precision, if a model tells 10 bad cases, there may be actually only 2 truly bad ones and more fake bad ones predicted which is not wanted in practice as well. In predictive task on an imbalanced dataset, does f1 score matter and if so how can I further improve the score? (add new (composite) features, cost sensitive methods?) 

Using , you'll see numbers left of the bar go up by one, so now you can get exact reconstruction of your input, since you input integers: 

The Normal distribution is the same as the Gaussian distribution. Its just two names for the same thing. Whatever you do - fit parameters, compute goodness-of-fit, etc - if the documentation says its for a Normal distribution then you can say "Gaussian" instead. Completely and totally identical. 

I don't know what your is, so I'll assume you've attached and its the column from there. PLEASE edit your question and show ALL your working so anyone else can get your results. Let's proceed: 

No. Unless you have some other idea of the scale and spread and distribution of the feature values. You can construct data sets that have any given mean and median with no outliers or with one massive outlier. For example, f2 looks like a well-balanced set of numbers with a very close mean and median, maybe all those values are from a N(0.184,3) distribution. Scale that up by a linear transformation to an N(X,Y) and you'll get a mean and median much like f6. Exercise: find X and Y. 

I fit a dataset with a binary target class by the random forest. In python, I can do it either by randomforestclassifier or randomforestregressor. I can get the classification directly from randomforestclassifier or I could run randomforestregressor first and get back a set of estimated probabilities. Then I can find a cutoff value to derive the predicted classes out of the set of probabilities. Both methods can achieve the same goal (i.e. predict the classes for the test data). Also I can observe that 

I realised the Python function below cannot be directly used for this purpose and please advice some codes for this task. 

A concept class C is a set of true functions f. Hypothesis class H is the set of candidates to formulate as the final output of a learning algorithm to well approximate the true function f. Hypothesis class H is chosen before seeing the data (training process). C and H can be either same or not and we can treat them independently. 

Its "Precision at 1", or how often the highest ranked document is relevant: $URL$ Suppose you are looking for items about monkeys. Your query engine queries documents for "monkeys" and ranks by relevance. If the highest ranked document is indeed about monkeys, then that's a win for your query algorithm. But if the highest ranked document is ranked 1 because it has the text "Enough of your monkey business" then its a loss, because that's not really about monkeys. Repeat over a bunch of search terms. The Precision-at-one is then the number of wins over the total number of search terms tried. 

You want to find the attribute of the tag in tags. An XPath something like this will do that: See example here: $URL$ and click "Test" to see results set. Learn up on XPath notation for more. 

I would look at the package for this, which can read in raw binary data and present it as NxM grids. It can even extract subsets of large binary grids without having to read in the whole file (the R raster object itself is just a proxy to the data, not the data itself).