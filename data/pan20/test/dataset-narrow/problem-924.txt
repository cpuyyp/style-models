216.239.32.21 is owned by google, so it looks like they are emitting the redirect to www.vekslers.org. When they say they do not handle "naked" domains I think they mean you cannot serve your site from a naked domain. Instead you have to let them redirect the naked address to the non-naked one as they are doing above. 

The best thing you can do is have good runbooks. "If A happens, do B" Every alert from monitoring should have a procedure, even if that procedure is "Call Bob at: " Bob should know that if he wants the calls to stop he needs to create procedures or tools. For NOC operations it is often helpful to "apprentice" new team members, have them work the desk with an experienced person for 2-3 weeks. Make sure everyone on the team is aware that if the first line support people escalate, even incorrectly they should assist them. Be willing to harshly punish any 2nd tier support people who are unresponsive or mistreat the NOC personally when they escalate. (This is a major problem, the second tier person is going to resent any intrusion in their off time, but if they are not responsive the NOC will become "gun shy" and will stop escalating, or will avoid escalating to that individual.) 

If the address being mailed is a distribution list the default is to reject unauthenticated requests. (Don't allow external users to email distribution lists). If you want to allow this you'll need to uncheck the "require authentication" checkbox in the distribution list under "Mail Restrictions" 

This is generally because cron does not enable the PATH the same way as the shell does. Try typing "echo $PATH" at a prompt. Take the resulting PATH string and add it to the top of crontab file as PATH= That will probably fix it. Mark 

The definitive guide for online credit card processing is the Payment Card Industry PCIDSS standard: $URL$ All of the things on your list (and much, much more) are in that document. Doing it right is a lot of work, are you sure there isn't a already written app/service that will meet the client's needs? 

mysqldump does not take consistent snapshots across databases. It also locks the database while dumping. ie: When the dump is running your database is usually read only to clients. If this is not acceptable LVM snapshots are the next step. The database(s) need to be locked and flushed while the LVM snapshot is being taken. That is generally faster, only a few seconds. Generally mysqldump is an easy way to do backups, and it works well for smaller databases and less critical systems. Larger or more critical systems use LVM snapshots or commercial backups systems. 

If you just want reliable file sharing for a few machines with limited admin support I would consider one of the inexpensive network NAS devices from a major vendor. The WD Sharespace devices $URL$ from Western Digital have worked quite well for me. 

I've never seen the aspmx4 and aspmx5 entries before (but google does have then in DNS so they are probably good.) It might be that you have aspmx2-5 all with the same MX priority of 30. Google's generic instructions are: 

In the simplest setup, enable IP Forwarding on the redhat machine, configure both ethernet adapters to have IPs on their respective networks. Set the client machines default gateways to point to whichever redhat IP is on the same subnet. Now if you want this to work with external routers, you'll need to setup some static routes. That can get complicated fast. 

Do you need the postfix server to be responsible for example.com? (Do you ever want mail to be delivered there instead of to google?) If you always want mail for local example users to go to their google, remove $mydomain from the mydestination parameter in main.cf. This will not alter where the mails appear to be coming from for outside users. (That is controlled by "myorigin") Since postfix either thinks it is responsible for the domain or not, there is no easy way to split the domain up so some mail gets delivered locally and some gets delivered to google. You can do some tricks by putting in forwarding rules or with transport mapping, but then you'd need to specify google or local for each user. I kind of doubt you really want to do that. 

If you are just logging is there any reason you need to be able to run SQL queries on the data set? For logging flat files are often a lot easier to work with. Could you log to flat files, or extract the data periodically from the database? Or along the same lines, run a SQL script periodically that splits the data into tables by month. (This will require re-working your query scripts, but not your insert.) MySQL will handle tables that large, but you are in somewhat uncharted space. Appending records and keeping the number of indexes to a minimum will help. MyISAM may be slightly better than some of the alternatives if you can accept its limitations. You could also run two database instances (preferably on separate servers). Use replication to stream the log data to the second server and run your queries there. That will keep the queries on the log table from impacting your production server. 

In my research Amazon had a few more services that Rackspace/Slicehost did not (like SimpleDB.) Amazon is a little more expensive for the server instances, but Rackspace charges more for bandwidth. You might look at one of the company that provide an abstraction layer to the services so you can use either or both. I've heard Rightscale is quite nice, though I have not tried it myself. 

I did this with qpsmtpd... it is a qmail SMTP frontend replacement written in Perl. It has a bunch of plugins you can experiment with, but the most helpful is a maildir delivery plugin that can send all of the mail into a directory, or subdirectories by recipient address. Just install qpsmptd and run as a daemon, no other mail application is required if you just want the mails to get dumped into a directory. $URL$ 

If the DNS server and the hosts are on the same subnet (192.168.1.0/24) it is extremely unlikely that the packets traverse the machine that you are setting IPtables rules on. Devices on the same subnet normally send packets directly without the use of a router. You will need to either set the IPtables output rule on all devices, or you will need to restructure the network by placing either the clients or the DNS server) on separate subnets with the iptables machine in between. Once you get the packets crossing the iptables machine this rule will work: 

Technically, yes. Though a local DHCP server is generally a better choice. DHCP is a pretty "easy" thing, just setting up a old desktop with Linux on it at each site works well, and will be less failure prone that centralizing everything. If you want to do it, the best way is to connect the sites together via VPN. Either back to a master site, or a node on EC2. Then run a DHCP server on the master site and configure the routers or switches at the other sites to do DHCP forwarding. If the VPN links or the master site goes down, none of the other sites will be able to pull new addresses. Think carefully about the implications before doing so. 

I made it work with Kickstart. If you create a kickstart config file you can exclude base from the packages definition and get a really minimal install. I think it was so minimal it didn't even have yum and a few others, and I had to add those packages back in. 

The advice in the comments is completely correct. You really do need to restore from backup, but if the hacker's code looks EXACTLY like your example above you can try this (make a backup copy of your webroot first.) 

I think Pingdom $URL$ offers a DNS testing service, in addition to their standard Website testing service. I used their web testing service for awhile. They have a nice one time check tool at: $URL$ 

I've never done it, but you can have two DHCP servers that both make offers to a client. It is up to the client to pick one of the offers I think the trick is that you need to add the "authoritative" command to the top of the DHCP configuration on the Clonezilla machine but not on the PFSense box. That way if Clonezilla offers the client a DHCP address the client will accept that one first, otherwise it will fail back to the PFSense offer. YMMV The article makes mention of this issue near the bottom: $URL$ 

It could be a mis-configured firewall interfering with TCP windows. This is pretty rare these days but leads to similar behavior. You don't mention what OS your coworkers are using. Linux is more sensitive to broken firewalls than other OSes in my experience. I can't remember which options to disable, but try these as root: 

Also make sure CouchDB is listening on the VM's public IP address. By default CouchDB is bound to localhost only. 

We have a network of HP 2510G switches connected back to HP2912al for aggregation. We've noticed that long running connections like a MySQL DB dump start flooding out to all network ports once the mac-cache-timeout expires. Doing an "arping" against the destination IP stops the flooding (going back to port to port) until the cache timeout expires again. I can understand why this would happen for unidirectional UDP traffic, but I'm at a loss as to why it is happening for TCP. I would think the ACKs from the receiving machine would cause the Procurves to refresh the MAC address in their cache. Instead it seems like they only learn from ARPs. Any ideas? 

Harddrive errors tend to be caught by the kernel. Does your server have ECC RAM (it should)... without it memory errors can be missed. Same with any cache RAM on RAID adapters and the like. Pull the DIMMs, clean the contacts and try again, or try running Memtest. Checking for SMART errors on the drives can be helpful. Drives can fail without SMART errors, but usually marginal drives will have them. "smartctl -a /dev/sd[x]" or smartctl --test=long /dev/sd[x] should give some more information. 

Check the logs for hints. Do a "netstat -lvtn" on the mail server as root, see if the postfix process is attached to port 25. From the local server: "telnet localhost 25" Verify that you get a 250 reply after a few seconds. From a remote server elsewhere on the Internet and/or one on the same network: "telnet myserver.com 25" look for identical results If either step fails something is blocking the port, or postfix is failing. 

Most facilities I've seen just put the switches in the rear, and assume that they will find enough cool air to operate. Many switches are designed to operate in extended temps (they are often deployed in unconditioned wiring closets.) You may want to check with your vendor: Third party Vendors are addressing the switch issue for contained aisles. This product features plenums that provide cool air to the sides, and let hot air exit from the back of certain Cisco switches whether they are front or back mounted: $URL$ 

Generally there are two triggers for load balancing. When the site grows beyond the capabilities of one server or when you want eh redundancy and reliability that may come from having two servers in case one may fail. Also, sometimes it can be cheaper to buy/rent two or more low end servers and load balance, instead of one large server. 30k hits a day is less than one request per second, so unless your traffic has a significant peak, or you need the redundancy, it is probably pretty early to be considering it. Mark 

50,000 files should not be enough to cause a significant speed issue on Linux. You mention caching the listing, so I'm thinking you are doing some kinda of processing on the files instead of plain serving. I would look for issues on how you process the files. 

You can work around this by using openvpn and ethernet bridging to create a virtual layer two segment shared by all of the servers. 

Many Linux distros run some daily cron jobs early in the morning. Indexing the harddrive for slocate and log rotation will both produce intensive disk activity for a few minutes depending on the size of your harddisk. I'm not sure if Ubuntu is configured the same way, but Redhat based distros usually have an /etc/cron.daily file or directory. Read up on cron and how it works and look at the files in that directory. In my experience most hacked systems are used for their network connection, therefore intensive disk activity is not a great indicator of malicious activity. 

Could you run a "bookmarks" webpage somewhere that has a clickable link with the full Hostname/Port in a anchor tag ie: to make it easier for users? This only works if the browser or the OS shell has a handler for that URI protocol. (There are instructions online on how to create protocol handlers in Windows Shell) Mark 

Google's antispam takes into account many more variables than just SPF records. You need to check with them to understand why your mail is being blocked. Start by reading their guidelines for bulk senders: $URL$ Then if you are still unsure contact their postmaster. Make sure your domain has feedback loops setup with all the major destination mail providers as well, so you'll be notified of issues. 

Depending on your skill and hosting provider, you can generally setup host based firewall/port filtering (iptables on Linux, Windows Firewall, etc) for no charge. You would choose a hardware firewall for an additional measure of protection, or to mitigate certain types of attacks. Generally one is not required (I prefer hardware firewall for Windows, and generally stick to software firewalls and disabling services for Linux. YMMV.) If you go with a hardware firewall it should be matched to the bandwidth allocation and usage of the site. 10mb is reasonably large for a single server and a small to midsized site, depending on the site itself. 

Sudo is the best way, because you can limit users to only being able to run a few commands Giving a user rights to run everything is easy, add a line like this to /etc/sudoers: fred ALL=(ALL) ALL You can also limit fred to only certain stuff: fred ALL = /usr/local/restart_www 

That might fix it until the next reboot. If it works you can add it to /etc/sysctl.conf to make it permanent, but it will lessen performance when sending big files. 

Very unlikely. The modules are usually proprietary, even though many vendors just rebrand Avocent products. 

SSH tunnels are a pretty good "quick" solution for getting data thru firewalls but they are designed for interactive stuff like X-forwarding. They don't work well for long term, or bulk transfers. You should probably look at setting up a permanent VPN, if that is within the possible. If not, I would look a reducing the amount of work done over the tunnel, and making it more batch oriented. (grab a little data every hour, and drop the connection between runs) You might also experiment with ClientAlive and ServerAlive settings. This will cause the system to ping over the encrypted channel periodically. This will often keep firewalls from disconnected the idle TCP connection. 

You could setup modsecurity and add a rule to log the POST data whenever it sees the misspelled version. If it is someone editing it, or a SQL injection it should get logged. 

It is very likely that either your local firewall is blocking port 25, or your Internet provider is doing so. 

One solution is to use ksplice. If you use Ubuntu or CentOS kernels you can subscribe to the ksplice.com service, where for a small fee they will provide you with special kernel images that can be used to patch a running kernel. Reboots are not required for most updates. Pretty easy to use and setup. If you are particularly skilled you can use the ksplice patches to build your own enabled kernels without subscribing to the service, or for non-standard kernels. 

Sounds like you need to enable port mirroring on the HyperV virtual switch, that way machine two will receive all of the traffic from machine 1. This article should help: $URL$ I don't have a way to test but it is rumored there is an undocumented way to make this work on 2008. Look at the VM XML configuration file of machine 2 to figure out the switch and port name then try setting this reg key to 1: 

Load average is based on the processes waiting in the run queue. That means if you have processes that use fractional time slices often you can see a high load average without a high CPU utilization. The best example of this is mail. The amount of CPU time require to send a message is very limited, but when thousands of pieces of mail are moving around the system (especially if the mail daemon forks processes to handle each one) the run queue gets very long. It is common to see well functioning, responsive mail servers with load averages of 25, 50 to over 100. For a web server I would use page response time as the primary metric, do not worry about load average. Under modern schedulers load average less than twice the number of cores will usually have no negative effects. You may want to experiment with number of cores per VM versus total number of VMs. Some applications will benefit from many cores on a few machines, others are better at a small number of cores and many instances.