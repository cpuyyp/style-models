Am I missing a key difference between those approaches or is it a mere occurrence of weak communication between the database and TCS communities? 

For more details, see the section on "Trans-dichotomous algorithms" from the wikipedia page for "Integer Sorting". 

The concept of "witnesses" or "checkable proofs" is not totally new: as mentioned in the comments, look for the concept of "certificate". Three examples came to mind, there are more (in the comments and elsewhere): 

Given the spanning tree structure of the solution of the all pair minimum path and the fact that the weights are always decreased, I think that this gives linear time per update. I think you could easily get the same result for directed weighted graphs, at the cost of some extra space and a more complicated structure. 

The total precomputation time is within $O(1+2+4+...+n)\subseteq O(n)$ Answer a query for the $k$ smallest elements in $A$ in time $O(k)$: 

Have a look at the proceedings of the conference "Fun with Algorithms": they should provide you with a good selection of "Fun" problems to work on, and a venue where to submit your results for feedback. Check the publications of people known to consider fun problems and problems for fun. For instance: 

It took a few years (five!), but here is a partial answer to the question: $URL$ Optimal Prefix Free Codes With Partial Sorting Jérémy Barbay (Submitted on 29 Jan 2016) We describe an algorithm computing an optimal prefix free code for n unsorted positive weights in time within O(n(1+lgα))⊆O(nlgn), where the alternation α∈[1..n−1] measures the amount of sorting required by the computation. This asymptotical complexity is within a constant factor of the optimal in the algebraic decision tree computational model, in the worst case over all instances of size n and alternation α. Such results refine the state of the art complexity of Θ(nlgn) in the worst case over instances of size n in the same computational model, a landmark in compression and coding since 1952, by the mere combination of van Leeuwen's algorithm to compute optimal prefix free codes from sorted weights (known since 1976), with Deferred Data Structures to partially sort a multiset depending on the queries on it (known since 1988). 

I found the result hidden in an obscure 4p technical report: I share my results here in case others are interested. 

A SIGMOD 2014 paper from Microsoft Research states that the "importance of sorting almost sorted data quickly has just emerged over the last decade", and goes on to propose variants of Patience sort and Linear Merge, and measure their performance on synthetic "close to sorted" data. It seems to me that this description matches the theme of "Adaptive Sorting", covering algorithms taking advantage of existing preorder in sequences to be sorted, which has been the topic of various publications (albeit in the community of theoretical computer science rather than databases) from as early as 1979: 

Have a look at my (modest) proposal of "fun" problem below. If you work on it, get in touch with me! 

I named this problem "Binary Sorted Min Sum" for lack of a better name. The computational complexity of this problem in the worst case over instances of size $n$ is clearly $2n$ evaluations, $n-1$ comparisons, and $n$ additions, all within $O(n)$. A linear number of instances of this problem occur in the core of my colleague's solution, which complexity he analyzes as $O(n^2)$. Inspired by the fact that many instances of this problem can be solved in sublinear time (e.g. when $f$ is increasing much faster than $g$ is decreasing, one can certify that the minimum is at index $1$ in one single comparison, 4 accesses and 2 sums), I got some interesting adaptive results, showing tight bounds within $O(\delta\lg(n/\delta)$ for some parameter $\delta\in[1..n]$ measuring the difficulty of the instance, implying both the $\Theta(n)$ bound in the worst case and the $\Theta(\lg n)$ bound in easy instances: it is actually quite similar (but not reducible, it seems) to the "Binary Sorted Intersection" problem which I studied previously. A quick search on "Binary Sorted Min Sum" did not yield anything meaningful, yet the problem is quite simple, so I am wondering if someone introduced it before under another name? 

I'm not sure if this is what you're looking for but there's a sizable literature on the 3-SAT phase transition. Monasson, Zecchina, Kirkpatrcik, Selman and Troyansky had a paper in nature that talks about the phase transition of random k-SAT. They used a parameterization of the ratio of clauses to variables. For random 3-SAT, they found numerically that the transition point is around 4.3. Above this point random 3-SAT instances are over constrained and almost surely unsatsifiable and below this point problems are under constrained and satisfiable (with high probability). Mertens, Mezard and Zecchina use cavity method procedures to estimate the phase transition point to a higher degree of accuracy. Far away from the critical point, "dumb" algorithms work well for satisfiable instances (walk sat, etc.). From what I understand, deterministic solver run times grow exponentially at or near the phase transition (see here for more of a discussion?). A close cousin of belief propagation, Braunstein, Mezard and Zecchina have introduced survey propagation that is reported to solve satisfiable 3-SAT instances in millions of variables, even extremely close to the phase transition. Mezard has a lecture here on spin glasses (the theory of which he has used in analysis of random NP-Complete phase transitions) and Maneva has a lecture here on survey propagation. From the other direction, it still looks like our best solvers take exponential amount of time to prove unsatisfiability. See here, here and here for proofs/discussion of the exponential nature of some common methods in proving unsatisfiability (Davis-Putnam procedures and resolution methods). One has to be very careful about claims of 'easiness' or 'hardness' for random NP-Complete problems. Having an NP-Complete problem display a phase transition gives no guarantee as to where the hard problems are or whether there even are any. For example, the Hamiltoniain Cycle problem on Erdos-Renyi random graphs is provably easy even at or near the critical transition point. The Number Partition Problem doesn't seem to have any algorithms that solve it well into the probability 1 or 0 range, let alone near the critical threshold. From what I understand, random 3-SAT problems have algorithms that work well for satisfiable instances nearly at or below the critical threshold (survey propagation, walk sat, etc.) but no efficient algorithms above the critical threshold to prove unsatisfiability. This is just state of the art right now and could of course change in the future. 

Perhaps I'm missing the motivation for your question but there are many examples of empirical results motivating research, algorithms and other results. MP3 use psychoacoustic to optimize the algorithm for human encoding. Plouffe gives an account of discovering the BBP spigot algorithm for the digits of $\pi$ where he recounts the use of whatever Integer Relation Algorithm Mathematica was using to discover the formula. Along the same line, Bailey and Borwein are big proponents of experimental mathematics. See "The Computer as Crucible: An Introduction to Experimental Mathematics", "Computational Excursions in Number Theory" amongst others. One might argue that this is more experimental Mathematics but I would argue that at this level the discussion the distinction is semantic. Phase transitions of NP-Complete problems are another area where empirical results are heavily used. See Monasson, Zecchina, Kirkpatrick, Selman and Troyansky and Gent and Walsh for starters, though there are many, many more (see here for a brief survey). Though not quite on the level of Theoretical Computer Science or Mathematics, there is a discussion here about how the unix utility grep's average case runtime beats optimized worst case algorithms because it relies on the fact that it's searching human readable text (grep does as bad or worst on files with random characters in them). Even Gauss used experimental evidence to give his hypothesis of the Prime Number Theorem. Data mining (Bellkor's solution to the Netflix Prize to make a better recommendation system) might be argued to be a theory completely based on empirical evidence. Artificial Intelligence (genetic algorithms, neural networks, etc.) relies heavily on experimentation. Cryptography is in a constant push and pull between code makers and code breakers. I've really only named a few and if you relax your definition of empirical, then you could cast an even wider net. My apologies for being so scattered in answering your question but I hope I've given at least a few examples that are helpful. 

Our work on soft verification of contracts is related, at OOPSLA 2012 and ICFP 2014, allows you to write contracts, which are a lot like ACSL specs, and then either statically verify them or use them a dynamic checks at runtime. 

The ACL2 system includes all of the features of your language, and supports impressive automated theorem proving about program in that language. In ACL2, you would write the theorem you describe as: 

The result by Tim Griffin that control operators such as are related to classical logic, extending the Curry-Howard correspondence. Basically, the typing of is such that if $E$ has type $\neg\neg \tau$, then $\mathtt{call/cc}{(E)}$ has type $\tau$. This works out when interpreting the type $\neg\tau$ as $\tau\rightarrow\bot$, as is standard in logic, and corresponds to the type of a function that takes a $\tau$ and never returns. That's exactly what a continuation does, making the correspondence work. His paper, "A Formulae-as-types notion of control", appears in POPL 1990. 

First, this entirely depends on what you take to be your set of contexts. If is a context, then contextual equivalence is syntactic equivalence. Traditionally, contexts for contextual equivalence are taken to be contexts in which "expressions", in whatever meaning that has in the language, can appear. This rules out contexts like , where the context places its argument inside a string literal. These kinds of contexts were also, IIRC, ruled out by Quine when he originally described referential transparency. From this perspective, I think is also not a context. Instead, the contexts are the places where expression evaluation could potentially happen, such as in the body of a function or in the argument of an application. Potentially problematically, this means that in a Lisp program with macros (or a Racket or Scheme program) you don't know what the contexts are until you run the potentially-nonterminating macro expansion process, because you don't even know where the expressions are. Whether you think this is a problem or not is mostly a philosophical question rather than a technical one. 

There has been a lot of work on this, usually under the heading of "memory models". See, for example: 

All you really need is the definition of the untyped $\lambda$-calculus, which you can find in numerous places. Everything else follows from that. 

Game theory played a significant role in solutions to the "full abstraction problem" in programming language semantics. In particular, the first fully-abstract semantics for Plotkin's PCF were given using games as models. The relevant citations are: Full Abstraction for PCF, by Samson Abramsky, Radha Jagadeesan, and Pasquale Malacaria and On Full Abstraction for PCF: I, II, and III, by J.M.E. Hyland and C.-H.L. Ong which both appeared in Information and Computation, Volume 163, Issue 2, 15 December 2000. 

This looks a lot like the I/O API described by Felleisen et al in A Functional I/O System (or Fun for Freshman Kids). Basically, you write (in the simpler, non-distributed setting), a series of event handlers, each of which accepts the current state, and returns an updated state. Finally, there's a handler, which produces the "output" for each state. If we recast this API slightly, we can package up the handlers and the current state together, and each time a handler returns both a new state and a new set of handlers. We might call this package of state and operations an "object". :) If we then make the result a pair of this object, and the "output", then we have exactly the type of resumptions. Interestingly, in the paper, Felleisen et al do exactly this when moving to the distributed setting -- every operation returns a pair of new state and "output" in the form of messages to be sent to the other participants in the system.