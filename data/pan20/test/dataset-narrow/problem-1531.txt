If you used only a single neuron and no hidden layer, this network would only be able to learn linear decision boundaries. To learn non-linear decision boundaries when classifying the output, multiple neurons are required. By learning different functions approximating the output dataset, the hidden layers are able to reduce the dimensionality of the data as well as identify mode complex representations of the input data. If they all learned the same weights, they would be redundant and not useful. The way they will learn different "weights" and hence different functions when fed the same data, is that when backpropagation is used to train the network, the errors represented by the output are different for each neuron. These errors are worked backwards to the hidden layer and then to the input layer to determine the most optimum value of weights that would minimize these errors. This is why when implementing backpropagation algorithm, one of the most important steps is to randomly initialize the weights before starting the learning. If this is not done, then you would observe a large no. of neurons learning the exact same weights and give sub-optimal results. 

For a categorical variable, if the fitted model encounters a previously "unseen" category, i.e. it did not exist in the training set when the model was trained; then, you should skip that record. You could also opt to throw an error so that you're notified of the existence of new categories and can re-train the model based on that trigger. If you skip the records with new categories, you would still be able to evaluate the pipeline successfully. That may be the preferred option for a fully automated production setup. The one-hot encoding creates a new "column" of data for a new category and if this hasn't been used for training, the ML algorithm has no way of knowing how to use the new dummy variable. In your sample code, each of the categories are encoded into new variables e.g. = 0/1, = 0/1 , = 0/1, etc. If such a model is sent data with a new category "d", then it would be encoded in another column called = 0/1 , but the model would ignore this column (or throw an error) since it doesn't expect its input matrix to contain . Lets assume you've fit a linear regression model with the coefficients as: $$ y = 1 + 2.is\_category\_a + 3.is\_category\_b + 4.is\_category\_c $$ Now, when you try to evaluate a record with new category = "d", then, the model is not able to use the new dummy variable since it doesn't have a coefficient for . Hence, so you should not "tolerate" new categories and process such records. 

This is an art more than a science and your ability to understand the domain as well as good communication will help you achieve this goal. 

CUDA core count Memory bandwidth (GB/s) Memory per core (MB) Raw Speed (MHz) Total Memory available (GB) Performance on 16-bit, 32-bit floating ops/sec 

Since the output you're trying to predict is validity of the observation, keep "validity" = True/False, or, 1/0 as the target variable. One of the parameters is a categorical variable "species", and I'm expecting this to have a high cardinality. Since there are approximately 8.7 million species on earth, if you used this variable in a model it could possibly expand into 8.7 million individual columns (in on-hot encoding form). Even a conservative estimate of 100,000 species makes it nonviable to be used as it is. So you need a way to convert this species information to a fewer features. One approach you could try out is to create geographical clusters for each species (using only valid marked records), then identify the nearest center and max/avg./quartile measures of distance from their cluster center for each species. Do this for each quarter of the year separately to account for seasonal changes. Next, add this information back to the main dataset to indicate for each record - all the geographical centers of that species cluster. In the next step, for each record find the nearest cluster center and calculate this particular observation's distance from its cluster center. Then calculate the ratio of its distance from cluster center vs. max distance and vs. avg distance from that cluster's center. Use this metric instead of the geospatial coordinates and species identifier. Another approach could be to add additional features such as the climate of each location and average historic temperature at that location during the time of the year when the observation was taken. This is because some animals may migrate north/south based on the seasons and so if a species' location was found valid in the summer, it may be impossible to find it in the same location in winter due to it being unable to survive the cold weather. If you combine this with #3 above, it would enrich the observations significantly. 

Avoid over-fitting the model to training data: to avoid this use regularization and keep cross checking accuracy with an independent held-out validation set. Use cross-validation (10-fold) and repeat several times to get good estimates of the model's performance metrics on new data. Since the data is highly class imbalanced (many valid records but few invalid records by proportion), use a performance metric other than simple true positive accuracy. Try using F1 score, precision (of identifying invalid records), Kappa metric, etc. Due to high class imbalance, it would help if you either over-sampled the minority class (invalid) or under-sampled the majority class (valid), or did both together. This will improve the model's ability to classify mode precisely. Adjust the hyper-parameters such as learning rate and hidden layers/no. of units for best model performance. 

Purpose of the multiple inputs: Each input represents a feature of the input dataset. Purpose of the hidden layer: Each neuron learns a different set of weights to represent different functions over the input data. Purpose of the output layer: Each neuron represents a given class of the output (label/predicted variable). 

As already highlighted by Jan van der Vegt, its extremely odd that changing the no of features from 8 to 40 has no impact on test set accuracy. 

The seasonal and trend components should give you insights into any predictability of this process. Sometimes domain-specific hypothesis could give good leads for next steps for the analysis. 

Additionally, I recommend adding additional features such as occupation, gender, etc. since the features listed in the table (city, etc.) are too ambiguous and do not give much information to differentiate among customers. EDITED as per suggestion in comments: When using the model, each lead would get classified as prefers_morning=yes/no, prefers_noon=yes/no and prefers_evening=yes/no. Based on the time of the day, for example in the morning, the call center agent (or software) could pick up and call leads classified in the morning preference set. When its noon, the call software picks up form the noon preferred list, and so on. 

Irrespective of whether the data is huge or not, cross validation is a must when building any model. If this takes more time than an end consumer is willing to wait, you may need to reset their expectations, or get faster hardware/software to build the model; but do not skip cross validation. Plotting learning curves and cross-validation are effective steps to help guide us so we recognize and correct mistakes earlier in the process. I've experienced instances when a simple train-test set does not reveal any problems until I run cross-fold validations and find a large variance in the performance of the algorithm on different folds. Before sizing up a dataset, eliminate the records with missing values of key variables and outliers, columns of highly correlated variables, and near zero variance variables. This will give you a much better estimate of the real usable dataset. Sometimes you may end up with only a fraction of the available dataset that can actually be used to build a model. When sizing up a dataset for building a model, it is easier to estimate the computing resources if you enumerate the dataset in rows and columns and memory size of the final numeric matrix. Since every machine learning algorithm is ultimately going to convert the dataset into a numeric matrix, enumerating the dataset size in terms of GBs/TBs of raw input data (which may be mostly strings/textual nominal variables/etc.) is often misleading and the dataset may appear to be more daunting and gigantic to work with than it is. Once you know (or estimate) the final usable size of your dataset, check if you have a suitable machine to be able to load that into memory and train the model. If your dataset size is smaller than memory available/usable by the software, then you need not worry about the size any longer. If the dataset size is larger than the memory available to train a model, then you could try these approaches (starting from the simplest ones first):