Because it's a lot of effort for very little gain. Ignoring for the moment the difficulty of creating a nice, atheistically-pleasing collection of glyphs in the first place, looking at your list of "pros:" 

You can't do it with just that information. You need more. For example, I assume you also actually have a the world, view, and projection transformation matrices available to you. In that case, you can compute the vertex's position in view space by multiplying it by the world matrix and then by the view matrix. Since you're in view space, the eye is at (0, 0, 0) you can easily know the vector between that and the transformed, light-space vertex. You can do the same if you have the light position. Bring the light position in to view space, then you can easily determine the vector between that position and the eye (which is, again fixed at the origin). 

No. Not easily. You can get the renderer from the window, but it doesn't seem like you can go the other way around. This is interesting since if you examine the (internal) definition of the structure in you can see that it does contain an member referring to the window (named ) several bytes in (after a bunch of function pointers). However, the definition of is not provided to you in the public headers, so you can't read this member and there does not appear to be an API for exposing it to you. If you were particularly desperate you could copy-paste the definition from the source code to your own source code, cast the you have to it, and read the window that way... but that's probably a bad idea as it's quite brittle. A more practical solution is likely to use or to maintain your own association between SDL renderers and the window you use to create the renderer. 

The ID3DXSprite interface will modify the render states of the device when you call the method. The specifics of the state modifications that will be performed are detailed in the method's documentation. In particular, note that the source and destination blend modes are set to 'source alpha' and 'inverse source alpha,' respectively. This enables the typical blending behavior wherein the alpha component of the color is used to control how transparent a pixel is. Since this isn't what you want, you should pass the flag to and ensure, as the documentation notes, that 

Minimizing the responsibilities of an interface. In an ideal world, an interface would have but one task. Aggregation of functionality rather than inheritance. Instead of having the player be an agent, the player contains a reference to an agent it controls. This tends to also help you adhere to the idea that an inherited interface should be substitutable with its base interfaces, because a player probably does a lot more things, and a lot differently, than any other actor in the game. Logical lifetime separation. As you note, the player should persist outside the lifetime of the actual level, so while it would be okay for a player to refer to a "current level" much like the player could refer to the "currently controlled agent," you don't want the opposite where the player is potentially destroyed when the level is, thus requiring a complicated process of serializing and deserializing transferrable state between levels. 

Normalizing is, indeed, relatively simple. It's also an operation that is performed often and consequently should be very fast. Performance considerations are especially important when designing math library middleware like D3DX. The reason there are two parameters is for efficiency, and to some extent robustness. By providing by-reference (via pointer) output and input parameters, the D3DX normalize function can normalize a vector in place or to a new vector without unnecessary by-value copying of the vector object. By way of contrast, consider the alternatives: If the normalization function took only a single input vector, it would have only two reasonable options to get the result back to the caller: 

The code you posted first compiles the shaders, then binds them to the pipeline where they remain active for any subsequent draw calls until changed. It doesn't actually cause them to be executed. 

Just use Java, don't bother with anything else. If it becomes necessary to start augmenting the program with C++ bolt-ons, you can do so later once you've proven it to be to true. Until then, you'll be able to build a perfectly excellent game in Java that meets all your requirements and has the advantage of being significantly easier to build and deploy since you won't have to deal with the hassle that is C++'s lack of binary portability. If you're into it, it looks like it's also possible to use a developer preview version of Flash to get the hardware accelerated features bummzack mentions. 

It's hard to say exactly what the school (or any school, for that matter) will look for in a portfolio. Some places might want to see only the best work you have to offer, some might want to see a range of your work to show how you have grown and developed on your own. Some may not care if you demonstrate a very narrow focus in terms of media or subject matter, other may consider that a negative and prefer that you show off a broad range of styles, approaches, and subjects. In the end, you have the most control over making yourself happy and satisfied with your portfolio. Focus on that more than what you think other people want to see in you; that alone will make you better, and should help open the doors you want as well. 

The latter two options address the problem purely from a volume perspective, however -- they don't account for the fact that even if you play 500 background sounds quietly enough not to overwhelm a player's eardrums, you are still playing 500 sounds, which can be noisy not in the volume sense but in the confusing sense. To handle that problem, I think you are best off trying to reduce the total number of ambient emitters or taking only the top prioritized sounds (for reference, taking the top is more-or-less what we settled on for Guild Wars 2). 

I think you're overestimating the complexity of loading data from text or XML files. Most of the time you should use a library to do this, which makes the process of parsing the XML/JSON/whatever much easier. Yes, there is a bit of up-front cost to writing a level loading system that is data-driven but it will generally save you a lot of time in the long run versus tons of unique classes to represent each level. Smaller or simpler games may not have as much to gain from the longer-term investment of a data-driven approach, so unless the developers are using existing frameworks/engines that support it it may be more efficient for them to simply encode the data in the game code. There are of course a host of reasons why any given game may choose to put their data in external files (or not); it's not feasible to list them all. At some level they will usually all boil down to the additional iteration speed or flexibility that not-having-to-rebuild-the-game can provide. 

It really depends on the geometry that your world consists of and how you set your orthographic projection and camera up relative to that world. Most of the time, when people make 2D games with orthographic projections, they don't bother actually creating 3D geometry. Usually everything is a quad with a texture applied, or simple 3D geometry without much actual "depth" (since the projection renders that depth relatively useless). Thus, switching to a 3D view with perspective would result in a flat or oddly-spaced world with a lot of Paper Mario-style thin objects. Similarly, orthographic projection parameters are usually set up to map world units to pixels, or similar. It's possible to simply substitute a orthographic projection with a perspective one, but unless you've done a lot of careful pre-planning with the camera parameters and field-of-view, et cetera, you're likely to get a image of the world that is too "zoomed in" or too "zoomed out" and thus you'd want to make at least some minor adjustments to the camera in the perspective mode to account for this. Without more information about the background behind your question (what is the overall problem you are trying to solve), the answer is basically that "it depends." It's potentially doable in general, but whether it will work in practice for your project is hard to say. 

This is the typical naive approach to component updates (and there's nothing necessarily wrong with it being naive, if it works for you). One of the big problems it has you actually touched on -- you're operating through the interface of the component (for example ) so you know nothing about what it is you just updated. You likely also don't know anything about the ordering of components within the entity, so 

Since you're using rectangles and it sounds like you're also confining yourself to eight directional movement, you can probably get away with computing a vector from the ball's center to the bumper's, and examining that; it will tell you about their relative positions. If the magnitude of the Y component is greater the ball is hitting the top or bottom, if the X component is greater the ball is hitting the left or right, and if they are equal it's hitting exactly on the corner (maybe reverse both directions in that case?). A more robust solution would probably involve the separating axis theorem and use of penetration vectors to gauge response and real reflection vectors to compute the change in velocity. I don't know how robust a solution you need though. 

All of that said, I do agree with Sean's comment that you can avoid the need for this mapping entirely by providing good GUI support for adjusting keyboard bindings; then you can store out the locale-agnostic key values (virtual key codes on Windows) into your configuration file. 

I think you should just write your game, and perhaps you'll end up with an engine later. Writing the graphics engine (which is all it sounds like you're talking about) from scratch using nothing but OpenGL is likely only going to waste your time. It is a relatively well-solved problem so generally you'd only be changing the shape of the API without adding much in the way of significant new features relative to any other available third-party solution out there. If Cocos2D or some other graphics layer meets your requirements now, then use it. If your requirements change during development, you can swap out the rendering back end relatively easily -- if you truly believe you have the experience and wherewithal to create an engine yourself, you should certainly have what it takes to structure your game such that swapping out the rendering back end is a relatively trivial operation. Build your game, allow its specific needs to drive the feature set of the code you write, and write code with reusability and good architecture in mind. You will naturally end up with an "engine" after you finish a few projects like this, and you'll finish those projects faster because you're disallowing yourself from getting bogged down in framework-level feature creep. 

The particle system you are using as a base is explicitly designed to perform all its animation on the GPU. To disable that, you need to use a different design. You can modify the baseline you have to remove all position update/animation code from the vertex shader (see ) and move that to your application. You'll have to employ dynamic vertex buffers -- locking the buffer each frame, copying the position out of the particle system data structure and in to the buffer (although with the rest of the particle's vertex attributes), and then unlocking the buffer. You may want to remove/modify the rest of the methods in the shader, since any modifications they make to the particle system will clobber any updates you do on the CPU. Then you'll have complete CPU-side control over the particle attributes, at the cost of decreased performance due to the additional cost of updating the vertex buffer each frame -- this the bottleneck that the sample describes avoiding in the description: