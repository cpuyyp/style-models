It is quite possible that their conceptual schema has a lot of overlap with their physical schema. Knowing table names gives you a decent head start in planning your SQL injection attacks. It is very likely they don't have their conceptual schema documented. Organizations that let programmers design their own databases often don't have any rigour in their database design process, going straight to physical implementation without any initial design. They may not want to admit this, or they may not want to go to the time and trouble of back-creating a conceptual document that never existed. 

The question I would ask is whether the direct relationship between and is "important" to the system? There are all kinds of incidental relationships between tangible things that are recorded in any system. However, only certain of these are important for the purposes of the system itself. These are the ones that belong in a relational schema. If anthologies will always be made up of compositions, and compositions will always have composers, then you can always derive the relationship between anthologies and composers using a query. If you did it that way there would be no risk of inconsistent relationships. This model would look like this: 

I find with financial transactions you want to have a detailed audit trail of every movement of funds. This means a write-once (no updates) table containing fund movements using a double entry accounting model. In a simple version of this, you would have one table containing the list of payees and payers (your senders, administrators and receivers). Then you would have another table (or better two) that contain the financial transactions. The tables are roughly as follows: 

If all you want to do is keep track of which authors contributed to which papers, then all you need is a simple intersecting entity (see more) like this: 

From a logical ERD perspective, the appropriate design is clearly the entity supertype/subtype pattern. What you're wrestling with is how to implement this from a physical perspective. Like a lot of data modeling issues, there is no hard and fast rule. You are looking at a compromise. Do you want a fair bit more complexity in your application logic and queries (supertype/subtype) or do you want the risk that your application logic might not properly enforce your constraints and that a Car record in your consolidated table might get a non-null value in the column? The kinds of things that you should consider in deciding how to trade off these alternatives are: 

You don't need composite keys to enforce your referential integrity in your case. The reason is that you have a pretty straight-forward three tier hierarchy: 

You could really use either option: one translation table with a partitioning attribute, or a one translation table for each table requiring localization. The pros and cons of either approach are not likely to be related to the criteria you ask about: space or query performance. These aren't necessarily the most important criteria for you to be concerned about either. Unless you have really substantial scalability requirements then you should focus on code maintainability as a differentiator rather than disk space or query performance. Another factor to consider is how much time you want to spend explaining your choices to other developers. If you use a single table with a partitioning attribute, then you are opting for something very much like Entity Attribute Value (EAV) which is widely frowned upon for many mostly good reasons. On the other hand, having a translation table per translatable object lets you tailor your column sizes to the actual data rather than picking the largest common denominator and also use tools like declarative referential integrity which are generally seen as best practices. 

No, it is absolutely not proper normalization to move a name out to another table. Normalization reduces redundancy based on functional dependencies, not based on coincidental repetition of data values. Let's say there are 100 Joe Smiths in your database. If you move the name "Joe Smith" out to another table and have 100 records point at this name, what happens when one of the Joe Smiths decides to change his name to "Bob Smith"? Unless they all change their names in unison, you can't work with the data that way. 

A logical schema won't exist in your database. A logical schema is a design-centric database structure built to meet your business requirements. It is a model that exists on a white board or in a diagraming tool. It is like the architect's drawings of your database. A physical model is what is actually implemented in your DBMS. The two can be different for a variety of reasons and in several ways: 

If you're trying to get to the youngsters who've never searched for anything in their life without using Google, then why not try something like this: 

As the old saying goes: Technically correct is the best kind of correct. So following the technical tutorials is something you should do. However, I remember when normalization was first taught to me it didn't click in right away. Let's consider the basic steps in plain, but less accurate, English: First Normal Form (1NF) says to remove columns containing repeating groups to its own table. Your relation doesn't have any repeating groups in any of the columns, so your relation is already in 1NF. Second Normal Form (2NF) says that there shouldn't be any non-key attributes which depend on a subset of a multi-part candidate key. You have one multi-part key. That appears to be the details of the appointment, which seem to depend on the patient, the doctor and the date. That means that you need to move the appointment details out to a new relation (along with foreign keys that make a candidate key for the relation. When talking about 2NF, sometimes you'll hear the term "partial functional dependency". You want to avoid those. Third Normal Form (3NF) says that every attribute in a relation depends only on every candidate key (and nothing else). This is also know as a "transitive functional dependency" - also to be avoided. If you look at your two relations that you have after achieving 2NF, you have these candidate keys and the following fields which rely on those keys: 

You may want to recognize directional relationships (e.g. "is the mother of", "is the son of") or you may need to acknowledge that relationships are not bilateral insofar as the users don't grant each other exactly the same rights. In that case you need to keep two records in for each relationship. Otherwise, I would just capture each user linkage once. Union queries take about twice as long as single queries, but when these queries are built on fast operations like index seeks this isn't so bad. In any case you'd be trading this off against twice as much storage, which mitigates the impact slightly. I'd be much more concerned about the potential headaches of managing the redundant data (if it is indeed redundant). You could pick a convention like lowest first to make certain kinds of operations more predictable - such as updates and deletes. 

Save all of the raw ratings (i.e. individual user inputs) with the date on which the rating was recorded. Then you can calculate all of your weekly, monthly and yearly statistics based on the raw data each night. These rolled up statistics should also be dated (with a date range). In this way you can also keep multiple weeks, months and years so that you can look at trends if you like. This is often called a data warehouse. It is a common way of pre-calculating rolled-up statistics for quick and easy reporting. When you get past the longest of your roll-ups (e.g. 1 year) you can start to throw away your raw transactional data if you're worried about how much space it takes up.