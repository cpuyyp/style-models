$follows(x, y)$ : returns $true$ if $x$ and $y$ are in the same list and $y$ comes after $x$ (but is not necessarily adjacent to $x$) $first(x)$ : returns the first element of the list containing $x$ $next(x)$ : returns the the next element after $x$ in the list containing $x$ 

For company $A$/the firm/giant corporation/"big pharma"/"THE MAN", the strategy does not change from the symmetric version: Consider a round where the probability of seeing only lesser candidates thereafter is $> .5$. If company $A$ keeps the candidate, then it has a chance of winning $> .5$. If $A$ does not keep the candidate, then company $B$ can hire the candidate and company $A$ has a chance of winning $< .5$. So, obviously, company $A$ would hire (and company $B$ would attempt to hire) in this situation. For a candidate with winning odds of exactly $.5$, $A$ may or may not choose to hire, but $B$ would choose to hire because $B$ can never get odds better than $.5$. If company $A$ hired before it saw a candidate with winning chance $>= .5$, then the odds of a better future candidate existing (and hence $B$ winning) would be $> .5$. So $A$ will not hire until it sees a candidate of winning odds $>= .5$. Therefore, $A$'s strategy is identical to the symmetric case: hire the first candidate that yields winning odds of $> .5$. $B$'s strategy, then, is formed with $A$'s strategy in mind. Obviously, if $A$ hires (at or) before $B$, then $B$'s strategy is to hire the next candidate that is better than $A$'s, if any. Also, if a candidate comes by with winning odds $> .5$, $B$ should try to hire, even though $A$ will also try to hire (and force $B$ to keep looking). The only question left is: is it ever beneficial for $B$ to hire when the odds of winning is $<= .5$. The answer is: yes. Intuitively, say there is a round where the odds of winning with the candidate is $.5 - \epsilon$. Also, there is "likely to be" (explained later) a future candidate with winning odds $> .5 + \epsilon$. Then it would benefit $B$ to choose the earlier candidate. Let $d_r$ be the candidate interviewing in round $r$ for all $1 <= r <= N$. Officially, $B$'s strategy is: "hire $d_r$ if doing so yields better odds of winning than if not". The following is how we calculate such a decision. Let $p_{r,i}$ be the probability of winning after interviewing and hiring $d_r$ given $d_r$ is the $i$th best interviewed candidate. Then: $p_{r,i} = $ probability that $d_s < d_r$ for $s > r$ $ = (1-\frac{i}{r+1})(1-\frac{i}{r+2})\times ... \times (1-\frac{i}{N})$ ... $ = \frac{(N-i)!r!}{(r-i)!N!}$ Notably, $p_{r,i}$ is easily computable to constant accuracy. Let $P_{B,r}$ be the probability that $B$ wins given that neither company hired in rounds $1$ through $r-1$. Then $B$ would hire $d_r$ if the probability of winning after hiring $d_r$ is better than $P_{B,r+1}$. Note that $P_{B,N} = 0$, because if it is the last round, then $A$ is guaranteed to hire and $B$ will not hire anyone and loose. Then, in round $N-1$, $B$ is guaranteed to try to hire and will succeed unless $A$ hires as well. So: $$ P_{B,N-1} = \sum_{i=1}^{N-1}\frac{1}{N-1} \left\{ \begin{array}{lcl} p_{N-1,i} & : & p_{N-1,i} < .5 \\ 1-p_{N-1,i} & : & p_{N-1,i} >= .5 \end{array} \right. $$ Which leads to the recursive function: $$ P_{B,r} = \sum_{i=1}^{r}\frac{1}{r} \left\{ \begin{array}{lcl} 1-p_{r,i} & : & p_{r,i} >= .5 \\ p_{r,i} & : & P_{B,r+1} < p_{r,i} < .5 \\ P_{B,r+1} & : & \text{else} \end{array} \right. $$ It's pretty obvious that $P_{B,r}$ can be calculated to constant accuracy in polynomial time. The final question is: "what is the probability of $B$ winning?" The answer is $P_{B,1}$ and varies with $N$. As to the question of how often does $B$ win? I have not calculated exactly, but looking at $N$ from 1 to 100, it appears that as $N$ grows, that $B$'s winning rate approaches .4 or so. This result may be off as I just did a quick python script to check and did not pay close attention to rounding errors with floating numbers. It may very well end up that the real hard limit is .5. 

Ignoring the obvious edge cases (returning $-1$ when $H$ is not a subgraph of $G$) then the cost is basically $\sum_{i=0}^k \binom{|E(G)|}{i}$ computations of he subgraph isomorphism problem on inputs (basically) $G$ and $H$. And so, we are basically up against a possibly exponential number of subgraph isomorphism problems. The problem can be restated as a hitting set problem (also known as a vertex cover of a hypergraph problem). Let $S$ be the set of all sets, $T$, where $T$ is a set of $|E(H)|$ edges of $G$, and $H$ is isomorphic to $G - (E(G) - T)$. Note, $S$ is not every mapping of $H$ onto $G$, but is every edge set that can be mapped to. Let $h$ be the size of the minimum hitting set of $S$. Then $k=h-1$. This is neat, but it is not immediately useful. Calculating a minimum hitting set is NP-hard, and the input into the problem is possibly exponential in the size of the input of our original problem. Double-wam-o. However, it does provide us with a case for a fast approximation. If $|E(H)|$ is small (and by that I mean bounded by some constant $e_H$), then we have $|S| = O(\binom{|E(G)|}{e_H})$. Also, an $e_H$-approximation can be computed in polynomial time. Thus the real work is in finding $S$, which can be done with $\binom{E(G)}{e_H}$ cases of the graph isomorphism problem. The naive solution does not compute in polynomial time when $E(H)$ is bound because $k$ is not bound. 

OK, so there are alot of ways to store sets. I don't believe that any of them give you exactly what you want for the most general case, but here are several options. Let $S_1$ and $S_2$ be the sets, $k_1$ and $k_2$ be the number of unique elements in the sets, and $n_1$ and $n_2$ be the sum of the lengths of all elements in the sets. General Solutions Naive general solution in space $O(n_1+n_2)$ This solution involves keeping every string and is not time-optimized. Maintain an array of pointers to the unique strings. When a new string arrives, search through the set to see if it has already been inserted. If so, toss it, if not, insert it onto the end and resort. To check if the sets are identical, simply run through the strings in the first set, checking to make sure that each string exists in the second set. Also make sure that the size of the sets are the same. If both of these are true, then the sets are equal. This uses space $n_1+n_2$ to store the strings and space $k_1+k_2$ to store the array of pointers, so because $k_i < n_i$, we have space $O(k_1 + k_2 + n_1 + n_2) = O(n_1 + n_2)$. This solution is very naive and can be easily improved upon. By keeping the list sorted, one can use binary search to check for set inclusion. Set equality can be done by looping through both sets at the same time, checking for string equality. Trie solution in space $O(n_1 + n_2)$ This solution is similar to the naive solution and also gives precise answers. Instead of sorting the elements, insert them into a trie. Depending on your strings, you may end up getting decent compression. It is possible to get almost no compression, however, so in the worst case, the space is still $O(n_1 + n_2)$. It is faster than the naive solution, however, requiring time linear to the size of the string for both insertion and set inclusion (instead of $O(x \log(x))$ time for string length $x$). Fault-tolerant Solutions Set of Hashes for space $O(k_1 + k_2)$ This solution is much like the general solutions, but instead of storing the set of strings, store a set of hashes. The False Positive Rate (FPR) will depend on your hashing function and the number of elements in your sets, so unfortunately, the FPR will only be bound if there is a bound on the number of unique strings. Using the trie solution, after hashing the string, set inclusion and insertion takes only constant time. Set equality takes time $O(k_1 + k_2)$. Constant Time Set Equality @Yonatan N mentioned this solution as a comment, but I thought I'd detail it here. In this solution, keep the same set of hashes as the previous solution. However, for each set, also maintain a single number which is the XOR of all of the unique elements in that set. Set equality can then be checked by checking the equality of the two XOR of hashes. This operation would have only constant time and space consequences to insertion and storage. Using a Bloom Filter Under the assumption that you have a bound on the number of unique strings, you can use a Bloom Filter to decrease the space requirements even further. The effect of this is that you start with large space requirements $O(\text{bound on # of unique elements})$, but they never grow. This would be a good solution if you expect $k_1$ and $k_2$ to get close to your bound and want to keep your FPR guaranteed to be under a specific amount. Set equality can be done by checking to see if the bloom filters are equal (you could devise a similar constant-time equality by XORing hashes of the indexes in the Bloom Filter that are set to 1). Others There are other ways to do these things: Tree Sets, Windowed Sets, Stable Bloom Filters, compressing the strings, and more. Wikipedia is always your friend. Happy hunting. 

One greedy method would be to test the position which will minimize expected entropy (the amount of information that is unknown) weighted by the cost of testing the position. Assuming that we have determined that $c \le s \le d$ (via previous tests), then the current amount of entropy is: $$-\sum_{x=c}^d \ln\big(Pr(s = x \mid c \le x \le d)\big)Pr(s = x \mid c \le x \le d)$$ After testing a position, $y$, between $c$ and $d$, the entropy will be $$-\sum_{x=c}^y \ln\big(Pr(s = x \mid c \le x \le y)\big)Pr(s = x \mid c \le x \le y)$$ with probability $Pr(c \le s \le y \mid c \le s \le d)$ and $$-\sum_{x=y+1}^d \ln\big(Pr(s = x \mid y < x \le d)\big)Pr(s = x \mid y < x \le d)$$ with probability $Pr(y < s \le d \mid c \le s \le d)$ This makes the expected entropy: $$ -Pr(c \le s \le y \mid c \le s \le d)\sum_{x=c}^y \ln\big(Pr(s = x \mid c \le x \le y)\big)Pr(s = x \mid c \le x \le y) -Pr(y < s \le d \mid c \le s \le d)\sum_{x=y+1}^d \ln\big(Pr(s = x \mid y < x \le d)\big)Pr(s = x \mid y < x \le d) $$ Thus, to choose the test which minimizes expected entropy while penalizing cost, use the value of $y$ which minimizes: $$-p(y)\Big( Pr(c \le s \le y \mid c \le s \le d)\sum_{x=c}^y \ln\big(Pr(s = x \mid c \le x \le y)\big)Pr(s = x \mid c \le x \le y) +Pr(y < s \le d \mid c \le s \le d)\sum_{x=y+1}^d \ln\big(Pr(s = x \mid y < x \le d)\big)Pr(s = x \mid y < x \le d) \Big)$$ Repeat. Note, for uniform penalty and distribution, this becomes a binary search. I believe it will also work for your "penalize odds" example. 

For the following, $(w,x) >= (y,z)$ iff $w >= y$ and $x >= z$. I have a list, $L$, of $k$ points with integer coordinates ranging from $0$ to $n-1$. Each point has an associated set. I would like a data structure that can perform the following: $include(x,y,i)$ - include the element $i$ to all of the sets associated with a point $p$, where $p >= (x,y)$ $remove(x,y,i)$ - remove the element $i$ from all of the sets associated with a point $p$, where $p >= (x,y)$ $retrieve(x,y)$ - retrieve the set associated with point $p = (x,y)$ My intuition is telling me that $include$ and $remove$ should be possible in something like $\text{polylog}(k)$ and $retrieve$ should be possible in something like $\text{polylog}(k) \times (\text{size of set})$. The 2-dimensional aspect of this is stumping me, however. Motivation: this data structure could be very useful for quickly calculating edit distances for a data set I'm using at work. 

I believe there are two interpretations of your criterion. One interpretation is easily solvable, and the other is NP-Complete. First, a greedy algorithm for what you describe: Let $A_1$, $A_2$, ..., $A_N$ all start as the empty set. Loop through each $a_i \in A$. Put $a_i$ in the first $A_k$ such that for each $a_j$ already in $A_k$, $crit(a_i, a_j) = false$. After looping through all the elements of $A$, disregard any $A_i$ which is still empty. This greedy algorithm holds in the three examples you give. A second interpretation is: "Let the set $A_i$ be as large as possible but not including any elements from $A_1$ to $A_{i-1}$". Let $A'$ be an $N \times N$ matrix where $A_{i, j}'$ is 1 if $crit(a_i, a_j) = false$ and 0 otherwise. Then finding $A_1$ is exactly finding the maximum clique of a digraph with adjacency matrix $A'$. This, of course, makes the problem NP-Hard. Let's hope you only need the first. 

This problem can be transformed into the assignment problem, also known as maximum weighted bipartite matching problem. Note first that the edit distance equals the number of elements which need to change from one set to another. This equals the total number of elements minus the number of elements which do not need to change. So finding the minimum number of elements which do not change is equivalent to finding the maximum number of vertices that do not change. Let $A = \{ A_1, A_2, ..., A_k \}$ and $B = \{ B_1, B_2, ..., B_l \}$ be partitions of $[1, 2, ..., n]$. Also, without loss of generality, let $k \ge l$ (allowed because $edit(A, B) = edit(B, A)$). Then let $B_{l+1}$, $B_{l+2}$, ..., $B_k$ all be the empty set. Then the maximum number of vertices that do not change is: $\max_f \sum_{i=1}^k |A_i \cap B_{f(i)} |$ where $f$ is a permutation of $[1, 2, ..., k]$. This is exactly the assignment problem where the vertices are $A_1$, ..., $A_k$, $B_1$, ..., $B_k$ and the edges are pairs $(A_i, B_j)$ with weight $|A_i \cap B_j|$. This can be solved in $O(|V|^2 \log |V| + |V||E|)$ time. 

In the standard Secretary Problem, the goal is to hire the best secretary from a list of candidates. I've recently witnessed a failed hiring attempt for a needed position and it inspired a similar game: The goal is to maximize the net score. Each candidate has a value that (if hired) would be added to the score. And each day that goes by without hiring a candidate costs the company 1 point of score. Starting score is 0. Each day, the company either interviews a new candidate or attempts to hire a previously interviewed candidate. The value of the candidate is discovered during the interview and is uniformly distributed between $0$ and (known) $v$. There is an unlimited number of possible candidates to interview. If the job is offered to a candidate, then the candidate will accept with a probability of $d^k$ for (known) constant $d$ and $k$ being the number of days since the candidate was interviewed. You may only attempt to hire a candidate once. So the question is: what is the best strategy for this game given $v$ and $d$? Other natural questions: what is the best strategy if $v$ and/or $d$ are unknown? Any insights, answers, or references to similar games would be appreciated. 

We have a set, $L$, of lists of elements from the set $N = \{ 1, 2, 3, ..., n \}$. Each element from $N$ appears in a single list in $L$. I am looking for a data structure which can perform the following updates: 

$concat(x, y)$ : concatenates the list containing $y$ onto the end of the list containing $x$ $split(x)$ : splits the list containing $x$ directly after $x$ 

I have already come up with a data structure which performs these updates in $O(lg^2 (n))$ and queries in $O(lg (n))$ time. I'm mostly interested in whether or not there is already a data structure which can do this (hopefully faster?). Motivation: rooted directed forests can be represented with two of these list sets and they allow quick calculation of reachability in such forests. I want to see what else they can be used for and if all of this is already known. 

The question is very general and so the following is only a partial solution. My main motivation was to draw the connection between this and k-hitting sets. As Arindam has pointed out, this problem (or the decision version thereof) could be used to solve the subgraph isomorphism problem: "Is $H$ a subgraph of $G$ if any $0$ edges of $G$ are removed?". But as is often the case, we still want to know "is it possible to do better than the naive solution?" The naive solution: 

It sounds like what you need is the A* algorithm. This is a good algorithm for traversing a graph using a heuristic to estimate the "distance" each node is from the destination. In your case, the destination is any node which has a low weight. The heuristic for "distance from a low weight node" would then be a guess as to how close a node is from a low weight node (it sounds like you already know this). I would check out the wikipedia page I posted above for this algorithm and others like it.