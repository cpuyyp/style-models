You can then each of these files to see if you find any instances of or . For reference, you should be able to see the current value of the variable with: 

I don't know if this is likely, but if we imagine an additional record, essentially a duplicate of the data in record with id 6: 

Is creating columns for all the combinations the correct way? Based on the little information you have given us: No, columns for all combinations is probably not the right solution. Imagine if you suddenly get more products, then you will need to add more columns. Or if some products are removed. Designing your schema so you have to alter a table to deal with something as trivial is bad practice. No, you should probably use records instead. Let's say you have a table, something like this: 

No, is only relevant for IST. GCache is not used for storing incoming transactions to a joiner node during SST. Gcache is used for storing transactions in case another node requests a state transfer. If the GCache is large enough so that it holds all the needed transactions, then this will allow for an IST. Incoming transactions to a joiner node during SST are instead stored in the local receive queue (max size controlled by the variable and current size seen from the status variable ). This queue is independent from . 

although semantically the UPDATE above is exactly the same as if it did not have a WHERE clause (ignoring triggers), you will avoid a ton of I/O for tuples which already have some_bool_column = false. Second, if you can, try to take advantage of Heap-Only Tuples aka the HOT optimization for your UPDATEs. So if you can avoid having an index on some_bool_column, these bulk UPDATEs will be faster and may be able to avoid contributing to index bloat. Often, you don't want to have an index on a boolean column anyway, since the selectivity will be low (or if one of the boolean values is rare, just use a partial index on that value.) 

Right -- if you see that pg_stat_activity.waiting is "true" for an ALTER TABLE, that almost certainly means that it's patiently waiting for the ACCESS EXCLUSIVE lock on its target table, and its real work (rewriting the table if necessary, changing catalogs, rebuilding indexes, etc.) hasn't started yet. 

Amazon's RDS only offers PostgreSQL versions 9.3.x, and it seems unlikely that they'll ever offer to host older versions of Postgres. So by jumping from a local 8.4 install directly to RDS, you would in effect be making two significant changes at once (jumping up several Postgres versions, as well as switching to managed hosting). That may be alright or not -- it all depends on what features you're using and depending on. You should do some reading on RDS's limitations (no external hot standby, limited extensions, no shell access to the database instance, etc.) and benefits (hopefully much less maintenance work) and decide whether it's right for you. Also, I suggest you walk through the steps of dumping and restoring your data into RDS and ensure your application works OK, as well as reading through the Postgres major-version release notes for 9.0, 9.1, 9.2, and 9.3, paying particular note to the incompatibilities listed to see if any of them would affect you. 

The relationship between and is one-to-one, so if you wanted to simplify your table structure, then these two tables could be merged into one. Are you 100% certain that every question will have only 5 (or less) options? For flexibility reasons, it might make sense to have only one answer option per record. In that case, the table must remain separate from the table, and it becomes a one-to-many relationship. Which is the right answer option? You should probably indicate that somehow. In the table, the is obviously the primary key. The is a foreign key. And you seem to have forgotten a foreign key to , as well as a foreign key to a table to indicate who gave this answer. A minor issue: In the table, the column is a . This is up to 16 million bytes. Do you really need that many? A column can store up to 64K bytes, maybe that would be enough? Also, you would want to use for your columns. The reason for this is that auto incrementing ints start at 1, so by allowing signed s you're effectively wasting a bit and limiting the range of values you can use. There may be other opportunities for improvements, these are just a few I noticed. 

These modes are enabled by default in MySQL 5.7 (but not in MariaDB 10.2). So, after removing these from the sql_mode system variable, we get the desired result: 

Erwin's answer does a good job of answering the question as originally stated, however you added the comment: 

In addition to Craig's thorough answer, I wanted to add that the cover of the book you reference says: 

The output of that command will give you a decimal count of what number WAL file you are on. Run that command again in, say, 10 minutes. Subtract the first number from the second, and that gives you how many 16 MiB WAL segments would need to be synchronized between the primary and the standby in that period of time. That will answer your question about: 

You can also see in the pg_constraint table (e.g. via for some referencing foreign key ) that PostgreSQL is internally tracking this dependency by the OID (in particular, the column in ) of the index rather than the name of the index. The OID of the index does not change when you use , which is how Postgres avoids becoming confused by the rename. 

which will more-or-less double the size of the table, since the UPDATE has to keep old row versions around. You may want to read up a bit on how PostgreSQL implements MVCC and how vacuuming works, but to answer this question: 

and raise an alarm if that max. age is over 1 Billion or so. And if it gets close to 2 Billion you are in imminent danger! You can use a tool like check_postgres.pl which will handle this check for you with some configurable thresholds. 

Check your config files and and look for the socket parameter. Make sure it's the same for client and server. A good place for the socket parameter is in the [client-server] section in . The socket file is created by the MariaDB server when it starts up. Note that there are other places where MariaDB config files could reside, and these could potentially override your settings in the two files mentioned above. For more details, see this page in the MariaDB KB. Also note that you may be able to connect to MariaDB through TCP rather than through the socket. You can specify that you want to use TCP rather than the socket with . If this is successful, you can then do to find out where the socket file really is (or should be). You can then connect with or edit the config files so that the client reads it correctly from there. In some cases you may also be able to figure out what socket file location is used by listing the processes: which could output something like 

You can manipulate system variables like innodb_old_blocks_time (increase this - 1000 = 1 second) and innodb_old_blocks_pct (default is 37 - allowed range is from 5 to 95, set a smaller value to evict data from and similar faster). Both these variables are dynamic, so they can be given special values just before you run mysqldump, and then restored to the original values once it has completed. For details, see Making the Buffer Pool Scan Resistant. With MySQL 5.6+ (or MariaDB 10.0+) it's also possible to run a special command to dump the buffer pool contents to disk, and to load the contents back from disk into the buffer pool again later. (See MySQL Dumping and Reloading the InnoDB Buffer Pool | mysqlserverteam.com.) This way you can still use or other tools that "pollute" the buffer pool and then restore it afterwards. A way to prevent that running backup is unintentionally evicting your working set data at all would be to replace your backup method with Percona Xtrabackup or another physical backup tool that doesn't access the InnoDB buffer pool as such. Physical backup methods are also faster, and can be less disruptive than mysqldump. The disadvantage is that you'll need the exact same MySQL version and configuration on the system where the backup is restored. 

You can look under the "base" subdirectory of the data directory, and you should see something like this: 

This behavior is controlled by the parameters max_standby_streaming_delay / max_standby_archive_delay. You can fiddle with these parameters in the RDS Parameter Group used by your Read Replica instance to allow more time for queries against your Read Replica to complete. 

Here, Postgres knows that the index is bad and marks it as such in . There are countless other ways to create a corrupt index that Postgres won't immediately notice: Erwin's answer, mismatched glibc or other collation behavior between a primary and a standby, writing arbitrary bytes to the files behind those indexes, performing bogus updates to Postgres' catalog tables, and on and on. 

(though as you hopefully got to see, canceling that expensive can save the database from a lot of unnecessary grinding if you're just going to anyway.) 

Assuming you are talking about the column in , or some status field derived therefrom, you can create an invalid (or intentionally corrupt, if you prefer) index like so: 

So, 300k rows total doesn't seem like a huge amount, I wouldn't be overly worried unless you have a particular cause for concern (e.g. your UPDATE taking way too long, holding row locks for too long, etc). But two suggestions which may be helpful for your particular use-case: First, make sure that your UPDATE statement does not touch rows it does not need to. If you want to set all values of some_bool_column to false, do it like this: 

Yes, selecting from the table is interesting, but not surprising given the warning. (I don't have a MariaDB 10.2 instance at the moment, but testing on dbfiddle.uk indicates it too fails, though again not silently.) MariaDB 10.3.6, default sql_mode (strict) No warnings. select * from g; Gives: 

Looks like maybe a comment in one of the option files (such as /etc/my.cnf and any .cnf file included from that file) has become un-commented. Search through these files for "Disabling symbolic-links is recommended" and make sure there is a comment character (i.e. '#') in front of it, then save the file and try starting mysqld again. 

(Disclaimer: I'm no expert on NDB Cluster) A few options to try: Have you tried with the option? Description on the ndb_mgmd documentation page As the error message suggests, maybe try with the option? I'm guessing any small number might be valid. (Note: guessing!) Description on the Common NDB Options documentation page 

That said, there are cases where InnoDB will create on-disk temporary tables even if the query could potentially have been handled in memory. See 8.4.4 Internal Temporary Table Use in MySQL for details. These are cases when a RAM disk could perhaps have been helpful. Here's a blog entry (from 2012) about how to put the MySQL tmpdir on RAM-disk. However, any RAM used for the tmpdir in a RAM-disk solution is RAM that could potentially have been used for the all-important InnoDB buffer pool. So make sure you have a large enough buffer pool for your data working set before you consider using any of your RAM on a RAM disk. Assuming you put the tables' data files on the RAM disk and you plan to snapshot it, then you also need to take steps to ensure you're getting a point-in-time consistent backup. These kinds of steps will make the RAM disk slower. So alternatively, instead of using a RAM disk, you could use Galera as is, but take all possible precautions to avoid the creation of on-disk internal temporary tables. You should obviously also make sure to use SSD instead of spinning disks. Another technology to consider may be MySQL NDB Cluster: 

Not sure why Teradata has that limitation, but should be fine in PostgreSQL even when other tables have foreign keys depending on that index. PostgreSQL has fairly sophisticated tracking of such dependencies -- for example, if you tried to do you would see a complaint like: 

Supposedly it is possible to hook up Bucardo to RDS now that RDS Postgres supports the session replication role, but if you want a nightly snapshot I think you'll be much better off using RDS instance snapshots. 

You can set seq_page_cost and random_page_cost per tablespace via ALTER TABLESPACE without restarting Postgres. 

PostgreSQL reads its (index, heap, etc.) blocks either from shared_buffers, if they are available there, or from disk if not. Postgres does not need to read out of its WAL files other than for crash recovery (similarly, standby) purposes. 

Postgres can actually (in the following contrived case) use an index to satisfy queries without adding range scan kludges like the suggested . See the comments on Craig's questions for why Postgres is choosing this index in this particular case, and the note about using partial indexes. 

The space utilized by the table should go down if you run a or ; probably a alone will not be sufficient to immediately reclaim space. 

However, this is really not what CHECK constraints are supposed to be used for, and it actually introduces a race condition if you have multiple transactions writing to example_table at the same time (can you see how?). Use the UNIQUE constraints that PostgreSQL provides. If your values are too large for the UNIQUE constraint's B-Tree index, create the UNIQUE constraint on an MD5() of the value. 

Then you can update the "default" state. (If you use DELETE/INSERT, then you need a different trigger): 

This should show a character set for the table. If any collation is defined for the column or table, it will also be displayed in the output from that command. To see the character set and collation for your database, you can do: 

The following assumes that you feel the gaps in the sequence is a problem. This usually isn't a problem, unless these gaps become enormous and there is a risk that at some point you will exceed the maximum value. I'm further going to assume the table is defined similar to this: 

Verifying that a MariaDB or MySQL instance is "OK" is a complicated task, not least because there are different levels of "OK", and one man's "OK" is another man's "not OK". There are various tools you can use, though. To name a few in no particular order: 

... then this new record will be displayed as well. If you want to avoid that, we can use instead of . With the CTE + window function approach it's also trivial to get, let's say, the second most recent purchase order per sku, or the two most recent ones, and so forth, by modifying the clause. 

You can use an audit plugin, such as MariaDB's audit plugin which also works for MySQL, see: $URL$ For MariaDB's plugin, you can configure it to log DCL queries only, which should cover CREATE/DROP PROCEDURE.