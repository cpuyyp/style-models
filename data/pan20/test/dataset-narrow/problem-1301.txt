Because your code is broken and buggy. Mathematically, . If you do not get this result, then either your equality test is wrong (floating-point precision issues) or your matrix multiplication code is broken. 

It's complicated. In the pre-shader days, there were significant differences between OpenGL and D3D. There were also other APIs, like GLIDE. These have since fallen by the wayside. If the drivers for some particular hardware didn't support OpenGL very well, it might support D3D, so you needed a D3D path. But you could do more stuff with OpenGL via extensions. So having a GL rendering pipeline was important for features. 

UVIndex represents a series of indices that index into the UV array. However, OpenGL (and D3D) do not allow you to use multiple indices. Each attribute cannot have its own index; the index represents all attributes. Therefore, you need to massage your data a bit, so that everything comes from a single index list. To put it another way, you cannot take an FBX file, turn it into JSON, load it, and expect OpenGL to render with it. You need to use a tool to regularize the vertex data. 

It is always the responsibility of the animator to not position the character in such a way that their bodies interpenetrate. The software can only do so much for you. 

As with all other "preferable" or "best" approaches, it depends on what your end goal is. For example, if you associate the particle list with it's emitter, this means that the emitter must continue to live until the particles have all decayed away. So your emitter needs to have more state. It needs to know whether it should be actively emitting more particles or in a "wait until all particles are gone" state. The latter doesn't make new ones. If you don't have this multi-state system, then the particles will all vanish the moment the particle system gets rid of them. "Global" particle systems (it doesn't have to be "global" in the strictest sense. Just not bound to an emitter) allow you to de-couple emitters from the particles they emit. So your emitters can be very simple. Even better, you now have the ability to do different things you couldn't before. For example, you might want particles to respond to forces, like having particles near an explosion be pushed away by the impulse. Doing that with an emitter-based approach would be painful; you'd have to iterate through every particle emitter to make it work. With a "global" approach, you don't. Then, there's the question of depth sorting. If multiple particle emitters start to interact, how do they sort with one another? Obviously this requires some level of 3D for it to matter. Again, the "global" approach makes this easier. Of course, you may not care about depth sorting between particle emitters, particularly considering sorting cost. But you have the option of caring with the "global" approach. 

Why would you want to? You don't need to calculate how far a player goes "down"; you'll find out how far down it goes as the object goes through the simulation. 

Your server does not need to know or care about the visual representation of tiles. All it needs to care about is the conceptual representation. The user adds a tile here, so it changes that tile index. 

What do you mean by "make an OpenGL ES 2.0 application work?" If you mean, "Can I modify it to run on a desktop GL 3.x context with relatively minimal effort", then the answer is, "it depends". Obviously you'll need to change the version declaration of shaders, and your texture uploads may well be slower. ES 2.0 programs often use PVR compressed textures, which generally are not supported on desktop GL hardware. A bigger problem is with binary shaders. Even if an implementation supports the ES2_compatibility extension, there's no guarantee that the implementation will support the specific binary shader formats that the program requires. So your program should be using text shaders. Also, don't make a core or forward-compatible 3.x context; use the compatibility profile instead. But if you have some executable that uses ES 2.0, you cannot simply run it on a desktop GL 3.x computer and expect it to work. 

Multisampling is a property of the framebuffer. You enable multisampling by rendering to a multisampled framebuffer while is enabled. If is not enabled when you render to a multisample framebuffer, the pixels covered by that rendering operation replicate their data across all covered samples. To make a framebuffer object a multisampled framebuffer, you attach multisampled images to it. They will all need to use the same sample count, and all images in the framebuffer must be multisample. To make the default framebuffer multisampled, you must create your context with multiple samples. This requires the use of the WGL/GLX_ARB_multisample extension, as appropriate to your platform. Nowadays, the general pattern is to leave the default framebuffer non-multisampled and create your own multisample FBO images. Then, you blit from your multisample FBO to the default framebuffer's back buffer to display the multisampled data. CSAA is an NVIDIA thing, with an extension to govern its use. FXAA is not a piece of hardware; it's an algorithm. Some drivers have control panel settings that back-door it in. That's generally a bad idea, since they can't tell when you're rendering the UI (something that shouldn't use the FXAA algorithm). So it's generally up to you to implement it. 

The very first question you need to ask yourself about the "class" Sprite is this: what is a sprite? Is it the image? Is it an entity in the game world that has gameplay properties on it? Is it a collision box? Etc. If a "Sprite" represents only the position and orientation of an image (possibly selected from a sheet of images), then you don't really have different "kinds" of sprites. You have a Sprite, which can be rendered. There's no need for inheritance here. You can have sprites that have different images and so forth, but that's just the data stored in the sprite. Let's call the class for this "SpriteImage." If a "Sprite" is fundamentally a gameplay entity, then this entity would contain an object that represents how it gets drawn. You don't draw entities; entities don't draw themselves either. Entities contain one or more objects that they will give a position and orientation to, which represents how the visual representation of the entity is presented. Let us call this class "SpriteEntity". This class might contain one or more of the SpriteImage objects, and it would be responsible for providing the position and orientation of these objects. It would also handle animating them (selecting which image to show). However, it would not draw them; the drawing of these images would be handled by an object that also holds a reference to these SpriteImage objects. Here, you should employ smart pointer usage; SpiteEntitys would contain shared_ptr's to their SpriteImages, while the SpriteImageDrawer would have a list of weak_ptrs to every SpriteImage that is created. Of course, this is just one way of structuring things. You can choose your own, but the separation of "entity" from "image" is a very good idea. For example, the above architecture would allow some "SpriteEntity" objects to have no visual representation at all. This is useful to create collision areas (since entities would have collision areas, even if you don't see them), so that you can detect when another entity touches that area and do something based on that. It would also allow some "SpriteEntity" objects to have multiple sprites, possibly hierarchically layered. This would be for doing things like drawing weapons on characters, etc. 

I want to start by making a distinction between frequent random elements and infrequent ones. If you're playing a game where you attack something once every 5-30 seconds, then damage happens frequently. If you're playing a game where you attack less frequently than that, maybe once every minute, then it is not frequent. As an example, table-top RPGs, by the nature of being a board game played by human beings who have to do math and talk to each other, does not qualify as "frequent". Your RPG gaming group would be exceedingly fast if you make an attack roll every 1 minute; you're probably looking more at 3-5 minutes. And that's just for the time you're in combat; the time spent out of combat involves no attack rolls, and it can take just as long. So maybe half of your RPG time is spent out of combat (depending on the group, of course). Let's say you get one attack roll every 10 minutes. Compare this with any videogame RPG. In fact, let's go straight for Diablo. In a 4 hours session, how many attacks have you made against monsters? In a 4 hour session, you've probably killed more stuff than the table-top group even encounters in an entire campaign. What does this mean? Well, for the table-top player, each roll matters. It matters a lot. Each roll is precious, so you spend a lot of time maximizing the potential of each roll. You spend time acquiring weapons, items, and buffs to make each roll matter as much as possible. Those 9.5 minutes between attack rolls are there to ensure that when it comes time to roll for attack, you get the best bonuses and circumstances possible. Some table-top players have dice superstitions (though some only do them in jest). Dice are hallowed among some table-top players, for they live and die based on them. For the Diablo player, the random element means... nothing. Each roll doesn't matter that much, because 2 seconds later, you'll just make another one. If that attack did minimum damage, that's fine because you're about to make another one. The only time it might matter is that you might run into a streak of bad luck. But really, how can you notice when you're making attacks once every 2 seconds, and many of your attacks are hitting a half-dozen monsters? Can you truly say that any particular death you may have suffered was due to bad damage rolls, rather than just too many enemies all attacking you? Therefore, I submit the following idea: