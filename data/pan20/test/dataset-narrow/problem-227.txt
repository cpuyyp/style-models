Also, important stuff to pay attention to understand the different behavior: are the DB2 and DB3 identical in minor versions of MySQL? You only provided the major version which is 5.6. But from 5.6.6 there have been some important changes mainly on this issue. The replication to DB3 is done using the binlogs created by DB2 and applying the sql_mode specified on DB3. So these should be checked and seen what exactly is trying DB3 to replicate from the binlogs created by DB2? Can you paste the query here? Out of curiosity , if you test an insert from DB1 to DB2 and it goes ok, mysql doesn't throw any warnings? 

But it can be easily hacked if you change the with So instead you should really use the parameters as the sp_executesql is supposed to be used. 

If you still want to develop smth custom where you can log what info you want take these steps as start: Create tables with servers, databases linked to server id, backup info linked both to server id and db id. Create scripts that will be launched on destination servers using xp_cmdshell. The scripts will insert collected info to destination tempdb table and insert through linked server to your DBA server. This is very shortly. you can contact me on private if you need more details. I am using this method as it is the best way to track many stuff. 

I have figured this on my own and I wrote on my blog. For those interested in the solution visit this posts: RangeS-S, RangeS-U, RangeX-X 

once the filter is activated, slave server will skip all the statements which meet the given pattern. but before doing any change read how MySQL handles replication rules because you can skip important tables if you don't understand the rules. 

I came to a dead point in a deadlock analyze. According to msdn: RangeX-X are Exclusive range, exclusive resource lock; used when updating a key in a range. RangeI-N are Insert range, null resource lock; used to test ranges before inserting a new key into an index. So I understand that if I have an Index on 2 key columns - and I insert a new key I would have RangeI-N lock but if I update an existing key from the index I would have RangeX-X. But my question is more or less complicated. Say I have the index IX_keys_included on column A, B and included column C. In Serializable isolation mode I insert a new value for the included column C. Will there be RangeI-N or RangeX-X locks for the index IX_keys_included? Actually , will there be any locks given the fact that I insert a new column for an included column in the index? 

Only one base table is updated All other tables are key-preserved: each of them must have at most one row for each row of the base table. 

This is an interesting question: When does Oracle really delete data physically ? The unit of data in Oracle is a block. Let's see what happens when we delete a row. Here's an example with a simple table on 11gR2 (see "How to dump Oracle Data Block?"): 

Range partitioning involves a bit more maintenance because you have to create the partitions yourself. However, once the partitions are created, range and interval work similarly. 

You should never rely on implicit conversions because the behaviour is context dependent. Your interval filter is wrong: two intervals and will intersect if and only if: 

(additional restrictions on updating views apply) In your example you update table only. Oracle has to make sure that for a single row of this table, only one row of the other can be found. This seems to be the case since you have a PK on . Therefore you should be able to update the join. See for example this SQLFiddle with a similar setup. In your case you should either: 

The jobs submitted while the window is closed should be queued and run later when the window is opened. 

The most likely cause of a mutating table error is the misuse of triggers. Here is a typical example: 

I agree the Oracle docs can be a bit bland, however they are in general very complete. Once you learn how to locate the relevant piece of information you're looking for, they are often the best resource you can find online. In your case I would suggest you take a look at the PL/SQL Packages and Types Reference book, where you will find the complete documentation of all standard packages. The chapter, contains a collection of examples, in particular how to set up your directories to enable access. Once you have setup your directory object, you can create a file in PL/SQL with something like the following : 

Background I have inherited a system that has several hundred databases. The primary "Clients" Database represents all clients with several tables like: WebUsers = username | password | server_id Servers = server_id | ip_address | The remaining "ClientInfo" databases are intended for one client. Tables like: Customers = id | first_name | last_name I am not a DBA. I'm a web developer who inherited a rough built system. So I apologize ahead of tie if this question seems silly. The process The client visit a login server: login.example.com. This server has access to the "Clients" database. They login to their account which takes them to the correct web application server, sass1.example.com, that has credentials found in the "Servers" database and grants them a secure session. I think this is a pretty basic SASS web application setup. Think sales-force, get-satisfaction, that kind of thing. My Problem All databases are managed under a single database login. So the login server technically has access to all the client databases. And each application server has access to all databases. The only limiter being a single string in the session that identifies the user. From a security standpoint it is my understanding that these databases should each have their own credentials. So that if one is breached via code injection remaining databases are secure. For example my code logs into the client db using environment variables. Then selects the database name that is stored in the secure session. A code injection could easily submit a query that changes the active database. What is the secure way to manage this system? My rough plan was to have the login server point to the "Clients" database through a unique db user that has access to only that database. Then I would store the db user/password for each clients database in a "ClientList" table. Then on login the client specific database credentials would be pulled from an encrypted value in the "server" table. So the session would check if it's in a valid login state then instead of the string pointing to a database name it has an id that points to the "ClientList" table that has encrypted login credentials to just their database. But this doesn't seem much different than just managing all db's on the same login since the clients database still has all the passwords. A code injection would have access to the decrypt algorithms and just log in with different credentials. What is the right (secure) way to manage this system? 

There seems to be something wrong here: the 0 cost on the index full scan is suspicious and if I had to guess I would say that you're missing something: probably the stats on the index. This in turn leads the optimizer to believe that it can run the FULL INDEX SCAN "for free" and goes on with a suboptimal plan. This could also be a rounding error problem, since there is very little data (1k tiny rows, probably fits in a single block!). So either there is some stats missing, or too little data to be meaningful. Interestingly, if we run your test with a large sample (say 1M rows), the optimizer is happy to go with an index scan. If we insert some data instead and do a standard stats analyze, we find a more logical plan (11.2.0.3): 

You need to have SQL*Net installed in order to connect PL/SQL Dev to Oracle. SQL*Net is installed by default with most Oracle DB products (Oracle client for example). Since PL/SQL dev is a Windows app, you can see what Oracle products you have installed by looking into the registry (HKEY_LOCAL_MACHINE\SOFTWARE\Oracle). Also in PL/SQL dev settings (Tools/Preferences/Connection) you will have a list of all Oracle Homes (which allows you to specify which one you want to use if you have multiple homes). If you already have an Oracle Home, the file is by default in the directory . You can set the registry key if you want to specify another directory. 

In the first case the database will interpret PROCESSED as a variable, and its value, even if constant won't be learned until execution time. An index on will be used only if status has a strong selectivity for all values (ie there are many different values). Since you have only 3 values, Oracle makes a FULL SCAN since from its point of view any of the 3 values could be used. In the second case, an index will be used if you have statistics on this column that show that the value 0 is very selective (ie there are few rows processed). Oracle knows at compile time that this value won't change thus an index scan will always be effective. So, if you're sure that there are few rows processed, you will have to help the optimizer, for example with an hint like . Or you could use comments in your SQL: . Also remember that FULL SCAN are not evil. 

Looks like the disk subsystem on Server B is performing worse than on Server A but, I used to see disk issues not entirely related to disk specs. You could collect some other performance counters such as physical disk --> avg disk sec write (> 25 ms very slow), memory --> page file usage (> 70% bad), cpu --> processor queue length (> 12 very bad), memory --> pages/sec(> 600 slow, > 2500 very slow disk subsystem), sql server: buffer manager --> page life expectancy (< 300 memory issue). You can also limit the growth of 6 tempdb data files to the size they have now. See what happens. You should check the waitstats as well on both servers. If you see a lot of PAGELATCH_XX then you will know where to dig more. Jonathan Kehayias has a good article on this theme. And Paul Randal has also a lot of analysis which could help. 

I think you are misusing the explicit_defaults_for_timestamp. In your case this variable should be set to 0. Enabling it you are in fact restricting the insert of Null values into timestamp data type columns. $URL$ 

You can prevent creating a procedure with invalid objects before executing the Create Procedure statement like that: You will see a red line under the invalid object which underlines the error when you go with the mouse over the red line. I am not aware of other method, because the SQL will parse the procedure before executing it considering only the validity of the SQL syntax. 

first of all, the replication variables have effect on the slave only when activated on a replicated "slave" server. you need to understand that the filtering rules on master differ from the ones on slave. on master you can choose only to log a whole db or not. on slave you have more options. here is described $URL$ and here: $URL$ I think, you want to skip replicating a set of tables with a given pattern on slave. So, the variables must be configured on the slave. Change the configuration file on the salve and add the db_name instead of % for db part. 

You can create the database link by connecting directly to the remote database. As suggested in the askTom discussion, you can also use or to create a distinct remote transaction that can initiate the DDL statement. 

One way to force data to actually be overwritten would be to update it to a meaningless value before deleting the row. This wouldn't work with indexes since updates are translated to delete+insert in a b*tree index. 

If your table is updated concurrently, a bitmap index with a unique value will be a point of contention and shouldn't be used. 

Most likely using a cluster for this query won't be beneficial. A cluster in Oracle allows data from multiple tables to be stored physically close when they share a common key (here I suppose). This allows some query to perform better but a cluster will intrinsically consume more space than standard heap tables because for each key there will be some unused space. Insert-only heap tables on the other hand are one of the most efficient way to store data space-wise, since the rows fill all blocks nicely up to the HWM. In your case since you don't have a filter so all rows will be read, producing a FULL SCAN of the data. Because the rows are stored in a more compact manner in heap tables, the cost will be less than the cost for the cluster. The cluster, however, should have an edge when you look for a specific key, but this will also depend on the distribution of the data (number of rows per key), and on the length of the rows. You could build an example where the heap tables with regular B-Tree indexes will outperform a cluster for single-key queries. In conclusion, clustering tables in Oracle will help for some queries, but will also be hurtful to others, it has restrictions and drawbacks, it is not a silver bullet for optimal performance. Heap tables are the default for a good reason: they have good performance for most queries.