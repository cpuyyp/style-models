You might want to have a look at Microsoft XNA. It's not the whole DirectX deal but it comes close in terms of features. To understand the interface, you need (basically) no prior knowledge of DirectX/C++. It comes with its own abstractions and might thus be more approachable for you if you don't need as much low-level access as DirectX (and its managed wrappers) would normally allow. Furthermore, the XNA Game Studio should come with some samples and tutorials that make it easier to get into graphics programming. I'm sorry I can't say for sure, though, as I've only gone the C++/DirectX route myself. 

According to the documentation, the nvassemble tool shipped with nVidia's Texture Tools 2 suite should be able to assemble image files into volume textures, cube maps and texture arrays. $URL$ Unfortunately, the parts necessary for volume texture and texure array assembly are commented out in the accompanying source code and I could not find a reason for that. EDIT: Some time ago I contacted the author of the nVidia Texture Tools (Ignacio Castano) to find out why some parts of the nvassemble code are commented out. Here's what he had to say: 

In my Direct3D 11 application, I am using several sampler states to retrieve texture data. Some of them are used in all pixel shaders, some of them are only used in very specific ones. The question may seem simple, but I was not able to find any decent information on the topic- Are there any penalties (performance or otherwise) involved in creating a number of sampler states for the pixel shader stage of D3D11, setting them all at once and just never touching them again? My shaders would reference these samplers in HLSL if they need them (for instance some shaders would reference while some others would reference , possibly without referencing s0 at all. Also, is it correct, that if I never want to reference these samplers in my c++ code after creation and setting, I could just release my own external references after they are set and they will not get destroyed, as D3D is keeping internal references as long as they are "bound"(?) ? And even if had to somehow get a reference, I could simply use ID3D11DeviceContext::PSGetSamplers (of course accepting the involved performance penalty of the lookup), correct? 

So what this allows me to do is launch my application from any half-decent scripting environment (say, windows command prompt via batch scripts - but I actually use Ruby for that purpose), set a dump file path, load a map, leave it running for a few minutes, quit the running game, set another dump file path, switch the algorithm to use, load the map again, rinse, repeat. The Ruby script communicates with the game in-flight by creating that special file the command line module is looking for and putting the desired commands in the syntax the command line understands in it. I don't actually have use continuous integration on this project right now, but nothing would prevent me from souping up that Ruby script to also parse the generated performance logs and produce xUnit-compatible XML to communicate with the CI-system when performance has unexpectedly gone haywire for some reason and run the script on every full build on the build sever. Alright, so neither is my game written in XNA (it's plain C++ and DirectX), nor does this approach respect the fact that you don't actually want to run the game on your build sever. It also is nowhere near as flexible as what you are probably out for - but still, it is a neat, low-tech aproach to automated performance measuring that is somewhat CI-friendly (provided one has a beefy build server). Edit: As for how far I have actually taken that approach - I only compare performance measurings gained from different implementations of just this one module. But the whole system is set up in a way that would allow me to dump any single one of the categories defined for my lightweight profiling framework and use the external scripting environment to interpret the results in whatever way seems useful right there and then. Taking the performance profiling aspect out of the equation, I further intend to 

If you'd like another perspective on this, check out this blog post about "Wiring physical devices to abstract inputs". It covers a lightweight mapping framework in close detail and demonstrates a fundamentally different approach to the problem. It basically works as follows: Input from all devices is gathered in a structure that can be queried from all systems that want to subscribe to input. A system can then define "Actions" (single keystrokes or chords) for which the abstract device structure is queried. If the specified action has been performed on the device, the -operator of the action returns , indicating that the "input condition" the system has been waiting for is fulfilled. Note: This system relies on C++ templates to avoid run time type identification. If you are not coding in C++ you can probably just use a regular dispatch mechanism provided by your language as performance will probably not be your primary concern then. 

I take it you completely want to rule "Run the actual game" out, so basically my answer is disqualified from the get-go. But maybe you can take something away from it, hence why I post this regardless: For my Master's thesis, I have various independent/parallel implementations to achieve the same thing for some modules of my game engine and I need to carry out some performance measurements. Technically, nothing would prevent me from just running the game and looking at the numbers displayed in the on-screen profiler, but I still wanted to automate that because it is a tedious process to perform whenever my implementation changes. So what I have is this: 

This Presentation by AMD should answer your question about shader compilation in DirectX and intermediate representations of shader code: $URL$ I have only skimmed over it, but HLSL shader object code seems to be hardware independent indeed. Also, from my own (albeit small) experience: I compile shaders using fxc.exe and distribute the shader object files in an archive just as any other type of game asset. During run-time, I just read those object files into ordinary byte buffers and pass them to ID3D11Device::CreateVertexShader() for instance. How would that ever work if shader object files were not platform independent? EDIT: Sorry, I only just noticed that you were specifically asking whether the output of D3DCompile was platform independent. Since that output can (and must) be fed to Create...Shader just as fxc-output can, I suppose it's pretty much the same thing, added to the library for convenience. 

nVidia D3D SDK 10.5 samples page (it's at the bottom of the page) nVidia White Paper on Texture Arrays and Terrain Rendering nVidia Demo Project Download I'm aware this is a) not directly regarding XNA and b) possibly not quite what you want but maybe you can gain get a hold of further leads there. 

Here's a blog post that provides a practical example in task management. It assumes some knowledge about parallel engine design on your part but you might still find it useful as 'advanced reading'. 

I'm not a professional and my answer is not OpenGL-specific, so take things with a grain of salt, please. That said, what I did in my own little rendering framework was this: Everything that would qualify as a "GameObject" (in a strict OOP perception model) is called an entity. Entities are represented by just integer keys retrieved from an entity store. Entities can consist of any number of components. Those contain the real functionality and would be Mesh, Material, Transform, CollisionVolume, AudioPlayer, etc. When one wants to attach a component to an entity, they hand the entity key to a store for the wanted component. Behind the scenes, that store creates another component in a linear array of same-type components and remembers that whenever someone wants the retrieve a component with the same entity key, it's the one just created. That way, if some components can be "updated" in bulk without knowing about anything else (Collision Volumes for instance) they are all linearly laid out in the component store's memory and if someone wants to retrieve just one of them, they can do so via the key. So much for the general thing. Now regarding rendering, I'm still left in some kind of halfway immediate mode, where I tell the renderer the entities I want rendered via entity key. When I'm done declaring entities to be rendered, the renderer fetches all the components required for rendering, e.g. Mesh, Material, Transform (wich is Scale, Rotation, Transformation). Mesh- and Material-ID as retrieved from their component stores are aggregated into a sort-key, so that "entities to be drawn" are then grouped by similar meshes and materials. The Transforms have to sorted alongside so I can pack them up and provide them to instanced draw calls where many entities with the same mesh are submitted with just a single draw call and all necessary position data. Christer Ericson has a great post on this on his blog. I'm just throwing this out, not knowing whether that last bit is a good idea at all (in OpenGL ES, anyway). I'm also aware that I glossed over a lot of details but it ought to be kept simple so you can evaluate the idea. Anyway, this whole thing should scale pretty well (provided the component stores and sorting mechanisms are implemented efficiently) and should also lend itself well to be data-driven as one just needs a mechanism that reads entity/component descriptions and then hits the stores to plug the components together. One big question mark for me is how to handle interaction between components, but you wanted to know about rendering, so that should do for now :) EDIT: Check out Niklas Frykholm's blog for a clever way to manage associating entity-keys to components.