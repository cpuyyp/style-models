The Database Engine has some built-in defaults for handling data types. The definition of the data types and is as follows: 

If you have a file size limitation on the SMTP (Mail) server ..., ask the mail administrator (postmaster) to increase the limit for file attachments to allow your SQL Server Account (or other affected account) to send attachments with a larger file size. 

You have to be careful to run the SELECT first to check if the correct commands are being returned. You could also backup the msdb before you start as a precaution. To be on the safer side, you can right click a job and "Script Job As | Drop and Create To | New Query Editor Window" and then modify the path and execute. Be sure to remove the line containing in the sp_add_jobschedule part before you attempt to recreate the job. 

Reference: Populate Full-Text Indexes (Microsoft Docs) You might want to consider using Incremental population based on timestamp which is mentioned in the same document. 

This is possibly due to the fact that the value that you are implementing is faulty. This can have multiple reasons, none of which seem to documented outside of Microsoft. Official Documentation The only useful reference to comes from the official Microsoft Documentation CREATE STATISTICS (Transact-SQL) in which Microsoft states: 

This will return a list of database that are attached to your SQL Server instance. This will not list databases that have been detached from your SQL Server instance. Deprecated View sysaltfiles You shouldn't be using the view as this is deprecated and has been added for backwards compatibility only as is mentioned in the reference given by MD Haidar Ali Khan. 

You read the title and are still confused? Well I am too. I've just started a new job as DBA for a new employer and have come across some creative ways of installing SQL Server. My previous experience with SQL Server is all based on single MSSQLSERVER instances running on either virtual or physical hardware. We used to avoid multi-instance installations of SQL Server just to keep everything really separated and simple. Here at my new employer they have bunched quite a few instances of SQL Server Standard Edition on one piece of virtual hardware. Their (well I guess I should now call it, our...) reasoning: 

Your quote from the documentation (9.3.8 Comparison of B-Tree and Hash Indexes) is correct but for under the hood optimzations. If you force the query to not use the index with this: 

(This is the default setting for a database in SQL Server 2012) Test Scripts The following scripts were executed using the standard SQL Server SSMS client settings and the standard SQL Server settings. Client connections settings The client has been set to use the Transaction Isolation Level as per the Query Options in SSMS. Query 1 The following query was executed in a Query window with the SPID 57 

Let's follow on with the simplistic model of my TLog file and the situation where the TLog file has to grow. Two transactions modify data. Before 

Hints Just a quick summary of possible ways to provide you with additional information. Job History You might get additional information if you query the job history: 

The easiest solution might be to create a new database role and grant the delete permissions to that role. This way you will only have to assign the delete permisions to the single database_role once (and when you add new tables) and then assign the users that require the delete permissions to this database role. 1. Create New Database Role Create a new database role and assign it to a fixed database role: 

In my past years of being a SQL Server DBA this formula has been so far sufficient to leave enough free memory for SQL Server by-products that don't pull from the MIN and MAX memory settings and leave enough memory for the Operating System. With 32 GB of RAM the recommended setting would be 25 GB or 25600 MB for the MAX memory setting for SQL Server. 

Truncating the logs allows for the VLFs to be reused. That's all. This process can have the benefit of preventing the TLog file from growing IF auto-growth settings have been set. If no auto-growth settings have been set, because your requirements engineering process determined that the TLog file would have a fixed size, then the worst case here is that the TLog fills up because no TLog backup occurs and hence no VLFs are being freed. The TLog can't grow and the VLFs are not freed up to allow for further transactions to be written to the TLog file (or VLFs internally). 

Seeing as you might be querying all months at some time, you could just add all the months to the crosstab query beforehand. 

If you want to use the standard features provided by Microsoft then you might want to consider using the available DMVs (Dynamic Management Views) which are an integral part of Microsoft SQL Server. Best starting point Try out the starting page for System Dynamic Management Views (Microsoft Docs). This page provides an overview of the DMV grouped into categories. The benefit of using these DMVs is that you get to know SQL Server better, just by using them on a daily basis. You don't have to rely on updated third-party tools, because you always have the up-to-date information already at hand. That said, even the DMVs will change over time which is noted in the following statement found on the DMV overview: 

And then carry on from there. Reference: Identifying Object Name for CREATE and ALTER Using fn_dblog() 

** Before you continue** Create a database backup () and store this database in a safe location. Even better: Seeing as your disk might be faulty, backup the database () to an external drive and store in a safe location. Possible reasons for data corruption on data growth 

Start the instance using the PFILE (editable settings) instead of the SPFILE. You can modify the SGA settings in the PFILE before you start the instance: 

From my understanding of SQL Server the default behaviour is for the second query to not display any results until the first query has been committed. If the first query does a ROLLBACK instead of a COMMIT, then you will have a missing ID in your column. Basic Configuration Database Table I created a database table with the following structure: 

Depending on your requirements and/or the size of the data, you might have to reorganise some indexes earlier. Very simple example You have a table with 200 Million records (online archive) and 10'000 records are modified on a daily basis and 10'000 records are created daily. Doing the simple maths results in a maximum fragmentation of 0.01% which will never trigger the index reorg or rebuild. It would take 500 days to reach 5% fragmentation. In a production environment this could lead to potential performance issues, depending on the size of the table involved and the complexity of the queries. Your question answered Yes, you are good for fragmentation, with the above limitations. 

You might have to run to alter the permissions of the files in the databases directories. You might have to perform a with a recursive option to change the group / user owner on the file structure(s). Re-install MariaDB on the slave and then perform a restore of the backed up database from the master instance. Mounting the filesystem on the slave with Read-Write access. 

a. replace with the name of the database b. replace with a Windows Account (e.g. ) or a SQL Login (e.g. ). 

Answering your Question No, just because a Domain group is assigned to a database via a SQL Server login linked to a Database user, does not have to be a security risk. It might be the easiest way to grant a lot of users access to a database. Yes, if a Domain group has been granted permissions to access a database (via SQL Login and Database user) and to query information it should not be allowed to, then this can be a security risk. As with so many things in SQL Server: It depends. Check the database permissions assigned to the SQL Server login and verify that these permissions are required. From a hacker's standpoint: Every permission granted can be pose a security risk.