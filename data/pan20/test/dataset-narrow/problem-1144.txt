Given a set of points $V \subset \mathbb{R}^d$, the Voronoi diagram divides $\mathbb{R}^d$ into $|V|$ parts such that for every $v \in V$, the part of $\mathbb{R}^d$ for which $v$ is closer than any other point in $V$ is exactly a part of the diagram. See also Wikipedia. It is well known that for $d=2$, its complexity is $\Theta(n)$ and it can be computed in $O(n \log n)$ time. For $d=3$ however, there are families of pointsets for which the Voronoi diagram has complexity $\Theta(n^2)$, and for $d > 3$ there are even worse cases. However, from what I've seen, all these bad cases seem to occur 'inside' the pointset, and for the purpose I have in mind, this inside is irrelevant. Let $m = \max \{ |uv| \mid u, v \in V \}$. Suppose therefore we take a bounding box around our pointset $V$ and increase its size by some constant factor $c$ so that any point in $V$ is at least $m (c-1)/2$ away from this larger box. We take the Voronoi diagram for $V$ and remove the part that is inside the box. We name this new diagram the outer Voronoi diagram. 

--General Idea-- $\small\mathcal{Theory}\subseteq\mathcal{Theory'}\iff\mathcal{Model'}\subseteq\mathcal{Model}$ --A specific example with Haskell class-to-instance relation -- 1 $\small\mathcal{class(BUILDING)}\subseteq_{C}\mathcal{class(HOUSE)}\iff\mathcal{instance(HOUSE)}\subseteq_{I}\mathcal{instance(BUILDING)}$ ---------------- 

**> Replace COQ implies triplet with CafeOBJ conditional equation, using the following: **> [A -> B -> C] = [(A & B) -> C] = [C = TRUE if (A & B)] ceq [EQUATION] : R x z = true if ((R x y) and (R y z)) . **> Normal rewriting cannot deal with extra variable y in the condition on the RHS. **> Hence user controlled rewriting required using start/apply commands. } open TRANSITIVE2 . **> Using start/apply op X : -> D . **> If any variable x is related to arbitrary constant X eq [e1] : R x X = true . **> Then x is related to itself. **> CafeOBJ's start/apply commands allow selective bi-directional rewriting start R x x . apply .EQUATION with y = X at term . apply reduce at term . -- Result true : Bool close 

I proved a variation on Rice's theorem that answers your question for most properties. I'll try to explain myself clearer this time (Travis Service's answer was much clearer and more general than my previous answer). 

A group of friends and I are working on a space-RTS game as a fun side project. We're using a lot of the stuff we've learned at Computer Science to make it highly efficient, enabling us to make massive armies later on. For this purpose we've considered using kd-trees, but we quickly dismissed them: insertions and deletions are extremely common in our program (consider a ship flying through space), and this is an unholy mess with kd-trees. We therefore picked octrees for our game. 

When tinkering with noncanonical LR parsing, I thought up a parsing method (with infinitely sized tables, which makes it somewhat unpractical) capable of parsing exactly the unambiguous grammars in $O(n^2)$ time, and I wondered if it is possible to do better: 

If an oracle tells me that $G_1$ and $G_2$ are isomorphic, do I still need to look through all $v!$ permutations of the vertices? I ask because I also think about knot equivalence. As far as I know, it's not known to be, but say detecting the unknot were in $\mathsf{P}$. Actually finding a sequence of Reidemeister moves that untie the knot might still take exponential time... 

The above protocol is not perfect, some kinks I think would need to be worked out. For example, it's not clear how to generate two random graphs $G_0$ and $G_1$ that satisfy good properties of rigidity, for example, nor is it clear how to adjust the difficulty other than by testing for graphs with more or less vertices. However, I think these are probably surmountable. But for a similar protocol on knottedness, replace random permutations on the adjacency matrix of one of the two graphs $G_1$ and $G_2$ with some other random operations on knot diagrams or grid diagrams... or something. I don’t think random Reidemeister moves work, because the space becomes too unwieldy too quickly. [HTY05] proposed an Arthur-Merlin protocol for knottedness, but unfortunately there was an error and they withdrew their claim. [Kup11] showed that, assuming the Generalized Riemann Hypothesis, knottedness is in $\mathsf{NP}$, and mentions that this also puts knottedness in $\mathsf{AM}$, but I’ll be honest I don’t know how to translate this into the above framework; the $\mathsf{AM}$ protocol of [Kup11] I think involves finding a rare prime $p$ modulo which a system of polynomial equations is $0$. The prime $p$ is rare in that $H(p)=0$, and the system of polynomial equations corresponds to a representation of the knot complement group. Of note, see this answer to a similar question on a sister site, which also addresses the utility of such "useful" proofs-of-work. 

Can we consider a Haskell class as a loose signature-only-specification (denoting a theory) and an instance as an implementation (denoting a model)? In the example below the specification of the class $\mathcal{BUILDING}$ is textually smaller than the specification of the class $\mathcal{HOUSE}$, though in Haskell $\mathcal{BUILDING}$ is a super class of $\mathcal{HOUSE}$. Is there a Galois correspondence between a Haskell class hierarchy and its instance hierarchy? 

Q2. Does the reduction of the EQUATION in TRANSITIVE2 prove reflexivity? I use a form selective application of bi-directional rewriting. This form of rewriting is highly controller by the user. Space does not permit a full description, but in this case the condition in the EQUATION is executed first and the assumption R x Y is applied during the rewriting process. We could describe this as a manual proof with some machine support. Q3. How do these approaches differ? Form my web searches I get implicit and explicit as follows: The implicitly specification of function or relation asserts property that its value must satisfy. Implicit definitions take the form of a logical predicate over the input and result variables gives the result's properties. This approach seems to be distinct from the normal Peano style equational axioms (e.g. N + 0 = N). The PROPERTY approach seems to fit this description. Explicitly defined functions or relations are those where the definition can be used to to calculate an output from the arguments. The EQUATION approach seems to fit this description. Are these reasonable distinctions? Regards, Pat 

Lastly, consider what happens when we try the dynamic programming algorithm from SS on SP. Because we use products rather than sums, the numbers involved blow up enormously, and the arbitrary precision math required suddenly becomes a factor in the running time. This is why the algorithm cannot solve SP instances quickly even if the numbers are in unary. 

If you prove the above problem NP-hard, then for any $k$, if we modify the problem so that every vertex chooses $k$ vertices and every vertex is chosen exactly $k$ times, then the resulting problem is NP-hard as well: you can reduce the problem for any $k$ to the Hamiltonian Subcycle problem by 'eating up' the number of choices each vertex has by attaching certain gadgets to them. I came across this problem when I believed a certain problem was in fact equivalent to the above problem. That later turned out to be wrong, but the problem interested me nonetheless. I've developed an algorithm that sometimes finds the correct answer, but often doesn't end at all. It's based on using Lagrangian relaxation on the TSP variant of the above problem, which is simply TSP where more than one cycle is allowed (as long as the cycles are disjoint). I doubt it's possible to fix the algorithm so it works all the time, so I haven't included it in this question, though I could always do that if needed. 

I am using an example from COQ: 1.4 predicate Calculus which I have written as two CafeOBJ theories or loose specifications. They are my two attempts to represent FOPL in EL. I am not sure if the approaches are valid. Here is the description of the relation R from COQ. Hypothesis R_symmetric : $\forall x y:D, R x y \implies R y x$. Hypothesis R_transitive : $\forall x y z:D, R x y \implies R y z \implies R x z$. Prove that R is reflexive in any point x which has an R successor. For any x and y, R x y implies R y x by symmetry, then by transitivity, we have R x x. Symmetry and transitivity are not enough to prove reflexivity, we must also assume the x is related to something (e.g R x ? or R ? x exists). Consider the 2 equations labelled PROPERTY in the module TRANSITIVE1 and EQUATION in the module TRANSITIVE2. My questions are these: Q1. Does the PROPERTY equation and its reduction represent and prove reflexivity? The results of the reductions would appear to represent a proof. 

I have a (hopefully simple, maybe dumb) question on Babai's landmark paper showing that $\mathsf{GI}$ is quasipolynomial. Babai showed how to produce a certificate that two graphs $G_i=(V_i,E_i)$ for $i\in\{1,2\}$ are isomorphic, in time quasipolynomial in $v=|V_i|$. 

although I can't find it in any errata. But more importantly, I believe the comment was to the effect of "finding preimages of hashes can be challenging, and the only obvious way for Merlin to execute the public coin $\mathsf{GNI}$ protocol is to go through each and every permutation $\pi$ of $G_1$ or $G_2$ and calculate $h(\pi(G_i))$ until he finds an $h(\pi(G_i))=y$." But is it the case that all such public-coin protocols put a heavy burden on the prover? For example, the hash $h$ is not required to be difficult to invert; it just needs to be strongly universal, which, as I understand, is not the same as being one-way. So if Arthur were to give Merlin an image $y$ for some "easy-to-invert" hash function $h$, then it might not be so difficult for Merlin to find the preimage. 

The concept seems to be introduced in "Extending context-free grammars with permutation phrases". Therein it is also described how to parse these phrases in linear time using an LL(1) parser. The paper "Parsing permutation phrases" describes a method for parsing permutation phrases using parser combinators. These are the only two papers I've found that talk about permutation phrases and how to parse them. Seeing that we can easily parse these kinds of permutation phrases with LL(1) based parsers, my guess would be that we can do the same with LR(1) style parsers. My question is therefore: 

As proven here, general TSP is NPO-complete. This means you can either solve it exactly in exponential time or approximate it in polynomial time with an approximation factor growing exponentially with the length of the input. This means that your algorithm, when applied to general TSP instances, cannot give a better guarantee than a factor $O(2^{n^\epsilon})$ for some $\epsilon > 0$. Different versions of TSP admit different bounds on approximation algorithms. For an overview, the site "A compendium of NP optimization problems" lists a number of TSP variants here. Wikipedia also gives a lot of bounds on its page about TSP, in this section and in this section. 

I am interested in using Equational Theories (ET) together with Equational Logic (EL) found in algebraic specification languages such as CafeOBJ . I wish to use ET+EL to represent and prove sentences in First Order Predicate Logic (FOPL). The advantage of such an approach is that one can easily map loose theories written pseudo-FOPL to more concrete theories which may have initial models (using views). Translating FOPL to EL seems to require auxiliary techniques such as Skolemization and sentences splitting. I am concerned that there maybe some FOPL sentences which cannot be represented using EL even using these auxiliary techniques. I am aware that in general EL is regarded as a sub-logic of FOPL and any valid EL theorem is a valid FOPL theorem (but not vice versa). Goguen and Malcolm1 and Goguen and Malcolm2 describe FOPL as the background for equational proof scores in OBJ which was a predecessor of CafeOBJ/Maude. They also provide general advice on how use EL to prove FOPL theorems. 

A difference between a standard graph non-isomorphism protocol and the present protocol is in step 3. That is, conventionally in step 3, Vicky just randomly chooses any old element $\pi\in S_n$ to apply to the graph $G_i$, but in the above protocol in step 3, Vicky walks along the Cayley graph of $S_n$ with generators $\langle\pi_1,\pi_2,\cdots,\pi_r\rangle$ a total of $t$ times. Notice that because each step of the Markov chain is, by definition, an invertible permutation of the configuration space (the configuration space being the set of all adjacency matrices equivalent to $M_i$), such a walk is a doubly-stochastic Markov chain, meaning each column (resp. row) of the transition matrix sums to $1$. Thus, after the selected adjacency matrix $M_i$ is properly mixed, the matrix that Vicky will present to Peggy is uniformly distributed over all matrices isomorphic to $G_i$. Accordingly, if $G_1\cong G_2$, then Peggy would not have better than even chance of deducing $i$, because she will just be given two random matrices isomorphic to $G_i$; hence, the protocol is sound. A key concern is that we need to make sure that the matrices are properly mixed, that is, that the number of steps $t$ along the Cayley graph is large enough to mix the given graphs to the limiting distribution. I believe, although I can't prove it for now, that for most generating sets $\{\pi_1,\pi_2,\cdots,\pi_r\}$ of the symmetric group $S_n$, $t$ is probably $O(\log n)$. But, now to generalize, given any two elements chosen from any discrete configuration space, not just adjacency matrices representing graphs, I think we have a sound zero-knowledge protocol to show that the two elements are not in the same equivalence class, as long as we can construct a fast-mixing doubly-stochastic Markov chain to walk through the configuration space.