I'm sort of new to SQL Server so bear if the doubts are silly. I have configured log shipping for my DR purpose. SQL Server 2012 is the version. Today I encountered the following error in my log shipping instance. 

Does select_catalog_role if granted to a schema provide access only to dba tables and v$ views only and not to other schemas created for any application? The reason i'm asking this is I need to grant select_catalog_role to some administrators but not to the application schemas where in sensitive information resides. 

My main concern, regardless of the migration mechanism used, would be for the consistency of the migrated data. How are you going to ensure that you don't miss changes while the data is transferred. One of the classic Informix tools could be DB-Export, which (by default) locks the database so it isn't being changed while the export occurs. It gives you the schematic and unloaded (text) data files. Another option would be to make an archive of the existing system and then restore that into a new (temporary) instance, and you could then run your migration against the temporary instance without affecting the working instance. But you'd still have to worry about later changes. If your existing machine is not too woefully under-powered, you should be OK running the export as you suggested, but you still face the issue of ensuring consistency of the migrated data if users are changing the source system while you are migrating. 

Oracle 12c database comes with default Unified auditing enabled even if we set it to FALSE which has ORA_SECURECONFIG policy enabled by default. Apparently, the audit options for that particular policy captures many default options such as Alter and create statements. My question is, by default for any new installation, mixed mode is enabled which means that the traditional auditing is enabled and also Unified auditing is enabled as well both capturing the same audit_options. Why is it that way? What is the point of Mixed mode? When by default tradionally Oracle audits certain user actions, why should ORA_SECURECONFIG should also audit the same default user actions and write to AUDSYS schema? And if I need to keep only Unified auditing, then how to disable traditional auditing? Should I change audit_trail from DB to NONE? 

(When the expression is quoted, you get the error .) You have some curious types in the table — DECIMAL(2,0) is not a common type (consider , for example — though I suspect you mean ). The other problem is that you've passed 26 arguments to the procedure, but the you created only takes 22 arguments. You're also trying to pass where the procedure expects a . In fact, after the second argument, the alignment of arguments given and arguments expected goes haywire; I can't work out what is supposed to be what. If ever you needed an argument against huge parameter lists, this code is it. This mismatch leads to the error because functions in Informix can be overloaded based on the argument list, so your call doesn't match the function that is defined, so presumably you were attempting to call a different function that couldn't be found. When you get past those issues, by editing the EXECUTE PROCEDURE statement, you then run into problems with the notation: 

Correct me if i'm wrong, so ASYNC noaffirm means the log transfer method is ARCH right? Meaning the archives generated in primary gets copied to standby and then applied there. Right? The database is primary has been generating too much archives beccause of which my DR lags behind when archives are more. Oracle MOS asks me to create standby redo to resolve the same. My question is that how will standby redo help in reducing the lag since the SRL is used so as not to lose data in case of disaster right? Also, SRL should be used only when the parameter is LGWR right? Since i have mentioned it as ASYNC NOAFFIRM and there is no standby, the archives are only sent to DR and not redo. 

Contract — holding the single-valued data about a contract, excluding receipts. It might record the latest receipt number for the contract, but that would be an optimization, storing derivable data. Primary Key: Contract Number (aka Lot Number). Contract Items — holding the 1-6 items for the contract. Primary Key: Lot Number, Lot Sequence Number. Lot Number is a Foreign Key reference to Contract. Receipts — holding information about receipts. Primary Key: Receipt Number. Foreign Key: Contract Number reference to Contract again. 

The tool assumes that it must create the database (and won't work with an existing database). There isn't a utility that migrates data like you're trying to do. You have multiple options. One is to use one of the replication schemes (ER, HDR, RSS, etc) to create a clone of the original database. Another is to create the relevant SQL yourself and run that via DB-Access or another tool. You might find external tables or HPL relevant for getting good performance on the loading. What do you do with the data that already exists in the target database? Delete it before importing the new? Save it in case the new doesn't import cleanly? Do you have constraints — primary key, foreign key — that will complicate any data loading? Do you need to defer those constraints while all the loading takes place? Do you want to rebuild the tables from scratch? What size of database is under discussion? Megabytes, gigabytes, terabytes, petabytes? How many tables? How big are the biggest tables — row size, number of columns, number of rows? 

As per compatibility matrix, the target and the RMAN client version has to be the same. The workaround is connect to the catalog database from the target database ( whose version is lower than that of the RMAN client) and register the database in the catalog. Thanks! 

I have enabled MySQL master-master replication with 2nd master acting as read-only. Recently I checked the out the size of the databases in both master and slave and I found that database size in two databases is 1mb less than the Master. Does it mean that the setup which I made for master-slave was not done properly or the replication is not happening properly? Also, how can I make the changes so that it remains the same in both master and slave? Thanks! 

Server 2 is obliged by its duties under the 2PC to contact the coordinator for advice on the state of the transaction. Until it gets a Commit or Rollback from the coordinator, it must hold the transaction in the 'Ready to Commit' state, locks and all — even if it is restarted. This could go on indefinitely, of course, so in practice there are 'Presumed Fail' or 'Presumed Succeed' heuristics that can be applied, but it is not something to undertake lightly. If you get into a heuristic operation, then you probably do have an inconsistent set of databases — which is just one reason not to go 'heuristic' in a hurry. Of course, if the Coordinator or the network are out of commission for any extended period of time, you probably have other problems than just database consistency. You can find out a lot from reading Concurrency Control and Recovery in Database Systems by Philip A. Bernstein, Vassos Hadzilacos, Nathan Goodman (available for download). That is tough going, though. You could look in Date (Database Fundamentals, 8th Edn), but it is covered very briefly there, or Recovery Mechanisms in Database Systems by Kumar and Hsu, which has a substantial discussion of 2PC in chapter 13. 

I have about 5 schemas in my oracle 12c database for which I need the DDL for all the indexes for each of the schemas. DDL I have checked it on how to get the code from it, but for all the indexes and for each schema in one go.. is it possible? If yes, help me out.! Thanks.! 

SQL developer is free or do you require license for oracle database to use the sql or pl/sql developer? Also, Is it secure to connect to the oracle database since it connects to the db over a remote port right? 

Using means 'the table called found in the database ', whereas you want to refer to the table owned by user in the current database. For that, you need a (not a ) between the user name (schema name in standard SQL) and the table name. When that's fixed, you get the error: 

You would need the 32-bit CSDK or Informix Connect (effectively the CSDK Runtime) on the 64-bit system (as well as a 64-bit CSDK or I-Connect for 64-bit applications). Apart from that, I believe that it should work. If it runs into problems, it is because of the Windows registry and the need for separate INFORMIXDIRs. On 64-bit Unix systems, it was common to have both 32-bit and 64-bit versions of Informix CSDK or I-Connect on a single machine in separate INFORMIXDIRs. These days, there is less 32-bit code — there are no longer any 32-bit servers on 64-bit Unix systems, and usually there isn't a 32-bit CSDK either. 

My question is that can I update my plugin which is higher version than my OEM itself? If yes, should I update my plugin one version at a time? Say, my deployed oracle database plugin version is 12.1.0.4 and available downloaded versions for the same I have is 12.1.0.6 and 12.1.0.7. So I should update the 6 release first and then the 7? Also how to go ahead with the update? Are there upgrades for the OEM itself? from 12.1.0.4 to a higher version? If yes, how to go ahead with it? Are there any MOS if you could share?