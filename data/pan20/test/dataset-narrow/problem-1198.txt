Two main difficulties. Incompleteness (see Gödel's Incompleteness Theorems) and the vast size of the search space (there are vastly more uninteresting theorems than interesting ones). Considerable progress has been made using proof assistants (Coq, Isabelle, Agda, etc). With these the mathematician writes the theorems and lemmas and the proof assistant helps finding proofs and ensures that the proofs are logically valid. One simple way a computer could prove a new theorem is to take two existing theorems, $P$ and $Q$ and combine them to make the theorem $P\wedge Q$. Of course, if the search procedure is more clever, then you can discover more clever things, but to find something truly interesting and original would require a lot of intelligent tricks and techniques on the part of the programmer. Gödel's incompleteness theorems are relevant here because they place a fundamental limit on what can be discovered within any proof system: a computer could never discover the proof of the consistency of its own logic. This paper describes how the proof assistant Coq is used to prove the four colour theorem. Mechanized mathematics (overview ) is one area of TCS devoted to (semi)automatically proving theorems (and in general using computers to help mathematicians). One area where automated theorem proving (of sorts) is making an impact is in model checking and model finding. Model checking deals with determining whether a given system satisfies a given property, whereas model finding finds a system to satisfy a given collection of properties. The tool Alloy employs model checking and model finding to good effect, and it's quite usable. 

When in doubt, consult the original source: Clinger's Thesis. Clinger shows that fair merge cannot be written in a nondeterministic dataflow program. He does this by considering the semantic domains in which such a function would be defined, ultimately deriving a contradiction to the assumption that fair merge exists. Firstly, as Daniel Apon points out, $merge(\bot,1^\omega)=\{1^\omega\}$. This is the only reasonable definition of merge for these arguments (see page 84 of Clinger's thesis). Secondly, $merge(\bot,1^\omega)\subseteq merge(0,1^\omega)$, because $\bot\le 0$, as stream $0$ is a possible extension of stream $\bot$ (bottom of page 73), and by assumption $merge$ is monotone. In more detail, the ordering on the domain of strings states that any prefix of a string, is $\le$ the string, thus the empty string $\bot$ is the smallest element, and hence $\bot\le 0$, where $0$ is the string with just the element $0$. The function $merge$ has type $S\times S\to\mathcal{P}(S)$, where $S$ is string and $\mathcal{P}(-)$ is powerset. The assumption that $merge$ is monotone means that if $a\le b$ then $merge(a,s)\subseteq merge(b,s)$. (Similarly for the second argument.) Now $1^\omega\in merge(0,1^\omega)$ violates the assumption that $merge$ is fair – a fair merge of $0$ and $1^\omega$ should to contain the $0$. In more detail, $merge(S,T)$ is interleaving the "cards" from $S$ and $T$, without shuffling them. The significance of the stream $1^\omega$ is that it is infinite. $0$ is simply some character different from $1$. A fair merge of $0$ and $1^\omega$ should result in a string such as $1^*01^\omega$. Fairness means that within some finite time it selects elements from each string, thus the character $0$ is eventually chosen. Hence $merge$ cannot be written as a nondeterministic dataflow program. 

The classic application of scene recognition algorithms is the classify the image content into semantically meaningful entities in order to understand the image content. Then one could use content-based indexing to categorise the images for latter retrieval. Imagine a google for images which is not based on the name of the image and the context in which the images is found (such as page it's on and the tags), but rather based on the image content. One older reference is A. Vailaya, M. Figueiredo, A. Jain and H.-J. Zhang, Image classification for content-based indexing, IEEE Trans. Image Processing, 10(1):117-130, 2001. Being able to recognise images has applications for automatic tour guides too. For instance, this paper explores such a scenario: see the Eiffel tower, your camera phone will give you information about the Eiffel tower. Another one is Lost Robot Localization. There are undoubtedly many more applications, if you do some reference/citation chasing from these papers. 

I'll have a go at the answer, though I do not have a concrete program for you to try. In one Eelco Visser blog entry you can see a screen shot containing essentially the lambda calculus encoded in Stratego (I'm sure this code is available elsewhere). He presents an interpreter for the lambda calculus in Stratego. Presumably, this is sufficient to show that Stratego is Turing-complete. If you are interested in a more specific notion of expressiveness, then I suggest that you rephrase your question. In terms of practical usage, I found Stratego's main limitation that it operated over only one data structure, namely, the tree being transformed. Using other data structures such as a table of symbols was unnatural (at least in Stratego from 5+ years ago). Even though Stratego is ideal for transformations on a single tree, performing operations on two trees simultaneously is not easy. 

Ben Zorn did some work measuring actual costs of different garbage collection algorithms, though the following is more recent paper presents a much more comprehensive comparison: 

The first paper encodes two styles of ownership types, namely owners-as-dominators and owners-as-locks, in terms of Boyland's fractional permissions, which are a capability system developed for reasoning about programs. The second paper takes confinement ideas similar to those used in ownership types and adds them to separation logic. The third paper has developed a semantic approach which is used to encode various confinement disciplines such as ownership types. I'm not sure whether their system covers separation logic as well, and I cannot access it at the moment. Their approach is rather ad hoc; it can be seen as a more formal and systematic to a paper I wrote a while ago with James Noble and others: 

I'd say that you can ignore garbage collection. Interestingly, perhaps, all the work I've read on garbage collection fails to mention computational complexity. This is because the algorithms are $O(n)$ in the size of the heap, though typically they do not do a whole heap scan every time they are invoked. The main problem with garbage collection is not the computational complexity, but unpredictability. This is most relevant when considering real-time systems. A lot of work on real-time garbage collection tries to address this issue. Others have simply given up. For example, the Real-time Specification for Java relies on programmer specified region which are allocated and deallocated programmatically with $O(1)$ cost, irrespective of their size. The definitive garbage collection reference is: 

There's a very recent survey paper available on Alias Analysis for Object-Oriented Programs. It will be published in April in the LNCS state-of-the-art volume (gratuitous advertising alert): Aliasing in Object-Oriented Programming: Types, Analysis and Verification. Lecture Notes in Computer Science, Vol. 7850. Dave Clarke, Tobias Wrigstad, James Noble (Eds.) 

When I was younger I'd report every single typo and grammatical error. Now I just don't have the time. Reporting some is always helpful: pick the most serious ones. Also recommend that the paper be proofread (by a native speaker). For journal papers be more thorough. If it has way too many errors, then is shouldn't have been submitted to a journal, which alone is reason for rejection (IMHO). 

"Penumbra" is a word I selected to describe the concept. It may well be called something else. I found inspiration in mathematical morphology, hence my visual metaphor, but the two worlds are parsecs apart. Is there any useful work there? Or perhaps in the world of rough sets? Can anyone shed light? 

It is not clear to me how induction helps with the lemma you are trying to prove, so I'll take an alternative approach. Firstly, I prove two helper lemmas that break apart and put back together with at the tail, rather than at the start. Based on these lemmas, I can manipulate the result using your definition of to get the desired result. My impression is that the core lemma relies too heavily on the somewhat odd definition of , which possibly allowed me to side-step any inductive argument you had in mind. So, although the lemma you are interested in does not require induction over the transitive relation, the two helper lemmas do. Note: proofs have not been optimised or beautified. The first lemma inverts instances of so that the single step reduction occurs at the end, rather than at the beginning. The proof relies on induction and was undemanding. 

The second lemma plugs a single step reduction onto the end of a transitive reduction to again get a transitive reduction. The proof is by straightforward induction. 

The work on temporal linear logic has been applied in agent-oriented programming and coordination, making essential use of the interpretation of the modalities described above: 

One of the key differences is that logical relations are used as a technique for showing that a class of programs (eg, input to a compiler) correspond to another class of programs (eg, the output of the compiler), whereas simulation relations are used to show the correspondence between two programs. The similarity between the two notions is that they both define a relation used to show the correspondence between two different entities. In some sense, one can think of a logical relation as a simulation relation that is defined inductively on the syntax of types. But different kinds of simulation relations exist. Logical relations can used to show the correspondence between a language such as ML and its translation into assembly language, as in the paper you read. A logical relation is defined inductively on the type structure. A logical relation provides a compositional means for showing, for example, that a translation is correct, by showing that the translation is correct for each type constructor. At function types the correctness condition condition would say something like, the translation of this function takes well-translated input to well-translated output. Logical relations are a versatile technique for languages based on the lambda calculus. Other applications of logical relations include (from here): characterising lambda definability, relating denotational semantic definitions, characterising parametric polymorphism, modelling abstract interpretation, verifying data representations, defining fully abstract semantics and modelling local state in higher-order languages. Simulation relations are generally used to show the equivalence of two programs. Typically such programs produce some kind of observation, such as sending messages on channels. One program P simulates another Q if P can do everything that Q can do, though perhaps more. Bisimulation, roughly, is two simulation relations put together. You show that program P and simulate program Q and that program Q can simulate program P and you have a bisimulation, though additional conditions are generally present. Wikipedia's entry on bisimulation is a good (more precise) starting point. Thousands of variants of the idea exist, but it is a fundamental idea that has been reinvented in more or less the same form computer science, modal logic and model theory. Sangiorgi's article gives a wonderful history of the idea. One paper establishing a relationship between the two notions is A Note on Logical Relations Between Semantics and Syntax by Andy Pitts which uses logical relations, ultimately a semantic notion defined syntactally, to prove a certain property about applicative bisimulation, which is a purely syntactic notion. 

The theorem that this corresponds to is known as Kuhn's Theorem and it states that Every finite extensive game with perfect information has a subgame perfect equilibrium. Basically, each player has a strategy which will result in the best result for them, regardless of what the other player plays. Any algorithm for computing this, such as the min-max algorithm, will potentially have to explore the entire game tree. There will of course be whole subtrees that can be pruned, if for example, the value of the remaining cards is less than the difference between the two players' scores. If you also keep track of the fact that you will see various positions multiple times throughout the game, you can make further, possibly significant, savings (using dynamic programming). 

The programming languages people use day to day don't fit so well into the Curry-Howard correspondence, because their type systems are too weak. To say something interesting using Curry-Howard for imperative programs, one needs to have a more sophisticated type system. The book Adapting Proofs-as-programs pushes this angle with the aim of synthesising imperative programs. With dependent types becoming more and more popular, certainly in research functional languages (Agda, Epigram), the distinction is becoming blurrier. Of course you can do program synthesis/extraction within the Coq theorem prover (and presumably others), which is of course based on Curry-Howard. The Curry-Howard correspondence can also be used in situations where the proofs do not so clearly correspond to programs (or they are not programs anyone will ever run). One example of this is in Proof-carrying authorization. The propositions correspond to statements about who is authorized to do what. Proofs provide the required evidence that a proposition holds, thus an authorization request is permitted. In order to encode the proofs, proof terms are introduced (via Curry-Howard). Proof terms are sent between parties as representations of proofs of the validity of authorization requests, but they are not considered programs. 

There is certainly no disadvantage doing a little more study and some work in the real world before starting a PhD program. Having a broader background is always an advantage, as you have more diverse knowledge to draw on when addressing problems. Working in the real world will make you more grounded, and perhaps will help direct your research to realistic problems (which may then help the world at large). On the other hand, plenty of people have jumped straight into PhD programs and have succeeded. You can readily pick up any material you missed (except perhaps the hands-on lab sessions), by reading one or two books in the area. You'll be doing a lot of this anyway in your PhD, so doing a bit of less related reading on won't hurt. In some sense the actual answer depends upon which country you plan to do your PhD in. US-style PhDs are very different from those in other countries (e.g., Belgium, Australia, The Netherlands). US-style PhDs involve lots of coursework in the early years. Some other countries have no coursework in the early years.