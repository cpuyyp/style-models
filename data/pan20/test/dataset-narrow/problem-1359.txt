I'm working on a XNA project (not really a game) and I'm having some trouble with picking algorithm. I have a few types of 3D models that I draw to the screen, and one of them is a switch. So I'm trying to make a picking algorithm that would enable the user to click on the switch and that would trigger some other function. The problem is that the BoundingSphere.Intersect() method always returns null as result. This is the code I'm using: In the declaration section: ` 

As you see, this code fills some variables that are then used to define the model that will be drawn and the coordinates where it will be drawn. The problem that I'm having is that I don't know how to distribute that file to other computers (obviously on another computer it would have another path)? Do you have some advices on how to do this? P.S I tried to put it in the Content and set the Build Action on None, and I can see the file in the content directory, but when I change it, nothing happens (the models don't change as they should) 

Is it possible for one prefab to contain multiple game objects, preferably in a parent-child relation. I am making a complex enemy that I want to separate into a couple of game objects with separate sprite renderers, scripts and other parameters. This would simplify the code I need to write for each of the elements. I know I can create one (the parent) object, and then instantiate the others in the Awake() or Start() methods, but I'm asking if there is a way to create a prefab that would contain all of the necessary elements (so I can create the entire thing with one Instantiate() call. Thanks 

EDIT In.LightDir is computed for each pixel. It is first computed for each vertex in the vertex shader Out.LightDir = normalize(mul(lightPos - In.Pos, Matrix3));, then it is interpolated across the whole triangle. lightPos is just the camera's position (which coincides with the light): 

I worked around the problem by adding the triangles which vertex/vertices go over the split plane position to the other child too so they exist in both children. Of course, this affects the subsequent split positions slightly but it's not a serious problem. One solution would be to clip the triangles which go over the split position. 

I'm making a software renderer which does per-polygon rasterization using a floating point digital differential analyzer algorithm. My idea was to create two threads for rasterization and have them work like so: one thread draws each even scanline in a polygon and the other thread draws each odd scanline, and they both start working at the same time, but the main application waits for both of them to finish and then pauses them before continuing with other computations. As this is the first time I'm making a threaded application, I'm not sure if the following method for thread synchronization is correct: First of all, I use two global variables to control the two threads, if a global variable is set to 1, that means the thread can start working, otherwise it must not work. This is checked by the thread running an infinite loop and if it detects that the global variable has changed its value, it does its job and then sets the variable back to 0 again. The main program also uses an empty while to check when both variables become 0 after setting them to 1. Second, each thread is assigned a global structure which contains information about the triangle that is about to be rasterized. The structures are filled in by the main program before setting the global variables to 1. My dilemma is that, while this process works under some conditions, it slows down the program considerably, and also it fails to run properly when compiled for Release in Visual Studio, or when compiled with any sort of -O optimization with gcc (i.e. nothing on screen, even SEGFAULTs). The program isn't much faster by default without threads, which you can see for yourself by commenting out the #define THREADS directive, but if I apply optimizations, it becomes much faster (especially with gcc -Ofast -march=native). N.B. It might not compile with gcc because of fscanf_s calls, but you can replace those with the usual fscanf, if you wish to use gcc. Because there is a lot of code, too much for here or pastebin, I created a git repository where you can view it. My questions are: 

Is it possible to place a MenuStrip into a XNA 4.0 project. This project is intended for PC-s only and I already have openFile dialogs, but I cant get the MenuStrip drawn. If this is not possible what would be a good alternative. I've been trying to get find it on create.msdn but it keeps redirecting me to the WinPhone/XBox selection page and I cant access the tutorials at all. 

I have a really weird problem. I made a 3D simulator of an "assembly line" as a part of a college project. Among other things it needs to be able to detect when a box object passes in front of sensor. I tried to solve this by making a model of a laser and checking if the box collides with it. I had some problems with BoundingSpheres of models meshes so I simply create a BoundingSphere and place it in the same place as the model. I organized them into a list of BoundingSpheres called "spheres" and for each model I create one BoundingSphere. All models except the box are static, so the box object has its own BoundingSphere (not a member of the "spheres" list). I also implemented a picking algorithm that I use to start the movement. This is the code that checks for collision: 

I have a problem with the usage of XBox controller in Unity game. I connected the controller to the PC and it works fine, I also managed to use it in the game itself - I'm using "A" button to fire and Start to start the game and they work properly. However the game doesn't recognize the x-axis movement. I'm not sure if the problem is in the input settings or my code. This is how the Horizontal movement is defined in the game: 

I solved this by creating a custom class that creates an object that handles the touch events. In case anyone is curious as to how to go about this: 

Step one is to design your touch events for a 160 dpi device. This translates into one actual pixel for every density independent pixel. I would also suggest using a very small screen size in order to understand exactly how big your touch event areas need to be. Step two is to set up your custom touch handler class to get the device screen density on instantiate and pass this into your constructor. Use: 

... rinse and repeat No mystery. A person cannot both be pushing on the screen and not pushing on the screen. Why do you think this should not be happening? Could it be an error within your 

Depending on the amount of calculations you have going on in your loop you may need to slow it down to 60fps otherwise your animations may be occurring too frequently. 

There are solutions to this question all over the web. Android documentation gives an excellent series of tutorials on this subject. Specifically: 

These same principles can be applied to the 2d textures you are a using for your openGL objects. Although, I have never worked with openFL bitmap manipulation is handled in android using 

Again a call to MotionEvent ACTION_DOWN, and most importantly it moves your sprite the way it is intended. ACTION_DOWN triggers your: 

After that you declare a variable in your engine or renderer class to store whatever event you want to trigger the mouth movement has a occurred, and a counter to keep track of the number of frames: 

` The "spheres" list gets filled when the 3D object data gets loaded (I read it from a .txt file). For every object marked as switch (I use simple numbers to determine which object is to be drawn), a BoundingSphere is created (center is on the coordinates of the 3D object, and the diameter is always the same), and added to the list. The objects are drawn normally (and spheres.Count is not 0), I can see them on the screen, but the Window title always says "Empty" (of course this is just for testing purposes, I will add the real function when I get positive results) meaning that there is no intersection between the ControlRay and any of the bounding spheres. I think that my basic matrices (world, view and projection) are making some problems, but I cant figure out what. Please help. 

Bullets is a global private variable of type Pool. The problem we are having is that when we try to shoot, Unity trhows the Object not set to an instance error. Is what I'm trying possible, or is it like in the regular C# that no crossthread calls are allowed. If it's not possible, what are our options? I know we could probably make a short animation and run that in the coroutine while the main thread instantiates everything. Any advice is very welcome. Thanks in advance. 

All variables are more or less self explanatory, the OnMove is a delegate that just applies the transform to the gameObject in question. The entire code is inside a void method called Move(). That method is called inside an if statement: 

And finally, the fourth matrix is the projection matrix (P). Bearing in mind that the eye is at the origin of the world space and the projection plane is defined by z = 1, the projection matrix is: 

I'm trying to make a real-time GPU (CUDA) ray tracer, and for now I'm tracing single rays, but I've ran into a problem: the BVH. This [PDF]paper has been my inspiration for the theoretical part, and as you can see, the BVH is composed of Axis Aligned Bounding Boxes, however, the stackless rope-based algorithm for the ray-AABB intersection does not take into account overlapping siblings, which occur quite a lot with the AABB creation algorithm I've read about in multiple places on the Internet, which is averaging the centroid of each triangle in the current triangle list and deciding in which child to place each triangle based on the projection of the average on the axis parallel to the longest edge of the parent box. The use of AABBs in the paper indicates that there indeed exists a method to efficiently (in terms of speed) make AABB trees without overlapping siblings. Unfortunately, I can't find such a method. Would someone please describe a fast method for creating an AABB tree without overlapping children? I'd also appreciate it if they'd post pseudocode too. Thank you. 

OK, I have fixed it. I used a more simple shape as per Nathan Reed's suggestion. Turns out the eye was too close to the projection plane. I have set a d of 1000 units and now it appears to render properly. 

I'm applying phong shading onto a single giant triangle, and I'd like the light's coordinates to coincide with the camera's coordinates in 3D space. In order to do this, whenever I update the camera's coordinates, I also update the light's coordinates. However, diffuse and specular lighting don't "focus" exactly at the camera position when I bring the camera close to the triangle, instead they do it a few units too far. The triangle is .5 units below the XZ plane and parallel to it. Here is a picture demonstrating this effect. 

Unity seems to suggest it's a three step process which seem to be missing from your list. If not it could be a driver error. 

Well you touched the screen and release it there are two calls to MotionEvent one with event.ACTION_DOWN and one with event.ACTION_UP 

... and compare them to your now converted numbers from step 1 to determine if the device has been touched in that area. Keep in mind that this results in a touch area that will not grow on bigger screens or shrink on smaller ones. It will always be the same relative size. Kudos to rajarko.com for helping me see the light. 

Very cool app idea! I suppose there are more than one way to handle this problem. First, you could make a sprite sheet for the mouth and use 

I hope this is gives you some logic to play around with. You will still need to do all the standard openGL-ES things. Cheers. 

This will return the pixel translated based on the devices dpi. It works for both x and Y coordinates. At this point you now have everything you need to build your touch events that will scale across all devices. You can take your 

This will return the devices density. For a 160 dpi screen it will be 1, for 480dpi it will be 1.5, etc.. refer to Android Developer for the whole list. Step three is where the magic happens. You take the points of your desired touch location that you designed in step one, and build them based on the actual devices screen density. You do this by taking your density independent numbers (step 1) and multiplying them by the dynamically obtained device density (step 2) at run time. 

As you can see, the method I'm using is not perfect either, as it leaves small gaps from time to time (at least I think that's what's happening). I don't mind using assembly optimizations. Pseudocode or actual code (C/C++ or similar) is appreciated. Thanks in advance. 

After I get the result, I divide x and y coordinates by w to get the actual screen coordinates. Apparenly, I'm doing something wrong or missing something completely here, because it's not rendering properly. Here's a picture of what is supposed to be the bottom side of the Stanford Dragon: 

I'm trying to get an object from object space, into projected space using these intermediate matrices: The first matrix (I) is the one that transforms from object space into inertial space, but since my object is not rotated or translated in any way inside the object space, this matrix is the 4x4 identity matrix. The second matrix (W) is the one that transforms from inertial space into world space, which is just a scale transform matrix of factor a = 14.1 on all coordinates, since the inertial space origin coincides with the world space origin. 

One way I deal with such stuff is I put a member in the base class that describes the type of the derived object. For instance, GameObject can have a member called type which is an enum or w/e. Then, you could do 

Why does adding these two threads slow down my application? Why doesn't it work when compiling for Release or with optimizations? Can I speed up the application with threads? If so, how?