The drawbacks to this approach are that now your writes take longer so that you can do faster reads, and this might not be suitable if the filter that you use in your query on the column isn't predictable. 

It could go something like this. Grab the old data into a temp table. Insert it into the history table then delete from the live table. I'll leave the error handling as an exercise for the reader. 

Any queries that read the table or with isolation level will proceed despite your . More info about lock modes can be found here: $URL$ Personally I'd say that if you have queries reading from the table then they should fully anticipate that they will get bad results so blocking those reads wouldn't be a priority. If your those queries need to return data that is actually correct then they should not use . $URL$ 

When you created the clustered index with the rows in the table were shuffled to be in that order. When you dropped the index the rows were not reshuffled. There is no guarantee that the rows will be returned by a query in the order that they are stored, but often this is what happens. If the order is important then you would add an (and would be well advised to cluster the table on that field too to avoid sorting every time). But don't choose a clustering key just because of that. For a more thorough discussion of clustering and heaps see $URL$ 

So to get to that format I have a query which grabs the distinct values of and by generating a of the values in each level, like this: 

Your table is extremely wide, and every time that you run a query it needs to read the entire table to find the rows needed. There are two ways that you could reduce the amount that must be read, firstly by reducing the number of rows that the DB must read (e.g. by clustering the table by ) but you suggest that most of the time, most of the rows will match the date criteria, so that probably won't get you very far. The other possibility to reduce the size of the data is to reduce the width of the rows. I don't know a whole lot about baseball so I don't understand what those fields are storing, but I can guess that almost all of the fields in your table don't need to store numbers as high as 2 billion. I think I counted 88 fields for about 352 bytes per row. Changing them to or could save 180-260 bytes per row. Along similar lines I think there are 28 fields devoted to ids of umpires, managers and pitchers. Perhaps if this data is infrequently queried it could go into another table with the same primary key; only join the tables when necessary. Maybe some other fields too. If you could do both of these then scanning the table would probably be far quicker which can make up for be fact that your queries seem very difficult to index for. 

I would suggest an table which stores the current appointments for each doctor. We can add some constraints on this table which limit the appointment start times to even ten-minute times (e.g. 9.00, 9.10, 9.20) plus add some other common sense checks like after and doctor can't have two appointments starting at the same time. Assume that you'd also like doctors to only work between 9am and 5pm, because everyone needs some work-life balance. 

After the data is inserted, it counts how many rows in the table overlap the time range of the inserted row. If there are more than 1 (remember, the row has already been inserted so it will overlap) then the transaction is rolled back. 

It is not necessary to distinguish between measures and dimensions when defining your columnstore. All columns are stored in the same manner. All columns included in any query should be in the columnstore, in most cases it is best to put every column in the table into the columnstore. 

Test this and see if progress is being made. Further steps may be necessary but hopefully this is in the right direction. 

Storing in mysql table should be fine. You need to consider how the data is going to be accessed and retrieved when you're designing the table and indexes on it. I'd say that you're likely to be doing queries which return all of the past searches for a particular user (or perhaps even just the most recent by user) so it would probably make sense to cluster the table by and . This design should also help reduce contention if lots of inserts are happening at the same time since each user will be inserting into a different part of the table. 

That looks a little awkward, so if anyone can improve that date logic happy to take suggestions. If a doctor wants a break, then enter the break as an appointment and it won't be available for booking. Note that the table constraints don't enforce non-overlapping appointments. This is possible but it's more complicated. If this were my system I'd think about some system (e.g. trigger) to finally verify that the appointment doesn't overlap with an existing one at the time of insert, but that's up to you. 

You could deal with the issue by specifying the style and doing an explicit from string to datetime rather than an implicit conversion. $URL$ 

Or you could use a to find people with both tags. (Note that the was added to handle the case that a person may have the same tag twice. If that is impossible, it's safe to remove.) 

This is an alternative scheme to the ones already proposed, it's a bit like the summary table, except that it isn't aggregated, just 'partitioned' by player. 

In SQL server you can only add a single user per statement, so your script is about as good as it can get (aside from adding Billy and Bobby to the role). $URL$ 

This is probably not the sort of case where filtered indexes really shine - typically that is when the index represents only a small portion of rows in the table (like, a quarter or less) and in this case the filter predicate matches about three quarters of the table. That's not to say that you wouldn't get gains here, but it might be a bit of a tricky research project to get good designs. (Then again, indexing is always like that.) What about the current indexes on the table. Are they being used at the moment? Are they only being used in queries that have the predicate that you mentioned? If so, then it might be an easy win to convert some of your existing indexes to have a filter condition (instead of adding a new, filtered index). You probably already knew this, but filtered indexes won't be used by where the predicate that matches the filter uses a variable or parameter. $URL$ 

From there, you could build a TSQL string of commands and execute it. (I hardcoded for the schema. Presumably this will never change.): 

Background: I have numerous databases with a large number of VIEW's, and an extremely large number of SYNONYM's. For example, one db has more than 10k VIEW's and 2+ million SYNONYM's. General Problem: Queries involving (and system tables in general) tend to be slow. Queries involving are glacial. I am wondering what I can do to improve performance. Specific Example This command is run by a third party tool. It is slow in both the app, and in SSMS: 

I'm a huge fan of the event-driven approach for a scenario like yours. Meaning, I don't want to sleuth who did it after the fact, I want SQL to tell me when it happens. For that reason, I'd suggest a DDL Trigger. Here's an example: 

Here's an example that can be run manually in a single batch from SSMS. Just make sure to replace all occurrences of "2016" in the script with your SPID. 

I have a few "gaps" in my understanding of external memory pressure. I've found quite a bit of helpful information online, including this info from SQLSkills.com. 

1st Attempt This was slower than all of Erik's queries I listed here...at least in terms of elapsed time. 

Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math. I used the Stack Exchange Data Explorer (Along with and ) to test the queries. For a point of reference, here are some queries and the CPU/IO statistics: QUERY 1 

I'm trying to tweak the "Server Activity" Data Collection set as outlined in this blog post (I wrote the post, btw. Sorry--it's kinda long). It is working in SQL 2008 R2 and also in SQL 2014. However, when I run on SQL 2012, I get this error: 

Here are two queries I have used to compare permissions between database users. The first shows the database roles (if any) to which the database user has membership. The second shows individual GRANT and DENY permissions. 

I'm passing in a non-NULL value for , so I fully understand why I'm getting the error on SQL 2012. Questions: 

There's a lot of moving parts there. But you might be able to make it work for you. I wrote a few related blog posts about TRY...CATCH that may be helpful: The Unfulfilled Promise of TRY...CATCH Enhanced T-SQL Error Handling With Extended Events Part 2: Enhanced T-SQL Error Handling With Extended Events 

I'm having difficulty "optimizing" indexes as a minimally logged operation. Before the index maintenance is performed, the database recovery model is switched from FULL to BULK LOGGED. Depending on the fragmentation percent, each index is the recipient of a REBUILD or a REORGANIZE (or no action is taken). After index maintenance is finished, the recovery model is reverted to FULL. One database in particular is causing me some pain. The datafiles are about 64GB (including any free space). A defrag operation bloated the log file to 38GB, until it filled the logical drive. Then the level-17 alerts started rolling in. I tried duplicating this in a test environment. Numerous attempts were made with different recovery models, index REORG vs REBUILD, different transaction isolation levels, read committed snapshot ON vs OFF, different index fragmentation levels, etc. Index REBUILDs on DBâ€™s in the FULL recovery model always bloated the t-logs. No other testing variations did, however. This was frustrating because I could not duplicate what was happening in production. What am I missing? How can I optimize indexes without bloating the log files? UPDATE 09/23/2015 Not surprisingly, Ola Hallengren's name came up soon after I posted my question. Although I don't use his scripts, I am somewhat familiar with them. For those who are interested, there is a wonderful PASS session video--Ola is the presenter. Near the 48 min mark, an audience member asks a question similar to mine. 

2nd Attempt Here I opted for a variable to store the total number of users (instead of a sub-query). The scan count increased from 1 to 17 compared to the 1st attempt. Logical reads stayed the same. However, elapsed time dropped considerably. 

If the developers have individual logins, you might consider a DDL Trigger. Here's an example for the , , and events. 

Create and start an Extended Events Session: it will capture events, filtered primarily by SPID. Execute a statement in a block. Within (or after) the block, read the XEvents session data. Use the available data to respond to the error(s) as appropriate. Stop and drop the XEvents session. 

This is a sad commentary on the state of software development and deployments that rely on an RDBMS such as SQL Server. The development and/or DevOps teams should know what level of authorization is needed by applications, both at runtime and at deployment time. It's very important to make the distinction between these two. Unfortunately, development and/or DevOps teams never figure this out because {reasons}. So here you are... You've mentioned that you are familiar with fixed server roles and fixed database roles. I don't want to insult your intelligence, but I think it's worth mentioning for those that are not familiar. At the server/instance level, the CONTROL SERVER permission is a much better option than membership in . At the database level, I would never let a non-dba own a database. They connect as -- DENY permissions have no effect. You can deny permissions to a database user that is a member of -- this is also a better option than membership in . But it is still usually overkill. I generally prefer to give a database user membership in , , and . Additionally, I'll GRANT EXECUTE, CREATE SCHEMA, and VIEW DEFINITION. Based on your needs, your mileage may vary. Another thing to note: you cannot DENY permission on DBCC commands. Members of can do a lot with DBCC, and to a lesser extent, members of can too. After all of that, I've still had to give authorization to non-DBA types on a regular basis for deployments. In those scenarios, the requestor had to submit their request to a Change Advisory Board for approval. Membership in was temporary and I always set up a SQL Agent job to remove group membership after an agreed upon amount of time (usually one week). I did this enough times that I wrote re-usable code to quickly create the necessary job, job steps, schedule, etc. Moving on... Whether it's temporary or permanent, you still have to deal with non-DBAs that have elevated permissions. SQL Server has some built-in features that can help you handle events that you might not want to happen: DDL Triggers: these are a great way to handle certain SQL Server events synchronously. Maybe you just want to know an event happened. Grab the and send an alert or log the info to a table for later inspection. If you're more hands on, you can inspect the as events occur and optionally ROLLBACK the bad stuff. It's really powerful. I kept an eye on ALTER_INSTANCE, ALTER_DATABASE, DROP_DATABASE, and a raft of authorization-related events. (Note: ROLLBACK doesn't work with the ALTER_DATABASE event.) EVENT NOTIFICATIONS: some events can't be handled synchronously with DDL triggers, but they can be handled with event notifications. ROLLBACK is not an option, but you can at least grab the for post-mortem analysis. This option involves Service Broker, and the learning curve to get started is much higher than working with DDL triggers. I think it's worth it, though. There's nothing to prevent a member of from "sidestepping" a DDL trigger or Event Notification (other than their own ignorance). But that's a severe offense IMO, worthy of disciplinary action. Another feature you may want to look at is Policy Management. I've not used it myself, but it appears to be quite powerful and flexible. It may also be a bit easier to use as compared to DDL triggers and Event Notifications. I'll let you be the judge of that.