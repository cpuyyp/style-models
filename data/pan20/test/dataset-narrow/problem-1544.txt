This is equivalent to using , but is probably faster. For example to generate a random number that is with , you get a random number from . Now, in 90% of the cases, this random number is smaller than 0.9, and in 10% of the cases it is larger than 0.9. So to get a random number that is with , you can call - which is exactly what they do. tl;dr: Pick a new random number from the range for each hidden unit in each iteration. Compare it to your probability with to make it binary. 

The full answer to your question is yes and no. Let me elaborate: Yes, you can train such a model with SGD: Yes, you can train a model like you described. This is well known under the name "Autoencoder". The goal of an autoencoder is to find a (usually smaller) representation of the input data, which can then be used to reconstruct the data. This looks as follows: Image taken from standford.edu We try to make $\hat{x}_1$ equal to $x_1$. In your code, you add the additional constraint that $W_1$ (the weights from the input layer to the hidden layer) are equal to $W_2$ (the weights from the hidden layer to the output layer). The reconstruction error is usually either the squared error or the cross-entropy. Such an autoencoder can be trained with SGD, as you proposed. So, the answer is: yes, what you propose does work and is known as Autoencoder. No, you can't train an RBM with SGD: In the previous section, I said that training an RBM with SGD, as described in your question, works. This is not the full truth: this is NOT an RBM anymore! As described in the answer by Quittend a restricted Boltzmann machine models the probabilistic distribution. The goal of RBM training is not to exactly reconstruct the input, but to learn the probability distribution of the data. The reconstruction is thus not exactly the same as the input, but is a sample from the same probability distribution. The goal is to assign a high probability to the input data we train on. The probability of an input vector $\mathbf{v}$ is given by $$p(\mathbf{v}) = \frac 1Z \sum_{\mathbf{h}} e^{-E(\mathbf{v}, \mathbf{h})}$$ where $E$ is the energy function, and $Z$ is the partition function (to normalize $p$ such that it is a valid probability). During training, we want to increase $p(\mathbf v)$ for all $\mathbf v$ in our training data, so we have to calculate the derivative $$\frac{\partial \log p(\mathbf v)}{\partial w_{ij}}$$ and change the weights $w_{ij}$ accordingly. This is what we do with the contrastive divergence (CD-k) algorithm, and this is not possible with gradient descent (SGD). tl;dr: Training an unsupervised neural network with SGD exists and is known as autoencoder. An RBM is a different concept, which models the probability distribution and does not strictly reconstruct the input. RBMs can not be trained with SGD. 

which means they create a CNN by "altering between" these layer types (e.g. ). Or with the wording of the first definition, you "place one layer" of each type "between" the other existing layers (i.e. start with a couple of layers, then place layer between two layers, and so on). The final architecture would look somewhat like this: 

This choice mainly depends on what your output represents. Given a vector $\mathbf{x}$, the sigmoid function is given by $$ \sigma(x_i) = \frac{\exp(x_i)}{1 + \exp(x_i)} $$ while the softmax is given by $$ \mathrm{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=0}^N \exp(x_j)}$$ A key difference is that the output of the sigmoid function applied to $x_i$ only depends on $x_i$. The output of the softmax function depends on all elements of the vector $\mathbf{x}$. The sigmoid will squash each $x_i$ into the range $(0, 1)$, which enables you to interpret $\sigma(x_i)$ as the probability of $x_i$. But, if you have multiple classes, e.g. 0-9 in MNIST, each probability is independent, that means you could have probability $p=0.9$ for the digit 1 and $p=0.5$ for the digit 3. This is undesirable if you want to distinguish between multiple classes. However, if multiple classes can appear at the same time, then sigmoid is well suited. Now, the softmax is basically a sigmoid function which is normalized such that $\sum_{j=0}^N \mathrm{softmax}(x_j) = 1$. This means that the output of a softmax layer is a valid probability mass function, i.e. the output of your neural network is the probability of the input belonging to a certain class. So, if you want to classify between e.g. the 10 digits as in MNIST, then softmax is the way to go. If the classes are independent, i.e. it is possible that an image shows a cat and a dog at the same time, then sigmoid is the way to go. 

An easy way to run different versions of multiple frameworks alongside each other is to use Docker. With Docker, you can create a new "Container", kind of a light-weight virtual machine, for each project. Docker handles all hardware abstraction and separates the different Frameworks. Note: To use the GPU within a Docker container, you need to install Nvidia-Docker, as a "normal" Docker installation can not share the GPU with a container. In your example, you could have a Docker container for Digits, and a separate one for TensorFlow - then the two frameworks will be completely isolated from each other. This approach comes with multiple advantages: 

So, this "interleaved" CNN is a network where the different layer types are applied in series, layer-by-layer. As a comparison, in "inception" type CNN, you apply the different layer types in parallel (the graphic below is 1 layer): 

And just to be clear: you run this iteration for every data point in the training set. Actually, you usually make so-called mini-batches of maybe 10 data points, which you run at the same time. Then you calculate the average weight update from this batch. But then, you do this iteration again, and again, and again, until you have finished your training. 

Now, we have the probability $p(h_j=1)$ for each hidden unit $j \in [1,H]$. Now, this is only a probability. And we need a binary number, either or . So the only thing we can do is pick a random sample from the probability distribution of $h_j$, which is a Bernoulli distribution. As all hidden units are independent, we need to get one sample for each hidden unit independently. And also, in each training step, we need to draw new samples. To draw these samples from the Bernoulli distribution, you can use the built-in functions of e.g. MATLAB () or Python (). Note that these functions are to sample from a binomial distribution, but the Bernoulli distribution is just a special case of the binomial distribution with . In MATLAB, that would be something like 

The mode uses the Bayesian regularization backpropagation. This method was presented in 1, which presents a regression problem with the loss function $$ E_D = \sum_{i=1}^n (t_i - a_i)^2 $$ where $t_i$ is the target and $a_i$ is the network's response. The paper proposes to add a regularization term, leading to a loss function $F$ of the form $$ F = \beta E_D + \alpha E_W $$ where $E_W$ is the square of the sums of all network weights, i.e. $E_W = \sum_{i,j} \| w_{ij} \|^2$. The two parameters $\alpha$ and $\beta$ control the weighing of the two parts $E_D$ and $E_W$: For $\alpha \ll \beta$, the network will minimize the loss, without really trying to keep weights low. For $\alpha \gg \beta$, the network will minimize the weights, allowing for some more error. In reality, this means a large $\alpha$ will stop the network from overfitting, which leads to a better generalization at the cost of a larger training error. The key to find a train a model which generalizes well, but still has a low error rate, is the right setting of $\alpha$ and $\beta$. This is achieved by treating them as random variables and finding an optimal setting, using the Bayesian methods presented in 2. (I won't talk about the details on that here, you can find that in the two linked papers.) Finally, the paper presents an algorithm, which calculates the optimal $\alpha$ and $\beta$ in each training iteration. This makes this algorithm generalize really well, especially in the presence of noisy input signals. However, as described, the loss function is a weighted sum between the MSE ($E_D$) and the regularization term ($E_W$). So, in short: you can only use it with the MSE, and not with cross-entropy. You'll need a different training algorithm, you can find a list in the MATLAB documentation, here. References: 

That is exactly what Hinton does in his RBM code: he gets a random number for each hidden unit using , i.e. randomly sampled from the uniform distribution between . He then does the comparison: 

Yes, calculating $\sum_{h} e^{-E(v, h)}$ does include summing over $2^N$ possible configurations of the hidden vector $h$. However you don't need to actually calculate it: If we have the negative log likelihood function $$ \mathcal{L}(W, c, b) = - \frac 1T \sum_{t=1}^T \log\left(p(v^{(t)})\right) $$ where $p(v^{(t)}) = \frac 1Z \sum_h e^{-E(v^{(t)},h)}$, we don't need to calculate $\mathcal{L}(W, c, b)$ directly, instead we want to minimize it using gradient descent. Using $\theta$ as the full parameter vector instead of $W, c, b$ for simplicity, we get: $$\begin{array}{ll} \nabla\mathcal{L}(\theta) &\displaystyle = \frac{\partial}{\partial\theta} \left(- \frac 1T \sum_{t=1}^T \log\left(p(v^{(t)})\right)\right) \\ &\displaystyle= \frac 1T \sum_{t=1}^T \left( \sum_h p(h \mid v^{(t)}) \frac{\partial E(v^{(t)},h)}{\partial\theta} - \sum_h\sum_vp(v,h)\frac{\partial E(v,h)}{\partial\theta} \right) \end{array}$$ The first term of the difference above is the expected value of $\frac{\partial E(v^{(t)},h)}{\partial\theta}$, given a specific visible vector (i.e. a training example) $v^{(t)}$. This can easily be calculated for each parameter $W, c, b$. The second term is the expected value of the energy, calculated over the full distribution of the RBM. Unfortunately, this is again intractable. Here, the contrastive divergence algorithm by Hinton comes in useful: we run Gibbs sampling for a number of steps and estimate the second term using the resulting states $\tilde{v}$. tl;dr: yes, the problem is intractable, which is why we use the contrastive divergence algorithm by Hinton, which is only an approximation but works well in practice.