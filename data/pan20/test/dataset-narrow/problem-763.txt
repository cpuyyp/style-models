Open the zip file. Get the first file. Read the data out of the file, say in lines. For each csv line, write out a new zipfile containing the line. Rotate the file selected (say five zipfiles) using the output of one line. Once you reach a certain size (say 50GB) create a brand new zip file. 

Insert equal or greater sized new USB disk. Using vgextend add the said disk to the existing volume group. Using pvmove switch the extents from the old root partition over to the new USB disk. Using vgreduce remove old old USB disk from the volume group. Remove old drive. 

The kernel will support 2 million connections with any comfort. Node or java will support 2 million connections with any comfort. 

This may just end up overwriting the bad superblock with one of the known good ones, which could be enough to get the entire disk remounted. Then again you may lose key inodes (like your root filesystem inode). Mileage may vary. 

That way you’re getting all the redundancy benefits of raid10, but also gives the benefits of snapshotting the VM for example before a major change giving you rollback opportunities too! Although LVM IS optional 

Make sure you have related and established connections allowed in via the firewall or else the response won’t be able to come back in 

I’d personally go for the 4 x 8TB option. That way you can have it in RAID10. Granted your rebuild times will be longer but 16TB of redundant storage and not losing anything to parity isn’t bad 

It does work, in the sense that you can achieve setting up raid... however there will be zero redundancy and completely invalidates the point of having it. Here’s my advice: 

Try installing the WordPress plugin ‘wordfence’ I use it a lot as it not only tracks live traffic you can block IPs it will also send you emails about missing security updates for plugins (including itself) and the entire WordPress installation. It can also scan the WP files and let you know for dodgy content. You can also manually run head and tail commands on Linux against WordPress files or grep them all, you want to look out for any base64 in the files. That part was more for the clean up, although wordfence will also identify things like this too 

Now, the untrusted user of course has complete ownership of that IP, but you can use IPtables in the parent namespace to do DNATing to the specific port they are allowed to bind to and restrict communication out to only that port. Its impossible for the user to effectively deny service on another port for another user as he has a specific IP that works only from within that users namespace. Note things are more complicated if you use or for name resolution service as the unix socket that is used for inter process communication is invalid in the child namespace. The only way I was able to fix this was to patch these programs to provide a TCP transport and do name resolution services in the parent namespace over TCP. I also wrote a program in C that uses netlink calls to set all the above up. The program is closed source so unfortunately I cannot share it with you. For the record, doing this in Fedora is a bit easier because you can create named network namespaces in it. But EL6 does not provide the functionality to do this. 

If you don’t see a block of unallocated space then your drive is faulty. Before submitting an RMA ensure that your raid card firmware is completely up to date, have enough power (I.e your server PSU is powerful enough to support all your hardware) etc as they’ll make you jump through them hoops before they actually do anything anyway 

If a system has snoopy installed (which logs commands in auth.log) then it would be retrievable from there but also bash history would have it in. Also - where is your script located as it could probably be just read from within the script 

Are queries actually being made? If there is nothing to write from the master than it won’t work. If there was any problem such as network issue etc. Then the slave would stop itself. 

Surely you could just check the password used by the 900 applications in order to determine what it is? Have you checked if they’re stored in plain text? E.g web.config sql connection string? Also, could you not setup a new user and password and deploy it to the 900 applications? I know if I had 900 applications running with hard coded user details I would have a deployment system setup.. 

is used for this purpose and its typically ran from init scripts. Its basically without the PAM integration. 

It looks as though you want FTP to be able to be used for normal users (who have content in /home). There exists a boolean to resolve this problem. You can work this out doing the following.. 

I do not know the answer to this, however if both are behaving posixly correct, they should both behave the same way. If you attempt to read from a socket that is not yet prepared to send you data blocking the stream indefinitely is the anticipated behaviour. Overall, I think your best choice of attack is to expect your command to have completed within a specific time period. If it does not kill it and report you did so. 

Directories only ever grow in size, not shrink. Try moving all those files out into a a temporary directory (like log2) then rmdir the old directory and rename the temp one as the new permanent one. 

Check the terms of service. In fairness you have no legal obligation as far as I’m aware to justify your licensing to them.. however we are not lawyers and for accurate advice you should really contact some legal professionals. I don’t believe they have any right to dictate you install software however this may well be in the terms of service. With regards to the admin access.. don’t they have that already if it’s managed hosting? You should contact the company’s help desk to ensure they actually sent that email BEFORE making any action. A simple who is against your ip will probably reveal who owns the IP and subsequently your hosting provider is and then start a social engineering attempt against you 

Both are great. I would use nginx personally as you’re not running multiple back ends from that perspective (from what I understand) so you don’t need haproxy on that side of things. Just have nginx setup with ssl and proxy pass :) 

This behaviour only applies to POSIX ACL entries. The reason this is here is if you have a folder and inside that folder exists a file, you can acl as rwx (for example) the folder and the file. If the group permissions of the file are rw- (which they might be as a typical scenario) the mask thus gives the acl the effective permissions of rw- even though the ACL explicitly denotes rwx. On the other hand, the directory which nearly always is +x has effective ACL mask permissions also permitting +x. In summary, this mask is basically used to differentiate permission between files and folders for the POSIX ACL set so that a file does not become executable when it should not normally be. 

Is a symlink to somewhere else? Normally you wouldn't put your libraries here, but only as a symlink. If so running might be beneficial here. For the second issue reported below to work you need to ensure SELinux is aware what kind of behaviour you expect from your httpd service. 

Configure NGINX to point to that page for genuine 404 errors. Once that’s done, when you want to redirect your app in places just redirect them to a bogus page A previous answer said it correctly, when you’re pulling your 404 page at it’s genuine place, it will return a 200, because you’ve requested a page that does exist. 

Check you can ping the FQDN. You’re using another dns server and not say google dns. If you can ping it, you can do it. If ping works, check firewall. If you’re running against the FQDN even tho you’re on the same server without a host entry then you’re no longer using the private profile in firewall and need to disable or add an exception for the public profile. 

That does indicate your drive had a problem but it doesn’t necessarily indicate the drive is failing and needs replacing. That is actually saying that the drive has noticed a problem and will try to correct it. If it succeeds... happy days. If not then you’ll need to replace the drive. Offline uncorrectable value is the one you should be worried about. If that one is anything other than 0, then it’s time to replace the drive. Additionally, if you’re using mdraid check it’s not degrading the array as that might cause you boot problems tl;dr don’t worry. If it graduates to offline uncorrectable then worry. If you have raid then you can sit back and relax. 

Install SAR, it provides snapshot of data every 10 minutes by default, but you can change the rate of infomation using the cron job. It will provide lots of useful data, such as memory, load cpu usage and disk I/O stats. 

Your version of php is higher than the available php-devel. Either downgrade php to 5.3.3-22 or find a channel/repo that offers you the later version of php-devel. 

Unfortunately, the best solution would be the ability to checkpoint your tasks. Its a shame that most of the implementations are just not that concrete enough yet. Alternatively, you could wait a couple of years for proper checkpoint/restore to be available in the kernel! 

You need to relabel the folder to be writable by httpd. If http is the only thing (and say ftp or ssh) that will be writing to it then this should work.