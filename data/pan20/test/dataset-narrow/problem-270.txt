Oh, and if you're on SQL Server 2014/2016 Enterprise Edition, your staging table could be in-memory. 

"Efficient" could apply to log file usage, I/O performance, CPU time or execution time. I would try to achieve a minimally logged operation, which would be fairly efficient from a logging perspective. This should save you some execution timeas a bonus. If you have the tempdb space, the following might work for you. 

There are two ways (not mutually exclusive) to improve query performance. First, you could re-write the correlated subquery into a subquery with an aggregate: 

Remember to make the index clustered or the column(s) your returning in the clause. The non-clustered index with the will probably be faster, unless you're collecting every column in the table anyway. If you're running SQL Server 2014/2016 Enterprise Edition or SQL Server 2016 SP1, you could try making the table in-memory. 

The way I understand your question is that you have an existing table with a column that has up until now been populated with manual values, and now you want to (1) make this column an column, and (2) make sure that the starts from the most recent value in the existing rows. First off, some test data to play with: 

As long as you're using a manageable amount of parameters (two in your case), you could probably just a bunch of seek queries - basically all the permutations of search criteria. If you have three criteria, this'll look messy, with four it'll be completely unmanageable. You've been warned. 

A solution that might work for you is using the OUTPUT clause, which spits out all the inserted rows, so you can re-insert them into a different table. However, this puts limitations on foreign key constraints on Table2, if memory serves. Anyway, the solution would look something like this: 

This query will (a) eliminate the Sort operator (which is expensive because it is blocking and requires a memory grant), (b) create a Merge Join (which you could force with a join hint, but it should happen automatically with enough data). As a bonus, it'll also (c) eliminate the Index Scan. All in all, the MIN/MAX query uses a highly optimal index on the Orders table to identify a range of order numbers (included in the clustering key) from a non-clustered index on the date column: 

.. with one exception: Your subquery won't work if it returns more than one record (i.e. if ID isn't unique in the tblPerson table). GROUP BY, on the other hand, is used to aggregate data. That is, to calculate sums, counts, and so on. If you want to know how many orders each buyer placed, you would use GROUP BY along with an aggregate function (SUM(), MAX(), COUNT(), etc), like this: 

I wrote a script a while ago, that does just this. I've posted it on my blog, $URL$ Remember that a Windows user can be a member of a Windows group, and in SQL Server, you can't see those memberships, so you'll have to look at Windows users and Windows groups separately, if you're doing a security audit, for instance. If you want to read up on SQL Server security, there's also a series of posts on the blog on that subject. 

The second query assumes that the primary key of is . Since you haven't specified which rdbms you're running, you may have to tweak my code a bit to make it run. Edit: The part eliminates posts that have any other category_id than 1, 2 or 3. You may want to skip this part depending on if you want to return a) all posts that have categories 1, 2 and 3 or b) all posts that have exactly categories 1, 2 and 3 and no other categories. 

With a grand total of 727, each group should have a score of about 182 for the distribution to be perfect. The difference between the group's score and 182 is what we're putting in the column. As you can see now, in the best of worlds, we should move about 40 points worth of rows from group 1 to group 2 and about 24 points from group 3 to group 0. Here's the code to identify those candidate rows: 

The construct lets SQL Server "loop" over each row in and for each query in that table, it runs an aggregate query on the much larger table, for which we'll optimize with the following index: 

I just recently solved the same issue at a client's, and I blogged about the solution. The short version of it is: I've set up a stored procedure that runs on each of the replicas, connects through a linked server to the primary replica to retrieve login SIDs and password hashes, and applies those to the secondary replicas where neccessary. The SID (the unique identifier of the account) is what connects the login (the server principal) to the user (the database principal), so if you're creating new logins on the secondary replicas, you'll need to bring your own SID with you, so the login and the user matches. The SID can be found in , the hashed passwords in . Those two tables are connected by the column. To address your security concerns: The client application itself never has permission to do this. Rather, I'm running a SQL Server Agent job that operates on a very strict set of permissions. The synchronization doesn't happen immediately, because it's scheduled, so you'll either have to set the job to run relatively frequently or provide some type of DDL trigger like @SeanGallardy suggests. Disclaimer: don't use T-SQL code from strangers on the Internet without testing it first. I'm providing it on a best-effort basis. 

The example is simplified, but it still illustrates what's going on. Your query Try the following, then go through the results, looking for invalid dates like 31st of november, 0th of january, 29th of february on non-leap-years, negative values (I'm assuming is numeric), etc. 

This should give you a nice, efficient seek (range scan, really) on (SoftwareId, SampleDate). I removed the expression and just sorted by the expression instead. It'll make your code a little more readable and perhaps even save you a few microseconds of CPU time. :) Just crossing Ts and dotting Is, here's an index recommendation on as well, although I suspect this table doesn't really contain that many rows: 

The error message is there for a reason. Using on thousands of tables in a single query is performance suicide. Yes, I understand that your reporting application is already in production, but I would still strongly recommend that you to go back to your development team and update the application or, even better, rebuild its logic in a stored procedure instead. As you suggest in your question, create a temp table, run each of those queries sequentially and dump the results into the temp table. Then, compile the report from the temp table. All of this can be done in a stored procedure, which will also cover other important aspects like security and performance. This carries a range of advantages: