will only start databases that have the third parameter in the file set to . Check the value in that file. You also don't need to pass the as an argument; not sure if you actually meant to pass the SID, but anyway, the home is taken from the too. The script itself says that it should only be executed as part of the system boot procedure, e.g. from a script run during boot from an directory. The parameter that's passed in is only used to start the listener; the home for each database is taken from . According to the documentation, is deprecated in favour of Oracle Restart (in non-RAC environments) in 11g Release 2; I guess that means you should use to start the listener and database from then. That also uses the third value to decide whether to start the database. 

You don't need to grant privileges, and shouldn't unless really necessary. You should follow the principle of least privilege. From Oracle's security guidelines: 

I am trying to move data from a csv file into a SQL server database. Some of my values are in the scientific notation. I figured out on how to get most of them converted but for one value I get the Arithmetic overflow error. The value that is causing the error is . If I change the part before the E by removing the 1 so it reads the import works fine. All the other values I need to import work fine. I use a format file to import the data. Below the line for the column that is giving me grief: 

I am writing a script to restore databases from backup. The script should need as little input as possible to fulfill this task (in an efficient way). The restore command takes the parallelism parameter. The idea is to set this parameter to the number of tablespaces that are not temporary tablespaces. I found the db2ckbkp command which will not only verifies the backup file, but also outputs lots of (useful) information. Currently I was planning to run it with -t to get the tablespace information. I just have troubles to interpret the information printed. following the output that is printed for one of the tablespaces. 

That would defeat the whole purpose of encrypting the backup - so, unfortunately, the answer is that there is no way. There is no back door or gated access to get the data, it is encrypted by the certificate of which no one has a copy but you. That's what makes it safe and secure (well, partially). 

I'll add it to my documentation list, and because of this I'll go ahead and answer it here, hoping others may find it. The Overall Logical Steps Please note that this does not cover if the node is acting as a forwarder in a distributed availability group. 

I wouldn't use named instances, default instances will cause less issues in the long run. We'll get to one of those issues... 

For the underlying WSFC you'll need at a minimum: Node1 - IP Address for each unique subnet for each network interface Node2 - IP Address for each unique subnet for each network interface CNO - IP Address for each unique subnet EX: 2 nodes, 2 subnets, 1 interface per node, subnets 192.168.1.1/24 and 192.168.2.1/24 Node1: 192.168.1.10 Node2: 192.168.2.10 CNO: 192.168.1.20, 192.168.2.20 

AFAIK, you cannot force a plan to stay in cache. However, a query can be thrown out of the cache for several reasons. Read a blog about execution plans. It states some reasons why execution plans get invalidated: 

To determine whether it is an temp space or not, the is not suitable, since it is 6 for all of the tablespaces. However the container type seems to be different for temp spaces. is 0 for temporary tablespaces and 6 for all other tablespaces. When I use the parameter, I get another piece of information. 

Another pitfall could be that the query changes. This can happen if the where clause changes (e.g. you filter by date) and you don't use bind Thinking about, you haven't stated that you checked whether the execution plan is still in cache or not. You should query the cache to figure that out. However, since you say your query runs only once a day, the plan might just be expired. The blog actually mentions the formula on how sql server determines when to expire a plan: 

12c's Multitenant model gives you a single container database that manages the resources at operating system level and shares them with the pluggable databases as needed, potentially significantly reducing the total resources the O/S server needs, and hiding/simplifying the management of the plugged-in dtabases. It's somewhat analogous to running virtual machines on a physical server - the physical server needs fewer actual CPUs and less memory than are allocated in total to all the VMs, and the VM management layer controls the balancing and contention of the resources depending on the actual needs and loads of all the VMs. The VMs themselves aren't really aware of that happening, they just use what they need; similarly pluggable databases aren't really aware that the container is managing their resources rather than the operating system. 

Mostly just for my own amusement, you can do this with a materialized view which has a unique index: 

For experimenting it's ideal, in my experience. Of course, it's unsupported, but as you won't have a support contract anyway that's a bit of a moot point. As David said, PL/SQL is integral to the database, not a separate component, so it is available in the VM image. You get some tools too, including SQL Developer, but you might have less friction running that natively and connecting it to the DB in the VM. 

Except for a very specific scenario, this will not work. In all other scenarios you'll lose quorum before you'd ever be able to automatically fail over (which requires... quorum). The best answer would be to have the DR side be manual failover (it could still be synchronous) with the proper documentation on how to force quorum and bring the AG online. You could also invest more into VMWare and use their technologies but that assumes the infrastructure, licensing, and ability to implement those products for a specific service such as this. 

I don't know if it's local to the server (does the FTP program really know who is primary?) or if it is a "local" share/Dfs. If that server can "see" the file, it'll probably try to run as per normal. While I highly doubt synonyms are used, I never want to assume. Additional Thoughts It would make sense to run this with some debug output/tracing. Put some print statements around your internal checks. I can only theorize the code path it might take not knowing the environment. If you don't want to go through that, doing the checks for your agent job steps as I have above will stop this from happening. 

Then attempting to insert an record for a different department with the same username gives a unique constraint violation, although not until you commit: 

Have a look at the package. First set to some big number so the output isn't truncated, and set the SQL terminator so you can see how statements are separated: 

The let you mount and open the database so that it is accessible to users. Clause Use the clause to mount the database. Do not use this clause when the database is already mounted. You can specify to mount a physical standby database. The keywords are optional, because Oracle Database determines automatically whether the database to be mounted is a primary or standby database. As soon as this statement executes, the standby instance can receive redo data from the primary instance. 

A non-CDB is anything that isn't a CDB; that is, any database before 12c, or a 12c database created without the enable pluggable database clause. If you create a non-CDB it isn't Multitentant and is a single-instance standalone like a pre-12c database. 

I guess it has something to do with the fact, that db2 will return an error code of 1 when no records were found. 

The format in the database is decimal(18,9). Any suggestions on how to avoid this error without manually changing values in the source file? To put it into perspective. The CSV file contains more than 2.2 million rows with 154 columns each. Which results in a CSV file size of more than 2GB. Currently I am working with a test file. When the final go live comes. I need to switch over fast. Which means I can not analyze and edit the file for several days. Update I played around with the values a little bit. 

You should be able to create a stored procedure that executes your query to get the table names. Then you loop over the results and execute your effective date query for each table individually. You might even be able to do this without wrapping it into an stored procedure.