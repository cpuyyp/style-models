The number of downvotes on the link that is mentioned in the question could indicate that there are issues with the instructions. Perhaps one could use native commands to backup the database. For example, this answer shows how to backup a postgres database that runs in docker using the pg_dump command. 

There are three different pipelines, i.e. , and If a certain project inside the monorepo changes, different pipelines should be triggered, i.e.: If a commit is done in folder then pipeline should be triggered (). If a change occurs in directory or then pipeline should be run () and if either or dir changes then pipeline should start running (). How to achieve this? Most CI's are triggered by changes in a repository, e.g. one commits to a repo and a pipeline is triggered. There is a 1 on 1 relation. If this is translated to a monorepo that means that 1 change results in 3 pipeline triggers, while in some cases only 1 pipeline should be triggered. 

There seems to be an open issue at the moment. Another user experienced the same issue as well and added the following comment: 

Note: one could use to ensure that the volume is read-only to prevent that multiple containers will write at the same time to the same volume and prevent issues. 

According to this documentation two vault servers have to be run. One of them has to be active and the other passive. In order to run such a setup both vault server need to be unsealed. 

How to record websites that are encrypted using Certbot? This topic has been created in the Google user group. 

The Ansible module index has been consulted, but no module was found that checks whether a port is available? Is there a module that supports this? What does mean? Available means that a port, e.g. is not in LISTEN state. The following indicates that port 111 is unavailable. 

is that it is impossible as one of the features of container orchestration technologies like docker-swarm is auto restart. This means that a container is restarted automatically when it was shutdown. So if the database container goes down, it will be restarted automatically. 

One could decide to use a tool that visualizes the whole chain, e.g. from development to deployment to production. This will facilitate to find (potential) vulnerabilities and indicate the data flow and in what places it is stored. 

I would recommend to create an issue about CD in the ticket system of salesforce. I have googled a little bit and this seems to be their issue tracker. I would prevent to implement CD in salesforce as that will be very cumbersome and will take a lot of time. In my opinion this issue should be solved by SalesForce itself. 

According to me, a DevOps mindset is that one works together and that one focuses on deliverables, i.e. new software live within two weeks. If one wants to create all microservices in advance then a lot of assumptions have to be done and nothing will be put to production, i.e. available to the customer. This happened in one of my projects. The architects was continuously talking about "what if this would happen and this" and so on. Outcome? After one year, nothing was running in production. In my opinion, some companies start with microservices because they have heard about it, but do not know exactly what microservices are. Microservices are not the holy grail, e.g. one could create nanoservices, one should master automation. My point of view about microservices is that one should start with a monolith first, e.g. if you want a webshop just start with it and ensures that it is running in production within two weeks. Secondly, start monitoring it, check if there are problems in certain areas, e.g. more bugs in component X or component Y is extremely slow. If it is clear that component Y is slow, create a plan of extracting it, rewrite it within two weeks, apply autoscaling. I am in favor of the "cheese slicer method". As the cheese itself is a product that delivers value and is running in production and the X=>Y approach is used. Start with a problem and check what could solve this problem, instead of thinking about a solution, e.g. microservices (Y) that could solve a (hypothetical) problem. For your information. I am currently busy with a personal microservices project that consists of two microservices, e.g. an api and a write microservice. I am first solving the impediments and cleaning up my backlog. Off course I have some ideas about next microservices, but I want to put first things first and add value to the customers instead of doing a lot without finishing this microservice project that I have already experienced a couple of times. 

What will if someone in the organization accidentally removed a secret. Is it still possible to recover this? 

One of the options is to configure SSL in NGinx and to put this in front of Jenkins. Are there any other options for securing Jenkins? I am looking for an answer that is focused on best practices that harden the layer 6. 

Autoscaling means that the number of instances increases if the load intensifies. Docker is used in for example Kubernetes in order to autoscale. Are distributed distributions like NixOS suitable candidates for autoscaling as well? 

In summary, in k8s it is possible to assign certain privileges to a pod. Assign the privileges that are required by a certain container. There is an open issue: $URL$ 

Now I wonder how to define constants in Ansible. My first impression was to use the same convention as in Python as Ansible is written in python, but when one checks the variables information page then this does not seem to be a best practice. Current approach $URL$ 

I wonder whether this is possible and what is the advantage of this over starting a build when merged into master. 

This page indicates something about regions and zones, but does not show any resource usage. I think that the resource usage of a zone is not visible. 

Thanks to @BoomShadow I am able to parse the API programmatically and this shows the latest version in a consistent matter. Handy if one would like to execute the same rest call: $URL$ 

It is possible to force that multiple keys have to be entered in order to unseal the vault. How does this work in practice? If someone has entered a key and there is another key required to unseal the vault should one ask a colleague to enter another key and how should this be done to prevent that colleague A or someone else sees that key? 

This github discussion was found. It seems that it is allowed to build a docker image while a certain ENV does not have been set. How to ensure that the docker build fails if an ENV has not been set? 

Yes, DMAIC makes sense. If processes like CD need to be improved it is important to have a baseline. Therefore the Measure step is useful. The team investigates the status of process X, e.g. CD by documenting the current status. 

Why create this Q&A on DevOps?: The reason is that the ultimate goal is to close an issue in bitbucket by Jenkins when a pull request has been merged. If updating issues using the API works then that will hopefully work as well. Conclusion/question: It is possible to update all BRI at once? 

After a spending a lot of time I decided not to continue with this approach and decided to use Jenkins helm. 

In this section Hashicorp compares with other solutions. How does Hashicorp's Vault compare to Lastpass? Attempt to answer the question It is possible to login to LastPass from a web browser, create secrets and share these with other users. There does not seem to be an audit log. Imagine that a password of one of the users is retrieved one could retrieve a password from a database without knowing it. 

If for example 1000 identical containers that use a different environment variable have to be deployed, multiple could be added to a script, instead of copy-paste multiple times in one docker-compose file. 

Based on the comment of @Tensibai the first impression was that there were insufficient resources, but further analysis using indicated that in this case the livenessProbe check failed: 

Follow-up to this Q&A. How to ensure that a certain minimal code coverage is reached in Golang? E.g. if the CI runs, the build should fail if the code coverage is less than X percent. In Java, one could use Jacoco and define a minimum code coverage. Other language like Python support minimum code coverage as well. One option is to simply create a bash script that parsers the output of , but I would like to know if there is a off the shelf solution. 

If one uses in k8s then it is possible to assign an IP to an app so it could be accessed on the www. It is also possible to configure an ingress. This is basically a rproxy. What are reasons to use an ingress over ? 

An advantage is that it is clear that all configuration resides in one repository, but how to trigger a certain build if a certain customer folder change and deploy it to the environment of the customer? It is possible to create a custom groovy script in Jenkins that does some filtering or use branches in certain VCS repositories like bitbucket, but I wonder whether there are more efficient solutions. Preferred answer A table that gives an overview of the pros and cons: