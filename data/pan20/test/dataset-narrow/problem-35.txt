If there is no divergence (i.e. all threads in a wave take the same branch) newer GPU's can skip all the work within the if-branch. If there's divergence, then code in both branches is executed, but thread execution mask basically defines which threads execute code in which branch (code in non-executed branches for threads are effectively NOPed out). This is basically the same as predicated branching but happens dynamically based on divergence. On GCN architecture at least the branching itself is basically free or at least very cheap (handled by a separate unit running parallel to ALU for example), but something to keep in mind is that branching also tends to increase GPR pressure, which in turn lowers the occupancy. Low occupancy means there can be less threads in flight at once which influences GPU's ability to hide memory latencies. This may or may not be an issue in your shaders depending on how they access memory, i.e. you should see performance increase along with increased occupancy in shaders with heavy memory access. So it's still good to optimize out branching where reasonable, but I think it's one of the things good left to the optimization stage where you need to squeeze max performance out of your shaders. 

This doesn't really answer the question but is a workaround. Beware it's hack time :) Instead of using the and I'll be using the method and a simple to sample the texture. As I am using a sphere mesh, I generate 2D UVs like this. 

I am working on a specific blur effect that implies several behaviours. But before I would like to know what you think about those blurring algorithms according to performances and quality : Kawase blur, Box blur and two pass Gaussian blur. Next are the effects I am currently working on and I would be glad to have your thoughts on those on the proper way to achieve this. Here is a schematic view followed by my questions. 

The Nine Patches algorithm is a really nice and powerful solution if you are using textures but I ended up by using a fully procedural solution. I am just drawing a round box like this. 

I am trying to implement the Parallax Refraction effect explained by Jorge Jimenez on this presentation: $URL$ and I am facing some difficulties. Here is a screenshot of the interesting part. 

Lowering register pressure doesn't necessarily give you any performance boost though. I recently went through this exercise myself on GCN architectures (for a simple ray tracer) and reduced register pressure so that it increased occupancy from 2 to 4, which had no impact on performance. It's generally a good idea to reduce the pressure if you need to hide memory latencies, but it really depends on what the real bottlenecks in your shaders are. Since you are working on a GPU ray tracer, you might get better improvement in performance by paying attention to the divergence of threads and try to improve this instead. For example by trying to group rays based on location and direction to reduce divergence in KD-tree traversal per wave. I'm not familiar with the paper you are referring to, and you may already have considered this though. 

For light sources with larger solid angle and where the shadow caster is relatively closer to the light than the receiver, you get notable soft shadowing effect. So if you render larger light sources closer to the shadow receiver it's important to handle soft shadows properly for realistic lighting. Even with the Sun which has quite small solid angle, you can still see fairly large penumbras from tall buildings. PCSS is fairly simple algorithm for implementing contact hardening shadows, but it has some notable issues. PCSS fails when you have soft and hard shadows intersecting (blocker search fails), so it's not a good algorithm to use when large penumbras are needed. See in the below image where shadows from the box & triangle intersect. This artifact is particularly disturbing when animated. 

As I am working with Unity, I found an interesting way of calculating the parallax offset inside the engine. It's not physically based but it gives better results than the classic parallax refraction. 

I discovered that some engines use derivative maps instead of tangent space normal maps. After some reading, it seems to be a really awesome way to replace tangent space normals but are there some disadvantage using them ? Why still continue using tangent space normals ? Is it possible to compare both with advantage and disadvantage ? 

I am using this Kawase Blur, to apply blur onto my buffer. But I would like to apply this blur in an uniform way, no matter the distance of the objects from the camera. I guess that I need to use the depth buffer but how should I change the offset according to depth ? 

I am using the Inigo Quilez SDF function to generate a round box shape. The aim is to be able to control the shape scale, smoothness and roundness. I've achieved a pretty good result but I am facing an issue when I change the scale. Here is my current code. 

I'm assuming you mean $A$ is object$\rightarrow$world matrix and $B$ is your camera$\rightarrow$world matrix? If that's the case, to calculate object object$\rightarrow$camera matrix you calculate: $$C=B^{-1}*A$$ In general your math is correct, i.e. the order of matrix multiplication is reversed for the inverse calculation. It's just that normally these transformations are combined to a single 4x4 transformation matrix which does rotation, scaling and translation, and then you just work with that single matrix instead of it being decomposed to separate rotation & translation matrices. Anyway, if you want to decompose the matrix multiplication, based on the above you would do: $$A=T_A*Ry_A*Rx_A$$ $$B=T_B*Ry_B*Rx_B$$ $$C=Rx_B^{-1}*Ry_B^{-1}*T_B^{-1}*T_A*Ry_A*Rx_A$$ 

You can also use importance sampling for faster convergence by focusing more rays to the directions that matter more. I.e. by focusing rays based on BRDF (more towards the BRDF spike using probability density function) or to the light source, or get best of the two worlds and using multiple importance sampling. 

Ray marching is a ray tracing method where you take multiple steps along a ray to find intersection with geometry or to perform integration of in-scattered light from participating media (fog, clouds, water, etc.) along the ray. Signed distance fields doesn't really help you with participating media rendering since it's a method of finding the ray intersection instead of helping with volume integration. So when you perform ray marching along the ray for volume integration, for each step you need to calculate incoming light to that point and evaluate the "phase function" to calculate how much of that light scatters towards the eye. This phase function can be simple isotropic function (i.e. constant $\frac{\rho}{4\pi}$) or more complex anisotropic function such as Mie phase function shown below, depending on the modeled participating media. 

I am currently trying to implement a specific directional light type. This light type has been used in the game INSIDE and is called orthogonal spotlight (aka local directional light). I assume that this is a directional light which behaves like a spot light and have a squared or rectangular attenuation but I have some difficulties to integrate it in my deferred pipeline and get the general concept of this light type. Classical directional light : 

You can simply avoid the 3rd dimension if you don't need it. This method comes from the Distance function article from Inigo Quilez. 

I also have another question about the Physically based Refraction part which is not really well explained and I am missing how some values are calculated such as , or . If anyone could provide some explanations about those values. Thanks a lot. 

I think that I got a solution but I would gladly know if there are some optimizations possible. My UVs and local coordinates values are corresponding. I mean that they are in the same range value. That said, I can use my XY vertices values for sampling the occlusion texture. The main problem is that the local coordinates are dependent to the rotation of my object which doesn't solve anything... To solve that I am converting my local coordinates into world space using a objectToWorld matrix (inverse of current world matrix). Then I am converting it back into object space using the inverse TRS matrix but without taking into account the rotation of the object. It's a bit hacky and I think that I could avoid some steps but this is working. Any advice is welcome :) 

Here's a non-exhaustive list of Vulkan and DirectX 12. This is cobbled together using criteria similar to that of Nathan's. Overall both APIs are surprisingly similar. Things like shader stages remain unchanged from DX11 and OpenGL. And obviously, DirectX uses views to make things visible to shaders. Vulkan also uses views, but they are less frequent. Shader visibility behavior differs a bit between the two. Vulkan uses a mask to determine if a descriptor is visible to the various shader stages. DX12 handles this a little differently, resource visibility is either done on single stage or all stages. I broke the descriptor set / root parameter stuff down best I could. Descriptor handling is one of the areas that vary greatly between the two APIs. However, the end result is fairly similar. API Basics 

Barriers on both APIs break down a bit different, but have similar net result. RenderPasses / RenderTargets 

* **RootSignature - not an exact equivalent to VkPipelineLayout. DX12 combines the vertex attribute and binding into a single description. Images and Buffers 

If you now pick two colors with the same beige background (e.g. green and yellow), and feed them into the above alpha blending equation you get 4 equations with 4 unknowns, which you can then solve. I.e. $$f_1=c_1*\alpha+b_1*(1-\alpha)$$ $$f_2=c_1*\alpha+b_2*(1-\alpha)$$ $$f_3=c_2*\alpha+b_1*(1-\alpha)$$ $$f_4=c_2*\alpha+b_2*(1-\alpha)$$ where $f_1$ & $f_2$ = brighter & darker green colors respectively, $f_3$ & $f_4$ = brighter & darker yellow colors respectively, $c_1$ & $c_2$ = the unknown green & yellow colors respectively you try to solve, $\alpha$ = alpha you try to solve, $b_1$ = beige background color, $b_2$ = unknown background color in the center. Then you can repeat this process for all the colors, or because you solved $\alpha$ you can use this information to solve the rest of the colors simply by: $$c=\frac{f-(b_1*(1-\alpha))}{\alpha}$$ 

Additionally you need to take the extinction coefficient of the media into account, which is spatially varying for heterogeneous media, for calculating transmittance and to properly "attenuate" the in-scattered light as it's being blocked by media between eye and the point (Beer-Lambert law). This approach assumes homogeneous media between steps and for proper physically based integration you could take a look at SIGGRAPH 2015 presentation "Physically-based & Unified Volumetric Rendering in Frostbite" by Sebastien Hillaire. To know how much light reaches a point in space for each of those ray steps, you can use path tracing. This can get quite expensive though since light can take random paths in the media until it reaches the point. This is known as "multiple scattering", which is analogous to indirect illumination. When performing the integration over the sphere for each step along the ray, you can use Woodcock tracking to get unbiased results for heterogeneous media. Also if you use Mie phase function you should use importance sampling as the phase function has very strong forward peak. 

But I want to avoid the deformation behaviour when stretching, I want to keep margins when stretching my texture or simply cut it in the middle and stretching it. Here is an illustration below. 

I am trying to scale and repeat a Cubemap with Latitude-Longitude mapping layout just as you would do with classical UV mapping but without any interesting result. This should be used as a skybox. This comes from the fact that the coordinates are in 3D space and we can't apply this simple formula How would you handle such features : scaling which involves tiling and offsetting. 

This gives the standard tiling/stretching and offset behaviour when you tile/stretche a clamped texture as you can see in the image below. First is normal, second is offset and last is stretching. 

But this document lacks a bit of explanations especially the second part with the Physically based Refraction. Here is what I've achieved for the moment. This is the simple Parallax Refraction effect and, as you might notice in the screenshot, there is small glitch at grazing angles in the iris when the Parallax Scale value is too high. This is caused by the parallax calculation but are there some tricks to avoid or minimize such issue ? I don't want to go deeper with parallax mapping for the moment, so I don't want to use Steep Parallax Mapping or Parallax Occlusion Mapping. 

Next about your second question of importance sampling being inefficient due to band-limited signal - It simply means that the incident radiance is sampled with finite resolution (cubemap resolution) and thus higher frequencies have been filtered out. But because importance sampling biases samples to important regions of the function (peak of the GGX BRDF) this region may be sampled in unnecessarily high frequency thus wasting samples. 

You can determine by calculating line-plane intersection. Your line starts at and has direction , and plane . This can be done as follows: $$x=\frac{(V-P)\cdot N}{N\cdot D}$$ is the distance from along to the intersection point with (assuming both and are unit vectors) 

3D object surface is 2D domain and can thus be parametrized with a 2D function. In case of a unit cube you could parametrize the surface for example by mapping each 6 faces to a 2D image strip. So if you know two points of the ray that intersects the cube, you can define it with 4D function using the parametrization 

Verbiage about command pool/allocator from Vulkan/DX12 docs state the behavior in very different words - but the actual behavior is pretty similar. Users are free to allocate many command buffers/lists from the pool. However, only one command buffer/list from the pool can be recording. Pools cannot be shared between threads. So multiple threads require multiple pools. You can also begin recording immediately after submitting the command buffer/list on both. DX12 command list are created in an open state. I find this a bit annoying since I'm used to Vulkan. DX12 also requires and explicit reset of the command allocator and command list. This is an optional behavior in Vulkan. Descriptors 

** RootParameter - not an exact equivalent to VkDescriptorSetLayoutBinding but similar thinking in the bigger picture. VkDescriptorPool and ID3D12DescriptorHeaps are sort of similar (thanks Nicolas) in that they both manage allocation of the descriptors themselves. It should be noted that DX12 only supports at most two descriptor heaps bound to a command list at any given time. One CBVSRVUAV and one sampler. You can have as many descriptor tables as you want referencing these heaps. On the Vulkan side, there is a hard limit to the max number of descriptor sets that you tell the descriptor pool. On both you have to do a bit of manual accounting on the number of descriptors per type the pool/heap can have. Vulkan is also more explicit with the type of descriptors. Whereas on DX12 descriptors are either CBVSRVUAV or sampler. DX12 also has a feature where you can sort of bind a CBV on the fly using SetGraphicsRootConstantBufferView. However, the SRV version of this, SetGraphicsRootShaderResourceView, does not work on textures. It's in the docs - but may also take you a couple of hours to figure this out if you're not a careful reader. Pipeline