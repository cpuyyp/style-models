(As you seem familiar with other virtualisation technologies like Virtual Box, you might remark that these technologies can also conveniently address the three points above. Nowadays, there is quite a small spectrum of virtualisation technologies, and we can compare the question of their popularity in certain contexts with the popularity of computer languages: it depends probably of the technical merits of each individual solution, but also to a lot of factors that I will just label “chance”.) 

Using a customised bash profile is possible, using the assignment but I would consider this esoteric enough to puzzle the maintenance programmer. 

This list could be extended ad lib and while studying the various reasons leading to some third-party resource to disappear is much more entertaining as it sounds, it first of all has for us a disastrous consequence: the build-pipeline is broken. What the is the best way to cover the two following cases when building docker images: 

Complex commands can be turned into function so that they can be fed to the . It is also possible to chain calls of several functions, but maybe not very desirable. (Using more esoteric features of the shell, it is certainly possible to make the accept complex commands, but it is in all aspects preferable to wrap these complex commands into functions.) If we want to ignore Advice #2, the resulting Dockerfile snippet would be 

There are several options that can be used here. Maybe one of the easiest is to install each function as a real script in some directory that is added to the path. Here is how to do this: First, we choose a path name where to store all these functions, it can be a a directory in our project, where other utility scripts used in our Makefile live. We call it nvm_install_dir Then we write a script which use the name it is called as to trigger the right function, and create as many file aliases to as there are functions in the script. Note the leading underscore, hinting at the “private” character of the script. This script can go along the lines of 

to consider a minimal set of automated tasks we want to perform. If we want to change something about how the test verb is implemented, it is easy to visit the folder corresponding to that artefact in the appropriate repository and then discover the jenkins-specific automation items that needs to be updated. Instead, if the automation recipes are structured around technical solutions, then we need to figure out of the blue that jenkins is involved in the test procedures and find there the artefact related automation items. In complex situations, the organisation around technical solutions makes updates very hard, because we have to know a priori all the technical solutions involved in some service to update them accordingly. For instance a repository containing the code for a website and a micro-service “a” could have the following sub-directories dedicated to operations: 

This is a correct hitch which leads us to ask ourselves why Docker proves to be popular as a ad hoc packaging system, while it is not intended to be one. (See above.) The “official” packaging system from a given distribution is just a possibility among many others to install software in some computing environment. There are many other sources available, like community-specific package managers such as npm or opam, port trees like pkgsrc and plain source code distribution. From this perspective, it is easy to understand the success of Docker as an ad hoc packaging system: 

where the is the base64-encoded version of . My third advice is for people who wants to limit the size and the number of layers at the possible cost of longer builds. 

be easier to read than the corresponding yum invocation found in a shell-script? Furthermore, anybody who had contact to AppleScript dies laughing when they read “human readable automation”. 1.2 No special coding skills required – What is coding if not writing formal specifications? They have conditionals, variables, so, how is it not coding? And why would I need something I cannot program, that would henceforth be inflexible? The statement is happily inaccurate! 1.3 Tasks executed in order – Well, maybe some codegolf aficionados are aware of languages that execute tasks in disorder, but executing tasks in order hardly looks exceptional. 1.4 Get productive quickly – Skilled shell programmers are productive now. This counter-argument is just as serious as the initial argument. 2. Ansible is powerful A popular salesman trick to sell artefacts is to fool people into believing they will acquire the “power” of these artefacts. The history of advertisement for cars or isotonic drinks should supply a convincing list of examples. Here Ansible can do “app deployment” – but shell script surely do, “configuration management” but this is a mere statement of the purpose of the tool, not a feature, and “workflow orchestration” which looks a bit pretentious but no example goes beyond what GNU Parallel can do. 3. Ansible is agentless To populate the column, they wrote in three different manners that this only needs ssh, which, as everybody knows is a daemon and has nothing to do with these agents pervading the world configuration management! The rest of the video The rest of the video introduces inventories, which are static lists of resources (like servers) and demonstrates how to deploy Apache on three servers simultaneously. This really does not match the way I work, where resources are highly dynamic and can be enumerated by command-line tooling provided by my cloud provider, and consumed by my shell functions using the pipe operator. Also, I do not deploy Apache on three servers simultaneously, rather, I build a master instance image that I then use to start 3 instances which are exact replicas one of the other. So the “orchestrating” part of the argumentation does not look very pertinent. Random documentation step 1: Integration with EC2 EC2 is the computing service from Amazon, interacting with it is supported by some Ansible module. (Other popular cloud computing providers are also provided): 

(In your specific situation, the would be more complicated, adding the nodesource repositories and some helper packages such as apt-transport-https.) It is therefore really possible to use Debian packages and Docker simultaneously, however … 

This kind of approaches turns out to scale very well when working with Makefiles. It is therefore tempting to use it again when writing complex ansible systems. On the one hand, it seems to fit quite naturally in the language used by ansible. On the other hand, it generates quite a payload of “skipped” tasks in ansible logs, which raises a red sign to me. Is the approach I describe a sensible way to group data structures with the processes using it? (That is, to implement reutilisation!) If not, what would be a better way? 

A build-pipeline often requires to access third party resources, aside from the source code of the artefacts it is meant to build. Each of these third party resources introduce a risk factor in builds, for instance: 

Here we use to remove any path element from the name under which our script is called. We install that script under and run once the following utility script: 

which is not so easy to read and maintain because of the obfuscation. See how the shell-script variant outs emphasis on the important part while the chained- variant buries that part in the middle of noise. 

Both version are essentially identical, the bulk of the payload is the enumeration of the initialisation values in a YAML or JSON structures. Random documentation step 2: Continuous Delivery and Rolling Upgrades The largest part of this guide does not display any really interesting feature: it introduces variables (IIRC, shell scripts also have variables)!, and an Ansible module handles mysql, so that if instead of searching after “how do I create a mysql user with privileges on X Y” and end with something like 

A docker image is actually a linked list of filesystem layers. Each instruction in a Dockerfile creates a filesystem layer that describes the differences in the filesystem before and after execution of the corresponding instruction. The subcommand can be used on a docker image to reveal its nature of being a linked list of filesystem layers. The number of layers used in an image is important 

There are two usages of the word “artefact” and one makes source code an artefact while the second makes it not being an artefact: this can indeed be quite confusing! “artefact” as a concrete thing, vs. an ideal thing – This meaning is the common meaning of the word “an object made by a human being, typically one of cultural or historical interest” and is not technical jargon. Here is an example in technical context: When you debug a software, you learn something about the software. It is often a valuable investment to turn this learning into a software artefact, like a regression test. Otherwise this learning will be forgotten and the the effort made to acquire it will be wasted. In this meaning, source code is considered an artefact. “artefact” as something produced by a recipe – This meaning uses the popular image of the alchemist using some esoteric recipe to produce a magical device, often called an artefact. It is technical jargon used to distinguish between the source code, which corresponds to the recipe in the alchemist's metaphor, and anything derived from that source code, which corresponds to an artefact in the alchemist's metaphor. For instance I just automatised the artefact production for my plop-fizz program, now source tarballs, signature files, DEB and RPM packages can all be instantiated in just one command! This meaning does not recognise source code as an artefact, as the term is used to denote what is produced from this source code. 

It is a good practice to pack the nvm_install procedure in a real function instead of just inlining its body in the script, as it gives better testing options. (It is easier to comment out the call to nvm_install than the body of the function, if we want to experiment with the script.) Ater this, the directory pointed to by nvm_install_dir is populated with aliases to that are executable and delegate their work to the corresponding function. We only need to add this directory to our PATH when running make. A second approach would be to generate pseudo commands for each nvm functions, with the following script: 

Cloud technologies shifted the frontier between hardware and software so that many technical operations formerly exclusive citizen of the hardware world are also subjects of the software realm. Shared computing environments might be as old as computers themselves1 but cloud technologies could popularise them by offering convenient and familiar metaphors to interact with them: cloud users reserve an instance, a complete computer or a mimic, whereas older shared computing environments have all possible set of unwieldy limitations and “your program have to be uploaded on that FTP server, will run in environment X (usually with 10y old version of whatever software you want to use), for at most 60 minutes” could sound familiar to former or actual users of computing centres. The practical consequence of this shift is that deployment procedures can now be represented by software artefacts. (Deployment procedures are the instructions telling how to setup an infrastructure, with databases, web-servers or whatever belongs to this infrastructure, together with the network where they run on.) Geared with these new lenses, manual maintenance of servers pretty much looks like manual patching of production code – which is only in very rare occasions a desirable thing. Manual maintenance is susceptible to introduce discrepancies between the systems actually running in production and the code describing these systems, which in turns mean irreproducible behaviour and impossible bug analysis, dual bug fixing, and other calamities. The immutable server pattern is just the transposition for cloud operations of the above mantra, according to which we should avoid manual maintenance of running programs. Instead of manually configuring severs, the immutable server pattern recommends to automatise this configuration. Implementation flavours While the general idea of the immutable server pattern is quite clear, there is a lot of implementation nuances. For instance, some approaches suggest to not update servers at all but to systematically replace servers instead. This is because updating yields situation where a deployment consists of servers having been started at several distinct times and having gone through several, distinct, updating processes which implies an inhomogeneous set of servers and can lead to subtle differences in how servers handle their jobs. A second popular variation point is the discipline regarding remote access to the servers. Some like to disable completely remote administrative access to servers, in order to guarantee that manual maintenance never happens. History note To the best of my knowledge, the term “immutable server” has been popularised by Kief Morris but the idea itself is much older. In 1999 FreeBSD jails already popularised the idea of fully automatising the configuration of disposable computing environments, this is how I started to implement the “immutable server” pattern many years before I heard this name to describe this technique. Immutability, in the guise of physical immutability based on CD-ROMS, has also been a popular measure to manufacture trusted computing systems. This is not to be mistaken with the immutable server pattern. 

I never used Ansible but since a few weeks, I try to figure out what good Ansible could be in comparison with shell scrips–Which proves, at least in my case, that the haunting ad-campaigns they run are effective! After many unsuccessful attempts–which proves how their documentation fail at answering one of the most obvious question–I think I finally got it: Now, let's watch the introduction video and go randomly as a potential new user through the introduction material to Ansible ans let's compare it to what a skilled shell programmer can produce right-off the shelf. My conclusion is that over shell scripting, Ansible essentially offers 1. The possibility of checking that a system agrees with a desired state, 2. the ability to integrate with Ansible Tower, which is a paying system that seems to include monitoring abilities. In some important cases, like when implementing the immutable server pattern, the point 1 is probably not very useful, so the list, is rather thin. My conclusion is that the benefits offered by Ansible over shell-scripting, as the documentation present them, could be sensible in a few handful of optimistic cases well covered by available modules but are small or even hypothetical in the general case. For a skilled shell-programmer probably, these benefits are most likely counter-balanced by other aspects of the trade-off. But this maybe only proves how bad the introduction material is! The quick start video: There is a quick start video. It starts with a page claiming that… well these are not really claims, these are bullet lists, an artefact commonly used to suspend critical judgement in presentations (since the logic is not shown, it cannot be criticised!) 1. Ansible is simple: 1.1 Human readable automation – Specifications are technical documents, how could 

Unaddressed concern: the maintainability The introductory material from Ansible ignores the question of the maintainability. With essentially no type system, shell-scripting has the maintainability ease of JavaScript, Lisp or Python: extensive refactorings can only be achieved successfully with the help of an extensive automated testsuite – or at least designs that allows easy interactive testing. That said, while shell scripting is the lingua franca from system configuration and maintenance, nearly each programming language has an interface to the shell. It is therefore totally feasible to leverage the maintainability advantage of advanced languages, by using them to glue together the various the bits of shell-configuration bits. For OCaml, I wrote Rashell that essentially provides a hand of common interaction patterns for subprocesses, which makes the translation of configuration scripts to OCaml essentially trivial. On the side from Ansible, the very weak structure of playbooks and the presence of a meta-programming feature make the situation essentially as bad as it is for shell scripting, with the minus points that it is not obvious how to write unit tests for Ansible, and the argument of introducing ad-hoc a higher-level language cannot be mimiced. Idempotency of configuration steps The documentation of Ansible draws the attention on the necessity of writing idempotent configuration steps. More precisely, configuration steps should be written so that the step sequence a b a can be simplified to a b, i.e. we do not need to repeat configuration step. This is a stronger condition than idempotency. Since Ansible allows playbooks to use arbitrary shell commands, Ansible itself is unable to guarantee that this stronger condition is respected. This only relies on the programmer's discipline. 

While there is some tiny overlapping region Docker and the Debian packaging systems essentially solve two very different problems: 

There are several plausible strategies that can be considered here. For instance using a proxy and configuring the build system to route all its requests through this proxy . A second, cheap, possibility would be to wrap curl calls to store results in a local cache. (In this question I would consider sharing the cache among several units of the build pipeline as a question of secondary importance.)