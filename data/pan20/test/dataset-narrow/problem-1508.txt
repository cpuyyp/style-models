I have developed a machine learning model with Python (Anaconda+Flask) on my workstation and all goes well. Later I tried to ship this program onto an other machine where of course I tried to set up the same environment but the program fails to run. I have copied the program to other machines where it also runs smoothly. I cannot figure out what is the problem in the failed case (both the program code and the error message is copious so I am not able to present here) but I'm almost sure that it should be something with the different versions of the dependencies. So, my question is that given an environment where a certain program runs well, how can I clone it to another where it should run well also. Of course, without the cloning of the full system ;) 

I have generated a common Sklearn model for text classification which I want to make accessible in the cloud (there is no provider preference) as an API. So far the closest solution that I managed to find is in this tutorial but it seems quite complex (getting the venv.zip dependencies package at the beginning is unclear, for instance) and specific (in my case NLTK and an external Stanford segmenter is involved for preprocessing and I cannot figure out where to put all these modules and how to invoke them). Is there a decent and robust way to solve this challenge? 

How could I split randomly a data matrix and the corresponding label vector into a X_train, X_test, X_val, y_train, y_test, y_val with Sklearn? As far as I know, is only capable of splitting into two, not in three... 

I could find max pooling in most -- actually, exactly all -- concrete CNN implementations I've seen. However, I see that there are several other kind (min, average, global average pooling etc.) but I cannot see what they differ on and what would be the best criteria to select them at application. Has anyone a clue? I have learnt that pooling is a sort of downsampling where we can preserve the locations of the original image that showed the strongest correlation to the specific features we searching for. So far, so reasonable. But what is the point of find weakest correlations with min pooling, for example? 

I am familiar with the concepts of Supervised and Unsupervised Learning but recently Reinforcement (Reinforced?) Learning also popped up before me a couple of times. Could anyone give a hint what is it, what is the main difference from the other two (or from the Unsupervised type, specifically), with some good example? Is it a promising alternative worth to explore or just some hyped niche curiosity? 

I cannot interpret this message; isn't Dense layers' node numbers are independent of the previous layers and free to define? What is not passing here? Addition: the whole error message is: 

I often read that in case of Deep Learning models the usual practice is to apply mini batches (generally a small one, 32/64) over several training epochs. I cannot really fathom the reason behind this. Unless I'm mistaken, the batch size is the number of training instances let seen by the model during a training iteration; and epoch is a full turn when each of the training instances have been seen by the model. If so, I cannot see the advantage of iterate over an almost insignificant subset of the training instances several times in contrast with applying a "max batch" by expose all the available training instances in each turn to the model (assuming, of course, enough the memory). What is the advantage of this approach? 

I have two corpora (in the form of list of sentences); one is the "original" one, the other is a version of it with some minor changes (with contractions expaned, for instance): 

I try to construct a classic querying system where I find the most probable candidate text for a query by computing cosine similarities of TFIDF vectors of normalized text of possible answers. This works quite well if the query and the candidate texts contain identical (normalized) words. So, the following question: 

My first question is, what is the difference between the two groups (g-something and p-something)? Both group mentions "GPUs along with CPU", but no further clue for Deep Learning usability. My second problem is that I have been running my job on g2.2 and g2.8 as well, and while the task processing toke quite long time to run, the workload of the GPUs was relatively low (20-40%). Why didn't the framework increase the workload if there is spare processor capacity? Is is necessary/possible to paramter/set anything to optimize the work? 

No problem in this case. I guessed that I should install the missing package in the tensorflow environment as well, but when I try to, I get informed that the package in question has already been installed and nothing happens: 

Silhouette measures BOTH the separation between clusters AND cohesion in respective clusters. Intuitively speaking, it is the difference between separation B (average distance between each point and all points of its nearest cluster) and cohesion A (average distance between each point and all other points in its cluster) divided by max(A,B). It is a value between -1 and 1, the higher the better (negative value means that the point is more closer to the nearest cluster than to its own, which is quite a problem). 

I try to evaluate several NA imputation methods with supervised approach: I clone my original data frame with no NAs, artifically insert NAs into the resulting Data Frame and apply imputations to the latter. Now, I'd like to evaluate the imputations by comparing the imputed new DFs with the original one. I wonder what would be the best metod; is there any distance method for instance that I could apply to the original/imputed DF pairs? (My DF contains only numeric data, but a solution that handles factor variables as well would be especially handy). 

Either way, is it necessary to pad the label sequences to the maximum possible length? How I decode the model output to normal, readable format? 

I have a quite understandable request of extracting information (invoice number, invoice data, due date, total etc.) from scanned invoices (the digital format is image, not PDF), preferably in Python. The good thing is that the necessary information is more or less certain to exist on the page, and the (regexp-like) textual format of these is also tend to be consequent. The downside on the other hand is that the layout of the invoices are very diverse. I have played with the following possible approaches: 

I study the possibilities to increase Tesseract OCR accuracy by training with the usual way (jTessBoxEditor and command line). I can more or less follow the workflow (albeit the lack of detailed official documentation is a bit frustrating) but I still have some fundamental questions. 

I try to build a NN classifier on the well-known MNIST image database with Sklearn's Grid Search according the following: 

How does Keras' fit function calculate when validation set/validation split are NOT defined (I understand that the default values are None/0.0 respectively, so being not defined is practically default)? One always need a reference set to evaluate model performance... 

my model has no predict_proba function. Why not? Is it because of the multiple-output nature of the model? 

I have a good general understanding of the role and mechanism of convolutional layers in Deep Learning for image processing in case of 2D or 3D implementations - they "simply" try to catch 2D pattern in images (in 3 channels in case of the 3D). But recently I bumped into 1D convolutional layers in the context of Natural Language Processing, which is a kind of surprise for me, because in my understanding the 2D convolution is especially used to catch 2D patterns that are impossible to reveal in 1D (vector) form of image pixels. What is the logic behind 1D convolution? 

I have studied the activation function types for neural networks. The functions themselves are quite straightforward, but the application difference is not entirely clear. It's reasonable that one differentiates between logical and linear type functions, depending on the desired binary/continuous output but what is the advantage of sigmoid function over the simple linear one? ReLU is especially difficult to understand for me, for instance: what is the point to use a function that behaves like linear in case of positive inputs but is "flat" in case of negatives? What is the intuition behind this? Or is it just a simple trial-error thing, nothing more? 

I try to detect the probability of common authorship (person, company) of different kind of source code texts (webpages, program codes). My first idea is to apply the usual NLP tools like any token based document representation (TF-IDF or embeddings) and computing similarity on these but somehow I find this approach a bit clumsy. I want to detect "handprints" (characteristic comment and abbreviation style, folder structure, used 3rd party tools, order of elements in the code etc.) that seem out of the scope of this approach. Moreover, I cannot find place for any proper machine learning here. Clearly, finding weights for the any future quantitative features would be nice but this similarity task is not classification/regression, so how to define the target? Clustering seems to be a better tool but we cannot define as many categories as potential authors. Could you kindly suggest any more reliable method? Does any literature exist for this topic? 

I usually try to form my ANNs with classic fine-tuning approach but I recently learned that there are different "predefined" networks specially for certain tasks. Is there a good summary about these? Are they really perform better than home-made ones? 

I tried to define a custom metric fuction (F1-Score) in Keras (Tensorflow backend) according to the following: 

I struggle to interpret the Keras coding difference for one-to-many (e. g. classification of single images) and many-to-many (e. g. classification of image sequences) sequence labeling. I frequently see two different kind of codes: Type 1 is where no TimeDistributed applied like this: 

I use Keras-Tensorflow combo installed with CPU option (it was said to be more robust), but now I'd like to try it with GPU-version. Is there a convenient way to switch? Or shall I re-install fully Tensorflow? Is the GPU version reliable? 

So far, so good. My problem is that I want to find this answer in the case of questions with the same meaning but slightly different synonym wording, like: 

I try to solve a multi-character handwriting problem with CNN and I encounter with the problem that both training loss (~125.0) and validation loss (~130.0) are high and don't decrease. I use the following architecture with Keras: 

I want to get an igraph representation of a full graph with these nodes in R. As far as I know igraph's function cannot take DF as input and I cannot really want to create an intermediary DF with all possible conection combination. Any clever idea? 

I'd like to get some rendering of the corresponding sentence pairs where IS some difference between the sentences. Is there any elegant solution to this? 

I have installed TensorFlow on Linux (Anaconda) by following the documentation which states that one should create and activate a virtual environment . So far, so good (albeit it is not entirely clear why is this virtual environment is necessary when I want to incorporate TF to my existing env). But when I activate the tensorflow environment I observe that several packages are unavailable in the new environment while available outside the environment at the same time: 

I'd like to perform NLP analysis on Wikileaks US Diplomatic Cable Leaks documents ($URL$ preferably as Python's NLTK3 corpus od Mongo DB documents. I couldn't find any option for download these in any raw text format, so I'm afraid I'm forced to apply some kind of scraping I guess, but I'd be thankful if anyone would give a clue for some simpler solution, if exists any. 

I guess this is because originally there is no "None" factor level in the column, but is it the true reason? If so, how could I add a new "None" level to the factor? (In case you would ask why didn't I convert NAs into "None" in the phase: in other columns NA really does mean missing data). 

I have a data frame with both numeric and categorical columns and I want to apply function to all categorical columns (print out count of level occurances per column). Is there an elegant solution to this, perhaps with ? 

What is the problem here? The fact that my f1_score function inputs are not Tensorflow arrays? If so, where/how can I convert them correctly? 

Usually the tutorials cope with only one manually corrected new document. I find it weird that updating the out-of-the box version with only a handful correction has a lasting effect on accuracy; this seems to contradict the nature of machine learning where one single further training instance usually don't influence the overall outcome. How is it explained? In case of a bunch of documents with fix layouts where Tesseract tend to make the same errors in the same place in each document am I supposed to correct only one document or the more still the better (so I should use batch training)? Is my understanding correct and at the final phase with my custom corrections are simply "added" to the default? Or is it a brand new "branch" which is specialized only for the manually corrected font(s)/errors? Is there a possibility to switch between the original and the updated version? Could an update decrease accuracy? The process strongly focuses on font types, we even should incorporate the font name into the files. Apart from the fact that in case of an incoming document from external source one has usually no idea what is the name of the used fonts, I cannot decide what is the proper usage in case for example of a document with several font types? I should restrict the corrections in one font type in one "boxing session" and file and focus on another font in another round? Or am I supposed to correct every errors irrespective of the font differences? 

I am currently studying Deep Learning based Machine Translation systems but I'm not sure in my understanding the logic of the process. I understand that the source and destination language translation sentence pairs must be represented as word2vec vectors, but why is it necessary to apply two (encoder-decoder) Recurrent Neural Networks? My first idea would be applying only one RNN, where the input is the source language examples (in the form of word2vec vectors) and the output is simply a word2vec sequence of the destination language. Why is it necessary to use another RNN? My additinal question is if this system is flexible enough to cope with synonimes, word order variations and other disambiguities? Is it capable of approximate the correct meaning of a new to-be-translated source language sentence? And last but not least: how could one evaluate such a model where many translations can be correct at the same time? 

Now I want to extract the "spike values" (over a certain threshold say 15,000). In this case there is fifteen. How could this be done with Python? (There is no predefined number of spikes but the threshold is a reliable filter value.) 

I understand that one should standardize and normalize the test data (or any "unlabeled" data) with the training mean and sd. How can I implement this in R language? Is there a kind of "fitting" to the training set and a kind of applying to the test data? 

An additional question: I have read that PCA is a kind of MDS (or vice versa), apart from the fact that the former focuses on variance, the latter on keeping distance. Am I right that the two "converges" somehow (in case of visualization with the two first component, for instance)? 

I'm a bit confused the proper usage of Embedding layer in Keras for seq2seq purpose (I'd like to reconstruct the TensorFlow se2seq machine translation tutorial in Keras). My questions are the following: I understand that Embedding layers turn word values in a sentence into fixed-dimension-long representation. But I observe two distinct usage of Embedding layers: one on one hand (like this tutorial on Keras Blog) utilizes external pre-trained word2vec vectors via the parameter: