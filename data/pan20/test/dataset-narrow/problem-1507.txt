Since you had not provided the tag on which language you are using. I am providing a sample solution in python with pandas for loading the data and transforming. 

As noted many times by the writers of pandas, the ideal size of memory for analyzing with pandas is around 5-10 times the load of data you are giving. That being said, if you can afford to load data is chunks for pre-processing and dump only the columns that are needed, I recommended option to load data in pieces (check option ). as you mentioned might be good to go with. But there is which is built around stack of and for distributed work loads which has support for and . But I never followed anyone who was successful in using it in production (or atleast mentioned that they had used it). May be some of the people here can vouch for it. There is another library called to load the data way out of memory as the author suggested in one of his presentations. It keeps on serializing data onto disk. Hope I summarized it well. 

Explanation Get the original grouped counts as mentioned before the first edit but this time you need to join with the combinations that are missing to get the zero counts. Use to get all combinations of gender and rating and right join it with original grouped frame on and to get merged which has values if no count is present and then use method to fill it with zero. The only loophole is if a rating say is not there in the original data, there is no combination that follows it. Hope this helps. 

If this is not what you want, comment. Hope this helps. I would recommend using online regex matchers like here. They have short and sweet references which really help us get to the context. 

which is true, the first matrix,an all zero matrix created and the second, is the updated one. Edit: After substituting when $i==j$, the resultant matrix still has four elements but this time with $(0,0)$ and $(1,1)$ indices being $0$. So the result is 

Logistic regression can work on multi-class. Frankly it is not big different from binary classifier. The question is how does it find the coefficients for each class which you had displayed. By default to find the coefficients for a single class, it goes with . So vs remaining all classes. It gets coefficients iteratively by minimizing cross entropy. So ultimately you will end up with a matrix having a size of . Coming to the interpretation part of the question. To interpret the importance of a single feature, you can do it as - Given the coefficients of every other feature to be same (in comparing a class with another) including intercept, the class that has the feature with highest coefficient has greater chance of engulfing the new point. This being said the features having and contribute good for a data point to belong to class . But it does not make much sense to interpret each coefficient individually. Hope I made some sense. 

Hope this is what you want to pull. Edit: As mentioned, I did not account for zero values. You need to do one more step additional to get what you want. Finding the combinations that are missing and then joining it. The one liner solution would be 

Yes, of course. Assuming that you have used sklearn's OneVsRestClassifier and so you have a decision function for example a Support Vector Classifier with say linear kernel. Use to change key to , default is . Use this in the OneVsRestClassifier classifier and then go with the inbuilt function like 

Normally the cases in which you use a Regression model is when you want to predict a continuous value from a set of given independent variables. E.g : Let the following values be of the type [independent_variable, dependent_variable] or simply $[marks,height]$ and the values be $[2,0],[3,2],[4,5],[1,1]$. You fit a line or curve through these values ($[2,0],[3,2]$ etc.) and then see the case when a value of $[10,y]$ is given or marks of $10$ are obtained, what can be the $y$ (height) value from the fitted line or curve you had modeled. Take a look at Linear Regression for the above type. Classification model is used in the case when you got a set of independent variables as in the previous case but the dependent value used in training is not continuous value but tells what class the value belong to. E.g : $([2,1],fail),([3,2],fail),([4,5],pass),([1,2],fail)$. Here [2,1] belongs to class $fail$ etc. So later time when a point say [7,8] is given, you will be finding which class (pass or fail) it might probably belong to. For example SVM's for this case create a hyperplane(a multidimensional plane) and based on where the points falls in the space, it is going to find the class with some probability. Simply, choose Regression if the dependent value is continuous else choose Classification if the dependent value is a class. 

You are looking at a Classification problem. Logistic regression, Decision trees, SVM. Any of the above can solve the job for you. But selecting the best model depends upon how good it is able to predict the test set. Use cross-validations and see which model can give you better accuracy. So split you test and train set accordingly. You don't expect the model to predict, if you haven't trained it. See stratified sampling for starters. Though your predictors might be categorical or ordinals, they can be treated as numericals. You have built in functions like predict_proba to find class probability. The references for this can be found in above links. But go through how they work and what is the policy for selecting best plane in any classification, so that you don't feel like you are using a black-box. Since you are saying that you are a beginner, pandas will be a very useful module for reading data into dataframes and mending it as you like. Hope this clears something. 

This answer is from my view point of how I would insert distance component into restaurant recommendations. Firstly, I don't feel it is right to insert this distance component into cosine similarity because it is just a innocent dot product which says how similar vectors are and nothing more. But sorting order of recommendations can be changed by using a simple formula like Sort order based on the values $r_i*(1+e^{-b(dist)})$ or any similar formula for that reason. where $b(.)$ is the bucketed value for the distance between the restaurant and customer which can be found from latitude longitude position of the same by using Haversine formula and $r_i$ the original restaurant rating. This does not change the restaurant rating but merely changes the sorting order based on user location. Example : Let [(A, 3.5, 2 miles), (B, 4, 10 miles)] be a sample list of elements of tuple type (Restaurant, rating, distance) Our buckets are say 0-2 miles is 0 and 2-5 miles as 1 and 5-10 miles as 2 etc. So A and B compound rating comes to $7$, $4.5$. So A will be on top. I hope this kind of helps. Any edits giving some cool ideas and good references are always welcome. 

The data on the curve are bumpier than the roads in my country. So I think you should start by smoothing the curve. There are many smoothing filters like from the simplest median smoothing to Local Regression models like LOESS. There are some parameters to tweak. Take a look at the example. Finding the local maxima. Python's numpy has an implementation for this and this should help. 

And assume that your activity went so good that everyone rated you a 5-star. Then correlation turns as $NaN$ as denominator goes to zero as no $Y$ can be explained by $X$. Hope this helps. 

Adding on top of @El Burro's answer, most of the training/testing proof of concepts of model making happens on manipulating which provides easy functionalities like chaining operations, broadcasting, filling missing values etc and is one such library. It has datatype inferring too and it uses python stack which is fast manipulating arrays. Other than that, as you asked if you want to check format of incoming data that is being passed to the model. You can use here like (this is just a demo that you can achieve all kinds of stuff, there might be a better way to pull this off) 

If you are trying to see whether the survey had made any positive effect on the people, modeling a relationship between before and after with correlation is a fair approach while avoiding some edge cases. Correlation looks whether $X's$ and $Y's$ are linearly correlated or how one variable is influenced by other. But I guess what you might be looking for is just something simpler like average increase in rating after the activity. For example 

You are going to the positive to the gradient while you should be doing is going to the negative direction of it 

Your approach seems fair enough. Create a low dimensional vector of text features or if your corpus of text is small (like in comments) then make a representation or based word scoring (after cleaning) and use them as normal features by flattening them with other features. The thing is if you go with word embeddings and when you want to un-blackbox the model you had created based on text, it can be hard as each individual dimension in does not hold any meaning. So start with bad of words and move to embeddings and see the metrics. 

fills the values with a given number with which you want to substitute. It gives you an option to fill according to the index of rows of a or on the name of the columns in the form of a python . But is a god in filling. It gives you the flexibility to fill the missing values with many kinds of interpolations between the values like linear (which does not provide) in the example provided below and many more interpolations possible. For example 

I am getting that you want extract numbers with either '$' or even not without a dollar. Why are not using just [0-9]+ for that field? The results will be like 

Instead of diving into LDA directly, I would be rather start with simpler ones like TF-IDF and see whether it can extract keywords from each class/blog. Recently I got into this kind of problem where I need to extract topics out of tweets and I got fruitful results with TF-IDF being a part of my method. I would treat each blog as individual data point rather than merging them, so that documents can be clubbed based on similarity of words obtained and then extracting the topic out of them. At the end you can use views to see the average views each topic has got. Well you got tools like LSA which constructs a matrix based on word counts. This matrix is reduced by SVD which can be computationally time taking on matrices of huge sizes. So before trying any of the bigger methods, do try simpler ones and if the results are unsatisfactory, approach other methods. Hope it helps. 

The result has basically two elements with $(0,1)$ and $(1,0)$ indices being $1$ and $1$ and shape is just $(2,)$ when flattened which does not make sense as matrix is still of size $(4,)$ . So python output is basically giving all the values in the matrix with index. If I am rightly understanding what you are assuming, since the matrix is sparse, you are expecting it to give only the values where the elements are non-zero, well this does not do that. Hope it helps. 

No need to use all the features. That is the cool part of having an ensemble. If one tree who is using a small set of data with some set of features is unable to use the feature, some other tree will use it. If the point is a true outlier, the score that is calculated later will offer the due reflection. 

If the number of values belonging to each class are unbalanced, using stratified sampling is a good thing. You are basically asking the model to take the training and test set such that the class proportion is same as of the whole dataset, which is the right thing to do. If your classes are balanced then a shuffle (no stratification needed here) can basically guarantee a fair test and train split. Now your model will be capable or at least enough equipped to predict the outnumbered class (class with lesser points in number). That is why instead of just calculating Accuracy, you have been given other metrics like Sensitivity and Specificity. Keep a watch on these, these are the guardians. Hope this helps. 

Hope this helps. Edit 1: From your comments, the order of probability returned for you is the order how classes are registered in class attribute. You can check the class order from the attribute . If your classifier class instance is on which you applied , then is the order of the classes. This is the order the spits the probability. Most of the times the order is ascending if classes are ordinals. 

A model is said to have overfitted when it performs awesomely on the training set (error's low), but when tested on test set, it fails badly. This might be due to bad sampling distribution over our training and tes divisions, so that model might not have seen this kind of data or due to using many irrelevant features in the samples than we were supposed to etc. You cannot really say that the cat is dead until you open the box. So until you test, you would never know. 

If you want to include '\$' sign, then '\$' can be repeated zero or once or by typo more, then regex would be 

Edit: You can use your old with to find the distance from the hyperplane and convert them to probabilities like 

Unfortunately, it seems like there is no parameter incorporated into . Calculation of anomaly score is just based on the depth each point settles to and by the average path length. The only way to tune in a bit is by using contamination which calculates the threshold needed to set for anomaly score. To achieve the granularity that was given in the original paper using to detect a cluster of small points, using a lot of estimators may solve the problem (still depends heavily on how you sample data from smaller cluster into a lot of estimators). But if that small cluster of data is very less in number, I don't think this idea works and there is nothing much we can do from the current implementation in sklearn. Hope this helps. 

As @spacedman correctly mentioned, this will answered quicker at StackOverflow. But you can use this to create a dictionary like this. There might be a better way but this is a quick work around.