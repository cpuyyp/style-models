In proof complexity the use of Gröbner bases has been proposed by Clegg, Edmonds, Impagliazzo to refute CNFs. There are cases in which this proof system outperforms Resolution exponentially but it does not seem to me that there is a real performance improvement for general instances. It is also true that many of the lower bounds for Resolution hold for Polynomial Calculus (a proof system based on Gröbner bases). The exceptions usually are built for the characteristic of the underlying field. This means that working in $GF(2)$ can help you on some formulas but not on others. Yet Polynomial Calculus has not been studied as much as Resolution, thus well tested heuristics are not available. See also this this for application in cryptanalysyis (I don't know very much about that). 

I'm not sure that I understand your question. The way I understand it leads to the following construction which is an invertible function from $\{0,1\}^n$ to 3CNFs: Input: $x \in \{0,1\}^n$ ($x$ is indexed from 0 to $n-1$) Output: a 3CNF with $\frac{7}{3}n$ clauses (assume 3 divides $n$) For $0 \leq i < n/3$ pick input variables $x_{3i},x_{3i+1},x_{3i+2}$ and add to the final formula the seven clauses on these three variables which are satisfied by the input. The output 3CNF is clearly satisfiable only by the assignment. This procedure is well defined for every input and it is invertible. Unfortunately these formulas are easy to recognize, so they are easy to solve. Another RANDOM process is to add to the formula random clauses among the ones which are not falsified by the given assignment. The formula will stay satisfiable, but no other assignment will satisfy it if you add many generated clauses (I guess $100n$ would be more than enough). Formulas generated in this way are usually hard to distinguish from purely random 3CNFs with a similar clause/variables ratio (which are unsatisfiable with high probability). 

It is known that first order logic is too general to be decidable. Adding axioms with special meaning (e.g. expressing notions such as necessity/obligation, provability, etc.) leads us to modal logics but some of them (especially multidimensional fusions or products of them) are undecidable too. As I understand, then undecidability generally is created by quantifiers - it is quite hard to prove of refute formula, that should be valid in all worlds (if accessibility relation allow them). The undecidability can be handled by several approaches: 1) lowering the expressivity of the logic or considering the less expressive fragments; 2) following ideas of max-SAT problems; 3) moving to approximate reasoning (probabilistic, fuzzy-logic, etc.). All these approaches has this drawback: they tries to reformulate or make less relevant the original problem that was created as conscise and clear model of the real world problem. Maybe there is better way how to approach such undecidable situations - simply by adding additional axioms that reflect the real-world situation. E.g. axiom can be added that the number of variables is bounded in the logic (there can be different axioms depending on how we model the reason why there are only finite number of instances of some class in the world - e.g. bounded resources, restricion by law, etc.). So - the question is - is there some research trend that investigates the modal logics with aim - what axioms should be added to them the regain the decidability of logics (and according - the suitability for them for automated reasoning tasks, for use in autonomous systems)? 

It is known that many logic problems (e.g. satisfiability problems of several modal logics) are not decidable. There are also many undecidable problems in algorithm theory, e.g. in combinatorial optimization. But in practice heuristcs and approximate algorithms works well for practical algorithms. So one can expect that approximate algorithms for logic problems can be suitable as well. However the only reseach trend along these lines I have managed to find is the max-SAT problem and its development was active in nineties. Are there some other active research trends, workshops, keywords, good references for the use and development of approximate methods for modal logics, logic programming and so on? If automated reasoning is expected to gain prominence in the future applications of computer science then one will have to be able to go beyond undecidability constraints of logics and approximate methods or heuristics can be natural path to follow, isn't it so? 

I notice that no one is giving the "small" tutorial for GIT, so I'll try to cover it. GIT is faster and superior to SVN, but maybe it is easier for you to get an SVN account on a server at your university, since SVN is well established. Also may of your collaborators would know how to use it. Even if you collaborate using SVN you may want to use GIT for your own local versioning (I do!). First bit of warning: GIT is very powerful and for basic usage is only slightly harder to use than SVN (e.g., one option to be added in the command line; two steps commit for central repository). Second bit of warning: GIT has the philosophy of considering a set of changes to be atomic (a $\Delta$ as they call it) even if the set spans several files. Also in GIT you have the notion of local repository and central repository. GOOD: You can work offline. BAD: You need two steps commit to a central server. Basic commands assuming you already have a repository 

There are some relatively recent papers by Emanuele Viola et al., which deal with the complexity of sampling distributions. They focus on restricted model of computations, like bounded depth decision trees or bounded depth circuits. Unfortunately they don't discuss reversible gates. On the contrary there is often loss in the output length. Nevertheless these papers may be a good starting point. Bounded-Depth Circuits Cannot Sample Good Codes The complexity of Distributions 

Resolution is a scheme to prove unsatisfiability of CNFs. A proof in resolution is a logical deduction of the empty clause for the initial clauses in of the CNF. In particular any initial clause can be inferred, and from two clauses $A \lor x$ and $B \lor \neg{x}$ the clause $A \lor B$ can be deduced as well. A refutation is a sequence of deductions which ends with an empty clause. If such refutation is implemented, we can consider a procedure that keeps some clauses in memory. In case a non-initial clause must be used again and it is not in memory anymore, the algorithm should must it again from scratch or from the ones in memory. Let $Sp(F)$ the smallest number of clauses to be kept in memory to reach the empty clauses. This is called the clause space complexity of $F$. We say say that $Sp(F)=\infty$ is $F$ is satisfiable. The problem I'm suggesting is this: consider two CNFs $A=\bigwedge_{i=1}^m A_i$ and $B=\bigwedge_{j=1}^n B_j$, and let the CNF $$A \lor B = \bigwedge_{i=1}^m \bigwedge_{j=1}^n A_i \lor B_j$$ What is the relation of $Sp(A \lor B)$ with $Sp(A)$ and $Sp(B)$? The obvious upper bound is $Sp(A \lor B) \leq Sp(A) + Sp(B) -1$. Is this tight? 

Recently such themes as semantic web, modal logics, business rules have seen increased interest as research topics in computer science (alhough many of then have more than 80 years of history), but are there any open source projects (even better if they are actively used in commercial applications as well) that use these technologies and where one can participate and get new challenges for research? I have found a lot of research projects (e.g. satisfiability solvers, logic programming environments) but at present I don't know many succesfull applied applications. There have been few reports on some of them in the jornal of "Theory and Practice of Logic Programming" - mostly about solving scheduling problems with logic programming methods. But maybe there is some open source projects as well? Sometimes it seems to me that modal logics (semantic web is part of them) and logic programming gives false impression about their power and their generality has few applications, e.g. the concrete algorithms of integer programming or scheduling are more powerfull and ready for applications than general methods suggested by the former ones. 

I am acquinted with the basics of such notions as logic programming, monotonic and non-monotonic reasoning, modal logic (especially dynamic logic) and now I am wondering - does logic programming provides anything new to any logic? As far as I understand, then (at least in dynamic logic) the logic programming refers to the formalization of state transitions (by actions, i.e. - logic programing can be understood just as logic about actions). But the same notion "logic programing" seems to be in use even in domains, where there is not state transition, just exploration of one state (or set of states - in case then the initial set of premises are vague enough to describe more than one state). It seems to me that some authors simply use the notions "logic programming" for describing the procedure how to evaluate the query (I am reading currently about defeasible logic programming). But such procedure (although practical indeed) does not add anything new to the underlying logic. E.g. there is notion of "rational closure" (e.g. used for adaptive logics; just the consequence set for some set of premises) which should contain all the possible knowledge about state and therefore all the possible results of "logic programing" (if it is indeed perceived just as state exploration, derivation). So - the question is - does logic programming provides anything new to the logic and does every logic (to be completely understood and readied for applications) need to have its own logic programming? Maybe I am just missing the point... Just for reference I find the following works interesting about this subject, if there are more along this line, then it would be great to hear! 

I'm sure you know the following paper, but I put a link to it because other readers may be interested: Interpolation by Games This paper is an attempt to use the communication complexity framework to show lower bounds for cutting planes. The protocol is used to produce an interpolant circuit for unsatisfiable CNF: $$ A(x,y)\lor B(x,z). $$ Player $A$ gets input $a$ and $y^a$, player $B$ gets $b$ and $z^b$. If there is a shallow tree-like proof in cutting planes then the two players have a communication protocol such that 

The merge is automatic as long there are no simultaneous edits on the same parts of some files. If the merge fails you working directory remains in a "merge state", which means that you have to fix the conflicts and then you have to commit the merged copy. If you still have unmanaged conflicts in you files then the commit would fail again, no garbage committed. 

You need to understand that $\mathsf{CSP}$ problems have a structure that generic $\mathbf{SAT}$ problems do not have. I will give you a simple example. Let $\Gamma=\{\{(0,0),(1,1)\},\{(0,1),(1,0)\}\}$. This language is such that you can only express equality and inequality between two variables. Clearly any such set of constraints is solvable in polynomial time. I will give you two arguments to clarify the relation between $\mathsf{CSP}$ and clauses. Notice that all that follows assumes $\mathbf{P}\not=\mathbf{NP}$. First: constraints have a fixed number of variables, while the encoding of intermediate problems may need large clauses. This is not necessarily an issue when such large constraints can be expressed as a conjunction of small ones using auxiliary variables. Unfortunately this is not always the case for general $\Gamma$. Assume $\Gamma$ to just contain the $\mathsf{OR}$ of five variables. Clearly you can express the $\mathsf{OR}$ of less variables by repeating inputs. You cannot express a larger $\mathsf{OR}$ because the way to do it using extension variables requires disjunctions of positive and negative literals. $\Gamma$ represents relations on variables, not on literals. Indeed when you think about 3-$\mathbf{SAT}$ as a $\mathsf{CSP}$ you need $\Gamma$ to contain four relations of disjunction with some negated inputs (from zero to three). Second: each relation in $\Gamma$ can be expressed as a batch of clauses with (say) three literals. Each constraint must be a whole batch of such clauses. In the example with equality/inequality constraints you cannot have a binary $\mathsf{AND}$ (i.e. relation ${(1,1)}$) without enforcing a binary negated $\mathsf{OR}$ (i.e. relation ${(0,0)}$) on the same variables. I hope this illustrates to you that $\mathbf{SAT}$ instances obtained from $\mathsf{CSP}$s have a very peculiar structure, which is enforced by the nature of $\Gamma$. If the structure is too tight then you cannot express hard problems. A corollary of Schaefer Theorem is that whenever $\Gamma$ enforces a structure loose enough to express $\mathbf{NP\backslash P}$ decision problems, then the same $\Gamma$ allows enough freedom to express general 3-$\mathsf{SAT}$ instances.