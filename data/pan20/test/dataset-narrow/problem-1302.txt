Now, in computer graphics, the transformation from one space to another is encoded using a matrix. So when rendering your full scene, what developers generally do is store: 

For example, if we wanted to implement a cell shader, what we could do is generate a ramp texture that emulates a step function: 

Volumetric rendering refers to a technique for generating a visual representation of data that is contained in a three dimensional space (volume). Examples of this are atmospheric effects such as smoke and fog and techniques applied to scientific visualization such as marching cubes to generate surfaces from volumetric data. In marching cubes, data is represented as voxels, but in the sense that we store some floating point value in a structured grid in three dimensions. Volume rendering is contrasted with Surface rendering because the data that is being rendered is coming from a three dimensional data set rather than a two dimensional data set (a surface). In surface rendering, we are only concerned with the effects of lighting interacting with a material at a surface and how to best describe the color at a particular surface. In volume rendering, we are interested in some data at a given location in 3D space, and we must figure out how to convey its meaning visually. For example, if we wanted to render cigarette smoke, we might use a volume rendering technique known as ray marching. We shoot a ray into our scene, and then we move slowly along the ray. At each step, we see how much smoke density there is at that particular position in 3D space based on some data set. From this density, we can choose how much opacity this sample contributes to the final pixel value. In terms of game development, my experience tells me that volume rendering is rarely used. Dealing with three dimensional data sets and the algorithms used for getting good visual results are usually detrimental to real-time performance both in terms of memory and computational efficiency. The advantages of volume rendering come from their use in the scientific computing community and their ability to very accurately depict data in a visual way. There is a saying in computer graphics that "if something looks right, it is right". This saying gains criticism in the scientific visualization community where "if it is right, it is right". 

Obviously the amount of time that you set your timer to will determine how fast your sprite moves. There are a few variations on this theme depending on the kind of behavior you want as well. For example, you can move the sprite once immediately upon detecting a tap and ignore step #4. 

Assuming you don't have any pathfinding constraints, what you need to do is calculate the offset from your unit's position (let's say ) to your target's position (let's say ). Then, the velocity of your unit will be , normalized times the speed of your unit. You can then move your unit's position towards the target location. In pseudocode: 

is now the direction in world space that corresponds to the Z-axis of the camera in eye space. Construct by translating by : . Using and , construct a line . Compute the intersection of with the plane whose normal is at distance away from the origin where is the desired Y value of your camera in world units. 

Memory Usage: If you have a lot of sprites with a lot of animation, you're going to need a lot of memory to store each of those images. This can quickly get out of hand if you're targeting a mobile device or you plan on having many different animated sprites on your screen at once. This can be remedied by techniques such as texture compression. For small projects (or projects with low quality sprites) this is usually not an issue. Smoothness: There is no ability to ease animation with sprites. In general, you will have choppy animations if you do not animate everything to an extremely fine level. This is usually not an issue for games with 8-bit graphics, but if you're debating between using sprites and animated skeletal models, you should be aware that you're going to have to have a lot of animation frames to get smooth looking animations. No animation blending: This folds into the previous point, but you're likely not going to be able to do things like look in a certain direction while jumping if you use sprites for animation. If you want to split up the abilities your character can do, or have your animations interact with things in your world (like grabbing a door handle), then you will need a separate sprite for each height of door handle. Again, this isn't too much of a problem if you have a simple game, but it can easily become one if you're thinking about something more complicated. 

The easiest way to do this seems to be to simply generate a random point on the mesh and walk to it. You can restrict the point to be within a radius of the monster's position in order to avoid really long random walks too. 

In order to take some coordinate from the unprojected field into the projected field, the easiest way to do this is to first figure out how far away from the bottom left corner of the field it is. In other words, we would like to find such that 

This is not overkill, and is in fact the correct technique. It is not too difficult: Imagine this problem from the perspective of the platform... The "up" direction will be the vector <0, 1> rotated by the same amount that your platform is rotated. The code for rotating a vector (counterclockwise) by an angle is: 

Batching the way you're thinking about it is usually only used for static objects. In other words, everything that doesn't move and shares a material, transformation, etc. gets grouped up into a common buffer. That way, you don't have to update your vertex buffers every frame. It seems like you had the right idea for grouping things based on material, vertex format, etc. This produces an optimization by reducing the number of state changes, so that you don't have to keep sending data to different parts of GPU memory. In other words, it's good to change loops that look like: 

This is usually solved by changing with from to . The behavior differences are due to the differences in OS APIs that GLFW uses and how they interact with the windowing system. In the documentation, it says that should be used for 3D camera controls, and should be used when rendering a custom cursor. 

Then when you go to do your collision detection, simply collide against the collision rect. Be sure to update your variable after you do your collision detection: 

I live in the United States, and I was curious whether or not any governmental grants or NGOs (Non-Governmental Organizations) support game developers via grants or other funding options for games that are not directly developed for commercial gain. After a quick googling, the only examples I could find were the NEA media arts grant which is restricted to games that can be claimed to be art (arguably all of them), and the ESA Foundation which focus on games for learning but are restricted to giving funding to non-profit organizations. Are there any other institutions that accept applications for games from individuals? Perhaps similar to Y-Combinator where you have to present a short video of yourself, and a working prototype of your game along with background of the team you will be using to develop it? I'd also like to know whether or not there are institutions that would like to promote a certain cause or aspect through game development by providing developers with resources to pursue this? (Similar to this DOE project) 

you're performing an inexact arithmetic computation. The small error accumulates over time. If your application only needs to move in X, you can get away with using an analytic solution, i.e. 

The way to deal with this is to set a timer once the person taps the phone. The most user friendly scenario that you'd implement would look something like this: 

It seems like what you're looking for is the Optimal Reciprocal Collision Avoidance algorithm. The preceding paper is also worth a read. Although the paper may be a bit involved the theory behind the algorithm is fairly straightforward: Assume that you already have a simulation (game) with agents (units) that have some sort of bounding volume around them. This bounding volume is likely what you're already using to perform the collision detection and response. For each agent, define a preferred velocity that may or may not be based on the agent's goal. Now, to perform the simulation: