In a looping automaton, the transition function is not assumed to be total. That is, for a state $q$ and a letter $\sigma$, it could be the case that $\delta(q,\sigma)$ is undefined. Intuitively, this is the same as having a transition to a rejecting sink. So they are "deterministic" in the sense that there is no non-determinism. But still, this means that for every word either there is a single run, or there are no runs. Thus, saying that a word is accepted iff it has a run on it is not a trivial acceptance condition. The reason for studying these automata is that they correspond to safety properties. A safety property is a language $L$ such that for every word $w\notin L$, there exists some prefix $x$ of $w$ such that for every suffix $y$, $x\cdot y\notin L$. That is, once you read $x$, there is no way to get in the language. These properties are useful in practice, as they correspond to "system failure", in a way. That is, once something really bad happens, you can't recover. Technically, these autoamta are useful because of their simple acceptance condition. As for references, start here. Also, Orna Kupferman has several papers using these objects (but I suspect you already know that... :) ) 

This is a question about properties of large directed graphs which are preserved when we randomly sample edges. Imagine I have an infinite sequence of positively weighted directed graphs. The graphs are represented by adjacency matrices $$M_1,M_2,...,M_n,...$$ where each $M_n \in \mathbb{R}^{n \times n}$. The sup-norm $\|M_n\|_{\infty} = \max_{i} \sum_{j=1}^n |M_{n}(i,j)|$ is uniformly bounded by some bound $B$ for all $n \in \mathbb{N}$. Furthermore, for any $\ell < n $ we have $M_{\ell}(i,j) = M_n(i,j)$ as long as $i,j \leq \ell$. Many times we do not observe the full graph. Instead of observing $M_n$, we observe a random variable $\tilde{M_n}$ where $$\tilde{M_n}(i,j) = \begin{cases} M_n(i,j) & \text{ with probability } \pi \\ 0 & \text{ with probability } 1-\pi.\end{cases}$$ Let $\tilde{y_n} = f(\tilde{M_n})$ be a real-valued function of the matrix $\tilde{M_n}$. Using a Chernoff bound, I know that if $f(\tilde{M_{n}}) = \sum_{i,j} a_{ij} \tilde{M_n(i,j)}$ is a linear function, then the value of $f(\tilde{M_{n}})$ is tightly concentrated around its expectation, with the probability of it being ``far away'' form $E[f(\tilde{M_n})]$ decreasing exponentially with $n$. Is this true for non-linear properties? For example, if $f(\tilde{M_n})$ is a convex function, do we have a similar concentration bound? 

An alternating tree automaton for arbitrary degree trees has a transition function of the following type: $$\delta:Q\times \Sigma\times D\to {\cal B}(\mathbb{N}\times Q)$$ where ${\cal B}$ is the set of Boolean functions over the given set. This has a limitation that $\delta(q,\sigma,d)$ only outputs values between $1,...,d$ (so it fits the degree of the tree). Thus, the transition function is defined separately for each degree, so it's effectively a family of automata. In particular, this means that if you want to handle arbitrary degrees, you need an infinite description (or a finite description of an infinite object). While this is mathematically possible, it can of course make things undecidable, if (for example) your transition function is given by a TM. In your example, you can clearly construct such an automaton, since you can construct one for every degree. 

Let $U$ be a universal Turing Machine. Suppose I have a Kolmogorov incompressible string $s$ of length $n$. Let $A:\{1,...,n\} \to \{0,1\}$ be an algorithm such that $A(i) = s_i$. I believe that the time it takes for the universal Turing machine to evaluate $A$ on input $i$ should be $\Theta(n)$. I'm not familiar with Kolmogorov complexity, and I wanted to ask if the following intuition is correct 

Is there a known algorithm for sampling a set $S \subset \{1,...,n\}$ with probability $p_S = \frac{e^{f(S)}}{\sum_{T \subset \{1,...,n\}} e^{f(T)}}$ where $f: 2^{\{1,...,n\}} \to \mathbb{R}$ is a submodular function? Thanks! 

I'm looking into communication complexity with real numbers. One problem if we want to define this is that one can encode many real numbers $0.a_1a_2a_3... , 0.b_1b_2b_3..., 0.c_1c_2c_3...$ using only one number $0.a_1b_1c_1a_2b_2c_2...$ To get around this problem, existing papers that deal with this issue (such as Abelson (1978), Luo and Tsitiklis, and Chen(1994)) assume that the messages that can be sent between Alice and Bob must be continuously differentiable functions of the inputs $x_1,...,x_n$. Do we need differentiability? Is there any problem if the messages are assumed to be continuous (not necessarily differentiable) functions? I know that there's no continuous bijective function $f: \mathbb{R}^n \to \mathbb{R}$ for $n > 1$, so it seems like just assuming continuity should be enough. Thanks! 

Sounds right. Denote by $G$ the original graph, and by $G'$ the graph after introducing the new vertices. The first direction is trivial - every Steiner tree in $G$ corresponds to a Steiner tree in $G'$, with the same weight (by splitting each edge with its new vertex). The second direction is less trivial. Still, consider a minimal Steiner tree $T$ in $G'$. We can assume w.l.o.g that $T$ does not contain any leaf that corresponds to a new edge-vertex. Indeed, this vertex is not in $S$, so if it is a leaf, it can be removed from the tree, keeping it a Steiner tree with minimal weight (unless you allow negative weights, which I assume you don't). Now that we have a Steiner tree without edge-vertex leafs, we can convert it to a Steiner tree in $G$: root the tree by some vertex that is not an edge-vertex, then by the construction, all the edge-vertices have a single child in this tree. So you can replace each pair of edges $(u,X),(X,v)$ by $(u,v)$ in the original graph, obtaining a Steiner tree with the same weight. I don't see any bugs here, but maybe I missed something. 

Does the Johnson-Lindenstrauss Lemma apply to any finite-dimensional Hilbert Space? In particular, I am interested in the space of random variables $X = (X_1,...,X_N)$ over $N$ uncertain states. If $\pi_i$ is the probability of state $i$, then this space has an inner product $\langle X, Y \rangle = E(XY) = \sum_{i=1}^n \pi_i X_i Y_i$ and a norm $\|X\|^2_{\pi} = \langle X, X \rangle = \sum_{i=1}^N \pi_i X_i^2$. The standard JL lemma says that, if $S$ is a set of $m$ points in $\mathbb{R}^n$ and $n > C \frac{ln(m)}{\varepsilon^2}$ then there a (suitably scaled) random orthogonal projection $f:\mathbb{R}^N \to \mathbb{R}^n$ will satisfy $$(1-\varepsilon)\|u-v\|^2_{N} \leq \|f(u) - f(v)\|_{n}^2 \leq (1+\varepsilon)\|u-v\|_{N}^2$$ where I have used $\|\cdot\|_{n},\|\cdot\|_{N}$ to denote the standard euclidean norms in $\mathbb{R}^n$ and $\mathbb{R}^N.$ Does there exist a version of the lemma with the weighted norm $\|X\|^2_{\pi}$? What would the corresponding norm in the lower $n$-dimensional space look like? 

Post's Correspondence problem with a fixed number of tiles of between 3 and 6. While it is not really simple to describe, it does have a very "playful" description, and I find it suitable for intuition-level talks. 

This question is probably more suitable in cs.se, but until it gets migrated, here is an answer. Rice's theorem regards non-trivial semantic properties. Formally, a semantic property is a set of Turing machines $P$ such that for every two TMs $M_1,M_2$, if $L(M_1)=L(M_2)$, then either $M_1,M_2\in P$, or $M_1,M_2\notin P$. That is, membership in $P$ is determined solely by the language of the machine, and not by its "inner working". Rice's theorem states that for every such property $P$ that is nontrivial (so it is not empty, nor contains every machine), it holds that deciding membership in $P$ is undecidable. EDIT per the OP's request: An intuitive way to look at a semantic property is that it only concerns what a program does, rather than how it does it. That is - memory management, pointers, data, types, etc - are all syntactic, in a way, and thus are not semantic properties. This is because for every program you can build a different program that works entirely differently, but achieves the same result. Semantic properties only look at the "output" of the program (in cases where this is well-defined). Thus, your intuition is incorrect. None of the internal workings of a program are subject to Rice's theorem, exactly because if you change entirely the inner workings (while maintaining the output), then either both programs should have the property, or both should not have them. 

Imagine I have a weighted directed graph $G = (V,E)$ with $n$ vertices and a weighted adjacency matrix $W$. Assume the indegree of vertex $i$ is bounded away from 1 , so that there exists a constant $\lambda$ such that $\sum_{j=1}^{n} W_{ji} < \lambda < 1$. I can compute the adjacency matrix of the transitive closure as $X = (I-W)^{-1} = I + W + W^2 + ...$ and compute the indegree of vertex $i$ in the transitive closure as $d_i = \sum_{j=1}^n X_{ji}$. I now sample the edges of $G$ to form a new graph $G' = (V,E')$ where each edge of $E$ is in $E'$ independently with probability $p > 0$. Let $X'(p)$ be the corresponding transitive closure adjacency matrix, and let $d_i'(p) = \sum_{j=1}^n X_{ji}'(p)$. I'm trying to show that $\lim_{p \to 1} d_i'(p) = d_i$ uniformly over all $i$ and all $n$. That is, for every $\epsilon > 0$, there exists a $\delta$ independent of $n$ and $i$, such that for all $p > 1- \delta$ we have $|d_i'(p) - d_i| < \epsilon$. I know this is true if we allow $\delta$ to depend on $n$, but I'm not sure where I would start thinking about this to prove the uniform limit. 

The book "Proofs from The Book", referencing ErdÅ‘s' notion of God's book, which contains the most beautiful proofs, was published in 1998. 

The first observation about this question is that it seems impossible to solve using information-theoretic tools, since we crucially rely on our ability to obtain information by running the machines without the oracle. 

Now, there is obviously an encoding issue here, since the numbers need to be transcendental, so describing them is problematic. Any solution would do, but specifically, I am interested in the case where all the numbers are of the form $e^{a_i}$, where the $a_i$'s are linearly independent over $\mathbb Q$. The motivation for this question is understanding whether a (positive) solution to Schanuel's conjecture would automatically lead to a constructive solution. UPDATE (based on the comments): the encoding of the numbers is an issue. I'll accept any answer which suggests either a reasonable encoding for a subset of the reals for which the problem is non-trivial, or a solution that uses a computational model that allows real numbers. 

An upper bound on the running time of $A$ is $O(n)$, since the description of $A$ can just list the whole string $s$ and return $s_i$ on input $i$. The universal Turing machine on inputs $(A,i)$ simply reads the description of $A$ (Which is $O(n)$ in length) and outputs the $i^{th}$ coordinate (which takes time $\log n$) A lower bound on the running time of $A$ can be proved as follows. a. The description length of $A$ must be at least $n - log n - c$ for a constant $c > 0$. This is because the program $B$: "Run $A$ on every input from $1$ to $n$" has description length $DL(B) = DL(A) + log(n) + c$ and generates the string $s$. Thus $DL(B) \geq n$ and $DL(A) \geq n- \log(n) - c$. b. The universal Turing machine $U$ on input $(A,i)$ needs to read both $A$ and $i$, which requires at least $DL(A) + \log n \geq n-c$ operations.