One of my favorite results is the use of topological arguments in Lovasz's proof of the Kneser conjecture, and the use of topological (and group-theoretic) methods in the Kahn-Saks-Sturtevant attack on the strong Aandera-Rosenberg-Karp conjecture on evasiveness. 

Usually any such ex-parte communication needs to go through the relevant PC. For example I was once on a PC where a subreviewer identified a bug in a paper they were reviewing for me. They relayed it to me, I discussed it with the other PC members assigned to the paper, and then we presented our conclusions to the PC chair who initiated contact with the authors. This is for conferences without a rebuttal process (typically true for theory confs). If the conference has a rebuttal process then there's a formal mechanism for discussing this with the authors. Finally, if you're not sure if there's a bug, then merely raise your doubt with the PC. Update: a secondary issue that comes to mind after seeing Jeff's comment is a matter of fairness. Suppose ex-parte communications are not officially sanctioned, but are not frowned on. Then an author who happens to know people on the PC gets a slight advantage, because PC members might be more likely to contact the author. However, an official policy on how to handle such communication that does not depend on "knowing the author" implicitly or explicitly seems fairer. 

Let $G$ be an edge-weighted graph, and let (S, V-S) be a feasible pair if S is a maximal independent set. The weight of a feasible pair is computed by finding for each element of V-S the lightest edge connecting it to S, and summing the costs (such an edge must exist since S maximal). The goal is to find a minimum weight pair. This problem is NP-hard and is likely to be fairly hard to approximate (it encodes vertex cover at the very least). We're looking at heuristics that might be effective in practice, and one idea is to do some kind of metropolis-hastings-type walk on the space of all independent sets (since given an independent set, computing the weight is easy). I know that there's a fair bit of study of sampling over independent sets (for example, this). My question is: 

Short answer: it depends. Long answer: a degree in TCS can help you a lot in areas like biology (bioinformatics, computational biology, Dick Karp!) and physics (quantum computing, for example). But it almost entirely depends on the area of TCS you focus on. Obviously, a specialty in scheduling theory won't help much with quantum cryptography. An advanced degree in math can also help you in these disciplines: ultimately though, it depends on what kind of work you want to do. For modeling, math background is great. For computational aspects, a TCS degree might be more helpful. 

Calendar for conference activities Problem: It's hard to keep track of multiple sessions, and often in a large conference I don't want to attend all the talks, or would like to maintain a special list of talks I'm going to. Solution: The conference should at a minimum provide a calendar file that can be slotted into google calendar or whatever calendaring app you have on a phone/laptop. The ICS format is pretty standard. Note: Conferences can go much further - at KDD you logged into a social networking site associated with the conference and could export a customized calendar based on what talks you're interested in. But that requires more infrastructure and money, and is not within the scope of this question. 

This is somewhat heavy machinery for what you want, but there's been a large body of work on techniques for designing ever more refined LPs (SDPs) that get closer and closer to the desired integer program. A good reference that reviews these approaches is by Monique Laurent: A comparison of the Sherali-Adams, Lovasz-Schrijver and Laserre Relaxations for 0-1 programming. Apart from that, I am not aware of a single good source of references: I assume you've at least perused the relevant chapters in Vijay Vazirani's book ? 

There was a workshop last month at DIMACS on this topic: perusing the abstracts will give you more pointers. 

From a practical side, it's important to remember that NP-Completeness is not a barrier for many problems in practice. The twin tools of SAT solvers and CPLEX (for integer linear programming) are powerful enough and well-engineered enough that it's often possible to solve large instances of NP-complete problems by either framing the problem as a suitable ILP or by reduction to SAT. I'm not aware of similarly well-engineered solvers for problems in PSPACE. 

Both the original parametrized complexity book by Downey and Fellows, and the newer book by Flum and Grohe, are good references for reduction techniques. 

Origami. I lead off with the 5-point star problem (this works well in american contexts, because of the connection to the american flag) and let students try to figure out how to make a five-point star with folding + 1 cut. I talk about the "resource" (cutting) and how algorithm design is about working with limited resources. Then I talk about other origami questions and applications in the real world (heart valves, NASA telescopes, crumple zones in cars). Sorting pancakes: there's a beautiful connection between sorting pancakes and genome rearrangement, and I actually made stacks of pancakes from foam for students to play with. Works great, and lets me talk about algorithms, gene sequencing, Bill Gates (!), and other fun things. 

Well, if there were known cases, then we'd be able to separate P and NC. But there are many problems known to be P-complete (i.e under logspace reductions), and they present the same kind of barriers to showing P = NC as NP-complete problems do for P = NP. Among them include linear programming and matching (and max flows in general). Ketan Mulmuley proved a result separating P and a weak form of NC (without bit operations) back in 1994. In a sense, his current program for P vs NP takes off from where that left off (in a very loose way). The book 'Limits on Parallel Computation' has more on this. 

With the caveat that I am not quite sure what you're asking about, have you looked at the book on Approximation Algorithms by Vijay Vazirani ? There are two techniques (that are quite related to each other) for using LPs to design combinatorial algorithms. One is of course the primal-dual method that Vijay has propounded for a while, and is explained nicely in the book. Another is the local-ratio method developed by Reuven Bar-Yehuda and others, which it tuns out is also closely related to primal-dual methods. Another perspective that's proven to be useful is the 'pricing' model, which has a more game-theoretic flavor but is also another way of navigating primals and duals in a combinatorial manner.