You should consider checking the Azure Machine Learning platform. It is online and you can use it with a free account. Azure ML provides you with a workflow by using modules in a graphic user interface. Many of them are related with Data Munging and you can easily clean your data. If there is something that you cannot do in the GUI, then you can just add a module which let you run custom R or Python script to manipulate your data. The nice part of it, is that you can easily visualise your data at any time and check simple stats like the of the R. 

I recently found this use cases on Kaggle for Data Science and Data Analytics. Data Science Use Cases However, I am curious to find examples and case studies of real reports from other professionals in some of those use cases on the link. Including hypothesis, testing, reports, conclusions (and maybe also the datasets they have used) Do you have any kind of those reports to share? I am aware of the difficulty to share this kind of details and this is why it is not a Google-search-and-find answer. 

Connect your file in an SQL database and work with it from there. Use another tool like delimitware which let you open files up to 2 billions rows. Use Python Pandas with and then to check the first 5 rows. Split the file in smaller files with less than 1m rows per each chunk and use Numbers or Microsoft Excel to read it. Upload it to Azure ML and work on a new experiment with it. 

I trained a prediction model with Scikit Learn in Python (Random Forest Regressor) and I want to extract somehow the weights of each feature to create an excel tool for manual prediction. The only thing that I found is the but it doesn't help. Is there any way to achieve it? 

The long answer: Don't worry about RAM. Yes, if you have a crappy RAM, you will also have a problem with your performance. But, the most important thing in Data Analysis is the performance of your code. An example from my personal experience: I had a text dataset, about 30GB. I needed to create a corpus for a simple TF-IDF. The brute-force code, spent about 15 hours, whereas a map-reduce algorithm spent about 1.5 minute for the same dataset. I work on a MacBook Pro with 16GB RAM. I didn't have any problem with performance. However, if you have huge datasets and even code improvements and RAM upgrades cannot help, you may reconsider to move to Hadoop or similar. 

Optical character recognition is a well-studied problem with many possible solutions (ressources). CNNs have proven to work extremely well even for hand-written character recognition. Take a look at this two papers: 

To do regression and predict future data points, you would need to build a training dataset consisting of a sequence of events. Let's say a value $x$ for every timestamp $t$. Your data seems to have 1 dimension, so both the network input layer and the output layer would consist of 1 unit. You would then train your model to predict $(x_{t+1})$ given $(x_{t})$. Let $M$ be our trained model and let's say you want to forecast a data point at time $k$ and you know the current value at time $t$. $M(x_{t}) = (x_{t+1})$ $t = t+1$ $M(x_{t}) = (x_{t+1})$ $...$ increment $t$ and keep predicting until $t+1 = k-1$ $M(x_{t+1}) = (x_{k})$ Put in pictures this corresponds to: 

TensorFlow is a tool to write computation using data flow graphs; this being said, if you want your app to use a pre-trained model only, there is no requirements to use TensorFlow specifically. You could even use one of the ML library written in Javascript to import and run the pre-trained model. 

If you are looking for a way to compile code written for the Keras API to code only using the Tensorflow API, there is no ready-to-use solution for that. And I cannot see any good reason why someone would want to do that in the first place... You can simply look at the Keras backend source code for Tensorflow and rewrite your entire implementation. But once again I don't see any valid reason that would make this worth it. 

I would still stick with using a CNN for that specific application. Think about CNNs being used to detect various types of cancer in noisy images with an insane precision (Stanford, Google). This type of input is actually very similar to yours with cancer cells hiding in a cluster of healthy ones. And yet the models are performing as good as cancer experts in some cases. CNN have shown to work best when trained with a HUGE amount of data. If possible try to provide more training data with a decent class distribution (roughly the same number of positive and negative examples). Moreover, apart from tuning hyperparameters you could also experiment with different CNN architectures. You will fin plenty of inspiration in the litterature. 

I would impressed if there is already a dedicated 9gag clustering :P However, you can read this blog post about Hierarchical clustering in Python for images, which is close to what you want. The problem is that the author uses the average color of the image as a feature and it could proved crude and not inefficient. You may find something more interesting to use. But in the end, you need to experiment a lot with your own dataset. 

Then change the type of columns that can be numeric. If you don't know which are those columns, use this that will automatically change the type to numeric and ignore those that cannot be changed. 

I wasn't sure if I had to ask it here or in Stackoverflow, but since I am also seeking research papers/algorithms and not only code, I decided to do it here. When I have a text, I can manually write a regex to find all the possible outputs from what I want to extract from the file. What I want to do, is to find an algorithm or a research, which can let you highlight (set the input) different positions of the same (repeated) data you want to extract in the text file, train the algorithm and then identify all the others under the same contentions of those you set. For example, let's say that I have a text with several titles which are following with and starting with . It is easy with regex, but I want to do it dynamically. An idea is to build an algorithm which will take examples and create regex automatically. But I am not aware of any research like this and maybe there are also other techniques that you can achieve it. Any ideas? 

I am working on Azure ML Studio and try to create a regression model to predict a numerical value. I will try to describe my features and what I have done until now. My data with about 3 million rows : Features: 

If you are not familiar with Python code, just ask it and I will write it for you :) Edit with a code sample: 

I isn't actually answering your question, but it is an idea of how you can improve it. In my opinion, I don't believe that you will be able to build a classification model with only those data. And if you do it, it will not have high enough accuracy. In your position, I would start looking for more data to use as features. Here are a few examples: 

Yes and no depending on how you define "good enough samples". You will likely end up with a chicken and egg problem: you want to use the GAN to generate training data, but the GAN doesn't have enough training data itself to generate convincing enough samples. Other techniques exist for data synthesis of training images. For example: adding noise, flipping axis, change luminosity, change color, random cropping, random distorsion. 

Once you installed the GPU version of Tensorflow, you don't have anything to do in Keras. As written in the Keras documentation, "If you are running on the TensorFlow backend, your code will automatically run on GPU if any available GPU is detected." And if you want to check that the GPU is correctly detected, start your script with: 

Here is a beginner tutorial to do just that with Tensorflow: $URL$ If you need extra data to train your model, take a look at the MNIST dataset: $URL$ 

The approach you use to do dimensions reduction is agnostic to the method you use for classification. You can use PCA to preprocess your data before to train any type of classifier, including artificial neural networks if that's what you want to use. 

The equations are almost the same. First of all, they are written in a slightly different form. In Cho, et al. (2014), it is written that $W_r$ and $U_r$ are weight matrices which are learned. While in Colah's blog, both matrices are referred to as $W$. For example you could write these two equivalent forms: $z_j = \sigma([W_z x]_j + [W_z h_{t-1}]_j)$ $z_j = \sigma(W_z[x,h_{t-1}]_j)$ Secondly, there is a small mistake in Colah's for $h_t$. Let's ignore the different notation $h_j^{(t)}$ used in the paper and use Colah's notation $h_t$ instead. Then the equation for $h$ in Cho, et al. (2014) can be written as: $h_t = z_t \ast h_{t - 1} + (1 - z_t) \ast \tilde{h_t}$ and if we use the same order as Colah's equation: Paper's equation: $h_t = (1 - z_t) \ast \tilde{h_t} + z_t \ast h_{t - 1}$ Colah's equation: $h_t = (1 - z_t) \ast h_{t - 1} + z_t \ast \tilde{h_t}$ So we can actually see that for Colah's equation to match the one in the paper, we have to swap $\tilde{h_t}$ and $h_{t - 1}$ in this equation. If you check the comments next to the GRU example on Colah's blog, you can actually see that some other people found the same mistake. Well spotted, I didn't saw it at first! :) 

I'm busy with a supervised machine learning problem where I am predicting contract cancellation. Although a lengthy question, I do hope someone will take the time as I'm convinced it will help others out there (I've just been unable to find ANY solutions that have helped me) I have the following two datasets: 1) "Modelling Dataset" Contains about 400k contracts (rows) with 300 features and a single label (0 = "Not Cancelled", 1 = "Cancelled"). Each row represents a single contract, and each contract is only represented once in the data. There are 350k "Not Cancelled" and 50k "Cancelled" cases. Features are all extracted as at a specific date for each contract. This date is referred to as the "Effective Date". For "Cancelled" contracts, the "Effective Date" is the date of cancellation. For "Not Cancelled" contracts, the "Effective Date" is a date say 6 months ago. This will be explained in a moment. 2) "Live Dataset" Contains 300k contracts (rows) with the same list of 300 features. All these contracts are "Not Cancelled" of course, as we want to predict which of them will cancel. These contracts were followed for a period of 2 months, and I then added a Label to this data to indicate whether it actually ended up cancelling in those two months: 0 = "Not Cancelled", 1 = "Cancelled" The problem: I get amazing results on the "Modelling Dataset" (random train/test split) (eg Precision 95%, AUC 0.98), but as soon as that model is applied to the "Live Dataset", it performs poorly (cannot predict well which contracts ends up cancelling) (eg Precision 50%, AUC 0.7). On the Modelling Dataset, the results are great, almost irrespective of model or data preparation. I test a number of models (E.g. SkLearn random forest, Keras neural network, Microsoft GbmLight, SkLearn Recursive feature elimination). Even with default settings, the models generally perform well. I've standardized features. I've binned features to attempt improving how well it will generalize. Nothing has help it generalize to the "Live Dataset" My suspicion: In my mind, this is not an over-training issue because I've got a test set within the "Modelling Dataset" and those results are great on the test set. It is not a modelling or even a hyper-parameter optimization issue, as the results are already great. I've also investigated whether there are significant differences in the profile of the features between the two datasets by looking at histograms, feature-by-feature. Nothing is worryingly different. I suspect the issue lies therein that the same contracts that are marked as "Not Cancelled" in the "Modelling Dataset", which the model trains to recognize "Not Cancelled" of course, is basically the exact same contracts in the "Live Dataset", except that 6 months have now passed. I suspect that the features for the "Not Cancelled" cases has not changed enough to now make the model recognize some of them as about to be "Cancelled". In other words, the contracts have not moved enough in the feature space. My questions: Firstly, does my suspicion sound correct? Secondly, if I've stated the problem to be solved incorrectly, how would I then set up the problem-statement if the purpose is to predict cancellation of something like contracts (when the data on which you train will almost certainly contain the data one which you want to predict)? For the record, the problem-statement I've used here is similar to the way others have done this. And they reported great results. But I'm not sure that the models were ever tested in real live. In other cases, the problem to be solved was lightly different, e.g. hotel booking cancellations, which is different because there a stream of new incoming bookings and booking duration is relatively short, so no bookings in common between the modelling and live dataset. Contracts on the other hand have long duration and can cancel at any time, and sometimes never. 

To make our life easier, let's omit optimization methods here (e.g. Learning Rate, Momentum) and focus on vanilla Backpropagation. As you know, backprop consists of computing the partial derivative with respect to each weight in order to penalize each weight for its role in the global output error; such that: \begin{equation} \frac{\partial E}{\partial W_{ij}} = e_{i}y_{j} \end{equation} With $E$ the global output error to minimize, $W$ the weight to update, $y$ the output value, and $e$ the local error. The local error $e$ is the unknown we want to compute! :) Using the chain rule, the gradient can be expressed as: \begin{equation} \frac{\partial E}{\partial W_{ij}} = \frac{\partial E}{\partial y_{i}} \frac{\partial y_{i}}{\partial x_{i}} \frac{\partial x_{i}}{\partial W_{ij}} \end{equation} With $x_{i}$ the input value to neuron $i$. First, the partial derivative with respect to $W_{ij}$ can be computed as follows: \begin{equation} \frac{\partial x_{i}}{\partial W_{ij}} = y_{j} \end{equation} Which is simply equal to the output value $y_{j}$! Secondly, the partial derivative with respect to $x_{i}$ is: \begin{equation} \frac{\partial y_{i}}{\partial x_{i}} = \frac{\partial \phi(x_i)}{\partial x_{i}} \end{equation} Which is simply $\frac{\partial \phi(x_i)}{\partial x_{i}}$, namely the derivative of the activation function (i.e. Sigmoid, Tanh, ReLU...). Thirdly, the partial derivative with respect to $y_{i}$ can be computed as follows: \begin{equation} \frac{\partial E}{\partial y_{i}} = \begin{cases} \begin{aligned} \frac{\partial}{\partial y_{i}} (T_i - y_i) \end{aligned} & \text{if $i \in$ output layer,} \\ \begin{aligned} \frac{\partial}{\partial y_{i}} \left(\sum\limits^{n}_{j=1}W_{ij}\frac{\partial E}{\partial y_{j}}\right) \end{aligned} & \text{otherwise;} \end{cases} \end{equation} With $T$ the target expected output (i.e. the label). The local error $e$ is computed differently depending on the layer position. For the output layer, the error is proportional to the difference between the predicted value (i.e. $y_{i}$) and the expected output (i.e. $T_{i}$). Otherwise, the error in hidden layers is proportional to the weighted sum of errors from connected layers. Fourthly, combining the partial derivatives allow the computation of the error at a given neuron $i$ such as: \begin{equation} \frac{\partial E}{\partial y_{i}} \frac{\partial y_{i}}{\partial x_{i}} = e_{i} = \begin{cases} \begin{aligned} \frac{\partial \phi(x_i)}{\partial x_{i}} (T_i - y_i) \end{aligned} & \text{if $i \in$ output layer,} \\ \begin{aligned} \frac{\partial \phi(x_i)}{\partial x_{i}} \left(\sum\limits^{n}_{j=1}W_{ij} e_{j}\right) \end{aligned} & \text{otherwise;} \end{cases} \end{equation} As you can see here, you need to recursively compute the local error $e$ all the way to the input layer, that's why it's called backprop! ;) I think that what you missed! If we now simplify: \begin{equation} \frac{\partial E}{\partial W_{ij}} = \frac{\partial E}{\partial y_{i}} \frac{\partial y_{i}}{\partial x_{i}} \frac{\partial x_{i}}{\partial W_{ij}} = e_{i}y_{j} \end{equation} We already know $y_{j}$, and we now know how to compute $e_{i}$ all the way to the input layer. So the weight can finally be updated from the gradient as follows: \begin{equation} W_{ij} = W_{ij} - \,e_{i}y_{j} \end{equation} Sources: