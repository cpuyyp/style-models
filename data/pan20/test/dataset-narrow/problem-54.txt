Your original scenario wouldn't work in the real world, because the assumption for AS6501 would be for it to incur a cost to get your bits across it to you from the ISP(s). AS6501 wouldn't be getting benefit by hauling your bits across it to your ISP so you don't have to pay your ISP. What's more likely is AS6501 would charge you to do this, at which point, you might as well just pay your ISP(s) anyway. ** The term peering is overloaded. It can be both used to describe a BGP session (e or i variety), as well as the colloquial/political sense, which means you and another network connect to one another and directly exchange traffic (via BGP) for mutual benefit - think opposite of transit. If you're running BGP with your ISP (transit), you're still technically peering with your ISP, because it's an eBGP peering. To help avoid confusion, it's better to use peering to refer to the act of exchanging traffic with another network at no cost (note that this isn't always the case) and BGP session to refer to the actual technical term for exchanging prefixes via BGP (iBGP or eBGP) with another router, whether a cost is involved or not. 

The sole difference being "NetFlow is Cisco proprietary, sFlow is not" isn't exactly correct. NetFlow originally started out as Cisco proprietary, but kind of went the same way as GRE or EIGRP. Since NetFlow v5 it has been implemented and supported on other vendors' hardware. The main difference between NetFlow and sFlow is that NetFlow is restricted to IP only, whereas sFlow has the ability sample everything (network layer independent). EDIT: The above appears to be no longer correct (at least as of the IPFIX standard). I've found the following blog post (warning: seems to be an "sflow" specific URL, so take it with a grain of salt if you wish) does a pretty good job of outlining the differences between the IPFIX spec and sFlow 

ZR optics are spec'ed to 80km but they're not part of the 10GE standard; they're rather a "Cisco special." In terms of what actually allows them to transmit light over this distance may be "protected" information from Cisco (both the ZR and ER operate on 1550nm SMF), but they've published some of the optical parameters for the ZR XENPAK here. EDIT: While I thought that 10GBASE-ZR was Cisco-specific, it's not actually. The wikipedia page for the 10Gig standard states "multiple manufacturers" have introduced "ZR" optics which are based on the 80km PHY described in the OC192 SONET specs. 

I know this doesn't fit your requirement of not using a signaling protocol, but to be honest I'm not sure if any requirement of not using one really makes much sense. Maybe you could elaborate on why you don't want to use RSVP/LDP for signalling? 

There are a couple ways to do this, without having to specify every single prefix that you're receiving from AS6400 in a prefix-list (I would personally advise against doing this because as you mentioned, the administrative overhead is high and the process will become exponentially more error-prone as the number of prefixes increases). 1) Tag routes you've received from AS6400 with the no export community. You would do this within a route-map: 

As you said, it matters on how many devices we're talking about, but aside from that, what you should avoid if at all possible is only having "in-band" management of your devices. You don't want to have your management traffic riding the same network as your production traffic. If not all of your switches have a separate Ethernet interface for management, that's fine, but nearly every single piece of equipment out there has some form of serial console. USE IT. Especially as a backup for in-band management. This will save your ass if a device falls off of the planet. I'm also suggesting using a completely separate physical infrastructure for management connectivity to your equipment. This applies doubly for serial console access. If you're using one of your interfaces on your switches (that's not a dedicated management interface), that's also not as big of a deal, as long as you've got a separate network to connect it to. 50 devices isn't too unreasonable of a footprint to use a single VLAN (again, assuming you are not doing in-band management) and have one broadcast domain for - you may be trying to optimize too early at this stage. If your core switches are modular boxes then they should most definitely have Ethernet management interfaces - I would advise you to use those rather than an SVI or a physical routed interface. edit: my personal preferences basically outline what I've advised above: always serial consoles. Use dedicated Ethernet management interfaces where applicable. If dedicated Ethernet management interfaces are not available, then burn a physical port on the box, but always always always a separate network, for serial consoles at the absolute minimum. 

If the switch is running L3 capable code, this creates a directly connected network in the switch's routing table. If you repeated this with 2 different VLANs with different subnets, you can now route between those three VLANs in one box. 

To get the router to show the entirety of the running config, including the defaults which are normally hidden. Look for in there. 

The Opaque LSA has 32 bits allocated for the Opaque Type (8 bits) and the Opaque ID (24 bits). There are (AFAIK) currently four Opaque types that have been allocated by the IANA: 

Assuming you're wanting to monitor this data via SNMP (I'm not familiar with PRTG), your best bet is to monitor for: 

The "classes" came from the way they split up the address space for use between "host" and "network". Keep in mind that back then (way way back, from the days of ARPANET), subnet masks did not exist, and the network was intended to be inferred from the address itself. So, with the above in mind, this is what they came up with (this is intended to be binary representation - each or represents a single bit in the 32-bit address): 

As others have commented above, having the vendor of your switch would be useful. In addition, the reasons why you're attempting to simulate two different switches with a single switch is unclear. If your VLANs are representative of the same subnet, you'd need to use a trunk to interconnect the VLANs. This should be possible (although highly unlikely that it would actually work) with a single cable, and configuring each port that the cable connects to as a VLAN trunk (best practices typically dictate IEEE 802.1Q for this) will be required, in addition to allowing each VLAN across the trunk. Again, best practices would dictate, however, that you should have separate subnets that each VLAN would represent, and then configure Layer 3 interfaces for each VLAN. 

This is achieved through using a base configuration for whatever IOS images you want to use. In that base configuration, all you're doing is changing the line speed of the console port to something that's faster than 9600 (maximum line speed is 115200). This blog post does a good job of explaining how to set said base configurations. 

Regarding jumbo frame support on the Cat6k, IIRC the default system MTU size is 9216, but there are some modules where a maximum ingress frame size of only 8092 bytes is supported vs 9216 bytes (they'll drop frames larger than 8092 bytes at ingress). Double check the documentation to find out which modules exactly have this limitation. Regarding the port-channels, if both ports in the port-channel are already set to the same MTU of 9216, it shouldn't make a difference if you configure the port-channel interface to have a 9216 MTU, but it wouldn't hurt for completeness'/clarity's sake in your configuration. You actually can't have two ports in an EtherChannel that have different MTU sizes. Regarding the SVI's, there's really not much of a difference between routing jumbo frames across a routed L3 interface or an SVI (they're essentially the same thing) - just realize that if fragmentation is required across an L3 interface (ie a jumbo frame is routed from one L3 interface to another with a smaller MTU), the frame is punted to the CPU for fragmentation in software, and this can lead to problems. The rule of thumb here is to keep your jumbo frame traffic localized to other destinations/networks that also support jumbo frames. While changing the MTU on the VLANs themselves shouldn't really be necessary, make sure that the interface MTU is the same across all the interfaces in the VLAN before setting the VLAN MTU to that same value. And similar to the port-channels, if you already have 9216 as the interface MTU's and the VLAN MTU for those interfaces, it shouldn't actually cause problems to set the MTU of the SVI for those VLANs to 9216. 

Last time I checked/tried to do this, it wasn't possible in IOS. I'm not sure about NX-OS or IOS-XE/XR though. It doesn't help you, but this is possible to do on Juniper gear. Regarding your configuration - make sure you're using the keyword in your route-maps to set the communities, otherwise you will not be adding communities to the list, but rather replacing any existing communities with the one you're setting in your route-maps. 

You can use any of these for a CoS ID, but using the PCP bits keeps you from having to look into the L3 header for DSCP values, and you can have 8 individual CoS ID's if you want (one for each PCP bit). No matter what though, (if you do choose to go with using the PCP bits) you have to use all of them, and there can't be any overlap. It's also a possibility that your hardware doesn't have the ability to do QoS based on PCP bits, in which case doing it via the DSCP values in the IP header will work. Ultimately in order for anyone to give you a useful answer of what kind of QoS policy to mock up, it would be beneficial to get the service-level specs from your metroE provider, because they should have told you what sort of bandwidth profile you'd be getting with your service... and if they haven't, then you should definitely ask. :-) 

Where and would represent your addresses that you wish to filter. Continue to add addresses as you see fit. Don't forget the explicit at the end. Once you have the ACL created, you would then apply it to your desired interfaces in the inbound direction. 

More than likely, if all you had was inband management access via the management VRF. To combat this, your options are a separate network for Ethernet management connectivity, or investing in serial console terminal servers. My suggestion is to have remote serial console access (preferably on a separate network, but it's understandable that this may not be feasible due to various reasons) as a backup to inband management at the very minimum. EDIT: I suppose you could try to get access to the box if it has a routed interface or VE on it, but I'm not sure what your ACL's are on interfaces of that nature. In general, best practices would dictate to deny management/control plane access via data plane ports (unless you don't have dedicated Ethernet management interfaces).