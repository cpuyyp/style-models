Having an application that uses shaders that have been wrote in GLSL, what is the best strategy for the distribution in the real world and for the desktop and mobile? I'm aiming to distribute this in a binary form or as plain serialized text, i would like a good suggestion on this. 

What steps usually takes or modify a trainer? How a trainer can change the way a game behaves? Edit: with the word trainer I mean the applications, usually a single executable, that can unlock and give extra points and features during a gameplay, kind of a series of cheats that you can activate with this executable. 

Yes, also this is a common scenario for both desktop and mobile users, with OpenGL the fixed pipeline approach is just deprecated and when using old code on modern devices you are just guessing, because any GPU maker that wants to stick with a modern OpenGL approach is not forced to support old functions and the old approach for the pipeline. These days you get OpenGL ES 2 capable devices from the low end market up to the high end, and the OpenGL ES 3 will come soon, adopting OpenGL ES 1.x is just an old and deprecated approach for the market of today. OpenGL ES 2 introduces the programmable pipeline on mobile devices discarding the old fixed-pipeline approach; probably for this reason you find it more difficult to use, but just a change of mindset and some hours passed on coding will make you change your idea about this. 

find the number of contact points (contact point = {pair, position, normal} ) divide the ball into balls with the same center of mass (basically sharing the same position and volume, of course not colliding with each other), but having the initial sphere's mass divided by now simply compute for each sphere the necessary impulse required to push it out of the wall when all contact points are solved by finding appropriate impulses, just add those impulses together and apply them to your big sphere Inside a frame, to assure convergence, you can perform 4 such iterations to find a solution (relaxation). 

Is there any resource location on how to view a 3D scene from an application or a game on multiple windows or monitors? Each window should continue drawing from where the neighbouring one left off (in the end, the result should be a mosaic of the scene). 

The approach I proposed is not by a long shot an original contribution, many games use it with plausible results and it was best employed by Jakobsen in his Hitman game engine. From a somewhat practical experience, penalty forces (similar to linear or exponential springs getting their input from the penetration distance) do not properly solve penetrations when other forces from the bodies that collide manage to be greater than them. That's why I chose to combine three (almost redundant) approaches: Newtonian reaction forces (you push the wall, the wall pushes back), impulse derived velocities (snooker balls colliding) and a non-natural "move the bodies away from each other geometrically" solution. Together they seem to provide everything: get rid of most ugly interpenetration artifacts, colliding bodies tend to interact with each other on the long run (due to restitution velocities and forces - at least the forces that tended to drag the bodies in a collision scenario are cancelled out and the bodies bounce away from each other). Lastly, for further understanding of these simple but common concepts, I suggest analysing these slides. My "abused method" epithet describing the Verlet integration steps is targeted at a popular culture belief that this is the Holy Grail of integration methods. It is just marginally better than its Symplectic Euler (also called by some semi-implicit Euler) cousin. Way more complicated integration methods exist (and all bear the implicit name in them). Powerful game engines make use of them, but indie developers don't have the time to experiment with those since Verlet, when tuned to a specific scenario, really does wonders. Also, there is absolutely no integration method that can deal with stiff constraints without a little cheating being involved (can't find the link, but the paper I'm referring to should be called "X.Provot - "Deformation Constraints in a Mass-spring Model to Describe Rigid Cloth Behaviour" ". 

There are frameworks like FreeGLUT that give a basic input/output abilities with keyboard and mouse, but if you want more there are the Visual C++ APIs for Windows. 

By the way Collada is just XML, I would just remind you that, the default .blend format file is probably not so standard, but there are prepacked alternatives. 

I know this 2 products but i have never used them for production, since I have a bunch of cool shaders i would like to use them under OpenGL with GLSL but i don't know where to start. If it's not possible to convert them directly, where i can learn the Renderman syntax for the shaders? 

If i have a vertex shader that manage some movements and variations about the position of some vertex in my OpenGL context, OpenGL is smart enough to just run this shader on only the vertex visible on the screen? This part of the OpenGL programmable pipeline is not clear to me because all the sources are not really really clear about this, they talk about fragments and pixels and I get that, but what about vertex shaders? If you need a reference i'm reading from this right now and this online book has a couple of examples about this. 

you will be fine with a commercial solution, otherwise if you want the maximum flexibility and you have the know-how, you probably want to code your own stuff and avoid spending money and legal issues. Also all the software that you mentioned offers legal problems when it comes to using them on the job, some of them offers more complex issues, because for example the UDK, it's not really free for every use, if you are going to use it in the place where you work, you have to pay, no matter what you are producing with it. There are also nasty things like the standard Autodesk EULA allows Autodesk to basically fetch your computer for data without explicit warnings ans in "silent mode". If i was you, i would switch to Blender and Gimp, this 2 are really powerful software, with a rich set of APIs and 0 legal issues, and they are free. 

If an active program cannot find an uniform in neither places, you should decide whether you supply default values in the program/material file or if you somehow forbid such a possibility by throwing exceptions. 

The problem: A player entity wishes to throw a ball from a known position with a known initial velocity. The surrounding environment contains physical objects that can alter the trajectory of the ball. How can this trajectory be as accurately as possible be rendered? One solution: perform small steps and simulate throwing the ball. During each small step, a raycast is performed against the world to find potential colliding bodies. In case a collision is reported, it must be handled accordingly. The intermediate positions of the ball will then be saved in an array and the trajectory should be described by the succession of these points. While this solution works (it is tested and doesn't fail almost never :) ), are there any workarounds or possible optimizations that can be done to avoid performing the raycast queries too often? It would be nice if there were such a thing as a parabolic caster instead of a linear ray caster. Do you know of any existing solution that is better of the one I just described (that either avoids those many raycasts or performs a more clever kind of query to reduce the number of intermediate steps and replace the actual line segments with parabola segments)? 

I'm starting with the programmable pipeline and the shaders in C++ for OpenGL 3.0+, i would love to be able to change some settings on the fly, for example replacing a function with another function, supposing that i have a shader with an operation like 

There is this Windows Advanced Rasterization Platform on the most recent Windows platforms which i think it's what you are looking for or there are commercial solutions like Pixomatic. 

The only real thing that is different is the amount of devices, Apple just sell 1-2-3 new product each year, Android offers 1 new product every day/week. The emulator it's not buggy, it's just not intended for profiling, if you want to profile an Android application you have to do the same thing that you have done for iOS: consider the lowest profile device that match your requirements and buying it. You are supposed to have at least a basic know-how about the ARM architecture, otherwise you can make a difference between all the devices on the market, begin to outline the hardware features that are important for your application and buy that device for real testing. 

Safe? No, in theory and in practice not even the ones that you pay are safe, the fact that this images are free or not tells you nothing about their copyright and most important of all, who is the copyright holder. The real solution is more like a good suggestion: always mix and work with this textures in way that you will never use just 1 big texture alone. If you really need a complete big texture of something you better go out with your camera and take some pictures of the natures avoiding famouse places/buildings and private spaces. 

Sounds like your engine constantly tries to update the parameters of that object. The object should freeze in position once it reaches a state that Bullet has decided upon to be "stable". Otherwise, numerical errors won't ever result in accurate collision handling, therefore that jittery motion (forces build up eventually, objects are penetrating again, oscillations result). Some ideas are mentioned here. I would start investigating the "sleep issue". Perhaps someone who has hands on experience with this engine could provide a way to set the sleeping threshold efficiently in your case. 

We know the user can drag the slingshot by defining a vector having its origin at the tip of the downward pointing triangle (like in my figure). The user can define thus vectors that point toward the bottom of the screen (restricted programmatically), and whose radii cannot be larger than a dMax threshold. Whenever the user shoots, the input vector has a length between [0, dMax]. This interval must map into a range interval: [rNear, rFar]. But since the range in real life is usually not a linear function of the launch velocity, we're free to suppose that the transfer function between [0,dMax] and [rNear,rFar] is looking somewhat like this: 

The tiny details and patternless distribution of ornamental textures make me think that these terrains were not generated using a standard heightmap-blendmap method. 

If your two quaternions are and , they represent the same rotation if either of these two conditions hold: 

This isn't clear for me, if i use the drivers from the GPU manufacturer and they support OpenGL 3.0 and/or above, i can always make an OpenGL ES 2.0 application work? 

Well, Game engine is a generic term, Physics engine is more specific, the "problem" is that the functionalities that a game engine provides are up to the developers that have coded that particular game engine. There are very basic game engine that have no physic support or they expect you to add to it manually, and game engine that support physics and fractures in real time. Your view shouldn't be about how they work together in the first place, just look at what a game engine offers and if you need a physics engine add it to your code. There are also some engines that mimic the physic with pre-baked collision and explosions, there are several approach to this, depending on what you have, what you want to achieve and what is your target machine, you better look to the features and how they are implemented, only the name "physics engine" can't tell you what you are dealing with. 

Only the person, the group of people, or the activity that owns the code can change its own license. If that code is yours you can change license every time you want, decide to be payed or not, you can do what you want, but if the code it's not yours you stick with the license if you want to avoid troubles; big troubles. The owner can also refuse to accept to be payed, it's a situation like the one that you have with the patents, if you can prove that you own the code doesn't mean that you are putting it on the market for money, many many times the patent or the license is supposed to force the market or the user to stay in a confined space. The answer is no and the only way to change the license is hoping that the owner will do that. 

UPDATE Basic maths: if your input is a variable called X and lies in the [a,b] interval, but you'd like to have it translated to the [c,d] interval, the common sense way of doing that is via a linear/affine transformation (affine operators are functions of the form AoX+B where o is a multiplication like operation - A and X could be matrices or other multidimensional elements). For a scalar x in [a,b] we have the following transform: 

Scenario: several objects (o1, o2,.., on) have to be rendered with the z test disable, but the z values must be written to the depth buffer. In another pass, some other objects (t1, t2,..., tm) need to be rendered considering with the z test enabled, and considering the prior values from the previous pass. Is it possible to achieve this with Unity's material script? The goal is to support a custom order for the rendering of transparent objects (the o1,..,on in the scenario description) alongside with matte objects (t1,..., tm). I tried using the features mentioned here, but the results were incorrect, i.e. as if the z values were completely discarded inbetween passes. Can anyone, perhaps, share a code stub for this scenario? 

See the paper in the GPU Gems series treating this subject: $URL$ What you can do is to adapt that idea (Gerstner waves) and compute the normals for each of your rendered fragment. The way to do that would be to assign a water texture (without too much light information in it, since you're gonna compute colours on in the shader anyway). That texture also comes with a texcoord set mapping (for each of your 4 vertices you get an uv pair of texture coordinates in the range of [0,1]). All in all, you have a plane you can deform in the fragment shader: the [0,1]x[0,1] square described by your uv texcoordset. Each uv pair that will be interpolated and present in your fragment shader can be used to recover a normal vector like you would in the vertex shader. That is useful only if the water surface is relatively far away from the camera: since you can't deform/displace vertices, you can observe light reflecting in different directions and shading that surface according to a time variable. It should work in a sense and it should be easier and more convincing than distorting/moving a texture through texcoord manipulation as older game effects do. Implementation details 

I think that you are approaching this on the wrong side. Question: what you need to license, the source code or a compiled software? In the first scenario you probably want to stick with something like the GPL, BSD or MIT licenses, in the second case you probably just need an EULA. You also can mix this 2 requirements but i don't think that you need to give a license for your source code in your case, the user will never see your source code; you also appear to not being interested in patents, and patents are the only way to prove that you own a particular asset of your software or you own the rights for an UI, a file format, or some other pieces of your code including the design and its own implementations. 

There are different technologies for this, there is no standard, at least no one that i'm aware of. The multi-monitor technology from ATI is named eyefinity and it's probably the most mature technology among the ones available on the market. The eyefinity capabilities are accessible through the AMD display library SDK . Nvidia calls its multi-monitor technology nVidia Surround and there are little to none informations for the developers, there is this page that mix the surround technology with the 3D technology and i don't think that is useful at all. If you are interested in this you can try to browser and ask in the Developer Zone. 

So, instead of multiplying quaternions and achieving a chaotic wobbly effect, you should only update the angular position and use that to achieve the rolling effect. Later edit: What happens when your sphere moves from position A to position B: 

Say now you want to find out how to transform directly a point Qb into a point Qa (notices that the b and a following the letter stand for the frame that vector/point is expressed in - Q is the same physical point, but it is seen from different places with different measurements). You do the same and express Qb in the world frame as Q and then take this Q and express it as Qa: 

As mh01 suggested ("Viewing the generated assembly in PIX or a similar tool can be very helpful here."), you should use a compiler tool to examine the output. In my experience, nVidia's Cg tool (Cg is still widely used today because of its cross platform capabilities) gave a perfect illustration of the behavior mentioned in the GPU gems condition codes (predication) paragraph. Thus, regardless of the trigger value, both branches were evaluated on a per fragment basis, and only at the end, the right one was placed in the output registry. Nevertheless, the computation time was wasted. Back then, I thought that branching will help performance, especially because all fragments in that shader relied on an uniform value to decide on the right branch - that did not happen as intended. So, a major caveat here ( e.g. avoid ubershaders - possibly the greatest source of branching hell).