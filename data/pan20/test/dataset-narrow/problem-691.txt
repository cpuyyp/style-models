Small differences due to precision are expected and can usually be ignored. 12 shuffles like that are a bit much, though not necessarily avoidable, depending on whether there is AVX support. With AVX, it is better to literally broadcast from memory, rather than emulate broadcasting with a load and shuffles. Even though this means there will be more loads, the shuffles are a bigger problem: loads (even broadcasting loads) typically have a throughput of 2/cycle while the shuffles typically have a throughput of 1/cycle. This means that is actually the better option, as long as you compile with AVX support. That wouldn't use the wider width yet though. Also annoying is that MSVC apparently likes to reload those pointers etc continuously. There is really no spare load-throughput for that, despite the presence of 9 FP additions (which are also bad though, since they're 1/cycle pre-Skylake), there are 21 loads there, limiting the throughput of the loop to once every 10.5 cycles (which is more than 9, if barely). Fortunately it can very easily be convinced to stop doing that, simply by copying them into local variables before the loop. Next, I don't know what MSVC is up to there with its refusal to contract addition and multiplication into FMA, for processors that support it it should definitely be used. Using the corresponding intrinsic does the trick of course, but that makes it harder to compile for pre-FMA targets (Ivy Bridge and older). Anyway if I write the high level code like this: 

Which to be fair is super common and it is the most obvious way to do. But it has a problem: for a long enough vector, can overflow. That is undefined behaviour in C++, and it is also actually bad in practice. The most likely consequence is that results in a (very large) negative number, subsequently dividing it by 2 still results in a negative number that is only slightly less big. Indexing into the vector with a negative index will very likely crash. Here are some ways to avoid that: 

But that is not great either. Ideally there should just be no horizontal operation in the inner loop. Even more ideally, not anywhere. And that can be arranged: a block of 8 results can be computed by rearranging the computation so that horizontal operations turn into broadcasts. Broadcasting from memory is pretty cheap (naturally it sort of "wastes" the load by loading only one thing, but it's not a slow operation), and there are much fewer of them, so that's probably better. Sort of like this (to show the general idea, not tested) 

I've done some testing now, and made some improvement specifically for larger matrices (below N=64 it doesn't really help). Some results on Haswell, compiled with MSVC[1] 2017, measuring the time (in cycles) per element of the result matrix (so you can mentally compare it to how much time it should take). Time results were eyeballed and rounded to a "typical" value. 

[1] MSVC is a C++ compiler so my examples are technically not C, but it's really about the approach anyway and it's trivial to convert. 

I've also changed some thing to , because some useless sign-extension was going on sometimes (depending on how it was compiled). and are now calculated outside the inner loop, normally we can expect the compiler to be pretty clever about that too but compiling with OpenMP support seemed to make Clang less clever, it put the actual multiplication between and inside the inner loop. That's not good, because on Intel that would go to p1, and p1 is already completely packed with the float-to-int conversions and fp-multiplications (eg on Skylake both of those go to p01). That loop should be able to run at 1 iteration every 4 cycles in the best case on most architectures, but with that extra in there that wouldn't be possible. To convert to something similar can be used, but with and no packing to bytes. 

So with conversion to , especially useful if you actually wanted a floating point division all along: 

Yet an other version, with even more tiling and with rearranging matrix 2. Added to the table of times above. Of course, some time can be saved if that matrix can be assumed to already be in that order, but I counted the rearranging in the benchmarks. That overhead scales as O(N²) while the meat of the algorithm scales as O(N³) so for a large matrix it does not represent a significant cost anyway. It seems to behave well now, staying around 110% of the theoretical optimum for any size I test. Perhaps some small tweaks are still possible. For example, unrolling the loop by 4 instead of 2 improved it slightly in my tests, but the difference is kind of small. v5: 

This loses the print-back of the input, if you want to preserve that you could use an . By the way, in Java 8 you can use to make a comma-separated string from your list, you don't have to do that manually. 

Getting good performance out of matrix multiplication is tricky. Many concerns have been addressed already, but there are a couple of big ones that I have not seen mentioned. The main issue in medium-large matrix multiplication (for tiny matrices it's 100% micro-optimization) has historically been, still is, and will always be: reuse of data at several levels. The most obvious is at the cache level, but even at the register level you must economize. For example, on a modern Intel processor (similar principles are fairly universal, but that's where the exact numbers will come from) it is possible to execute two vector FMAs (fused multiply-add) in a cycle, and it is possible to load two vectors from memory provided the loads hit L1 cache. Two FMAs together have 6 inputs, but you can load only 2 things, so in order to get anywhere near a good performance there has to be significant reuse. In a dot-product, you can reuse the summation variable(s), but that is not enough, that still requires twice as many loads as can possibly be executed. So a perhaps surprising conclusion is that you cannot do just one dot-product at the time. There would have to be several in progress side by side, to enable sufficient reuse of data. Also you might think about different computational structures than dot products, such as small outer products (of rectangular, yet not too narrow, tiles). An other (but related) issue is that while many FMAs can be executed per unit time, any individual FMA is actually quite slow in terms of latency. Of course it is not surprising that it should take at least one cycle, and even that is already too long to be executing two FMAs every cycle with a naive dot-product. The problem there is that the sum variable takes a while to be updated, and this will delay the next FMA if the code is written the obvious way. Actually a latency of 4 or 5 cycles and a throughput of 2/cycle is common enough (Haswell through Skylake, Ryzen). That means a simple dot-product, even if vectorized (scalar is not even worth considering), can be 8 to 10 times as slow as a proper implementation, though "fast math" flags can alleviate this. Anyway this problem also disappears when performing a sufficient number of simultaneous dot-products - every individual one of them will still be about as slow as a naive implementation, but the total throughput can be increased to near the theoretical maximum. Once you have written a fast kernel like that (for reference, "fast" means close to 32 sp fp-ops per cycle on Haswell-Skylake and close to 16 on Ryzen), there are still more levels of data reuse that must be taken care of to get good results for large matrices. Loop blocking is often mentioned, but what's less commonly noted (and actually super important) is repacking the current tile from the matrix into a contiguous block. Without repacking, the tile may indeed fit in the cache, but may span more pages than the TLB can simultaneously cache address translations for. What the right tile size and shape ratio are, is also an interesting question that I frankly don't know the answer to, except to experiment. I don't really have great ready-made code to show you for this, it really is difficult and takes a lot of engineering and experimentation to get right, taking care of more (and weirder) details than I addressed. The good implementations that you can find elsewhere have had a lot of work put into them. I did give it a try, but the results were not as close to what they should be as I had hoped, reaching 28.5 sp fl-ops per cycle at best on as Haswell, and that was after writing a highly unrolled loop in assembly using all the vector registers and with various dirty tricks such as loading some data an iteration in advance and pre-computing addresses to avoid some complex memory operands with associated µop delamination, and despite my best efforts with tiling/repacking I still saw a performance drop-off for larger matrices. 

The unsigned average of two non-negative signed numbers neither overflows nor wraps. Converting the result back to signed is always safe. 

This dominates not just horizontal screen space, but also the performance (or lack thereof) of the loop. Actual extracts (an extract with an index of 0 is turned into by a reasonable compiler, which is less of a problem) have a throughput of only 1 per cycle on typical CPUs. There are 7 here, so even with just this horizontal addition the loop body could only execute once every 7 cycles (but there is other stuff in there too so it's worse). There are slightly faster ways to do horizontal sums, for example (not really tuned or anything, just some simple "better than totally naive" hsum) 

There is some room for improvement. The memory access pattern through is not ideal, the inner loop iterates over the rows and only half of a cache line is used every time, for a large enough matrix that means half of every cache miss is wasted. It's probably better to unroll the middle loop by 2 (or 4) again. As a bonus, we get to re-use the broadcast from . Sort of like this (not tested, showing 2x) 

One problem that immediately jumps out at me is the loop-carried dependency through , which has a latency of either 3 or 4 (depending on the processor) while there are not nearly enough instructions there to fill all that time, so it's lost throughput. The typical solution is unrolling and using multiple accumulators. There's too much stuff there for me to comfortably write the actual code, but the general idea is something like this: 

Subtracting a smaller non-negative number from a bigger non-negative number is always safe. Adding half of it to the lower number is also safe because the average must be somewhere "between" the inputs so it cannot go out of range. 

is based on the throughput of on Haswell, which is one every two cycles. So per cycle there can be 4 multiplications, we need N of them, so N/4 is the ideal time per element. For small sizes that's not so hard to get near, but for bigger matrices the memory access pattern messes up everything. v1 and v2 are the versions from above. v3 adds loop tiling to significantly improve the performance for bigger matrices. Understandably this causes some overhead, noticeable for smaller matrices. v4 unrolls the loop by 2x. Annoyingly, the best choice for the block size depends not only on cache size, but also on the size of the matrix. The times above are not all with the same parameters, but tuned a bit per N. I'm not sure where to go from here but it seems as though some improvement should still be possible, for bigger sizes it's still a decent factor away from the ideal. v3: