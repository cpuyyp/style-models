Judging from the strictness of the requirements, I believe there is no formal name for all those requirements. I would call it a directed tree (the term polytree seems to be valid according to wikipedia, but I have never seen it being used). I have used graphs that are close to your description in distributed computing scheduling, where we just called them "graphs" or maybe "scheduling graphs". In these graphs, there were two classes of nodes : The "branch" nodes are like the routing nodes you describe and there is also activity nodes, which usually depict "activity" or "computing" done in a processor. 

I guess that you might be more interested in complex analysis used directly in the proof. However, here are two examples from a graduate level Algorithms class I am currently attending: a) Fast Fourier Transform, for example used in polynomial multiplication. Although the implementation can be done with modulo arithmetic or floating point (and some arithmetic analysis), the proof is best understood in terms of complex numbers and their roots of unity. I have not delved into the subject, but I am aware that FFT has a wide range of applications. b) In general, equipping the RAM model with the ability to handle complex numbers in constant time (the real and imaginary parts still have finite precision) allows one to cleverly encode problems and exploit properties of the complex numbers that might reveal a solution (see also the comments why this won't allow you to be faster). 

edge_weight = pointing_node_weight. Find edge starting from "pointing node" with the maximum weight. Let this weight be next_edge_weight. edge_weight += next_edge_weight 

Edit: Upon further thinking, here's how you can find a problem in $\mathbb{NP}$ that suits your needs: 

I have coded exactly such a system in the past. Are you studying a peer to peer content delivery network by any chance? I don't believe it is easy to create an analytical solution to this My study was based on simulation, which was around 2k lines of code built on top of a simulation library. The simulation would be more useful too, because it can be adjusted easily to different parameters, whereas developing a general mathematical model with a lot of variables wouldn't be so easy. 

I believe the "100 years" statement refers that the theory is general, but requires deep understanding and new results in representation theory and algebraic geometry to progress, something that might be slow to progress (I want to make a comparison to number theory, but I am not sure how apt it is). Also, there is a loss of precision when translating to the algebro-geometric world: Instead of proving a lower bound against the properties of a complexity class (i.e. polynomials that vanish when objects in that class are given as input), you are proving it against its Zariski closure (of aforementioned polynomials). It is conceivable that in order to separate the two, one has to examine the boundary of that closure (those polynomials that occur only in the closure but on the original set). It is believed that in the determinant vs permanent variant of the GCT program, this is likely the case. Finally from personal experience, the skillset required to understand GCT in depth is quite different than what is usually the focus of undergraduate or even master programs in CS, essentially picking up the prerequisites is a natural follow-up of choosing to study GCT. 

A matching $M$ is called perfect for a graph $G$ when all vertices of a graph are matched in $M$. Computing the number of perfect matchings is a well-known #P-complete problem and shown to be equivalent to computing the permanent of the biadjacency matrix of the graph. Now note that each perfect matching, by removing edges produces $2^{n}-1$ different matchings (consider each subset of edges, which is a smaller matching, we do not consider the empty set). Also, each perfect matching prohibits exactly $\binom{n}{k}$ matchings of size $k$ from being maximal but non-perfect, since they are subsets of this perfect matching. Given those kinds of restrictions, one can certainly upper bound the number of perfect matchings given the number of matchings and perhaps it is also to use additional information to do a precise calculation. Perhaps you should take a look at the paper "Two-dimensional monomer-dimer systems are computationally intractable" (reference 5 in the wikipedia article). It seems to rely on the fact that counting matchings (all of them, see the comments) remains #P-complete for planar graphs. Interestingly enough, counting perfect matchings for planar graphs in in $P$, i.e. can be computed in polynomial time (in a deterministic way). 

From the question I assume you are talking about exact algorithms and not approximate ones. Since this is the case, you will have to "write" the entire input, using the same space as before, just using a different structure. Although this might provide a speedup in time, the space used will be at least the same, assuming that you use a reasonably efficient representation (e.g. not a unary one). Using less tape is usually achieved by space re-use or by carefully selecting which cells to overwrite, so that you can use some information later on the computation. However usually you cannot re-use or overwrite the input, since you need it throughout the process of solving the problem. Even if the above problems were dealt with, the usual model for space complexity has a separate input tape, which is read only and is not measured as used space , so we can have sublinear space complexities and avoid the problems above. Perhaps you are referring to a different model? 

Now, for most classes you should probably not be able to answer all those questions, but you can answer some, maybe most of them. Now you have an idea how this class behaves and whether it would be interesting for your line of research. If you deem it worthy, you can start asking people from the area, e.g. your advisor or a faculty that is familiar with the subject, what they think. 

Every $s \in L$ of length $p>0$ can be written as $xy^{i}z$ where $ x = z = \epsilon $ , $ y = w \neq \epsilon$ . It's obvious that $|xy| \leq p $ and $ |y| = |w| > 0 $. It follows that the language is regular for non-empty inputs, by the pumping lemma. For $ w= \epsilon $ , the definition holds, since a NDFA that accepts the empty string will also accept any number of empty strings. The union of the above languages is the language L and since regular languages are closed under union, it follows that every circular language is regular. By Rice's theorem, $CIRCULARITY/TM$ is undecidable. The proof is similar to regularity. 

Recently, a new Sino-Danish institute was announced, Center for the Theory of Interactive Computation(CTIC). From what I've read, I got the impression that Chinese students are welcome to visit Denmark for their PhD and vice versa. You will probably be aware of the Erasmus Programme, however I am not sure if Chinese students can apply to it directly, they can certainly do once they join a European university as students and spend a seimester visiting another university. 

I assume that the edge weights express how much the users interact to one another, rather than the amount of communication , i.e. the communication is always mutual and not in risk of being just one-sided. This is more an issue of modeling and not of the problem itself. The sets in general are not disjoint, as a cut would require them to be, unless you are referring to the subgraph of a certain person's friends (the adjacent vertices). In the social network graph, complex relations can be formed. For example, think of a triangle graph of 3 friends: You (A) might care for a friend B who also cares for your mutual friend C, however you ignore C. It is unclear in which subgraph C belongs. Experimenting with graphs allows you to find a lot of examples where the two sets are not disjoint. Even if the subgraphs were disjoint and you wanted to know how communities, i.e. groups of people that care for each other, appear in the graph, this is the min-k-cut problem , which is NP-complete, unless you know the number of communities (which in general you don't). However, you can easily obtain the two subgraphs you want, by selecting a threshold and then on one graph include all edges (and their adjacent vertices) that pass that threshold, constructing the "who-cares_about-who" subgraph and in the other one include the edges that do not and also invert the weights, so that the bigger the weight, the higher the lack of interest between two friends. The problem with this approach is that deciding the value of the threshold is an optimization problem which seems to be reduced to the previous NP-complete problem. Still, you can use a number of techniques from statistics and experimental data to have a fixed threshold, perhaps associated with a level of confidence. Furthermore, you can use fuzzy logic so that the threshold is an interval instead of a fixed point, increasing the accuracy but risking false positives. 

Compare the given instance with the instance encoded in the machine. If they are equal, return the solution encoded. Else, do a brute-force search. 

Split the original universe U into two parts, A and B, that are equal to U and obey its theory.Then make an addition that is unique to every part: Assume that in part A S holds, while in part B of the universe S is false. This is not the same as saying that S or its denial is an axiom in the respective part. It might be possible that a proof exists, however you are not aware of it (yet). If S is false in both parts, then it must be also false in the whole universe. In the part B of universe that it is false (proven so by an unknown proof) you don't have to worry about it. If this argument does not sound very convincing, you can consider part B as small as you like: for example, if B consisted of only 2 atoms, you could easily verify the law of energy conservation. Now, consider the part A. If you could show that A does not obey its own theory,that is the original theory and the theorem that S is true, it could not exist. However, you know that the original theory is true, therefore it must be that last theorem that does not allow part A to exist. Therefore, since part A does exist, S must be false. Now you have that S is false in both parts of your universe, so it is false for the whole universe as well. 

I will use the following formula for information gain: $G(S,A)=E(S)-\sum_{u\epsilon Values(A)}\frac{|S_{u}|}{|S|}E(S_{u})$ $A$ is a candidate for the separation attribute, that can take values from the set $Values(A)$ , $S_{u}$ is the dataset entries such that $A=u$ and $E(X)$ is the information entropy , where $X$ is a (data)set. If $E(S)$, the dataset entropy is zero, the information gain for all attributes will also be zero. However, this means that no decision process is required for this dataset, so we'll consider it a trivial case. If we demand $E(S)$ to be nonzero, we have that : $|S| E(S) = \sum_{u\epsilon Values(A)} |S_{U}| E(S_{u})$ This equality tells us that if we separate the dataset with the attribute A, the entropy will be the same (therefore the information gain is 0). If this is true for all attribute, you should follow the algorithm when the information gains of 2 or more attributes are equal and choose one arbitrarily. It is possible that in the next step the information gain of an attribute will be non-zero, for example: Assume that we have a dataset of 1000 people on whether or not they like going to the movies. We have recorded each person's age as young or old and its gender as either male or female. 800 people like going to the movies while 200 don't. Therefore, the initial entropy is $E(S) = 0.72$. If 80% of males and 80% of females do so, there is no information gain. Likewise for young and old people. We arbitrarily choose gender as the separating attribute. Now assume that for some reason, young males are more fond of going to the movies than older males, while the trend is reverse for females. In each respected subset of the original dataset, we will be able to use age to have non-zero information gain and thus give some meaning into continuing building the decision tree. This applies for "vanilla" decision-tree building. The use of any number of attributes for separation could locate the pattern in the example in just one step. There is a variety of techniques for how you should separate the dataset, that have spanned from problems like this.