You should definitely check out the Halo Wars GDC presentation, "The Terrain of Next-Gen." It discusses using full vector field displacement instead of simple height field displacement. For something a little less revolutionary, maybe check into geometry clipmaps. There's a good article in GPU Gems 2 here. 

I also asked about this over on the UDK forums, but haven't had much luck getting any responses. Basically, I have some experience with UT3 modding, but I'm just getting started with the UDK, and I have a few questions about the degree of control over the rendering you have. 

There's a chance this is the same problem I saw working on a UT3 mod a long time ago. If your pawn is at risk of interpenetrating the world geometry, the pawn is killed via the event FellOutOfWorld. This happens quite frequently if your pawn is forced to ragdoll and subsequently gets pushed around. I believe the interpenetration=kill hack was originally implemented in native code in UT3, and I couldn't tell you if it has since been moved out into UnrealScript in subsequent updates to the UDK sources. My workaround, taken from my pawn class, ended up looking like this (again, this is against UT3 code): 

Definitely not. is just a tool and you should use it when it fits your desired result. If your widgets are supposed to be laid out in a tabular way, then use . Otherwise, it's perfectly okay to position your widgets manually if that's what you need. Two alternatives to 

My question: Can I configure PhysicsEditor so that it saves shapes in this format automatically? If not, is there some software that allows that kind of configuration? 

I'd like to know if someone has found a way to build a component-based entity system in their game(s) without using IDs. I find that IDs tend to do away with one of the major (possible) advantages of OOP: not having to be aware of the type of a certain object. 

What good free and widely used tools are there for editing 2D skeletal animations? Preferably, one that allows me to write custom animation exporters. One pretty good indie tool that I know of Demina, but it's not ideal. It doesn't allow you to export the data as you want (although, it is open source, so you can change that) and I find it clunky in how you edit individual joints. What other tools would you recommend? 

Not a book, but you should check out the math curriculum over at the Khan Academy. I'm in the process of using these videos to brush up on my own math skills. They cover an extremely broad range of material, and the author has been praised for his teaching style. IMO, truly an amazing resource. $URL$ Algebra 

There are a few relevant resources on the UDN about this subject (keywords "level streaming"), and even a convenient "hub" article that links to all of them: $URL$ "The level streaming functionality in Unreal Engine 3 makes it possible to load and unload map files into memory as well as toggle their visibility all during play. This makes it possible to have worlds broken up into smaller chunks so that only the relevant parts of the world are taking up resources and being rendered at any point. If done properly, this allows for the creation of very large, seamless levels that can make the player feel as if they are playing within a world that dwarfs them in size." 

"Dedicated" in the configuration name refers to the game's dedicated multiplayer server. It's for sever operators exclusively, and runs an empty multiplayer server that exists solely to receive connections from other clients. You don't want that. Run a configuration that starts with "Debug" or "Release" instead of "Dedicated." 

I have a very general question: In games, what use does the programming concept of a window have? Or, in other words, why do some game dev libraries offer interfaces through which to create multiple windows? — Why would you need more than one windows in a game? Are multiple windows used as different views/states of the game? (I.e. in-game, main menu, pause menu, etc.) 

Is there any editor out there that would allow me to define complex entities, with joins connecting their multiple bodies, instead of regular single body entities? For example, an editor that would allow me to 'define' a car as having a main body with two circles as wheels, connected through joints. Clarification: I realize I haven't been clear enough about what I need. I'd like to make my engine data-driven, so all entities (and therefore their Box2D bodies) should be defined externally, not in code. I'm looking for a program like Code 'N' Web's PhysicsEditor, except that one only handles single body entities, no joints or anything like that. Like PhysicsEditor, the program should be configurable so that I can save the data in whatever format I want to. Does anyone know of any such software? 

Direct3D 10+ applications don't have this concept of a lost device state, and avoid most of this bookkeeping work in the process, I believe by virtue of the fact that the WDDM implements "virtual memory" for video RAM. That might explain why you wouldn't see this behavior in Metro applications. 

However, this will move the entity's left edge to the colliding tile's left edge. That's bad - it places your entity squarely inside the collidee. Your top and left checks need to take into account the width and height of your entity. 

I swear I added this an answer to this question back closer to when it was originally asked... Check the "max frames to render ahead" setting in the NVIDIA Control Panel, or whatever the comparable setting in the AMD drivers is if using an AMD card. I believe it defaults to 3, which would correspond exactly to what PIX is showing you (the GPU is rendering 3 frames ahead of what it's currently displaying). 

If you were to employ post-process antialiasing such as SMAA, it would not differentiate between geometric edges and texel edges. This might be sufficient in conjunction with the nearest neighbor texture filtering. 

I'd like to know how others have handled the issue of storing the entity's position. (Or maybe it's not an issue and I just make it too complicated.) I'm undecided on whether to store the position of each entity in the world map file or in the entity's script file. From what I figured, both approaches have their own good and bad points: If you store the entity's position in the map: 

An event system will do for a lot of cases, but that is more suited for propagating information that changes or is generated 'rarely' (like the death of the player). For something like entity health and position — which need to be known every frame — an event system isn't well suited. 

Imagine I have a skeleton — that is a set of bodies held together through various constraints and joints — and I want to flip it. Bodies cannot be flipped in Box2D, so how can I fake that? Here's an example: I have a humanoid skeleton, made out of different Box2D bodies: the head, torso, upper arm, lower leg, etc. These Box2D bodies are held together by joints. Some of the joints have angle constraints, like the ones between the upper and lower arm, which do not allow the 'elbow' to twist unnaturally. When I turn my character the other way around, I should flip the skeleton and its joints should be flipped as well. How can I achieve that in Box2D? 

I would say that the most time-consuming task that needs to happen in most games is recreating video memory resources (textures, render targets, meshes) in response to a "device lost" event (note that this is specific to Direct3D 9 and earlier). Taken from MSDN: $URL$ 

There are so many special case considerations with the simulation of wheeled vehicle physics that you are unlikely to get a satisfying result by simply torquing cylindrical collision primitives. You should consider instead using the special Wheel Collider component. 

Have you read Johannes Spohr's thesis on "Pace" and its renderer? It describes a so-called "submission engine"* parallel renderer, and may give you some ideas. Here is the summary page (in German), and here is a direct link to the PDF which is in English. (*: this link also goes to the article where I originally heard about the thesis.) EDIT: I'd only skimmed this previously, and I just looked at it again... and realized it really glosses over scene graph details. I guess I didn't realize how orthogonal his design was. Sorry if it's not particularly helpful. 

Is it possible to override all the materials on the client (en masse), such as those being used for the terrain or BSP geometry in the current level? (For implementing alternate vision modes and things of that nature.) 

If you have only a basic knowledge of C++, I don't think that's going to be enough to (easily) develop something like a platformer. Python is an easy to learn language so it's not a bad choice at all. If you pick C++ I recommend you use SFML for graphics and sound, and either Box2D or Chipmunk for physics, if you want such a thing. If you do pick Python have a look at Pygame (graphics) and Pymunk (physics). But developing a platformer isn't exactly easy; you might want to start with something easier, like an adventure game (where you only click around the screen and you don't have to worry about complex character interactions). 

You mentioned you want the game to be flexible. Do you mean that as only adding new features (better lighting, physics, etc), or also flexibility in terms of game-design? If you also want the game-design part of the development to be flexible, component-based entity systems would be pretty useful. Unfortunately, I can't give you a competent answer to your second question. If you choose to implement a CBS, there's two important things you need to ensure: 

I'm writing a component-based entity system and one of the components is the entity's state, which dictates how it reacts to game events. In case anyone has experience with implementing states, how granular should they be? To give you an idea about how granular they are in my case, I have a state and a state (as opposed to simply a state) and I fear that they might be too granular. What do you think? 

Materials in the Direct3D 9 sense were simply parameters for the fixed function pipeline, which implements a Gouraud shading model (IIRC). The values of a D3DMATERIAL9 struct can be thought of as nothing more than shader constants. The definition of a "material" is entirely dependent upon context. It's the distinction between fixed function and programmable that is most important here. When you're using shaders, you define the shading model yourself. Therefore, even though many of the surface properties that are described by a D3DMATERIAL9 (e.g. diffuse color) would likely apply to your shading model of choice, it's up to you to do something with those values. The fixed function pipeline was essentially one monolithic "effect" (or pair of vertex/pixel shaders) that you couldn't change. You could only pass it different parameters to control the behavior, whether these parameters were render states or D3DMATERIAL9 structs. You might find this instructive: FixedFuncShader It's a partial reimplementation of the fixed function pipeline using D3DX effects. It'll help you understand the parallels between the two concepts. 

realtimecollisiondetection.net blog The blog of Christer Ericson, author of the excellent book Real-Time Collision Detection (who'd have guessed? ;). Equal parts general programming, physics programming, and graphics programming. Selected reading: