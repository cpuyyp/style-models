It seems that compatibility level can have an affect on the publishing and subscribing databases, with respect to object ownership and the existence (or not) of a particular schema. If you aren't seeing that, it's probably not a problem for you. As far as the distribution database goes, there doesn't seem to be any effect. I would leave the compatibility level of any of the SQL Server system databases (master, msdb, tempdb, the distribution database and even the model database) to whatever MS set it to unless Microsoft or a third-party vendor tells me otherwise, gives me a reason and says that I'll be unsupported if I don't change it. 

The "bitness" of your workstation should not prevent you from performing a task on a SQL Server, regardless of what the "bitness" of the server is. I've used 32 bit workstations (and therefore 32 bit SSMS installations) to administer 64 bit servers for something like twelve years. I would suspect that SSMS 2005 might have problems when administering SQL 2012. I would want to upgrade my workstation's SSMS to the same version as whatever the newest server that I have to administer is. SSMS compatibility usually works "backwards", meaning that SSMS 2012 can handle SQL Server 2005, but not the other way around. 

Since no one else has answered, I'll share some opinions and do some hand-waving. As long as you aren't locking common resources, or are locking resources in the same order, you shouldn't have problems with deadlocks. I'd look at separate tables before separate databases. Each additional database will definitely cost you more but additional tables won't necessarily cost you more. You might need to use more than 1 database becuase of the sheer volume of data you will store or because of the rate at which you need to store your burst traffic. If you can manage it, I think that a table-level granularlity will be more flexible and possibly a good deal cheaper than starting with a database-level granularity. The problem with putting each device's data into it's own tables is that it makes the reporting hard since all of the table names will be different. I presume that you have some way of detecting when you get a "failure resend" of data. You don't want to put the same value in a table twice and I i'm sure that the devices can fail (local power failure?) in ways that have nothing to do with whether or not earlier values where properly stored. WAG: Assuming each "value" is 4 bytes, I calculated about 11.5 MB of collected data per device, per day. (This ignores all kinds of stuff, like device identifiers and timestamps, but I think it is OK as a rough estimate.) So, with "thousands" of sites, we are looking at tens of GB, per day. You don't mention any kind of lifetime on that data. The largest Azure database currently maxes out at 150 GB. You could fill those up pretty quickly. Getting anything to happen in a web browser in a short period of time is iffy. When you are reading from (possibly multiple) databases with GBs of data, continuously inserting lots of new data into the tables you are reading from and interacting with web servers across the open internet, "real time" is wishful thinking. IMO. "Fast enough" is the usual goal. If you can't keep all of the data you need in a single report in one SQL Azure database, it's a problem. There are no linked servers or distributed views (at this point). There is no simple way to aggregate accross many Azure databases. You'd have to pull all of the data to a central location and report from there. I'd guess that the aggregated data would be too large to store in a single SQL Azure database, so you'd have to go to on-premise or maybe EC2. A data mart or warehouse with a star-schema structure would be the classic answer there, but that takes significant processing time and that means no "real time". Also, that's potentially a lot more data transfer from Azure to wherever it goes, and that will cost you. I wouldn't commit to this strategy without a pilot program first. The first thing to do would be to build a single instance (can it handle 400 sensor values a second? (Is that a series of rows, a big denormalized row, an XML document or something else? The format of the incoming data will affect how fast the data can be stored. Can you do bulk inserts, or does it have to be row-by-row?) How about 4,000 sensor values a second? It's possible that an single SQL Azure instance might not be able to store that much that quickly.) and see how it handles insertions at your expected rates and see how the reporting might work. And I'd talk to Microsoft too. Just dealing with the billing for hundreds or thousands of seperate databases might be quirky. I don't know if this is applicable to you, but have you looked at Microsoft's "Stream Insight" product? It seems to be aimed at situations like yours. Caveat: I've never used it. The marketting blurb: Effectively analyze large amounts of event data streaming in from multiple sources. Derive insights from critical information in near real time by using Microsoft StreamInsight. Monitor, analyze, and act on data in motion and make informed decisions almost instantaneously While doing some quickly googling, I noticed a blog posting which states that StreamInsight available on SQL Azure as a CTP last year. it might be ready for prime time by now. Good luck, it sounds like an interesting project. 

I see that you found that TableDiffGui has bugs. Have you tried using TableDiff directly? You might not run into the same problems. 

In the past, I have used ErWin from Computer Associates and PowerDesigner to move schema between RDBMSes. It's been many years since then, and I would evaluate their current feature set if I had to pick something today. 

Is there a particular reason that your PK is clustered? Many people do this because it defaults that way, or they think that PKs must be clustered. No so. Clustered indexes are usually best for range queries (like this one) or on the foreign key of a child table. An effect of a clustering index is that it bunches all of the data together because the data is stored on the leaf nodes of the cluster b tree. So, assuming that you are not asking for 'too wide' of a range, the optimizer will know exactly what part of the b tree contains the data and it won't have to find a row identifer and then hop over to where the data is (like it does when dealing with a NC index). What is 'too wide' of a range? A ridiculous example would be asking for 11 months of data from a table that only has a year's worth of records. Pulling one day of data should not be a problem, assuming that your statistics are up to date. (Though, the optimizer may get into trouble if you are looking for yesterday's data and you haven't updated stats for three days.) Since you are running a "SELECT *" query, the engine will need to return all of the columns in the table (even if someone adds a new one that your app doesn't need at that moment) so a covering index or an index with included columns won't help much, if at all. (If you are including every column from the table in an index, you are doing something wrong.) The optimizer will probably ignore those NC indexes. So, what to do? My suggestion would be to drop the NC index, change the clustered PK to nonclustered and create a clustered index on [DateEntered]. Simpler is better, until it is proved otherwise.