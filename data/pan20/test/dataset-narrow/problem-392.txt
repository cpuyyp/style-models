Head on over and tackle Erland's excellent article - not only will it pay dividends today, but it will continue to pay off over your career as you solve this problem again and again. The solution that works well for your query today is likely to be very different than the solution you use for another query tomorrow. 

Because this is AdventureWorks and obviously not a production server, you're probably not going to get great answers here. I'd just restart the server. (Is that the right answer? No, but we're obviously talking about a development playground box here, and I wanna get you across the finish line quickly.) If you really wanted to troubleshoot it, try sp_WhoIsActive: 

Yes, but you don't need different instances to do that. You can do just one instance per server, and run different Availability Groups. In your diagram, just remove Instance B2 on both nodes, and run the reporting AG on the Prod instances. 

When you perform a transaction - like deleting 600K records all at once - SQL Server has to log what it's doing in the transaction log file as it works. Say the process takes 5 minutes, and 4 minutes into it, the server reboots for some reason. SQL Server starts up, opens the log file, and then needs to undo all that work (because the transaction never committed. To avoid having a log file that large, you can: 

As to what dezso said, ledgers are quite special depending on the business that is using them. You can try looking at other opensource applications with the same features and functions as your program if you're looking for a basis for your database. You can look for some opensource projects at Ohloh.net and Github.com But as a fellow application developer, I would suggest you take the time to read up on some basic database design best practices and principles. And a bit of normalization, at least up to the Third Normal Form. Having a faulty database can give you a lot of headaches in the long run when maintaining your application. Refactoring database changes aren't that easy with a mature program. 

I would personally go for the historic table approach with the supertype/subtype design because it will allow you to track the changes to each table independently while still being able to efficiently query the latest data from the non-historic table. But keep in mind to update the main tables with the latest info, i.e. update Printer with the values to be inserted in PrinterHistory. Use datetime instead of date alone so that you can accommodate for changes done on the same day, and instead of joining the historical tables, just view them separately to avoid conflict of joining them in the case that the supertype is updated independently from the subtype. But if splitting the hisotry table of the supertype and the subtype is not possible, then create a slightly denormalized history table for each device type. i.e. 

An easy php based web application to produce very realistic data to populate your table. Its online site has limitation, but if you download the source code and install it on your PC then you can generate every kind of data that you want. It supports csv,sql,html,etc file formats. 

When I insert a record I need to return the inserted id by . The problem is that the table is partitioned and in partitioned table I can't use . I run multiple queries at once so I'm in dire need of . Is there a way to accomplish this? 

I have DB with about 30M records in a collection with about 100GB collection size (total documents and indexes). I have a compound index that filters data based on user_id and some other fields like: , , etc. With I see slow queries of about 10s, 20s or even 40 seconds! I ran the exact same query and result is fetched less than 500ms (though it may get cached on second try). When I get that ongoing stat, I see the following lock status: 

You've got a couple of questions in here: 1) Any ideas on why the SQL Server process would not be following the limit set on the server? Max server memory refers to the buffer pool, but not everything that the server uses memory for. For example, SSIS isn't part of the SQL Server engine - it's another application that just happens to come free in the box with SQL Server. (Same thing with SSAS and SSRS.) Max server memory has no impact on SSIS. This is one of the reasons you'll often hear recommendations that you should separate SSIS onto its own instance - especially if you're using SQL Server Enterprise in virtualization. At $7k USD per core, you're talking about $28k worth of licensing here, and you're working with something like $500 of RAM. 2) Does anyone know a good way of profiling memory usage within SQL server? We can see that page life expectancy is dropping and more memory is being allocated to the process, but is there any way to see what SQL is allocating the memory to (i.e. procedure cache, cached results, etc). Yes, inside the SQL Server engine, you can use DMVs like sys.dm_exec_memory_clerks. If you're using the free Opserver tool from the StackExchange guys for SQL Server monitoring, look at the memory clerks listing. If not, you can start with the memory clerks query it uses. However, outside of the SQL Server engine - like with your SSIS packages - SQL Server DMVs can't help because this is happening out of process. You'll need to use conventional systems administration tools to do process monitoring and watch which processes use RAM. 

I have enabled SSL on with optional SSL connection: . I have used in order to obtain SSL certificates. SSLs works as expected and in file I can see that: 

When I edit my.cnf in MySQL 5.0 in filemanager and add the following settings, I get logged out automatically from Kloxo and when I remove these line and restart server everything is ok then: 

I want to insert about 14000 records in a junction table, but the problem arise when there is a duplicate key for unique(iq_id,q_id)? what do do? 

I've seen so many times that we use numbers like 5,6 or 10 at most. What is this concurrency? Doesn't it refer to number of simultaneous users in bench? EDIT I'm not talking about any specific benchmarking tools, I mean in general. In benchmarks for databases OR for apache servers we use low concurrent numbers. why is that the case? 

I have used to generate SSL certification. All the private key, chain and certificate is generated by `let's encrypt. Now to use it in I first merged and into a file called : 

There are four tough problems to solve when trying to anonymize data. I'm going to summarize an old blog post of mine about it: 1: Anonymized data may grow in size. If you have peoples' names, for example, you don't really want all of the anonymous ones to have exactly the same length as the original data. If you change the data as it goes out, that means the solution needs to understand the original field name's length to avoid growing data out to something too large to fit. 2: Anonymized data has different statistics distribution. Say you've got a date-of-birth field - you could completely randomize the dates, but then it'll be much less meaningful on the other side, especially if you're trying to do performance testing. Plus... 3: Anonymized data breaks referential integrity. I know it's a bad idea, but sometimes applications join on fields that need to be anonymized. I hate that I've seen this, but I've seen folks using email address in multiple tables, and then expecting to join on those, or find related orders. 4: Anonymizing slows down the export. You'll need to run some kind of business logic to determine which columns need to be anonymized, and then generate the new data in its place. I used to say that if you could solve this problem, you'd be rich - but then you've got some competition. Delphix does data masking for DB2, Oracle, SQL Server, etc, but I don't think they've got Postgres or Heroku covered - yet. 

I've read in a forum that if your database capacity is more than 1G you should buy its license. Is this true? How much will it cost? 

As option separates table files instead of putting all data and indexes of DBs into one ibdata file, is using this option improve speed of alter table? I have a table of 40M rows and when I alter a specific table it takes about 5 to 6 hours. Does this solution help? Is there other ways around to improve alter table speed on heavy tables? 

How filtering fields affect performance? Is the performance related to the size of data that is transmitted over network? or the size of data that will be hold in memory? How exactly this performance is improved? What is this performance that is been mentioned in documentation? I have slow MongoDB queries. Is returning a subset affect my slow query (I have compound index on the field)? 

When operations are slow, and a user that has many records take so long it has domino effect on all the other operations after a few seconds. When I explain the query on the large collection I can see the result that it has used an index: 

You can also right-click on the database in SSMS and click Properties, then look at the files from there. 

Here, you've already got a single field that satisfies all of those requests: CLIENT_ID, which you defined as an identity field. You could just start by using that field alone - adding SERVER_ID doesn't really buy you anything. However, I'm a little suspicious of the table design because SERVER_ID is only a CHAR(1). That gives you a limited number of servers to work with per CLIENT_ID. I don't know what you're trying to model, but generally an ID wouldn't be a single character. 

The easiest way to test AG network throughput is to rebuild one of your largest indexes. While the index rebuild is running (and afterwards), measure the latency for each replica using any of these scripts: 

That code can fail. Long term, consider using GETUTCDATE() instead and storing dates with DATETIMEOFFSET - but that's a big code change that you can't really do quickly before changing the date on a server. 

Now start a NEW TAB in SSMS so that you're in a different session - we're going to call this Window #2 - and run: 

This is just some supplemental information from G-Nuget's answer since I'm still unable to post comments. The passwords are for database users. The users have a set of permissions(SELECT, INSERT, EXEC...), Roles(Root,DBA,...) and access to tables/views/databases which can also be configured to be on able to be accessible only from a specific ip address/range. For example, you can restrict root to only be accessible from localhost, meaning that any remote device trying to remotely login to your db server as root will be denied. Also, you should always use multiple users on your database/tables in order to lock down permissions even if you only have one database being accessed by one application. Since you're using phpmyAdmin, its safe to assume that you are creating a website or web application. Database and web apps mean that you allow input and output from forms for most cases, in the case of form inputs, you are susceptible to SQL Injection attacks which could possibly pull out a drop table from what should be just a simple and harmless select or insert query. Of course, proper code sanitation can also prevent this, but its still better safe than sorry. Even if you are only on a dev environment, still practice using multiple users to lock down database and table permissions in order to keep it as a best practice and reduce the effort of moving from a single user database to a multi-user locked down database. Doing so will add some complexity to the program, but its better than having your data stolen, or worse, destroyed. 

Look for warnings about CPU and/or memory nodes being offline. SQL Server Standard Edition only sees the first 4 CPU sockets, and you may have configured the VM as something like 6 dual-core CPUs. It'll end up hitting an issue similar to how Enterprise Edition's 20-core-limits cap the amount of memory you can see. If you want to share sp_Blitz's output here, you can run it like this to output to Markdown, which you can then copy/paste into your question: 

My bigger concern would be the fact that your databases are in simple recovery model. That means you can't do a point-in-time recovery. If something goes wrong, you're restoring all the way back to your last good full backup. Businesses are often uncomfortable with that much data loss. 

This isn't a really widely used tool for SQL Server database administrators. For better answers, I'd contact DBVisualizer's support. 

Checkpoints already happen much more frequently than daily - the default in 2016 is every minute. So to answer the question: 

Ideally, you do this in a different database so you don't bloat the data & log files with your temporary export work. Then, use SSMS's magical wizards to export the data from your staging table. (If you have to do this a lot, though, check out SSIS.) 

It's good to note that I have done the same exact procedure on another server and it went all ok and MongoDB works as expected. Now my config is as below: 

Now I do not have access to primary node and it cannot show my dbs. When I disable SSL, everything works and node gets back to primary state. Why is this happening, while I have done the exact same thing on a node in another datacenter? 

I want to get one post(e.g p_id=8) and all of its comments. how to do that? How to get count(*) from comment table while your getting your post record? 

I want to fill this table with lots of data. Let's say millions of records. How to do this? I need to produce unique emails. Different data for emails. I know I can use but don't know how to produce random data 

Now users in the name of who has been added by uid=60 should not have be shown! The result of this query is empty. I can't figure that out, what I'm doing wrong?