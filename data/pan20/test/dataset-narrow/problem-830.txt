We faced a very similar problem. We eventually concluded that while integrated NTLM logon support in Internet Explorer and Firefox is convenient, there are so many exception cases which result in failure that we changed our approach. The problem with integrated authentication is that it works only when the currently logged on username and password are still correct and properly authorized to access the resource. There are more circumstances where it doesn't work however: 

I've got a standalone Windows Server 2003 running SQL Server 2005 and a Windows Server 2003 Active Directory domain controller. Using maintenance plans/SQL Server Agent, I'm trying to write the database dumps from the standalone box to a share on the DC. I know the usual rules about accessing remote shares (e.g. must use a logon account which has proper rights, etc.). In fact, writing the dumps to another non-DC server in the same domain as the DC works fine. I'm trying to set the SQL Server Agent account's logon credentials to "domain\username" (or username@addomainname). If I specify a username in the form of "domain\username, the error I get (regardless of password) is: "The account name is invalid or does not exist, or the password is invalid for the account name specified". If I specify a username in the form of "user@addomainname", the error I get (regardless of password) is: "The specified domain either does not exist or could not be contacted." I've turned on logon failure auditing on the DC and I see no failures in the log, which suggests to me that the machine isn't even trying to authenticate, but rather failing prior to that. I know that users on non-member servers can authenticate to shares on a DC, because doing an interactive logon (e.g. "net use * \dcname\c$ /user:username@addomainname", or using the other form of the username) works fine. The above example is about SQL Server but applies to any Windows service. Why can't the service log on with the domain account, but an interactive logon (drive mapping) using that same account works? 

Let's say my users have accounts on some mail server mail.example.com. I currently have my mx record set to mail.example.com and all is good. Now let's say I want to have mails initially delivered to an external service (e.g. Postini. Note that this is not a postini-specific question though). In the normal situation where my mx is set directly to my mail server mail.example.com, sending MTAs will of course look up my MX and send to mail.example.com. In my new situation I'd have my mx set to mx.othermailservice.com and emails would be received there. OtherEmailService.com will then relay the emails (while keeping the return-path header the same) to mail.example.com. Do the emails that are received at mail.example.com after be relayed from the other service "look" any different than emails that go directly to it as would be the case where the mx was set to mail.example.com? 

Making backups is really a game of probability. Assuming the data is successfully written to any media (as confirmed by the backup program's "verify backup" function), the weak link becomes the shelf life/survivability of the media. Backup tapes can break and be demagnetized. Hard disks can crash. Optical media (like DVDs and Bluray disks) degrade over time. I view the question of "Is it safe to back up to media X?" less of a yes or no question and more one of your goals and retention requirements. If you're looking at a one-time/one-off/adhoc backup that you plan to use in the short term for recovery, then it's less an issue of reliability and more a question of convenience. Assuming that you're looking at a corporate server backup solution (e.g. ongoing backups, some media rotation schedule and some retention period requirement for each backup), it's still less of an issue of reliability (since you'll assumingly have at least a daily backup) and more one of convenience. So assuming your backup process is rigorous (done according to a schedule and verified for errors) and frequent, I see no issue taking advantage of the larger capacity of Blu-ray disks. Under no circumstances though would I rely on any optical media for long term storage. For long term, tape will be most reliable. To really reduce risk of long term backup storage and avoid restore failures I think it's important to have multiple backups stored in different locations. 

Information Builders released mainframe database application FOCUS in 1975. You can read the whole story on Wikipedia: $URL$ What does the acronym FOCUS stand for? 

Windows Command-line scriptable, so we can automate it...sorry, FileZilla (?) FTPS, as it seems to perform better than SFTP The ability to send KeepAlive commands to the FTPS control port during a transfer No passwords sent on the command line...sorry, curl 

Our IT environment provides 10 shared, Microsoft Windows 7 laptops for an office staff of several hundred people. After checking-out and logging into a laptop with an Active Directory domain account, office staff frequently run Microsoft Outlook 2010. However, the first time office staff do this, Microsoft Outlook 2010 prompts the user to create and configure their local account. This takes just several clicks, as Microsoft Outlook 2010 auto-detects the office staff member's Microsoft Exchange Server 2007 (SP3) account. The problem is: all office staff have to do this on each new laptop they use. Until they do so, some functionality does not work (for example, Microsoft Word 2010 Save & Send fails with error "There was a problem creating the message"). How might our IT department "pre-configure" the shared laptops so office staff can simply log-in and use Microsoft Outlook 2010 functionality without the need to configure a local account? 

IBM Z/OS FTP SERVER PASSWORD RESET MECHANISM IBM z/OS FTP server extends File Transfer Protocol (FTP) command "PASS", to grant FTP users the ability to initiate the password change, via the FTP client. Specifically, enter "oldpass/newpass/newpass", in FTP client field "password" (substitute the actual old and new passwords, for "oldpass" and "newpass", respectively, preserving the forward-slash delimiters). Upon successful login, this triggers the FTP server to subsequently change the FTP user password, on the remote server. Subsequent logins require the FTP user to provide only the new password. Additional constraints exist; please refer to the IBM FTP command "PASS" documentation for full details. AUTOMATING IBM Z/OS PASSWORD RESETS Automation depends on both the FTP server password reset mechanism and an FTP client which lends itself to automation. We currently use Ipswitch WS_FTP Professional Client v12: 

After researching this issue, it appears no versions support FTP server command extension MFMT, as of the date of this answer. 

Attempting to run Altera Quartus II (2) 9.0 on a remote RHEL5 Linux (2.6.18-308.1.1.el5 kernel 64-bit) box over a SSH connection via PuTTY 0.62 and a local Xming 6.9.0.31 X server running on Windows XP/Win7. I have set PuTTY up to forward X11 and successfully remotely run other X programs on this server. If I use Cygwin/X X server, Quartus II 9.0 displays correctly. Xming X server does not report any visible errors on the command line; it just seems to hang after displaying an initial blank, white box. Is this a known issue with Altera Quartus II 9.0 and Xming? Is it possible to know the root cause of the incompatibility? 

Start > Run... > perfmon Select menu View > Customize... Uncheck option "Standard menus (Action and View)" Select button "OK" 

What FTP Service versions of Microsoft Internet Information Services, if any, support the proposed (and now expired?) FTP server command extension MFMT? NOTES The proposed MFMT ("Modify Fact: Modification Time") FTP command extension modifies a file or folder's last modified date and time information. This complements existing FTP command MDTM ("Modification Time"), which "which was only intended to read the modification time and not to set it as some implementations do." 

We wish to solve the business problem of how to schedule automated password updates, on a remote FTP server (note: z/OS) over which we have no administrative control, before the password expires. For example, once each month, update user JDoe's password, both locally and on remote FTP server ftp.abc.com. Our business process necessitates transferring files, using protocol FTPS, to/from a remote FTP server (note: z/OS). A separate organization administers this remote FTP server, providing our team with a user account but no server-level administrative control. FTP server policy automatically expires user account passwords, after a period of time. When this happens, FTP server staff require the person tied to the named account to call their FTP server help desk, to verify their identity. Upon successful verification, FTP server help desk staff reset the password, requiring the user to choose a new password upon the next login. The FTP server allows users to reset their password via the command-line, by setting password to string "oldpw/newpw/newpw"; subsequently, users login with only "newpw". FTP server administrators will not set an FTP user password to never expire. Years ago, a now-retired team member created an in-house app to perform this task, using FTPS functionality provided by ComponentPro "FTP/SSL Client Component for .NET". Per resource constraints, we'd prefer a solution we don't have to maintain in-house. Note: if possible; if in-house represents the way to go, so be it. How can we do this better?