Labels To get the labels for these images, if you don't have them, you will obviously need to create them. There are tools for that, depending on your images and what you want to get from them. Check here. here and here to get you started. 

Do you have a link to the actual source you are referring to? I haven't myself seen it before in the context of association rules and interestingness. 

Lastly, there is a transition: between these state, given the inputs. These are the observables, as far as you are concerned. With HMMs, we are however assuming that there is more than meets the eye; there are some hiddens states (latent states), which are unobservable to use. The model keeps track of these and uses them in conjunction with the observables list above to decide how to act - it has the full mapping function. Below is a schematic diagram of such a model: 

Of course a bar can also mean things such as the mean or expectation value of some values (I have even seen people using it to mean the unobserved or predicted value!), but with the information you provide, the complement seems most likely. Here also from the same Wiki page linked above for those who are unsure, a visual definition of the complement: 

I find the second reason to be particularly interesting and relevant to your setup. This is because you actually only have three layers. You are therefore not allowing must freedom to fine-tune, and the final layer was likely very dependent on its relationship to the preceding layer. What you might expect to see as a result of using a pretrained model, is rather that the final model exhibits better generalisation. This may indeed come at the cost of a lower test accuracy on the hold-out set of the specific dataset you train on. Here are another thoughts, summarised well by the amazing (and free) Stanford CS231n course: 

Should you be worried? In short, no. A validation loss curve like yours can be perfectly fine and deliver reasonable results; however, I would try some of the steps I mentioned above before settling for it. Should you just pick the best performing model? If you mean taking the model at its point with best validation loss (validation accuracy), then I would say to be more careful. On your plot above, this might equate to around epoch 30, but I would personally take a point that has trained a little more, where the curve gets a little less volatile. Again, after having tried some of the steps outlined above. 

Another way that people might include prior information into a model, is to use a Probabilistic modelling, which incorporates ideas from Bayesian statistics. You can do things such as define a prior distribution for your outputs, conditioned on your inputs - essentially allowing you to provide information to the model (such as weight being correlated with obesity, ceteris paribus) - this then nudges the model to go along these lines. If you would like to get into it, there are already some great libraries to make it really easy: 

This seems to be a nice overview of some of the methods and tools, but I haven't read through it all. 

Reading the relevant paragrpah, we can gain a little more insight. It seems to come down to the idea of delayed reward. By only updating the model (i.e. the agent) every once in a while, we are inherently inducing the agent to make moves which are benficial over longer periods of time. Here is the paragraphs from the post: 

There doesn't appear to be an easy way - there is no mention of an API to get the data programmatically. You could try contacting them via Twitter or something. Perhaps they would be happy to give you a dump of the data. Otherwise, you might have to go down the route of writing a scraper e.g. using Selenium and BeautifulSoup. That would allow you to: 

Saying each of the operations is applied randomly, I would say, means your images will be generated sometime with and sometimes without the augmentation steps, independently from one another. If that doesn't convince you, here is the relevant snippet from the source code: 

Additional points Have a look for some overviews and comparisons of the deep learning frameworks in general to get a better understanding of when you might use one over another. Here are some I found useful: 

Stan - which has interfaces to many languages: Python, R, State etc. Edward - probabilistic programming with modern GPU accelration and deep learning integration (Tensorflow and Keras). 

There is also a full tutorial from matplotlib on dealing with images, which is very useful. Do do the above for a folder full of, say 100 images, you need to get all the file names, just like is defined in the example above. To do this, a common way is to use the package in python, which helps you list the contents of a directory: 

No FC layers! Definitions will, however, play a big role in deciding whether or not there are FC layers in the model. 

The default output activation of the Scikit-Learn is 'identity', which actually does nothing to the weights it receives. As was mentioned by @David Masip in his answer, changing the final activation layer would allow this. Doing so in frameworks such as Pytorch, Keras and Tensorflow is fairly straight-forward. Doing it in your code with the means using an object attribute that isn't a standard parameter, namely . Here are the built-in options that I can see in the documentation: 

I will outline some points about the libraries and point you to some good comparisons that I have read. The GitHub star counts are just another reference point to help compare popularity. While I don't condone simply following the masses, those stars do help you see what a lot of other people think about the frameworks. Tensorflow 

How many passengers can we expect next Saturday? Which are peak travel times in a certain region? which factors should we improve to increase customer feedback scores? 

EDIT: Here is the Scikit-Learn class which can do basic variance thresholding for you - there is also a short tutorial. They also present some ways to do recursive feature selection, similar in nature to my final approach outlined above. 

The code for the rotation step is a little more involved, but contained within the same class that I liked above. 

Now assuming this shows there is a finite number (more than likely the case), You could look into using a sparse representation of each sample (that means for each day). For example, if all the words in the entire dataset were: (duplicates removed), then for one single sample with column 4 holding e.g. , your sparse representation for this sample would be: , because the sample has two 'hi', one 'hello', zero 'wasup' and so on. This sparse representation would then be your single input for column 4 for the timestamp (that single sample). It might be worth looking into something like the DictVectorizer and CountVectorizer from Scikit-Learn. Dealing with number columns As I mentioned right at the beginning, you could pad these to a chosen length, perhaps matching the length of the string based representation above (or not!), depending on your final model. You can then pad the inputs with a value that makes sense for your model (see the kind of options I mentioned at the beginning of my answer). This should land you with, once again, a single vector for the given day, containing the information in the numerical column. 

In the bigger scheme of things (beyond a single Inception module), we have first to distinguish between the train and test time architectures. At train time there are auxilliary branches, which do indeed have a few fully connected layers. These are used to force intermediate layers (or inception modules) to be more aggressive in their quest for a final answer, or in the words of the authors, to be more discriminate. From the paper (page 6 [Szegedy et al., 2014]): 

I can save compute, as we could formulate the graph to then only perform backprop after our buffer is full. For buffer size not close to 1, this would potentially halve the training time (making the gross simplication of an assumption that learning occurs at the same rate) It may be equivalent to using batches, in that it could help smooth the learning process. If you have a batch size of 1 sample, the error that is measures and used to update weights may be quite erratic. Using a batch has a nice averaging/smoothin effect. Saving up errors in a buffer, one could argue, may have a similar effect. 

Pick a value that seems ok for you and your dataset by eye-balling it then simply cut variables below the theshold from the dataset Create a function, which given a threshold, tells you how many variables would be removed, if you used that threshold. Then create a simple plot and see if there is a certain level that seems appealing (this depends on your target model once data is ready). Use some smarter functions that d a little more for you, e.g. the NearZeroVar function in the Caret package in R 

From what I understand, you would like to predict the next 10 days, meaning you need . In order to try this with your example, I think you will need to make two changes. Change #1 The shape of your train and test sets will need to match the new horizon. Each sample of your model input (the and can stay the same as before. However, each sample in your test set will have to contain the next values of the label, not just a single value. Here is a rough example of how you might do this: 

Example Set A has a superset B, and that superset has its own superset C. In this case, A must also be contained within C, but the set B lies within that realm and so is the immedaite superset of A. Perhaps it is best visualised with a Venn diagram, where B is the immediate superset of A and equally the immediate subset of C: 

Specific to your task, I would also suggest trying to perhaps unfreeze another layer, to increase the scope of your fine-tuning. This will give the Resnet-18 a little more freedom to learn, based on your data. Regarding your last question: 

Another way that I could suggest is to play with the architecture of your model and the idea of auxilliary models. Have a look at my recent answer on a question talking about the Inception model from Szegedy et al.. The idea is that you have branches coming off the model at train time only, which also make predictions and produce error to be backpropagated through the preceding weights. You could make a side model that predicts the obesity, based on e.g. the input weights and perhaps some related features extracted from the first layer of your neural network. This would make the idea or importance of this relationship more prominent during training and the weights would subsequently be tuned to take that into account. At test time, you simply ignore these auxilliary branches. 

I see in the code for the MLPRegressor, that the final activation comes from a general initialisation function in the parent class: , and the logic for what you want is shown around Line 271. 

Whether or not padding is ppropriate really depends on the entire structure of your dataset, how relevant the different variables/columns are and also the type of model you want to run at the end. Padding would be used, whereby you would have to fix the length of each sample (either to the length of the longest sample, or to a fixed length - longer samples would be trimmed or filtered somehow to fit into that length). Variables that are strings can be padded with empty strings, variables with number can be padded with zeros. There are however many other ways to pad, e.g. using the average of a numerical variable, or even model-based padding, padding with values that "make most sense" for filling gaps in that specific sample. Getting deep into it like that might more generally be called imputation, instead of padding - it is common in time series data, where gaps aren't always at one end of a sample. Below I outline one approach to padding or standardizing the length of each sample. It it not specifically padding. As you did not mention a programming language, I will give and code snippet in Python, but the same is easily achievable in other languages such as R and Julia. The Approach Based on the two examples you provide, it seems each example would be a calendar day, on which there are a variable number of observations. There are also columns that are strings, and others are strings of numbers (e.g. column 5 in sample 2). In time-series analysis in general, it is desirable to have a continuous frequency of data. That means have one day give one input. So my approach would be to make your data into a form that resembles a single input for each of the variables (i.e. columns) of each sample. This is general approach, and you will have to try things out or do more research on how this would look in reality for your specific data at large. Timestamps I would use these as a kind of index, like the index in a Pandas DataFrame. One row = one timestamp. Multiple variables are then different columns. Dealing with strings I will assume that your dataset has a finite number of possible strings in each column. For example, that column 4 (holding names), will always hold a name from a given set. One could perform to see which values there are (removing duplicates). Or even: