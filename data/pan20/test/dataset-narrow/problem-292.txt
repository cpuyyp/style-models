BTW there is a heads you need to be aware of: GROUP_CONCAT's max length is 1024 by default. You should extend it to 10M. 

Every subfolder under is considered fair game to be registered as a database. You could just mysqldump that entire database and then drop the database. SUGGESTION If you have to leave the database present but inaccessible, here is something radical you can try: EXAMPLE Suppose you have a database called and you want to disable access to it. Go into the Linux OS and do the following: 

If none of the passwords are of length 16, this may explain PHP's reluctance to login. Sad to say, but the alternative would be to setup 16-character passwords, but you cannot reverse-engineer 41-character passwords. You would have to manually setup the 16-character passwords using the original plain-text values. For example, if root@localhost had 'helloworld' as the password, it would have convert it using the OLD_PASSWORD function. Here is a comparison: 

They still don't match. That's because your second query is only checking one side. Let's check your first query against every value with only (A,B) and not (B,A): 

These numbers are going by the actual working set in the InnoDB Buffer Pool I hope this little explanation helped you !!! 

Second, I would use periodic mysqldumps. Since the database is very small, you could run the following script every 24 hours: 

There are occasions when certain SELECTs on a Master may yield different results on a Slave. For example, let's say you ran this query on a a MASTER 

In your comments, you stated you have the following rights upon login (from the output of the function): 

COURSE OF ACTION You need to adjust the MATCH clause to query three string tokens. You do that by putting double quotes around and : 

Unfortunately, you cannot force the ordering of columns in a mysqldump. You can, however, use the table . You will need the following: 

Disabling it may or may not give you a more stable count in terms of setting up EXPLAIN plans. It may affect performance of in either a good way, bad way, or not at all. Give it a Try and See !!! 

This will hold the slave 24 hours behind for 23 hours 59 minutes Try running this in a crontab at midnight 

In case you expect bad side effects, backup the entire /var/lib/mysql and put it somewhere in case you want to copy ibdata1, ib_logfile0, and ib_logfile1 and retry normal recovery. If mysql is fully up in one of the modes 

Your query is looking for the token = 'Baha'. You need a wildcard search for 'Baha*'. Please try the following 

Did you see that? A user with privilege can create a table in a test database and fill it with data. This a clear and present danger. This is why I strongly recommend deleteing those test entries out of mysql.db to deter anonymous users from reaching test databases or accessing newly created test databases (via making a subfolder under the default ). As a reminder, this is how you do it: 

PROPOSED QUERY EXPLAINED In the subquery AA, I compute the number of seconds elapsed using UNIX_TIMESTAMP() by subtracting FROM . If the patient is still in the bed (as indicated by discharged being ), I assign the current time NOW(). Then, I do the subtract. This will give you an up-to-the-minute duration for any patient still in the ward. Then, I aggregate the sum of the seconds by . Finally, I take the seconds for each patient and use SEC_TO_TIME() to display hours, minutes, and seconds of the patient stay. GIVE IT A TRY !!! 

How about a count on foobar from scratch ??? First, insert any new data into foobar Then, do a fresh count on foobar into the temp table: 

Convert the first value using user variables to load the True/False values. Then, compare it to the value 'True' PROPOSED SOLUTION 

Since you have already converted to MyISAM and indexed, I would recommend making snapshots of that table based on a timestamp range. What I mean by snapshot is a temp table that contains just the events you wish to mark. Suppose you want to log entries from the last 10 minutes. This Dynamic SQL should do it for you : 

As for the runtime settings and actual defaults, it's a little different STEP 01) Install mysql on a DevServer with no /etc/my.cnf STEP 02) STEP 03) Run STEP 04) Run the Diff 

Please make sure you create indexes or any appropriate tuning that will support each case. Give it a Try !!! 

That will erase all binary logs and start with the first one (like ) BINARY LOGGING IS NOT ENABLED Simply go the the OS and run the delete command ( for Linux, for Windows) PURGE BINARY LOGS If you want to keep binary logs from the last 48 hours, you run 

You can then let this file grow tremendously and you will have to purge the table every so often. Here is how to purge the general_log table and keep the last 3 days: 

Please send these queries to your IT people and have them email the results back Please keep in mind that mysqldumps do not contain indexes. A mysqldump is simply a logical representation of the data plus the commands and directive to crate the table, load the table, and make indexes. What get generated physically upon restore can be known by these queries before launching the mysqldump. 

You need to stop replication, make the Slave have the same specs as the Master, then start replication. Make sure the Slave has no incoming connections. Otherwise, that will make the SQL thread on the Slave compete with incoming connections that are running queries against the same table you are running . If you cannot reroute the incoming connections, you will have to rerun the in chunks (perhaps 5000 rows at a time) on the Slave locally. As a last resort, rebuild the Slave (after scaling up the Slave's hardware and configs). 

Now that you have scripts to dump databases or individual tables, you can load that data at your discretion. If you need to get SQL executed from the binary logs on the master, you can use and give it the position ot datetime and output the SQL to other text files. You just have to perform due diligence to find the amount of data you need from whatever timestamps the bnary logs have. Just remember that every binary log's timestamp in the OS represents that last time it was written. 

You can change the grep option in the header of the for loop to locate a specific user or specific string in the query. If you have MySQL 5.1 where the processlist is in the INFORMATION_SCHEMA, you can do this to generate the KILL QUERY commands in bulk from within the mysql client: 

Using an API for updating single variable and doing a compulsory restart of the RDS instance to implement the change? That's quite a painful process to tweek any one option. If you want to scale up MySQL, please use EC2. Then, you can tweek to your liking like you have always done and have been used to. 

The differences TRANSACTION 1 iphone_device_time = '2011-06-06 05:24:42', last_checkin = '2011-06-06 05:35:07' TRANSACTION 2 iphone_device_time = '2011-06-06 05:35:09', last_checkin = '2011-06-06 05:24:42' Please notice that the column values are flipped. Normally, a deadlock occurs when two different transactions are accessing two locks from two tables with TX1 (Transaction 1) getting row A and then row B while TX2 is getting row B and then row A. In this case, it is TX1 and TX2 are accessing the same row but changing two different columns (iphone_device_time,last_checkin). The values do not make any sense. At 5:24:42, your last checkin was 5:35:07. Ten minutes and 27 seconds later (5:35:07 - 05:24:42), the column values are reversed. The big question is: Why is TX1 held up for almost 11 min ??? This is not really an answer. This is just bandwidth and throughout from me. I hope these observations help. UPDATE 2011-06-06 09:57 Please check out this link concerning innodb_locks_unsafe_for_binlog : The reason I suggest reading this is something else I saw in your INNODB STATUS display. The phrase lock_mode X (exclusive lock) and lock_mode S (shared lock) indicates both locks being imposed (or attempting to impose) on the same row . There may be some internal serialization going on doing next row locking. The default is OFF. After reading this, you may need to consider enabling it. UPDATE 2011-06-06 10:03 Another reason to examine this line of thought is the fact that all the transactions are traversing the PRIMARY key. Since the PRIMARY is a clustered index in InnoDB, the PRIMARY key and the row itself are together. Thus, traversing a row and and the PRIMARY KEY are one and the same. Therefore, any index lock on the PRIMARY KEY is a row level lock as well. UPDATE 2011-06-06 19:21 Check what auocommit value you have. If autocommit is off, I can see two(2) possible problems