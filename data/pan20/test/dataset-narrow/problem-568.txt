2.2. function_traits.h These are simple template class specializations that provide the required function traits for the implementation of . I realize that specializations for and functions are required. 

Imagine a first call to in thread A makes it inside the statement and gets stalled just before executing . Now another thread B calls and successfully gets past ; the writer now has a value different than reader because of the compare/exchange operation. However, the store of an element has not happened yet since thread A is still stalled. What is thread B going to pop? It's going to access whatever garbage memory is at that location. Issue #2: Malfunctioning empty function The load from and the load from followed by a comparison is not thread safe. 

What this means is that when the comparison fails, E is updated to the current value of X. If the comparison succeeds, X is updated with the value of N. Therefore, you don't need to update X again! That is the whole point of a compare/exchange operation; you're basically saying: "I was the last to modify X." Issue #1: Unsynchronized access to writer In your code, you perform a store to the atomic variable with a copy of a value you last obtained from the update performed in the compare/exchange operation, but this is not atomic. 

After a long try I've been able to create a script in vba which can successfully handle webpages with lazy-load. It can reach the bottom of a slow loading webpage if the hardcoded number of the loop is set accurately. I tried with few such pages and found it working flawlessly. The one I'm pasting below is created using site. It can parse the title of different news after going down to a certain level of that page according to the loop I've defined. Now, what I wanna expect to have is do the same thing without using hardcoded delay what I've already used in my script. Thanks in advance for any guidance to the improvement. Here is what I've written: 

I had a desire to make a recursive web crawler in vba. As I don't have much knowledge on vba programming, so it took me a while to understand how the pattern might be. Finally, I've created one. The crawler I've created is doing just awesome. It starts from the first page of a torrent site then tracking the site's next page link it moves on while extracting names until all links are exhausted. Any input on this to make it more robust will be a great help. Thanks in advance. Here is what I've written: 

3. Sample usage Here's some sample usage to show how a user can get the return values. Basically, the user would send in their function/functor to the dispatcher and would take care of the rest; the dispatcher would return the from its submit-a-function function. 3.1. Example 1 This example is pretty much just a test of the template deduction rules and shows how you can use a collection to erase types. The main feature is that you can have a collection of functions that all have different signatures and return types. 

1. Description 1.1. Functionality The objective of these types is to provide type erasure for any function while maintaining the ability to provide a return value through a combination of and its associated . 1.2. Motivation This is useful for any sort of dispatch manager, a thread pool that you can submit tasks to being the actual target; where a task is any function with any parameters and return type. 2. Implementation 2.1. deferred_invoker.h This is the main functionality. Type erasure is provided through the base class, which simply has a virtual member function that is called when the function has to be invoked. The specializations will then know how to deal with the invocation and whether they have to save a result. They also take care of ensuring the correct initialization arguments required for the call are initialized and saved in a tuple. This applies for template parameter which is any function. A simple tag dispatch system takes care of return types. I've omitted a third implementation of for lambda and functor types to focus the review. It's basically the same style as the other two (possible design issue?), but it takes a copy of the lambda/functor and the arguments, instead of a pointer to the function. 

I've written some code for the purpose of scraping names and urls from several links found in the left sided bar in a webpage and populate the data in several sheets [also giving each sheet a new name taking a customized portion from url] in a workbook so that things do not get messy and the data can be located separately. I tried to do the whole thing accurately. Here is what I did: 

I've written a script in VBA which is able to scrape images from a webpage and save it to a customized folder successfully. Firstly, it scrapes the image link then downloads the image and rename it according to it's identity. It takes 2/3 seconds to accomplish the task. I tried to do the whole thing specklessly. Here is the script I tried with: 

Specialization for special types You can also provide a way to read whole lines through template specialization or through a different function, since it would be useful for strings (reading a phrase, for example). More functionality You can provide a way to obtain values based on a predicate. So that it can be a lot easier for people to get valid values. For example, I want to get easy input of an integer that is between 0 and 10 from an user. Example 

I've actually implemented the same functionality in the past, so here are my comments. Wrong behaviour? It depends on what you can consider wrong. Consider running your own example and inputting . The 2nd will be left in the stream buffer and will be automatically assigned to your 2nd variable. This clearly causes weird behaviour, but it is the same behaviour that occurs when normally using . Suggestions The following are what I consider to be useful features for such an utility function. Better interface In order to provide a nicer interface, you could instead read a single value from your stream and then discard anything else that's been left in the stream buffer; calls to will always return one single value and successive calls won't be forced to take what's left in the stream buffer. Example 

I have written some code in python in combination with selenium to parse all the names from facebook friend list. It was hard to manage the pop up notification and the process of scrolling to the end of that page. However, my scraper can do that successfully. I tried to do the whole thing very carefully. There are always rooms for improvement, though. here is the working code: 

I've written a script in python using requests module in combination with selenium along with regex to parse email address (if any exists) from any website. I tried to create it in such a way so that it can traverse javascript enabled sites as well. My crawler is supposed to track any website link (given in it's list storage) then find or etc keywords from that page and parsing the matching link it will go to the target page and using regular expression it will finally parse the email address from that page. It scrapes the email address along with the link address where it parses the email from. I tried with several links and most of the cases it succeeds. I know it's very hard to create a full-fledged one but I tried and it is not despairing at all. Any suggestion to improve this crawler will be vastly appreciated. Here is what I have written: 

To indicate that the first three files have been processed. You would then only have to check the first character of every line you read, instead of the whole file name, to know which files have been processed. This is the technique I would personally use, as it doesn't require an extra file and it keeps the original file mostly intact while providing a much faster comparison. 

Basic algorithm Since you have very large files, you should consider streaming the file one line at a time instead of loading them all once; your program would use a lot less memory. You would also not have to copy your , which is very large! In order to know which files you've already processed, you could create a new file that holds a list of all the files you've processed and write to it as you stream from the original file. This is of course, assuming you want to keep the original file intact. I will continue under this assumption as the other case (simply delete from the list as items are processed) is simpler. Check the Notes section for a better idea that you can implement with inspiration from the following section. Implementation The following is a sample implementation for the algorithm I described in the previous section. For brevity's sake, I did not include the items mentioned under the Form section, nor did I include exception checking; I will leave those things for you to figure out. It is also most likely not as optimal as it should be, but it's merely meant to show you streaming. Sample 

I've written some code in vba to scrape names and phone numbers from a webpage that has spread across some pages I don't wish to know of. The main interesting thing with this scraper is that It only needs to know the first page number then it traverse across all the pages and fetch the information I've mentioned above. I tried to make it error-free. Here is what I did: 

I have written a crawler in python with the combination of class and function. Few days back I saw a scraper in a tutorial more or less similar to what I did here. I found it hard the necessity of using class here. However, I decided to create one. My scraper is able to traverse all the next pages and print the collected results errorlesly. If there is any suggestion or input to give this scraper a better look, I'm ready to comply with that. Thanks in advance. Here is what I've written: 

Welcome to Code Review. This queue implementation is not truly concurrent-ready. Arbitrary initial capacity 

Why have you decided that the default size should be 100? Different people have different needs; having default values like this isn't a good idea because there is no true advantage to having a default size of 100. While this is subjective, I suggest you remove that default size and just have users be required to specify the size that they want. Compare/exchange confusion There seems to be a misunderstanding in regards to the use of compare/exchange. The compare/exchange operations work as follows: 

Notes A faster way to check if a file has been processed would be to add a character indicating that it has been processed, for example: 

You can expand on this by providing functions that use the "Error hiding pattern". That is that they return a to indicate whether the input operation succeeded or not, while the result is stored in a reference parameter. Other isn't the only object that can be streamed from. For example, you can also stream from files. You should provide a way for users of your function to specify what they want to stream from (this can be as simple as having a parameter.