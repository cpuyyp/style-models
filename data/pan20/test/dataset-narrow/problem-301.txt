It looks like a rogue client access point was added or someone manually moved/changed something. Your two instances are as such: 

It does say that there will be increasingly available but does not make any commitment toward a time or viability at release. 

That's what I would expect. This is because the hotfix adds support for the /L option (large attribute descriptor) for the NTFS filesystem. Here's the awesome part, you can only enable it by formatting the volume with this option. So, of course it didn't help as the volume isn't formatted with this option. You could create a new volume, format with /L, copy the files over or you could attempt to use 3rd party applications to try and compact the files (which should compact the metadata as well) and see if that works. Additionally, a larger NTFS allocation unit size will result (for SQL Server) much less NTFS metadata fragmentation. 

If you're having this many issues with Availability Groups I would not foray into Distributed Availability Groups on your own. It may or may not fit your use cases - but if I were you, I'd bring in someone who helps architect these types of solutions and have your use cases ready. Otherwise, you're going to make another post like this. 

From TechNet sys.dm_db_missing_index_details emphasis mine. Hopefully the above examples have provided some clarity around when and why you would have differences between the environments. 

By all accounts this may be a bugged behavior in the sys.stats_columns DMV. This appears to be causing problems when a statistic is updated by way of the parent index. I believe this to be due to the mechanism with which the statistics are being updated in a constraint change. Should you create a statistic manually and then wish to change the columns you must first drop and re-create which forces the meta-data to be updated in the DMV in question. In the operation you have demonstrated there appears to be a situation where the metadata is not updated under any circumstances (DBCC *, CHECKPOINT, server restart, statistics update through parent index change, etc) once the change has been made. From my initial testing I can find only one case when the metadata is properly updated which is the drop and re-create scenario. You can take a look at the Connect item on the issue and up-vote as appropriate. There is a work around query posted there but its mechanism is based on matching the index name to the statistic name and utilizing the index meta-data. 

SMSS isn't saying it can't use the database, it's asking for the password to open the master key. What's the fix? You have a few options, but here are the best two: 

The easiest way is to use sp_getapplock but you're forcing serializable concurrency when you do this for exclusive access and it's a terrible idea. 

This happens when attempting to put in a tracer token without an active subscription. This error seems pretty obvious. Error 18752 

Thank you for the code - it helps! The encrypted connection information shouldn't have any bearing on this, though. 

No idea, you didn't tell us the attack vectors you are looking to secure. In the rest of my answers, I'll assume the only attack vector is on disk at rest. 

I'm assuming the keys haven't been rotated per your comment. If this is the case you are most likely able to restore these to a test server and restore your TDE enabled databases to said test server without issue. If this is the case, you could re-use these keys assuming nothing else is relying on the DMK/Cert in your current instance. Since TDE works through the SMK (and not by passwords) you're currently "at risk". Sure, it's running but who knows when or if something will happen. I stated before, take new backups. The most straight forward approach would be to remove TDE from the databases on that instance, remove the cert, and remove the DMK (After backing them up first). Then proceed to create the DMK/Cert/TDE enabled. It's going to be disk and cpu intensive for a bit (depending on the size of database and hardware/disk/etc). Backup the new DMK/Cert with a guarded password. 

Upon installation, does not have a password. You should be able to connect without a password by just doing this: 

This will give you a list of users that currently have the SUPER privilege. If you want to manually revoke that privilege from every user except , login as root and do this: 

Results are roughly the same as yours: About 3x's slower. The, I read your stackoverflow link. Then I got to thinking: Look at the sample we used. We basically pushed MySQL to its limit to join 1M rows with 1M rows. This is the worst-case equi-join scenario. All things considered, the performance is pretty good. I remember my college days where I had to create a two-dimensional array using linked lists to construct a sparse array. If a node did not exist in a particular coordinate, the default value for the array was defined as zero in the app. Then, imagine this. Creating a 1000x1000 sparse array where all 1000000 (1 million) coordinates had a nonzero value represented. Now, you had at least 2.002 million pointers mapping all adjacent nodes. This is in additional to the 1 million 4-byte integers for the data. Fetching a single value out of it required more CPU for navigation than the retrieval of the actual data. Doing a INNER JOIN of 1M rows from part1 to part2 where part2 has absolutely every key requires more resources for navigation (temp table creation, key comparison, value population). Denormalizing can sometimes be demorializing if the right side of a LEFT JOIN isn't very sparse or the left side of the LEFT JOIN is huge. In your case, separating the original into part1 and part2 buys you nothing if they have to be frequently referenced together and in bulk. In other words, separating columns that do not form repeating groups isn't true normalization. The 3 options I gave would do a world of good for part1 and fetching part2 on a row by row basis. Think about the following cases: 

Digging into the mechanics of this wait you have the log blocks being transmitted and hardened but recovery not completed on the remote servers. With this being the case and given that you added additional replicas it stands to reason that your HADR_SYNC_COMMIT may increase due to the increase in bandwidth requirements. In this case Aaron Bertrand is exactly correct in his comments on the question. Source: $URL$ Digging into the second part of your question about how this wait could be related to application slowdowns. This I believe is a causality issue. You are looking at your waits increasing and a recent user complaint and drawing the conclusion potentially incorrectly that the two have a relationship when this may not be the case at all. The fact that you added tempdb files and your application became more responsive to me indicates that you may have had some underlying contention issues that could have been exacerbated by the additional overhead of the implicit snapshot isolation level overhead when a database is in an availability group. This may have had little or nothing to do with your HADR_SYNC_COMMIT waits. If you wanted to test this you could utilize an extended event trace that looks at the hadr_db_commit_mgr_update_harden XEvent on your primary replica and get a baseline. Once you have your baseline you can then add your replicas back in one at a time and see how the trace changes. I would strongly encourage you to use a file that resides on a volume that does not contain any databases and set a rollover and maximum size. Please adjust the duration filter as needed to gather events that match up with your waits so that you can further troubleshoot and correlate this with any other teams that need to be involved. 

You can go to www.drbd.org to find out the latest version number and update the aforemtioned script Give it a Try !!! 

I made this query up based on staring at the output It would have been nice to have a table called name_country perhaps like this: 

When you look inside the text file, you do not see as a command line startup option. The other four variables I showed you can be used on the command line and in . If you have data to load, you can set that before loading: 

Yes, you should be fine. You can on the Master should replicate to the Slave, execute the part on the Slave, do nothing, and be successful without erroring out. I have done this before. As an alternative, you could also run this on the Master: 

When you use , all it does is add a row to with host set to , set to or , and the password column is filled in. All the DB privileges are defaulted to . When you connect, run this: 

Question 5 On INSERT queries on the master, what form of the query is written into the binary log? Is it the 'raw' form of the query, or the one which already has the auto-generated value of the auto-increment key? Answer to Question 5 Whichever form is presented. Here is what I mean: The raw form would usually not include the auto_increment column expressed explicitly. On the other hand, it you import a mysqldump into a DB server with binary logging, the rows being inserted would explicitly be given. Either version of INSERT would be allowed execution in mysqld. In like fashion, either version of INSERT would be recorded AS IS... 

You could utilize a sequence object to facilitate the mechanism of generating the IDs if that is the sole purpose of the CommentParents table. Take a look at the TechNet documentation for the TechNet SQL Server 2014 Sequence Documentation for more detailed information. What this sequence object will do that your current table can't do is allow you to grab a value and assign it to a variable ahead of time without worrying about doing an INSERT/SELECT SCOPE_IDENTITY() process. The advantage here is at great scale the current design you have would break down due to the need to maintain metadata overhead where SQL Server has caching for Sequence objects. 

To answer the question of will you get the same results running an analysis such as Brent's sp_Blitz scripts in a non-production environment the answer will lie in what information you are trying to gather and what that information depends on. Take the two examples below as a means of showing the difference in results (or no difference) based on the type information sought. 1) Suppose I am interested in analyzing the indexes in the database to determine if I have duplicate indexes. To do this I would look at the database metadata which would be part of the database structure itself. In this case the results should be the same regardless of environment because the data that is being queried to draw the conclusion is part of the physical database structure. Brent's sp_IndexBlitz does this as one of the steps in its analysis, among others. 2) Suppose I am interested in analyzing an index to find out if the index is utilized. To do this I would examine the DMV (sys.dm_db_index_usage_stats) to determine if the index in question has any scans, seeks, lookups or updates. Using a combination of this data I could then determine if this index is making inserts run slower or is a benefit to select performance in a way that justifies its overhead. In this case though the data will be different in results between production and the non-production environment unless the exact same workload and configuration is running in both environments. Brent's sp_IndexBlitz also performs this same check and will provide the usage details based on the settings specified. To further clarify on the "why would data be different" which seems to be a sticking point here lets dig in to what DMVs are. A DMV is at high level just an abstraction layer that provides a view of the instrumentation that SQL Server has running. With this being said as mentioned by Tony when SQL Server restarts the data that is within these DMVs is not persisted. For this reason when you perform a backup and restore it is functionally equivalent to a server restart for the purposes of the discussion around DMVs. Once the database has been decoupled from the original host of this instrumentation data the data would be lost unless it was persisted elsewhere. This is mentioned in Microsoft's documentation as shown below: 

With this setup in mind, here is what you need to do: STEP 01) Use Query to Present Columns Ordered Per Database/Table Here is the Query: 

You are overlooking something far more basic MySQL Documentation clear says the following on GET_LOCK() 

I really hope that is a misprint. Why? The first GA release of MySQL 5.1 is 5.1.30. Mediatemple is offering MySQL 5.1.26. Why on God's green earth would someone offer a Pre-GA version of MySQL 5.1? Please, make sure you are using MySQL 5.5.21 (or 5.1.61 if you have to have MySQL 5.1) UPDATE 2012-03-05 22:51 EST You should setup MySQL Replication is addition as a secondary form of redundancy to an outside hosting service (such as Amazon EC2) for a while. That way, any missing rows should at the very least show up in the binary logs on the Master and the relay logs on the Slave. If your data is missing in MediaTemplate but is present and accounted for on the Slave, I would consider switching out of MediaTemplate Grid Services. If it happens more than once, switch out immediately. If the data on the Slave is exactly the same, you can rule out Database Replication at all levels. From here it is time for a deep dive into WordPress configuration. You can also look through the binary logs to see the data you had was once there at one point disappears at another point. 

First, thank you for the information and the reproducing code/situation. I've taken this and filed an internal item, it's been assigned and will be looked at shortly. I'll update this answer with more information as it becomes available. 

The SQL Server log isn't really human readable, though the functions help. I stated before, it's not always that easy as change 100 to 150. Let's say you changed 'Hi' to 'Sean'. This might cause a forwarding record, or a page split. It might be the first record in a table, or a heap, or be changed because of multiple indexes. I'm not trying to put you off, just that academia and the real world and generally two very different things. 

Before you do this, change the synchronization type to be synchronous and let the secondary catch up. Then, when the secondary is synchronized you can failover without issue. Should you failover while the replica is still set to be asynchronous it will not guarantee that the databases in the AGs are all caught up to the same time. Additionally it'll cause you more issues as all database replicas will have their data movement suspended. 

This is happening on the primary (if it truly is blocking), you'll need to find what sessions are blocking it at why (on the primary for the primary). 

ANSWER TO QUESTION #2 You could take the time if you do mysqlbinlog at different times of the day. You could save yourself some off that housekeeping information by simply running or the same time every night. For example, you could script a cronjob to run at 11:59 PM like this 

These numbers assume that every block of data and index for InnoDB has a place in the doublewrite buffer to go to. A more realistic number may be this: 

ANSWER TO QUESTION #1 Depends on the default settings you allow and what setting you use to override. The parameter is enabled by default. That enables , , , , , , , and . If you have a database with all InnoDB tables or a mix of InnoDB tables and read-only MyISAM tables, you can just use . This will create a clean point-in-time snapshot of all tables involved in the mysqldump. Any INSERT/UPDATE/DELETE queries against an InnoDB table will not interfere with the point-in-time consistency of the mysqldump. By the way, --single-transaction is mutually exclusive to --lock-all-tables. Thus, using --single-transaction will disable --lock-all-tables. QUESTION #2 

On the particular hardware I ran this on, your first expression is about 2.67 times faster. Give it a Try !!! 

Have you looked at the SQL Server Migration Assistant tool? This would probably assist you greatly in the migration as it maps source tables to destination tables despite possible naming irregularities. The tool is provided to my knowledge free of charge. $URL$ $URL$ 

Under the circumstances that you have indicated have you looked at VSS backups through a VSS provider that is either 3rd party or Microsoft based? You can perform a COPY_ONLY backup that will not break your production recovery chain and you should end up with a backup that of all of the databases that you can then recover elsewhere to within your reasonable margins. Keep in mind that a VSS backup has some of the same mechanisms and downfalls as database snapshots in that a very active database could cause a disk space issue due to the sparse files used. Take a look at the TechNet resources on the SQL Writer service here and VSS backups of SQL Server here. To do this through Windows Server Backup you will follow the wizard steps for a manual backup ensuring that you select VSS copy backup on the custom configuration settings under VSS Settings. This will allow your Windows Server backup to not interfere with any other backups taken on the server. See Windows Server Backup reference for details.