An entity is thing which exists in the world that is not represented by terrain. Entities generally have a position in the world, collision with the terrain and/or other entities, etc. A sprite is a 2D bitmap that is drawn to the screen somewhere. Entities can have sprites, but they don't have to (in which case, they generally aren't drawn), but sprites cannot have entities. A sprite is purely a visual construct. Sprites do not even have to be associated with entities at all. Particle systems, which are usually just visual fluff (ie: don't have gameplay effects) will generate one or more sprites. In short, you need a separation between what is happening in terms of the game and what the player is watching. The player sees sprites, while the gameplay code deals with entities and collision regions. Sprite-less entities are often used for just their collision-detection properties. That is, you place an entity somewhere in the world and set something to happen when the player or something else touches their collision region. This lets you know when the player has reached a location or something. 

Light, from point-like sources, falls of with the square of the distance. That's physical reality. Linear attenuation is often stated to appear superior. But this is only true when working in a non-linear colorspace. That is, if you don't have proper gamma correction active. The reason is quite simple. If you're writing linear RGB values to a non-linear display without gamma correction, then your linear values will be mangled by the monitor's built-in gamma ramp. This effectively darkens the scene compared to what you actually intended. Assuming a gamma of 2.2, your monitor will effectively raise all of the colors to the power of 2.2 when displaying them. This is linear attenuation: . This is linear attenuation with the monitor's gamma ramp applied: . That's pretty close to a proper inverse-squared relationship. But the actual inverse squared: becomes: . This makes the light attenuation fall off much more sharply than expected. In general, if you're using proper gamma correction (like rendering to an sRGB framebuffer), you shouldn't use linear attenuation. It won't look right. At all. And if you're not using gamma correction... what's wrong with you ;) In any case, if you're trying to mimic reality, you want inverse-squared (and gamma correct). If you're not, then you can do whatever you need to for your scene. 

Occlusion culling? You are drawing tiles, not 3D polygonal meshes. They're axis-aligned. It takes very simple math to know exactly which tiles are on-screen. Plus, even for Android hardware, drawing a few things off-screen will do nothing to your performance. As long as you're using proper texture atlasing (no texture changes between tiles), you could probably just draw the entire 100x75 map every frame with no problems. Batching means bundling stuff into a single draw call. And batching is at its most efficient when the batches are static: they don't change from frame to frame. If you have to rebuild your index list or add new vertices, you're batching wrong. You didn't say what screen size you're rendering to, so I'll assume it's around 1024x768. If, and only if, you profile your application and detect that simply drawing the 100x75 map is too slow, you can break the map up into 32x32 segments. If one of these segments is on the screen, you draw it. Even if it's just one tile. That's the best way to avoid non-static batching. Again, only bother to do this if you actually have a performance deficiency. As in, you ran it on the hardware of choice and it wasn't performing the way you wanted it to. 

Because that doesn't make sense. The only way for to be true would be if . And that is only true of symmetric matrices. 

And what makes you think that it would be "balanced and interesting" in a videogame? For example, NeverWinter Nights used a simplified version of D&D 3.0's ruleset, but it certainly wasn't balanced by most definitions of that term. There were a lot of classes and power combinations that were flat out better than others. What works on the tabletop does not necessarily work in a videogame. Take Charisma, for example. In D&D 3.x-based games, it's already as close to a dump-stat as it gets. It's utility is based primarily around non-combat encounters. Unlike every other stat, it has no inherent utility in battle. Str gives a hit and damage bonus to melee; Dex gives a bonus to AC, to-hit with ranged, and reflex saves; Con gives a bonus to HP at level-up and fortitude saves; Int gives bonuses to skill point increases; and the Wis bonus is added into your will save. Charisma has no inherent utility; everything it does is based on some class feature. In a videogame environment, non-combat encounters have to be substantially simplified. A dialog tree is about the best you're going to be able to do. Charisma and most of the abilities around it will be of relatively limited utility compared to, for example, casting a fireball. There's a reason why most games use different rules from tabletop games. They are meant to be played by human beings, with other human beings, and with a human being as a neutral arbitrator. Those rules do not necessarily work anymore if you take all but one of the human beings out of the equation. You can certainly start with something like Pathfinder, but you should never let it limit how your gameplay can develop and evolve. Let it inspire your design, not define it. Always make sure that you are working towards an RPG system that is fun as a videogame. 

Once upon a time, clearing the color and depth buffers actually took time. Doing a clear meant that the graphics card would have to walk every pixel of the framebuffer and write a value to it. Because of this, game developers found that it would be more efficient to simply assume that every pixel would be rendered over again. They developed many techniques for doing this. The color buffer is the easiest to ignore. Less easy is the depth buffer, because it would be polluted with old data. So what they did was simple. On frame 0, they would render with a (or the D3D equivalent) of (0, 0.5), and they would use a of (or ). This means that the farthest depth value you would ever get in the depth buffer is 0.5. So the largest value in the depth buffer at the end of frame 0 is 0.5 (assuming you wrote to every pixel). On frame 1, they would change the depth range to (1, 0.5). Notice that in this case, the near depth value is larger than the far depth. But they would also change the depth func to (or ), which reverses the meaning of the depth test. Because the largest value in the depth buffer is 0.5, everything you write will have a value larger than this. Since the depth test was reversed, this effectively means that whatever was written on frame 0 is now farther away than anything that could possibly be written on frame 1. At the end of frame 1, the smallest value in the depth buffer is now 0.5. And then they repeat. On any hardware made since around 2003, this is no longer an optimization. Indeed, it is a negative optimization. Clearing the depth buffer actually makes hardware faster. No, really. Basically, what happens is that clearing buffers doesn't actually write anything. They store some bits in the GPU's caches that let the system know what color/depth they have been cleared to. When the system tries to write to a cache line of the framebuffer, it doesn't bother to read what's there, because it already knows that it is a blank field of the clear color/depth value. If you try to blend with what's there or do a depth test, again, no need to read: it knows what value to blend/test with/against. So every first read/modify/write that you do on each cache line after a clear is basically a write. It's free. Plus, having a jagged depth buffer can work against Hyper-Z/Hierarchial-Z/any Z-culling optimizations in hardware. Yes, your scene will work against those eventually as you add detail. But if your depth buffer is jagged from previous renderings, even if those background objects are in the background, it can affect the efficiency of Z-culling techniques. And that's not going to help performance. So you should never do this depth reversal technique in modern games. Note: Jari makes a good point on tile-based rendering architectures (as found in most mobile platforms). Not clearing the depth can make things unpleasant there too. 

is quite simple: your matrices are row-major. So many people use row-major or transposed matrices, that they forget that matrices are not naturally oriented that way. So they see a translation matrix as this: 

If your entity factory function has to create objects for/from the rendering system, the obviously your entity factory function needs to create objects for/from the rendering system. There's no avoiding that; it's a part of what you need to do. A component system is not supposed to eliminate dependencies. It's supposed to minimize them and focus them on the necessary dependencies, not the unimportant ones. Users of your entity class shouldn't need to specifically know about the rendering system, unless they are dealing with the visual representation of an entity. Users of your entity creation functions may need to because, as code responsible for building entities, they need to know about all of the things that entities depend on. Thus, the dependency is focused on where it is needed. Doing this doesn't violate encapsulation; the entity creation process needs to create rendering system objects (as well as possibly sound objects and other such things). It needs to know about these things in order to do its job. To create an entity, the entity factory code must either be given some kind of rendering system object to create the visual representation from, or it must go out and find it. One of these things has to happen. You can create abstraction after abstraction, and it still will need to happen somewhere. If an entity needs a visual representation, then some piece of code somewhere is going to have to know about both entities and visual representations. 

Now, because the W component of a position under non-projective transforms remains 1, you can just think of it as taking away the W. But in terms of rigorous 4D homogenous math, you're doing division. If the homogenous representation of a vector direction always has W equal to 0, then how do you go back to Cartesian space? Speaking purely in terms of mathematics, you don't. A W of zero represents a "position" infinitely far from the plane of projection. Such a position cannot properly be represented in a Cartesian space. Indeed, this "projection at infinity" is one of the reason why homogenous spaces are used. It's the only way to talk about such a concept and do math relating to it. However, this inability to have homogenous vector directions is also why you generally do not put vector directions into homogenous coordinates. Oh, sometimes we put them in a and stick a W = 0 in there, in order to multiply them with a 4x4 matrix. But that's the only reason we do it: to make the math work. It's not really a 4D homogenous direction; it's just a 3D vector that has a W of 0. The W being 0 is there to keep the translation component of the matrix from affecting the directionality of the vector. It doesn't mean the same thing has having a 4D homogenous position. Therefore by convention, you simply chop off the W when dealing with normals. Usually, graphics programmers just use a 3x3 matrix for normal transformations, unless they're not doing the inverse-transpose. Note that you cannot really project vector directions, because projection is a non-linear transformation. Therefore, the transformation from homogenous to Cartesian space is non-linear. And directions can't really be non-linear. So again, there isn't really such a thing as converting a 3D direction into a homogenous direction and back again. 

That's an interesting assumption. Opening it might be faster, depending on the size of the text file. But reading it is another matter altogether. If all your startup code does is open the file and read a couple of entries out, then it would be faster. But if you're reading the entire thing into memory, then a DB is going to be slower than the text file. Not unless you write your own parser, or use a bad parser framework. And of course, once it's in memory, finding the data for monster X will be faster than an SQLite DB. Assuming you use an appropriate data structure. This is the only requirement that would suggest using an SQLite DB, so you probably shouldn't use that. Lastly, remember that SQL databases are designed and optimized for complex searching for stuff. If all you're searching for in your DB is a monster by name or something, you're really using an over-designed tool for the job. 

I'd say that your main problem is that you're giving meshes non-mesh properties, thus leading to a fat interface. A mesh is a set of per-vertex data and the information about what those vertex attributes means. That's all it should be. It may have some information, like where the "center" of it is and a bounding box perhaps, but that's about it. A mesh should have no understand of animations. It should not have a world-space position. Meshes should not have shaders. And so forth. All of that information should live elsewhere. As to the specific division of information, that depends on your needs. I would design it based on layered functionality. Figure out what things need to talk to each other, then build objects that facilitate this communication. For example, a mesh holds vertex data. But that alone isn't enough to render it; you need a material, which contains the shader and associated parameters necessary for rendering. So a renderable object would contain a reference to a material and a mesh. 

In general, if you're thinking solely in terms of 2D graphics, "sprites and textures", rendering performance is not going to be a concern. Not on any hardware you can actually have bought in the last 8 years. You might be drawing, what, a few hundred quads every frame? Maybe a thousand, tops? You could do that on a Voodoo 1 at 60fps no problem, and that was with software T&L. You don't need to concern yourself too much with parts of quads that happen to be off-screen. GPUs aren't stupid; they will not do work that will not have a visible effect. Remember: hardware makers make money based on how fast their cards go. And spending time on the part of a triangle that's off-screen is time that could have gone to stuff that's on-screen. Now, that being said, it's not a bad idea to do some simple visibility checks to see if a quad is actually on screen or not. But the key word is "simple". At no time should you modify a quad so that you're only rendering the visible portion; just let the GPU do its job. 

Is GLUT 3.7 out of date? Absolutely. You should never use it for anything. However, FreeGLUT is perfectly fine to use. It is 100% backwards compatible with GLUT 3.7. FreeGLUT is a good utility to have when you need to slap together a quick and dirty program.