Is there a standard way to deal with this? ------------ EDIT: -------------- On Github, I found this: $URL$ This suggests that if a centroid becomes an "orphan", it should be assigned to the point that is the furthest from its centroid. This seems like a sound method, is there any paper or theory supporting this? 

The text is obfuscated for you but to the computer, this is as incomprehensible as English for instance. When classifying text, you always first vectorize your input, meaning that your text becomes a vector. This vector typically represents the frequency of the tokens in your vocabulary in that particular string (bag-of-words representation). What you can do is represent each document/string as a vector representing the frequencies of characters, character n-grams, or even words. One you have your input, you can deal with it as any classification task. Naive Bayes has shown to be a good lower bound for these types of tasks. 

Clustering is definitely something that can help. As you describe, the issue is that you can see that not all instances "behave" the same way. So if you can cluster them into groups that do behave more similarly, you would probably improve. Any clustering technique would work in theory. Although, techniques like K-means for instance do favour clusters of even-size, and so if your problem isn't necessarily "balanced", I would be cautious. What you could also do to improve your clustering is to look at which features are more discriminative (perhaps look at the information gain) and use a weighted distance to favour these features. Another way I feel could help you is if you used meta learning/stacking. By that I mean that you could use decision trees to split your data in a supervised way (as opposed to clustering), and at the leaves, use a more elaborate regressor than majority voting. 

You would need to do this process for each class you're interested in predicting, and then train and test one classifier per class and evaluate each classifier according to your chosen metrics (usually accuracy, precision, or recall or some variation thereof). As far as choosing features, this requires quite a bit of thinking. Features can be highly dependent on the type of text you're trying to classify, so be sure to explore your dataset and get a sense for how people are writing in each domain. Qualitative investigation isn't enough to decide once and for all what are good features, but it is a good way to get ideas. Also, look into TF-IDF weighting of terms instead of just using their frequency within each instance of your dataset. This will help you pick up on (a) terms that are prevalent within a document (and possibly a target class) and (b) terms that distinguish a given document from other documents. I hope this helps a little. 

Check out LightSide for a GUI introduction to text mining in general, and more specifically for generating features quickly. It was developed (and I believe is still being developed) by researchers at CMU and it's how I got hooked on text mining. There is quite a bit of functionality right out of the box, and you can quickly import CSV data into the application, extract features, and start running experiments. It also makes use of suites of algorithms from several other prominent, well-regarded open source toolkits like Weka and LibLinear so you know you're using something credible under the hood. That being said, both of these last mentioned toolkits are definitely worth checking out for added functionality, even if they have a bit of a steeper learning curve. Hope that helps. 

As far as I understand, it is a simple autoencoder, meaning that all it does is trying to map the input into another space, so no fancy training, just some plain feed-forward and backprop. This is why it's rather fast to train. If you want to use pre-trained embeddings, you can do it that way 

Let's say that I want to create a pseudorandom number generator, and I'd like to make sure that it's truly close to random. For simplicity, let's assume that I want to output a 0 or a 1 randomly. I have heard of the monobit, runs, poker test etc. but are there "machine learning" ways to evaluate a pseudorandom number generator? As in, one could try to predict the number that will be outputted given the first k numbers that were previously outputted, and the performance of that model would give you how well the pseudorandom generator is performing. It might be way over my head, but could a generative adversarial network learn a truly pseudorandom generator that way? 

Yes, it will affect the performance of Naive Bayes. It is called Naive because it assumes an independence between the features, which in practice is rarely the case. However, it's shown to be fairly robust to this and to be able to perform well on real-world problems. So having correlation will go against the Naive assumption. However, correlation is not necessarily a bad or a good thing for the performance of your model. Correlation between features in Naive Bayes simply means that if one feature "says" it's class A, then the other feature(s) will often say the same. Therefore, if your correlated features happen to be good predictors, your model will actually benefit from it, if they happen to be bad predictors, your model will be worse off. 

it would look like the following in the case where you're interested in detecting the politics class against everything else: 

It sounds to me like you have a binary classification problem (classifying good vs. bad documents for some definitions of good and bad) and the words are being used as features or "signals" for what predicts good vs. bad documents. One thing you might try is to measure some type of correlation statistic between unigrams and each class you're interested in. This preserves your requirement of measuring occurrences of words given a target class over groups of documents. So, to be a bit more concrete, you could split your documents into two sets (good and bad), and then tokenize your documents to obtain individual terms. From here you could really choose whichever term weighting scheme you like (TF, TF normalized against the length of the document, TF-IDF) and measure your correlation statistic between all these unigrams and the class of interest. You could then produce a ranking based on the correlation coefficients for each term, and take the top-k terms. Some correlation statistics you might try could be Chi-squared (which would measure "lack of independence" between terms and a class). There's also a nice implementation of Chi-squared test for feature selection in Python's Scikit-Learn machine learning library that may be a place to start for this task. Hopefully, that helps! 

Your features could be binary (0 if reason X is not applied, 1 if it is). You could have one for each reason, and even combination of reasons etc. You might run into some issues if you have many possible reasons (your features will be very sparse). If this is the case, I'd look into dimensionality reduction to make your features denser. 

Here is my understanding of the difference between a Bernoulli and a Multinomial Naive Bayes: Bernoulli explicitly models the presence/absence of a feature, whereas Multinomial doesn't. 

You can do that by using binning. If you want to use K-Means for categorical data, you can use hamming distance instead of Euclidean distance. 

Categorical data can be ordered or not. Let's say that you have 'one', 'two', and 'three' as categorical data. Of course, you could transpose them as 1, 2, and 3. But in most cases, categorical data cannot be ordered nicely. So you can transform into numerical data by using one-hot encoding 

First of all, it seems that your data is discrete, and therefore I would advise using Multinomial Naive Bayes (scikit-learn also provides an implementation). I ran your code and the reason your code breaks is not because of the implementation, but because of the data. Fitting works, but if you run , you will see that the variance of your features equals 0. This will cause a runtime error since Gaussian Naive Bayes models class probabilities as follows: Since sigma equals zero here, your code will crash. If you simply change your code to: 

Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. LDA does exactly the same thing you wanted to do. 

I am building a deep learning model for NLP. I am pretty comfortable with adding word embedding from word2vec or Glove vectors as extra word features but I wanted to add other word features like POS tag of a word, NER tag of word along with embedding as features. How can I do this. Should I give these word features by concatenating their vector with the word vectors. Or is there some other method. Please suggest. 

You can not get enough data related to your task. Even anyhow you manage to get the data it will be either to general or will be some other domain based so relying on the result would be difficult. 

Finding a robust model for what you are looking or trying to build is quite difficult at this point of time and as per I know there are no any such existing algorithms to do this. One approach that you can follow is make a Knowledge base for your primes containing info/word it can be related with. To handle most of the relation you will need a word dictionary which contains all the possible related words. Making such an exhaustive dictionary is not completely possible/feasible. But if you have some domain specific work you can make a dictionary which can handle 80% of you cases. You can use wordnet/word2vec to find the most similar words of your base primes and can extrapolate those words. Apart from these you would need a set of possible relations that the word can have. For this you can use models like Open IE to extract the Subject-Object-Relation and you can associate these relations with your primes. But for this you will require a dataset of your domain from which you can get the possible relations.