I would say hierarchical clustering is usually preferable, as it is both more flexible and has fewer hidden assumptions about the distribution of the underlying data. With k-Means clustering, you need to have a sense ahead-of-time what your desired number of clusters is (this is the 'k' value). Also, k-means will often give unintuitive results if (a) your data is not well-separated into sphere-like clusters, (b) you pick a 'k' not well-suited to the shape of your data, i.e. you pick a value too high or too low, or (c) you have weird initial values for your cluster centroids (one strategy is to run a bunch of k-means algorithms with random starting centroids and take some common clustering result as the final result). In contrast, hierarchical clustering has fewer assumptions about the distribution of your data - the only requirement (which k-means also shares) is that a distance can be calculated each pair of data points. Hierarchical clustering typically 'joins' nearby points into a cluster, and then successively adds nearby points to the nearest group. You end up with a 'dendogram', or a sort of connectivity plot. You can use that plot to decide after the fact how many clusters your data has, by cutting the dendogram at different heights. Of course, if you need to pre-decide how many clusters you want (based on some sort of business need) you can do that too. Hierarchical clustering can be more computationally expensive, but usually produces more intuitive results. 

Welcome! I think you asked in the right place. If you have familiarity with Python, you might look at the numpy and Pandas libraries. Numpy implements fast array and matrix manipulations, and Pandas arranges numpy objects into tables. Along with scipy, they make the basis of the Python numerical computing stack. Without more detail it's difficult to share a code example that'll do what you want, but if you run into any issues feel free to ask. Edit: if your dataset is REALLY big and won't fit in memory, there are ways around that too, just specify. 

Fundamentally there is no difference. Say you have data and you want to build a model of it. As the name suggests, modeling is about finding a model, that is, a simplified representation of your data. In turn, we can view the model as an underlying process that generated your data in the first place, plus some noise. From that point of view, the data you see was generated by the model - and we can say that some of the points you see are less likely to have been generated by your model than others. For example, if you build a linear regression model, points far away from the regression line are less likely to have been generated by the model. That's what people mean when they talk about 'residuals' in normal statistical parlance. It's also called the likelihood of the data. Data points that have low likelihood, according to the model you've created, are anomalies or outliers. From a model-building point of view, they are the same thing. Colloquially, people use the term 'outlier' to mean "something I should remove from the dataset so that it doesn't skew my model I'm building", usually because they have a hunch that there is something wrong with that data and the model they want to build shouldn't need to account for it. An outlier is often considered to being a hinderance to building a model that describes the data overall - simply because the model will ALSO try to explain the outlier, which is not what the practitioner wants. On the other hand, you can use the fact that a model also assigns a likelihood to each data point to your advantage - might build a model that describes a simpler trend in the data, and then actively look for existing or new values that have very low likelihood. These are what people mean when they say 'anomalies'. If your goal is to detect anomalies, especially in new data, this is a great thing. One person's outlier is another person's anomaly! 

Ultimately the proof is in the pudding. That is, do model validation like you would with any other machine learning project. If your model predicts well for hold-out data, you can feel somewhat confident in using it. But like any other ML project - if your test data is ever significantly different than your training data, your model will not perform well. 

I'm not sure what you mean by 'intrinsic'. A higher training accuracy means you have found a model of the training data that doesn't describe the validation data equally well. Because of this, any statistically significant difference in training-validation accuracy is due to overfitting. Figuring out what 'statistically significant' means in this context is not totally trivial, unfortunately, but the general point still stands. 

In general time series are not really different from other machine learning problems - you want your test set to 'look like' your training set, because you want the model you learned on your training set to still be appropriate for your test set. That's the important underlying concept regarding stationarity. Time series have the additional complexity that there may be long term structure in your data that your model may not be sophisticated enough to learn. For example, when using an autoregressive lag of N, we can't learn dependencies over intervals longer than N. Hence, when using simple models like ARIMA, we want data to also be locally stationary. 

As a general rule, convolutional deep networks will perform better than SVM and Random Forest models on data that has hierarchical spatial/temporal structure, so for example sound, images, video. Deep networks are very good at learning this spatial/temporal structure at different levels of granularity. For classic datasets like Iris uses for teaching ML, however, I seriously doubt deep networks would perform better than SVM or Random Forest. 

The main criterion is that you need enough data in your training set to get a good model fit - which is both a function of the data quantity and the model complexity. When you don't have much training data, you want as much data in your training set as possible - in this case, you can do "leave one out" cross-validation. This is like k-fold, except it's N-fold, where N is the number of observations you have. This means you hold one observation out, build a model using everything else, validate using that one sample, and repeat N times. When you have more data with a simple model, you can use a much more loose validation strategy. For example, if you have a million records and are doing linear regression with one predictor, you can get away with using a few thousand samples to build your model, since you'll get really good parameter estimates from thousands of samples (or fewer). Those are the extremes. The suggestions you've seen live somewhere in-between. When you pick high K, you have to build lots of models (this takes time), but each model is built with more data (good for complex models). When you pick low K, you have to build fewer models, but each model is built with less data. Personally, I pick a low value for K when I'm doing model exploration, but a higher value when I'm ready for validation of a good candidate model. Actually, in this case you should have three sets - a training set, a validation set (that you use to find a good set of parameters), and a second validation set that you've held out the ENTIRE time (since, in a way, the other validation sets are helping you 'train' the model parameters). 

Usually to find an optimum you set the derivative of the function equal to 0. In your case that gives $$ \frac{dG_j}{dw_J} = G_j + (H_j+\lambda)w_j = 0 $$ leading to $$ w_j = \frac{-G_j}{H_j+\lambda}$$ 

The threshold you choose depends on the specifics of the problem you are trying to solve. More specifically, it should be based on how you weigh false positives vs. false negatives, i.e. how bad each of these are relative to each other. You mention that you are trying to maximize recall on the positive class, but if that were true you would should just classify everything as a positive class, and get a recall of 1.0. Based on the domain you are working in, you should decide how much a false positive 'costs' vs. how much a false negative 'costs'. Once you decide this, you can find the threshold that minimizes the function total cost = false negative count x FN cost + false positive count x FP cost 

Using either of these techniques will work with xgboost, but as xgboost only accepts numeric inputs, you will have to choose one of these methods as a data pre-processing step. 

At the end of the day, as an applied data scientist you have a bag of tools you can use to solve business problems. It's helpful to know what your tools are capable of, where each is useful, and also what their limits are. If you don't have at least some theory, knowing which method to use in which circumstance will seem like a meaningless list of details, and when things break, you may not know why - is there an error in my code, or am I making a fundamental error in applying an algorithm in a scenario where it has fundamental limitations? You can be pragmatic in applying tools as you've seen them applied before. Theory will help you have an intuition about whether it's possible or reasonable to apply given tools to new situations that you haven't seen. It can help you answer questions like: how will this algorithm scale with the size or dimensionality of data? SVMs in particular suffer from poor scaling with data size, precisely because they rely on kernels. That kind of basic knowledge is probably more useful than being able to write down the kernel trick. 

It will help to define what you mean by information. The usual definition is 'Shannon information', which is equivalent to the amount of uncertainty an observation resolves. If you mean something else, nothing I am about to write will apply. Perhaps what you are looking for is the idea of mutual information. It is a way to measure the shared information between two variables - literally how much your uncertainty about one variable is reduced by knowing the value of another variable. Closely related conditional entropy, which measures the amount of uncertainty we still have about a variable given that we know the value of a second variable. Those references might be worth reading to get a sense of how information between two variables is defined in a technical sense. In your case, your operation actually does not increase the information in your dataset. Your logical operation does not bring in any outside observation to the system, but is merely a rule-baesd transformation. It is therefore subject to the data processing inequality. Edit: Re-reading your question, I think you are asking about the difference between I(X;Y) and I(X';Y), where I is the mutual information function, X is your input data, and Y is your target label. So basically the answer to your question is Info gain = I(X';Y) - I(X;Y). You can look up mutual information approximation methods - there are packages for most languages. 

I'd plot your data and see if the proportion of the label you are trying to predict is similar or different among the labels for your feature in question. If similar, you can probably safely remove the column using the function. If different - then the feature is predictive! Keep it, and convert the column into dummy variables. For example, imagine I was trying to use a diamond's color (a categorical variable) to determine whether it was expensive (a categorical variable I invent below...) 

I think predicting model error () makes a lot of sense and is worth trying. Since it's reasonable to expect that the error is a function of the overall rocket distance, I would keep as an input to the error-prediction model. Regarding outliers...I suspect that's a function of your model type. It's difficult to say without more details of what you did.