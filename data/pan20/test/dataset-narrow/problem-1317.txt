There is no normal approach because engines generally don't let you do that. Generally speaking, engines take one of two approaches: either the user provides none of a shader or all of a shader. Never parts of a shader. Engines that allow the user to provide shaders will establish some form of convention for communication between the application and the shader. They'll say that the position must be named "position" or whatever, and you're expected to provide vertex shaders with that attribute name. The various transformation matrices will similarly have specific uniform names and types, and the user providing the shader must adhere to those names and types. Some engines allow you to create shaders from raw components via some system. But even there, it's the system that's creating those shaders. If that system has some mechanism for inserting user-provided fragments of code, then it will be specific to that system, and it likely won't be in GLSL anymore. And even then, those fragments will have to adhere to a specific convention based on the needs of that system. In any case, if you're truly writing an engine for yourself or a single team that you're supporting, you really shouldn't bother with this. Just use shaders directly. 

It's the same offset used in the non-perspective version of the tutorial. This is the orthographic shader: 

This strongly suggests that you're not representing your tiles correctly, as this would mean that each tile is ~80 bytes in size. What you need to understand is that there needs to be a separation between the gameplay concept of a tile, and the visual tile that the user sees. These two concepts are not the same thing. Take Terraria for example. The smallest Terraria world takes up 4200x1200 tiles, which is 5 million tiles. Now, how much memory does it take to represent that world? Well, each tile has a foreground layer, a background layer (the background walls), a "wire layer" where wires go, and a "furniture layer" where furniture items go. How much memory does each tile take up? Again, we're just talking conceptually, not visually. A foreground tile could easily be stored in an unsigned short. There aren't more than 65536 foreground tile types, so there's no point in using more memory than this. The background tiles could easily be in an unsigned byte, as there are fewer than 256 different types of background tiles. The wire layer is purely binary: either a tile has a wire in it or it does not. So that's one bit per tile. And the furniture layer could again be an unsigned byte, depending on how many possible different pieces of furniture there are. Total memory size per tile: 2 bytes + 1 byte + 1 bit + 1 byte: 4 bytes + 1 bit. Thus, the total size for a small Terraria map is 20790000 bytes, or ~20MB. (note: these calculations are based on Terraria 1.1. The game has expanded a lot since then, but even modern Terraria could fit within 8 bytes per tile location, or ~40MB. Still quite tolerable). You should never have this representation stored as arrays of C# classes. They should be arrays of integers or something similar. A C# struct would work as well. Now, when it comes time to draw part of a map (note the emphasis), Terraria needs to convert these conceptual tiles into actual tiles. Each tile needs to actually pick a foreground image, background image, an optional furniture image, and have a wire image. This is where XNA comes in with its various sprite sheets and such. What you need to do is to convert the visible part of your conceptual map into actual XNA sprite sheet tiles. You should not be trying to convert the entire thing at once. Each tile you store should just be an index saying that "I'm tile type X," where X is an integer. You use that integer index to fetch which sprite you use to display it. And you use XNA's sprite sheets to make this faster than just drawing individual quads. Now the visible region of tiles needs to be broken up into various chunks, so that you're not constantly building sprite sheets whenever the camera moves. So you might have 64x64 chunks of the world as sprite sheets. Whichever 64x64 chunks of the world are visible from the player's current camera position are the chunks you draw. Any other chunks don't even have sprite sheets; if a chunk falls off the screen, you throw that sheet out (note: you don't really delete it; you keep it around and respecify it for a new chunk that may become visible later). 

Um, why don't you just use multisampling like everyone else? Even if you're using deferred rendering, there are ways to use multisampling in tandem with that. Multisampling covers triangle edge aliasing, while anisotropic filtering covers texture aliasing. Between those two, you pretty much have all the antialiasing techniques you need. Unless you're procedurally generating textures, of course. 

The core 3.2 version is not "faster". It's simply a more intuitive API. For example, core 3.2 it doesn't use the tortured and syntax. It also requires you to specify the input and output primitive types directly in the shader, rather than forcing you to use pre-link program parameters. Also, you can use interface blocks to group inputs and outputs together, which makes talking about them much easier. The only downside really is that you can't use it with removed primitive types (, etc). 

The most vital element when creating a competitive game is this: player stratification. Good players must consistently beat weaker players. Good players must have the tools to beat weaker players. The system must have enough depth that good players are able to do things that weaker players can't, and those things must lead to victory. The use of these tools becomes what decides who is good and who is not. Exactly how you bring this about is up to you. However, consider that competitive gamers like the feel of owning the game and the metagame. They like finding and exploiting glitches; they get off on pushing a game so far that it breaks and becomes a different game. In part because that enhances player stratification. As a fellow "Scrub" (urge to rant rising...), the hardest thing you will have to deal with is the simple fact that competitive games are not made for you. What Sirlin derogatorily calls a "Scrub" is simply someone who does not enjoy a competitive play environment. We see the design of what the game ought to feel like, and any deviation from that is inherently antithetical to that designed purpose. Because of that, you will constantly want to assert the purity of your original design over the organic growth of your game in development. Resist this urge. Constantly re-evaluate your design as you see the game coming into shape. Constantly remind yourself, anytime you see anything "wrong", that you are making the game for people who are not you. At all times, you should strive to remember who the game is for. You should make decisions about what goes into the game and how it changes based on how best to serve a competitive gamer. So if a bug comes along that your "Scrub" senses tell you is unfair, evaluate it based on how it actually affects competitive play. And if it helps create useful depth of play, then you should keep it (or at least promote it to a full-fledged feature and work with the good parts). 

What a position means is far from standardized in the industry. In general, game producers in big publishers in the US have little to no impact on the actual product. They're facilitators, keeping different departments abreast of things and making sure that all the pieces come together. And again, this is in general; this will vary between publishers and developers. And Japan probably has a different structure than the US in terms of the role of the producer. One constant however is the rule of Executive Producer. Just as with movies, EPs often have little actual power or role in the production. Sometimes, they're purely ceremonial, used as a way to credit someone for part of the work. Some EPs do have some authority, but this is primarily through their role as facilitating communication between the upper level executives and the people actually making the work. When people speak of "Executive Meddling", the EP is often the go-between or facilitator for this. Thus they can gain de-facto power by simply lying or misrepresenting how things are going in production to the people with actual power. Some EPs have veto power, some don't. Even so, the EP's power is usually via the commandment: "Do this!" Exactly how it gets done tends to be up to the developers. Though again, there are exceptions. Basically, you can say nothing about how much influence over the production of those games that Sakaguchi had. At best, you can vaguely say that he probably had more direct, intimate control when he was Director, some control as Producer, and probably less moment-to-moment control over things as Executive Producer. Anything more than that is sheer speculation. 

LWJGL prefaces all core functions and enumerators with the OpenGL version that those functions were introduced in. So , a function introduced in OpenGL version 1.5, is called . That's just how it works. It's not really importing "a bunch of different OpenGL versions"; it's just using the version those functions/enumerators was introduced in as a "namespace" scope. That way, if you're writing to a specific OpenGL version, it's easier to prevent yourself from using functions that aren't available in that version. Of course, OpenGL core extensions kinda confound this notion.