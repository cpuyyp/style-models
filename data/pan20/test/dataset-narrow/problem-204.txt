The optimiser takes the submitted SQL and translates it into a set of actions called the query plan. In doing this it is free to arrange those actions in any sequence which is logically equivalent to the SQL. This may result in an object mentioned in the SQL being accessed once, many times, or not at all. This applies to objects in the CTE. So, although it is commonly observed that objects mentioned in the CTE are accessed only once there is no guarantee that this will happen. Like any other SQL statement the data processed through the CTE part is only in scope for the duration of that statement. If the same data is referenced in a subsequent statement it will be accessed again. 

The full keys to your data are more complex that you indicate. Sales tax rates can vary by jurisdiction and goods can be categorised as luxury or exempt. Wage rates can vary by industry and public holiday / normal work day. Exchange rates have be keyed on the source and destination currencies. While your values may all be fractional numbers they have different units, which makes them different things. Tax is "percentage", wages are "dollars per hour", ForEx rates are dimensionless ratios. Your data scope my be constrained to eliminate these considerations, of course. Only you can say. Joe Celko covers this topic in depth here. 

I'm not familiar enough with Access' syntax to give you a cut-and-paste answer. In pseudo-code, though, you can use something of this form: 

If there two different columns, each of which by itself could be used as a primary key for the table, we say there are two candidate keys. One candidate key is chosen as the primary key. The other candidate keys which were not chosen can be referred to as alternate keys. Any key can be a single column or a combination of columns. Keys made from more than one column are called composite keys. A column may be part of more than one key. For example candidate keys may be (col_a, col_b), (col_a, col_c), (col_d); col_a is in more than one candidate key. The first two of these are composite candidate keys, the last is not. 

Fragmentation occurs when the storage engine can't physically place a row in its correct logical position. So you need a table with no internal space, then insert rows in the "middle". The easiest way to fill pages is to ensure only one row will fit 

One difficult bit is that all columns are compared, so if further values are required for subsequent processing this fragment has to be in a sub-select with an additional JOIN. As you may guess this can lead to many table operations and performance difficulties. I add this for information only. My choice would be the NOT EXISTS form. 

If an index can be defined on the search criteria then this will work OK at any scale. BTree indexes have O(log N) performance IIRC so working through 100,000 rows will be 5/3 the time needed for 1,000 rows. However .. most systems can't fulfill that criteria. Often a user can fill as many or few search fields as she chooses. Then things get complicated. Sometimes it may be best to define separate queries, and indexes, for each combination of search values. This quickly gets large. 5! = 120; that's a lot of indexes and queries. Some DBMS have optimisations for generic searches - OPTION (RECOMPILE) in SQL Server, for example, will cause parameter embedding. Probably some combination of often-used specific queries and a generic catch-all will work. As the list gets hugh users will expect ever fuzzier searches. At some stage a full text search on the product name and / or description may be useful. Maybe, eventually, you offload that task entirely. 

My question - how and when does the optimizer collect these statistics? Inside the SQL Server relational engine statistics can be created implicitly or explicitly, they age as writes occur, and can be refreshed explicitly or during table maintenance. Are there similar strategies employed in PolyBase? Given the data may be loaded into Hadoop/ Azure blob without SQL Server seeing it on the way in, and data volumes will be large (most likely) run-time ad hoc statistic creation through sampling or similar seems an unlikely strategy to me. 

I specifically do not call this "user" since participants need not be people. You will have a participant record, too. 

Conversely if the values are not too long (say a few hundred bytes) and accessed in most queries you may see a performance improvement by storing them in-row and avoiding the pointer lookup and secondary IO. This can be controlled through sp_tableoption. 1 Not that you ever would, of course. 

Do not have GameScore or OvertimeScore. Instead have QuarterScore keyed by GameID and Quarter. For normal games there will be four rows per game. For overtime there will be 5, 6, or however many are needed. Total score and a count of quarters can be summarised as a column in Game, if this is useful. 

I'd suggest a key-value DBMS, but I throw this out there for interest's sake. Instead of performing INSERT & DELETE statements, only do UPDATEs. The table structure will be something like 

Where X is big enough to ensure this background aggregation is not reading from the same data page transactions are actively writing to, about two pages I'd guess. 

For this I would create a second monitoring job to look at in msdb. This will work best if your current job typically runs within a fairly well defined time interval. If it doesn't then you cannot schedule the monitoring job until quite late in the day, by when it may be too late to do anything meaningful to correct the situation. Be aware that holds a limited number of rows. If you have other jobs which run very frequently they may push this job's history out of the table before the monitoring job can see it. If this is a problem create a custom logging table in an application database and get your current job to write to it. I haven't yet figured out how to get informed if the monitoring job doesn't run :-) 

An index provides a path to rapidly identify the rows that match a given predicate. They help most when you need to reference a small fraction of a table at a time. Since you need all rows this is unlikely to be helpful. They also enforce a sequence. This may be helpful if your algorithm would benefit from ordering, though you do not mention it. The DBMS copies data from disk into memory before processing the rows. So more memory helps because more data can be held in memory, with corresponding fewer IO waits. Since you need to process 280GB the last 4GB will be in memory at the end. During the next cycle that data will be evicted and fresh data read. Unless you are able to install over 280GB more memory is unlikely to help. Partitioning is more of an archiving thing. Sharding may help, though. That would mean spreading the data across several servers. Four servers would, approximately, quarter the time to process. You'll need a custom process to stitch the answers together, though, which may not be simple. The simple fact is that if you need to process 300M rows you need to read 300M rows. This is likely limited by storage bandwidth. An SSD will obviously help here. However, is there a way that you wouldn't have to process all rows each time? Could you pre-calculate these values, cache intermediate results or only process deltas between queries? Edit Are you reading all rows only to parse some values out of the text column, but you actually do not use the majority of rows in any particular query? LIKE is not going to help eliminate IO since the majority of values will be somewhere in the middle of the text. An index will not help here. The best you can achieve is to push filtering onto the DB server eliminating network time. A full text search may help find labels within the large text column. Perhaps redefining the column as JSON and adding a secondary index would help. But really these are kludgy workarounds for a sub-optimal design. Relational databases are built to process relations i.e. tables. Structure the data properly as a table and you will have the best chance of getting good performance. In this case that means separating the key-value pairs embeded in the text column onto separate rows, one per value. An index on the "key" column (and whatever other columns are in the WHERE clause) will be a great start. Yes, the DB may become a little larger, but the software's designed for this. There are many, many systems around with terabytes of data in billions of rows. It will be OK. 

There is already a mechanism in SQL to flag missing information. It is NULL. It should be considered a flag indicating meta data about the corresponding column rather than a value of that column itself. If NULL is too blunt & you need to distinguish various reasons for data being missing you can add your own flags. These will not have built-in support, of course. You will have to implement them as additional columns within the table and check them explicitly in code. You will have to think through the truth tables for your new flags so out handle them consistently in predicates and logic. If you have to hold customer-specific prices then you will need a table with a customer key in it. Something like: 

If lock de-escalation occurred I would expect complete symmetry in the two trace files - every lock acquired during UPDATE, and the escalation, would have a corresponding release during ROLLBACK. What I observed: 

Use the clause of the statement to capture the identity values created in . Then use that to write to . 

One difference to consider is the amount of IO happening. For Postgres data is normalised into tables and tables are read one page at at time. To complete the SQL it need only read however much disk is required to store all of table . Mongo's unit of IO, however, is a whole document. All the denormalised/ duplicated stuff comes in along with the three fields in which you're interested. Of course there a lot of things assumed here - RAM, compression, disk performance and data models being some. 

There are two major concerns here. First you are using significant server resource (memory, CPU, disk) on a produciton box which, presumably, is busy. This will be delaying other work. In the extreme, your huge query may fully consume some shared resource and cause other work to fail. Second, your query may be locking data others are trying to use. There are ways around this which either compromise the integrity of your results or use further server resources. Mitigations include 

The optimizer is able to use the knowledge that a column is unique when deciding how to retrieve rows predicated on that column. It may produce a different, more efficient, execution plan with the unique constraint in place than without it. Without the unique constraint the optimizer has to interpolate from the statistics object created on that column, which is a perpetual source of execution plan problems. Of course the index created to enforce the unique constraint is a great source of performance improvement, too. 

These companies use a mix of commercial products and new technologies developed in-house. Many are documented. Several are open source. The larger companies maintain separate research organisations to bring along the next generation of products. Many papers are available and make for interesting reading. Broadly, the NoSQL movement was driven by these organisations' needs that could not be met by products available at the time. Cassandra, DynamoDB and LevelDB, amongst others, came from this movement. More recently NewSQL has emerged, giving global scale and ACID guarantees. One example is Google Spanner. 

By aggregating the rows on ID we can count the non-null values. A comparison to the total number of columns in the source table will identify rows containing one or more NULL. 

The PIVOT pattern would work for this. It converts rows' values to columns in a single row, according to their common key. There are a few ways to implement this. Some require only a single table scan. After the PIVOT you would have a table with one row per postcode and a column per value. The remainder of the query would be written as though it referenced a single table. 

Neo4j is a schema-optional graph database. You could choose to enforce a full schema and reap the benefits. MongoDB v3.2 has validation of structure and data type. 

Certainly not one table for each document. Perhaps one for each document type. In general you are correct - reading or writing to larger tables typically takes longer than the same operation on smaller tables. There are techniques to help with this - indexing for example. Modern DBMSs can easily handle tables of several million rows, however, so unless you are pushing these limits it is better for your design to match your requirements than to work around the limitations of the technology. If you commonly several different documents of different types simultaneously then a single table would be beneficial. If each document type is processed independently then separate tables would be better. 

I have not yet found a notification or activation on job completion. The nearest I can think of is putting a tirgger on the job-related systems tables in MSDB. This is very dodgy, however, and not at all recommended, and may not be possible. In the past, when I've needed to merge two job streams that I can't edit, I've used a check-and-wait stored procedure like this: 

What you describe is the default behaviour of SQL Server's query execution and client connection protocols. Rows are delivered to the server's network buffer as they are produced from an executing query plan. The network delivers them to the client, which makes them available to the application. I think the pause you see is caused by how the application consumes the dataset returned. It's been a while, but I think SqlDataReader.Read() allows stream-like behaviour. The risk is that the application falls behind SQL Server and the connection and server resources remain locked up until the application explicitly releases them. 

Having all your data in fewer, common tables is obviously going to make your coding simpler. I would suggest you pursue this option. Partitioning is not that scary. Annualised data like yours is the archetypal usage for partitioning. It will help with your query performance and may help with your data management tasks (load / delete), depending on MonetDB's capabilities. You say some data is in different formats in different years - tinyint v/s character. This will be a problem for you whatever architecture you choose because you will eventually want to combine these in one resultset. Fix this sooner rather than later. Fixing in the data and table definition will be less work overall than fixing it in each query as you write it. In summary 1) Fairly, especially if you have to reconcile type differences 2) "A bit" if your queries are simple to "a lot" of you get it badly wrong 3) not a MonetDB expert but most RDBMS handle data many times the size of RAM without problem. 

I've used a Numbers table. There are any number of posts explaining what this is, why its useful and how to get one efficiently. I don't use any built-in functionality to convert accented characters to the non-accented equivalent. Instead I build a lookup list which you will populate with the conversions you require. You will have to use and define your translations as , of course. Thanks to this post for the row concatenation tip. 

I had to do this some years ago for a SQL Server 2005 DB. It was awful. I've not had to do it on 2008, thankfully, but as far as I know it still works the same way. Unless specified explicitly, the collation is copied from the next higher object when an object is created. So creating a table will copy the database's collation. Collations are not inherited dynamically at runtime i.e. changing the DB's collation will not change any table's collation. So you are on the right track. You have to change the collation at every level from column up. Moreover you will almost certainly want to change the collation for the system databases (master, model, msdb and tempdb) otherwise you may get collation conflicts when a query plan uses tempdb and you will have such plans. Furthermore I remember that the new column collations did not take effect until the data had been written. I ended up performing 

Although the number of teeth is nominally fixed in adults, there are pathologies which cause a higer or lower number. Your first thought would not allow for this. So I would go with the second approach. This has the flexibility to deal with normal adult teeth, milk teeth and abnormal circumstances. I would suggest these tables. 

You say you have a date table. You don't give a name so I shall refer to it as and the column in it as . You need to select from this table and to it. That way all dates in the range will have a row in the output with NULL for rows where you don't have "real" data. You can convert these to zero using . I'll use your first posted query as an example: