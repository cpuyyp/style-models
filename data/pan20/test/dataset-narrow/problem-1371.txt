I am trying to draw a full screen quad without a vertex buffer. SV_VertexID should do the trick, although I get a weird error in my shader on the following line: 

I have a delta in screen coordinates that I want to transform to a 3d delta in world space. The delta is in this case a movement across the (x,y) coordinates in screen space on the near projection plane for a known z in the 3d space. The goal is to move an object in 3d space horizontally and vertically. How would I do this? I guess it has something to do with getting rid of the translation component in the world-view-projection matrix, as the delta is only influenced by the scaling and rotation part of the matrices. I tried the naive approach by setting the translation component in the world-view-projection matrix to 0, but that did not work. The only working solution i found so far is to unproject the (0,0,z) coordinate of the object to world space and then unproject the delta (x,y,z) to world space. The result of these 2 un-projections is then substracted to get the delta in world space. 

As far as I understand for DirectX11 it is recommended to have multiple single pass shaders, so the the method with multiple passes in one shader file is deprecated. Is my understanding correct and where can I find more information about that? How would I go about creating a multi pass shader in DirectX11? The way I understand it, one would have to apply a shader and then copy the shader result to be used as input for the next shader pass. Is this the right way to do it? Any information on this subject is very appreciated. 

I have an orthographic projection and I try to unproject a point from screen space. Following are the view and projection matrices: 

In Android development I found nine patch images very useful to draw images that have a fixed border. When the image is scaled the inner part scales, but the border stays at a fixed size. I was looking for something similar in DirectX. Does anyone have information/pointers to nine patch images in DirectX? 

You would usually cull objects first. Only then do you walk your potentially visible set and build batches for rendering. If culling your objects proves to be slow you could try grouping them into bounding volume hierarchies and cull those first. 

In the shader you then receive the geometry data (the mesh vertex in its local space) and the instance data (the world space matrix) and can use both to render the instance. This means we can now render 100 identical meshes at different world positions in one go. What if you'd like the models to use different textures? We can do that as well by filling our additional (per-instance) vertex buffer with structs where the world space matrix is just one field and some other field might indicate which texture to use. In the sample you refered to that's called InstanceInfo. 

Bind the vertex and index buffers to the device, containing the mesh geomtry Bind the effect (shader) to the device Bind an additional vertex buffer to the device, containing 100 world space matrices Draw the mesh 100 times with instancing 

The name "Draw" is somewhat misleading. It should better be called "AddToBatch". What it actually does is batch your Draw calls. Only when you call ID3DXSprite::Flush or Id3DXSprite::End it fills the sprites into one big vertex buffer and renders that onto the current render target. So if you want to switch render targets in between, you will have to do something along these lines (pseudo-code): 

Bind the vertex and index buffers to the device Bind the effect (shader) to the device Set the world matrix for the effect to the first instance's world matrix Draw the mesh Set the world matrix for the effect to the second instance's world matrix Draw the mesh 

You might want to read Christer Ericson's excellent blog post on draw call bucketing: $URL$ Alpha blended objects will have to be draw back to front to avoid artifacts. State batching will not help with reducing the amount of alpha blended objects being rendered. If you're looking for a way to discard objects that do not visually contribute to the scene, then what you're looking for is culling. What state batching will help with is to ensure that your alpha blended objects are actually drawn last and in reverse depth-order. It will also ensure that within both your alpha and non-alpha buckets all objects are sorted so as to keep expensive state changes to a minimum (primitive, shader, texture, shader constant, etc.). As for rendering opaque objects front-to-back (with depth testing enabled): If your pixels are expensive to compute (read: complex pixel shaders), then not having to compute the same pixel multiple times can be an enourmous gain - which gives rise to the idea of an early depth-only rendering pass (Z-Pre-Pass). 

I'm using pyglet for game development, and sometimes encounter "bus errors". They are not consistently repeatable, and whenever I try to do simple debugging (eg move/remove variables, etc), the errors will eventually disappear entirely. I feel like I am encountering the notorious "heisenbug" class of errors. Intuitively, I feel like the problem is likely due to pyglet/opengl/video card interaction, but of course this is only a hunch. So my question is: how do I debug this kind of problem? 

I am making 3d planets in my game; these will be viewed as "globes". Some of them will need cloud layers. I looked at various Blender tutorials for creating "earth", and for their cloud layers they use earth cloud maps from NASA. However I will be creating a fictional universe with many procedurally-generated planets. So I would like to use many variations. I'm hoping there's a way to procedurally generate cloud maps such as the NASA link. I will also need to create gas giants, so I will also need other kinds of cloud texture maps. If that is too difficult, I could fall back to creating several variations of cloud maps. For example, 3 for earth-like, 3 for gas giants, etc. So how do I statically create or programmatically generate such cloud maps? 

The rings are casting shadows on the right-most planets. I want the rings to only cast shadows on their own planet. Is this possible? 

I am trying to figure out how to import .obj files generated using Blender into my Pyglet game. There is an importer object, example code, and an example .obj/.mtl file within Pyglet's contrib directory. The pyglet contrib version works (and is flickery; comment out ) with the pyglet/contrib .obj/.mtl files. When I try to run it with my .obj and .mtl files I get a blank screen. Why am I seeing a blank screen instead of my Blender-generated object? FWIW, here's my test repo. 

I pursued the pyglet_obj_test solution further and got it working. So, to answer my own question, here's a complete example: 

Without seeing the code, I am having trouble conceptualizing the problem. That said, here are a few ideas that may or may not be useful: 

There were no normals, so nothing was displaying. I updated the git repo to warn on missing normals. Unfortunately, I have no ground-breaking debugging techniques to report. I opened up the working .obj file and the "broken" .obj file, and by comparing them I noticed that one of the files had normals, and the other one didn't. 

Q: How are my transforms ( in InstanceBuffer ) being passed to the shader? A: By binding an additional instance stream. In the example you referred to it's these lines: 

If you wanted to draw 100 instances of the same mesh at different world transforms, you would thus have to repeat steps 4 and 5 for each instance. What's so bad about that? Well, you have to issue at least one shader constant switch and a draw call per model and minimizing these numbers usually helps improving performance. Now, wouldn't it be nice if we could just fill some buffer with the 100 world space matrices we'd like our meshes to be rendered to and just pass that buffer to a single draw call? Instancing does just that. It allows you to issue a single draw call that will repeat the rendered primitives a given number of times while advancing and addional vertex stream only every Nth primitive (once after each mesh). In our sample case the code would go like this: 

You might want to have a look at Inigo Quilez' website. Especially his article on advanced perlin noise and terrain raymarching could be interesting. 

Q: How does the graphics device know that I want to draw using a specific effect when trying to draw instanced geometry? A: By biding the effect to the device, setting up shared parameters before the draw call and passing per-instance effect parameters in the instance data stream. Q: Exactly what should I be removing from my 'VertexElements' variable - what happens to each element? Are they being passed to a shader? Am I right to assume I dont need the 5th element? A: Let's have a look how you would go about rendering the same mesh twice without instancing: 

You basically time your function calls and then store those timings in some kind of database. You can then add a debug overlay mode to your game and display those timing values. Niklas Fryholm's wrote a post on the bitsquid engine's development blog which I used as a starting point: $URL$ When you have the basics working most of the further work is identifying what data you want to collect and how you want to display it. I found looking at screenshots from RAD's Telemetry quite helpful in this regard: $URL$