I made no special case handling for clamping the back-side of the basis in the tool - my experiments and research lead me to conclude that the entire hemisphere should be represented for each basis, even if intuitively the backwards-facing samples seem like they would 'subtract' colour because of a negative dot product. 

I have had luck with cosine weighted hemisphere samples, and I know how to generate stratified uniform samples, but I wanted to experiment with combining the two. However, how do I correctly stratify the hemisphere when performing the cosine weighted sampling? Do I cosine-weight the stratification? How many rays do I cast in each stratum? My application is the collection of indirect samples in a lightmapper. 

I have a working light mapper that captures global illumination. I evaluate a hemisphere when collecting the data and store the result in the texel. However, I want to support directional lightmaps so that I can more realistically light normal maps at runtime. What strategies are there to adapt my light mapper's final gather operation to capture this data? Is it as simple as performing three hemisphere samples per texel in orthogonal directions? Or should I uniformly sample the hemisphere once, and project the results onto the basis vectors? Or do I sample the hemisphere and project each sample on to the basis. I've done some tests with the latter - projecting the each sample on the hemisphere onto the three basis vectors - and this works mostly, but I'm not sure about whether the projected samples should be clamped when their dot product is facing away from the basis or not. When reading all of Valve's HL2 Radiosity Normal Mapping articles, the need to generate three lightmaps for the bases vectors is clear, as are the coordinates for the bases vectors, as is the reconstruction in a shader of the data - this is straight forward enough. See Valve SIGGRAPH06, Valve Tutorial and Peter Houska on Directional Lightmaps). The complication comes from how to generate those three samples: For example - from a Valve GDC talk: 

Most raytracers do both! Have you ever seen a ray-triangle intersection test where the triangle is transformed so that one vertex is at the origin and the triangle is flat along one of the axes? That would simplify the test, but storing the inverse transformation to apply it to the ray takes more space than the triangle itself. So you could say that almost all ray-triangle tests do apply the forward transformation to the object. And yet most of the time you are rendering a mesh of many triangles, and "most raytracers" don't transform every vertex in the mesh by the object's transform matrix, they inverse transform the ray into object space. There isn't necessarily a good reason for this—it can definitely be more performant in some cases to preprocess the mesh and put every vertex in world space. But then say you're doing animation studio-scale path tracing. Now your scene geometry might not all fit in RAM at once. It's no longer a performance win to pre-transform all your vertices/control points, because (a) you need one copy of the object per instance, and (b) you need to transform it each time you read it into RAM. When data bound, the cost of two matrix multiplies per ray is insignificant anyway. Floating point precision is a potential reason as well: when you have an object a long way from the origin, you get more error from floating point positions, and if you pre-transform the object you apply that error in different directions to each vertex, whereas if you inverse transform the ray you have one fixed error amount from the imprecise position of the ray, but your vertices have less error relative to each other. Although I suspect the real answer to "why do most raytracers inverse transform the ray" is that most raytracers with public source code implement a wide range of geometric primitives, either for educational purposes or as a proof-of-concept, and it's easier to implement one cheap ray inverse transform than N shape forward transforms. (Plus, as has been mentioned, some intersection algorithms are much simpler if the object is in its canonical space.) 

When converting from uniform hemisphere sampling to cosine weighted hemisphere sampling I am confused by a statement in an article. My current indirect contribution is calculated as: 

Where the dot product is cos(θ) But in this article on better sampling ($URL$ the author suggests the PDF is (cos(θ) / pi), and there is no evidence of the N dot L calculation. My question is - does that mean that I no longer need to perform the normal dot rayDirection because it is included in the PDF, or is it in addition to the pdf? 

In the end, what I gleaned from the Valve paper, and what I found in an existing light mapper implementation, led me to the following conclusion: When performing my final global illumination integration, instead of using a cosine-weighted hemisphere of samples, use a uniform hemisphere of samples. For each sample, project the sample's direction on each of the three basis vectors, scaling the sample colour by the resulting projection. The three sums are accumulated, and then divided by the number of samples. At runtime, the normal map is projected onto the basis and the contributing lightmap colour for each basis is scaled by the projection prior to accumulating. 

Given that I am currently sampling on a hemisphere, do I simply project each sample onto the bases and store those accumulations? If I do that, how do I handle the fact that data within my original hemisphere, when projected onto the bases, could create a projection that actually subtracts energy, given that a sample ray pointing away from the bases gives a negative dot product. 

I think it is fair to say that the reason there are so many niche variations of GLSL/HLSL/Cg/whatnot is because no programming language is nor will ever be a one size fits all tool. Different problems require different tools, so sometimes it is worth the effort of developing a custom built tool if it is going to pay off in the long run. Stock GLSL is by itself pretty much unusable. It hasn't even acquired much stability across versions, so for a program that targets more than one OpenGL version, some sort of preprocessing is a must. HLSL is a bit more stable across versions, but again, if targeting more than one D3D version, some work will need to done to get good portability. The kinds of things people usually do are pretty much what you said, like a adding support for basic programming features such as modules and uniform syntax across versions, or even portability across different APIs (GL/D3D) without having to rewrite the shader code. More sophisticated things include fully fledged material systems or things like generating shader programs on-the-fly. Shading languages will probably get better and more generic in the future, incorporating things that today are commonly hand-rolled as core features. The new GCN architecture is a sign of that. So shading languages will be more usable out-of-the-box a while from now, but custom built solutions will never go away because there's only so much you can generalize.