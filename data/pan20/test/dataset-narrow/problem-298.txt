There is no perfect answer for this only you can decide the best option. And it has a lot to do with tradeoffs, and preferences. How much time you have now vs maintenance in the future. You could have: 

There are lots of articles and references available that explain how they work together such as this one. The keys words to search for are "sql server grant & deny". Permissions can be granted on the whole database, a schema or a specific object. (Table, view, stored proc etc) This can be applied per user or via a Role. Rather than modify the existing roles, you could create a new role and grant the permissions you want to the role, and the give users that. The most suitable method depends on how many tables they should/shouldnt have access to and how many users you would have to repeat this for. If there is 100 tables in the db, and the user should read 90 of them, id grant + deny. If you had to do the same thing for 5/10/50 users i'd create a role. 

I'm trying to get started with R services on SQL Server 2016. I was following this example from MS on youtube. 2 min 22sec into the vid John shows a reference to library(RODBC)... which I don't have. So I have downloaded the RODBC driver from CRAN According to the documentation: 

You could add date fields to record things like when they gained the qualification, or when it expires. The advantage of this approach is that an employee can have as many skills as you like without altering the table schema. The disadvantage is that the query logic is a bit more difficult when trying to find employees with multiple, mandatory or missing skills than if the were in different columns. And it gets very difficult to enforce rules and constraints at database level. Instead you must rely upon your application logic to handle this. You could easily extend this model by having a reference table of valid skills or a table describing positions and skill requirements. My last comment is that a flexible design is desirable. Because the real world has a habit of being messy and not following dbas rules. Org charts change. People get promoted. 1 guy has 2 jobs. People resign and then come back. Management decide that your 4 level hierarchy now has 5 levels or a 2nd tier manager now reports to another 2nd tier manager. The right answer for you will depend on many factors. And the result is almost always a compromise depending on how much time you have and how sophisticated the result needs to be. 

An important note: Do not set the value to where either the or the would be an odd number. NUMA loves balance, and by rule of thumb, needs to be divisible by 2. For instance, a configuration of 4 cores to 3 sockets would be imbalanced. In fact, if you were to run with this type of configuration, it would toss a warning about this. Section 3.3 in Architecting Microsoft SQL Server on VMware vSphere (PDF warning) outlines this in detail. The practices outlined in the whitepaper are applicable to most all on-premise virtualization of SQL Server. Here are a few more resources I have compiled through my research after Brent's post: 

You're specifying , an invalid time zone as it is a misspelling. The time zone you are looking for is . Incorrect: 

I have a table that captures the host platform that a user is running on. The table's definition is straightforward: 

I have built a database project within SSDT for Visual Studio 2017. I have also created Publish profiles that succesfully deploy to our dev, QA, staging, and production databases with no issue, including proper Pre/PostDeployment scripts. The issue that I now face is that I need to "dynamically" publish the database project to 300+ databases. By dynamically, I mean that each of the 300+ databases are relatively out of sync. For instance, I have objects that I want to drop in database ABC that might not necessarily be in XYZ. I'll also have objects in XYZ that I want to drop that aren't in ABC. The objects I want to keep are all in source control, so I am using the Publish option to . There might be table definitions that slightly differ from one database to the next, especially with regards to indexes and constraints, or temporary backup tables that were created during account specific investigations that should be dropped. Because of these inconsistencies across the schemas, an approach to build the project once and deploy that build script to all databases won't work as it might miss objects that need to be dropped or recreated in database XYZ. The goal is to essentially sync all of these databases to the same schema. I have read a few articles that delve into utilizing batch scripts, but they all seem to cover the case of building the database build script once and deploying that single script across all databases. I have not yet found a method that will build and publish the project for each of the 300+ databases individually. I understand that this is likely to be a rather long deployment process, but given the inconsistencies throughout all the schemas due to mixed deployment procedures prior to my coming on with the company, we need to get the schema to a state that is consistent across the board. We are using Visual Studio Team Services for our TFS repository, so I believe there may also be an option to create a Build Definition workflow in this scope. But alas, there seems to be relatively minimal information on this use case. PowerShell calls to SQLCMD might also be another option, but I have little to zero experience with this. 

Create a user db with a decent file size, and set the file growth to be 100Mb or 1Gb. Ensure each table has a primary key defined in sql server. Not just a unique index. Access wont allow you to update a sql table without a primary key defined in the linked table definition. The equivalent of an access autonumber is an identity column. If you have staging tables with duplicates its worth adding an identity column as a pkey. 

Another reason to use a data warehouse or dedicated reporting server is to avoid placing load on a production OLTP server. Or if you have to write reports which include data from different systems. The way you have described your db I doubt there would be much advantage in denormalising unless you wanted to build OLAP cubes on it. In which case an OLAP friendly schema might have significant benefits. 

I believe we've found another workaround. I'm posting my answer as I think it may be useful, and it's different enough from wBob's suggestion. We've changed the insert part of the merge statement so that it inserts to a temp table rather than the original target. Once the merge statement executes, we then insert from the #table into the target. It's not ideal, but at least the merge still handles the complexity of the 'upsert' by marking rows that have been retired/expired. We found this to be an acceptable trade-off compared with completely re-writing the merge as separate inserts and updates. 

If its an ETL process high PLE isnt very important. Your focus should be on throughput rather than cacheing. PLE is an indication of how long a page is held in memory. The more volatile the data is the lower the PLE. For lookup of reference data, A high PLE indicates that is is being cached in memory and is being reused without reloading from disk. ETL processes will have low PLE as they are deleting tables and reinserting, anything that queries that data will have to fetch it from disk. That's normal. You will also see high IO (disk reads & writes) when inserting and deleting data, regardless of how much memory you have. All transactions are written to the transaction log (even if your db is in simple recovery mode) before being saved to the datafile. The more tables and queries your data moves through during your ETL the more read and writes you will see. To improve throughput and performance you should investigate if your ETL process is using bulk inserts and truncates rather than individual deletes and inserts, as significant gains can be made here. Depending upon the ETL tools you are using there are a number of features that may assist with throughput performance. I suggest you start by reading about optimizing bulk imports and minimal logging. 

If we are performing row-level security and isolating the data based on and , is the primary key better served as above in a composite key, or separately? E.g.: 

We have been experiencing an odd behavior in our application where various modules will begin to timeout in SQL Server 2012. Each time we stumble across this issue, we find that the statistics require update and that running fixes the issue. After updating index statistics, the timeouts go away. However, the frightening issue is the frequency in which we have been experiencing the need to update the statistics, and the fact that nearly all of the statistics are showing a need to be updated across all of our tables related to order processing. Due to the frequency, we have setup a job to run prior to business hours at 7:30 AM. This seemed to have calmed the issue while we continued to investigate until it occurred again today. Not 10 minutes after business opening, they were receiving timeouts. I immediately ran and the timeouts disappeared and application function returned to normal. The order volume since business opening was low (less than 20 rows added to the primary order table), yet the statistics became so bad between 7:30 AM and 8:10 AM that we began experiencing time outs. A couple of notes: 

Based on my understanding of what I have delved into, the composite key is only a good idea if we will always be looking up the data on all three columns always. Since we are wanting to isolate the data, I cannot foresee instances where we wouldn't want to look-up the as well as the before seeking to the . Perhaps, however, I am misunderstanding the pros and cons and am better served utilizing only for the primary key, coupled with an index against and . Is my thinking flawed? I'm still in the infant stages of the schema's development, so I'll be running plenty of performance tests with large quantities of dummy data once I get the initial sketches completed. But as a general practice, what is recommended in this scenario? Furthermore and generally encapsulating multi-tenant database architecture that must ensure high levels of data isolation, has there been any significant movement forward that doesn't lean itself towards utilizing a two-valued key combination? I have read and watched a good deal on the topic, primarily referencing Salesforce's Mulitenant Magic Webinar and Google's F1 white paper. More recent articles still tend to follow the concepts they've outlined even in their age, and while I am building a schema for a database that will not be anywhere close the scale of Salesforce and AdWords, I find myself leaning towards the principles that they have resonated. 

Short version is yes its possible to do what you are describing. Your report will have a spatial query to display the shapes you want. Whether that's a map or a floor plan. You can then add analytical data in a second query to colour the map. The two queries needs to be joined by a key. Im not familiar with qgis but ideally you'd want both the spatial data and analytics in sql server. The other option is to have 2 layers. The first layer gives the 'map' shape and the 2nd adds points on top. If you search Google images for "SSRS map" you'll see lots of examples. 

If you are comfortable with excel you could import the data directly with an odbc connection. If you wanted olap capability you can do that with a tabular model within excel. In general most BI solutions will work with any relational database via an OLE or ODBC connection. If the product doesnt have a native adapter you could use a 3rd party ETL tool to move the raw data to a db that is supported. 

Yes you can work offline but it will be difficult to develop and test without the input/output connections files/tables your package will use. One option is to install SQL Server on your pc, create a database and copy the source and target tables and some test data to your pc. Then you can develop and test locally. Deployment will be difficult unless you have ssdt on the server. Otherwise you will need access to deploy to the server from another machine. 

You need to refer to the parameters in your main sql query. Something like: Where databaseName = @databases Or if you want to be able to select multiple databases. Where databaseName in (@databases) Here is an example using date parameters and multi-select parameters in a report $URL$ If you Google " ssrs parameter sql query" there are lots of working examples and even some youtube videos giving step by step instructions. 

The execution plan (PasteThePlan) is vastly different with this small change. It is tossing a warning that there is within the predicate. It was my assumption that would implicitly satisfy this argument as it does with . However, given that the operator returns each row from the initial input when there is a matching row in the second input, and because the warning exists, query optimizer assumes that each row is a matching row. Therefore all subsequent batch operations are ran against all 2048 rows in the table. What can I look into to better interpret what the execution plan is describing so that I can understand how to properly resolve the issue? Or, alternatively, am I simply missing the purpose of the operator when filtering result sets based on a static input? 

Login to the vSphere Web Client for your VMware cluster, and browse to the Virtual Machine that hosts SQL Server. Your VM must be offline in order to adjust CPU and memory configurations. Within the primary pane, go to , click the button in the top right-hand corner. You will open up a context menu that has . For reference, the below image is the incorrect configuration. Note that I have set to . Given the limitations of SQL Server Standard Edition, this is a bad configuration. 

Maintenance plans have not changed. Each night I run an integrity check, a index rebuild/reorganization (based on fragmentation level), a statistics update, a full backup, and log backups every 15 minutes before and after working hours from 6:00 AM to 7:00 PM. As far as volume goes, Tenant A sees roughly ~10,000 orders each day. With Tenant B added, we now see a combined ~25,000 orders each day. The volume has increased dramatically. The schema does not differ between the two tenants, business logic is the same save for tenant parameterization, etc.. With all this, I am a bit perplexed how we are seeing an increase in database performance with much more volume. I am going to continue to monitor and gather statistics throughout the week, but so far, both tenants are incredibly happy with the performance of the system. The tenant that was already on the database has brought up how the application is behaving much more quickly than before. Has anyone noticed such an odd case before, or perhaps know of anything to pay mind to?