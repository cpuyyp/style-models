for testing this, but you would probably want a , , datatype depending on the data you return and how accurate you need it. Also this doesn't include the or fields. You could use which would give you the timestamp field. The database name is tricky, as you would need to carry two variables across, and my script (above) only allows for one (if you are only looking at one database you could just type it directly into the query / set a default value). 

Am I missing something here. I'm looking at using to allow two applications to co-ordinate which data to wok on. From reading the manual it suggests that I need to apply a timeout to the command. e.g. 

values so that the two Databases don't generate overlapping primary keys. $URL$ After that on your Slave DB (B), simply run . Use those details to issue the statement on DB A. What you should find is that anything executed on A is then replicated to B and vice versa. However anything executed on A shouldn't be replicated back from B to A. And as such, the only queries written back to A will be those that are executed directly on B. Finally, you will want to set B to read_only to prevent anyone accidentally writing to it, until such time as you want them to. 

There are several possible things that can contribute. In the above case the row size was doubled with a single so that would easily explain the overall table size doubling. Even in the case that the row itself doesn't double in size the fact that the row becomes wider can cause page splits and increase the level of internal fragmentation. This is the case in the article you linked. It goes from having pages 99.8% full to 55% full. Every leaf page needed to be split as there was not sufficient room to accommodate the wider rows so that doubles the number of leaf pages. 

For a view to be updatable without using triggers "SQL Server must be able to unambiguously trace modifications from the view definition to one base table.". There is no performance disadvantage to Updating these Views as SQL Server will just generate a query plan for the base table affected. One possible disadvantage might be that it adds a layer of obfuscation so unless you are using Views as a security layer it is clearer to just write code that updates the base table directly. Another one might be if the View contains a one to many and you update the "one" side with a value from the "many" side it is undeterministic what result you end up with but the same applies to SQL Server's proprietary syntax. You would need to use or a scalar correlated sub query to avoid this possible issue. For Views that are not updatable and require an trigger there are performance implications as the and pseudo tables need to be generated from the base table so if possible updating the base tables directly will likely be more efficient. 

I'm trying to install the Percona Audit_log plugin for my instance of MySQL. I ran it previously so I know it works ok with MySQL (I'm sure it says so in the documentation anyway). I have MySQL 5.5.47, so I downloaded the files for Percona server 5.5.47-37.7 and extracted the file. I put that file into my plugin directory on the server (debian), but when I try to install it I get: 

I'm tryng to set up a new Slave DB by copying another Slave DB. Normally I would simply use to dump the Slave and create a file, then import that into the new database. However, as our new server is a Linux (Debian 7) server (the slave I'm copying is on a Windows server), I figured I would try to pipe the dump straight into the new database. This seemed to go well, until my putty session expired and it all stopped. I tried it again, this time using . But still the dump stopped as soon as my session expired. Is there a way to make this work? My full dump command is: 

What is the datatype of itemTimestamp? Is that column indexed? Is the column nullable? Assuming the answers are , Yes, and No respectively you might consider 

You cannot make schema changes when the database is read only but you could put all your user tables on a new file group and mark that as read only. You can expect a modest performance benefit from absence of locking. On versions of SQL Server prior to 2012 statistics can't be auto created or updated on read only databases. Before making it read only you might as well remove all logical fragmentation and make page density as high as possible. Any non default settings will not be of benefit in a read only environment. Additionally create/update any statistics anticipated to be of use for queries if on version < 2012. 

Also when viewing the stored procedures alphabetically sorted - e.g. in Object explorer this prefix may well not be helpful in trying to find a stored procedure and as they are ordered lexicographically it may be somewhat inconvenient that precedes . 

I have also tried stopping at (the equivalent of) pos 104627, 104628, 104629 (e.g around the previous ). But each time, replication continues until the start of the next relay_log, when it stops. Am I missing something? 

I'm sure there's a really obvious answer but: Why is the (byte length) of a string in a field shorter than the actual string? e.g. 

The IP Addresses in your variables need to be enclosed in single quotes in order to make them character strings (as they have been Declared). The variables need to be moved below all the statements. You also need to add a clause above and the matching clause before your final clause. 

I think I found the problem. Apparently the command ended at the symbol, so when the session expired the second part of the command stopped with the session. To get round this I ran: 

It turned out to be an issue with our Proxy Server, blocking downloads from percona.com Running revealed that every connection to percona.com 'failed', and so I got that added to the approved list, and after that it installed fine. 

No this isn't possible. This is invalid syntax in earlier versions and will cause a compile error. It is not possible to hide the in an inside the catch block either as a parameterless throw must be directly contained inside the catch. You would need to deploy the code version you want according to the version of SQL Server you are deploying to (and unfortunately there is not good support for this either in the SSDT tooling that I am aware of - no equivalent of including code lines selectively through conditional compilation) 

The execution plan I get for the query in the question first does the concatenation for every row in the outer query and then removes the duplicates by . This is incredibly wasteful. The following rewrite avoids this. 

Your numbers table is a heap and is potentially being fully scanned each time. Add a clustered primary key on and try the following with a hint to get the desired seek. As far as I can tell this hint is needed as SQL Server just estimates that 27% of the table will match the predicate (30% for the and reduced down to 27% by the ). And therefore that it will only have to read 3-4 rows before finding one that matches and it can exit the semi join. So the scan option is costed very cheaply. But in fact if any palindromes do exist then it will have to read the whole table so this is not a good plan. 

This may seem a bit strange, but I am trying to get a to execute on both Master and Slave. I have two databases set up in a Master (A) to Master (B) replication environment. Master A is Master B is I create a user on both Databases: 

Is there some trick to getting the Visual Explain feature to work in MySQL Workbench? I am running version 6.3.6 on Windows 7. I've tried simple queries with just one join. I've tried it with complex queries with 12 Joins. I've tried it with MySQL 5.5 and 5.7. But every time I just get I've looked on the Bugs for MySQL Workbench > Visual Explain, but there is no recent bugs, making me think it is something I am doing. But I can't see what. Does anyone have some suggestions? 

Do you use statement based or row based logging? If it is statement based then you most likely have set in your replication filters e.g. so that only queries where the database is are written to your binary log. e.g. 

You could also fire this off in a few concurrent sessions. Only one session will be able to get the exclusive lock at any one time. 

The obvious difference between the two plans is that the fast one is parallel and the slower one serial. This is one of the limitations of plans that insert into table variables. As mentioned in the comments (and it seems as though it had the desired effect) you could try doing 

In practice it seems that SQL Server does allow some additional cases beyond that mentioned in the documentation however. As you show in your question does in fact work so the restrictions on changes to columns used in indexes appear to be the same as those for user created statistics. Moreover the caveat mentioned about Primary Keys seems to be untrue also. The following works fine. 

The Top N Sort then re-sorts the data to implement the . In your case the Distinct Sort is logically not required for multiple reasons. The and branches are mutually exclusive (and this could be determined at compile time) and additionally the semantics wouldn't be affected even if there were erroneous duplicates. An alternative way of writing the query (that gives a better plan using the indexes to avoid any sorts at all) is 

OK, from what I can tell, Percona Audit Log Plugin, has very little in the way of configuration. I looked at McAfee Audit Plugin, but that doesn't seem to work on Debian MySQL. I also looked at MariaDB Audit Plugin, and that seems to have some level of filtering (DDL or DML). Unfortunately it requires 5.5.42 , and most of my DBs are 5.5.40. Still, on my test system it seems to work. 

In other words, get the value for all entries per minute for the past hour. However, in some cases I don't have an entry for some minutes, but I still need to know that there were 0 results for that minute. Is there a way of achieving this in MySQL? so I might get something like: 

They all use the same table. The thing is, I am pretty certain that the queries weren't running that long. I checked to see what was going on that confirmed the were active for that amount of time. 

To process each partition in turn. Note the plan still has a scan (with a seek predicate to select the partition) but this is not a full scan of the partition. The scan is in index order with direction "BACKWARD". The iterator can stop requesting rows from the scan after the first one is received. 

You can't. The return datatype must be specified in a function. It is not an optional part of the grammar. You can return though. 

So it might be provisionally assumed that the first ("unnested") syntax is potentially beneficial as it allows more potential join orders to be considered but I haven't done exhaustive enough testing to have much confidence in this as a general rule. It may well be entirely possible to come up with counter examples where Query 2 performs better. Try both and look at the execution plans. 

As you don't care about the order of the concatenated items it would be quite easy to knock up a custom CLR aggregate to do this and it will likely out perform the XML method, there is an example of one in this article. There is a quick and easy change you can make to your existing code though. Instead of 

I've tried every possible combination of I can think of, but it still doesn't work. What have I missed please? 

But as far as I can see the 'referenced table' is The 'referenced column' is , and is the so automatically has a index, right? I checked the documentation, and as far as I can see I meet all the criteria: 

You can also try . I sometimes find one works when the other doesn't for some reason. Failing that just delete them with like you said. It's not technically correct, but it shouldn't be a problem (I did this frequently in the past before I learned about the commands, and never had any problems). Of course you also want to make sure you have set as well to keep them down in future. I also just saw this note: 

I have the below Data / Query. Is this really the most efficient way of doing this? As it seems like a lot of work to get the result. I've tried reducing it / combining the parts but this is the only way I can get it to work. (n.b. The actual table has a lot more entries over multiple days - hence the WHERE filters at the end)