The Metropolis–Hastings algorithm was published in 1953 and dates back earlier to the Manhattan project, long before Rabin. Like many of the early randomized methods given in other answers, it is a Monte Carlo algorithm. Is it possible that the claim on the Wikipedia page is a garbled form of the claim that Rabin's was the first Las Vegas algorithm? 

It's not linear time, but here at least is an $O(n^{3/2})$ randomized solution to the problem in which you are given a set of $n$ elements together with the sums of $n$ randomly-generated triples of the elements, and you have to find a triple that matches one of the given sums. 

It's NP-complete for any number three or more of subgraphs. Suppose you have a graph $G$ for which 3-coloring is hard. Then partitioning $G\times K_7$ into three claw-free induced subgraphs is also hard. Here, $\times$ is the tensor product of graphs. Each vertex of $G$ is blown up into an independent set of size $7$, and each edge of $G$ is blown up into a subgraph $K_{7,7}$. Given a coloring of $G$, you can partition it into claw-free induced subgraphs (actually, independent sets, but independent sets are claw-free) by applying the same coloring to $G\times K_7$. On the other hand, suppose you have a claw-free partition of $G\times K_7$; then for each vertex $v$ of $G$ choose a subgraph of the partition that has at least three vertices of $v\times K_7$ and use that choice to color $v$. The result should be a proper coloring, because if not it contains a claw (actually, a $K_{3,3}$, but $K_{3,3}$ contains a claw). The same reduction (using a tensor product with a larger complete graph) works for partitioning into any larger number of claw-free induced subgraphs. I suspect that even partitioning into two claw-free induced subgraphs is hard, but that would take a different proof. 

Replace any negative value with zero. Then, think of the numbers in the range $1\dots W$ as vertices in a graph, and think of a pair $(x,y)$ as corresponding to an edge from vertex $x$ to vertex $x+y$ (or to $W$ if $x+y>W$), with weight $f(x,y)$. This graph is a DAG, and what you are looking for are the nonzero edges on a longest path from $1$ to $W$ in this DAG. DAG longest path problems can be solved in time linear in the size of the DAG (here, $O(WH)$); see $URL$ 

For which undirected graphs are all depth-first-search trees (for all possible starting vertices and for all choices of which neighbors to search first) directed paths? That is, every DFS tree should have only one leaf, and every other vertex should have exactly one child. For instance, it's true for cycles, complete graphs, and balanced complete bipartite graphs. Finding a DFS tree that is not a path is obviously in NP. Is it NP-complete, or polynomial? 

One area where unconditional and nontrivial time lower bounds are known is in data structures, where the time is for individual data structure operations (or sequences of operations). The standard model for this sort of thing is called the "cell probe model"; it assumes only that main memory is divided into words of a certain size and that the CPU has a very limited amount of local storage (e.g. in registers or cache), and bounds the number of word read/write operations that must be performed. For instance, in the cell probe model one can prove that the problem of maintaining prefix sums of word-length values (that is, maintain an array of values subject to operations that change one value in an array or that query the sum of a prefix of the values) requires $\Omega(n\log n)$ time to perform a sequence of $n$ operations on an array of length $n$. The difficulty here comes in part from the fact that the operations have to be performed online (not knowing in advance the sequence of future operations). Fredman and Saks' STOC'89 paper "The cell probe complexity of dynamic data structures" and Patrascu and Demaine's SICOMP'05 "Logarithmic lower bounds in the cell-probe model" are probably good starting points; look up cell probe lower bounds in Google scholar for much more. 

I'm pretty sure that the best known polynomial time solution is still Alon, Yuster, and Zwick's color coding technique (JACM 1995) which finds paths of logarithmic length in polynomial time (without needing the assumptions that the graph is cubic and Hamiltonian, only that it has a path of that length). This part is not the best known; see Björklund's answer for longer paths in polynomial time. A related problem is the best (exponential) time bound for finding the whole Hamiltonian cycle, which I believe is O(1.251^n) due to Iwama and Nakashima (COCOON 2007) improving one of my papers. 

For 3-regular graphs, having a simple dual is the same thing as having a polyhedral embedding (an embedding in which each face is a simple cycle and every two faces intersect in an empty set, a vertex, or an edge). But testing for the existence of a polyhedral embedding is known to be NP-complete: see B. Mohar, Existence of polyhedral embeddings of graphs, Combinatorica 21 (2001), 395–401, $URL$ I'm not sure whether polyhedral embeddability is known to remain NP-complete when restricted to 3-regular graphs — it doesn't seem to be in Mohar's paper — but it seems very likely to be true. 

Cantor spaces also come up in the theory of cellular automata: the Curtis–Hedlund–Lyndon theorem characterizes cellular automaton rules as the functions on Cantor space (viewed as the space of states of a CA grid) that are both continuous and equivariant with respect to translations of the space. 

Because we're computer scientists as well as mathematicians, and from the computer science side of the world we've learned the same thing the software engineers have learned about why not to use single-letter names for things other than the most local of variables: longer and more semantically meaningful names are easier to remember and keep straight, easier for others to read and understand, and using them helps avoid certain common errors such as using $k$ to mean three different things in overlapping parts of a paper. (Actually, I suspect that for the specific case you had in mind, names of complexity classes, this reason is historically inaccurate. But I still believe it's a good reason for using longer names.) 

There's also a linear time sorting algorithm (for large enough machine words) by Albers and Hagerup 1997 but I'm not sure which operations it needs or whether it could be adapted to use only arithmetic and not bit manipulation. The reference is: Albers, Susanne; Hagerup, Torben (1997), "Improved parallel integer sorting without concurrent writing", Information and Computation 136 (1): 25–51. 

Testing whether a pair of points $p_i$ and $p_j$ are the endpoints of a Delaunay edge can be solved as a linear programming feasability problem: Lift each point to one higher dimension by making its last coordinate be the sum of squares of the other coordinates. Then look for a hyperplane passing through $p_i$ and $p_j$, such that all the other points are on one side of it. This can be represented in linear inequality constraints by seeking a vector $v$ and scalar $c$ such that the dot product of $v$ with each point is at least $c$, and is exactly $c$ in the case of $p_i$ and $p_j$. So for any set of points in any dimension, it's possible to find the Delaunay edges one by one, by solving a number of linear programs that is polynomial in the number of points. The reason Delaunay triangulation is hard is not because of the edges, it's because there may be too many higher-dimensional features. 

This fact can be found in Godsil, C. D. (1985), "Inverses of trees", Combinatorica 5 (1): 33–39, doi:10.1007/BF02579440 (without proof, near the bottom of the first page): "noting that a tree with a perfect matching has just one perfect matching". The same paper provides a more general characterization of the bipartite graphs with a unique perfect matching: this is true for the subgraphs of the half graph that have perfect matchings, and for no other bipartite graphs. 

So if I understand it correctly you want the neighbors of each vertex to form a small number of contiguous blocks of the ordering? Then no, you need a larger number of blocks than that. Let's suppose we have a bipartite graph with a small number $k$ of (labeled) vertices on one side and $n$ on the other. Each edge from one side to the other can be present or absent independently of the others, so there are $kn$ bits of information required to specify the whole graph. On the other hand, suppose you could find an ordering with the neighbors of each vertex forming at most $b$ blocks. Then you could specify the same graph by listing the ordering of the big side of the bipartition (representable in $\le n\log_2 n$ bits of information) and the positions and lengths of the blocks for each of the $k$ vertices on the small side of the bipartition ($\le 2bk\log_2 n$ bits). So to have enough information to specify the whole graph we need $$ n\log_2 n+2bk\log_2 n\ge kn $$ or equivalently $$ b\ge\frac{kn-n\log_2 n}{2k\log_2 n} $$ Setting $k=2\log_2 n$ gives $b=\Omega(n/\log n)$, significantly greater than $\sqrt m=\Theta((n\log n)^{1/2})$. 

It seems unlikely to me for information-theoretic reasons. Expressing the answer to a sorting problem requires $\Omega(n\log n)$ bits of information. On the other hand, the answer to a maximum flow problem on an $m$-edge graph can be expressed (via a network simplex formulation) using only $O(m)$ bits of information (which edges are saturated, which are unused, and which form a spanning tree of used but unsaturated edges). Similar arguments apply to minimum-cost flow, etc. So to make a flow problem that has enough information in the solution to recover the sorted order, you need $\Omega(n\log n)$ edges. 

You will need to make some assumptions about what kinds of functions are allowed to get anywhere with this. The version of the problem where the elements of $S$ are linear functions from $\mathbb{R}$ to $\mathbb{R}$ has been studied, in a projectively dual form: if you think of each linear function $y=ax+b$ as being coordinatized by the pair of parameters $(a,b)$, and reinterpret that pair as the Cartesian coordinates of a point, then finding the minimizer for a given $x$ becomes the problem of minimizing a linear function over a dynamic set of two-dimensional points. This can be handled in polylog time per update by a dynamic convex hull data structure, such as the one by Overmars and van Leeuwen 1981 or its more recent improvements by Timothy Chan. Piecewise-linear functions can be handled by using a weight-balanced segment tree on the set of breakpoints of the functions to replace each query on a set of piecewise-linear functions by logarithmically many queries on subsets of linear functions. This duality approach can also be generalized to higher degree polynomials instead of linear functions but at the expense of turning the problem into a range searching problem in more than two dimensions, for which the time per update will be significantly higher. 

Just to get things going, rather than trying to close out this problem: there is an obvious nondeterministic algorithm using logarithmically many bits of space (search for a single path through the dynamic programming matrix) so by Savitch's theorem there is a deterministic algorithm with space $O(\log^2 n)$. Its time must be of the form $n^{O(\log n)}$, quasi-polynomial rather than exponential. There are some space lower bounds for edit distance in $URL$ but I don't think they match your version of the problem. 

For an extreme example, chordal graphs can have as many as $\binom{n}{2}$ edges but chordal graphs that happen to also be bipartite can have only $n-1$ edges (they are forests). Or even more extremely, consider complete graphs versus (complete $\cap$ bipartite) graphs. But perhaps it makes sense to restrict your problem only to classes of graphs that are naturally sparse, as the planar and minor-closed graph families are. It is not true that all minor-closed families are biparted; in particular, the graphs of treewidth $k$ are not. An $n$-vertex graph with treewidth $k$ has at most $\binom{k}{2}+k(n-k)$ edges, but (for large $n$ and small $k$) the complete bipartite graph $K_{n-k,k}$ has treewidth $k$ and is almost as dense with $k(n-k)$ edges. One can extend the bounds from planar graphs to $k$-apex graphs (graphs such that you can delete $k$ vertices and get a planar graph), and their generalization to $(g,k)$-apex graphs for bounded genus $g$: these graphs have at most $(k+3)n+O(g)$ edges but their bipartite subclasses have at most $(k+2)n+O(g)$. Moving beyond minor-closed properties but continuing with graphs related to planar graphs, the 1-planar graphs (graphs that can be drawn with at most one crossing per edge) can have at most $4n-8$ edges, but bipartite 1-planar graphs can have at most $3n-6$. See Czap and Hudák, "On drawings and decompositions of 1-planar graphs", Elect. J. Comb. 2013. And the graphs with book thickness $k$ have at most $(k+1)n-O(1)$ edges ($2n-3$ for the first page and $n-3$ for each subsequent page), but when they are bipartite the constant factor is lower because the first page is outerplanar. I suspect that similar things happen with other types of almost-planar graph.