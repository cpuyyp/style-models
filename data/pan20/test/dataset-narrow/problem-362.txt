Which indexes would be considered under-performing, or which ones could be modified for better performance? 

What does "parallel plan running in serial" mean ? I see a lot of 0x01000000, and a few 0x08000000's in my trace. How can i determine whether one query is hogging CPUs and if reducing it to 4 will help ? 

SQL Server Processor NUMA config - all 16 processors are on 1 NUMA node - But I thought since the host has 2 sockets, there would be 2 NUMA nodes ? vCenter Socket/Core config - 16 sockets with 1 core per socket - Is it better to change this to 2 sockets with 8 cores per socket since that matched underlying hardware ? I recently heard at PASS conference, that if an entire VM is being dedicated to a single host, then leave Hyper-Threading disabled. Anybody agree/disagree ? As a basic pen-paper calculation, we need to double our 'compute capacity' from 50 (16 cores x 3.2Ghz) to ~100 GHz. Is it better to go for higher core-count across 2 sockets: E5-4669 : 2 socket x 22 cores x 2.2 Ghz = 96.8 Ghz or higher clock speed across 4 sockets: E5-4627 : 4 socket x 10 cores x 2.6 Ghz = 104 Ghz 

Full SP here In what order are the locks being taken ? Is there any way to avoid these deadlocks ? Adding CPU, Memory, Indexes, or Locking granularity ? 

All articles I looked at mention setting ANSI_WARNINGS to ON to get it to work. But I Absolutely do not want to see those Warnings, because I'm sending query results as attachment in sp_send_dbmail which has that Warning message right at the top of attachment. Is it possible to suppress the warnings for heterogenous queries ? 

I need to setup a sql audit to capture all queries running on a server from any unauthorized locations, and one of the columns that is required is Row Count. In sql trace, this is possible. But I can't seem to find corresponding column in extended events. I looked into these 2 DMVs: 

By having more tables you're certainly not going to increase the memory allocation for MySQL. Number of tables has no influence on memory usage. Discussing memory allocation is out of scope now but this may be useful to take a look: MySQL memory calculator. 

OOM is usually a result of bad my.cnf config and actually your my.cnf overcommits memory big time. Run mysqltuner.pl to get a sense of what parameters needs tweaking. Many of your configuration values are way high and some of them doesn't even make sense. For example: 1) binlog_cache_size 3Gb? default value is 32768 bytes(!) According to dev.mysql.org: 

InnoDb ruby is a great tool which does exactly what you want. Troubleshoot If you don't use this can also be normal data growth so confirm this first. If you use then it is most likely indeed the undo history. You can confirm that by running: 

If it is not then you have to restart MySQL to make it happen. Oracle is working on getting more and more variable dynamic and you can see the trend going through 5.5 -> 5.6 -> 5.7. Unfortunately though there's always going to be parameters which require restart. 

Timestamp would be a great candidate for PK if you can keep it since it's most likely will participate in most of your queries and most like as a range. (PK is at the end of every secondary key). You could also experiment with the an instead. Use whichever gives better performance. Size Try to keep the row size as small as possible to avoid sharding/partitioning/archiving as long as possible (eventually you will need to but MySQL is quite good with tens of millions of rows even). To achieve this use only IDs in this table and have lookup tables where necessary. For example: instead of use . is 1 byte whereas action will be always at least 2 assuming , or similar small character set but longer the string becomes the more byte it will use obviously. can use up to 4 bytes / character. Only put index on columns where you really need to. Daily, monthly, etc aggregation tables will serve you better than heavy indexes on this table. A possible schema 

We run on Cisco UCS + VMware ESX + HP 3PAR. Host blade config : UCSB-B200-M4 , Xeon E5-2667 v3 3.1GHz , 2 sockets , 8 cores each, Hyper-Threading Active So total 16 physical cores, or 32 logical cores. We have Software Assurance with M$, so all are Enterprise editions, and lot of additional SQL core licenses paid for, so money is not an issue. Our primary single OLTP SQL VM is 'dedicated' to one of the hosts, i.e. no other VMs are allowed to run on it, cos it requires all 16 cores of power. Even with that, CPU regularly runs ~60-80% , so we're planning to upgrade hardware. Questions below : $URL$ 

Although inherent column order does not matter, but since you're looking for 'industry standards', you can search publicly available databases, like Microsoft's AdventureWorks, to see where Foreign Key columns are placed. It's usually at the beginning. This MSSQL query lists all FKs in a database : 

I've been performance tuning SQL servers for a few years as a DBA. I'm trying to create a fast-food version of performance metrics that can quickly (in 5 minutes) and accurately (provable) answer a question from Management "Does this server need more/faster _ ?" '_' being one of these 4 possible bottelenecks in an IT-stack bottom up (from server-perspective, without going in to the app/code/ui): 

I'm trying to troubleshoot a complex deadlock issue. There are 2 separate processes (service and agent job) which often concurrently execute the same SP but with different @BatchID parameters. This SP is called within an explicit BEGIN TRAN. There are 30 different Insert/Update statements within the SP. The 2 processes deadlock 5 times a day on the same table/index, around the same time each day, and the service process is always the victim. The table has many unnecessary/redundant indexes and a couple of Insert/Update Triggers. Lock Escalation is Disabled. Deadlock graph: 

There is a discussion going on in our company on what the ideal CPU count and Max Degree of Parallelism are for a 3rd party database server. The server has 12 CPUs, 32GB RAM and all database sizes add up to < 30GB so they can all fit in memory (I tried to force this by doing a select * from every table). On certain payroll days, the CPU gets maxed out to 100% for a few seconds. MAXDOP was originally set to the default 0. We later changed it to 8 based on several 'best-practices' articles. However the vendor suggests to change it to 1 (no parallelism), while others suggest changing it to 4, so that one run-away query doesn't hog most of the CPUs. I'd like to find out how many CPUs are actually being used by queries. There is a Degree of Parallelism event in Profiler. The BinaryData column says : 

With of course setting your threshold to the right level. The above example assumes 1% which is huge in terms of geo coordinates but you get the idea. Since MySQL can only use one column for range queries from a composite key having a combined index won't help but separate index on latitude and longitude will. Then MySQL can choose the one which is more distinctive for the specific case. 

Note that the reltuples is an estimate of the rows not the actual count at the moment but generally it's quite accurate (depends on how up to date your statistics are). 

The records are stored in a B+Tree on the Primary Key Every single secondary indexes will have your primary key appended to them 

It depends on your table structures, data layout and usage pattern. However I wouldn't consider 12 gb or actually anything that can fit into the RAM of a reasonable priced commodity server big data (~100-400 Gb). I work with MySQL servers happily running with terabytes of data. If your tables are well designed, indexed and your queries are well written you won't see issues. After that it's usually much easier to split your dataset by common usage pattern and logical dependencies. Move tables to a different database. For example split your log tables to a new server. You can repeat that as long as want until you reach the write limit of a single server and your cannot split your database anymore (all tables are strongly related). Then you can think about sharding. 

Be aware in this case you will get the results back in arbitrary order not necessary the order of ids produced by the subselect. 3) Run the select directly You can get the ids to a list from the database and use them to query from your model. 

If you want to directly pipe it to mail you can A) use the flag. The help is a bit confusing by saying: 

If you see [tablename].MYD and [tablename].MYI files it's MYISAM. If you see [tablename].ibd or only the .frm file then it's InnoDB. Based on that the process: MyISAM You can simply copy the .MYI, .MYD and .frm files to any existing MySQL instance's database directory. For example: You have your database in then you find the table which will be (.MYD, .MYI and .frm). You can copy only these three files to an arbitrary database on a newly installed MySQL like . InnoDB It's possible to import tablespace since 5.6 but if you can it's much simpler to copy the whole directoy. It's a common procedure to clone new replication slaves. In your case you copy the whole directory onto the new server and start MySQL over it. Make sure the innodb_log_file_size and innodb_log_files_in_group matches the size and number of what you have now. These are the files called and . 

We know that the survivor/victim are SQL sessions (SPID), NOT SQL statements. We also know that the UPDATE statements shown in Frame1 of both sessions are involved in the Deadlock. And How are they involved ? They are both REQUESTORS of the U lock on the Index Key. Am I correct so far.. But which statements are the OWNERS ? And when did they start holding the X lock on the Index Key? One of the standard recommendations for resolving/reducing deadlocks is to Shorten Transactions. But if there are a hundred similar statements, without knowing WHICH statement started holding the lock and WHEN, it's not easy to go about shortening a transaction.. 

We have a main analytics SP that calls 100 other sub SPs, all of which act upon the same set of data (Claims), check it for some business rules, and output discrepancies. The SPs need not all be run sequentially, they can be broken out into sections, which are dependent on previous sections (sequential), but independent within the section (parallel). After looking at many options like Service Broker, Agent Jobs, Batch Files, SSIS etc., I used this CLR code to parallelize the sections, and it gave great performance improvement. However, when I run multiple (5, 10, 15) main SPs concurrently (each of which analyzes different claims) , performance starts to taper as concurrency increases. I guess this is because of the overhead of creating multiple parallel threads through the CLR. I also see lot of XTP_THREAD_POOL sessions idle in sp_who2. Has anyone used CLR for parallelizing Stored Procedures in Critical OLTP Production workloads ? Are there any best practices for performance tuning SQL CLR ? Is there a threshold for number of parallel threads that can be opened before the overhead makes things worse ? If my system has 20 cores, does it mean creating > 20 parallel threads does not help ? 

There are thousands of counters, articles, products to help monitor these. But is there a simple, instant and accurate script that can pinpoint if any of these 4 need to be scaled up or scaled out ? e.g. sys.dm_os_wait_stats - SOS_SCHEDULER_YIELD has high signal waits => need either more or faster CPU. PAGEIO_LATCH => need more files or faster DISK Are these 2 accurate? Does Page Life Expectancy accurately 'PROVE' the need for more memory ? What is your GO-TO script for diagnosing a performance issue ? I've used sp_whoisactive, sp_blitz, Glenn's DMVs, Spotlight, Idera etc. but am yet to come across a script that will satisfy a CIO's question about where to spend budget money, or that will correctly blame the problems on bad code, or slow SAN, or the ISP. Everytime any (Network/Systems/DBA/App) engineer points fingers at the other team, we have to 'PROVE' our statement, and with Virtualization and Cloud, without ideal test environments, without downtime, it's getting increasingly difficult to provably pinpoint the source of server performance issues, other than maybe Task Manager <excuse the rant> 

Create a slave with xtrabackup (set master_info_repository to table if you have MySQL 5.6.2+) Have it replicated and let it catch up Stop the replication: (optional) Make a backup of master info file if you have MySQL older than 5.6.2 Make a note of the position with Make the mysqldump Destroy the database and all the files in and remove all the relay logs Restore the database with mysql_install_db and execute the dump (make sure you have file_per_table settings) (only on <5.6.2) If necessary restore replication (if it was in table than the dump contains the information so no need) 

In case of postgresql: You didn't write which version you have from postgresql. You can use LATERAL JOINs (available from 9.3) or window functions. Example for window function: 

Use Innodb unless you have a specific reason not to. If your coming from Oracle you will find its internal behaviour familiar therefore learning curve is shorter. For what you described you probably will want to enable (check which format suits your use case better: , or ). For hot backups I recommend to use percona-xtrabackup. It can stream data directly to a remote server so you don't even have to have double the free space available on the backup host. The documentation is very good and detailed and you can also find many source online. I hope this helps getting you started. Good luck! 

One remark on : in most cases you will find that it is quite expensive. Although very useful for OLAP type scenarios it might not be very well suited for OLTP because the operation can take significant time of your query and as you can see sometimes the suboptimal execution plan is actually faster. Fortunately MySQL provides switches for optimizer so you can customize it as you wish. For all the option you can run: 

For further optimizations if necessary you can look into spatial indexes (you haven't mentioned which MySQL version and engine you're using). Also you can geohash the coordinates into a single numerical column and have proximity match on that then filter by the coordinates which I use on bigger database and works like a charm. $URL$ $URL$ 

But all they have is reads/writes, but not actual number of rows. Is it possible to get this info in SQL 2008 or SQL 2008 R2 ? 

We're looking at converting several normal T-SQL Stored Procedures into Natively Compiled SPs for performance. The SPs use a ton of #tmp tables - select into, delete from, update from, and several unsupported constructs like SUM/MAX/ROW_NUMBER OVER PARTITION BY, WITH, LIKE etc. We have figured workarounds for many of these : Select into - Create table type ahead of time Delete from - use while loop as cursor ... I was able to work around SUM/MAX OVER PARTITION BY.. but don't know how to do it for ROW_NUMBER OVER PARTITION BY. Is there a way ? Also, has anyone done large-scale conversion of SPs to Natively-Compiled and any roadblocks you encountered ? 

We frequently see blocking on our SSRS box which houses both front-end Report Manager ReportingService and the SQL database engine with ReportServer & ReportServerTempDB Catalog databases 

Usually my criteria for dropping an index if all the reads (user seeks+scans+lookups) are 0 (or very close to it), and it has lot of user updates, meaning the overhead of maintenance is not worth the speedup advantage. Below are index usage stats of 2 tables on a server: 

Is vSphere Replication safe for SQL databases? We are a virtual environment (Cisco hardware + VMware hosts + HP san). We have a primary and disaster recovery datacenter, with SAN-level replication already setup for every 4-hours. There are several HA/DR solutions for SQL - alwayson, logshipping, clustering, replication etc. But we want the easiest administrative solution that can deliver good RPO & RTO. So we're looking to replicate at the VM-level instead of SQL-level strategies. Apparently, any VM-level snapshot/backup solution needs to be VSS (Microsoft's volume shadow service) aware to ensure highly transactional applications like Exchange or SQL Server comes back up in a consistent state after restore. VMWare's SRM(Site Recovery Manager) gives you the choice of array based replication or vSphere Replication. I read here that vSphere Replication is VSS aware and can offer utmost 15 minute RPO, which is very acceptable for most of our applications. Does anyone else use this for enterprise oltp applications with 15-minute DR RPO (not high-availability) ? Does it freeze IO on SQL server for unusable period of time ? Does it definitely ensure consistency when VM turns back on at the DR site ?