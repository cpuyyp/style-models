and then create as many sub-interfaces on each physical interface as you like. These sub-interfaces will show up in the running-configuration as independent interfaces allowing you to, for example, create a network with hundreds of interfaces, subnets and so forth. 

Your operating system - I'll take Linux as an example, but it can well be any proprietary system - should be able to detect the physical ethernet devices, instantiate them in the kernel and start their device drivers. Eventually these devices will show up as, say, "eth0" and "eth1". From that point on, the networking stack will just work as usual (e.g. you can assign IP addresses to them). It makes no difference whether the devices are connected over RJ45 cables or by lines on the PCB or backplane. 

means that the interface will not be considered for any IP protocol activity (i.e. sending and receiving IP packets) 

The computer connected to the modem would start one PPP (or earlier, SLIP) session, and it would get one IP address from the ISP. So yes, one connection for every dial-up connection. There were methods to "share" this with other computers on the same LAN (similar to what we do with Hotspots today). Of course the speeds would be so low that more than one user could not have done much anyway - and also the need for more than one computer in the home (or for that matter, in the office) with an internet connection was simply not felt :-) 

There is a "service unsupported-transceiver" configuration command that you can apply (at least on some models and IOS versions) which will allow you to continue to use transceivers manufactured by other vendors. This will, however, cause a syslog message to be emitted, and if you open a TAC case, however, the service rep may ask you to use a Cisco transceiver instead. 

I'll attempt to answer this bit, but this is only speculation since I don't have first-hand knowledge of vendor ASIC specifications. Forget about VXLAN for the moment, and consider how an Ethernet switch ASIC works. Ethernet switch ASICs have built-in support for replication. That is, when a BUM packet enters one interface, it is replicated in the ASIC to interfaces on the same vlan. This happens at the "full switching rate" of the ASIC switching engine, i.e. the ASIC forwarding pipeline sees only one packet irrespective of the number of copies that are generated. So coming back to VXLAN, then, it is possible that the ASIC is able to extend the same replication mechanism to VXLAN as well; only, instead of physical interfaces, the ASIC now has to replicate the packet to multiple VTEPs. If this can happen at the "full switching rate" of the ASIC, i.e. the ASIC forwarding pipeline sees only one packet irrespective of the number of copies that are generated, this is obviously awesome. Less capable ASICs would rely on recirculation [Digression: Recirculation is the process by which a packet is transmitted out of an interface and it comes back in on another interface - the interfaces in this case are not external interfaces on the front panel of the switch, rather they are "internal" interfaces in the ASIC silicon, but for the purpose of visualization you can think of two physical interfaces connected back-to-back. Switch designers rely on recirculation when the ASIC is incapable of achieving some functionality in one pass, and the packet has to be sent through the ASIC multiple times before the output packet emerges on the desired interface/vlan with the desired encapsulation]. So if the ASIC's VXLAN switching path relies on recirculation, then think about what happens to a BUM packet: if there are 5 VTEPs that the packet is to be replicated to, then the ASIC pipeline sees 6 packets (1 original plus 5 copies), which reduces the ASIC's overall throughput. So in summary, if your intention is to evaluate and compare the ASIC capabilities from Vendor A and Vendor B, the question to ask them is whether their ASIC employs replication in the ASIC for Head-End Replication of VXLAN packets, or whether their ASIC uses recirculation. 

When the o/p interface starts seeing congestion, and there is DBL enabled, this table is consulted to see which flow(s) to drop packets from. 

As far as I could see, there is no Cisco document that describes what happens in the scenario that you mention. But even if there was such a document, you would be best advised to verify the behaviour for yourself, with your device and software version, because syslog is such an important component. Here is how I would verify it: 

See this WTD = Weighted Tail Drop AQM = Advanced Queue Management (I think) Network Ports = "downlink ports" i.e. non-uplink ports Translation: The 3850 is marking some traffic headed out of one or more downlink ports for tail drop 

So you can see that every device only has to configure its own networks, and the protocol does the rest. Note that this will only work because you said your core devices are connected over a switched ethernet network (or something that behaves like it). 

The AUX port is a serial interface used for managing the router itself. It cannot be used as a network interface, i.e. for sending/receiving routed packets. If three routers are enough for your lab, and you just need lots of interfaces, you can try connecting them up like this (since you say each router has two interfaces available): 

From this link I can see: The number of frames received on an interface that are smaller than 64 bytes (or 68 bytes for VLAN-tagged frames) and that have valid FCS values. The frame size includes the FCS bits but excludes the frame header bits. Now, why this should show up for 214-bytes frames is hard to say. Is it possible that the frames matching against this counter are not the same ones you see on wireshark? 

If I understand correctly, the problem is how to charge users based on how fast they consume data, rather than (or in addition to) how much data they consumed Cisco IOS has a "show interfaces" command, where you can see this: 

I believe what you're really needing to know is whether the interface is being used "optimally" and how much "spare capacity" you have on that link. These are fuzzy terms, but important because you need to know whether, for example, that 10Gbps interface needs to be replaced with a 40Gbps. From the "show" command output we know that in the last five minutes there was very little traffic on the interface. But maybe three days ago, at 2pm, there was 9 Gbps traffic for a few seconds or milliseconds. The "show" command output right now is not going to give that information. The only way for you to get that data is to monitor the interface over a period of time (like maybe a week or a month), especially when there are peak workloads. 

Let's forget about SDN for the moment, and consider what happens in a traditional router. There is some software that looks at the BGP configuration and also the BGP information from peers, and reaches some conclusion about what prefixes must be forwarded to what interface. Now the same router may also have other routing protocols running simultaneously, and there may be some statically configured routes as well. The router "distills" the information coming from these disparate sources (routing protocols, static routes and others) and comes up with a final "Forwarding Table". When a routable packet arrives on an interface, the router hardware (ASIC) or forwarding software will consult the "Forwarding Table" to see where (over what interface) that packet must be forwarded. In other words, even in traditional routers, there is a logical separation between the software that derives the Forwarding Table, and the software (or hardware ASIC) that uses the Forwarding Table. Coming now to controller based architectures: here the idea is that we move the derivation of the Forwarding Table to a different physical device (the controller). The actual router is now left with the task of merely using the Forwarding Table that the controller sends it. 

(1) Identification: The router would have to examine these packets, check if they are IPv4 or IPv6 (with the fragmentation extension header), then look at the fragmentation fields to identify reassembleable fragments. (Not all packets are IPv4 or IPv6, and the implementation would have to leave these packets alone.) (2) Transmit order: It is possible that P1, P2 and P5 are fragments of one datagram, and P3, P4 and P6 are fragments of a different datagram. The implementation would therefore have to first reassemble and transmit (P1 + P2 + P5), then (P3 + P4 + P6). Normally queues are first-come-first-served, but now you'd have to "cherry-pick" fragments from across the entire queue. Also consider what would happen if P5 is not the last fragment of the datagram; so you have to wait till the last fragment showed up in the queue, but in the meanwhile (P3 + P4 + P6) is ready to be reassembled and transmitted, so would you transmit it? (3) Out-of-order fragments: Note also that it is possible that P2 might in fact be the first fragment, P1 the second and P5 the third. This is because these fragments may have taken different paths on their journey from A to R. Normally, end hosts deal with this situation of out-of-order fragments, but if routers start doing reassembly, this is something that they have to take care of as well. (4) Checksum recomputation : Another thing you'd have to take care of after reassembly is checksum recomputation. Note that earlier in the routing pipeline we have already recomputed the checksum once (after decrementing the TTL), and now in the output queueing stage, we'd have to do it again after reassembly. (5) Refragmentation: Another thing you'd have to consider is refragmentation: if in the above example P1, P2 and P3 were of sizes 4000, 4000 and 1500 bytes, and the output interface had an MTU of 9000, would you leave the three fragments alone or would you refragment into two packets of size 9000 and 500 ? (6) Then finally you'd have to think about performance. All the above processing would have to be done at line rate, i.e. after every enqueue to the queue. For a router that supports even 10Gbps line rate performance you can calculate how fast the reassembly related processing described above has to happen. In summary I'd say that this is possible in principle, but the practical issues are many. And the benefit does not justify the engineering cost involved in implementing this (read: if you were a buyer, how much more money would you be willing to spend on a router that can reassemble versus a router that can't?). Having said that, if some smart end-user application designer can build an application that can demonstrate superior performance (measured in terms of $$ :-)) by using routers that support reassembly, then it's a different story. 

OK, so why is R3 ARPing for 202.100.30.252? Shouldn't it just send the ICMP packet towards R1? Well, that's because you have this on R3: 

To start off I'm going to first ask you to ignore queueing. Imagine that routers in the following text have no queues at all. It is possible to understand congestion without bringing queues into the picture. So in the text below, a packet enters a router, and either gets transmitted immediately or gets dropped. (I will write a couple of lines about queueing at the end of this answer.) The first thing to understand about congestion is this: interfaces can only operate at a finite speed. A 10Gbps ethernet interface can transmit packets at maximum rate of 10Gbps, no more (this translates to about a million packets per second, if the packets are about 1000 bytes in length). OK, now consider a router like this: 

Looks like a bug. Get tech support. In the meantime you can try and narrow down the problem by restarting your switches with the smallest possible hardware and software configuration and see what triggers the problem. (1) Hardware: 4510RE is a dual-supervisor chassis with line cards. So, pull out all the line cards, pull out the redundant power supply, pull out the redundant supervisor, and start with just the one supervisor and see if the abnormal termination still happens. If it does, you really have no option other than to wait for support. If it boots fine, then you can try inserting all the removed components one-by-one. (2) Software: Sometimes abnormal terminations can happen when there is something in the startup-configuration that the software does not "digest" properly. Cisco provides a way to boot the device by not reading the startup-configuration (this is actually a part of the password recovery process) - you can try this process and see if there is something in the startup-configuration that is causing the problem. To reiterate: the above is just a stopgap arrangement until your tech support finds the problem. My personal opinion is that is is better to just wait for support. 

So, if your users are all on different physical interfaces, you can use the above information. Obviously you need to have external software that continuously polls for this and stores it somewhere, because the 5 minute rate displayed by IOS is only for the last 5 minutes in time; there is no historical record. But is this really for an enterprise deployment? If not, this question is off-topic on this site :-). 

Hubs are bad news, from both a network performance as well as a security point of view. As a network admin you may find it handy to have a hub or two sitting in the store for the day when you want to do a packet capture (which for some reason cannot be done in the usual way (span etc.)), but don't leave them running in a production network. 

The 4500 does not support switching on the line card at all. All packets are received on the line card, travel over the backplane, reach the supervisor where the ASIC is located where they get switched, then travel over the backplane again to the linecard with the egress interface on the way out. The 6500 supports switching on the line card (aka distributed switching), but you have to check specific combinations of linecards/supervisors/IOS versions. Yes, linecards that are capable of distributed switching do contain on-board ASICs to do the job. 

Since you have used Cisco terminology in your question, I will assume that you are only talking about Cisco equipment. Also I will assume that you are only interested in L2+L3 devices (like the Cisco Catalyst family of switches), not in the pure L3 devices (like the ISR and ASR routers). As Zac67 points out, there are some models of Cisco switches with just pure Layer 2 capability and zero Layer 3 capability. All switches in the Catalyst series are, however, capable of some layer 3 functionality, although the software may deliberately disable some layer 3 functionality based on the license etc. and I will confine my answer to these L2+L3 devices. Rather than distinguish the behaviour based on "this is what a layer 2 switch contains" and "this is what a layer 3 switch contains", a more useful discussion would be based on how layer 2 switching (bridging) is done and how layer 3 switching (routing) is achieved. I will use the word "forwarding" to mean both bridging and routing. At a high level, there are two approaches: software forwarding and hardware forwarding. As the name suggests, in hardware forwarding the ASIC forwards the packet. In software forwarding the packet reaches the CPU where the software code will examine the various fields of the packet and determine which interface(s) the packet will have to be sent out on. Hardware forwarding is much faster, but software forwarding is more flexible because it is just based on code that some programmer writes. It is important to note that hardware and software forwarding co-exist. In the ideal case all packets will be forwarded in hardware, but there are situations where the packet cannot be forwarded in hardware and must be forwarded in software. There are many examples of this. For example, there is no Cisco ASIC that supports Appletalk routing, but there are IOS versions that still support Appletalk. If the switch receives an Appletalk packet, and Appletalk is configured, the packet is sent to the software where the Appletalk routing code will route the packet to the correct interface. Another example is an IPv4 packet with one or more Header Options fields present. Another example is when there are so many routes that the hardware table (i.e. ASIC TCAM) is unable to accommodate more routes. Cisco IOS uses multiple techniques for L3 routing a packet in software: (1) process switching (2) fast switching and (3) CEF switching. These are all different software techniques, with different performance in terms of the maximum number of packets that can be routed per second. Fast-switching #2 is somewhat obsolete. CEF switching uses a software data structure called FIB in order to determine the output interface to which a packet must be sent. L2 bridging in software has no specific named technique. It's just called "L2 bridging in software". Coming now to hardware forwarding. ASICs are designed by the vendor keeping in view the requirements of the market segment, one of them being performance and the other requirement being cost. So the components that go into an ASIC are basically the cheapest components that can be put in while at the same time meeting the performance criteria. What I am trying to say is that there is no hard and fast rule that says that L2 bridging must always use a CAM. Yes, for L3 routing, because the requirement is to match on variable length CIDR masks, a TCAM is the most efficient component to use going by today's available technology. However, for doing, say, L2 MAC address lookup (which is a full 48-bit lookup), an ASIC designer may be able to get away by using a cheaper RAM-like component (especially if the entries can be hashed or sorted or arranged in such a way that lookup time can satisfy the performance constraint). The layer 3 route lookup TCAM in Cisco switches is a hardware representation of the FIB. In other words, the same FIB data structure that is used in software L3 routing technique #3 above is programmed in the hardware TCAM to achieve hardware routing of IPv4 and IPv6 packets. Note that TCAMs are also used in ASICs for other reasons than L3 routing. One example is to implement security ACLs, and to identify packets for QoS treatment. Cisco 4500 and 3850 switches have TCAMs for both L3 routing as well as for security/QoS. Final note on "merchant silicon". In recent years, there is a school of thought among major equipment vendors (Cisco, Juniper, Arista...) that there is only so much "secret sauce" that can be put into ASICs, i.e. there isn't much competitive advantage to designing their own custom ASICs for doing L2/L3 forwarding. The competition is now in software innovation, and for this reason the thought process is "why not just source the ASIC from vendors like Broadcom, and focus the innovation efforts in software?" Having said that, at least Cisco has invested several billions of dollars over several decades to build in-house ASICs, and there is a reluctance to just throw it all away.