I have gone through your code and results. There seems to be something wrong with the implementation as none of your predictions give a value greater than 0.5 . I couldn't pin down the problem. Some debugging will be needed on your side. What's paradoxical is that your loss is reducing. I suspect this might be because your data set is unbalanced i.e., you have 221 0s and about 30 1s. This could be the reason for other problems as well. Consider the wikipedia example, where the values are correctly matching. 

Not necessarily. The reason a fixed length is used in keras, is because it greatly improves performance by creating tensors of fixed shapes. But that's only for training. After training, you'll have learned the right weights for your task. Let's assume, after training for hours, you realise your model's max length wasn't big/small enough and you now need to change the time steps, just extract the learned weights from the old model, build a new model with the new time steps and inject the learned weights into it. You can probably do this using something like: 

Now, tanh treats all of them differently. Note You are doing a classification task. Using mean squared error as a Cost Function won't yield the best results here. You should instead use Cross Entropy Loss with One Hot Vectors and softmax. As to why should be a whole different answer. Here is a link for the same. 

Let's start by answering your first question. Is it required to balance the dataset? Absolutely, the reason is simple in failing to do so you end up with algorithmic bias. This means that if you train your classifier without balancing the classifier has a high chance of favoring one of the classes with the most examples. This is especially the case with boosted trees. Even normal decision trees, in general, have the same effect. So it is always important to balance the dataset Now let's discuss the three different scenarios placed. Choice A): This would be what I explained all along. I'm not saying necessarily you will have a bias. It depends on the dataset itself. If the nature of the dataset has a very fine distinction with the boundaries then the chance of misclassification is reduced, you might get a decent result but it's still not recommended. Also if the data does not have good boundaries then the rate of misclassification rises a lot. Choice B): Since you are placing weights for each sample you are trying to overcome the bias with a penalty. This is also called as an Asymmetric method. Normally these methods increase the accuracy of a model by a slight margin but that mostly depends on the machine learning algorithm you are using. In examples like Adaboost such a model the effectivity of the model increases. This method is also called Asymmetric Adaboost. But this might not necessarily work with all algorithms. Choice C): Assuming you have weighted the samples accordingly it should do the same as either choice A or choice B. I'll leave this for you to extrapolate based on my previous explanations. 

So basically, we feature engineered the word vectors. For autoencoders, It takes in $X$ as the input and tries to predict $X$ again, in the process learning a latent representation of the input signal X. The input hidden representation in Layer $L2$ can be used in other tasks. (Note: here X and X hat are the same) 

If your data input to tanh has a magnitude of 10 or more, tanh produces an output of 1. That means it treats 10, 50, 100, 1000 the same. We don't want that. This would explain 

TL;DR: Represent words as word vectors. Then add extra dimensions to the word vectors. In these extra dimensions, include POS, NER, etc. features in a numeric form. Longer Version: Say you have a word2vec/Glove model with each word represented by a 100 dimensional vector. Additionally, you have features like POS, NER, etc. for each word. Instead of representing each word with just 100 dimensions, represent it with dimensions and fill these slots with the nlp features. This will give the vectors an additional meaning which you have defined manually. Suggestions You could benefit from using other models like Doc2Vec/Sent2Vec which can represent a whole sentence as vector. With these vectors you could query similarity of sentences. So say you've trained it on your data, just query : 

First and foremost you need to know the difference between the type of data you are trying to predict. The two general categories are discrete and continuous. Most people tend to miss out that classification is at its core a discrete "regression". The values are predicted for discrete variables by considering a decision-based approach which ultimately leads to finding or predicting the variable in question. You can call this classification. For continuous variables, however, since the values can have a certain range it most closely mirrors a curve fit with the addition of an error boundary. I think this should clarify what you're essentially trying to ask. 

Now if it is equal to the median there is no problem in choosing whichever value but in real time it's most likely to follow the other two cases. The reason the mean is greater or lesser than the median, in this case, is because it skews to either higher computational time or lower computational time. What this skew represents is you could say where the most likely frequency of computational times would be. This implies it gives a more normalized approach and it shows where you can expect the computational times of most of your execution runs expected to be. So choose mean over median. 

If you don't have the background image, you can try methods like this provided by opencv Basically what you are looking for is background subtraction/foreground detection. Hope this helps. image source: $URL$ 

You need to do some Background Subtraction on the images. If you have the Background image without the animal, you can simply subtract it from the current image to get just the animal. Once you have just the animal, you can apply SIFT or CNNs or whatever. This is called frame differencing. 

I haven't tried it out myself. Please try it and post your results here for everyone's benefit. Here are some links: one two 

As per Andrew Ng's Course, if you use the sigmoid activation, the outputs represent the probability of either outcome 0 or outcome 1. So the decision boundary is 0.5 if prediction > 0.5 , the prediction is 1 if prediction <= 0.5 , the prediction in 0 Here's a screenshot from Andrew Ng's slides: 

Learn Word2Vec by implementing it in Word2Vec : an article by me explaining word2vec. (Shameless self-advertising here but I feel the article is good and relevant) Andrew Ng's unsupervised feature learning website 

TL;DR: Yes. You can (iiuc) Longer Version: In fact, this is what many popular algorithms like Word2Vec and AutoEncoders do. (With respect to hidden layer outputs) Word2Vec: Given an input word ('chicken'), the model tries to predict the neighbouring word ('wings') In the process of trying to predict the correct neighbour, the model learns a hidden layer representation of the word which helps it achieve its task. Finally, we just remove the last layer and use the hidden layer representation of the word as its $N$ dimensional vector. 

Should we use PCA in machine learning algorithms more often? Well, that strictly depends, using PCA reduces the accuracy of your data set so unless you need to save up some space caused due to a lot of features with bad correlation and the overall accuracy doesn't matter. If your machine learning model scenario is similar to this then it is ok to proceed. However, most use of PCA is as you asked before for visualizing higher dimensionality data to determine the data trend and to check which model fits best. 

This should mostly do the job. Use the arr1 ,arr2,arr3 in the function you mentioned. They are the 1d array of the columns you split 

The paragraph you mentioned explains a the parametric procedure of creating training data and testing data from a given data set. Let us take an example let us consider that the distribution of a certain dataset follows Normal distribution (Gaussian) This means that 68% of the data lies near the mean of the dataset. Also since the dataset has been identified as gaussian we also know the expected probability function (pdf) of the data set assuming we know the mean and variance of the given dataset. $P(x) = \frac{1}{\sqrt{2 \pi \sigma ^2}} e^{\frac{-(x-\mu)^2}{2 \sigma ^2}}$ Now that we have the formula we can use random variate generation techniques on this formula to create training and test data separately which can be used for the model to learn and test its efficiency. To learn more about random variate generation I'd direct you to this resource here. It has a great chapter which can help you with understanding the statistical technique behind it.