Also note that procedural content generation isn't necessarily incompatible with a fixed world: you could just pick a fixed RNG seed and use it to generate your world. This can be useful if you want a huge world for the players to explore, but want to keep it the same for every game and every player. Note that, if you do this (or possibly even if you don't), you really should design your world generator to work in a hierarchical fashion, using multiple RNG instances, such that e.g. the overall map generator would keep a single RNG instance that it would use to generate subregions, assigning to each subregion a different seed value that the region generator would then use to seed a separate RNG instance that it'd use to generate the region, and so on. This is to avoid the "butterfly effect", where changing even the tiniest detail of the tiniest part of the map could throw the RNG out of sync and cause everything else in the world to be completely different. Another important way to avoid the butterfly effect, particularly if you're generating the world "on the fly" as the player explores it, is to avoid normal RNGs entirely (except for processes that happen "instantaneously" from the player's viewpoint, like generating a new level when the player enters it) and instead use random number generation methods that don't store any internal state. For example, when choosing the seed for a subregion the player is about to enter, the overall world generator could take the coordinates of the subregion (and its own overall seed) and feed them to a hash function to generate the subregion seed. That way, each region will always look the same, regardless of the order in which the player enters them. 

All in all, this seems like a rather strange way to design a game, at least if taken at face value, and I suspect that "allowing more social interaction among players" is far from the only problem you'd face in trying to implement something like it. That said, I'd certainly like to see someone try this, if only for the weirdness factor. 

You say you already have code to let the units avoid each other while in transit, so why not just keep that part of the code active even after reaching the goal? In fact, to keep the units from clustering too tightly at the destination, you may want to increase the preferred separation for each unit as they reach the goal, so that they'll move out a little and give room for new arrivals. 

Ps. One possible option for a suitable existing license, which I only thought of while writing the above, would be the GNU LGPL. It's a more permissive variant of the GPL, specifically intended for libraries and other components that are meant to be usable as parts of other programs without those other programs having to be GPL-licensed, and it does include a user-visible attribution clause of sorts (emphasis mine): 

would very likely also work for picking up the red chrysanthemum. Indeed, players would (and, indeed, still do) very much expect any well written game to understand such natural synonyms. Similarly, an amulet might also match the word , especially if it was described as being or resembling one anywhere in the game, and the circular amulet might also be described as . Indeed, this parsing style is still very much in use in modern interactive fiction. For example, you might want to take a look at how the parser recognizes objects in Inform 7, a special-purpose language designed for writing this kind of games. 

By the way, a common way to make procedural and manually generated content mesh more or less seamlessly is to start the manual generation process by using the procedural world generator (maybe suitably tweaked, e.g. to generate a village or a mountain peak where you want it) to initialize the region you're designing, and then manually tweaking it to add and adjust the details you want. (This can also allow very compact level storage, by only saving the manual changes and the generator seed and parameters used.) Other ways to combine manual and procedural content include leaving blank spots in your manually designed levels, to be filled in by randomized content (e.g. a castle could include a randomly generated maze in the dungeons, with only the outline and the exits fixed), or the manual design could specify only the broad outline of some parts of the level (e.g. the placement of houses in a town) and leave the details (such as the exact appearance of each house) to be picked randomly. In fact, even many games that have entirely fixed game world use some form of procedural generation as part of their level design process, since there are some aspects of world generation (like producing natural-looking terrain, or placing individual trees in a forest) that are difficult and/or tedious to do by hand, but relatively easy to automate. Conversely, most procedural world generators will use at least some manually designed objects and elements to build the world out of. One could even easily have multiple levels of nested random / manual generation: for example, a procedurally generated world could include a manually generated town surrounded by procedurally generated woods with a manually drawn outline, with procedurally generated trees bearing manually designed leaves. 

Here, the green dot is fleeing from the red dot, and has two choices for a path to take. Going down the right-hand path would allow it to get much further from the red dot's current position, but would eventually trap the green dot in a dead end. The optimal strategy, instead, is for the green dot to keep running around the circle, trying to stay on the opposite side of it from the red dot. To correctly find such escape strategies, you'll need an adversarial search algorithm like minimax search or its refinements such as alpha-beta pruning. Such an algorithm, applied to the scenario above with a sufficient search depth, will correctly deduce that taking the dead end path to the right will inevitably lead to capture, whereas staying on the circle will not (as long as the green dot can outrun the red one). Of course, if there are multiple actors of either type, all of these will need to plan their own strategies — either separately or, if the actors are cooperating, together. Such multi-actor chase/escape strategies can become surprisingly complex; for example, one possible strategy for a fleeing actor is to try to distract the enemy by leading it towards a more tempting target. Of course, this will affect the optimal strategy of the other target, and so on... In practice, you probably won't be able to perform very deep searches in real time with lots of agents, so you're going to have to rely on heuristics a lot. The choice of these heuristics will then determine the "psychology" of your actors — how smart they act, how much attention they pay to different strategies, how cooperative or independent they are, etc. 

If you want your game to have consistent behavior across platforms, you really should have a fixed frame rate for your game logic and physics code. That way, your time step will be constant, which means that any (inevitable) roundoff and other errors caused by the finite time step will be the same for all players. The one major exception to this is games that don't (always) run in real time. For such games, you want to keep the time step constant in game time, but you can allow game time to run as fast compared to real time as the player wants and the platform can handle. It's OK — and, in fact, often very useful — to allow your graphics front-end code to have a variable frame rate, but even then, there's obviously no point in letting the graphics frame rate exceed the physics frame rate. (Actually, that's not completely true; if your drawing code can interpolate / extrapolate movement between physics updates, it may be useful to run it more often. However, doing that well involves some relatively advanced issues.) There's also no point in letting your graphics frame rate exceed the screen refresh rate (which is typically around 60 to 75 Hz), even if your physics frame rate is higher than that e.g. due to accelerated game time. Any effort spent on drawing more than one frame per screen refresh is completely wasted, since the player is only going to see one of those frames anyway. 

where x and y are the coordinate values, scaled to lie between 0 and 1. This function takes the value 0 at the center of the map (at x = y = 0.5) and tends to infinity at the edges. Thus, subtracting it (scaled by a suitable constant factor) from your height map ensures that the height values will also tend to minus infinity near the map edges. The just pick any arbitrary height you want and call it the sea level. As ollipekka notes, this approach will not guarantee that the island will be contiguous. However, scaling the bias function by a fairly small scale factor should make it mostly flat in the middle area of the map (thus not affecting your terrain much), with a significant bias appearing only near the edges. Thus, doing so should give you a squarish, mostly contiguous island with, at most, possible a few tiny sub-islands near the edges. Of course, if you don't mind the possibility of disconnected terrain, a somewhat larger scaling factor should give you more water and a more natural-looking island shape. Adjusting the sea level and/or the scale of your original heightmap can also be used to vary the size and shape of the resulting island(s). 

I'm not sure what your actual problem is (I first thought it was that you wanted to bias the initial midpoint range, but it looks like you actually don't), but here's a simple midpoint displacement implementation in Python: 

The "color of the object", i.e. the RGB value of the texture, formally represents its reflectance spectrum, discretized by averaging it over the response curves of the three color receptors in the human eye. Thus, to get the apparent color of a point on an object's surface, you simply sum together the colors of the lights that fall on it and multiply the result with the color of the surface, doing this separately for all the three color channels. Of course, this doesn't account for the fact that real lights and real surfaces may have complicated spectra that are not fully represented by a simple RGB triplet, but in practice it works pretty well — the spectral details that are not well represented by the RGB approximation tend to also be the ones that the human eye can't distinguish.