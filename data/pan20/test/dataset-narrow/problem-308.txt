Now if this snippet runs concurrently by 2 transactions, T1 and T2 what will happen? I know that in MySQL by default the isolation level is . So let's say that amount initially is 500. So when the 2 transactions finish will the final amount be 300 or 100? I mean when they start both read from table which I guess is a shared lock. When one updates the table it gets an exclusive lock right? So what happens to the other transaction? Will it proceed with the amount it "view" when it started i.e. 500? 

I have a table with a column that is of type . I am doing the following filter which can not use and index: 

Can someone please explain why the delete shows a different number of rows than the count? I am using the same join. Are they not equivalent? 

There are two f3 values maximum per each f1,f2 combination. For this specific example, I want to get top-2 minimum values per f1 and range (f2 - max(f2) per f1). Example output: 

also has the same values for dbFields A and B. Is there any efficient way to actually retrieve all records that have the same value as the top-k records? For example, when I search for the first 100 records to get instead 102 records if the last two have the same values as the 100th record? Is there any index to accelerate such queries? I do not mind if it has to be done with pl/pgsql (and not plain SQL) if the implementation is efficient. 

I will have a huge PostgreSQL 9.3 database with many tables with more than 100M entries per table. This database will be basically be read-only (once I fill all the necessary tables and build the indexes no more write operations on the DB) and single-user access (run and benchmark multiple queries from localhost), since the DB will be used only for research purposes. Queries will always use JOIN on integer DB fields. I will probably buy a SSD (256-512GB) for this purpose. I have not used an SSD for a DB before, so is there anything I should be afraid of? Can I put the entire DB on the SSD, or just the indexes? Is there any particular advice / tutorial required for tuning PostgreSQL for SSDs? Note, that I have a good workstation with an i7 and 32Gb of RAM, so perhaps you can offer some advice there too. 

What happens to a transaction when the client connection is lost. Example the client is a web application code that starts a transaction to a remote server instance. The client sends the sql code to the server and waits for the transaction to complete. What happens if the network connection fails? Does it depend on the point we are in a transaction? Is it irrelevant? Is it aborted? 

But this seems useless to me. I mean if I set serialized to a client connection then I would want all currently running transactions to be sequential, right? Otherwise what's the point? 

Let's say I have an entity which has a location as part of its attributes Now a can have N of other that are close by. So seems to be a self referencing relationship 1-N (optional) If we know before-hand which exact are close by each other what would be the best way to represent that? Since we have a self-referencing 1-N relationship I think another table would be needed In this case we would store e.g. etc to show that pizza store with id 1 has 22, 23 and 78 near by etc Now in order to get these rows back in-order I would need to create a PK and query based on that. I was wondering would an auto-increment guarantee the insertion order? Or would I need to use a float representing the distance e.g. (where 2.04 is the distance in miles) I was also thinking is there a better way than this? We know that if is close to then is also close to right? Is there a more efficient way to represent this information? I think it would suffice to store just the row to capture that is close to and that is close to . But this way we are losing the order Update: All the answers are very useful. One thing though that perhaps is not clear. I already have the order of the entities based on the distance on insert time. I.e. I have a set of ids of objects already sorted based on distance and I want to insert this set in the database in a way that I can retrieve the rows back in the order I have inserted them. I want to avoid sorting on distance on retrieve since I already had the order at insert time. These ids would be used to join to another table (they are ids) and I want to retrieve the rows in the insert order 

For that purpose you can download the pre-built virtual Machines (VMs) provided by Oracle. They only need Oracle Virtual Box in order to check them out ($URL$ You can download them here: $URL$ This is the best solution for learning Oracle DB, without actually messing up your system. 

I believe that PostgreSQL can log slow or unsuccessful queries. Probably I can set also PostgreSQL to log all queries executed. Contrarily, I am interested to know if there is a way that a malicious attacker can get access to all the queries successfully executed on the PostgreSQL server, if I have disabled logging as much as possible. Is this possible? Or once a query has been successfully executed (might be a SELECT or UPDATE query) I can be 100% sure that the DB server has no memory of successfully executed queries and therefore no one else can get access to this information. I am using PostgreSQL 9.3. 

OR do I need any transactions or locks for the DELETE vs INSERT/SELECT? It seems not but I was thinking I may be misunderstanding something Note: My question is not about transactions and their benefits.My question is if 2 statements from different processes on the same table SELECT/DELETE need for any reason to be synchronized 

I know that transactions are meant to group several operations as one. But if for example in the same table one transaction does 

These block each other but not always! I can not understand this. These refer to different rows why do they block each other? 

I have noticed that even on huge tables with millions of records when I do a the index is created immediately (I get the console back in nsecs). Why is this? Doesn't it create the B-Tree with the table data at that point? I can only assume not due to the immediate return of the . But then how is the index created? On each access? 

The public schema is supposed to be visible by all users. You should not restrict rights of public schema to just one group. So, if you do not use: 

If the query has not got any specific ordering, it returns the first 10 rows it receives. But if there is a "" or "" clause, it must first get the full resultset and then fetch the first 10 rows. If a simple "" with no "" or "" clause takes that long, I would probably suggest to ANALYZE the table $URL$ Also you should check if this DB table has a primary key. If not, setting one would probably speed up the query. 

Please be more specific: "It will contain a lot of rows". How many? Millions, thousands or billions. "Is it a good idea?" It depends. If your queries are like the ones you mention, you should create two b-tree indexes, one for each field. Instructions are here: $URL$ You should only create one index for both fields, ONLY IF all your queries are like: 

When using triggers, if an update is done to a table then a trigger is executed. This is very convenient. But what I would need is to execute an external script. Is it possible to configure MySQL somehow so on a trigger/change of a value in a table an external process/scrip is executed? 

Databases use the file system to store the data. As far as I know it is not possible to delete a record from a random access file. So does that mean that when we do a the size of the table i.e. the file that stores the table never decreases? So databases essentially keep growing and never reduce in size? 

I have a rather basic question on transactions. Assume a table with a column amount. Now we have the following code: 

When having a table in SQLite that has a data type, is there any implication if the length of the contained string differ significantly? 

I want to generate 10 sets of such integers. Each set must have an integer identifier, something like that: 

First of all, for 2000 records (as others have stated) everything will work. So, for the OP the natural key will still work. On the other hand VARCHAR fields for primary keys are (in most of the cases) a bad idea. They are inefficient, hard to index and provide slow performance. In most of the cases a numeric field ( int / bigint) will probably work better. So, in the argument natural vs surrogate, the correct answer is: It depends. Stick with the natural key if it fits your and your app purposes. If not, use a surrogate. 

To create the field RNG is not required. I only added it to show that for , there are 3 ranges: created by the distinct values of for . For each such range I want to calculate the top-k minimum values per f1 and range. SQL Fiddle here: $URL$ Building the Ranges may be done by: