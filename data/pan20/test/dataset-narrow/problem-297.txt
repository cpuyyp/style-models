I think if you look at your model and you consider the requirements that it doesn't handle (i.e. your questions) then you will find that you need to expand your model somewhat. Consider the following ERD: (Note I use the James Martin crows foot notation which is a little more compact than what you have used but should be pretty simple to understand. The only slightly distinct feature is the use of an upper case "I" to indicate that a relationship is part of the identifier of the entity) 

You should be able to make in the above table your primary key if you want. Since the value is already unique (by virtue of being IDENTITY) I would question why you want to make a compound primary key. Are you sure that this is the design you want? 

You need to add a location to maintain your customer email preferences. The model for that depends on what the rules are for recording the preferences. Your question isn't explicit enough on this point. B. You might also want to include an table with a foreign key from to this new table. That will allow you to record which template was used for each email. C. You aren't clear about what your m:n relationship is recording. Are these columns: , , foreign keys? If so, are they single or multivalued? (i.e. can you have multiple email addresses in any or all of these fields?) If they are single valued foreign keys, then you can do away with the many-to-many and make them each a many-to-one from to (so you will have three 1:m relationships) If they are multi-valued foreign keys, then you need to create an explicit intersection table instead of using a simple m:n relationship and you will want to have an column ("TO", "CC", "BCC") as part of the composite key. 

The physical model is what is created in most physical implementations of a many-to-many relationship. However, it does not correctly replicate the rules in the logical model. The physical model allows for zero or more in both directions. The logical model indicates one or more from project to employee. The reason that many physical implementations of any relationships do this is that in many DBMSs the SQL declarative referential constraints (i.e. foreign key constraints) don't allow for the rule to be suspended while data is inserted into two tables at one time. The SQL standard provides for "deferrable constraints" and some DBMSs implement these, but many don't and the ones that do haven't always, so not everyone uses them, even if they are available. Old habits can die hard. For the ones that don't implement deferrable constraints, think about it this way: For there to be any kind of mandatory relationship in both directions (i.e. 1:1,M or even 1:1) then you would have to insert the parent and the child at the same time. While you can create a transaction that ensures the parent and child are created in one logical unit of work, the referential integrity rules aren't built to "suspend judgement" until the whole transaction finishes. This is why in practice, physical implementations without deferrable constraints are essentially always optional at the child end. This lets you insert the parent and then insert the child. The practical way around this is generally to write additional application logic to ensure that there is at least one child per parent when 1:1,M is the rule or to just leave it and hope that users do the right thing. These work-arounds aren't necessary if you do happen to have access to deferrable constraints. 

It depends on the situation. If normalization leads you to split your relations into multiple tables, and if you build those tables with primary keys and foreign keys which are defined to the database so that they are properly indexed, then in that sense query performance is enhanced. Similarly, if your table is not in 1NF you might have to do expensive table scans and text parsing to find examples of a particular data item in a multivalued column. In 3NF this column could be indexed making the query more efficient. It's worth noting that normalizing your schema may also make your queries simpler and easier to maintain for the same reasons and in the same situations. On the other hand, when data has become static (e.g. historical data) it is often denormalized to make retrieval faster. This frequently takes the form of precalculating values, especially aggregate values. This is the approach of data warehousing. It is a good practice, as long as the risk of data insert/update/delete anomalies is very, very low or easy to manage. 

It isn't necessary to have a surrogate primary key on your clubs_chains table. The combination of the two foreign keys in clubs_chains is adequate for the primary key. You can use a foreign key constraint to ensure that your clubs_chains_paymethod table references an existing record in clubs_chains using the compound primary key. This might be helpful since it would allow more direct joins between clubs_chains_paymethod and your clubs and chains tables, without sacrificing any referential integrity. On the other hand, some people just love using a surrogate primary key, even on intersection tables. If that is your data modelling style, then it's OK too. 

There was a school of thought that said you should try to make user credentials hard to find in your database to make it harder for hackers to get at this information. I think that this is ultimately wrong-minded, because once a hacker gets into your database it's not going to be much of a problem for them to unload your data and figure out what it is, regardless of how many tables you've got and how much trouble you've gone to spreading columns around. The right place to thwart hackers is outside your firewall. You should keep your database schema as simple as possible to make it efficient and maintainable and take all of the necessary precautions to keep hackers from getting any access to your database. One other thing you should do by way of keeping sensitive data safe is to encrypt sensitive data. Don't keep things like passwords or credit card numbers in plain text. Use strong encryption for these kinds of data elements. 

NoSQL and RDBMS are totally different animals. You should pick the one which suits your data and how you intend to record and access it. Don't pick your style of database system on the grounds of performance. Performance can be managed in a range. If HBase isn't performant at your start up volumes, then maybe you'll just have to work harder to tune it, or throw more money at hardware, or just live with slightly lower performance until your volumes catch up to HBase's sweet spot. 

Depending on how many parking lots you're operating, I wouldn't be too worried about how many records you have to manage. It's not like this data would be fast changing, since you'd probably have to get signs printed up with the prices marked and so on. Still, assuming that you have many lots (many of which have the same pricing) and that you want to minimize data maintenance, you could build a price schedule and map lots to the schedule. Consider something like this: 

If this is the case, your concern is that recording the "default" (duplicate) comment in the second table is wasteful or even dangerous since the data could get out of whack. In this situation, you can use the SQL function along with a to solve your problem. Using the outer join lets you use a single SQL statement that pulls together the two tables (when there are records in each) or just pull data from the first (mandatory) table if the second (optional) record is missing. No complicated branching, just a single SQL select. The coalesce allows you to pick the first non-NULL value in a list of values. This is useful because it lets you take the first comment as a default if the second comment is NULL. The second comment can be NULL because either (a) there is no matching record in the second table or (b) the comment within the second table is NULL. It seems like in your case there will always be an entry. Sometimes this entry is created by a user and sometimes it is the result of an unsatisfactory . In this case you want to select: 

Keep you list of people in one table. Then keep another table with the list of things that various people might need. Normalizing people and needed things is important for being able to write simple, reliable queries about your data from the perspective of either an individual person or a particular needed thing. Use a third table to keep track of who needs what, and how badly they need it. 

If your hierarchy is rigid (i.e. the levels are consistent and mandatory) then there is nothing wrong with representing each level as it's own table. This is your first option, except that instead of having foreign keys on the person table to each of the organization levels, you should have a single foreign key on the person table to the leaf level of your organization, and then each level in turn refers to it's parent/container. i.e.: => => => => => With this modification, your tables will be in third normal form (3NF) which is always a good starting point from a design perspective. If, on the other hand, your hierarchy might be at all fluid, then keeping a hierarchy in a single table with a self-referencing (involuted) foreign key is a better option from a code stability perspective. This is your second option. One issue with this second option is that relational databases are not brilliant at handling unleveled trees in this form. There are, however, a couple of pretty useful techniques for managing this type of data in this format a little more easily. You can Google around for adjacency lists and visitation numbers to learn more. 

This gives you one table which will perform well enough because of the covering index which includes both and and it lets you maintain active and historical chatroom memberships in one place, so it's simple and efficient. Entering a chat involves just one insert. Leaving a chat involves just one update. 

There is nothing wrong with having as many entity sub-types in your model as are needed to reflect the reality of the data that you're trying to model. The question isn't whether sub-types are a bad practice. The issue may be is it a good model? For example, under your example, what do you do with something like an Audi A4 eTron - which is a plug-in hybrid? Is that an "electric car" or is it a "hybrid car"? The other question you have to ask yourself is why you're sub-typing at all? How many distinct predicates do you have in your sub-types? Are any of these predicates shared between sub-types? The situation could get complicated. Sub-typing isn't used in database design for classification. You can do classification with codes, foreign keys to code tables, or with flags. Sub-typing is used to model distinct predicate sets for different types of a thing of interest. If you're using sub-types merely for classification then that's a bad practice. If your sub-types clearly and unambiguously model different predicate sets for the things your database cares about, then it's a perfectly good practice, regardless of how many sub-types you need. 

What you are proposing is an entity subtyping approach. This would be one common solution to your design problem. Another would be Entity-Attribute-Value (EAV). Type "subtype" or "EAV" into the search box in the top right hand corner of the page and you'll see many questions describing and discussing the relative pitfalls and merits of each. Whether subtyping (or, alternatively EAV) is a "really bad design" in your case depends on exactly how many different unique features and manufacturers you need to account for and perhaps where you stand philosophically (some people insist that EAV is always evil-for example). To answer your specific question about enforcing a match between brands and their unique attributes, the only way to do this is with procedural code. Depending on your DBMS you might be able to do this with triggers. There isn't a way to to it declaratively. 

If it fits within the rules of normalization, then 1:1 relationships can be normalized (by definition!) - In other words, there is nothing about 1:1 relationships that make it impossible for them to obey the normal forms. To answer your question about the practicality of 1:1 relationships, there are times when this is a perfectly useful construct, such as when you have subtypes with distinct predicates (columns). The reasons you would use 1:1 relationships depend on your point of view. DBAs tend to think of everything as being a performance decision. Data modelers and programmers tend to think of these decisions as being design or model oriented. In fact, there is a lot of overlap between these points of view. It depends on what your perspectives and priorities are. Here are some examples of motivations for 1:1 relationships: 

It is acceptable to have mutually exclusive attributes in a table. If you only have one pair of such attributes, this may be the most practical solution. However, some people may look at your situation as a sub-typing issue. In this view, you are missing the superset entity. You have a collection of (something), some of which are employees and some of which are customers and some of which are also users. Have you also considered what happens if an employee also happens to be a customer? Is that important to your system or is that something that would be considered coincidental? If you think its important to treat all users as a feature of one type of thing, then you need to implement the (something) which is currently missing from your table definitions. Consider the following ERD: 

In a practical system (as opposed to a textbook problem) you would want to have some kind of transaction identifier on any transaction like hiring one or more pieces of equipment. Assuming you are running an equipment hiring business, your transactions would probably have to account for letting out multiple pieces of equipment at once. Therefore you need to add an order table as follows: 

What would be more conventional is to have an AUTHOR table that lists the names of each author once, along with any other information you may eventually need, and then have your intersection table reference AUTHOR and then either BOOK or JOURNAL. A lot of people would have a different intersection table for each type of writing, or at least create a supertype above BOOK and JOURNAL that encompasses both. Your single intersection table could then reference the supertype. Most people would not have a dual-purpose intersection table, but I've seen it done. What doesn't make sense is your first example row, where A.R Smith authors one book and one journal in the same record. Each record in the intersection table should denote a single relationship otherwise you are exposing yourself to deletion anomalies. 

EDIT: Based on OP's clarification of requirements: What you need is not like a typical questionnaire model in the following ways: