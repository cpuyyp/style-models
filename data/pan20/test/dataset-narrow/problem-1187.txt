Parity is a regular language not computable in NLOGTIME, or even in nonuniform AC^0. The set of all strings whose length is a power of 2 is a non-regular language computable in DLOGTIME. 

If there were no solution of the search problem, the $2^m$ random choices would give $1$ and $2$ $(2^m-1)/3$ times, and $0$ $(2^m+2)/3$ times (one more). However, if $y$ solves the search problem, we tinkered with the elements $y,y+1,y+2$ (which hit all three residue classes) so that they only produce residues $1$ and $2$, which evens out the advantage of $0$. (I am assuming here wlog that $y<2^m-2$.) PPA-$3$ search problems A convenient way to define PPA-$3$ is as NP search problems many-one reducible to the following kind of problems. We have a fixed polynomial-time function $f(x,y)$, and a polynomial $p(n)$, such that for any input $x$ of length $n$, the induced mapping $f_x(y)=f(x,y)$ restricted to inputs $y$ of length $m=p(n)$ is a function $f_x\colon[0,2^m)\to[0,2^m)$ satisfying $f_x(f_x(f_x(y)))=y$ for every $y$. The task is, given $x$, find a fixpoint $y$ of $f_x$: $f_x(y)=y$. We can solve this in the way in the question as follows: given $x$ of length $n$, we generate a random $y$ of length $m=p(n)$, and output 

Most nonuniform complexity classes—$\mathrm{NC^1}$ included—are closed under the $\mathrm{BP}$ operator by the same argument as $\mathrm{BPP\subseteq P/poly}$: using the Chernoff–Hoeffding bound, the probability of error can be reduced below $2^{-n}$ by running $O(n)$ instances of the circuit with independent random bits in parallel, and taking a majority vote; then by the union bound, a sequence of random bits gives the correct result for all $2^n$ inputs of length $n$ simultaneously with nonzero probability, and in particular, there exists such a sequence. We can hardwire it into the circuit. This argument applies to any class of circuits that is closed under taking majority of $O(n)$ parallel copies of a circuit, and fixing input gates to constants. In practice, this means any decent nonuniform class above $\mathrm{TC^0}$, as majority is computable in $\mathrm{TC^0}$. The proof is more complicated for $\mathrm{AC^0}$, because this class does not compute the majority function. (While I haven’t seen the Ajtai and Ben-Or paper, I’d guess they use some sort of approximate majority.) 

$\mathrm{CSL}=\mathrm{NSPACE}(O(n))$. Thus, take your favourite PSPACE-complete problem. If it is decidable in $\mathrm{NSPACE}(O(n))$ (for example, QBF is), you are done. Otherwise, introduce a polynomial amount of padding to make it so. 

I’m not sure this is a research-level question, however solving linear systems over $\mathbb F_p$ is a $\mathrm{Mod}_p\mathrm L$-complete problem, hence in particular it is in $\mathrm{NC}^2$. More generally, linear algebra over $\mathbb F_{p^k}$ (at least for $k$ fixed) can be reduced to the $\mathbb F_p$ case. 

$\let\mr\mathrm\def\xx{\mr{FP^{NP[wit,\log]}}}\xx$ is not defined as a class of functions, but of potentially multivalued search problems. It is not difficult to construct $\xx$-complete search problems by variating the usual Cook–Levin theorem. Here is one, somewhat simplified from what I suggested in the comment above. First, as should be mentioned somewhere in the BKT paper, the class $\xx$ does not change if all the oracle queries are made to ordinary, nonwitnessing NP oracles, except for the final query, which is guaranteed to succeed, and returns a witness $(u,v)$ such that $u$ is the output of the algorithm. Moreover, since $\mr{P^{NP[\log]}}=\mr{P^{\|NP}}$, we can simulate the initial $O(\log n)$ nonwitnessing queries with polynomially many nonadaptive (parallel) NP queries. Now, if $A(x)$ is a polynomial-time algorithm making $n^c$ nonadaptive queries to an NP-oracle, and one witnessing query in the fashion described above, then $\qquad B(x,1^j)\iff{}$ “the $j$th parallel query made by $A$ on input $x$ is true” is an NP-predicate, hence by the Cook–Levin theorem, it is equivalent to the satisfiability of a poly-time (or log-space) constructible 3-CNF. Likewise, $\qquad C(x,a_1,\dots,a_{n^c})\iff{}$ “the final query is true in the run of $A$ on input $x$ such that the $j$th parallel query is answered with $a_j\in\{0,1\}$ for each $j$” is an NP-predicate, and the output of $A(x)$ can be read off from a witness to $C(x,\vec a)$ if supplied with the correct $\vec a$. Again, it can be expressed as a 3-CNF. Thus, the following search problem is $\xx$-complete: 

The best known lower bound is the $\Omega(M(n))$ mentioned above. The best known upper bound on $M(n)$ is $n\log n\,2^{O(\log^*n)}$ by Fürer’s algorithm. Figuring out the state of art for algebraic number approximation (that is, root finding) is rather harder, as it is still an area of active research, and the formulations of bounds in the literature are often not as clear as one would want them. Let $\tilde O(f(n))$ denote $O(f(n)\mathrm{polylog}(f(n)))$. For real root approximation, Pan and Tsigaridas give: 

Clearly, $\mathrm{RPH}\subseteq\mathrm{BPP}$. On the other hand, $\mathrm{BPP}=\mathrm{ZPP^{promiseRP}}$ (Buhrman&Fortnow, pdf), so the only way the hierarchy didn’t collapse to (at most) the second level and didn’t exhaust $\mathrm{BPP}$ would be for the unlikely reason that $\mathrm{RP}$ oracles were significantly weaker than $\mathrm{promiseRP}$ oracles. 

Note that proving that evaluation of $f$ takes at least as much time as integer multiplication only shows that it needs time $\Omega(M(n))$; while this is generally assumed to be at least $n\log n$, no superlinear bound has been proven. (However, I disagree that this wouldn’t yield anything nontrivial, as was formerly stated in Joshua Grochow’s answer. For one thing, there are plenty of analytic functions such as exp, log, sin where this $\Omega(M(n))$ bound is close to optimal up to one more logarithmic factor. For another thing, we normally regard a proof of NP-completeness of some problem as a strong evidence that it is hard, despite that with the current state of knowledge it doesn’t imply any superlinear lower bound either.) As pointed out by Joshua Grochow, you cannot say anything useful about the complexity of evaluation of analytic functions at integers. The correct statement you are looking for is that approximation of smooth functions on short real intervals is at least as hard as integer multiplication. This indeed follows from Taylor expansion, though in full generality there are some messy details where the devil is (in particular, we a priori know nothing about the coefficients of the Taylor series). As usual in this context, it is convenient to restrict attention to time bounds obeying some basic regularity conditions. Let me say that $T(n)$ is a reasonable bound if $T$ is nondecreasing, and $$c_1T(n)\le T(2n)\le c_2T(n)$$ for some constants $1<c_1\le c_2$. Commonly encountered bounds of polynomial growth involving powers of $n$, $\log n$, and similar, are all reasonable. In order to avoid issues with representation of real numbers, we will only considered real functions on bounded intervals. An $n$bit approximation of $x$ is then a dyadic rational $x'$ written in the usual binary notation with $n$ bits after the radix point, such that $|x-x'|\le2^{-n}$. A function $f\colon[a,b]\to\mathbb R$ can be computed in time $T(n)$, if given a dyadic rational $x\in[a,b]$ with $n$ bits after the radix point, we can compute in time $T(n)$ an $n$bit approximation of $f(x)$. 

The Löwenheim–Skolem property as above. (Completeness) The set of valid sentences can be described by an effectively axiomatized proof system (i.e., it is recursively enumerable). 

$\def\ac{\mathrm{AC}^0}$Yes, $\ac\mathrm{PAD}=\mathrm{PPAD}$. (Here and below, I’m assuming $\ac$ is defined as a uniform class. Of course, with nonuniform $\ac$ we’d just get $\mathrm{PPAD/poly}$.) The basic idea is quite simple: $\ac$ can do one step of a Turing machine computation, hence we can simulate one polynomial-time computable edge by a polynomially long line of $\ac$-computable edges. By a further extension of the idea, one could simulate edges computable in poly time with a PPAD oracle, that is, PPAD is closed under Turing reducibility; this argument is given in Buss and Johnson. There are many equivalent definition of PPAD in the literature that differ in various details, hence let me fix one here for definiteness. An NP search problem $S$ is in PPAD if there is a polynomial $p(n)$, and polynomial-time functions $f(x,u)$, $g(x,u)$, and $h(x,u)$ with the following properties. For each input $x$ of length $n$, $f$ and $g$ represent a directed graph $G_x=(V_x,E_x)$ without self-loops where $V_x=\{0,1\}^{p(n)}$, and every node has in-degree and out-degree at most $1$. The representation is such that if $(u,v)\in E_x$, then $f(x,u)=v$ and $g(x,v)=u$; if $u$ has out-degree $0$, $f(x,u)=u$; and if $u$ has in-degree $0$, $g(x,u)=u$. The node $0^{p(n)}\in V_x$ is a source (i.e., it has in-degree $0$ and out-degree $1$). If $u\in V_x$ is any source or sink (in-degree $1$, out-degree $0$) other than $0^{p(n)}$, then $h(x,u)$ is a solution to $S(x)$. We can define $\ac\mathrm{PAD}$ similarly, except we require $f,g,h$ to be in $\mathrm{FAC}^0$. I will ignore $h$ in the construction for simplicity. (It is not hard to show that one can take it to be a projection, hence $\ac$-computable.) So, consider a PPAD problem $S$ defined by $f$ and $g$, and fix Turing machines computing $f$ and $g$ in time $q(n)$. For any $x$, we define a directed graph $G'_x=(V'_x,E'_x)$ whose vertices are sequences of the following form: 

Let me show for the record the equivalence of the problem above with Papadimitriou’s complete problem for PPA-$3$, as this class is mostly neglected in the literature. The problem is mentioned in Buss, Johnson: “Propositional proofs and reductions between NP search problems”, but they do not state the equivalence. For PPA, a similar problem (LONELY) is given in Beame, Cook, Edmonds, Impagliazzo, and Pitassi: “The relative complexity of NP search problems”. There is nothing special about $3$, the argument below works mutatis mutandis for any odd prime. 

Contrary to what is stated in the question, abelian group isomorphism is not known to be in $\mathrm{TC^0}$. Needless to say, this also means it is not known to be in $\mathrm{AC^0}$. What is known is the following observation from [1]. Let $\mathrm{pow}$ denote the following problem: given a multiplication table of an abelian group $(A,{\cdot})$, elements $a,b\in A$, and $m$ in unary, determine if $b=a^m$. The structure theorem for finite abelian groups easily implies that if $A,B$ are two such groups of size $n$, then $$\tag{$*$}A\simeq B\iff\forall m\le n\:\bigl|\{a\in A:a^m=1\}\bigr|=\bigl|\{b\in B:b^m=1\}\bigr|.$$ Since we can count polynomial-size sets in $\mathrm{TC^0}$, we obtain 

(If the $b_i$ parameters are not easy to compute, we might also store them.) This arrangement uses space $$n\sum_{i=1}^k\frac{\log b_{i-1}}{b_i},$$ and query time $$O(k+b_k).$$ In the proposition, for 1., we let $k$ be constant, and put $b_i=\log^{(i)}n$ for $0<i<k$, $b_k=1$. For 2., we can take $k=\log^*n$, and $$b_i=i(\log i)^3\log^{(i)}n.$$ The space is then bounded by roughly $$n\sum_{i=1}^k\frac{\log^{(i)}n+2\log i}{i(\log i)^3\log^{(i)}n}\le n\sum_{i=1}^\infty\frac{3}{i(\log i)^2}=cn$$ for some constant $c$. 

It’s not exactly clear to me what is the input of the problem and how do you enforce the restriction $p=2^{\Omega(n)}$, however, under any reasonable formulation the answer is no for multivariate polynomials unless NP = RP, due to the reduction below. Given a prime power $q$ in binary and a Boolean circuit $C$ (wlog using only $\land$ and $\neg$ gates), we can construct in polynomial time an arithmetic circuit $C_q$ such that $C$ is unsatisfiable iff $C_q$ computes an identically zero polynomial over $\mathbb F_q$ as follows: translate $a\land b$ with $ab$, $\neg a$ with $1-a$, and a variable $x_i$ with $x_i^{q-1}$ (which can be expressed by a circuit of size $O(\log q)$ using repeated squaring). If $q=p$ is prime (which I don’t think actually matters) and sufficiently large, we can even make the reduction univariate: modify the definition of $C_p$ so that $x_i$ is translated with the polynomial $$f_i(x)=((x+i)^{(p-1)/2}+1)^{p-1}.$$ On the one hand, $f_i(a)\in\{0,1\}$ for every $a\in\mathbb F_p$, hence if $C$ is unsatisfiable, then $C_p(a)=0$ for every $a$. On the other hand, assume that $C$ is satisfiable, say $C(b_1,\dots,b_n)=1$, where $b_i\in\{0,1\}$. Notice that $$f_i(a)=\begin{cases}1&\text{if $a+i$ is a quadratic residue (including $0$),}\\ 0&\text{if $a+i$ is a quadratic nonresidue.}\end{cases}$$ Thus, we have $C_p(a)=1$ if $a\in\mathbb F_p$ is such that $$a+i\text{ is a quadratic residue }\iff b_i=1$$ for every $i=1,\dots,n$. Corollary 5 in Peralta implies that such $a$ always exists for $p\ge(1+o(1))2^{2n}n^2$. 

(where $x$ is the given input of length $n$) on an alternating TM with space $O(f(n)\log n)$ and $O(1)$ alternations, hence in $\mathrm{NSPACE}(O(f(n)\log n))$ by the Immerman–Szelepcsényi theorem. (The $\log n$ factor comes from the fact that a description of a circuit of size $f(n)$ takes $O(f(n)\log f(n))$ bits; we may assume $f(n)\le n^2$, thus $\log f(n)=O(\log n)$.) 

Concerning the last question: the time hierarchy theorem immediately implies that QP has no complete problems under polynomial-time many-one or Turing reductions. (On the other hand, every problem save $\varnothing$ and $\Sigma^*$ is QP-hard under quasi-polynomial reductions.) Thus, assuming Babai’s result is correct, GI is not QP-hard. 

It is clear from the definitions that this gives a uniform distribution on $\{0,1,2\}$, as the non-fixpoint $y$'s come in triples. 

Let $a$ be the input number, of length $n$. By repeated squaring, compute the sequence $b$, $b^2$, $b^4$, $b^8$, ..., $b^{2^k}$, where $k\le\log n$ is such that $a<b^{2^k}$. Using division with remainder, compute $a_0,a_1<b^{2^{k-1}}$ such that $a=a_1b^{2^{k-1}}+a_0$. Recurse with $a_0$ and $a_1$: That is, in the second stage, compute $a_{00},a_{01},a_{10},a_{11}<b^{2^{k-2}}$ such that $a_0=a_{01}b^{2^{k-2}}+a_{00}$ and $a_1=a_{11}b^{2^{k-2}}+a_{10}$. In the third stage, split each of the 4 numbers in two using division by $b^{2^{k-3}}$, and so on. After the $k$-th stage, we will have a sequence of numbers $<b$, which is the base-$b$ expansion of $a$. ($a$ is its own expansion in base $b^{2^k}$. After the first stage, $a_1a_0$ is the expansion of $a$ in base $b^{2^{k-1}}$. After the second stage, we have the expansion of $a$ in base $b^{2^{k-2}}$, etc.) The running time is $$O(M(n)+2M(n/2)+4M(n/4)+\dots)\le O(M(n)\log n).$$ 

In the same paper, they mention $\tilde O(d^3+d^2m)$ as the best known bound for complex root approximation. In a slightly more recent paper ($URL$ they seem to claim the same bounds for the complex case as for the real case, under the assumption that the given isolation disk has isolation ratio (the distance of the disk’s center to the nearest other root of $f$ divided by the radius of the disk) at least $1+1/\log d$ or so, but it’s written in a rather messy way and I may misinterpret it. 

It is not known if $\mathrm{pow}$ is computable in $\mathrm{TC^0}$. It seems that Corollary 2 is the best known result when it comes to the usual, “polynomial-size”, circuit classes. However, I observe that the problem is in the quasipolynomial version of $\mathrm{AC^0}$: 

Here, approximation of $z\bmod2\pi$ is defined so that when $z$ is very close to an integer multiple of $2\pi$, numbers close to $0$ and close to $2\pi$ are both considered valid outputs. Proof: For $1\le2$, use the identity $\cos z=1-2\sin^2(z/2)$. For $1\le3$, $\sin$ and $\cos$ are $2\pi$-periodic, and computable on $[0,2\pi]$ in $\tc$. For $3\le1$, we can compute $z\bmod2\pi$ from $u=\cos z$ and $v=\sin z$ in $\tc$ using $\arctan(v/u)$ and a little fiddling with quadrants. For $3\le4$, compute $2^y\bmod2\pi$ to $2\len(x)+O(1)$ bits of accuracy, and use $x2^y\bmod2\pi=x(2^y\bmod2\pi)\bmod2\pi$, which holds as $x$ is an integer. QED Curiously, the argument above gives a reduction of $\cos(x2^y)$ to $\sin(x2^y)$—or actually to $|\sin(x2^y)|$—and conversely we can reduce $|\sin(x2^y)|$ to $\cos(x2^y)$ using the identity $\sin z=\pm\sqrt{1-\cos^2z}$, but I don’t know how to reduce $\sin(x2^y)$ itself to $\cos(x2^y)$. Third, exponentially scaled uniform $\tc$ circuits can be evaluated with $O(1)$ alternations of existential and universal nondeterminism, and counting. That is, 

FWIW, the ostensibly semantic class APP defined in [1] was shown to be syntactic in [2]. [1] Valentine Kabanets, Charles Rackoff, Stephen A. Cook, Efficiently approximable real-valued functions, ECCC Report TR00-034, 2000. [2] Emil Jeřábek, Approximate counting in bounded arithmetic, Journal of Symbolic Logic 72 (2007), no. 3, pp. 959–993. doi, preprint 

Proof sketch: We have $\re e^\alpha=e^x\cos y$. We can approximate $e^x$, and floating point numbers are easily multiplied, the problem is to approximate $\cos y$ with relative error $2^{-m}$. We can compute the integer $k$ nearest to $2y/\pi$, whence $\cos y$ is $\pm\cos y'$ or $\pm\sin y'$, where $y'=y-k\frac\pi2$ satisfies $|y'|\le\pi/4$. The issue arises when we need to compute $\sin y'$ and $y'$ is very small, since in order to have $\sin y'$ to $m$ bits of relative accuracy, we need $m+\log|{y'}^{-1}|$ bits of absolute accuracy. Assume that $y=u/v$, where $u,v$ are integers given as input. If $y'$ is small, we have a good rational approximation of $\pi$: $$\left|\pi-\frac{2u}{kv}\right|\le\frac{2|y'|}k.$$ Now, $\pi$ is known to have a finite irrationality measure $\nu$ (the current bound is $7{.}6063$ by Salikhov). This implies $$|y'|\ge\frac1{2k^{\nu-1}v^\nu},$$ which gives a linear upper bound on $\log|{y'}^{-1}|$ in terms of the length of the input. We can thus compute $y'$ to the desired accuracy; evaluating $\sin y'$ is then not a problem (if $y'$ is sufficiently small, we can just take $\sin y'\approx y'$). QED I do not know whether the same holds for algebraic $\alpha$. In the argument above, one can get a bound on $\log|y'|$ from Baker’s theorem, however, none of its versions that I’ve seen is good enough to make the required accuracy linear in the size of the input: in particular, the bounds involve a (rather nasty) polynomial in $\deg(f)$. This makes the resulting algorithm polynomial in the size of the input, but with a ridiculous exponent.