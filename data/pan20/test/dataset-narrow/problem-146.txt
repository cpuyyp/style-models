Yes, it would be beneficial if you are angling the AP correctly (second AP would be up and to the right), even with the antennas on the other AP being vertical. If you angle this the wrong way (second AP is up and to the left), then you will be making things worse. This type of antenna produces a signal pattern in a toroid or doughnut shape (with the antenna forming an "axel" through this pattern). Angling the one would increase the energy that arrives at the higher AP and will allow for slightly better reception by the lower AP. However, this may only be true if all you are concerned about was the signal between the two APs. If you are providing service to other devices from the lower AP, this could negatively impact the service to those other devices. If it is only signal between the two APs, then I would question why you are using omnidirectional antennas over directional? Even if the higher AP is serving multiple lower APs, if the lower AP only connects to the higher one, it would be better served using directional antennas. If all the APs are lower than the higher one, then I would look for antennas with "down tilt" for the higher AP if possible or else something like a patch/panel pointed downward. With any wireless, you should make sure to understand the signal patterns of the antennas you choose. 

Your assumptions are flawed. When host 2 receives the ARP request from host 1, in addition to sending the ARP reply it should also update/add the entry for host 1 in it's own ARP table. So your order should be extended to look more like this: 

If you think about this a bit, it makes sense. An IP packet doesn't contain a MAC addresses, so this would be difficult. However, IP traffic is encapsulated in a L2 protocol, typically Ethernet which does utilize MAC addresses. 

The IEEE 802.11 standards define two methods for a client device to discover wireless networks in the area. Both methods are based on using 802.11 management frames as defined in these standards. The first method is a passive method. All 802.11 infrastructure devices will send out a beacon frame approximately every 100ms (this is the default for many devices, but often can be configured either higher or lower) for each wireless network it is provide service for at the time. These beacon frames will contain information about the wireless network such as the SSID (aka network name), encryption details (if any), supported data rates, etc. Wireless clients that are in the area will be able to "hear" the beacon and know the network is in the area, adding it to any The second method is an active method, and is the one most often used by operating systems. The client device can send a probe request frame either generally ("Is anyone out there?") or to a specific network ("Hey, Bill, are you out there?"). An access point (any in the first case, matching ones in the second) will respond to this probe request with a probe response that will contain information similar to a beacon frame. This probe request/probe response process is also part of the association process by which a station joins a wireless network. The reason that the active method is generally preferred by most operating systems is obvious when one sits down to think about it. To discover a network passively, the device will have to listen on a channel long enough to stand a good chance of hearing a beacon frame. Since a beacon is only sent every period of time, a device may have to listen (and only listen) for 200ms or more to make sure they hear all the networks. If you only consider 2.4GHz, that is at least 11 channels...do the math. The active method allows a station to switch to a channel, send a probe request, pause briefly (less than 100ms) and then move to the next channel. This makes this a faster process while still being fairly confident it has found the networks on that channel. 

Ultimately, if you are running into a need to exceed the speed of a link for a single flow, you would need to upgrade your interfaces to the next available speed technology (i.e. 1G to 10G, 10G to 40G, etc). Cisco is also spearheading a push for *multigigabit" providing speeds of 2.5G or 5G across Cat5e/6 cabling at distances up to 100 meters. 

According to the Cisco 100-Megabit Ethernet SFP Modules Compatibility Matrix, the WS-C3750X-24S can utilize GLC-GE-100FX transceivers so it can operate at either speed. This should work fine for your 100Mbps devices provided the assumption I made above is correct. 

I will start by saying that you should not be disabling auto-negotiation with Gigabit connections. This is what the standard has to say (from 802.3-2012 Section Three, which you can reference here): 

Edit: been a while since I worked on a Juniper device, so now having doubts. We may need to see more of the configuration to know exactly which changes are required to allow this. 

However, when you are talking about Cisco more generally, they definitely have a range of operating systems that do not look/feel/act like IOS. Here is a very brief and non-inclusive list of some of them: 

Now, all that being said, that is what should happen. RFC 826 does not specify any sort of check to verify that the Ethernet address of an ARP frame matches that of the SHA. That doesn't mean such a check doesn't exist, it may be added to the driver or implemented on the network to help prevent abuses of ARP that you allude to in asking your questions. Also, I should point out that if you step outside of Ethernet to FDDI, the FDDI source address will not match the SHA. There is some ARP "ugliness" that was necessary when Ethernet was bridged to Token Ring that required the payload of ARP packets to be modified by the bridge due to differences in the bit order used for addressing. When FDDI came along later, while it used the same bit order as Token Ring, for ARP payloads they used the Ethernet ordering of bits to avoid this "ugliness." 

I think your question is not actually about DHCP relay. You are really asking why when your Cisco device is set as a router you don't have access to the Internet from the second subnet. My question would be how does your Actiontec router know about the subnet behind the Cisco device? Does it support a dynamic routing protocol (and do you have it configured correctly on both devices)? Did you configure a static route? 

This is because the DHCP server must reside or have a relay/proxy on the same L2 network as the client. The DHCP OFFER is sent to the L2 address of the client (i.e. it's MAC address). If the request was relayed/proxied, then the DHCP OFFER goes to the relay/proxy which will then forward it to the correct L2 network. Broadcast traffic can be problematic for networks, so this reduces the amount of broadcast network that is necessary. 

Speaking from my experience, most devices indicating they are being powered by "passive PoE" means that the device does not support standards based PoE (802.3af or 802.3at). While standards based devices "negotiate" power needs, a passive device simply expects the power to be automatically provided. You may be required to use the vendor's power injectors and/or switch hardware to provide power to those devices. You may or may not be able to use a standards based PoE power source that can be configured to provide power without negotiation. End devices that are compliant with 802.3af should work with any device that supplies 802.3af or 802.3at power, even if from a different vendor. End devices that are compliant with 802.3at should work with any device that supplies 802.3at and may work with devices that supply 802.3af depending on they need the extra power that 802.3at can provide and/or if they can operate in a lower power mode on 802.3af. If you have a choice, always go with a devices that supports standards based PoE. It gives you more options due to the interoperability between devices of different vendors. 

If you don't have DHCP snooping, the Cisco device is on the same subnet (or supports multip SVIs so you can add an interface on the subnet), you are on a newer IOS (12.2ish or better) with access to TCL, and the device will respond to a ping, then you can use a TCL script. You can find many examples on the internet, one of which can be found here. Once it is able to ping the device (on the same subnet), it should be in the ARP table of the Cisco device. Generally, it would be quicker/easier to check on the L3 device for the ARP entry or the DHCP server then the second option. Old answer (prior to question change): Strictly answering your question, no there is no way to derive an IP address of a device from the MAC address table entry. The MAC address table is strictly speaking a L2 set of information, tying devices to an interface. At L2, there is no awareness of an IP address (as the L3 and above information is irrelevant to L2 and could just as easily be another protocol). You would need access to the L3 device for the remote network segment where you could look up the entry in the ARP table. 

As others have noted, the models are just that, models. So not everything in the real world always fits neatly into the models. ICMP is one such area where people will sometimes debate on which layer (L3, L3.5, or L4) it exists. Despite these debates, ICMP is generally considered a L3 or network protocol because it isn't a transport layer protocol and L3.5 isn't an actual layer. Specifically, ICMP only provides structured data in the IP packet; it doesn't add anything that is considered another layer of headers like a L4 protocl like TCP or UDP do. Additionally, while ICMP does require IP to function, it is implemented as part of the IP protocol and only exists to communicate information about the network layer. For example, ping (Echo requests and Echo replies) only provide information about L3 reachability. They don't provide any information about any higher layer protocols. 

Now to my view point as to why I don't personally hold to the power of two "rule of thumb" when it comes to link aggregation. To start, you really need to understand that no link aggregation ever balances link utilization across the LAG. Clearly there are differences in the load balancing method chosen, and some are better than others (although no single one is best in every circumstance). The load balancing method does not balance the traffic, rather it balances what you can consider "flows." Based on the platform, you have options to use source and/or destination values which may include MAC address, IP addresses, port numbers, etc. These values are then hashed to determine which link is selected for that particular flow. Frames with the same set of values will always receive the same hash and be assigned to the same link. Not all flows are equal and this means that it is not possible to balance the link utilization by these means. Not to take away from the value of LAG, as it often does do a pretty good job of balancing the utilization as well, but you need to understand the limitations. It is entirely possible for one or more of these flows to utilize their entire link in the LAG and starve other flows assigned to the same link. Meanwhile the other links are under utilized. When most people looks at charts like those in this blog post or this Cisco document, they see that the traffic is unbalanced. In the sense that some links receive more flows than others, this is true. My view of these tables is a little different. First, even with the unbalanced distribution, as you add links there is never any point where the percentage of assigned flows on a link increases; rather in the majority of cases the percentage decreases. Put another way, at no time is there less opportunity for a flow to have access to bandwidth, and in many cases, it will have more opportunity. Second, if I were to look at this chart to decide between two or three links in an LAG, in my mind I see this as follows: 

This could be entirely normal depending on the environment (number of users, amount of traffic, etc). However there are definitely issues that can create this problem as well. To start with I would ensure that you have a newer image on the wireless device. Haven't worked on the 880 ISR's myself, but I believe the AP runs a separate image then the router itself (someone with experience, please correct me if I am wrong). You should find details on the Cisco web site. Second, I would make sure that the wireless drivers are upgraded on the client devices as older drivers can be a source of significant performance problems. In Windows, this is unfortunately not always a simple process. With some wireless manufacturers, there are updates through Windows update, some provide driver updates on their website and some you may need to get the drivers from the chipset manufacturers website. From there, try to make sure you are on an uncongested channel. You will want to use channels 1, 6 or 11 (or 1, 7 and 13 if you are in a country that allows the use of channel 13). If you don't have the tools to measure channel utilization, you may have to experiment for the best channel (the AP should be able to provide you with channel utilization on it's current channel). A simple tool, such as inSSIDer will give you an idea of how many other networks will be overlapping with your current channel, however this isn't a complete picture as it doesn't indicate if any of those networks are busy (i.e. causing high channel utilization). Being on a channel with a very busy network will generally be worse than being on a channel with multiple non-busy networks, but often this will get you headed in the right direction. Also keep in mind that even with the right tools, you are only getting a picture of "this moment" and "this specific location" view of things. Beyond that, there are many things you can still do to improve performance, but I don't want to turn this into a novel. For instance, you can look at dropping lower data rates (1, 2, 5.5 to start), disabling 802.11b, turning off protection mechanisms (or turning them on depending on the situation), reducing the number of SSIDs (if you have more than one), using QoS, and many more wireless tweaks. Many of these are dependent on your environment and needs though, and there is nowhere near enough information in the question to know which may be best. 

No, as I said above, it is only necessary when you are sending data from more than one VLAN across a single link. 

You can also use Wireshark or any other packet capture tool if you prefer (display filters in Wireshark are just "cdp" or "lldp"). Additionally, there are software packages that are written to listen for this information, but I am not familiar with them myself (I generally stick to captures). 

The same reason it is not used in UDP or other protocols that lack these features. Less overhead and less complexity. 

This is entirely dependent on your situation and the goals that are most important to the organization.. There are other reasons to use cabinets as opposed to free standing racks. For instance, many cabinets provide for ways to provide 0U power solutions like PDUs that easily mount vertically out of the way. Or cooling, sometimes it is easier to create hot/cold aisle separation using cabinets or you could be planning on implementing cooling directly into the cabinets to increase the cooling efficiency. Some people also find cabinets a better as it helps to prevent inadvertent cable/power disconnects when people are working in neighboring racks. If someone can't be "brushing" up against cables, there is less chance for something to be disconnected. The reasons to use a cabinet besides security are varied. Of course, there are reasons to use free standing racks as well. 

As for benefits, first cables get sloppy and when you have sloppy, problems will occur. I have seen infrastructure cabling go bad for a number of reason in a data center. Need more cables? Then someone is messing with the cable plant and something could get damaged. Dealing with nearly 400 cables plugged into a device leads to more accidental disconnects than 48. It's just far easier to manage. Second, this helps to future proof. While there is 10Gbps copper, the distance limitations can be problematic depending on the situation. Additionally, 10G copper tends to draw additional power the longer you go. Third, FEXes can be more easily replaced. Want to switch from 1Gbps copper to 10Gbps SFP+, just change the FEX. Your core stays the same and the configuration stays largely in place. I don't see the negatives you provide, and only see benefits for doing so. Depending on your data center setup, I would go with either two fabric extenders at the top of the rack or one (if servers can shared with neighboring racks). Servers should be connected to two separate extenders. Each fabric extender can be connected with FET's to both Nexus 7k (which should also be connected). This should reduce your chance of failure. FEXes are an extension of the chassis (read designed for data center w/high MTBF) and more similar to a module in a 1U "body" as opposed to a secondary distribution or access device. They boot they boot the software off the core, so there is no software difference. You can lose a 7k or an extender without loss of service anywhere. Potentially a 7k and a number of extenders without losing service. You can also then manage this as a single logical unit, allowing things like servers can actually do link aggregation even while connected to two different extenders, both increasing performance and reducing chance of failure. I can't see how this would increase the latency in any way and it may actually improve it. When you start using the more advanced features of Nexus, I can only see more benefits. Ultimately, you need to make the choice for your own needs. But I will say this, if you research how the top internet companies run their data centers, you will find that most of the have some sort of top of rack deployment. They don't choose this because it increases their downtime or decreases performance. They do this because it reduces downtime, increases performance, and greatly increases manageability. Edit: Consolidating from my comments so I can delete. The comment train is too long at present on this answer to be useful.