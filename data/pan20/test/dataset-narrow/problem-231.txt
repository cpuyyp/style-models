Can you please advise how can I keep slave database up to date while it's in transactional replication with master. I mean what If I change indexes, columns, tables or sthm else in master dabase? What should I do in this case? Should I make this changes to both databases simultaneously or transactional article is enough to handle it? 

Does Transactional replication suit my problem the most among SQL Server replication mechanisms? If I have an opportunit to migrate from SQL Server 2008 to SQL Server 2012 would transactional replication have some breaking changes? I've read an article on technet that there might occur errors? Isn't SQL Server 2012 mechanism better to solve my problem(I'm considering this option as last one, as I'm still using 2008 R2, and yet migration to 2012 is planned by not soon)? 

I assume this might help somebody, as I've found what caused my problem. In our local company active directory controller was using self signed certificates, I'm not faminial with details of AD, but I've found too many errors in Server's event viewer manager. So once I've stopped using Windows Authentication for operations in SQL Server Management Studio for creation of replication, and used SQL credentials with SA level - error disappeared. Hope this will help somebody. 

I've faced a problem when my .Net web application is running slow, even when I've optimized all queries. As it turned out another developer, who is making reports runs a bunch of non optimized queries all the time. So I've decided to create a separate server for him. Currently I created Jobs making backup, copying it to another server and restoring it. But it's temporary solution, so in order for second server have up to date data or at least with minimal latency I'm investigating an opportunity to create a read only replica. As I'm not a DBA I've read a bunch or articles regarding SQL Server replication mechanism. The best option for me is when my PROD web application isn't affected by replication at all, I mean lots of lock on syncronizing tables. I don't need a real time sync as mirroring does it, also I don't need any king of cluster solution, just read only syncing copy of my PROD database. So I've choosed replication mechanism(not updatable) with asynchronous distributing policy(on schedule). So I've some question: 

I'm trying to set transactional replication between 2 SQL Server 2012 named instances. I've followed a step by step manual and created a Publication. But once I look into Replication Monitor I Get the following error: 

My question is would I go about setting up the following?: First Server (master) -> Second Server (slave) -> Third Server (slave) Tutorial links welcome! I tried to Google, but I'm sure I've gotten the wrong keywords 

Basically I have one (for now) mission critical postgresql database that I will need up to the minute recovery for, I've never implemented a working WAL archival before so I would like to know what is the best way to do this. The machine we are going to backup to is remote, and we have ssh keys set up, so I was going to have the WAL's rsync'd over. Is that wise with that archive_command option? A Step by Step guide with examples on how to set these things up would be amazing as I couldn't find one I understood (I'm not a DBA) 

I have two tables which I'm trying to reconcile the differences of in postgresql. Table A is old and needs updating. Table B is an updated, schema identical version of Table A which I have the data for in a temporary table in the database of Table A. Unfortunately some time after the two databases diverged someone changed the UUIDs of records in table B and I need table A to match table B. The schema for both tables is: 

I would have thought that databases would know enough about what they encounter often and be able to respond to the demands they're placed under that they could decide to add indexes to highly requested data. 

I need to search through Table A and Table B and match records based on template_folder_uuid_parent and heading, then set the UUID of the Table A record to the UUID from Table B. Once changed in Table A the UUID will cascade correctly. 

Basically part of our Postgresql table is used to keep server access logs, and as such sometimes during production this can get pretty large. is there any way of setting in postgresql to have a maximum number of records a table can have and to push off the oldest record? 

I want to automatically check for null ID's over all id columns in my database, I have to check all tables of which there is about 50,000 so manually doing this is infeasible 

Does anyone here know if there are any tools, not using mysql's built in replication that will sync up two databases when the slave has been inaccessible from the internet for extended periods of time? The idea is to set up a virtual machine basis for our web developers, who can power on the VM and have it automatically clone from a slave of the production DB for the purposes of testing/development. they all require their own DB's but they should all replicate the master DB on startup. Diagram of operation: Production DB -> Production Read Only Slave -> Dev Virtual Machine 

Actually they don't do the same. TDE encrypts the data in the database, you need to configure the wallet and have it open before you can see/use the data after that. VPD is an Access Control Mechanism, it allows you to define sub-datasets that will be owned by different users, allowing them to see/use only the data that they actually own (even if the same set of tables are shared among different VPDs) To have more detailed information about each tool, please review the documentation on $URL$ 

If there is no backup and no way to restoring the lost datafiles, what you can do is backup any other important datafile/tablespace and recreate the database. I think it will be the less painful way to get a fully working database. 

you need to set the search string in ASM. Failing to do so, will result in the gv$asm_diskgroup view to be empty. Tomorrow morning with a clearer head and a desktop to give you exact commands I will edit and extend this answer. 

So changes are made to blocks kept in the Database Buffer Cache (DBCache). Once commited, the changes are pushed to the Redo Log Buffer (RLB) which is dumped on a regular basis to the Redo Log Files (RLF) and, eventually to the database storage files (DF). Also on a regular basis and not completely unrelated to commit, the checkpoint process dumps the dirty blocks from the DBCache to permanent storage DF. During the checkpoint process the SCN associated with the latest DB block written to storage is written on the DF headers and the control file. That will be from that moment on the latest consistent estate of the database. 

Prior to resizing, you have to remove fragmentation in the datafile. That means every segment (sets of blocks assigned to each object in that tablespace) needs to bring together all rows inside their db blocks. For this you have several options: If you're on 11gR2 you can use 

Question: Is overwriting the PFILE with the database ON dangerous? Answer: No. The PFILE is read only when the database starts, and that's if no SPFILE is there or if the PFILE argument is passed when invoking the startup command. Otherwise, when you start the database, the SPFILE is read for initial memory values, control files location and such basic information. After that, everything will remain in memory. Note: If you change the PFILE at the OS level, you need to bounce the database to make the changes take effect. On the other hand, using the ALTER SYSTEM SET ... SCOPE=MEMORY may be used as @ibre5041 suggested and achieve the result you want. Nevertheless, you're "losing" those archives from the original location, which makes me think you could as well add the NOLOGGING clause and avoid the generation of the logs that are filling up your storage. So my personal suggestion would be to use the command