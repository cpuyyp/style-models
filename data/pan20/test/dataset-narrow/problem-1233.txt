What is the story behind the unusual author ordering in these papers? Are there any other examples of major TCS papers in which the order of the authors is not alphabetical? 

Are there any known constructions of binary locally testable codes with very low (e.g., independent of the length of the codeword) query complexity and "good" rate (e.g., mapping strings of length $k$ to strings of length $k^{1+c}$, for a small constant $c$) that are also locally decodable (even if the query complexity for decoding is very large (but still sublinear))? 

Say we have a function $f:\mathbb{Z}_2^n \to \mathbb{R}$ such that $$\forall x\in \mathbb{Z}_2^n \quad f(x) \in \left\{\frac{1}{2^n}, \frac{2}{2^n}, \ldots, \frac{2^n}{2^n} \right\},$$ and $f$ is a distribution, i.e., $\sum_{x\in \mathbb{Z}_2^n} f(x) = 1$. The Shannon entropy of $f$ is defined as follows: $$H(f) = -\sum _{x \in \mathbb{Z}_2^n} f(x) \log \left( f(x) \right) .$$ Let $\epsilon$ be some constant. Say we get an $\epsilon$-noisy version of $f(x)$, i.e., we get a function $\tilde{f}:\mathbb{Z}_2^n \to \mathbb{R}$ such that $|\tilde{f}(x)- f(x) | < \epsilon$ for every $x\in \mathbb{Z}_2^n$. What is the effect of the noise on the entropy? That is, can we bound $H(\tilde{f})$ by a "reasonable" function of $\epsilon$ and $H(f)$, such as: $$(1-\epsilon)H(f) < H(\tilde{f}) < (1+\epsilon)H(f),$$ or even, $$(1-\epsilon^c n)^d H(f) < H(\tilde{f}) < (1+\epsilon^c n)^d H(f),$$ for some constants $c,d$. Edit: Trying to get a feeling for the effect of noise on Shannon's entropy, any "reasonable" additive bound on $H(\tilde{f})$ would also be very interesting. 

We say that a Boolean function $f: \{0,1\}^n \to \{0,1\}$ is a $k$-junta if $f$ has at most $k$ influencing variables. Let $f: \{0,1\}^n \to \{0,1\}$ be a $2k$-junta. Denote the variables of $f$ by $x_1, x_2, \ldots, x_n$. Fix $$S_1 = \left\{ x_1, x_2, \ldots, x_{\frac{n}{2}} \right\},\quad S_2 = \left\{ x_{\frac{n}{2} + 1}, x_{\frac{n}{2} + 2}, \ldots, x_n \right\}.$$ Clearly, there exists $S \in \{S_1, S_2\}$ such that $S$ contains at least $k$ of the influencing variables of $f$. Now let $\epsilon > 0$, and assume that $f: \{0,1\}^n \to \{0,1\}$ is $\epsilon$-far from every $2k$-junta (i.e., one has to change a fraction of at least $\epsilon$ of the values of $f$ in order to make it a $2k$-junta). Can we make a "robust" version of the statement above? That is, is there a universal constant $c$, and a set $S \in \{S_1, S_2\}$ such that $f$ is $\frac{\epsilon}{c}$-far from every function that contains at most $k$ influencing variables in $S$? Note: In the original formulation of the question, $c$ was fixed as $2$. Neal's example shows that such value of $c$ does not suffice. However, since in property testing we are usually not too concerned with constants, I relaxed the condition a bit. 

An $\mathcal{MA}$ communication complexity protocol is communication complexity protocol that starts with an omniscient prover that sends a proof (that depends on the the specific input of the players, but not on their random bits) to both players. The players then communicate with each other, in order to verify the proof (for more details, see: On Arthur Merlin Games in Communication Complexity, by Hartmut Klauck). The are quite a few lower bounds (e.g., On the power of quantum proof, by Ran Raz and Amir Shplika) of the following form: Suppose we have a communication complexity problem $\mathcal{P}$ with a tight bound of $\Theta(T(n))$ on its communication complexity (for some function $T$). There exists a lower bound that shows that every $\mathcal{MA}$ communication complexity protocol that communicates $c$ bits and uses a proof of size $p$, must satisfy $c \cdot p = \Omega(T(n))$. So one can think of it as a tradeoff between the work that prover has to do, and the work that the verifiers have to do. Moreover, it seems that for every communication complexity problem that I know of (with a tight bound of $\Theta(T(n))$ on its communication complexity), there exists a protocol wherein the prover sends a proof of size $\tilde O(T(n))$, and the verifiers only uses $\tilde O(1)$ bits of communication (cf. the two papers I mentioned above). Thus, in a sense, all of the work has been delegated to the prover (achieving the extreme case of the aforementioned lower bounds). Is there a result that shows that a verifier-"heavy" protocol implies the existence of a prover-"heavy" protocol? Is there a counter example? What about other models (such as $\mathcal{MA}$ decision trees/query complexity) wherein our understanding of the behaviour of $\mathcal{MA}$ protocols is deeper? 

Given any undirected edge-weighted graph (with weights > 0) and some dimension d, is there a way to assign positions in $\mathbb{R}^d$ to those vertices such that all of the edges between them have euclidean distance equal to their weights? The resulting edges may be crossing, that is okay. I'm really interested in the decision problem of when it is or isn't possible but an efficient algorithm to do it when it is possible would also be nice. 

I have a square plate of size 1x1, full of lots of skittles. I want to eat all of the skittles, but the only way I can get the skittles is through these two oracles: $f(x, y, r)$ tells me how many skittles are in the circle of radius $r$ centered about the point $(x,y)$ on my plate $q(x, y, r)$ gives me all of the skittles that are in the circle of radius $r$ centered about the point $(x,y)$ on my plate However, my plate is very big, and my oracles are very small. So the $r$ given to $f$ and $q$ must be less than or equal to a given value $R$. Oracles don't come free these days: I have to pay a cost of 1 each time I call $f$. Each time I call $q(x, y, r)$, I must pay a cost of $f(x, y, r)/P$: 1 for each $P$ skittles that my oracle brings to me (rounded up). The $P$ is an input to this problem. Then those skittles are eaten. There are $N$ skittles on my plate. What is a strategy that doesn't cost very much to eat all of my skittles, in terms of $N, R$, and $P$? I'm curious about two cases: 

The blank symbol is a $0$ The tape alphabet is $\{0, 1\}$ The input to our turing machine is always nothing (the tape is always initialized to only containing $0$'s) There are only $n$ internal states, for some $n \in \mathbb{N}$. 

Problem: Given a finite set of strings $\{x_1, x_2, ..., x_n\}$ of length $\ell$ or less from some finite alphabet $\Sigma=\{a_1, a_2, ..., a_k\}$, find the minimal context free grammar that recognizes all of these strings. If $k$ is a constant, and $n = poly(\ell)$, what can one say about this problem's complexity as $\ell$ grows? This seems similar to the smallest grammar problem, which is NP-Hard for the optimization problem. However it is a little different because of having multiple strings, since now one needs to recognize each string independently instead of recognizing all of them at once. The smallest grammar problem is clearly a subset of this, but for small $\ell$ this might be much easier to solve, I'm not sure. Is there a way to approximately solve this problem efficiently? 

Given two rationals $a,b \in \mathbb{Q}$, call $c = a + ib$, i.e., the complex number represented by these two rationals. A point $c$ is contained within the Mandelbrot Set $M$ if the following procedure never halts: $z = c$ $while (abs(z) <= 2)$ $\hspace{0.3in} z = z*z+c$ Normally we pick some $k$ (say, 50) and then if it doesn't halt after that many iterations we assume it is in the Mandelbrot set and stop looping. Is there a polynomial time algorithm for determining if a given $c$ does not break out of this loop after $k$ steps, in terms of the magnitude of $k$ and in terms of $n$ bits representing the numerator and $n$ bits representing the denominator of $a$ and $b$? For reference, $abs(c) = \sqrt{a^2 + b^2}$ and $c*c = a^2 + 2abi +b^2i^2 = a^2 - b^2 + 2abi$ because $i^2 = -1$. 

In the formal description of Deterministic Pushdown Automata, they allow $\epsilon$ moves, where the machine can pop or push symbols onto the stack without reading a symbol from the input. If these $\epsilon$ moves aren't allowed, and the stack can only be modified once after each symbol read, are the resulting automata equal to power to DPDAs? There may be something trivial I am missing with regards to using the powerset of $\Gamma$ as your new $\Gamma$, allowing you to "compress" $\epsilon$ moves into the equivalent automaton without them, similar to how you can compress $\epsilon$ moves in a DFA. Just it seems that such a conversion is not as trivial as for DFAs, and I'm not sure it's even possible. So are the two equivalent in power? I'm just asking because everyone seems to assume that DPDAs have $\epsilon$ moves and I'm wondering why that assumption exists, since it seems like a more complex model.