with $cost[x,x] = 0$ as a base case (except $cost[b,b] = p(b)$ unless you are promised that a $1$ occurs in the interval). 

Generally a function being "undefined" on a given input corresponds to a Turing Machine not halting on that input. E.g. a naive translation of the function you present could be a simple algorithm that forever decreases $y$, repeatedly checking if $\phi(x,y) = 0$, without ever halting. As the comments say, it can depend how you choose to define terms. But wikipedia defines a function as computable if there exists a Turing Machine that fails to halt for exactly the inputs where the function is undefined (which are all of them) and halts with the correct answer on the others (which are none of them in this case). So this function is computable in the Turing Machine sense. (For further questions along these lines, please consider cs.stackexchange.com as a better fit...) 

Certainly not for a Turing-complete programming language, following up on D.W. and David's comments. Consider a program in this language that simulates a Turing Machine $M$ (where $M$ is hardcoded into the program) and outputs zero if $M$ halts. When your optimizer receives such a program where $M$ halts, it must output some least-cost program that always returns zero (because the original program always returns zero). If $M$ doesn't halt, then your optimizer can output any program that runs forever (as the original program always runs forever). OK, now let's show that your optimizer would allow us to solve the halting problem. Suppose that the statement "return zero" has cost $C$. (Thus, we know that any least-cost program that always returns zero has cost at most $C$.) Given a Turing Machine $M$, construct the program that simulates $M$ and outputs zero if $M$ halts. Feed it to the optimizer. Run the output program for up to total cost (time) $C+1$. If it has output zero, then the original program halts. If not, the original program does not halt. ...I agree with user17410's suggestion to look at the primitive recursive functions as they are likely to be able to cover most problems we want to solve practically. 

It seems to me that there are a few choices here about how to define this relaxation. (A) Is the relaxation now well defined if $f$ is lifted in anyway to a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ such that on the Boolean hypercube it matches the original value? Next we have in principle $2$ options about how to define $\mu$, (B1) $\mu$ can be defined as any function from $\mathbb{R}^n \rightarrow \mathbb{R}$ such (a) $ \sum_{x \in \{-1,1\}^n} \mu(x) =1$ and (b) when any function $g$ is taken from the real $SOS_d$ cone we would have $ \sum_{x \in \{-1,1\}^n} \mu(x)g(x) \geq 0$. (B2) Or is it necessary that $\mu$ and $g$ be satisfying the above properties but $\mu$ maps from $ \{-1,1\}^n \rightarrow \mathbb{R}$ and $g$ be a member of the $SOS_d$ cone of polynomials each of which is also a map, $ \{-1,1\}^n \rightarrow \mathbb{R}$? (..the "degree" of a polynomial as in the real $SOS_d$ cone may not match the degree of its representative in the $SOS_d$ cone of functions on the Boolean hypercube and there is no unique way to go the otherway..) 

More formally, suppose I have a game on $n$ players and a sequence of strategy profiles $(s_1^{(1)},\dots,s_n^{(1)}), (s_1^{(2)},\dots,s_n^{(2)}), (s_1^{(3)},\dots,s_n^{(3)}), \dots$. Each $(s_1^{(i)},\dots,s_n^{(i)})$ is a $\epsilon_i$-Nash equilibrium, and the sequence $\epsilon_1,\epsilon_2,\epsilon_3,\dots$ converges to zero. My questions: 

The fields of philosophy and CS apparently have different definitions/interpretations of the thesis. In CS, I believe it is standard/accepted to define the Church-Turing thesis as the article's "Misunderstandings" section's "Thesis M" (under the narrow/worldly view). However, the article claims that this is an incorrect definition of Church-Turing. So we simply disagree. (And let's try to avoid starting an argument with them about it ... pointless arguments are their forte, after all.) The approach taken by philosophers is unfortunate, as the average layman is probably interested in the CS Church-Turing thesis, not the philosophy one espoused in the article. So they will cite the article while thinking it refers to our practical/reasonable definition, when it doesn't. So my answers to your specific questions: 

I'm both curious about specific useful applications, and any more broad complexity implications (for instance, would this get us any closer to $P=BPP$?). Edit: by the way, this problem was (is?) the subject of the Polymath4 project, which produced this writeup, but I do not see anywhere that they discuss implications of truth of the conjecture. 

But in the Razborov-Sherstov paper I am unable to locate any such claim or proof. What am I missing? Is this somehow implicit? 

If one wants to say minimize a function $f : \{-1,1\}^n \rightarrow \mathbb{R}$ on its domain then a degree$-d$ Lasserre relaxation of it would be to solve the problem of $\min \mathbb{E}_\mu [f(x)]$ over degree$-d$ pseudo-distributions $\mu$. 

A terminology issue about what is ``low degree" : Around page 13 here, $URL$ when this was shown for the Boolean hypercube graph the author claimed that this is a degree $4$ SOS certificate. This terminology is a bit peculiar to me. Because as one sees in the proof it uses non-negativity of the pseudo-expectation of polynomials of the form $(fg)^2$ where $f$ and $g$ are degree at most $d$ polynomials in $n-1$ variables. So I think this should have been called a degree $4d$ SOS certificate. But somehow they want to see this as a degree $4$ SOS proof in the space of Fourier coeffients because the product $fg$ is obviously degree $2$ over Fourier coefficients. I am not sure if this way of thinking makes sense! It would be helpful to know why this way of counting makes sense! 

From what I understand (which is very little, so please correct me where I err!), theory of programming languages is often concerned with "intuitionistic" proofs. In my own interpretation, the approach requires us to take seriously the consequences of computation on logic and provability. A proof cannot exist unless there exists an algorithm constructing the consequences from the hypotheses. We might reject as an axiom the principle of the excluded middle, for instance, because it exhibits some object, which is either $X$ or $\lnot X$, nonconstructively. The above philosophy might lead us to prefer intuitionistically valid proofs over ones that are not. However, I have not seen any concern about actually using intuitionistic logic in papers in other areas of theoretical CS. We seem happy to prove our results using classical logic. For example, one might imagine using the principle of the excluded middle to prove that an algorithm is correct. In other words, we care about and take seriously a computationally-limited universe in our results, but not necessarily in our proofs of these results. 1. Are researchers in theoretical CS ever concerned about writing intuitionistically valid proofs? I could easily imagine a subfield of theoretical computer science that seeks to understand when TCS results, especially algorithmic ones, hold in intuitionistic logic (or more interestingly, when they don't). But I have not yet come across any. 2. Is there any philosophical argument that they should? It seems like one could claim that computer science results ought to be proven intuitionistically when possible, and we ought to know which results require e.g. PEM. Has anyone tried to make such an argument? Or perhaps there is a consensus that this question is just not very important? 3. As a side question, I am curious to know examples of cases where this actually matters: Are there important TCS results known to hold in classical logic but not in intuitionistic logic? Or suspected not to hold in intuitionistic logic. Apologies for the softness of the question! It may require rewording or reinterpretation after hearing from the experts. 

Before this paper was there no other function known in the $Th \circ Th$ or $Th \circ Maj = Maj \circ Th$ or in any other $TC^d$ class with such high sign-rank? What other functions do we know of which have a high sign-rank? I can see it to be known for only a handful of examples like the Minsky-Pappert function, a depth $3$ $AC^0$ generalization of it (in a Razborov-Sherstov paper) and another in a Bun-Thaler paper and Inner-Product-Mod-2. Are there others? (If yes, then can you kindly link to references to their proof?) The page 4 and 5 of this paper give some intuition about what makes a Boolean function have a sign-rank. It would be great to know if others have more insights to add to this discussion. Are there maybe any "folklore" thumb-rules which help judge if a function has a high sign-rank? 

(I am hugely editing the question. My initial question was if lowerbounds on threshold circuits say anything about P/NP and it seems that they dont. Irrespective of P/NP its an independently true fact that there exists Boolean functions which are exponentially hard for threshold circuits just that we have never seen them.) Let me ask my followup question in $3$ parts which I guess are related, 

Given an algorithm running in time $t(n)$, we can convert it into a "trivial" uniform circuit family for the same problem of size at most $\approx t(n)\log t(n)$. On the other hand, it might be that we have much smaller uniform circuits for that problem, even if $t(n)$ is an optimal running time. The circuits may take longer than $t(n)$ to generate, but they are small. But do we actually know how to build such things? I think the initial question to ask is 

(For example: If we show that $A$ requires time $2^{\sqrt{n}}$, should we say that $A$ requires exponential time?) If we are thinking in the style of EXPTIME, I think we would say "yes": interpret the statement as saying that $A$ requires $2^{n^c}$ time for some $c > 0$. But thinking in the style of E, we would say no. For instance, the Exponential Time Hypothesis states informally that 3SAT requires exponential time, and formally it says that 3SAT requires $2^{cn}$ time for some $c$ (note the crucial difference!). Because of this, I am not sure if there is a commonly accepted interpretation of the above statement. Ideally we would state more precisely what we mean when using the term, unless it's clear from context. Perhaps others can mention if they think one interpretation or the other is more "standard". ... In summary: if $f(n) \leq 2^{\sqrt{n}}$, then $f$ is definitely at most exponential. But if $f(n) \geq 2^{\sqrt{n}}$, it is not clear to me whether we would say $f$ is at least exponential. 

If one looks at the space of all polynomial sized threshold circuits (at constant depth or not) then do we know of any natural complexity class in which they sit? The closest I know of is that the class of depth 3 threshold circuits with no weight restriction are known to be in $NP/poly$. What happens at higher or non-constant depths? Is there any class of circuits against which we know that there cannot be exponentially hard functions if $P=NP$? (Threshold circuits are clearly not of this type.) Are there reasons to believe that for each depth $d$ there exists a Boolean function which is easy for depth $d+1$ but exponentially hard for depth $d$? And if such functions are found then would it have any implications (separations) for the other usual complexity classes? 

My question is particularly about the set-up in section $8$ (``Analysis of the KV Max-Cut instances") of the paper, $URL$ 

I guess that the author is claiming the following identity,for all subsets $S$ of size at most $t$, $\sum_{S' \subseteq S, S' \text{independent in the subgraph induced by }S} ||U_S'||_2^2 = 1$ Can someone clarify/prove that this above statement follows from whatever has been defined about these vectors $U_{S'}$ on page 4? (Is the author claiming that this equality above is a consequence of how he has defined the feasible set of $U_{S'}$ vectors in the top box of page $5$?) 

Sorry for the brevity, maybe I can come back and expand later. There is work in voting and modeling voters that should be relevant. For example, take a model, i.e. in Mallows model, if A is truly cuter than B, then with probability $p > 0.5$ I rank $A$ ahead of $B$ and otherwise I rank B ahead of A. All rankings are made independently. Then the corresponding voting rule is to compute the MLE underlying "true" ranking that generates the votes we've seen. For this particular model it is the Kemeny rule and it's NP-hard, but there are other models too. ... (expansion) Condorcet in the 1700s first used the MLE idea. Say that every voter produces a ranking, and in that ranking any given pair of candidates are in the "correct" order with probability $p > 0.5$. Then, given a bunch of people's votes, we want to compute the single linear order that had the highest probability of generating those votes, the MLE. Young showed that the solution of this is the Kemeny voting rule, which minimizes the average bubblesort distance to all the rankings we are given. Conitzer and Sandholm ('05) extended this idea to other voting rules, with other noise models (e.g. I put the best candidate in the $n$th place with probability $2^{-n}$). Since then there's been work, e.g. by Xia and Conitzer (09), on extending these ideas to when people don't see the whole list of alternatives, but just get to see some of the alternatives (submit partial orders). This should be relevant to your setting, I think. There is also at least some empirical-type work in this vein. I am thinking specifically of Pfeiffer et al (12). The setting is basically the same as yours. They have some sort of adaptive strategy of which kitten pair (or whatever they used) to show to the next voter. The goal was to get a good ranking with fewer pairwise comparisons (i.e. not asking every voter to rank all the kittens). That's what I know of the theoretical and AI work. There's also a lot of statistics that I know less about. Probably the stats will be most helpful if you want to actually implement something that aggregates partial rankings/observations. I know that some of the most popular statistical models for these things are the Mallows model, Bradley-Terry, and Plackett-Luce. All of these are noise models for how voters randomly generate a ranking given the true underlying "correct" ranking. In fact, there is so much work about this going on in statistics that I might suggest you post on stats.se, because I am woefully ignorant but I think they (or perhaps the machine learning community as well) have a lot of work on algorithms for (approximately) finding solutions (e.g. the MLE) given a bunch of rankings and assuming that they were generated using one of these models. At least, I hope these keywords help you in your search! 

Consider an iterative algorithm of the form $x^{t+1} = x^t - \eta g(x^t)$. (..if necessary feel free to assume that a function $L$ is explicitly known such that $g = \frac{\partial L}{\partial x}$..). Suppose that now I want to show that there exists a point $x^*$ within some pre-determined ball which is a fixed point for this iterative algorithm. 

For what graphs do we know that their small set expansion property has a low degree SOS proof? Is this known to be true for say the complete graphs? 

I wanted to know as to how often are the submissions for ECCC cleared? Or do we know when is the next set of uploads going to be made on that site? Unlike arxiv where I know when my paper will go online, on ECCC there seems to be no way to know what is the timeline of my article. Last time I had submitted something on ECCC it took about 3 days for it go online. This time I am wondering as to why its taking longer. (..I had gone by this previous experience of 3 days to plan that I would refer to this online index number as reference for an upcoming talk of mine but now it seems I can't do that because the timeline looks indeterminate..) 

Well, at least $\#\mathsf{P}$-hard. Given a SAT formula, construct a graph with two vertices, $v_x$ and $v_x'$, for every possible assignment of variables $\vec{x}$. If $x$ is a satisfying assignment for the formula, draw an edge between $v_x$ and $v_x'$; these are the only edges. It is easy to construct the circuit for this graph from the SAT formula, and the number of odd vertices is exactly twice the number of satisfying assignments. 

Same question as (2), but relating to the actual equilibria computed by algorithms. I guess probably we will either get algorithmic/constructive answers or none at all, so the distinction doesn't matter much. 

$SL = L$. $RL$ stands for randomized logspace and $RL=L$ is a smaller version of the problem $RP=P$. A major stepping stone was the proof of Reingold in '04 ("Undirected S-T Connectivity in Logspace") that $SL = L$, where $S$ stands for "symmetric" and $SL$ is an intermediate class between $RL$ and $L$. The idea is that you can think of a randomized logspace Turing machine as an polynomial-sized directed graph, where nodes are states of the machine, and an RL algorithm takes a random walk that has good properties. SL corresponds to undirected graphs of this form. Reingold's proof built on work on expander graphs, particularly Reingold, Vadhan, and Wigderson's "zig-zag product", to take any random walk on an undirected graph with good properties and turn it into a psuedorandom walk retaining those properties. edit this question was posted before the question was explicitly changed to focus exclusively on P vs BPP ... I am leaving it up because it seems to be of interest.