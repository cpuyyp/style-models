The VCM Proxy and Backup Exec server can be the same box. Once you've got VCB and Backup Exec installed with the CALs, you just point it at the VirtualCenter or each ESX host individually, and tick the VMs you want to back up. You may need some cals for specific VMs. Exchange, SQL and Sharepoint will all need individual server CALs before you can back them up 'properly' (with full granular restore ability). 

We have a GPO set on our servers OU that runs a very simple logon script. It copies across the BGInfo exe, a logo, a couple of small vbscripts and a template .bgi file. Then it runs the template through BGInfo. The output looks like this. We don't distinguish between dev and prod because that's done via the host names, which are always visible. Number of admins, uptime and host type (physical/virtual) are vbscripts that BGInfo uses when it runs. I can post them up if they're useful to anyone. If your dev/prod/test/uat servers are seperated in AD by OU, or on seperate domains, then jigging a script like this to apply seperate wallpapers would be trivial. Just attach a slightly different script/BGI to each OU. 

It may be fruitful to investigate why the program needs admin priveledges. If it comes down to things like file or registry permissions, then you may be able to get the program operating under non-admin credentials by adjusting permissions to grant the user access. 

Obtain a network switch for the server equipment Plug it into the router/firewall on a spare port and configure the router/switch to service the new server subnet 10.x from this interface Migrate servers across to the new subnet until none remain on the workstation subnet Overnight, reconfiguret the workstation subnet from 0.x to 15.x 

Yep, you can access google apps with imap. Alternatively, you can have your Exchange infrastructure sync directly (and live) with google apps so that you have calendaring on both platforms. You can also leave some users entirely on exchange, while having others entirely within google apps. Calendar sync will work between them all. 

Try addressing the server via its FQDN DNS name, or its IP address directly. Addressing it via an unqualified hostname could lead to it being resolved via NetBIOS rather than DNS, which in turn can lead to less reliable name resolution. I've seen NetBIOS-resolved machines become invisible for a period when NetBIOS browser elections are being forced. 

Nope, never ever do this. The GUID is AD's means of locating/identifying a domain controller for replication. It's important that it remain unchanged (and most importantly, unique to each DC). Could you setup the machines to trust a central CA and have that issue the machine certs automatically? 

Do your pings traverse a gateway to reach your VM? The setting that should fix this for you is in control panel -> network -> vpn connection -> tcp/ip settings -> Advanced Look for 'use default gateway on remote network' and un-check it. You'll probably get stuck in the middle on this - on one hand, you want some traffic to route via the VPNs default gateway. On the other hand, you want some traffic to route via your own network. If the tick-box doesn't solve the issue entirely for you, you'll need to see what IP ranges are needed in which networks, and start adding static routes as per this guide: $URL$ 

If you know how many workstations you have, subtract the number of workstations listed in WSUS. You then have the number of workstations that are either updating directly from the Internet or not updating at all. For a network with 1000+ machines you almost certainly have a domain. You can use Group Policy to enforce a WSUS policy, and ensure machines connect to your WSUS server and receive regular updates. 

Given the size of your organisation and requirements, consider Microsoft's cloud Exchange offering: $URL$ It'll meet your requirements. Your scenario appears to match up perfectly with the current 'ideal case' for cloud-hosted email providers. You could also look at google's professional mail hosting option. Again you'll get your calendaring, address book, device sync, spam filtering, IMAP connectivity, all as part of the package. Managment will love the low up-front cost (basically just man hours to migrate) and the monthly per-user spend. 

Platespin Protect offers this functionality. It takes live copies of production machines, and is able to spin up a DR replica of the 'workload' on-demand. Replication runs in the background and can replicate across a WAN. It's source-agnostic, and supported targets include ESX, Hyper-V and Xen. Time to bring a DR copy online is about 5 minutes. It boots the target VM, installs relevant device drivers into the OS, then brings the box up with networking settings that you specify. You still have to have some means of your end user accessing this DR replica, however. The key issue you have here is your criteria that the replacement be brought online without any delay, while also being an identical copy of a highly customized machine. What this implies is having some kind of 1:1 permanent sync between the machines. This is clustering. You want to cluster desktops. My gut feeling is that providing a VMWare VDI environment for your users will be the best solution for you. This makes the local workstation irrelevant as long as you can get it to connect to the VMWare infrastructure. They'll also be able to run their personal environment customized in whatever manner they like, and on the back-end you can provide proper redundant power, cooling, and solid VCB-based backups. 

Check the event log to ensure that DFS isn't complaining about anything. The most common one is that it encountered a file larger than its staging space, and stopped replication. If this is the case you'll need to increase the staging space, or snipe the file out of the share to things can continue. Just a word of caution - Don't manually copy files from one folder to another if they're both supposed to be replicating with DFS. It'll get in a big tizzy if you try and manually override it. 

30 VMs served from just 2 spindles (disks) will probably suffer an IO bottleneck, even if those VMs aren't particularly IO intensive (either random or sequential). You're looking at 30 separate concurrent read requests occurring across widely separated areas of the disks. Lots and lots of time wasted seeking between places. I'd recommend setting up a second drive array if the option is easily available to you (spare drive slots or a spare external housing), and migrating your VMs across to it. 4-6 disks min. Another improvement would be a larger read/write cache, if you're only running on a 128 or 256 chip. Another place to check is the vCPU allocations as Zypher mentioned - assigning too many vCPUs to each VM is (counter-intuitively) likely to slow all the VMs down (each VM has to wait for a free core for every single one of its vCPUs before it can get CPU time, so a 4vCPU VM may get less cycles than a 2vCPU VM) Edit: thinking about it a bit more, there are also some locking problems you might come across by having so many VMs on a single LUN. You can encounter per-datastore locks during various VM operations, possibly power-on/suspends etc. That'll start to stack up quite quickly so slow boot-ups etc may be caused by this. You can get around this by setting up separate datastores within the same amount of drive space (resize the current partition to half, then create a new partition in the blank space. Spread VMs evenly between the datastores). About 15 Vms per datastore is a good maximum.