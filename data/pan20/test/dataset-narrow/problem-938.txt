A couple of things might be being written. The most likely is file access times, but there are others (for example, checking a filesystem as done after an unexpected power loss will usually write to it). Now, as to file corruption, there are three probable sources: 

Given all of this, if you actually care about security, don't try to wipe SSD's electronically (If it's a TCG Opal compliant SED however, and you trust the manufacturer, take that route), and don't' try to wipe hard drives electronically if they show any evidence of past bad sectors. 

USB itself. USB is horrible for storage devices, period. It's unreliable, has poor performance (even with UAS support and USB 3.1), and has absolutely atrocious error recovery. It is used for this type of thing simply because it's easy, and for most people that trumps reliability. If you can find some way to avoid using USB, you will immediately make things more reliable (eSATA is pretty good for what it's worth, and Thunderbolt is great as an alternative). Note that most of this is either a result of power issues (a lot of USB hard drives need more power than one USB port can provide, hence the weird Y shaped cables some come with), or odd USB host controllers (the USB controllers found in most computers have very few issues most of the time as long as the mechanical connection between the cable and port is solid, but a lot of mobile USB controllers have odd edges cases where they don't work right). The circuitry to control the disk. There is no such thing as a USB hard drive, all of them are SATA or IDE hard drives connected to a USB-to-SATA or USB-to-IDE adapter, and such adapters are notoriously problematic as well, although some brands are better than others (ASMedia seems to make really good chipsets for these, but of course it's hard to find out what chipset one is using without buying it yourself). Actual media problems. If it's a regular hard drive, you have to deal with any issues caused by rough handling. If it's an SSD (or flash drive), you have to deal with poor data retention (flash memory is not good at long-term data retention, in fact it's far worse than traditional magnetic media at it). Unless you're being really rough with the disk, this shouldn't be an issue this fast, but do keep in mind that just dropping a traditional hard drive can be enough to cause data corruption. 

There isn't a way to directly merge the contents of a mount point like this. The closest option is use a union/overly mount, but those don't work reliably with networked filesystems, and what you would have to do to get it to work at all in this case is rather non-intuitive. Ideally, you should probably either: 

Actually, typically the refresh rate is less than 1ms. The JEDEC standard mandates that it happens at least every 64ms, and most systems don't refresh any faster than every 16ms (because refresh cycles are expensive in terms of time, so the more frequently you have them, the lower your regular memory performance). As a point of comparison, every 64ms is equivalent to 156.25 Hz, and every 16ms is 625 Hz, both significantly slower than typical clock speeds for memory. Now, the actual amount of time it takes for a given cell (bit) of DRAM to leak enough of it's charge to not consistently a stable value, or to change values, is not a fixed amount of time, because there are a bunch of physical processes involved that are not inherent to the design of the DRAM chips (at minimum, it depends on the temperature of the cell and how much EMI is present). The refresh rates listed above are conservative in that they fall well below the amount of time it takes even in the worst of typical conditions for that charge to leak out. So, even at normal room temperature in a well shielded case, a vast majority of the bits are likely to retain their state for at least a few seconds, with a decent percentage retaining their state for longer (in statistics terms, the upper standard deviation from the average is significantly higher than the lower standard deviation), and therefore you just have to be a little lucky and be really careful about not overwriting the data you want if you boot the system within a couple of minutes. In particularly cold conditions, the state is retained for potentially far longer. There are anecdotal stories of determined government agencies using liquid nitrogen to cool memory modules for an online transfer to another system to retrieve data from them. While such stories are somewhat far fetched (actually connecting the memory modules to another system without unintentionally altering the contents would be at best tricky), the general physics behind them are sound. 

First off, do note that this is not universal behavior. Not all Linux systems are configured this way, and in fact I can't name any distributions that do configure things this way by default without doing some serious research. In most cases, any given Linux system will either require root privileges regardless (this is what most systems do), or will let anyone in a particular group do it, or will just let everyone do it (though this is rare, as it's a pretty significant security hole allowing for a trivial DoS attack if a network service is compromised). Why? The why is pretty simple, it's very easy to accidentally shut down the system remotely when someone is trying to use it. By requiring either physical presence at the console (what you're calling a normal session) or proof of authority over the system (by authenticating via to get root privileges), the system is making it much more difficult for such a situation to happen. The implication is that by being physically present at the system or providing root credentials, you have sufficient authority that it doesn't matter if someone else is using the system. How? This is somewhat more difficult to answer without knowing a lot more about the system in question, but a couple of methods come to mind: 

Assuming you don't have DNS set up for your local network, your best bet is probably to use NMap, which can (depending on what switches you pass it, how the hosts are set up, and how lucky you are) tell you the NIC vendor and MAC address, the services running on each host, and possibly even the version of those services and what OS the system is running. The particular command I would use for this is: 

Have you looked at Alpine? The standard edition (which which does not include OpenSSL, but allows it to be installed trivially with ) is roughly 110MB for 64-bit x86, and the extended edition is about 300MB (which does include OpenSSL according to the package list here). IF you're just looking for a bare-bones system with a known state, Alpine is probably one of your best pre-built options. If you're willing to do a bit more work, it would not be hard to put something together yourself with Buildroot. I've actually used that for similar one-sff stuff, and it's pretty easy to get a basic system that takes up less than 50MB. One word of caution though, be careful using such a system for a CA. The known and verified boot state is nice, but you're probably going to be starved for entropy at least initially unless you're running on a recent Intel CPU (AMD still doesn't have RDRAND or RDSEED yet) or manually inject entropy. 

A NAT router (all internal clients appear externally as one client with whatever your external IP address is), just like almost all consumer access points (in contrast to many corporate AP's, which are typically run as 802.3 to 802.11 bridges, not routers). A layer 3 gateway (because it's converting from IEE 802.11 and/or IEEE 802.3 signaling and framing to ITU-T J.112/J.122/J.222 signaling and framing), just like any modem that provides an ethernet connection that you connect your computer to. 

and are generally separate partitions in all but a few odd configurations that aren't supported by pretty much any Linux distribution. is the EFI system partition. It will be automatically created by the Windows install, and needs to be a FAT32 formatted partition with a particular type code (specifically, C12A7328-F81F-11D2-BA4B-00A0C93EC93B ). It contains EFI executables (actually a special version of the PE32+ format used by Windows) that are the various EIF bootloaders and system tools. For most systems, 128MB is a reasonable size (that will comfortably fit both the standard MS bootloaders, and a typical distro install of GRUB2 together with the stuff needed for Secure Boot on Linux if you actually use that). Your example listing has 3 directories: 

You can do some rather neat trickery with filesystem capabilities to allow for this, but you have to redo this every time the commands get updated. In the case of systemd, , , and are all provided by systemd, and end up being equivalent to various subcommands. Because of this and the session tracking done by , it should be possible to impose restrictions like this pretty easily. 

Note that this requires you to give each of the scripts reasonably descriptive names to be useful, but it's probably one of the quickest possible options without using extra software. 

As far as actual backups, just pay for a second cloud storage site (or if you're being sensible and using something like Amazon S3 that lets you specify location, set up a second storage location and back up to both). 

It sounds like what you want is authbind. I've linked the Wikipedia page here instead of the homepage, as the homepage is just a gitweb page for the official repository, and therefore is kind of useless for end users. In short, authbind uses LD_PRELOAD to indirect the call through a SUID helper program, thus allowing programs to bind reserved ports. Make sure you restrict access to authbind to accounts you trust, and keep in mind that it will let any user who can access it bind to any priveleged port. Alternatively, if you don't care at all about security, you can add this line to your sysctl configuration on the system in question: 

All of these scripts get put in one directory, which you then add to your path if you want them to be trivially accessible. Assuming they all go in , the following script should spit out a list mapping the file name to the command it runs: 

Your premise is flawed. The fact that there is essentially zero seek latency on an SSD means that the fact that the data isn't stored as a linear mapping is irrelevant to the performance of the device, and therefore you can treat it functionally as a simple linear mapping of blocks. Also note however, that the article you are quoting isn't entirely accurate. A filesystem block is usually 4kB, while the 'blocks' of data on the SSD are actually usually 4MB in size, so it's very possible for a file less than 4MB in length to be entirely contained in one physical location on the SSD. There are two other reasons that sequential I/O performance matters though: 

Having a few MB of dirty memory is normal on any reasonably busy system, and even spikes up to a few hundred MB are not unusual. The only time to really be worried about it is if it's consistently very high, which is usually a sign that your disks are a performance bottleneck for your system. 

Given your description and the assessment that it's not coil whine (and unless you've got a cheap PSU, you can generally rule out coil whine to begin with), it's probably mechanical. My first guess would be that a bearing is slightly out of alignment in one of the fans. Given the low rate at which the clicking is occurring, my guess is it's probably a large fan (they usually spin slower, so the sound would not happen as frequently), and given that system load doesn't affect things (and the reasonable assumption that you have fan speed scaling enabled and working correctly for your CPU and GPU fan), I would guess that it's either the PSU fan, or possibly a rear mounted case fan. My second guess would be the piezo-electric transducer that you find in many systems these days in place of an actual internal speaker. If there is some interference on the traces to that from other parts of the board, you can get stuff like this happening, though I've never seen it with any ASUS boards I've worked with, and it's usually pretty easy to isolate as the source with the case open. 

FIrst off, you actually can boot a 'live' system like this from a hard drive or other persistent storage pretty easily. Just write out the image you would put on the USB flash drive to whatever storage, and boot from that. Many kiosk type systems do this, as do a lot of thin-clients. Now, that said, this is probably not the best option for your use case. I would very much suggest taking a look at how Android and ChromeOS handle their root filesystems. Both are designed to be extremely resilient against corruption caused by unexpected power loss, and both also provide tamper protection by default (IOW, you'll know if someone modified the data while the device was off). Similar setups can be achieved without too much difficulty with a custom root filesystem made by Buildroot. As an alternative to that, you might consider looking at Alpine Linux. It's designed to be run from read-only media, though I'm not sure how well it supports things other than 64-bit x86 systems. 

On a current upstream version of the Linux kernel, it will force the pages to be faulted back in, but appears to leave copies in swap. I believe most BSD systems and other UNIX systems behave similarly, not sure about Windows though. Delaying writing anything until after mlock() is called will prevent the data from being swapped out, but can not guarantee the data won't be written to disk unless ACPI S4 and OS mediated hibernation are not possible and you're not in a VM (if you're in a VM, you could be under a type 1 hypervisor, and therefore subject to the host system's virtual memory constraints, which you can't control). As a result, proper mitigation involves: