model of the game state, that stores all information of the game (e.g. board status / map, queue, move number / position in queue) a move generator, which gives you all valid moves for a given game state a "do move" and a "undo move" function; which apply / undo a given (valid) move to a game state. Whereas the "do move" function should store some "undo information" for the "undo" function. Copying the game state and modifying it in each iteration does slow down the search significantly! Try at least to store the state on the stack (= local variables, no dynamic allocation using "new"). an evaluation function, which gives a comparable score for each game state search function 

Note, a simple "calculation" will only work if the "optimal" way is free from obstacles (e.g. other cars, borders of the road) -and- if you can drive there without changing direction (e.g. needed when get out of a paring space). All more complicated actions can't be just calculated in a simple way. Even just stupidly using a A* search won't work with an "Ackermann drive". A former PhD collegue was looking for a student to do a masters thesis about that, he meant this would need to apply some algorithms as "sampling" (machine learning). This is what is meant by "sampling" in matters of machine learning: $URL$ Instead of trying to optimally calculate it, I would let the AI use some machine learning technique. For automatically steering air planes the "Temporal Difference" (=Reinforced Learning) algorithm has been successfully used. I think this would be applicable here. Reinforce your agents by: speed (positive), not obeying the traffic rules (very negative), fuel usage (negative). Try to train different agents with different "personalities", as speed-affine, high-speed aversive, very accident avoident (very high negative reinforcement for crashes), fuel-safer, only negative reinforcement then getting caught while not obeying rules. You'll get a really realistic scenario this way. You'll even be able to see how changes to street signs, traffic control, speed limits, speed cams will affect driving behaviour and safety on the street in a quite realistic way. 

Categorization The Answer isn't easy. The game theory has some classifications for games, but there seems to be no clear 1:1-match for that game to a special theory. It's a special form of combinatorial problem. It's not traveling salesman, which would be deciding for an order in which you visit "nodes" with some cost to reach the next node from the last one. You can't reorder the queue, nor do you have to use all fields on the map. Knapsack doesn't match because some fields become empty while putting some items into the "knapsack". So it's maybe some extended form of that, but most possibly the algorithms will not be applicable because of this. Wikipedia gives some hints on categorization here: $URL$ I would categorize it as "discrete-time optimal control problem" ($URL$ but I don't think this will help you. Algorithms In case you really know the complete queue, you could apply tree search algorithms. As you said, the complexity of the problem grows very fast with the queue length. I suggest to use an algorithm like "Depth-first search (DFS)", which doesn't require much memory. As the score does not matter to you, you could just stop after having found the first solution. To decide which sub-branch to search first, you should apply a heuristic for ordering. That means you should write an evaluation function (e.g.: number of empty fields; the more sophisticated this one is, the better), that gives a score to compare which very next move is the most promising. You then only need the following parts: 

You can also do a combination of these methods. You can delay information releasing up to some limit (e.g. 50 ms and then start ). So you wait until all players have send their next actions -or- some timeout (e.g. 50 ms) is reached and then distribute them to the player together with the timestamp registered at the server. The server just simulates this. If an action is arriving too late at the server it is executed within the next round / simulation tick. The result is, that up to the limit (e.g. 50 ms) the player will have a synchroneous play, the server does not need to resimulate and only players with longer delays will have re-simulations at their client and will actually suffer from the problem. It also enables to have a quicker update period, if all players have a shorter ping that the limit (e.g. 50ms). Other Game Types... In games which are more cooperative in nature and lots of players (as MMORPGs for example) the lag for all other players should be at least be limited to a reasonable amount. You'll always have players with slower connections, so they would at the end define the speed of the whole simulation. Depending on the type of game, the acceptable lag can be up to 400ms without being noticed by the players. This might especially work for real time strategy games... Especially in real-time strategy games repeatedly re-simulating and excessive "beaming" of actors would not be acceptable, if this is happening for all players. "What happens if I delay commands for x milliseconds in this game?" is what matters. For example in a real time strategy game it would even sound logical that it takes some time to tell the acting unit what it shall do, then it maybe needs to plan the route and finally starts walking. So a short delay won't disturb to much. 

This kind of tree structure is much faster, since dynamically allocating memory is really really slow! But, storing the search tree is quite slow either... So this is more an inspiration. 

If you already have such huge problems with 10 simulated objects, you'll need to optimize the code! Your nested loop would cause only 10*10 iterations of which 10 iterations are skipped (same object), resulting in 90 iterations of the inner loop. If you only achieve 2 FPS, this would mean that your performance is so bad, that you only achieve 180 iterations of the inner loop per second. I suggest you to do the following: 

Observed learning (by giving samples): Generate a lot of random situations. Calculate (or manually choose) your optimal solution. For a grid bases simulation you can use the A* algorithm to find the optimal route to the goal. Then, use the first step in the path as desired output. Unobserved learning: Train your network with reinforcements as: a) how much nearer (good) / farer (bad) it moved to the goal; b) weather it hit a bomb (very negative). 

Correct! You need to search all 5^4 (or even 6^4, as you can walk in 4 directions, stop and "put a bomb"?) actions for each game tick. BUT, when a player already decided to move, it takes some time till the move is executed (e.g. 10 game ticks). During this period the number of possibilities reduces. 

Above we have the current game state as a node. It's connected to the possible following states (timesteps are "downwards"), left and right. At each following state you follow the new possible moves until you reach the maximum depth (here: 4) or until the game ends. Then you evaluate the state. In the minimax/negamax algorithm each player does its move in its own time step (knowing the other players move of the previous time steps). A player always chooses the path, that guarantees the -best- result for her/him. This result is recursively returned by the search function. In 2-person games an "evaluation function" is often like "Player A wins" = positive number, "Neither Player wins (draw)" = 0, "Player B wins" = negative number. So player A tries to maximize the result of the evaluation of the game and player B tries to minimize it. We always evaluate the game at the leave nodes (here: depth = 4). Between the depth levels (0 to 4) there are the moves of one player each: 

Note: The characters and tiles must be drawn in the same loop. That means a tile left, below or under the character must always drawn before the character. You should also determine if the player is -within- a building and possibly not draw anything that is in the level above the player. So you can still see the player moving inside the building. This can be achieved by marking special tile types for the floor inside of buildings (= saves memory). If the player is now located at a special position you use the type of tile below to determine, if your player is within a building. If so, you only display tiles up to the current z-index of the players position. Alternatively, you can store this information on each map position (= needs more memory). Some example code: 

The different ratings must be summed up by weighting function (factor_a * rating_a + factor_b * ranting_b + ...) for all units... In strategy games also the resources (gold, wood, ...) left must be taken into account. If your evaluation function is well enough, you do not need to really search "deep" into the tree for most cases. So you probably only need to take a closer look at the 3 or 10 most promising choices. See next chapter... Possible moves at each position The most problematic thing about using min/max for strategy games is that you can command multiple units in one turn, whereas in chess you are only be allowed to command one unit (except for castling, but this is a clearly defined move combination). This causes 5^N possible moves for N units for each "position" (chess term), if you would only decide between "move north, south, west, east OR stop" for each unit. You could solve this by breaking down the complex command into the low level commands: e.g. choose action for unit A, go into depth and decide for unit B.... decide for unit N ... and then end this turn. But, this alone doesn't change the complexity! You must optimize the order in which actions are assigned to units (e.g. first unit B, C, D and then unit A). You could record the impact of the decision for each unit during the last calculation and then sort by importance. This way alpha-beta pruning can be used to cut away any bad combination from the search tree very early. The highest priority should always be "do nothing more and end your turn" (null move pruning) in each iteration. This way you can "skip" assigning most tasks to most units and let them just continue what they did before. This way the search will go into depth quickly by just taking a look at the "critical" units (e.g. the ones really in combat right now). Make sure to only command each unit once... You can also use some randomness to make sure that the "important" units are getting a command from time to time, too. Especially, units finishing some job (e.g. harvesting - or having no enemy assigned anymore) should slightly increase in importance. Iterative Deepening + Caching/Hash Table Then, you can "interative deepening" to go into depth more and more until some time limit has been reached. So you will search deeper if there are less units, and you have always some "result" if you stop searching for a better solution. Iterative deepening would require to use a hash table to cache former results of searches. This also enables to reuse some of the results from the last turns search (the branch of the search tree that covers the commands that were actually executed in the last turn). To implement this, you need a very good hashing function (have a look at the "zobrist key"), which is able to be iteratively updated. Updating the hash key means, that you can just take the hash key of the old "position" and can just kick in the change in the position (e.g. take away unit at position x and put it at position y). This way calculating the hash key is quick and you don't need to process the whole boards situation to calculate it, just to check if the hash contains a former entry for this position. In a way you must make sure that no hash collisions happen. Non-deterministic Behaviour Non-deterministic behaviour is a problem for min/max searches. This means, it isn't sure if you will hit an attacked target (e.g. probability is 10%). Then you can not just plan this happens. In that case you need to modify the algorithm and put a "probabilty" layer in between. It's a bit like "its the probabilities turn". Each independent result must be regard separately. The evaluation through this depth "layer" must then be sampled (monte carlo sampling) and the result of the in-depth evaluation must be weighted by the probabilty of occurance. Different results of the probability layer must be regarded like different opponenent moves (but instead of min/max the "average" must be calculated). This will of course increase the complexity of the search tree. So this would in fact be something like min/max/avg-search (not min/max-search anymore). Summary When applying all those techniques (which are all used by current chess engines) to a deterministic game, you will surely be able to achieve reasonable results for a game, too. For non-deterministic games, this will be probably more complicated, but I think still manageable. A good resource for explanation of those techniques (for chess) is $URL$ You can even implement some sort of directed randomness in min/max searches. Instead of deterministically investigate the best results first in each iteration, you can just randomize this and let its order be decided by a probability distribution that is based on the current evaluations... 

You seem to have explained everything yourself. But, in case you need someone agree / disagree with that: Yes, your thoughts all seam reasonable. As you do not wait for all players inputs before start simulating the world, you will run into those problems. You can not solve this, as you don't have the information (what each player does) before the network has transferred it. So you only can "guess" what will happens and show this to your players, which is like predicting future (you might have luck or not). Or, you show only what is already for sure which will cause the maximum lag of all users to effect all users experience. The only thing you can do about this is to improve your prediction algorithm for a user. E.g. detect movement patterns in acceleration, deceleration. But you'll never get it to perfection... Note, that if a user only pretends to be lagging (network lag) he might see his "simulated" future. So the user might cheat. To balance the chances, you should delay the information (what other players did) sent from the server to each user by the detected lag (minus the minimum lag of all players). EDIT #1 To answer your question in the comment below: 

As the moves in your simulation are "random" it will possibly take a very long time till one of the player actually looses (by not bombing itself). So you should limit how long a simulation runs at maximum. You should also use a more complex result evaluation not only based on final results (win/loss, but in case of "draw" you should e.g. subtract the number of bombs that will soon explode nearby). You'll of course need to copy/simulate the whole game state (for all players). Just choose random moves at each time step for each player and simulate them. BUT, it's better if you use an algorithm like "minimax". MCTS can be combined with "minimax", if your game contains randomness (as for example random behaviour for items gathered). Have a look at $URL$ for a good reference... In that case your search algorithm must be implemented recursively (see $URL$ For bomberman you'll need to adapt minimax a bit, if there will be more than 2 players. To quickly give the key idea of minimax, let us look at the following example search tree from the above wikipedia article. 

Assumed your Ackermann drive simulation model is correct, you can do the following for the usual driving situations (where primarily only need to steer direction): 

B. Store "Average Result" (= "sum" divided by "N") for move m Select move with highest average result and execute it 

He just tries through all values 0, 1, 2, 3... and calculates the hashes. If the hash matches, he has found the value for the hash. To prevent this approach to have success, your random number range should be very large (not just 0 to 5 or 0 to 65535, which a player could just quickly check out within short time). A 32 bit number should be fine. This is why the players should exchange the raw random number generator output and not the value "mod 6". If your random number generator does not give you that huge values generate multiple random numbers and combine them to a single big number (use bit-shift operations to combine them; ask, if you need to do that and don't know how to do it... its important to do this right!). But, in most cases the random value range of the generator is big enough... The second method would be to use a lookup table. If you know that the numbers are within a special range you could calculate all the hashes in advance and store them into a database, sorted alphabetically. Later you could just look up the hash to get the value. To prevent this, you can do two things: a) the number range (as above) must be very large so that it becomes impossible to store such a database. b) exchange a random string (contributed in parts by all players, doesn't need to be concealed) in each new round. That random string is to be appended to all the values when calculating the hashes. So you can't build a database before you know this random string - or you had to build a database for any such random string, which would need even more computational power and storage.