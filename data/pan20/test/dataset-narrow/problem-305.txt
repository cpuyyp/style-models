Got it. Well.. I've got one way. Might be kinda slow on a big set: since you can't use rand() in a function, I stole a view from @Pரதீப் in this post: $URL$ and modified it a little bit to look like this: 

On the next page, change the destination to Sheet1$ (or whatever you named the sheet). On the Review Data Type and Mapping page, you can change the data types if you want, so that dates and integers come out properly. On the Save and Run Package page, make sure you Save SSIS package to the File System. I used: 

You probably want to split out the customer table: One for the businesses and one for the people. On tblCarpets and tblOtherProducts, what about a keyword field? You could do some nifty stuff in Java with that. Each table could benefit from a create_timestamp, modify_timestamp and, if you've got a way to track users (a tblUsers), created_by and modified_by. Hide_ind - you might want to hide information from the front end, but keep it on the back end for reporting purposes. You can get rid of 'tbl' in SQL. Its like saying ATM Machine - the M stands for Machine. A discounts table, maybe? How would you track price changes? Do they have their own trucks? How many staff? Do you want to track who sent what where, and how? 

Those drivers are a total pain in the ass to deal with. You are better off using the SQL server data export wizard (or the import one, if you are importing) and saving the package, which you can then execute from a scheduled job. 

Finish the export wizard, and you should be able to go to your shared folder and see your file, full of data. We haven't used the _blank one yet. 4. Create the Job in SSMS: Go down to SQL server agent, right click on jobs and create new job. Give it a name, and specify an owner that isn't you (your SQL server service account for example). Go to steps and add a new step. First thing we are going to do is copy our _blank template into the shared folder. Step 1 for me will be: 

To make every number 10 digits, with zeroes on the left side if it is shorter, you could do it this way: 

Here's how you can join a table to itself randomly and update a column. This method will also maintain the distribution of your data: 

One thing to experiment with is changing your clustered indexes - in a lot of ways they aren't really indexes at all, they are the order in which the data is stored. make copies of your current clustered indexes first, then try adding the columns in the joins & where clause to the clustered index. Clustered indexes are often the same as primary keys, but they don't have to be. your primary key and clustered index can be completely different. Another thing - the - do you need every column from that table? If you only take the columns you need, it might help. Then, there must be a way to stop a query if a website user bails... Other things you can do - archive some data. I'll bet some of your tables are pretty big. Do you need all the data there? With Enterprise edition, you can also partition tables. Check for locking - if you have two users putting this query to your DB at the same time, are they blocking each other? I'm going to catch some shit for suggesting this, but you could join with (NOLOCK) and see if that helps. Oh, and when did you last update your statistics? The query tuning videos on Brent's site are really good, if you need more detail. 

We just discovered another server with a database on it. The database is owned by a SQL account , with the and roles. The other login is , with only . I can only connect through this one. is , and even if it wasn't, we might not know the password. Question: Is there any way to , or create another sql login with ? 

I'm trying to break all the rules of databasing using the stuff function. I want to smush every applicable row into just one, for science, you know? Problem is, I have to write a new function every time. They look like this: 

Step 2 will be the .dtsx package. Change the step type to be SQL Server Integration Services Package, and at the bottom, specify the path for the package, which we saved at 

I discovered today that we have a really ridiculously powerful SMTP server, when I forgot my where clause and sent out 20,000 emails in 4 seconds. How do I put a limit on my database mail to only send out a few dozen emails per minute? 

Today, I saw that our residents have put a bajillion hours into an excel spreadsheet, tracking everything they need to know about our patients. The excel sheet is really good for humans to read from, but absolutely horrendous for machines. I'd like to help them out, but I'm stuck on one part: creating a list of every diagnosis per patient - and presenting it in a single field. I've got this, which will concatenate the entire table: 

You're nearly there. the trick is writing your group by. I don't want to bitch about your question being too long to read, but I definitely didn't read the whole thing. one other suggestion, if you're only putting dates into a dateTime column, consider altering it to a date column. 

But I'd rather use the same thing everyone else uses. Question: What's the right way to do ? Answer: From SpBlitzErik - use Ola hallengren's scripts, you dummy. Followup Questions: 

I do it more or less the same way as you, and I don't get duplicate messages. How are you actually sending the messages out? If you're doing them in batches and getting duplicates, then my guess would be that one batch is starting before the previous batch has been written to the SentMessages table. To get around this, I'd start with smaller batches - or slow down the frequency of the job, so that the last batch has been written as sent before the next batch starts. If that won't work, I'd put it in a cursor that looks something like: 

You can also do it using a date table and ordering it by newid(). I've used this technique to scramble lots and lots of data in the past. One advantage is that you can scramble any field by joining the table to itself on Note: if your person table is bigger than your date table, in this example, loop the date table insert a few times until it is bigger. 

Maintenance plans will do just fine most of the time. Ola Hallengren's scripts will do just fine most of the time. In very rare cases, you might have to grow your own. As Jyao said, it comes down to which you are most comfortable working with. If your co-worker is most comfortable with maintenance plans, why get your knickers in a twist? If he's been databasing for 20+ years, he's already written his own maintenance scripts. Right about the time you were learning to drive, there was probably some young punk in cargo shorts and fliplops that came along and was all like "hey, you old codger, maintenance plans are better - and pull your pants down, you look ridiculous!". Then there was probably a 4 year battle where the uppity youngster slipped in a maintenance plan whenever he could. Now this other young punk with skinny jeans and a freakin bow tie is telling him to go back to scripts. It's enough to turn your hair gray. Three things to consider: Are these examples of Hallengrenite superiority actually applicable to your environment? Is it going to cause you an actual problem if he uses maintenance plans? If you convince him to use Hallengren's scripts and there's an issue, will he be able to resolve it himself or will he have to call you? 

The security folks want all AD passwords to expire every three months. I'm really not excited about this, since I definitely won't remember, and I'll likely be on my boat, drunk, when they expire. Question How often do you/should you change the passwords on your sql service accounts? 

We are building a new server setup following this guide: $URL$ And it reccomends 7x 960GB SSD's in Raid 5. Question: If the databases are on the SAN, what is all this extra space for? 

I've got a bunch of simple SSIS packages that output SQL data to excel, then I rename the files and move them out to the end user. Some of them use file paths with spaces in them, and they aren't excited about using new folders. Here's the code I'm using: 

Finally, schedule the job, then test it by right clicking the job and hitting start job at step. In a few moments, you should see an email that looks something like this: When you need to modify the package, fire up SSIS, open up the file and bash it around a bit. If it's stupid and it works, sometimes its still stupid. Question: How would you do this smarter? 

I've got an old 2005 server where 8/12 of the databases are in . This server has 81 jobs, many of them with unique id's for names. One of these jobs is called , and it calls a .cmd file every 30 minutes. The .cmd calls a .pl that looks like some sort of home-brewed log shipping. I'd like to bump the databases out of standby so I can back them up properly, but I'm worried it will break everything. To make matters worse, while the server is a VM, there is (according to the VM admins) a problem with the drives where it sits, making it impossible to back up. They are worried that the server could die at any time, so we have to do something. The databases are relatively small - I may be able to get in and out in the 30 minutes between the job. There's no test environment, though, so I can't find out easily. We do have vendor support, but they don't seem to be very good. Am I better off just handing them the keys? Question: How would you go about backing up this server without knocking it down in the process?