I've stared myself blind at this. It started as a more complex procedure, but I've stripped it down to the bare bones, trying to make it run. This is the current code: 

This is the information I am trying to recreate, using a TSQL query on the MSX server. I want to see the outcome and history of the jobs by all enlisted servers, similar to what is displayed by the GUI window. I've tried digging through the job-related tables and views of the database, but with no luck. The table on the MSX server contains no history from enlisted servers, and I can't seem to find any good documentation on how else I would go about gathering it. Is this data even accessible through TSQL? Any relevant resource is greatly appreciated. 

Question: In SQL Server 2016, does updating a column to the same value (e.g. updating a column from to ) produce the same amount of transaction-logging as when updating a column to a different value? Read below for more details. We have several SQL Agent jobs running on a schedule. These jobs select data from a source-table (replicated data, linked servers), transform it, then insert/update/deletes the rows of the local target-table accordingly. We have been through various strategies while trying to find the best way to achieve this. We've tried 

This example does not check whether any values has actually changed. If we ignore -checking and sane fallbacks for a moment, this could be checked in one of the following ways: 

When working with virtual machines often it can be very useful to have a base read-only VM then have several smaller VM's that use that larger VM as a base and write their changes to their own writeable copy. Is something similar available in Microsoft Sql Server? The situation I am in is we host a demo copy of our product that our clients can connect to for about 30 days or so. When we create a demo account we have to create a new copy of the base database the program uses and have the software point at that. Each image is a little over 2GB, but on average the demo users will only change about maybe 100MB worth of data in the database. What I would like to do is have a read-only database that acts as a base then have the demo databases create a "differencing database" and write out it's data and log information to that differencing database. Is this possible? I have been searching through the MSDN documentation but I have yet to find anything and unfortunately google searching has been fairly useless as searching for gets polluted with all the pages about doing differential backups (doing did not return any useful results either, all I got was information about differential deployment scripts). 

I looked in to the declaration of and found out the way it invalidates the snapshot is a simple query to update the table 

Reading the article "Dynamic Search Conditions in Tâ€‘SQL" I saw the following statement in the "Using sp_executesql" section. 

For the two former events, I'd store the type of event, the old value and the new value. This works fine as long as I stop here. However, if I want to log the 2 former-events, I would need to be able to store different data-types in the same column. The changes would look something like this: 

We have several tables which we "refresh" frequently by rebuilding them in a staging-table, then performing a metadata-switch to the production-table using the statement. These operations are causing a huge amount of logging, and we're not sure how to handle it. For now we just moved these tables and operations to a new database using the recovery model, but I would love to hear alternative solutions to this problem. How do you suggest we handle the massive log generation as a result of frequent table rebuilds? 

There are two reasons that prompts me to ask this question: tSQLt The T-SQL testing framework tSQLt considers it an issue of "High Severity" when there exists columns with a non-default collation. The author of the test states the following: 

It simply refuses. It's worth mentioning that if I run the code without the parameter-part ( being the sole parameter to the function) it runs fine. I've tried other syntax alternatives, such as 

Now, I'm only a junior DBD and my understanding of how the transaction-log works is very limited. That being said, my seniors have concluded that we cannot use the MERGE or UPDATE statements where all columns are processed in the same statement since it creates excessive logging. The argument for this is that when you perform an -statement in SQL Server, when you set the a column-value and the new value equals the old value, it is still marked as an update in the transaction-log. This apparently becomes costly when you perform lots and lots of pointless SET-operations. In the following example, we update the and of the target-table using values from the source table, joined by . 

XY Problem background info: I have a pull replication publisher in which I want to add or alter a index and have those changes be applied to the subscriber. The solution to that problem I would like to do is to generate a new snapshot and re-init the subscription. 

(Plan XML here) Have I stumbled upon some kind of bug or am I doing something wrong on my end that I should be doing differently? (P.S. I know what the warning means and how to fix it, I am more interested in the warning showing up in one place but not another.) EDIT: Here is the version information for my SSMS from the "About" help page. 

making those 3 columns I am interested in the clustered key. However this violates the "Keep the clustered key narrow" and the "Keep the clustered key sequential" guidelines I have read everywhere. Which of these approaches is the correct way to implement this? If neither are correct please tell me what the correct way to approach this is. 

The problem is from in the first outer join. can contain text in the ENTRY_CODE column, however all records that have set to the line number from where will always be numbers only. I perform the cast as may or may not have leading 0's and spaces so I am trying to get them to be well formed. What appears to me to be happening is adding to the outer query causes the clause not to be evaluated before the cast to int in the inner query. I have tried things like adding to both the inner and outer queries but it has no affect. 

It feels like there must be a smarter way around this, and I would appreciate any clarification and corrections to the statements made in this post. Like mentioned earlier, I'm just trying my best to understand why it has to be this way. Apologies for the lengthy post and thanks in advance. 

In Example #1, the SET-operation will update all columns, even if only 1 column has changed. In Example #2, the SET-operation will update all columns for all rows, falling back to the old value if the value is unchanged. In both examples, all columns are hit by the SET-operation, and, according to my seniors, this creates an unnecessary/problematic amount of transaction-logging when done frequently. The same applies for the -statement. Even if you check a matched row for changes, all columns are hit by the update. 

I am trying to create a log-table for storing events to a -object, and I am afraid I might be taking the wrong route. I've arrived at the conclusion that I should log different data-types in the same column, and it doesn't feel right. I'll explain the basic use-case with 2 tables; and . 

Our current database environment includes a cluster with 3 nodes(one primary and two read-only replicas), as well as a single, independent server standing next to the cluster. For brevity, I'll call the nodes N1, N2 and N3, and the independent server S1. Recently, we configured for our servers, using S1 as the MSX (master) server, and N1, N2 and N3 as TSX (target) servers. This means that SQL Agent Jobs that operate on the cluster-nodes are created and managed from S1. TSX servers report their state, outcome etc to the MSX server, which can be accessed using said . From the , the job-history of each enlisted server (TSX) can be accessed by selecting : 

From what I have gathered the 256 limit is a hard limit for "supported configurations", once you go over 256 some features may fail to work. For example I have a database with 629 merge articles and it works fine, but if I add a filtered article to it it will blow up with a error similar to this error "Message: Too many table names in the query. The maximum allowable is 256." when you try to build the snapshot. 

Would this be the correct approach to doing this or is there a better way to both return the resultset (without included) and update the 2nd table based on the returned rows? 

My Question: How do I, from the publisher, mark a pull merge replication publication as having a invalid snapshot such that if I did the column would return 0. Doing will cause the subscriber to re-initialize but it does not mark the snapshot as invalid. I know I could change a publication or article property then change it back and cause to invalidation to happen that way but I would really like to invalidate as the "primary action" instead of having the invalidation be a side effect of some other action. I am looking for something similar to transactional replication's procedure which has a parameter, but I could not find the equivalent for merge replication. Is there any way to invalidate a merge replication snapshot only without making some other kind of change that has snapshot invalidation as a side effect?