Raid 0 is not mirroring - it is striping. There is no redundancy, if one disk fails you lose everything. Raid 1 is mirroring. You don't get anywhere near twice the speed in practice. 

How big is your intranet? As an alternative you could try the free MS Search Server Express which we use on our intranet (2000 employees). There are some docs on the web comparing the two. 

Do you have a "delete first ask questions later" policy for music and movie files? Or are you more lenient? Just wondering if there's a difference between how large and small organizations deal with the issue. 

The jobs are stored in the system database MSDB. Did you back that up? You need to restore it to get your jobs back. 

My client has a Mac at home and wants to remote control (run apps) on a Windows XP PC at the office. What is the best software to enable this? Any Mac <-> Windows weirdness to be aware of? 

I've got several files in this directory that are non-zero files that haven't been rotated in days. If I run it tells me "log does not need rotating". also only has status for the last time I forced the rotation with -f (which was days ago). Just to make sure logrotate was actually even set to run I checked and the job to run cron.daily is definitely in there and cron.daily had an entry for logrotate. Anywho - IDK what's going on. Anyone have any ideas? 

If the event originated on another computer, the display information had to be saved with the event. The following information was included with the event: NT AUTHORITY\NETWORK SERVICE [CLIENT: ] The specified resource type cannot be found in the image file It may be helpful to note that this seemed to start happening right after we migrated WSUS from another server onto this one. I've been searching all over the internet for someone who has this exact error and I'm not finding anything. (Don't you hate it when that happens?) I've tried completely removing the WDS role and reinstalling it -- didn't help. I suppose I'd really just like some guidance on where to go from here if anyone has any ideas. 

If the servers are world wide (i.e. not on your local 10 Gbps network), then $URL$ might be a solution, too. 

This took 20 minutes on a 50 TB filesystem. Oddly enough most of the time was CPU time and not waiting for disk I/O. It used in the order of 100 GB RAM. Now the file system is usable again: 

/dev/sdam is clearly (hd13). The rest of the drives are a software RAID60. Can I force GRUB to install on /dev/sdam1 (hd13) without probing? 

GNU Parallel instead spawns a new process when one finishes - keeping the CPUs active and thus saving time: 

Installation If GNU Parallel is not packaged for your distribution, you can do a personal installation, which does not require root access. It can be done in 10 seconds by doing this: 

GNU Parallel is a general parallelizer and makes is easy to run jobs in parallel on the same machine or on multiple machines you have ssh access to. If you have 32 different jobs you want to run on 4 CPUs, a straight forward way to parallelize is to run 8 jobs on each CPU: 

If it were me, and I was in that much of a hurry, I'd go get a couple more flash drives and copy them. If you don't already have a few extras lying around they're cheap as dirt now-a-days... The other option would be to set up a PXE server and put the image in there, but that will likely take longer than simply installing them one at a time. Would help out in the future though if you have to do this kind of thing with any frequency. 

/tmp is shared by default. You can probably make it work out of there, but probably better to use a more traditional document root. If you want the files to be cleared out with any regularity you could just clear the directory with a scheduled script. 

This is not a disk space issue as there's plenty available on all disks. We're thinking it could be something with Windows internal database? but I honestly have no clue how to start troubleshooting that. There are other information messages in the Application log for MSSQL that may support this theory? 

I have a server with 2 internal disks with Adaptec hardware RAID and an external disk box connected via SAS. finds all the devices: 

But how do I identify the data to move? I cannot go by age, because the first 10 TB was filled the same day using . I have tried: 

But this does not work because the command is dequoted by ssh twice where as GNU Parallel only expects it to be dequoted once. So instead use again: 

For other installation options see $URL$ Learn more See more examples: $URL$ Watch the intro videos: $URL$ Walk through the tutorial: $URL$ Sign up for the email list to get support: $URL$ 

So and are clearly busy taking up 64% and 53% of a CPU respectively, but not near 100%. The chunk size (128k) of the RAID was chosen after measuring which chunksize gave the least CPU penalty. If this speed is normal: What is the limiting factor? Can I measure that? If this speed is not normal: How can I find the limiting factor? Can I change that? 

I had an existing 2-way DFS-R replication topology set up, consisting of two servers, one was Windows 2003 R2 and the other Windows 2008 R2. This was working fine. Last week I upgraded the Win 2003 server to Windows 2008 R2. It was a VM, so the upgrade process just involved creating a new Win 2008 R2 OS C: drive and attaching the data disks (vmdk files) from the old VM. I then renamed the old Win 2003 VM to server-old and renamed the new Win 2008 VM to the original old name, like so: 

You can try enabling the Traverse Folder/Execute File permission (need to go to Advanced display in the file security properties). At the same time disable read permission. I haven't confirmed this, but give it a go and let us know if it works. 

I need to backup many sites over slow, high latency satellite links. We are talking pings of 650ms to each site. I can get a dump of the initial data by sending out USB disk to site and restoring it at central office. From then on I intend to use rsync or DFS-R for byte-level incremental copies across the link. All the machines are Windows 2003 SP2 R2. I have read that rsync can hang on large files and is still a bit flakey for Windows? Alternatively should I use DFS-R which also does byte-level copies? I have tried DFS-R in the past and was not impressed by the lack of logging, it was very hard to find out what was actually going on. That's why I'm interested in rsync. Has anyone any real world experience of both methods? 

I have added it in /usr/lib/zabbix/externalscripts where I have a different script, that works. I have configured an item: 

I have recently transferred 10 TB through a 1 Gbps connection. The major problem was keeping the 1 Gbps filled at all time. That was no problem when transferring big files, but proved to be a problem when transferring small files as the sender could not seek fast enough. The solution was to run multiple transfers in parallel. A few with big files and a few transferring the rest. It was based on: $URL$ If your files are compressible, make sure to compress the transfer (rsync -z). In theory you should be able to use GNU Parallel and rsync on Windows 7, but even if you cannot you can probably use the idea of transferring big files in parallel with small files. 

Every VM conference I've been to has the VMware techies claiming that there is no type of server that can't be virtualised. They claim large Oracle VM installations that handle 3 times the amount of daily Visa credit card processing (this is with VShpere 4.0). But they live in an Ivory Tower. For us in the real world I would avoid virtualising database servers. Otherwise the DBAs will start accusing you of "slowing down their server" every time something happens (whether it is VM fault or not). It's not worth the hassle. 

If you have I'm interested in the user experience and admin experience. Did users prefer Outlook over Notes? Did the sysadmins prefer Exchange over Domino? If so why? 

I always thought the fact that you could change NT Workstation 3.51 to NT Server with a registry change was pretty cool. And says everything about Microsoft's market segementation strategies. 

Here's our scenario. Windows 2k8 print server hosting a large Ricoh multi-function color laser printer for everyone in the office to use. Printers are NOT deployed with Group Policy. Management wants the printer to default on printing black and white, but still allow people to print in color if they need to. Setting the default in the driver will work for a freshly added printer, but as soon as someone changes the setting to print in color on their client, it stays that way until they manually change it back. Ideally, we would like it to forget whatever settings the user applied when they close whatever application they're printing from, but at the very least, I'd like it to reset these settings on login. Is there a 'right' or 'easy' way to accomplish this? The only thing I can think of that I know will work, is pushing a login script that deletes and re-adds the printer... but this seems... hacky. My google-fu on this topic is evidently not very strong. I appreciate whatever help you can give me, thanks! 

You could still run a local login script on each pc, that connects to a "master" pc with the software on it and installs it from there. Checkout gpedit.msc and go to User Configuration -> Windows Settings -> Scripts (Logon/Logoff). Edit the Logon item and point to your script (batch file would be ok). The user would need admin rights locally and rights on the master pc. 

Easiest method is to run newsid.exe from Sysinternals on each distributed VM. It changes the SID (and optionally renames the PC) so conflicts don't occur. It's best to create the VM while the guest is an a Workgroup. Then distribute the VM, run newsid.exe and then add the guest to the domain. Alternatively you can use Sysprep to prepare the VM, it achieves the same thing and is more of an automated process. 

So you are running around 600 jobs per second. The overhead for a single GNU Parallel job is in the order of 2-5 ms, so when you are getting more than 200 jobs per second, GNU Parallel will not perform better without tweaking. The tweak is to have more s spawining jobs in parallel. From $URL$ 

The problem was not in the Linux end but in the storage end. What was needed was assigning Linux as an initiator on the storage device and disable LUN Masking. After doing that I simply to force a rescan. Then the disks showed up in : 

But I do not want to spam the app people if network is down or the server people if the application is down. So I need a way to express dependency on alarms: ping < login < app How do I express these dependencies?