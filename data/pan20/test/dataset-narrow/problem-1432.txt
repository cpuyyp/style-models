Create a grid around the player that contains a lot of "visibility points" (my game is semi tile-based so I create one point for every tile on the grid) - the size of the square's side is close to the radius where I make objects transparent. I found 6x6 to be a good value, so that's 36 visibility points total. For every visibility point on the grid, check if that point is in the player's line of sight. For every visibility point that is in the LOS, cast a ray from the camera to that point and mark all objects the ray hits as transparent. 

I'm working on a 3d game that has a view similar to classic isometric games (diablo, etc.). One of the things I'm trying to implement is the effect of turning walls transparent when the player walks behind them. By itself this is not a huge issue, but I'm having trouble determining which walls should be transparent exactly. I can't use a circle or square mask. There are a lot of cases where the wall piece at the same (relative) position has different visibility depending on the surrounding area. With the help of a friend I came up with this algorithm: 

You didn't specify what 2d means exactly in your situation. If you're doing 2d in a 3d engine, you could just render the planet with an ortographic projection. If you don't want to do that, but you can still render 2d shapes with custom texture coordinates and shaders, here's an idea (although a little complicated): 

If you absolutely want to do this, you can use a custom memory allocator that reserves a chunk for each Entity and places its components continously inside that chunk. However this may waste memory again, which can (in theory) cause more cache misses too... However... unless you have solid proof that this causes slowdowns for you, don't worry about it. Premature optimization can eat up a lot of your time, not only when creating the system, but also when maintaining it later. Most likely your game engine's speed isn't going to be determined by trivial things like this, not to mention that complex components (the ones that may actually slow things down for you) are most likely larger and would cause cache misses anyway. Having said that, your particular example may be somewhat valid. I think the position of an entity should be bundled together with the entity itself (part of the class, not a component) for two reasons. First of all, if you do a lot of data processing on the entities, it may improve performance. But more importantly, each entity can only have one position, and the possibility of adding several position components breaks this rule, causing problems. 

This is called vsync (vertical sync), which traditionally means that your rendering rate is synchronized with the vertical refresh rate of your monitor to avoid tearing. Nowadays LCD screens don't have "vertical" refresh rates, just simple refresh rates, but it's the same thing. As others have said in the responses, your video card driver settings cause this. Things to look out for: 

I believe having a lot of ifs in a fragment shader could have a serious performance impact. You could probably use the stencil buffer for this purpose. Or save up to 4 light types in the color channels of a separate rendertarget and do this: fragColor = shadingFun0(...) * texColor.r + shadingFun1(...) * texColor.b and so on. Or as a hack, you can manipulate the normals of some objects, so the light equation returns a different color. Or just do multiple passes, one for each light type. So one deferred shading pass, and then you just draw the other objects on the resulting FBO (keeping the depth buffer, I guess). Also I'm not sure about this, but isn't the whole point of using deferred shading that you can use the best lighting model for everything? 

In theory draw order only matters when using multiple draw calls. Since video cards have a parallel architecture for vertex and fragment processing, the rendering order inside of a given mesh is non-deterministic. Of course this means that your method may work on some or even all of your target devices, but trusting undocumented and somewhat random features is usually a bad idea. So you basically have to make a choice between multiple draw calls or some overdraw. However you may not have to worry about it - even mobile graphics cards can render way more than you would expect nowadays. Some mobile architectures like PowerVR even have special handling for this exact case to reduce the overhead. You can also try only rendering part of the mesh via occlusion culling or some other method, while having Unity's static batching enabled. But usually heavy premature optimization is a bad idea. 

The general solution to this problem is to have an update function with a completely fixed timestep. Unity calls this FixedUpdate, in physics engines you can run the simulation several times before updating the world, etc. but it's all the same concept. While this fixed update function will always be slightly out of sync with everything else in your world, if you select a small enough timestep (1/60 or 1/30 or so), this will be unnoticable. Another thing to remember is that you only need to use this update function for things that need to be absolutely deterministic, so it doesn't hog your cpu even if you need to run it several times before updating the world. And if it's not completely obvious, this function should have absolutely nothing to do with your system clock - this runs on a "simulated" clock that you keep track of, and that never actually steps over the real system clock, but is otherwise independent. Having said all this, in your case there may be simpler solutions. You can pre-calculate the path for your unit and interpolate on that path. The idea behind this solution is that you think of your unit's movement as a continous function instead of a physical simulation. Alternatively you can try to "snap" your ship to the correct position after every movement, so the arithmetic error doesn't add up. However this may cause visual glitches if your fps is low or not constant. 

At asset build time create a 2d projection of your 3d earth save it as a 2d shape, and (most important) save the texture coordinates render your 2d shape in the background with a custom shader and use texture scrolling to move your texture on it, creating a rotation effect. Since you're using the results of a projection, the texture coordinates will guarantee the proper distortion. 

A framework is a collection of (usually) lower level libraries and helper stuff that you can use to do whatever the hell you want (graphics, sounds, etc.). There is nothing game-related about a framework except they're usually optimized or designed to do things that are common in games. Example: an engine allows you to have a list of entities, each with a position on the map. A framework allows you to render a 3d object at a certain position. So you connect them by giving each of your entities a 3d object, and render them when needed. And ta-da, you have a game. 

Normal maps are mapped using the so-called tangent space, which is essentially a local space based on the model's texture-space. This should answer both of your questions. It's not viewpoint dependant because this space has nothing to do with the camera. In the normal map, Z is the up direction. If you look at the normals of a model, most of the normal vectors will be pointing directly out from the mesh. The mesh's surface is the texture space I was talking about, so in that local coordinate system, up is the "outward" direction. 

If nothing else, you can manually calculate shadows using techniques similar to low-resolution ray tracing, then somehow put them on screen. I think this would be fairly easy, since unity supports hit detection with custom rays and all scene objects that have a collider, but of course this is highly inefficient. If you can't afford unity pro (not even the student version), I suggest looking at other 3d or game engines that satisfy your needs better. 

You have to use shaders. And I also believe the arguments of glDrawArrays are wrong - you need to pass the number of indices as the last one. Oh, and you're using fairly large values for the triangle's position, but you have no view and projection matrix set up, so they will be way outside the screen. As for the flickering, you need to call glClear per draw, not just once. 

It is possible, you just need to scale it to the 0-1 range, and make sure to avoid precision issues. For example if you have 6 pixel types, you can do 

Implement the following methods: Add(point), Remove(point), Move(point) Move is not completeley necessary as it can be simulated by Remove and Add but that's less efficient. Use the following mouse events: press -> Add, release -> Remove, drag -> Move I assume you implemented your Remove and Move functions recursively. Return a boolean value from each Remove call in your Node, so you will know exactly which recursive path removed a point. If this value is true for a certain node, check if that node has any elements or child nodes inside. If it has neither, undo the subdivision. Another approach may be to track the number of points inside each node (including children). This trades some computation time for memory and may actually be more efficient. And a side-note: your example is not the best to demonstrate a quadtree. The idea behind quadtrees is that you only subdivide once a node has reached a certain number of points inside. If you only track one position (mouse pointer) you don't even have to subdivide at all. Instead, create a number of points that move around randomly. 

I assume you're talking about running the code on the end user's machine, not just for testing. You definitely should not call glGetError after every GL function call. You cannot know how it is implemented in certain drivers or how it will be implemented in the future. As a practical example, on NaCL (and possibly anywhere using Google's WebGL implementation on Windows) it's terribly slow as it halts some kind of multithreaded execution. Ideally, of course, you shouldn't need to call it in release mode ever, because there shouldn't be any bugs in your code, and all dynamic input should be validated upon loading. Are you sure you cannot guarantee this? Remember that having a graphics error is about as likely as having any other kind of programming error, yet you don't attempt to run some kind of debugger in release mode. If you absolutely need to call it, you could call it once per frame, and if you catch one, call it once per GL call and see if you can pinpoint the error. But honestly this method may fail anyway. Alternatively you could have the end user decide to run in "error report mode" or whatever you may want to call it. 

This algorithm works - not perfectly, but only requires some tuning - however this is very slow. As you can see, it requries 36 ray casts minimum, but most of the time 60-70 depending on the position. That's simply too much for the CPU. Is there a better way to do this? I'm using Unity 3D but I'm not looking for an engine-specific solution. 

You need to have money and you need to have a publisher with several published AAA titles. It is indeed a special piece of hardware - nowadays not so much because of the actual hardware (which is slightly different), but because it gives you more freedom when running games (dev mode, debugging, etc.) It's pretty pointless for indies anyway. However if you only want to experiment with native 360 stuff and not release a game, I believe there are some legally gray areas that allow you to do that. 

Of course I would suggest setting up a define in the shader, possibly programatically. When reading from the texture just do the exact opposite, multiply it by 5 and round the result to an int. You're probably safe from rounding and precision issues unless the number of possible pixel types is really high (I'd start worrying above 127). You also need to make sure that you write each fragment exactly once (no alpha blending), otherwise you'll get some kind of interpolation between pixel types, which may not be something you want to achieve. 

60 fps in focus mode is very suspicious, it must be vsync. Why vsync is turned off when the app loses focus is not a question I can answer though. 

If your first question is strictly about pathfinding, I believe you can simplify the problem by defining some preconditions. For example: each chunk is connected to every adjacent chunk by a single door. This door is never blocked and is always walkable. In this case to find your way anywhere you need to find two paths: one global, the sequence of doors to travel, and a lot of local ones (these can be generated when your agent enters a chunk and thrown away when it leaves it) for each chunk to travel to the next door. If you need anything more complicated, this is a starting point. You can easily modify this idea to have multiple doors, shortcuts between chunks, blocked doors, chunk penalties, etc. Of course there are other solutions to this problem too - small local graphs that include multiple chunks and are generated dynamically, or even pre-generating the graph for your entire world, and loading parts of it on demand. Each solution has its drawbacks, and you also need to consider which algorithm you're using. A* using Eucledian distance for its heuristic function is always a good idea, as it will keep look-ups very close to the origin in most situations, especially if you accept sub-optimal solutions. 

I assume you're talking about mobile, and a 3D game. If either of these is untrue, you should be perfectly fine in any situation. I'm currently working on a 3D tile-based mobile game. I've written my own tile engine for Unity (note: I didn't write my own renderer, I honestly don't see the point). In the game the camera rotation is fixed and it's ortographic, which allows some optimizations. While having a 200x200 map is easily possible (and has been done) with my system, older phones like my Samsung Galaxy S are simply too slow to use it, even after optimizations. Fps can be between 5 and 20 which means it's not enjoyable. One important thing I noticed is that after optimizing graphics a lot (custom static batching, throwing away invisible sides, generating a custom terrain mesh, etc.) the choke point wasn't really graphics. Since with isometric games you have a pretty good idea of what the player sees at each point, it's very easy to minimize the amount of data that needs to be loaded. So right now the weakest link is the amount of units and non-static stuff on the map. Pathfinding is acceptable with a pre-calculated graph (using A* from Aaron Granberg). For collisions I use an optimized model with boxes spanning multiple tiles (pre-calculated) and unity's physics engine, but I don't do any dynamic body stuff, only checking for collisions in certain situations (player movement mostly). So the final answer is: yes. You may have to cut back on the features, but it is definitely possible. 

Your problem may be backface culling. Google it. EDIT: Now I see that you mentioned it, so nevermind. 

Terrain mesh generation is a general term for the process of... generating a mesh for the terrain. Ok, let me elaborate. A mesh is a generic term for a 3D model. It usually consists of vertex positions (set up as triangles), normals, texture coordinates and other information required to render the model properly. If you don't know how meshes work, you need to look into this first - this is part of the very basics of 3D rendering. So, if a mesh is just a 3D model, why are terrains so special? For several reasons. Terrains are usually not edited entirely by hand. Even if you want to design a specific part of your terrain by hand, most of it will be procedural (computer generated based on certain inputs). This is also important because terrains can be very large (pseudo-infinite), so creating it by hand is even less feasible. Proceduralism is a great form of compression: even infinite (or really large) terrains can be compressed down to a few integers. Terrains also have another important property: most of the time you cannot go under them, so they can be described as a surface instead of a volume. The most common representation of this is the height-map: for each coordinate, you define the height of the terrain. This heigh-map can be procedurally generated, manually edited, infinite or finite. The heightmap also implicitly holds the surface normals. So the process of terrain mesh generation most often refers to the process of taking the "compressed", procedural, perhaps height-mapped representation of the terrain and turning it into a 3d mesh. Of course the exact procedure depends on the system: you can generate only a few chunks of the terrain that is visible to the camera, you can have a dynamic LOD/tessellation system that makes the most visible parts more detailed than the rest, etc. So it really is just a generic term.