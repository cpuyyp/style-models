(wow! after three years of time passing, this is now easy to answer. funny how that goes! --Daniel) This "Learning with (Signed) Errors" (LWSE) problem, as invented-and-stated above by me (three years ago), trivially reduces from the Extended Learning with Errors (eLWE) problem first introduced in the work Bi-Deniable Public-Key Encryption by O'Neill, Peikert, and Waters at CRYPTO 2011. The eLWE problem is defined analogously to the "standard" LWE (i.e. [Regev2005]), except the distributions' (efficient) distinguisher is additionally given "hints" on the LWE sample's error vector $\vec{x}$, in the form of (possibly noisy) inner products with an arbitrary vector $\vec{z}$. (In applications $\vec{z}$ is often the decryption-key vector of some cryptosystem.) Formally, the $\mathsf{eLWE}_{n,m,q,\chi,\beta}$ problem is described as follows: 

$(PK, SK) \leftarrow Gen(1^{secparam}; coins)$: generates encryption/decryption keys $c \leftarrow Enc(PK, m; coins)$: encrypts message $m$ as ciphertext $c$ under key $PK$ $m \leftarrow Dec(SK, c)$: decrypts ciphertext $c$ using key $SK$ to message $m$ $c^* \leftarrow Eval(C, c_1, ..., c_k)$: given ciphertexts $c_1, ..., c_k$ and a circuit description $C$, computes $c^* = Enc(C(m_1, ..., m_k))$ 

where "somewhat-homomorphic" means $Eval$ can only correctly (and succinctly) compute ciphertexts $c^*$ when the circuit $C$ has bounded depth (in some well-defined sense). Correctness just means that w.h.p. over honest $(PK, SK) \leftarrow Gen$, for all $C, \{m_i\}_i$, we have $C(m_1, ..., m_k) = Dec(SK, Eval(C, Enc(PK, m_1; coins_1), ..., Enc(PK, m_k; coins_k)))$. I.e. that if you use the scheme 'honestly,' you get correct decryption of (possibly $Eval$'d) ciphertexts. 

It is easy to see that eLWE captures "the spirit" of LWSE, though a formal reduction can be shown with not too much additional effort. Major follow-up ideas toward understanding the Extended-LWE problem are developed in the works: 

Regarding the Extended Church-Turing Thesis (meant as "A probabilistic Turing machine can efficiently simulate any physically computable function."): One possibility is the difference between classical and quantum computers. Specifically the question, "Is there a task that quantum computers can perform that classical computers cannot?" A recent ECCC report by Scott Aaronson (see Conjecture 9 on page 5) highlights a conjecture that, if proven, would provide strong evidence against the Extended Church-Turing Thesis. If one were to disprove the Extended Church-Turing Thesis, it could look like that -- specifically, by demonstrating an efficiently computable task that a (classical) Turing machine cannot efficiently compute. 

Note that once we finish work on one possible assignment, we don't need any information from it again to compute whether other assignments satisfy the formula; the only information we keep is whether it was satisfied -- in the second counter. In particular, since we can just apply binary addition of 1 to the assignment we are testing in order to always get the next assignment, and since every assignment can be checked by considering only one clause at a time (and re-using the same space for computation on every clause), the dominating factor in the space requirement is simply writing the formula itself. Alternatively, we just read from an input tape and never explicitly write the formula, then in the worst case, $k = n$ and it still takes $O(n)$ space to write the possible assignments and keep track of the number of satisfying assignments. Q.E.D. 

Or more generally -- what's going on here? Why is one (highly cited) paper (I use the following very liberally) "allowed" to restrict the possible protocols to round-robin communication patterns only? 

The decision problem of planar graph 4-colorability is in P. But obtaining the lexicographically first such solution is NP-hard (Khuller/Vazirani). Note that the property you're really interested in is self-reducibility (or rather, non-self-reducibility). In the planar graph coloring problem, the essential issue is that the method of self-reducing the general case of $k$-colorability will destroy planarity in a graph. 

Brief Background In Multi-Party Protocols by Chandra, Lipton, and Furst [CFL83], a Ramsey-theoretic proof is used to show a lower bound (and later, a matching upper bound) for the predicate Exactly-$n$ in the NOF multiparty communication complexity model. From the paragraph at the top of the second column of Page 1, we can see that they define the model such that the communication is strictly cyclic: e.g., for parties $P_0, P_1, P_2$, $P_0$ broadcasts at time $t=0$, $P_1$ broadcasts at time $t=1$, $P_2$ broadcasts at time $t=2$, then $P_0$ broadcasts at time $t=3$, and so on. In most other papers, this cyclic ordering restriction is not made. For (arbitrary) example, in Separating Deterministic from Nondeterministic NOF Multiparty Communication Complexity by Beame, David, Pitassi, and Woelfel [BDPW07], a counting argument over protocols separates $\bf{RP}^{cc}_k$ from $\bf{P}^{cc}_k$. By their definition, "a protocol specifies, for every possible [public] blackboard contents [i.e., broadcast history] whether or not the communication is over, the output if over and the next player to speak if not." (emphasis added) Importantly, the proof technique in [CFL83] appears (to my eyes) to crucially depend on the parties speaking in a cyclic/modular fashion. Question Allow me to play Devil's Advocate: 

Update: The obstruction set (i.e. the NxM "barrier" between colorable and uncolorable grid sizes) for all monochromatic-rectangle-free 4-colorings is now known. Anyone feel up to trying 5-colorings? ;) 

At a high-level (ignoring the messier details), recryption that boosts bounded-depth homomorphism to unbounded-depth homomorphism works as follows: Suppose you have a public-key "somewhat-homomorphic" encryption scheme with procedures: 

Disclaimer: Bill Gasarch has a $289 (USD) bounty on a positive answer to this question; you can reach him through his blog. A note on etiquette: I'll make sure he knows the source of any correct answer (should one arise). He brought it up again during a rump session at Barriers II, and I find it interesting, so I'm forwarding the question here (without his knowledge; though I highly doubt he would mind). 

For an integer $q = q(n) \ge 2$, and an error distribution $\chi = \chi(n)$ over $\mathbb{Z}_q$, the extended learning with errors problem is to distinguish between the following pairs of distributions: $$ \{\mathbf{A}, \vec{b} = \mathbf{A}^T\vec{s}+\vec{x}, \vec{z}, \langle\vec{z}, \vec{x}\rangle + x' \}, $$ $$ \{\mathbf{A}, \vec{u}, \vec{z}, \langle\vec{z}, \vec{x}\rangle + x' \}, $$ where $\mathbf{A}\leftarrow \mathbb{Z}_q^{n\times m}, \vec{s}\leftarrow\mathbb{Z}_q^n, \vec{u}\leftarrow\mathbb{Z}_q^m, \vec{x}, \vec{z}\leftarrow\chi^m,$ and $x'\leftarrow\mathcal{D}_{\beta q}$, and where $\mathcal{D}_\alpha$ is the (1-dimensional) Discrete Gaussian distribution with width $\alpha$. 

Given $a_1, ..., a_n, T\in\mathbb{Z}_M$, where the $a_i$ are chosen uniformly and $T$ is a sum of a random subset of the $a_i$, find a subset of the $a_i$ that sum to $T\pmod M$. That is, given the $\{a_i\}$ and $T = \sum_{i\in S} a_i\pmod M$, for some (unknown, random) $S\subseteq [n]$, find $S'\subseteq [n]$ such that $T = \sum_{i\in S'} a_i\pmod M$. ($S = S'$ is one possible solution.) 

I believe one core difference is that in CSP, processes synchronize when messages are received (i.e. a message cannot be sent from one process unless another process is in a receiving mode), while the Actor model is inherently asynchronous (i.e. messages are immediately sent to other processes' address, irrespective of whether they're actively waiting on a message or not). There should be another answer that is more well-developed, however. 

The "analogous" concept appears to be a manner of computing the communication complexity of solving a given protocol with access to different types of resources, but stops just short of defining proper communication complexity classes... Most of communication complexity seems to be relatively "low-level," in the sense that the overwhelming majority of results/theorems/etc. revolve around small-ish, specific, polynomial-sized values. This somewhat begs the question of why, say, $NEXP$ is interesting for computation but the analogous concept appears to be less interesting for communication. (Of course, I could just be at fault for simply being unaware of "higher-level" communication complexity concepts.) 

And here is yet another, even more recent, option for $\mathsf{PPAD}$-hardness, via private-key functional encryption: From Minicrypt to Obfustopia via Private-Key Functional Encryption 

In the spirit of your comment about "pretty good but not necessarily optimal," I present the following idea with absolutely no guarantee of optimality! For completeness, here is the pseudocode that you referred to (Remark: the algorithm linked assumes edge capacities are integers between 1 and C and that flow and residual capacity values are integral): 

Since integer factorization is known to be in both NP and co-NP, a proof that it is NP-complete would imply NP = co-NP, which is considered highly unlikely. There is an interesting discussion at this old post by Lance Fortnow. 

For the sake of completeness, let me update this answer with a direct proof. My original answer will remain below in case anyone finds it interesting. The basic idea of the direct proof is exactly as Evgenij suggests: check whether each possible assignment is satisfiable, keep a counter of satisfiable instances, and reuse space wherever possible. Claim: #$P \subseteq FPSPACE$ Proof: Assume #$SAT$ is #$P$-complete; then it suffices to show a polynomial-space algorithm to solve #$SAT$. Here is a linear-space, exponential-time algorithm. Initialize two counters of $k \le n$ bits to all 0, for number of variables $k$ in the $3CNF$ formula given as input. The first counter will keep track of which assignment we are checking (1 = true, 0 = false, for each variable $x_1$ to $x_k$). The second will keep track of the number of satisfying assignments (there are at most $2^k$ of them). We will also need enough space ($n$ bits) to write the formula, and enough space to do "scratch work" for one clause (some very small constant number of bits, since every clause will have exactly 3 literals by definition of #$SAT$). Here is the description of the algorithm: