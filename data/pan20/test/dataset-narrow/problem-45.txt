Render frame #0 to FBO A. Render frame #1 to FBO B, taking FBO A as history (i.e. hitherto accumulation) input. Render frame #2 to FBO A, taking FBO B as history input. Repeat from point 2. 

In particular, you could have a moving average in order for the particles to leave nice trails like that. Check out Brian Karis's presentation on Unreal Engine 4's temporal supersampling - slides 15-17 describe the moving average-based blend weighting. 

If you really need this level of debugging (spoiler: you don't, more on that later), AFAIK only NVIDIA's NSight can provide it. However, in order to remain interactive, you probably need two machines - one to actually run your shader, and the other to debug it - because stopping a GPU for debugging will - quite understandably - freeze the display of the computer, making it unusable. One step down from that is RenderDoc and its ability to swap shaders in a capture on the fly, as well as step through Direct3D bytecode and inspect its registers (i.e. a virtual machine for the DXBC, it's most often sufficiently detailed to actually find shader bugs). And what most graphics programmers do most days is simply what I like to call "your mom's debugger", which on the CPU means sprinkling your code with print statements with the intention of later reviewing the log; but in shaders, you just stuff your shader outputs with debug values instead and inspect those values. There are also ways to "print text" from shaders (for instance, this ShaderToy). 

Buffer objects contain your vertex data. Vertex array objects tell OpenGL how to interpret that data. Without the VAO, OpenGL just knows that you suck some bytes into some buffers. The VAO says that "in this buffer, at byte offset X, is an array of 4-vector floats, and that data will be fed to attribute index Y." They're state objects. VAOs are no more "magical" than this C++ data structure: 

is for when you want to make writes from one rendering command visible to reads from a subsequent rendering command. That is, you can do all the atomics you like during a rendering command. But without the barrier, there's no guarantee that any of those atomic actions will be visible to later commands. Yes, they will still be atomic, but this only ensures that they will each execute in some order and each operation will fully execute before the next. What they won't ensure is that operations which OpenGL defines has happening "later" will execute before earlier operations. Sometimes, you actually don't care about the order of the operations; being atomic may be enough. And sometimes, you do care. Consider the case of a "linked-list" style blending buffer. Each pixel in the image is an integer that represents one of the items in the list. Each FS invocation will use an atomic increment to get a unique identifier, then swap that identifier with the current value in the image. It writes its pixel data to the entry in an SSBO, along with the identifier it swapped out of the image. When doing that atomic increment on the unique identifier, you don't care if FS invocations from two different rendering commands execute out of order. You simply need the operation to be atomic. Similarly, the swap doesn't have to be in order; it just needs to be atomic. As long as they all execute, and the atomic operations do their jobs, then they will be fine. Now, when it comes time to do blending based on this data, that is when you need to be able to read all of the data that was previously written. And that rendering operation requires . Of course, you're not necessarily using atomic operations to read these values. 

Your geometry may be prepared for Direct3D, which has texture space origin placed differently (top-left vs bottom-left corner). Try flipping either the texture's Y coordinates (i.e. load it from the bottom to the top), or the V texture coordinate (). 

I have no knowledge of SceneKit whatsoever, but the effect appears to be a simple post-process Gaussian blur of the app icon grid, with the blur radius increasing over time (i.e. increasing the blur amount) during the transition, with a touch of colour correction tacked on (which causes the darkening). It should be easy enough to implement in any graphics API that allows rendering to textures and performing image filtering on them (for instance, in OpenGL you could use framebuffer objects to achieve this). You can have a look at an example WebGL implementation here. It's worth noting that it is a separable filter; meaning, it's a rectangle-shaped filter, but you can avoid sampling the same pixels (overlapping neighbouring samples) multiple times by splitting it into two passes – a horizontal and a vertical one – to the same effect. 

No, you do not. Buffer swapping does not necessarily mean that the current contents of the front buffer are preserved. When you do a swap, the contents of the back buffer become the front buffer, but the state of the back buffer is undefined after the swap. Some implementations use a true swap, where the two buffers really are switched. Other implementations copy the data from the back buffer on a swap, leaving the back buffer as whatever was there before. Still others could do something else. 

How much memory, of each kind, is available. How big each kind of resource will be. Textures of a particular format and size take up X bytes, require Y alignment, etc. 

BRDFs are functions the define the distribution of the way light reflects off of a surface. They effectively answer the question, "given a light source here, how much light is reflected to me?" Well, that question has the two directions right there: the direction from the surface point to the light source, and the direction from the surface point to "me": the viewer. BSDFs and BTDFs work in the same way, just asking slightly different questions. But the givens to those questions are the same: a light source and a viewer. And thus, they use the same directions. 

What you are describing is supersampling, a technique for combating aliasing (a.k.a. jaggies), which is a term from signal theory that also applies to graphics. It's a spatial domain approach. Basically it means that the sampling resolution (render target pixels in this case) makes signals with different frequencies be reconstructed as the same frequency. Or, in other words, the scene is sampled too infrequently for sub-pixel detail to be properly represented. It is especially prevalent in modern games since the recent advances in GPU computing power have made that quality/performance tradeoff (that you deem unjustified, but unless we're talking VR, everything above 60 FPS is just buttery-smooth and undistinguishable) even possible, and because the (fairly) recently introduced physically-based shading models are much more susceptible to aliasing from sources other than just undersampled geometry: a group of artifacts collectively called shading aliasing, with specular aliasing (a.k.a. fireflies) being especially visible. Epic Games' Brian Karis explains some of that in the opening of his presentation on temporal AA in UE4 (temporal AA is just another approach to anti-aliasing, one that distributes samples in time instead of space). To sum up: it's the brute force approach to fixing aliasing. An anti-aliasing technique that simply captures more samples of the scene, and thus captures more sub-pixel detail that would otherwise be lost. It is also worth mentioning that while it appears superficially similar to Multi-Sampled Anti-Aliasing, it differs in the sampling pattern (SSAA simply increases the pixel grid density, whereas MSAA has a sort of sheared grid pattern) and the sampled signal resolution (SSAA is a full colour capture, MSAA cheats slightly by sampling triangle coverage more finely than colour). 

It doesn't. There is no transfer or update in that fashion. All memory allocated through a Vulkan device represents memory that some device operations can read directly. If a Vulkan device advertises that it can use visible/coherent memory as source vertex data for rendering operations, that does not mean that some kind of upload is going on to make this happen. The device is reading that data from the memory you write to. The function merely expunges writes from CPU cache lines, so that they actually reach the memory. Memory that is host-coherent and host-visible would represent memory that either: 

Invalidation is a mechanism to allow the implementation to not delay the act of mapping the buffer (due to prior GPU commands that read from it) if you're going to overwrite all of the area in the buffer. The implementation can allocate a chunk of memory, and you can write to that. Then, it'll schedule the DMA from that chunk into the actual buffer storage when you unmap it, handling the synchronization operations as well. For whole buffer invalidation, it can allocate GPU-accessible memory and completely replace the buffer's storage. Of course, this only matters if you're mapping frequently. If you're using persistent mapped buffers for streaming (and you should be, if you're using DSA), then you don't care about invalidation. You're going to handle synchronization manually. 

This type of artifact is a tell-tale sign that you've messed up your texture image format at the byte level. It's hard to tell what exactly is wrong, but it's something along the lines of bad pitch or a component layout mismatch. Have a read of this wiki page and try again. should work, keep trying until it does. 

In computer graphics, one rarely uses the term "frequency" to refer to the inverse of wavelength of light. Usually, the meaning from signal theory is implied instead, i.e. the frequency of signal, or detail. "Low-frequency information" in this case means that diffuse lighting is usually "blurry," i.e. it does not carry high-resolution detail, and is approximately the same, or has a smooth gradient, over relatively large regions of space. To put it simply, "low frequency with respect to the distribution of $ L_i(l_k) $" intuitively means "the incoming light signal has low detail". 

You are essentially correct in assuming that it is technically feasible to distribute the rendering computation workload. It is, essentially, a computation workload like any other; possibly even better-suited for parallelization due to its very nature: many similarly structured units running the same code path (i.e. vertices, pixels). What you are forgetting, however, is latency. Having all the render power in the world is useless if the image does not arrive to the HMD fast enough not to cause discomfort to the user. Here is an article that explains the importance of motion-to-photon latency: $URL$ 

Well, it's unlikely that 10,000 cubemaps could fit into memory at all (at 128x128x4-bytes-per-pixel, 10,000 cubemaps would require upwards of 4GB of RAM). But as far as the mechanism to use a large number of such textures, that's pretty easy. You have two options. The option that's widely available is to use cubemap array textures. You allocate the storage for the array cubemap with the number of cubemaps you want * 6. Each set of 6 array layer-faces is a single cubemap. In your shader, you just provide an array index for the cubemap layer (not layer-face) that you want. The other option is to employ bindless textures. You create however many cubemaps you want, then provide them to the shader as an SSBO array of 64-bit integer handles. You index this array as needed and convert the handle into a sampler value before fetching. 

Drawing back-to-front is not optional with blending (unless you use an order-independent blending technique), so getting the visual effect you actually want has to trump performance. But drawing front-to-back is purely a performance thing. Specifically, it makes it possible for early-Z to save you fragment processing time, as earlier objects are in front of later ones. So it's a GPU performance improvement. Batching is a performance improvement for the CPU, in that you're spending less CPU time doing state change stuff. So you need to decide which is more important: CPU performance or GPU performance. With deferred rendering, saving FS computation time also means saving memory bandwidth. But the general construction of your scene, what you're actually drawing, can help inform which is more important. Oftentimes what happens is that you render static terrain first, perhaps using your visibility system to order what you render coarsely front-to-back. Then you render your dynamic objects. Because dynamic objects usually require a lot of state changes, you won't necessarily draw all of them in a single batch (though there are techniques you can use to hide even texture changes). In any case, the point is that there is a tradeoff. And the one you pick depends on exactly what you're rendering. A top-down game like StarCraft or DotA would never bother with front-to-back sorting; you just render the terrain after the characters. A tight-quarters FPS by contrast would probably really need to employ such techniques, particularly for terrain. 

Both the number of octaves used and the Gaussian blur kernel size affect the end result in terms of visual quality and performance, so you may need to do trade-offs. In other words, the glow effect that you seek is usually simply the original scene image, but thresholded and blurred, superimposed back onto the scene image. 

Render your scene (preferably in high dynamic range) to texture. Make a thresholding pass to another texture. I.e. pixels whose brightness is below a certain (configurable) threshold are are turned down to black. Downsample and blur the thresholded pixels. Usually, this is done in several "octaves", i.e. rendering a Gaussian blur with a small kernel to progressively smaller render targets: from full resolution to half resolution, from half to quarter etc. Composite the downsampled octaves back onto the scene image. 

Actually, if your bump or displacement map is saved with that 0.45 gamma correction, it's wrong. These maps contain geometrical information rather than colour, and as such, should be simply encoded linearly. For a related issue and a fix for Adobe Photoshop, see this slide deck: $URL$ 

What you want cannot reasonably be done. No graphics API defines byte-accurate results. Every implementation will have its own variances, which will be visible in the results. OpenGL provides no insurance that you will get invariant results across implementations. At the end of the day, you can't do testing in this fashion. 

Because memory bandwidth is not free. No matter how much RAM you have, it takes less time to read 1024 bytes than it does to read 1024 KB. Compressed textures, relative to 32-bpp, can offer compression ratios of 4:1 or better. That's at least one quarter of the memory bandwidth costs of fetching the texture. If your rendering uses lots of textures, reducing bandwidth pressure can improve performance. Alternatively, the more memory you have available, the more memory you can use. If you consider 1GB to be "plenty of memory", then using compressed textures allows you to store the equivalent of maybe 2-3GB worth of textures. So now your textures can be larger, more detailed, or more varied, all within the same "plenty of memory" space. Or you can use some of that reclaimed storage for more meshes. There's never enough memory; there's only a question of how much you're willing/able to use.