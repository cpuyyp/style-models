The check is a "<=" (I assume you overlooked the -infinity instead of +infinity) - so the value you put in is the upper bound in that case. ("Scale in as soon as CPUUtilization is between your value and -infinity") Just to mention that here: you could also use the "target tracking policy" - with this you can say that auto-scaling should scale in a way that your target value is matched - that includes scale-out AND scale-in out-of-the-box. Hope this helps! 

Is there a reason why you don't use EBS snapshots? You can use those to save the whole EBS device (incremental) with a simple API call and the snapshots are saved within S3. If you need an old version back just create a volume from this snapshot and connect it to your instance instead of the broken EBS. 

Debian Lenny is very old and I assume that's the reason why Oregon doesn't provide the kernels you need to run your image. The only thing I can think of is to update your PV-Grub and create new AMIs. Maybe this helps: $URL$ If you don't find matching AKI in the other region there is no way afaik. 

You should be able to see that information in the Billing Management Console with the Usage Reports for the Elastic Compute Cloud service. I can select "SnapshotUsage" for the different regions in there. 

Did you create a Cache Subnet Group in you custom VPC? You need to create a cache subnet in your VPC (inside the ElastiCache Management) first - after that your VPC/Subnet will appear for nodes. 

Security groups in AWS are way more comfortable AND even better from the security point of view (as the traffic you want to block doesn't reach your instance at all). So turn off the iptables and configure the security groups as limited as you can. 

So my call would be the last option - but that means you need to point the clients/browsers to a separate subdomain for all static assets (or for all POST requests). It sounds like you want to have a look at technologies like AngularJS or React to build a truly API-driven application in the browser. With this approach you're running a real API which is handling all the "dynamic" requests with an API Gateway and delivering the application itself from S3 as an static asset. Maybe looking at those might help you to find your way - even if you don't use them, the architectural pattern on how to build things like this is what you're asking for imho. 

Another solution is to ask specifically for files only by using . You can also suppress the printing of the starting points by using . 

tries to give you as specific information as possible (the opposite case would be to always print , which is technically correct but not very useful). ANSI is not a specific encoding, and UTF-8 is a superset of ASCII, so it will report ASCII for both if the bytes contained in the file all are inside the ASCII charset. 

As others noted at the linked page, this is a bad idea. Passwords are generally much less secure than using public/private key pairs. That said, if the server sent just it isn't using the new configuration yet. Try restarting . 

If you want to do some automated tests rather than manually checking the changes every time you can use Selenium. PHP has a lot of stupid bugs, and personally (after an MSc + 10 years as a programmer) I believe it is positively brain-damaging. It is so riddled with terrible mistakes that I hope you will consider using a more sane language, such as Java, Python, Ruby or even Perl. Bad habits take a long time to die, and PHP teaches a lot of bad habits. 

If you're asking how to run more than one statement with a single command, you can either simply separate them by semicolons: 

There's nothing fundamental about a desktop installation of Linux (unlike other operating systems) - you can "convert" one to the other by installing/removing packages and enabling/disabling services. For example, if you want to make sure to avoid any resource-intensive graphical logins it may be enough to remove the package and any Unity packages. You may also want to remove X and graphics drivers. Then check to see if any useless services are still running. 

ask for the password once, create a named pipe, link that to a background process, and pass all the SQL to the named pipe. 

Once you create a host from an AMI you can create a snapshot of the volume (should be done before starting the instance to avoid having irrelevant state on the disk), and create a new AMI from that snapshot. Then you have your own AMI with the same contents. As for which one to choose, there are only two 14.04 images available: EBS (presumably PV) and HVM. Which one is best for you depends on your requirements. 

I'm building an API which will only be accessible to local services by using in the directive. How can I make this return a 404 rather than 403 status code when accessed from a remote address? That way I can hide the fact that there is a service at the requested location. I'd rather preserve the semantics of than use . 

so far, so good. Now I want to have my external VIP (XXX.XXX.XXX.XXX) as a source IP on external sites. So when I run like curl ipinfo.io/ip from one of my hosts in the network, my VIP should be returned. At the moment, I get a physical IP of one gateway (ZZZ.ZZZ.ZZZ.ZZZ). Is this possible? How? Could I just set the default gw on the gateway to the VIP? keepalived config 

Rename all the *.config files in the Reporting Services directory to *.config.bak (so the installer thinks there are not present) Run the Repair option of the installer Configure the whole Reporting Service from scratch Delete all encrypted content (via RS Configuration Managers Encryption Key options) to avoid the rsReportServerNotActivated (due to no catalog access) error Re-Publish all the Reports 

I messed up my SSRS2008 Express' configuration I'm afraid. I tried to enable anonymous access, using different methods I found on the net, so users would be able to just open the (non-IE) browser with their reports. Now, I neither can deploy (a box wanting credentials pops up, which does not even accept the domain admin account, that has the dbo role assigned) nor access the report servers web frontend through IE I tried maybe hundreds of combinations of service account, authentication methods but the ERROR in the Log File stays the same: 

We have 2 Apache webserver behind a load balancer wich are connected to 2 (JBoss) application servers via mod ajp. To those webservers, mobile devices connect via a REST API. In our performance test we rather quickly ran into a lot of NonHttpResponse: errors which we identified to be coming from mod_reqtimeout: 

But this can't be the solution, since with some more simultaneous users the problem occured again (There is a requirement to be able to serve 300 concurrent users - 100 worked quite ok, with 300 we had over 10,000 of those Request header read timeout errors). I suspect it's the interaction of apaches KeepAlive, our mod_ajp configuration and mod_reqtimeout that leads mod_reqtimeout to the conclusion that there is a slowloris attack ongoing (to many open connections that do nothing) and I kindly ask for your help in tweaking those parameters. Additional problem is a firewall between webserver and application server, which I suspect to kill open idle connections. I read about deactivating KeepAlive completely to solve this, but as I said, all our clients are mobile devices, so that's probably not an option (?). Here are the other configs (parts of): workers.properties: 

pinging the returned IP has 100% packet loss. I also did various traceroutes, here's the one to Google: 

You are passing the input to the loop on standard input (file descriptor 0). Any commands inside the loop which read from standard input will consume that stream, making the content not available for the second time around. This is a common issue with passing standard input to what is effectively multiple commands. The easiest way to fix this is to use a file descriptor that the commands inside the loop are unlikely to use, for example: 

That's not running twice, you're just seeing the and the processes, which are separate. Try to see the process tree, explaining how they are related. 

It looks like the package is not installed with APT: "Package hadoop is not installed, so not removed". If you have installed by some other means, such as or , you'll have to look into how that package specifically can be uninstalled. 

The semantics of your code simply requires the to "be instantiated" (that is, run) before the child directory is created. There is no optionality implied by this - will run if exists, otherwise does not run. 

However, this changes the mode of every file in the directory. Since most of the files (thousands, changing every day) should be handled by another application, how can I tell Puppet to leave all files not in alone? I can't use because that requires that I know in advance the names (or at least globs) of files I don't want to manage. 

To reliably send a lot of traffic to a system under test you need to put the client and server as close as possible. The easiest way to do that is to put the client system on the same network as the server (unless you're testing the router or firewall). 

This sort of thing usually happens because the terminal has been corrupted in some way, often by ting a binary file. Have you tried a ? If it really is the Bash history that is broken that is stored in . 

(Assuming you're the one who will be setting it up, and you're only asking about which distro to use for the server.) Most distros are suitable for a simple server, so I'd recommend using the server flavour (if any) of a distro you're familiar with. There really isn't much to it - you'll need to do at least some rough scaling of the amount of RAM and disk + speed of CPU/network, but that's another subject. 

will be much faster than FTP for synchronising files repeatedly - it simply checks the file size and timestamp on the receiver to find out whether it should overwrite the file, thereby minimising the amount of data transferred for each synchronisation. If there is a lot of files you may want to consider creating a tarball automatically (using for example ) and syncing that, for example using (untested). can be used to make the testing a single command. Something like this should do the trick (you have to use Tab for indentation): 

I have a really strage phenomenon in one of our data centers. I can ping facebook.com, yahoo.com, deb.nodesource.com and a few others (DNS resolution works fine). But I can't ping google.com, archive.ubuntu.com and lots more. DNS works here, too. I can also ping 1.1.1.1 (Cloudflare DNS) but not 8.8.8.8 (Google DNS). Firewall rules look ok to me - there are no recent changes and it worked before. LAN hosts have an allow all rule anyway. I am also connected to the DC via VPN, which works great, too. So connectivity wise I should be fine. Help me out here guys, I don't know where to look anymore... How can I debug this any further? edit: I did a dig @1.1.1.1 for google.com: root@dhcp01:[~]: dig google.com @1.1.1.1 

I want to achieve the following: There is one DMZ-proxy over which we want to access multiple application-servers, represented by mod_jk workers. The web-context needs to be the same on all application servers, since they run the same application. We just want to access different servers via a additional "folder" within the URL. e.g.: some-fancy-url.com/hh1/some/application/ --> some-fancy-url.com/it1/some/application/ --> So depending on that keyword between URI and context we want to forward the request to different workers; the application server needs to receive the request without this keyword though. How can we achieve this? mod_rewrite I suppose? 

and the target server URL of my reports project is set to $URL$ (like the according virtual directory in the rsreportserver.config) I'll gladly post anything else you need, just tell. Any help is greatly appreciated! *This is a crosspost from SQLServerCentral.com 

I want to achieve a DHCP 4 failover with ISC Kea. I have a setup with 3 mySQL servers (1 Master at facility A, 2 Slaves at facility B, replication works) and 2 Kea DHCP servers (configured read only) in facility B which have the 2 mySQL slaves as their backend and are configured identically (we enter the values into the DB in facility A) So far so good. Now I searched the net for how to create a failover for both servers. I found this, but I don't know what to do, actually. Does Kea automatically check there is another server and I don't need to do anything? I could configure Kea to respond on a certain interface only (on another subnet maybe?) and then create a keepalived config to put both interfaces behind one IP in the actual subnet (I think). But is that necessary? 

we (a small 5 employee company) ordered a hosted server to use it as a use-from-everywhere database frontend. Our database basically is a SQL Server 2008 with an MS Access frontend, which I intend to install on the server, so every employee can login from everywhere via RDP. So far so good. The configuration wizard now gives me the option to configure the server as a) "Role based or feature based installation" or b) "Remote Desktop Services installation" Since I thought it'd be best to make full use of the new virtualisation features of Server 2012 and use b) I chose this, but the wizard asks me to add the server to the Domain first, which of course does not exist, since this is only a single server setup. I also can't create a domain, because I would need to chose a) in the first place. Since I am completely new to Windows Server 2012 and not really an admin (I am more like a IT generalist) I am kinda stuck here. Any help would be greatly appreciated!