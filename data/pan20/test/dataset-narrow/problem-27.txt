The idea of calculating the cie coordinate is best described with the mathematical formula: $coord = \sum_{i=0}^{n-1}x_i c_i \Delta\lambda$ where $n$ is the number of samples in the spectrum, $x_i$ is the $i-th$ cie function value for the wavelength of the $i-th$ sample and $c_i$ is the $i-th$ function value of the spectrum and $\Delta\lambda$ is the stride between two samples $\Delta\lambda = \lambda_{i+1} - \lambda_i$ 

Your way of calculating XYZ functions is probably the most efficient way to go about calculating accurate colors from a spectrum. It is standard practice afaik, for examples the books Physically Based Rendering (3rd) and Real-Time Rendering (3rd) both use this method. You can add the colors in RGB space, but only if you convert from sRGB to linear RGB first. Otherwise you need to take into account, that sRGB sums will not lead to correct colors. The blogpost Adventures with Gamma-Correct Rendering by Naty Hoffman is a good read regarding this, topc: 

You have missunderstood this. $D$ is a Normal Distribution Function (or short NDF), so it doesn't really give you a single normal, but a distribution. In a (specular) BRDF you are always using the normal that is the half vector between the incoming and outgoing light, since via your theory, every microfacet is a perfect mirror and thus every microfacet reflects light exactly along the (micro) surface and that one only. You also must not confuse roughness with microfacet normal. The roughness is a more or less arbitrary value (and a scalar, so a single one at that). The roughness is being used differently, depending on the overall BRDF (e.g. roughness in GGX(/Trowbridge-Reitz) is different from the Roughness in Oren-Nayar) and therefore can have different ranges. Still, if you look at one specific NDF, you will see, how the roughness is used. $D(\omega_m) = \frac{\alpha^2}{\pi((\omega_n \cdot \omega_m)^2 (\alpha^2-1)+1)^2}$ with $\alpha = roughness^2$ (a common remapping), $\omega_m$ is the microfacet normal, $\omega_n$ is the geometry normal. You can see here, that the roughness is being treated like a single value. The higher the roughness is, the more random your microfacets (or normals thereof) are distributed. The more that happens, the less your surface will reflect light concentrated into the same direction (i.e. the less highlights you will have). Thus, your material is more diffuse. Since now your roughness is a single value, the map makes sense to have only values in $\left[0, 1\right]$ and therefore is a gray scale image. 

Does this look like "typical" projective aliasing? Could dynamic rendering of the shadow map, using the view frustum clipped to the scene, possibly with cascading, address the issue? Edit: Adding a closeup of bilinear filtering, post shadow comparison, just to show what I get. Bad shadow acne shows up because of interior edges; I'm modeling with stacked virtual blocks and not doing a proper union operation. From my reading, implementing boolean operations on polyhedra is not trivial, but will also allow me to implement static shadow volumes as well as clean up for some 3D printing software. 

Shader compilers are extremely aggressive about unrolling since early HW often didn't have flow control and the cost on more recent HW can vary. If you have a benchmark you are actively testing against and a range of relevant hardware, then try things to see what happens. Your dynamic loop is more amenable to developer intervention than a static loop - but leaving it to the compiler is still good advice unless you have a benchmark available. With a benchmark, exploration is worthwhile (and fun). BTW, the biggest loss with a dynamic loop on a GPU is that individual "threads" in a wavefront/warp will finish at different times. The threads that stop later force all the ones that finish early to execute NOPs. Nested loops should be carefully thought through: I implemented a block based entropy decoder that encoded runs of zeros (for JPEG like compression). The natural implementation was to decode the runs in a tight inner loop - which meant often only one thread was making progress; by flattening the loop and explicitly testing in each thread if it was currently decoding a run or not, I kept all threads active through the fixed length loop (the decoded blocks were all the same size). If the threads were like CPU threads, the change would have been terrible, but on the GPU I was running on, I got a 6 fold increase in performance (which was still terrible - there weren't enough blocks to keep the GPU busy - but it was a proof of concept). 

radiant energy $Q$ (in joules, $\left[J\right]$) measures the energy, i.e. the energy of a photon times the number of photons. radiant flux $\Phi$ (in watts, $\left[W]\right]= \left[\frac{J}{s}\right])$ measures the energy per time, e.g. don't just count the number of photons but the number of photons per second. $\Phi = \frac{Q}{t}$ a) irradiance $E$ (in watts per square meter, $\left[\frac{W}{m^2}\right]$) measures the energy per time and surface area, or the flux per surface area. $E = \frac{Q}{t A} = \frac{\Phi}{A}$ b) radiosity $M$ (in some papers also $B$) is the same as irradiance, only it's leaving a surface and not arriving at it radiance $L$ (watts per sqaure meter per steradian, $\left[\frac{W}{m^2 sr}\right]$) is the radiant flux per area and solid angle, or the irradiance per solid angle. $L = \frac{\Phi}{A w}$ 

For a division is suggested, by either $64$ or the maximum AABB value. I get the latter, that can be used for more or less any model then, but why use the magic number $64$ if I don't want to calculate it? Is it common to have models that don't exceed this specific value? If I get good precision with (or respectively) to save bandwidth, but in my application actually need really large position values, can I leverage the better precision of with a multiplication for "full" and save myself from using variables? 

I've got bilinear filtering on in the texture (without it, I get serious shadow acne). Sadly, my attempt at PCF failed too - it looks just as ragged: 

History: ATI Introduced PN triangles (a basic approach to tessellation) in its first generation to include programmable HW - so it's been around about as long as programmable HW shaders. It was deprecated in ATI's next generation, but tessellation was revived in the HW that became the basis of the Xbox 360 (a few ATI demos showed it off on PCs). Microsoft then incorporated the feature into DX11 (although not compatible with ATI/AMD's existing HW), making tessellation support a de-facto requirement for all GPU makers. 

GPUs compress multisample data (for bandwidth) and dithering would defeat this - so this is not supported. GPUs also have techniques to sample at higher frequencies than they store values (see QUINCUNX anti aliasing for an example that's been around a while), saving overall memory. 

My experience working with shader compiler stacks a few years back is that they are extremely aggressive, and I doubt you will see any perf difference, but I would suggest testing as much as you can. I would generally recommend providing the compiler (and human readers) with more information where the language allows it, marking parameters according to their usage. Treating an in-only parameter as in/out is more error prone for humans than compilers. Some detail: shaders run almost entirely using registers for variables (I worked with architectures that supported up to 256 32 bit registers) - spilling is hugely expensive. Physical registers are shared between shader invocations - think of this as threads in a HyperThread sense sharing a register pool - and if the shader can be complied to use fewer registers there's greater parallelism. The result is shader compilers work very hard to minimize the number of registers used - without spilling, of course. Thus inlining is common since it's desirable to optimize register allocation across the whole shader anyway. 

If that is what you wanted, then you need to remove the fragment shader position calculation. On a sidenote: I had problems with using in a shader the other day, so it might just be that this doesn't compile. Try using instead. 

As to performance, I can't say anything about the three. As to quality, the Box Filter is rather simple, but that doesn't mean it doesn't suffice for your specific need. Just test them against each other. 

This assumes of course, that your model has vertices on either side of 0 on the x-axis. You shouldn't generally use statements in shaders, since they are bad for perfomance, but this shader is so simple that it doesn't matter. Also, if you go about it this way, then simply outputting the x coordinate from your model space position suffices, no need really to use the whole . 

So you start with an offset of , which far exceeds the texture coordinate interval and go up to which again exceeds the interval. This is probably the biggest problem you have in your code, calculation wise. Furthermore, you use a variable to increase your and your variables, but this has never been created. This should not even compile. Since your and are meant for the gaussian blur, I guess a new variable is needed. Your next problem is, that iirc a gaussian blur should offset the texture coordinates, not the color. You multiply the readout color though, with your method, which may be wrong. In any case, you don't even have the method, and it is not a standard glsl method. Again this should not compile. 

Assuming you are not going to filter the buffer using HW texture filtering: since you only need 4 values for the material index, pack it in the sign bits of uv x and y. 

To make the models more readable onscreen, I've implemented basic shadow mapping for a single directional light. Since my scene isn't changing, I just render the shadow map once using an axis aligned box of the scene to figure out what the bounds of the shadow map should be. The shadows and bounds look like I've got all the matrices right, but coming in a bit closer looks terrible: 

A group from UC Santa Barbara published the Siggraph 2015 paper A Machine Learning Approach for Filtering Monte Carlo Noise describing a technique which trains a neural network to select filtering parameters for path tracing. The paper details the structure of the neural network, the filter parameters used and how several secondary features are pre-computed to feed the neural network. 

I'm fiddling with simple procedural 3D modeling to make teeny buildings for 3D printing. An Example: 

From the reading I've done, I understand the peter panning and what I might do about it, but the ragged edges, which I believe is a form of projective aliasing, look so bad that I think something is wrong in my basic implementation. I hacked the pixel shader to show the shadow texel boundaries: 

I am writing an OpenSceneGraph based program that uses two cameras, one to render the preview for the user and one that uses a callback to make screenshots with MRT. The exporter camera is using an orthographic projection and always from the same position with the same view and up vector, whereas the preview camera can be moved by the user (which is why I use two cameras rather than one for both tasks). Upon starting the program, I create the preview camera, add the shaders to it, then set up the viewer and add the preview camera to the viewer. Afterwards I create the export camera, add its shaders, textures and callback and then I add the export camera as a child to the preview camera. Up to this point, nothing has been rendered (i.e. no call has been made). The call is issued (i.e. the break point is reached in Visual Studio, and stepping further I can see that the exporter camera is now a child of the preview camera). However, once I issue a command to actually make a screenshot, the exporter camera is not the child of the preview camera anymore (by now, a few render calls have been made). Why is this, and how can I fix it apart from adding the exporter camera again? 

GPUs can efficiently scale an image by an arbitrary amount (within limits - display options fall within those limits by design) either using a 3D rendering operation or as the signal is sent from GPU to the display. Both of these paths have fully dedicated hardware for the arbitrary resizing and are not likely to be optimized for doubling or halving. Both support a technique called "bilinear filtering" though advances in display hardware may provide higher quality. If a game is run at non native resolution on a laptop (TVs and desktop displays have their own scalers, but laptop displays generally rely on the GPU), one of these two methods will be used. Display scaling is essentially "free", so it is preferred and the lower the resolution the game is originally rendered the faster the frame rate (or the lower power consumption) - with no special performance benefit for, say, doubling. If a 3D operation is required (say, due to a limitation or issue in the display scaler), the scale operation is not free, but is going to go up with the source and destination sizes smoothly, with no special casing at doubling/halving. The game may look far better at certain scaling factors, but that's up to the user to decide. Edit: The intuition behind the question is correct: if GPUs didn't have dedicated scaling hardware, doubling/halving could implemented more efficiently than arbitrary scaling, either in shader code or via a slightly simpler hardware design - though the 3D rendering operation is a requirement of graphics APIs and display scaling is a requirement from systems vendors.