Throughput is the speed visible to the application. Often we aren't so much interested in the throughput of a link as the "end-to-end throughput" between two applications communicating over a path consisting of multiple links and routers. End-to-end throughput can be affected by the lowest transmission rate of the links on the path; the path error rate, packet re-ordering, latency and jitter affecting the TCP congestion-control algorithm; the path MTU and packet re-ordering affecting operating system efficiency; the choice of TCP congestion control algorithm used. 

A "NIC" is a "network interface card" (or, often these days, "network interface controller"). It's set of chips (these days, single chip) which implements the technology for the network interface. An "ethernet controller" will take a short queue of packets and implement DMAing them from RAM, framing the packets (adding the source address and ethernet CRC). A modern ethernet controller will also be able to do some Network and Transport layer functions so that the CPU can handle packets more efficiently -- these "offload" functions include calculating checksums and packetising TCP segments. The controller might also implement the more physical aspects of the ethernet protocol, such as Base-T's autonegotiation, or it might leave this to another chip we call a "PHY". The "ethernet contoller" for a switch takes this to extremes, implementing the entire "transparent ethernet bridging" forwarding plane between 4 to 16 ports. Recent controllers will handle most of the ethernet bridging control plane too. In almost all modern router designs the router ports attach to one or more switch ethernet controllers, typically a Broadcom Trident- or Tomahawk-series controller. That controller might implement the router's forwarding plane, or it might simply handle port fan-out. So, back in the IT Director's day routers would have a single ethernet port attached to a single ethernet network interface card, there might be four of these cards assembled with some routing logic to make a "line card". The "line card" would attach to the router's backplane. Today even a low-end server NIC will happily present 4 gigabit ethernet ports. A switch or router might present 16 ports from the one ethernet controller. So your IT Director really should starting saying "port". But before you go accuse your Director living in the past, consider that the job of a IT Director doesn't allow much time for the details of the technology; that's surprisingly low on the list of things they'd most like to know. So as much as they are being wrong in their terminology, bringing it to their attention in anything other than an aside, is to be wrong from the point of view of their expertise -- management. They have things to do with their time which are much more significant. I'd go no further than "just so you're not embarrassed in a presentation, they call them 'ports' these days". 

You can measure both computers' resource use, as there's a bottleneck. CPU is the most likely bottleneck, given that parallel transfers have superior performance. See the parameter for a easy way to determine single core versus multicore performance. Note that you've chosen really odd platforms. I've no idea what the per-packet overhead of what the MacBook Air's Thunderbolt-attached gigabit ethernet dongle might be compared with PCIe. So you are going to have to do a lot of the investigation of potential bottlenecks yourself. 

Also called "latency". Latency remains important as it's the one performance factor in a global network which isn't improving rapidly. As everything else improves then avoiding latency becomes increasingly important to improving performance. This has effects from avoiding round-trips packet exchanges in application and protocol designs; fielding new protocols which move data closer to their endpoint; and architectural responses such as content distribution networks. 

The real question is: why aren't R1 and R6 running OSPF on their interfaces? OSPF implementations have the feature which allows OSPF to not share information across an interface whilst still allowing that interface's subnet to be added to the OSPF database. This is exactly what we want for interfaces to leaf subnets, such as the interfaces to PC1 and PC2. Static routing between routers is a poor idea. It's easier to configure and simpler to operate a whole network with every interface running OSPF than it is to mix OSPF and static routing. This is a point a lot of beginning network designers miss, thus making their networks more complex in the mistaken belief that they are making them simpler. But to give an answer. R2 has the interfaces B-C, R2-R3, R2-R4, R2-R5 in OSPF. R2's configuration looks like , , , , . On R2 create a static route for 10.0.0.0/8 with the next-hop being the address of B: . Then on R2 add this static route into OSPF: , , , . That route-map permits network 10.0.0.0/8. You'll notice that we use a route-map to control every route entering the redistribution. This is because it's very easy to add static routes which we don't want in the main OSPF routing table (eg, to act as a discard destination). You'll also notice that we do an exact-match of the prefix, as this is best behaviour which prevents nasty surprises. We only configure the static route and its redistribution into OSPF on the nearest OSPF router(s) to the static segment. In this network, each of the static routes is configured and redistributed exactly once (10.0.0.0/8 at R2, 20.0.0.0/8 at R5). Multiple definitions of a static route and multiple redistributions are a common misunderstanding, Trust OSPF to carry the route information it has been provided with. R5 is similarly configured to R2. R3 and R4 have no static routes, no redistribution. You just list the interface subnets , , , ... 

It's not the same data. IRR converts ASN to routing policy. That application just needs ASN to text, so can use the ASN allocation records at ARIN, APNIC, etc. 

You're correct. The usual service is that the client-side frames are encapsulated at the network edge within service-side frames which then appear as usual 802.1q frames to other switches within the service provider. A de-encapsulation occurs at the far network edge, and the client-set VLAN tags are presented at the client-facing interface. Having said this, the "typical" practice of a service provider and your particular service provider's practice may vary, which in this case would be enough reason in this case to choose a more typical service provider. You should ask for a technical service description for any service you buy. That service description will also list other important parameters, such as how many MAC addresses can be visible to the service, and what happens when that number is exceeded. Practice there varies widely, with "one per client-facing interface" and "shutdown until manually restored" being common for services designed to run between IP routers (eg, connections into LINX), and maybe a few thousand being typical for services designed to run between a central router and a remote office switch. You want to ensure that the service description allows untagged frames to be passed. I know it would be weird for a service not to do this, but I have come across this case. Ensure the service provider's service description is be clear about the types of BPDUs which will be transparently transported across the service. You at least want to be able to pass your spanning-tree and LLDP PDUs unaltered. In an ideal service, all BPDUs are passed unaltered between your equipment. The stress-test service here is usually the control BPDUs for supporting MacSec encryption (and you really should think about running MacSec between your service-provider facing interfaces, it's the simplest way to protect yourself from subversion of the service provider's infrastructure; there's even SFPs which will run MacSec internally). 

Large sites like universities or hotels use "wireless controllers". These implement broadcast isolation to individual machines whilst ensuring that required broadcasts (such as ARP for the router's MAC address and DHCP) are permitted. These networks operate with the AP handling established traffic and punting before-unseen flows (such as client broadcasts) to the controller. If you have a large enough wireless network for the level of broadcasts to cause a noticeable loss of performance then you are in the market for a controller-based wireless system. You usually purchase the controller and access points as a coordinated system from a single vendor. Do take some care as this area of networking has all the downsides of modern networking equipment vendors: poor vendors, good vendors who charge far too much, outrageous lockin-by-protocol to inhibit competition, bandwidth-sizing lies (although the claims of firewall vendors are deeper lies), discontinued products due to acquisitions.