Most vendors don't have such a requirement. This is a convention often discussed based on the limitations of a three bit hash when load balancing. I personally disagree with this stance, but I will get to that after answering your questions. 

You don't give a lot of detail, but you do have a specific question. Yes it is entirely possible that traffic won't pass between a 1000/Full interface and a 100/Full interface. While many 1Gig connections will have the capability to run at 100/Full, there are also a fair number that do not. For instance, most 1Gig transceiver hardware will not work at 100/Full. I don't know which make/model of switch you have, but on most enterprise class switches there is a command that will give you all sorts of details on the the features and capabilities of the port. For example, on a Cisco switch you would use a command similar to show interfaces capabilities module [module number] command and you would get output similar to the below: 

Since all zeroes for the host part of the address meant "this host," it is unusable as a host address on the network. Logically, a network was referenced by using the "network" portion of the address followed by all zeros (i.e. no host bits in use). I can't find documentation to back up my memory but I believe that this was widely accepted long before RFC1060. Edit: Thanks to Ricky's mention of RFC919, I found the reference I had been looking for that illustrates what I recall about the general acceptance of the network address: 

Let me add a couple of images to provide examples take from 802.11 Wireless Networks: The Definitive Guide by O'Reilly Press: 

Whenever I am deploying something outside in an enclosure, this is the procedure I use. First, make sure you start with the proper enclosure (for example NEMA 3 or 4). If you penetrate the enclosure, make sure the penetrations are properly sealed to maintain the rating. (I assume this would be the case as you need electricity and because you mention that the enclosure is made of metal and no one wants their antennas inside a metal box, right?) Since your enclosure is going to be restricting air from flowing into/out of the enclosure, you now only need to contend with any moisture that is trapped within the enclosure with your devices. Your next step is to reduce the moisture in the trapped air. Low moisture means less chance of condensation and with no condensation, you get no frost. For this, I prefer to use silica packs. You can buy these cheaply, but even better they come in the packaging with many items (shoes, clothes, electronics, and so on). Most people throw them away, but I recommend saving them as they have many uses. Before you close you enclosure, add some silica packs. However, before you do that, make sure the packs are "fresh" by heating them. Heating them (120-150C/250-300F) will cause them to "release" the moisture they have gathered. For this, I use either an oven (longer, but you can easily set the temperature) or microwave on 1-2 minute cycles (faster, but you need to figure out a way to determine if you have heated them enough and some outside packages won't do as well). Now, silica packs will not remove all the moisture from the air, but you don't really need to remove all the moisture. The waste heat from your electronic devices will help to prevent the moisture from condensing on them and most of the condensation (if you have any) would be on the inside of the enclosure itself (which will tend to be colder). Make sure you account for this by not setting any electronics or power cords directly on the bottom of the enclosure. 

Your understanding is flawed. Multicast does exist, but like most management traffic on a wireless network it must run at the lowest supported base/basic/required data rate. By default this is typically the lowest data rate supported by the AP. The reason why is that anything that is broadcast or multicast from the AP to surrounding clients must be sent at a speed that all clients must support and can receive reliably. What you are referring to is a technique that many access points can employ, which is a multicast-to-unicast conversion. Because the radio medium is a shared medium, a multicast frame sent at the lowest data rate can take 300X or more "airtime" than a unicast frame sent at the highest data rate. This is often far more efficient than sending multicast as multicast traffic. In your example situation on a default configuration AP (802.11n or newer), it could take less time to deliver 10 unicast frames to each of 10 devices (i.e. 100 frames total) at the highest possible speed than it would to deliver even one multicast frame. Two other considerations for multicast on 802.11 that are often reasons to use multicast-to-unicast conversion are that multicast frames are never acknowledged whereas unicast frames are acknowledged and retransmitted by the AP if there is no ack (not to be confused with TCP acks, this is a L2 mechanism and part of the process for finding optimal data rates between AP and client). Second, if any client device in the BSS is using power save mode, multicast frames are only sent periodically (based on configured DTIM and beacon intervals) to ensure that multicast frames are again received by all clients. With multicast-to-unicast conversion, frames go to clients immediately if they aren't using power save mode and only delayed to clients that are in power save mode. 

You will not be able to capture any traffic using the high throughput (MCS) data rates. However, by default 802.11n devices will support and use legacy data rates depending on the situation. As long as the AP and/or client are using legacy data rates, you should be able to capture their traffic. This would include most (if not all) management and control frames which will typically use a lower/legacy data rate. This is opposed to data frames which run at the best possible data rate, including the EAPOL exchange of keying material. So you will miss that unless the AP and client both use a legacy data rate for this exchange. So in answer, yes there are many malicious operations that can be carried out on an 802.11n network by a legacy client. In particular anything that would exploit management and/or control frames, plus any data traffic that uses legacy data rates. 

You certainly can do without a controller. A number of vendors either provide management software with the purchase of the AP or have products that are designed to run without one. You can choose from Cisco, Aruba, Aerohive, Meraki (now Cisco), Ubiquiti and quite a few others (supposedly Meru has a solution soon to launch as well) for solutions that work without a controller. As for single SSID or multiple, that is really up to a design choice, but generally speaking you would want to only provide one for roaming if you do mobile computing. Wireless clients have been getting better about making good choices when presented with multiple wireless access points, with some notable exceptions (Apple devices seem to still have some issues periodically). If mobility isn't a concern and you are largely dealing with nomadic (i.e. they may move, but are used almost entirely in one position)or stationary clients, then you can balance your clients in several ways with unique SSIDs. However managing the clients and educating users can be troublesome if you don't put some thought into it. Controllers largely help with the management, reporting and coordination (and sometimes troubleshooting) of the wireless network. While they are nice to have, they are by no means essential for general wireless. If you are looking to do something more complex or that requires a higher class of service (such as wireless-VoIP), then you should look into a controller based solution. 

I would consider this too much for one rack. You always want to leave yourself at least some space for the future. I have seldom been in situations where less space is used in a rack over time, rather quite often the opposite (you end up needing more). Figure it this way: 

Depends on which vendor or product you are talking about. In general, I would define a virtual AP as a "logical AP" rather than a physical AP device. This may involve providing multiple logical APs on a single physical device or treating multiple physical devices as a single logical AP. 

Time for the control test, which should err-disable the port, which is exactly what occurs in about a second after the port transitions to up/up: 

For Cisco devices, it would not be totally inaccurate to say that, but I wouldn't say it that way as the phrasing gives the wrong impression. When stated as you put it, it almost makes it sound as if some of the entries from the IP source binding table are used to generate the DHCP snooping binding table. You can say that the DHCP snooping binding table is one of the sources used to generate the IP source binding table. 

While GNS3 can somewhat emulate a switch functionality by utilizing the NM-16ESW, it cannot do so fully because of the same reasons it can't emulate a switch. Many of the features are simply embedded into the hardware. Since switch forwarding is often done only in hardware, the processes to provide statistics about that traffic are also in hardware. Without those processes to feed the data back to the router, you end up with no statistics for those ports. 

As you noted, yes most clients will likely still be 802.11n, but this is changing quickly. In the BYOD environment I am currently in, we are running about 15-20% 802.11ac clients in areas that have 802.11ac deployed. Each 802.11ac client can achieve higher data rates making the use of spectrum more efficient on average. An 802.11n access point can be single band. 802.11ac are always dual band, so depending on the 802.11n access points currently deployed this can provide a significant capacity increase. 802.11ac has a number of improvements, such as beam forming, that are not client dependent. This will allow the clients to receive at higher signal strengths which often translates to higher data rates. This again increases efficiency. 

A very good question coming at it from a networking viewpoint. Your question is based on the networking models that were developed long after packet switching was proposed as a concept. However networks across any distance were interconnected over the existing telecommunications "networks" and while many network and telecommunications terms are the same, they will carry somewhat different meanings. Traditional phone connections are circuit switched, meaning that there is one established circuit forming a connection between two end points. A telephone switch in a phone system used to be the device that created and tore down these connections. If you go way back, human operators would work at "switch boards" manually making and removing these connections. Packet switching is a much more recent concept (relatively speaking) and refers to networks that would take the data and transmit the data units independently over the network. It is often far more efficient to "share" the resources of the underlying network in this way. Today, VoIP uses "virtual circuits" to emulate circuit switched connections over a packet switched network. The Wikipedia pages for Circuit Switching and Packet Switching are pretty well written and should provide more background. 

Keep in mind that a higher frequency doesn't translate to moving data faster, rather it is the modulation of the signal that determines that data rate. The 802.11n standard either requires or makes optional the same set of major features and modulations in both frequency ranges. However, that being said, there are a number of best or common practices that typically will make 5GHz faster than 2.4GHz. First, there is an option for channel width, either 20MHz or 40MHz. In 2.4GHz, you only have room for one non-overlapping 40MHz channel (in most regions). To reduce co-channel interference, you need at least three, so this option is generally not used in 2.4GHz. In 5GHz where you have many more non-overlapping channels available, this isn't a concern and 40MHz channels are the norm. Second, is the guard interval. 802.11b/g used a longer guard interval than 802.11a. 802.11n makes the short guard interval an option in 2.4GHz, but this is still often left as the long guard interval since some older devices have issues with a shorter guard interval. Deployments in the last couple of years are more likely to enable a short guard interval on 2.4GHz, but this still seems to be in the minority. These two factors alone account for over twice the performance between the two frequency ranges in a normal deployment. From there, it is not at all uncommon for enterprises to turn off any legacy support in 5GHz and run it in a 802.11n only or Greenfield mode. Not having to allow for older 802.11a devices can further increase performance. The reasoning behind this decision usually follows that the majority non-802.11n devices did not support 802.11a (5GHz), for those that were dual band, the 2.4GHz band still exists for them to connect. 

As in manual adjust the transmit power of the transmitter up or down? No. Many will adjust to some degree automatically, but you won't have control over it. However you can reduce the signal strength in a number of ways (or a combination of the below): 

In some cases yes, but in others no. You would have to evaluate your situation to determine this, keeping in mind that one is attempting to detect L2 failures, while the other is trying to detect L3 failures. Let's look at a concrete example. Consider two routers (R1 & R2) on the same network connected by fiber to a switch utilizing link aggregation on two ports each (R1/P1, etc). One day a single strand (R1/P1's transmit strand) has failed. R1/P1 is still able to receive from the switch so could believe the link to be up. Because R1 can believe R1/P1 to be up, it will send traffic to the switch on that port, which will be lost as the switch believes it to be down. If the BFD session traffic is being carried on R1/P2, BFD may not detect the problem while UDLD would detect the L2 failure. 

While you can't directly tell which interfaces connect to a switch and which connect the two servers directly, there may be ways to do this indirectly. Namely, look for the type of traffic that is generally produced by a switch and not by a server. Unless you have the servers configured to participate in STP (i.e. generally only a concern if you are bridging or running virtualization with a switch/bridge), they don't tend to send out BPDUs like a switch will if running STP. You can use tcpdump to capture these frames if present (substituting the appropriate interface): 

This is a bit of an out of the box answer, but depending on your wireless vendor, several of them offer a "remote AP" configuration that allows an AP to create a VPN tunnel back to the controller when it boots. Your remote (and temporary) users would then connect to the AP and back to the controller over VPN. You would have to come up with other solutions for things like printing (wireless printer?), but it would be possible especially for a 2 month deployment. 

Since the computer is operating in full duplex mode, it isn't utilizing CSMA-CD. This means it doesn't check if the medium is idle before it transmits, nor will it perceive any data it receives while transmitting as a collision. As such, the upload from the computer would remain largely unaffected. Conversely, the switch is utilizing CSMA-CD and will wait for the medium to be idle before it transmits. In addition, when the switch detects a collision, it immediately stops transmitting the frame and follows the CSMA-CD collision detect procedure. This has a significant performance impact on the traffic sent to the computer. When the traffic is TCP, the negative effect will be multiplied as any lost TCP ACK's going to the computer will cause a TCP retransmission. 

This particular blade does not use SFP format transceivers at all. It uses GBIC format transceivers. You can find more information for this blade on the data sheet. So, no, you won't be able to connect a SFP + LC connector with this hardware. There is no converter to allow you to insert a SFP into a GBIC slot. However, all you need is a different fiber cable to connect to a GBIC, they use SC connectors as opposed to LC. GBICs and SFPs will communicate with each other just fine as long as they are the same type. So if you have an SFP on the other end of the cable, you would simply need a SC-LC fiber cable. Since you mentioned single mode fiber, if the blade doesn't come with any GBICs (or the right kind), you would also most likely need a 1000BASE-LX/LH GBIC as well (most common SM fiber transceiver, however there are others). Fortunately, SFP has quickly overtaken GBIC as the standard form factor (main advantage being it's smaller size), so you can often find second hand GBIC transceivers at a very low cost.