On a short sentence: COLLATION defines sorting and comparing. So, collation determines the rules SQL Server uses to compare and sort character data. These rules are language/locale aware and may also be sensitive to case, accent, Kana and width. Collation suffixes identify dictionary rule (in)sensitivity: _CS (case sensitive), _CI (case insensitive), _AS (accent sensitive), _AI (accent insensitive) and _KS (Kana sensitive). Binary collations, identified by suffixes _BIN (binary) and _BIN2 (binary-code point), are sensitive in all regards. Different collations will certainly demand workarounds to avoid "cannot resolve collation conflict" errors and can kill performance due to the known non-sargable expressions. Dealing with different collations can be a nightmare (have been there) so that's why the recommendation to pick one and stick with it. More references: 

I guess I should use @@SERVERNAME and DB_NAME(db.database_id) someway but so far can't find how to do it, maybe is trivial but no luck find in it. 

Thanks Kin for pointing out your question and related answers. I've learned a lot in the process. By looking into your detailed question I thought on doing the same, comparing execution plans of our heaviest query...and voila!! The issues was a job that was supposed to be executing was already couple of weeks with schedule disabled. Now I must check why it was disabled and when exactly was disabled. Everything is running smoothly now. Blue line is one server not receiving requests due to maintenance, is not dead. 

We have on several databases on same instances, some stored procedures have references to tables on other databases. We are in the middle of a big project to take our database under source control. Creating the baselines is not an problem. But after baseline creation, when trying to deploy on other environments, say a local developer instance or staging or alike, this cross reference issue is causing us several headaches. After doing some research and checking on possible approaches I still can't find a reliable way of managing this issues. One approach proposed is to split baseline creation into "modules" that could be run individually so the cross referenced objects remain in one single script and be run alone. But what if the referenced objects on the other database has changed, then how I can track those changes? Maybe an index was added on referenced table and directly affects performance on the current sp? Or for whatever reason the referenced table changed column name, even deleted that column that is referenced and added a different one? What a nightmare. So, question is, is there a recommended solution for this issue? For solution I mean, methodology, step-by-step approach, or whatever else that helps getting close to a solution with the deploying issue when having cross reference between database objects. And for the moment, removing the database cross references is a no go. Way to much application dependent stuff to be corrected/updated and can't be done for the moment. Not completely sure if this question follows the standards, as it can maybe trigger some opinion based answers. But I don't have more information to throw in for the moment. If you feel it should be flagged, then go. 

As Shanky already commented, running scheduled shrinks is wrong. There is ton of info online about it. Find the root cause of that growth and solve it. I would get rid of it immediately. As for the error, seems related to db ownership. If you run a search with the error text your will find several related questions here, here and more. As a recommendation, I would suggest switching to Ola Hallengren maintenance solution for administrative tasks as DBCC, index maintenance, backups, etc. Buena suerte, feliz navidad y felices fiestas. 

It can be as complicated as you want, no issues with that. My own recommendation, for better manageability, if the tsql you are planning to use on the job step is too long, better create a stored procedure. This way you just call the sp from the job step and you do all your modifications there. And on the job itself just take care of the scheduling. 

As a recommendation, I would suggest switching to Ola Hallengren maintenance solution for the backups and some other administrative tasks, is really easy to configure. Run the main script Maintenance Solution, schedule required jobs and off you go. I hope that backups are copied to a different server than production server. Then, schedule restore jobs from the test server to restore those daily backups. The job would have minimum 2 steps, drop database and restore it. Maybe you will want to add some cleaning step to clean up or mask some critical data that shouldn't be available for testing, replacing emails to avoid sending emails to clients and stuff like that. Maybe even deleting some data from bigger tables to have smaller databases. Heck, maybe for some cases is enough to just have the skeleton of the databases. If there are few databases, you can handle it manually. If not, you could make use of dbatools as commented, is a set of powershell scripts really helpful for administrative tasks. Or you can search online, there are plenty of options available, 3rd party tools, scripts that you will need to tune for your needs, etc. 

We have an SSRS 2014 already in production and we want now use a secure site by linking it to a certificate (URL like "$URL$ So now the URL is fully qualified domain name(FQDN). We can access correctly to the report manager with this https URL. We have to register to access with the Windows account. The bug we have is that when we navigate, sometimes, we pass to an http page. Each time that we pass from an https to an http page, the browser (IE) ask to register again. For example, here we are on a https page. When I click right on a report -> manage: 

That's why when running it, tried to automatically create the snapshot. And as last snapshot is from couple of months, there is lot work to do in the middle. Ran with false value and now everything is ok, runnig smoothly and without issues. 

If I clic on a tab on the left pane (for example in Security), SSRS ask me to reconnect and jump to an http page. This is a real problem for us. Any suggestions? 

Maybe you need to do some extra investigation on what is running on your server and be able to add more information to what you already have. I recommend using sp_whoisactive and First Responder Kit to get more info on what is running. Check a answer I gave to a somewhat similar situation. $URL$ 

I'm doing some cleanup on one of our database for development purposes on a test server. There are a couple of tables with one column. Both tables take roughly 50Gb of space, being the image column the culprit as the the other files are just int, datetime and small size varchar. I ran an update on the tables, using recommendations I found here in SO and other sites, putting the db on simple recovery mode, disabling indexes, setting transaction isolation levet to read uncommited and updating in chunks of 1K rows. Updates takes some times to run, about an hour. Here is my code: 

Is there any security implications on this action I should be aware of? Other option is to use advanced properties of the linked server object on main server: 

place that step where you need to stop job execution save the job, it will detect that some of the steps will not be executed and warn you about, select yes and you're done. 

Back in the days there was sp_who, sp_who2, but then came Adam Machanic and created sp_whoisactive. And he saw everything that he had made, and behold, it was very good. And there was documentation and downloads on the following days. 

Change to Make sure that is the only authentication type specified on make sure all url's are absolute, without wildcards 

No, is not a good idea. There is a possible security issue underneath when using as owner of the job. All jobs are ran through SQL Server Agent and it sets the security context for job execution based on the role of the user owning the job. So, if you use then you know what it will happen. By default, SQL Server Agent executes job steps under the SQL Server Agent service account irrespective of job ownership, or under the context of a proxy account. The exception to this rule is T-SQL job steps, which execute under the security context of the job owner. If the job owner is a member of the sysadmin role, then the job step executes in the context of the SQL Server Agent service account. So, using as the job owner will cause all T-SQL job steps to execute as the SQL Agent service account, which is a system administrator account. A better option is to set a non-sysadmin account as the job owner, and explicitly grant only the required database permissions to this account. So, what I would do is to create a specific user on the DOMAIN and give that user the needed rights to execute those jobs. And that user should not be a person in the company as you could have same issue in the future, if that person goes away and the account is closed...same error again. 

The udf based on the field value makes some comparisons and returns a date. Then adds an amount of days to that date and compares to current date. In order to be able to use an existing index on I want to transform the operation to apply all the transformations to , let's say I want to make it , if possible. Also, if there is some recommendation on what can be done to improve the use of the udf even better. 

I don't have a full script for the task. But maybe this script could be a start. It returns a list of logins that don't have permissions granted. Using it searches on all databases using and . You could insert the result in a temp table and then do whatever you need to do with that list. I'm sure I found this script some time ago somewhere and made some small modifications to fit my needs. But can't find where I found the original one. So sorry if no link provided to the author(s). 

PS: if you don't have SQL Sentry Explorer installed, stop reading and go install the tool, is free and will make your life easier. 

We have several db servers on production, 4 of them with very similar hardware configuration. Dell PowerEdge R620, only difference is that 2 newest ones (bought and configured 3 months ago) have RAID controller v710, 256GB RAM and CPU is 2 physical Xeon E5-2680 2.80GHz. Old ones (bought and configured about 1year ago) have RAID controller v700, 128GB RAM and running on witl 2 physical Xeon E5-2690 2.90GHz. BIOS is updated, all drivers updated to last versions, etc. All runing SQL Server 2008R2 Enterprise (SP1) updated to last CU and Windows 2012R2 Standard. Both running on 200 GB SSD x5 RAID10. There is only one database running on each of them, synchronized using a job that calls an SSIS package. Our sysadmin has run lot of performance and stress test to rest assure we don't have any hardware or network miss configuration or failure. As expected, newest ones show better performance results. So far so good. The problem we have can be seen on the screen capture from Kibana. Yellow and orange are the 2 newer servers (6,7 on tables) and below all the other servers. Is perfectly visible that those 2 new server have a slower response time. And not only that but also those 2 servers have a bit less of load than the 2 older ones (light and dark blue lines- 4,5 on tables). 

Another option, if you expect to be "playing" around with the jobs order very often, you can follow the following steps. We use it extensively and it works flawlessly. 

I'm slowly collecting and trying to analyze lot of information on all of our instances (2008R2 SP1). I'm now going through the Error Logs and I've couple of question. There is a lot about error log size and found different recommendations on using to recycle the logs on say, weekly basis, to avoid having very big error logs and be able to handle them adequately. So... 

Have couple of monitoring scripts gathering information on perf counters. Have digged as far as possible with DMV's and third monitoring tools, I have lot information at hand. But there should be (ofc) something that I'm missing here as I can't find an answer to this slower response time. The 2 newest servers are using less RAM but I guess that's expected, when compared to the other older ones as those have a lower load. 

That's the equivalent of checking on the publication creation wizard the checkbox Create a snapshot immediately and keep the snapshot available to initialize subscriptions. 

According to comments the inner is not needed. But then, following OP question and provided sample data as the results don't match the Output suggested by him. Only if the OP is mistaken and all the time he is asking for datetime, as on the first sentence of his question.