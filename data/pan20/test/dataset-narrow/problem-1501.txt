I think instead of building something from scratch, you should utilize the domain knowledge of experts. One example: Turn Wikipedia's topic portals (e.g., the medicine portal) into a family tree. 

You could use a two-sample $t$-test to determine whether software usage is significantly lower for lost users. Plotting 'software usage rate'/'software usage rate relative to country average' against 'percentage of lost users' might give you an idea about a suitable threshold. 

1. Download a comprehensive list of English words. (I avoid calling it a 'dictionary' to avoid mix ups with the Python type of the same name). I found one here. Remove all words that contain non-alphabetic characters (including hyphenated words ... maybe not optimal). 

This is fun! Here is an implementation of Emre's idea in Python. I tried to avoid loops wherever possible to make the code fast. First, some imports and constants. 

3. Create the dictionary of hashed words. The dictionary maps each tuple to the set of corresponding words. 

No, you should only tune one hyperparameter at a time. If you change two hyperparameters and the performance increases, how do you know which of the two parameters is responsible? If you have the time, do a full grid search on the dropout rate and the training data size. 

You mention that files may have different file types. Be careful when converting between types: Make sure for instance that separators and quote characters are treated correctly. 

Instead of computing $A$, make your algorithm compute $10\times A.$ Then use your favourite loss function for continuous variables. The most common one is the squared $L^2$-norm $l(10\times A, B) = (10\times A-B)^2.$ Here is a plot of $l$ that I made with Wolfram Alpha:. 

This will be very fast, unless $m$ is so large that does not fit into memory. In this case, I would follow your approach and use a for-loop: 

If I understand you correctly, you are wondering whether it is possible to have inputs where one element is a list or a matrix. It is not. First, this would require a complete rewrite of all activation and decision functions in Keras. Take a linear activation for instance. Keras expects that it gets a bunch of numbers $x_1, \dots, x_n$ and can calculate a weighted sum $w_1 x_1 + \dots + w_n x_n$. What then should it do if you pass it a couple of lists $(x_1^1, \dots, x_1^{m_1}), \dots, (x_n^1, \dots, x_n^{m_n})$? Edit 1: Let me be a bit clearer about this point. Every component of a neural network is built on the assumption that the input tensors contain real values. Mathematically: The input must be of the form $X \in \mathbb{R}^{n_1 \times \dots \times n_k}.$ What you would like instead are elements of the set containing all real vectors of arbitrary length, $\mathcal{R} := \{x \in \mathbb{R}^m \mid m \in \mathbb{N}\}$: You want to input $X \in \mathcal{R}^{n_1 \times \dots \times n_k}.$ This would require you to rethink and redefine absolutely everything: Activations, loss functions, forward propagation, back propagation... End of edit 1. Second, it is unnecessary. Say your input consists of a list $(x_1^1, \dots, x_1^m)$ and two scalar inputs $x_2$ and $x_3$. Then you should simply pass Keras the $m+2$ scalar inputs $x_1^1, \dots, x_1^m, x_2, x_3$. See JahKnows' answer for a nice implementation. Things get a bit more tricky if your input lists have different lengths ... so your first observation contains a list with five items, your second observation a list with seven items, etc. In that case you need to use padding and masking. Edit 2: You say in the comments that you are unhappy with padding because some of your sequences are very long. Let me point out some alternatives: 

I don't think there is a way to build your graph from raw data without using at least basic programming skills. I'm not aware of a drag-and-drop interface for importing and displaying data. Graphs are just a bit too complex. Imagine trying to find the profit of selling a product if all you had was CSVs of receipts dropped into Excel. You'd need labels of the columns, some basic calculations, and so on before you had anything intelligible. Graphs are similar in this regard. Thankfully, there are open source solutions, with some elbow grease and a few days of work, you can probably get a nice visualization. Cypher queries are relatively simple to write. Using Neo4j and Cypher, you can create a basic visualization of your graph, which is displayed using D3.js GraphAlchemist recently open-sourced their project Alchemy.js which specializes in graph visualization. $URL$ 

Show the user one high-tier product and one low-tier product (A). Show the user two high-tier products and no low-tier products(B). Which page generates more revenue? Send out marketing emails for a seasonal sale 5 days in advance to one group of users (A). Send the same email to a different set of users 1 day in advance (B). 

Since you aren't sure which software you would like to use, I can explain this concept only in general, by example. The clearest example is a content management system like Drupal or Wordpress (not necessarily a data science tool but the idea is the same). In a content management system, there are different types of content which can be accessed. In your case, there would be two types - document type A and document type B. There can be an unlimited number of documents, each assigned one or more types. The software has a permission system that gives access to each document type. This is built with roles. In your example, there would be a system administrator role, and a general user role. Each role has permissions associated with it. You would configure system administrator to be allowed to view document types A and B, but you would configure general users to only be able to see document type A. Each user registers an account in the software system. A site administrator assigns each account one or more roles. So a user with the system administrator role would be able to see all documents of type A and B. However, if a user only has the general user role, they would only be able to see documents of type A. Also noteworthy is the fact that each user can have multiple roles and will get all permissions associated with every role they have. USER -has-> ROLE(s) ROLE -has-> PERMISSION(s) CONTENT TYPE -requires-> PERMISSION(s) Every time the content is requested, the software system ensures that the user making the request has the permission to access the content. When a search is run, the software system has to validate every potential result individually before displaying the search results to the user. The user must have the permission to see each piece of content that is be delivered back from the search. 

Kernels are considered valid if they are positive-definite, so we must have $$ \sum_{i=1}^n \sum_{j=1}^n c_i c_j K(x_i, x_j) \ge 0 $$ for all $n \in \mathbb{N}$ and $c_1, \dots, c_n \in \mathbb{R}$ and (in your case) all $x^1, \dots, x^n \in \mathbb{R}^2.$ Letting $n=1$ and $c_1 = 1$ and $x^1 = (0, 1)$ shows that $K$ is not positive-definite, since $$ K\big((0,1), (0,1)\big) = 0\times 0 - 1\times 1 = -1. $$ The problem with your approach is that you define $\Phi$ on $\mathbb{C}$ but write $K$ as the dot product for real-valued vectors. The feature map argument for positive definite vectors only works if the product is an inner product on $\Phi$'s codomain. The dot product for real-valued vectors is not an inner product on $\mathbb{C}^2$ (because $i\cdot i = -1 < 0$). You would have to use the generalized dot product $x \cdot y = \sum_i x_i \overline{y_i}$, or another inner product on $\mathbb{C}^2$. 

Have you tried the boring, straightforward approach? Get a list of all words and count how often they occur with a high or a low label. (Exclude words that occur only once or twice, and also the words that occur very often). For example: 

Then I built an LSTM model in Keras, following this example. I simulated processes with high autocorrelation of length , trained the neural network on the first 80% of it and let it do one-step-ahead predictions for the remaning 20%. If I set , everything works fine: 

Your example is cherry-picked: You mask out small numbers and keep a large one. But dropout is applied randomly. Each of the following six masks, and of the corresponding values for the vector length, is equally likely to appear: $$ \begin{align*} &(1, 1, 0, 0): &\sqrt{0.1^2 + 0.1^2} &\approx 0.1414,\\ &(1, 0, 1, 0): &\sqrt{0.1^2+ 0.2^2} &\approx 0.2236,\\ &(1, 0, 0, 1): &\sqrt{0.1^2+ 5^2} &\approx 5.0010,\\ &(0, 1, 1, 0): &\sqrt{0.1^2 + 0.2^2} &\approx 0.2236,\\ &(0, 1, 0, 1): &\sqrt{0.1^2 + 5^2} &\approx 5.0010,\\ &(0, 0, 1, 1): &\sqrt{0.2^2 + 5^2} &\approx 5.0040.\\ \end{align*} $$ The average vector length is $$ \frac16 (0.1414+0.2236+5.0010+0.2236+5.0010+5.0040) = 2.5991, $$ which is roughly half of the original vector length $5.006$. So it makes sense to divide it by the dropout rate of $50\%$. 

There are many ways to calculate similarity between articles. I have not seen anybody doing a vector conversion and running comparisons. However, there is a text mining strategy called "Term-Frequency / Inverse-Document-Frequency" which is a clever way to find unique words and phrases in documents. You can run this on multiple documents, and compare the extracted keywords to match them for recommendations. Check out my ebook for more details: $URL$ If you want to leverage this technique on web documents (like a blog), there is a free service to do so: $URL$ 

One way to handle this is to use 'supervised classification'. In this model, you manually classify a subset of the data and use it to train your algorithm. Then, you feed the remaining data into your software to classify it. This is accomplished with NLTK for Python (nltk.org). If you are simply looking for strings like "hardware" and "software", this is a simple use case, and you will likely get decent results using a 'feature extractor', which informs your classifier which phrases in the document are relevant. While it's possible to implement an automated method for finding the keywords, it sounds like you have a list in mind already, so you can skip that step and just use the tags you are aware of. (If your results aren't satisfactory the first time, this is something you might try later on). That's an overview for getting started. If you are unhappy with the initial results, you can refine your classifier by introducing more complex methods, such as sentence segmentation, identification of dialogue act types, and decision trees. The sky is the limit (or more likely, your time is the limit)! More info at: $URL$ 

All algorithms always do better with unreduced data sets (more data is better). More complex algorithms aren't necessarily better. 

There is a basic introduction to the Bayesian method for spam detection in the book "Doing Data Science - Straight Talk from the Frontline" by Cathy O'Neil, Rachel Schutt. The chapter is good, because it explains why other common data science models don't work for spam classifiers. The whole book uses R throughout, so only pick it up if you are interested in working with R. It uses the Enron email set as training data, since it has emails divided into spam/not spam already. 

Each sample (read: each row in your table/matrix) can be seen as the realization of an $m$-dimensional random variable $X_{i}$, where $m$ is the number of features (read: number of columns in your table/matrix). Independence means that the random variables $X_i, i = 1, \dots, n,$ that generate your samples are independent. StatsSorceress explains the intuition behind independence wonderfully. A different way of looking at it is: 'Does knowing sample $X_i$ give me additional information about another sample $X_j$?' One case where this is true (and the samples are not independent) is for time series. If your samples are stock prices, and you know that the stock price one second ago was USD 100, then you can be fairly sure that the stock price right now is also going to be close to USD 100. 

From your comments, it sounds like you are trying to predict the next value of a series $X_1, X_2, \dots,$ where each $X_i$ is $(1000,2)$-dimensional. How about this: To predict $X_i$, you feed in the tensor $(X_{i-k}, X_{i-(k+1)}, \dots, X_{i-1})$, which has dimensions $(1000, 2, k)$. You apply convolution with stride $1$ and appropriate padding to the first and the second dimension: this does not change dimensions. And you apply pooling to the third dimension only (for example, set $k=8$ and use three pooling layers with window size $2\times 2$). 

(You can also see that 2. and 3. are true directly from the definition of $d(x,y)^2$). Looks fine so far. But: 

First, choose the default value $x=1$ and compute $f(x=1, y=0) = 0.2$ and $f(x=1, y=1) = 0.3$. Second, choose the default value $y=1$ and compute $f(x=0,y=1)=0.1$ and $f(x=1, y=1) = 0.3$. Change the default values to the best value. In this case, this requires no change since $x=1$ and $y=1$ are the best values, respectively. The result did not change. Report $(x=1, y=1)$ as the best parameter combination. 

For the given decision function and , we get a prediction accuracy of 84%. This is not terribly good, and it is of course specific to our decision function, which seems fairly easy to predict. To see this, define a different decision function: 

Imagine each node in your graph as a user, and each edge as an action such as 'share'. It may also be a bidirectional relationship 'share' and 'view'. Some social scientists and engineers estimate the probability that a message is 'shared' and 'viewed' given that a particular user decides to share it. This process is called "information diffusion", "information propagation", "cascading", etc. If you are interested in the details of how these calculations are performed, check out these papers: $URL$ $URL$ $URL$ Similar topics: $URL$ 

I'm wondering how other developers are setting up their local environments for working on Spark projects. Do you configure a 'local' cluster using a tool like Vagrant? Or, is it most common to SSH into a cloud environment, such as a cluster on AWS? Perhaps there are many tasks where a single-node cluster is adequate, and can be run locally more easily. 

Let's assume I'm building a content recommendation engine for online content. I have web log data which I can import into a graph, containing a user ID, the page they viewed, and a timestamp for when they viewed it. Essentially, it's a history of which pages each user has viewed. I've used Neo4J and Cypher to write a simple traversal algorithm. For each page (node) I want to build recommendations for, I find which pages are most popular amongst other users who have also visited this page. That seems to give decent results. But, I'd like to explore alternatives to see which method gives the most relevant recommendations. In addition to my simple traversal, I'm curious if there are graph-level properties I can utilize to build another set of recommendations with this data set. I've looked at SNAP, it has a good library for algorithms like Page Rank, Clauset-Newman-Moore community detection, Girvan-Newman community detection, betweenness centrality, K core, and so on. There are many algorithms to choose from. Which algorithms have you had success with? Which would you consider trying? 

I am planning to set up a JSON storage system. It will store tens of millions of JSON records, all in the same format. I'd like to be able to query the data using Apache Drill. It looks like there is Drill support for MongoDB and Postgres. However, I'm unsure of the pros and cons of each, and how I'd structure the schema if I'd choose Postgres.