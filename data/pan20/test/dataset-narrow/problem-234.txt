In the WHERE clause, I've hard-coded Id = 1 and DateTest > '2017-02-03 00:00:00.000'. When this query runs, it returns no results: 

Sure, this is what compliance auditing solutions like IBM Guardium and Imperva do. They're appliances that sit in between your SQL Server and the rest of the network, and they capture everything that happens without affecting end user activities. They're expensive - think six figures and up for the full implementation, and it's not going to be discreet. If you were looking for something free or cheap, you can try using the built-in auditing features in SQL Server, or software products like Idera Compliance Manager, but be aware that sysadmins can disable those tools (because they're likely gonna be the ones installing them anyway.) 

Quest offers a free version of their Benchmark Factory that works for Oracle, SQL Server, and MySQL: $URL$ I recorded a video on getting started with it back when I worked for Quest: $URL$ You can call your own stored procs, replay traces, run TPC benchmark scripts, and more. It's not exactly easy to use, though - thus the video. 

I'm reaching here, but I can think of at least one dangerous scenario: if you restore a database that has a filetable, those files are now on your network by default (and specifically, on your SQL Server). You could restore a virus. That by itself won't do anything, of course - the virus doesn't suddenly become sentient - but if your users then try to access the file, they could be infected. (Hey, I said I was reaching.) I'm envisioning a scenario where an outside hacker wants to get malware in the door, and he then sends an email to Bob in accounting saying, "Here's the file: \sqlserver\filetableshare\myvirus.exe" - at that point it's gone past your firewalls without detection, and we're now down to your internal antivirus and anti-malware tools. 

Correct. The finding is "No Alerts for Corruption". If you've set up alerts for any of those, we assume you know what you're doing. (That's not always the case, of course, heh.) If you'd like to change that, here's the guide on contributing to sp_Blitz. (Disclaimer: I'm Brent Ozar, the script author.) 

I'm not sure this is an answerable question - yes, there are always sites on the web using wacko codepages for their defaults. The most obvious problem when dealing with different collations is joining them together (cross-database queries), especially TempDB. If TempDB has a different collation than your database, you can run into problems just creating a temp table and joining it to your data. $URL$ 

Microsoft's Bob Dorr wrote that trace flag 8048 is no longer required for SQL Server 2016 and newer: 

For SQL Server targets: On the target server (the one that's receiving the linked server query), run sp_WhoIsActive and look at the linked server query. If the query doesn't show up, something's going awry with the linked server query and it's never reaching its destination. If it does show up, look at the Wait column, and see what the wait type is. (It won't be OLEDB.) That'll help you troubleshoot the reason it's not completing. Feel free to post that in with your question, and I can elaborate more on that specific wait type. For non-SQL Server targets like the IBM/Rocket one in this case: you'll want to work with the sysadmin/DBA on that platform, and have them check the progress of the query. When you see OLEDB on the SQL Server sending side, you just can't tell anything about what the holdup is on the other end. 

From a general perspective (not platform-specific), here's what I'd recommend mastering for data warehouse projects: Know how to load data fast. BI projects usually involve nightly loads of large amounts of data. The ETL guys need to shove data in fast with a minimum of concurrency issues. This means knowing when to disable indexes, when to perform tasks in temporary staging databases rather than the production environment, and how to offload processing to the ETL server instead of the database server. Know how to handle large table scans. BI environments usually have multiple terabytes of data, beyond what can fit in memory. Index tuning can only take you so far. You need to know how to get as much throughput out of the SAN as possible. Know how to segment archive data. BI environments usually have a small percentage of live, changing data and a much larger percentage of read-only (or read-biased) archive data. You have to know how to recognize those patterns and how to separate that data out into different tables or different storage targets with a minimum of work required by your ETL people. Know how to handle maintenance tasks. Defragmenting or rebuilding indexes is easy on a 100GB database, but not so easy on a 1TB or larger database. Maintenance windows have to be carefully planned. Backups are another story altogether. Know when to design reporting tables. If your users constantly access the same aggregate data (like grouping sales data by month or by salesperson, or constantly recalculate a profit percentage) then you need to recognize those trends in the end user queries, design a pre-calculated set of reporting tables, and train the users to access the data that way. 

The eager spool has to do with the size of data you're deleting. In this case, you're updating a large quantity of rows on a table with several indexes. You're also updating a field that's covered by a lot of nonclustered indexes, and SQL Server has to update all of those indexes as well. To avoid this operation, update less rows (as a percentage of the table) or index the table less heavily. 

In traditional Always On Availability Groups, all replication traffic comes from the primary replica. If you have two replicas in a remote data center, they will both be updated from the primary replica, period. You can't do primary-to-secondary-to-secondary. SQL Server 2016 introduced Distributed Availability Groups, which might satisfy your design goal. However, this is going to be much more complex, much less documented, and I wouldn't recommend doing this for your first AG. 

In theory, each Availability Group needs its own IP address (and virtual network name) for the listener. This way, you can fail the primary around between places - after all, you could have one AG that's a primary on server1, while you fail a separate one of the AGs over to server2 because it's having corruption problems. In practice, you can use Availability Groups without a listener, and just have your apps connect to a specific SQL Server, bypassing the listener (and IP requirements) altogether. However, this means that: 

Q: Is there a good way to force the optimizer reuse query plans in terms of parameterized SQL? Yes - think about the above things that will impact plan reuse. Obviously if you have a Software-as-a-Service type model where every client gets their own database, you can't really help things there, but you can help the other two factors. Be consistent with your application's connection settings, don't inject different spacing or comments dynamically into queries, and make sure your datatypes in the application match the datatypes in the database. Q: Is this situation normal? It's not unusual to see out in the field, especially with older NHibernate/EF versions that used different parameter lengths based on whatever parameter you were passing in (as discussed in that link.) The performance symptoms are higher CPU use (due to increased query compilations) and higher plan cache memory use, but they're not usually the largest problem on a SQL Server. 

Holy cow, you've got a lot of questions in here. Let's break this down. Q: Will SQL "move" the existing rows to maintain the clustering, or will it let the table become "fragmented"? Think of a database as a collection of pages - literal pieces of paper laid out on your desk. Think about the dictionary for now. If you wanted to add more words to the dictionary, you could add them in place if the pages had empty space. When you first start out with an empty dictionary, this is relatively easy. But think about a mature dictionary with thousands of paper pages in it, all full. When you want to add more words to that mature dictionary, odds are there isn't going to be any space left on the page. SQL Server will "tear" a page - it will take a brand new page somewhere else, and move some of the words over onto that new page. The new page would be at the end of the dictionary. The good news is that immediately after that action, there's now a half-empty page at the end of your dictionary, and also at the middle, both with space to add words. If you happen to be adding them in that order, that is. (This is why the way you load data becomes increasingly important.) Could this cause a big performance hit if the import is done one row at a time? Forget the index for a second - adding data one row at a time is just plain inefficient regardless of the indexing structure. SQL Server is a set-based system - whenever you can work in sets, you probably should. What happens when I query the data? You didn't ask this, but I'm asking it for you, hahaha. Think back about the aftermath of our inserts. Now we've got a dictionary that's mostly ordered, but when you get to a few points of the dictionary, you'll have to jump to the back to read from a few other pages. If these pages are all cached in your memory (RAM, buffer pool, etc) then the overhead just isn't going to be that large. Most memory access is random anyway - it's not like SQL Server stores your dictionary in memory in order. On the other hand, if you need to fetch the data from conventional magnetic hard drives (spinning rust), then you can end up getting a bit of a performance benefit if that data is stored in order. The real design goal here, though, is to get the data from RAM instead of getting it from drives. The difference between defragmented data on disk versus fragmented data on disk is nowhere near as significant as the difference between getting it from disk versus getting it from RAM. Should I rather just not bother with the ordering of the rows and just add an identity column as the primary key and an index on the Date column to help with my queries? Bingo: this is the difference between physical database design and logical database design. Programmers have to worry a lot about physical database design initially, but as long as your database is under, say, 100GB in size, you can fix logical design in post, so to speak. Put an identity field on there for starters, cluster on it, and then after being live for a few months, revisit the index design to maximize performance. Now, having said that, once you're experienced with this type of decisionmaking, then you'll be better equipped to guesstimate indexes right from the start. Even so, I don't even usually put much thought into index design initially. Users never seem to query the data the way I would have expected. 

Why? Because SQL Server by default chooses the victim by whichever transaction is the easiest to roll back. In our case, Window #1 had done a lot of work already, updating thousands of rows, whereas Window #2 had done just a tiny amount of work. This is a contrived example in order to teach the lesson quickly - your scenario probably doesn't involve an ALTER TABLE command. Still, the same theory applies.