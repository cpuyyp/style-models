You can try iperf, it works as a client and server so you need to install it on two computers, here's how you use it run on Server 

I think that you are missing the "RewriteEngine On" line in your configuration, Here is an examle of how it shold all look like: 

I'm not completely sure what you mean by "Virtual servers": it could be two things: (a) You want to run other instances of ubuntu server (like other physical computers) on top of your existing system. The answer to that would be: To me the best option would be KVM - this is a kernel based virtual machine, meaning that it uses the host os's kernel for tasks such as memory allocation etc.... A good place to start would be: $URL$ OR (b) if you are talking about Apache virtual servers then you should definitely look into using webmin or virtualmin to manage your server: www.virtualmin.com Hope that helps, RayQuang 

I know I can set the values using and , but I would like to test that the is doing the right thing without rebooting. How can I make sure that the values are being set when people login using ssh without rebooting? 

But this does not work because the command is dequoted by ssh twice where as GNU Parallel only expects it to be dequoted once. So instead use again: 

I have added it in /usr/lib/zabbix/externalscripts where I have a different script, that works. I have configured an item: 

I have just installed WS 7 on our stage server. The test server was done some months ago. I have deployed the same applications to stage as on test. One application is causing me problems. I call the login servlet but the result is an 'Internal Server Error' message. From my logs I can see that the servlet processed the log in successfully and redirects to a JSP. That is the last message. I can find no further info in any of the other standard out/err logs. I have tried creating a simple JSP containing only HTML - same error. If I rename the file to test.html, it displays correctly. The plugin configuration appears to be identical with the one on the test server (where everything works) Ideas anyone? 

I can, however, not find a way reload these values with out rebooting. I have read that the values are reloaded when logging in; it works when I do but it does not work through . I have the pam_limits.so in /etc/pam.d: 

It gives 77 workers. GNU Parallel is a general parallelizer and makes is easy to run jobs in parallel on the same machine or on multiple machines you have ssh access to. It can often replace a loop. If you have 32 different jobs you want to run on 4 CPUs, a straight forward way to parallelize is to run 8 jobs on each CPU: 

Has anyone seen this kind of behaviour before? I can't figure out what might lie behind this. I'd be interested to hear anyone's opinion on this. Paul 

Problem is now solved. The installation process created a default server. I had ignored this thinking that it doesn't matter. I removed the server and regenerated the plug-in config and my app works fine. 

We're deploying a simple newsletter webapp on a stand-alone LAMP platform in the company DMZ. There is some discussion as to whether the MySQL server should be removed from the DMZ and put in the internal network. The server is behind a firewall with only port 80 open and MySql will be attached to a non-standard port. The database contains customer email addresses. Is this a secure setup (or secure enough)? How much more secure would it be by placing the data behind a second firewall? (I'm more of a developer so I'm not really aware of all the security aspects here - can someone enlighten me!) Update Just for clarification and to attact more comment here is our current setup: internet - firewall1 - http server - firewall2 - appserver - firewall3 - enterprise resources This new application was supposed to go completely within the DMZ between firewalls 1 and 2. We're currently discussing pulling the MySQL server in behind the 2nd firewall. 

I have a server with 2 internal disks with Adaptec hardware RAID and an external disk box connected via SAS. finds all the devices: 

So and are clearly busy taking up 64% and 53% of a CPU respectively, but not near 100%. The chunk size (128k) of the RAID was chosen after measuring which chunksize gave the least CPU penalty. If this speed is normal: What is the limiting factor? Can I measure that? If this speed is not normal: How can I find the limiting factor? Can I change that? 

First of all just changing the subnet will not hide the private network, If I were you I would just connect it directly your modem and not into any switch or router that is connected to your secure network. In some cases this may not be possible, so I would highly recommend using a computer as a router to separate these networks. Ideally it would have three network interfaces. For example eth0 for inet, eth1 for privatenet and eth2 for guestnet. Then you could use iptables to separate the networks and even if you wanted to, prioritize your traffic over the guests so that they wont waste your bandwidth. Some good tutorials would be: $URL$ $URL$ (a firewall generator) $URL$ Hope that helps, RayQuang 

If the servers are world wide (i.e. not on your local 10 Gbps network), then $URL$ might be a solution, too. 

Ian Howson gives a good answer on why it is slow. If you delete files in parallel you may see an increase in speed due to the deletion may use the same blocks and thus can save rewriting the same block many times. So try: 

I have stop-started the on server l, and mounting l:/disk/l on another dir still hangs on server e. After booting server l I can mount on server e and everything is fine. But I would like to avoid the reboot as that disturbs users. How can I kick the nfs-kernel-server so hard that it actually restarts completely and not just half-way restarts as seen above? Edit: I have also restarted portmap. When I restart nfs-kernel-server it spends a lot of time after printing 'nfsd'. It seems it is hanging at: /usr/bin/rpcinfo -u localhost nfs 3 

Our IT department has around 90 people. We manage systems and create applications. The systems are of varying size from SAP to Lotus Notes to Tivoli Access Manager (TAM). Increasingly, the applications we create connect to these systems via web services, LDAP queries etc. As time goes by it is easy to forget what connects where so, that when changes are made, service connections can become disrupted. Example: The TAM system contains an LDAP with user information including which applications they are authorised to view. An unrelated application fetches this user information to build a dynamic web-based menu containing personalised links. The TAM crew are told to tighten their security and so close all the ports. They have long forgotten about the mini web app that builds its menu every night. The next morning the production boss is in a rage because his people cannot start their applications because the menu is half empty. Question What kind of systems could be introduced to help remind all concerned which services are in use? 

So you are running around 600 jobs per second. The overhead for a single GNU Parallel job is in the order of 2-5 ms, so when you are getting more than 200 jobs per second, GNU Parallel will not perform better without tweaking. The tweak is to have more s spawining jobs in parallel. From $URL$ 

I have a RAID60 that I want to expand. The current is: 2 axles each having 9 disks + 2 spares. The future is: 4 axles each having 10 disks + 1 spare. So I need to do some --grow to reshape the drives. I thought this would be enough: