From this point I could, as mentioned, add more lines or delete existing lines. I might insert a new rule by creating an entry numbered 90, or add one after 100 if that makes more sense. The lines can also be resequenced (adding more available numbers). More to the point, all of this can occur while the ACL is still applied to the interface. This is a good place to start in the configuration guides about ACL setup in reasonably recent IOS versions. 

There are few things being mixed together here. Tagging generally takes place on the actual ethernet interface (i.e. eth0.2) while bridging doesn't usually require any kind of explicit tagging (although there are exceptions to this). OK - I am going to assume that you want VLAN 2 and VLAN 100 to pass over ethernet 0. 1.) You want to create eth0.2 and eth0.100. Don't put an IP address on either (inet manual). 2.) Create br2 and br100 (for convenience) and assign the IP's you'd like to use in these VLAN's (inet static). 3.) eth0.2 will be a bridge_port in br2. eth0.100 will be a bridge_port in br100. 

You could also add this to be run automatically by creating a script called that would look something like this: 

If no interface (or vty) mentions these ACL's then they are not applied. There's no such thing as a globally applied ACL in IOS - particularly given that an ACL can mean very different things in different contexts (i.e. route-maps, QoS and packet filtering can all use the same structure). 

You'd have to add logic to deal with duplicate names but you could start with a very basic usage of find, like find ./ -name *jpg -execdir /tmp -exec /bin/ln -s '{}' \; executed from the directory where you want the links to appear. 

Use multicast. Each client would need to join the group, but this is a very low overhead task. You'd have the additional benefit of being able to have clients on other machines be able to efficiently receive the message. Failing this, look into one of the various message bus packages (i.e. mqueue, rabbitmq, etc) that will allow for reliable delivery of programmatic information to various processes without a need to reinvent the wheel. 

Take a look and see if there are any errors accruing on the switch ports or the server's NIC ports - ideally before/after an extended session sending a lot of traffic in both directions. If the counters are staying clean then it will probably be OK for a few days but it's something that's so cheap and easy to fix that there's no reason to leave it in place any longer than you have to. 

Yes - this card appears to only support 10G. It is possible that if the card also has an SFP+ slot that it might support 1000baseSX fiber (or similar) but without knowing more about the model of NIC, etc it is hard to say. 

For the sake of convenience there are bandwidth-delay product calculators available - one such calculator is here. As to large windows causing issues in the event of packet loss - that's pretty much exactly why TCP windowing is variable. Upon packet loss the window size will decrease, allowing for less data in flight and a consequent reduction in transmission speed. After a period of time the window size will renegotiate. Your latency actually isn't that bad for satellite - a 1s RTT @ 1M is only a 125K window. A good number of modern operating systems would easily support this right out of the box, so additional modifications might not be required. As an aside - some have had very good luck with the various WAN optimizers available on the market. These tend to both optimize TCP window sizes as well as utilizing caching and compression to both push more through the link and improve apparent responsiveness. 

This is most likely a firmware bug, but the only situation I've heard of that -sources- packets from 0.0.0.0 is RARP (reverse ARP) which sends out requests from an all zero MAC to find its IP address. This used to be a component of diskless boot once upon a time but hasn't been seen in quite a while. It may be that the pinging station still held the ILO's MAC address in cache - and, of course, the intermediary switch. If the ILO had lost its address it could have been going through a RARP / BOOTP process. Or it just lost its mind. 

Both machines (or any other multicast capable device) can be a sender, a receiver or both. So, yes, assuming the protocol is appropriate your receiving machine can use the same multicast group to respond to the source. 

1.) The first hit on the server and the first invocation of the PHP script in question is actually causing a number of configurations to be parsed, interpreters (possibly) loaded, scripts loaded, modules in the web server coming online, etc. In subsequent queries it's almost invariably faster. 2.) Even under ideal conditions there are a number of moving parts in play during your 0.5 - 2 sec - between your machine going through the setup of several TCP sessions (each requiring a few msec of handshaking), your browser identifying itself and working out capabilities with the server, your browser finally submitting its request and then the server having to parse that information and pass it up and down the stack to then finally format it into HTML to send back to you. There's quite a bit of sophistication in the implementation of modern scripting / web platforms. It may seem a bit heavy when compared to pinging a server or putting up a static "Hello world!" html page, but the generalized capabilities are pretty impressive. The other point, of course, is that you're describing a completely stock system. There are almost always things that can be done to tune it to better respond to your particular workload. 

A given IP address a is the resolver that we wish to make more available. The a host is a member of the A /24 subnet. Anycast can be accomplished with specific host routes (i.e. a/32) but this is generally only seen within private networks, not on the general Internet. There is some mechanism in place such that the A subnet is dynamically announced only when the corresponding DNS service is operational. Please note (and this is really important) that the advertisement itself could be coming from a single host within a site that runs a resolver, from an entire physical site containing multiple instances of said resolver (i.e. many hosts running resolvers, the site as a whole sharing a single route). The same route (A) will be advertised from multiple points on the public Internet. This might take the form of a large provider (read: points of presence dispersed across the globe) presenting the same route at each point of interconnection with foreign networks or the same route coming from points hosted within multiple carriers. 

A VLAN is a layer 2 construct - generally an Ethernet broadcast domain mapped onto a set of ports on one or more switches. An IP subnet is a layer 3 construct and is a collection of hosts within a common address grouping with local reachability. An IP subnet can run on a VLAN, but it can also run on a wide variety of other media. It is possible for two or more IP subnets to run concurrently on a single VLAN as long as the subnets do not overlap. This often occurs in environments migrating from one addressing scheme to another or when port space / VLAN capacity are limited. Typically the mechanism in use is secondary IP addressing for one or more hosts - usually a router. A secondary address is equivalent to an IP alias and is essentially just a single network interface with addresses in multiple subnets. 

At first blush I would look for sources of RF interference, which are quite common in the 2.4GHz band. Microwave ovens and cordless phones are often the culprits but there are plenty of other things that can cause problems. I'd also suggest setting up a wireless sniffer on a laptop (or even a droid phone) and watching the general signal levels before, during and after a service interrupting event. There are quite a few out there to choose from based on your platform - various iterations of stumbler, setting up wireshark to run in monitor mode, etc.. These tools should let you get some sense of whether it's a general blast of RF causing your problems or something from a specific station and/or rogue AP. 

Look into the umask command. Set it as needed in /etc/profile and your users will, by default, create files with the privs specified. Depending on the FTP software you're using it also may be possible to set this behavior in its configuration file. If you haven't, I'd recommend taking a look at something like this which will encapsulate a lot of the capabilities you need while not necessarily requiring that your users have actual shell accounts. 

It may be limited by the network, but not necessarily simply a question of bandwidth. Latency of your remote test unit will have an effect on the number of connections pending at any given time (waiting 50ms for acknowledgements is a lot different than .5ms locally) as well as on the negotiation and stabilization of window sizes as the connection progresses. You're also likely exposed to some amount of packet loss - either as a function of congestion or as the mechanism of bandwidth limitation on the part of your carrier (or those upstream). I'd suggest eliminating as much as possible from the equation to draw a sensible baseline. Measure peak bandwidth, latency and packet loss from your server to a few points on the general Internet. As unlikely as it might sound, try searching for "Voip traffic test" or similar. Several providers of VOIP services have apps that can measure these sorts of patterns (bidirectionally) with a fair degree of accuracy. Once you have some valid empirical data as to the actual useful speed of your link then your results may well be validated. In addition to bandwidth tests it might also be useful to look at a packet capture of the sub-par web traffic to look for excessive numbers of retransmissions as well as measuring the apparent time your server is taking to respond to requests (..if this value is increasing substantially as a function of number of connections, this is a big clue). 

Have you considered using the IP-MROUTE-STD-MIB rather than the IGMP MIB? You can get statistics on a per-mroute basis - which will give you a much better view of the source in particular. There's also a set of Cisco extensions to this MIB that can provide more in the way of platform-specific info. One item you can potentially look for is a substantial difference in counters on your various routers through the path of the mroute. Some delta is to be expected but this is would be a good place to track thresholds. For tracking streams freezing there's a pretty easy answer: ip multicast heartbeat ($URL$ You can configure a given router to throw an SNMP trap if no packets are seen on a configured multicast group for 10 seconds. There is also a feature called mrm (multicast route monitor) that can be called from the Cisco CLI to set up and track synthetic multicast groups. You'd likely want to use EEM or similar to call it periodically and then throw a trap or syslog if it doesn't behave normally. This is also a good troubleshooting tool. Also - just as you (should) monitor for changes in IGP adjacency, so too should you track on PIM. Events like neighbor state changes, elections, etc can indicate instability in the tree. It's not -necessarily- a big deal in all cases but should generally be quiet on a stable network. I'm not sure which supervisor you're running in your 4500's, but some of the more recent models support netflow for multicast. This would give you a much more granular and global view of multicast performance and would naturally lend itself to statistical trending, storage, etc.. definitely a good way to go. I hope this helps-