While an EE undergrad I attended some lectures that presented a nice characterization of boolean circuits in terms of how many nested loops they have. In complexity, boolean circuits are often thought of as dags, but in real hardware cycles are common. Now, modulo some technicalities regarding what a loop is and what constitutes a nested loop, the claim was basically that in order to implement in hardware an automaton one needs two nested loops, and in order to implement a processor one needs three nested loops. (I might be off-by-one with these counts.) Two things bother me: 

Build a bipartite graph as follows: For each vertex $x$ in the original graph introduce a left vertex $x_L$, a right vertex $x_R$, and an arc $x_L\to x_R$ whose cost is huge (bigger than the sum of costs in the original graph). For each arc $x\to y$ in the original graph, introduce an arc $x_L\to y_R$ in the bipartite graph. Find a minimum cost perfect matching in the bipartite graph. 

Exact Exponential Algorithms is a nice recent book about such algorithms. Algorithm X for the exact cover problem is also good to know. 

It's called finding a minimal model of a Horn formula. This model is unique because the intersection of two models of a Horn formula is itself a model. In fact, [Horn, On sentences which are true of direct unions of algebras, 1951] proved the following: A boolean function can be expressed as a conjunction of Horn clauses if and only if its set of models is closed under intersection. 

a randomized reduction from USAT to U$\Pi$, since there's one from SAT to U$\Pi$. ​ For most natural problems, if U$\Pi$ is non-trivial then one can find a simple reduction from SAT to $\Pi$ that preserves number of solutions. ​ In particular, it's reasonable to expect a reduction from USAT to U$\Pi$. ​ However, I have no clue regarding whether-or-not there's necessarily 

$\{\hspace{-0.02 in}0,\hspace{-0.05 in}1,\hspace{-0.03 in}2,\hspace{-0.03 in}3,...\operatorname{length}(x_0)\hspace{-0.03 in}+\hspace{-0.03 in}\operatorname{length}(x_1)\hspace{-0.03 in}-\hspace{-0.04 in}1\hspace{-0.02 in}\}$, conditioned on the same event, $2/3$$<$ $\operatorname{Prob}[\text{Automaton gives more than } i \text{ bits of output and}$ 

represent distinct eventually-periodic strings and for each such eventually-periodic string, the left entry of its triple 

Your problem is equivalent to the CONF problem. $g^b \; = \; g^{\hspace{.02 in}b\cdot 1} \; = \; g^{\hspace{.02 in}b\cdot r\cdot \operatorname{modinv}(r\hspace{-0.02 in},\hspace{.02 in}p)} \; = \; \left(g^{\hspace{.02 in}b\cdot r}\hspace{-0.03 in}\right)^{\hspace{-0.02 in}\operatorname{modinv}(r\hspace{-0.02 in},\hspace{.02 in}p)} \; = \; \left(g^{br}\hspace{-0.03 in}\right)^{\hspace{-0.02 in}\operatorname{modinv}(r\hspace{-0.02 in},\hspace{.02 in}p)} \; = \; \left(g^{rb}\hspace{-0.03 in}\right)^{\hspace{-0.02 in}\operatorname{modinv}(r\hspace{-0.02 in},\hspace{.02 in}p)}$ One can find an suitable output for your problem by chossing $\: r=2 \:$ or $\: r=1$ 

(i.e., it's literally impossible for the player to lose) and p1_info and p2_info and number_of_choices are independent of the state 

Answering my own question, there's now a preprint on arXiv showing that double-exponential is the correct dependence, assuming the exponential time hypothesis. See "Known algorithms for EDGE CLIQUE COVER are probably optimal", Marek Cygan, Marcin Pilipczuk, and Michał Pilipczuk, arXiv:1203.1754 and SODA 2013 

It's not really a graph problem, I think. Problems in which you try to choose some objects to minimize the difference between the min and max chosen object are called "minimum range" problems — e.g. look up "minimum range cut", which is a graph problem — but this one looks like you're trying to find the minimum range basis of a partition matroid with one partition set for each of your groups. In general minimum range matroid basis problems can be solved in $O(n\log n)$ time, plus $O(n)$ steps of a subroutine that finds a basis of a set with corank one: sort the elements from smallest to largest and then process the elements one by one. While you process the elements maintain an independent set $I$; when you process an element $e$, add $e$ to $I$ and, if that addition causes $I$ to become dependent, kick out the minimum weight element in the unique circuit of $I$. The minimum range basis is one of the sets $I$ that you found in this process: the one that has full rank and has as small a range between min and max as possible. For your partition matroid special case it's easy to find the circuit in $I$ when it becomes dependent: a circuit happens when $e$ belongs to the same group as an element $f$ that you previously added, and $f$ is the element you should kick out of $I$. So the whole algorithm takes $O(n\log n)$ time, or possibly faster depending on how much time it takes to do the sorting step. 

Warmup: ​ With respect to polynomially-closed reduction classes that can't solve all of FNPSPACE, FNPSPACE-hardness is different from NPSPACE-hardness. Proof: ​ ​ ​ ​ ​ ​ ​ Let R be the relation given by xRy if and only if [x$\in$QBF$\hspace{.02 in}$ and y is the empty string]. 

takes n as input and outputs the lexicographically least oracle-circuit of size at most j+n$^{\hspace{.03 in}j}$ 

polynomial-length assignment such that when run with that presampling, the circuits' probability of finding a solution is greater than $1\hspace{-0.04 in}\big/\hspace{-0.07 in}\left(2\hspace{-0.06 in}\cdot \hspace{-0.06 in}\left(\hspace{-0.02 in}n^{2+j}\right)\hspace{-0.04 in}\right)$. ​ Since such circuits cannot make queries longer than j+n$^{\hspace{.03 in}j}$ bits, presampled inputs longer than that can be ignored, so such presampling can be efficiently-and-perfectly simulated with a random oracle and poly(n) hard-coded bits. ​ That means there are polynomial-size oracle circuits such that with a standard random oracle, the circuits' probability of finding a solution is greater than $1\hspace{-0.04 in}\big/\hspace{-0.07 in}\left(2\hspace{-0.06 in}\cdot \hspace{-0.06 in}\left(\hspace{-0.02 in}n^{2+j}\right)\hspace{-0.04 in}\right)$. ​ Such a random oracle can in turn be efficiently-and-perfectly simulated with just ordinary random bits, so there are polynomial-size probabilistic non-oracle circuits whose probability of finding a solution is greater than $1\hspace{-0.04 in}\big/\hspace{-0.07 in}\left(2\hspace{-0.06 in}\cdot \hspace{-0.06 in}\left(\hspace{-0.02 in}n^{2+j}\right)\hspace{-0.04 in}\right)$. ​ In turn, by hard-coding optical randomness, there are polynomial-size deterministic (non-oracle) circuits whose probability (over the choice of x) of finding a solution is greater than $1\hspace{-0.04 in}\big/\hspace{-0.07 in}\left(2\hspace{-0.06 in}\cdot \hspace{-0.06 in}\left(\hspace{-0.02 in}n^{2+j}\right)\hspace{-0.04 in}\right)$. 

C.A.R. Hoare, An Axiomatic Basis for Computer Programming. From the abstract: In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. It has six pages that are quite easy to follow. 

This is not an answer. It is a simple but long observation. I hope it will be useful. The decision version of your problem is: Does $\cal X$ contain a subset of $A$? This problem is related to the problem of evaluating monotone boolean functions of $n$ variables. A subset of $\{1,\ldots,n\}$ is equivalent to an $n$-bitstring, so the family $\cal X$ is equivalent to a boolean function $f$ of $n$ variables. Given a function $f$, one can define the least monotone function that is not bigger than $f$, namely $g(y)=(\exists x\subseteq y,\,f(x))$. The original problem is then reduced to evaluating $g(A)$. Conversely, the problem of evaluating a monotone boolean function can be reduced to the original problem, either naively by taking $f=g$ or by choosing an $f$ that makes $\cal X$ smaller. In practice BDDs tend to work well. So one possible approach is to build the BDD for $f$, derive from it the BDD for $g$, and then evaluate $g$. The average size of the BDD for $g$ must be $\Omega\left(\binom{n}{n/2}\right)$, because there are many monotone boolean functions. Hence, in theory this is a bad solution. But (1) a better analysis might be possible and (2) there might be tweaks to this approach that make it better. For example, I didn't use in any way the correlation between the size of $\cal X$ and the size of $g$'s BDD. (There must be a correlation, but I don't know if it is simple or usable here.) For completeness, a simple algorithm for computing the BDD for $g$ from the BDD for $f$ is the following. $$m(x?f_1:f_0)=x?(m(f_0)\lor m(f_1)):m(f_0)$$ Here $\lor$ is the standard or-operation on BDDs. 

Since $H_1$ is involved in the same place in each case, any collision in $H_2$ is also a collision in $H_1$. $\:$ By Merkle-Damgard's security, any collision in $H_1$ can be efficiently transformed 

CNF-SAT ​ $\to$ ​ your problem ​ ​ ​ : The subscripts are the variables and the superscripts indicate whether the 

efficiently computable (deterministic) function $\;\; f \: : \: \{0\hspace{.01 in},\hspace{-0.03 in}1\}^L \: \to \: \{n\text{-bit primes}\} \cup \{\perp\} \;\;$ and 

For a subset of $\mathcal{S}$ to be pairwise disjoint, it cannot have sets from opposite literals and 

of messages each of which can only have one length. Information quantity is subadditive with respect to forming a multiset of messages 

$[$the linear-exponent version of QIP[2]$] \;\; \cap \;\; [$the linear-exponent version of coQIP[2]$] \;\;\;\;\;$. 

smallest prime that is not less than ​ $2\hspace{-0.04 in}\cdot \hspace{-0.04 in}C\hspace{-0.04 in}\cdot \hspace{-0.04 in}j^{\hspace{.02 in}2}$ , ​ and then use wikipedia's string hashing method. 

$5$ $k$-ary comparisons suffice for a $\: \left(4\cdot \left\lfloor \frac{k}2 \right\rfloor \right)$-ary $\:$ comparison. For $\: 4\leq k \:$, $\:$ $5\:\:k$-ary comparisons suffice for a $\: \left(\frac32 \cdot k\right)$-ary $\:$ comparison. For $\: 2\leq k\leq n \:$, $\:$ $\log_{\frac32}\left(\frac{n}k\right) = \frac{\log_2\left(\frac{n}k\right)}{\log_2\left(\frac32\right)} \:$. For $\: 2\leq k\leq n \:$, $\:$ $5^{\left\lceil \frac{\log_2\left(\frac{n}k\right)}{\log_2\left(\frac32\right)} \right\rceil}$ $k$-ary comparisons are sufficient to sort. For $\: 2\leq n \:$, $\:$ at least one $k$-ary comparison is necessary to sort. 

Is this true or false? If the conjecture is true, then it is NP-hard to find the root of a minimal decision tree which represents a monotone boolean function given in DNF. In turn, this would imply that it is NP-hard to minimize a decision tree that represents a monotone boolean function. A related but weaker result appears in [Zantema, Bodlaender, Finding Small Equivalent Decision Trees is Hard, 1999]; in particular, their proof uses non-monotone functions. 

Since I was myself somewhat confused, I begin by clarifying a few concepts in the question. Collection. I see no reason to spend time rigorously defining what "collection" means when we can simply ask what happens for data structures in general. A data structure occupies a piece of memory and has some operations that may access that memory and that may be invoked by users. These users may be distinct processors or just different threads, it does not concern us. All that matters is that they may execute operations in parallel. Lock-free. Herlihy and Boss say that a data structure is lock-free when a crashing user does not prevent further uses of the data structure. For example, imagine one pours water on a processor that is in the midst of inserting a node in a sorted set. Well, if other processors try later to insert into that sorted set, they should succeed. (Edit: According to this definition, it is the case that if a data structure uses locks then it is not lock-free, but it is not the case that if a data structure does not use locks then it is lock-free.) With these definition, I think Herlihy and Boss basically say that the answer is to turn critical regions into transactions. But, you may ask, does this have the same complexity? I'm not sure the question makes sense. Consider . Is this a constant time operation? If you ignore the locking operation and hence other users then you can answer YES. If you do not wish to ignore other users, then there really is no way to say whether push will run in constant time. If you go one level up and see how the stack is used by some particular algorithm, then you might be able to say that push will always take constant time (measured now in terms of whatever happens to be the input of your parallel algorithm). But that really is a property of your algorithm, so it doesn't make sense to say that push is a constant time operation. In summary, if you ignore how much a user executing an operation waits for other users, then using transactions instead of critical regions answers your question affirmatively. If you don't ignore the waiting time, then you need to look at how the data structure is used. 

I don't believe that Python's grammar is context free. The requirement that lines in the same block of code have the same amount of indentation is not the sort of thing that context free grammars handle well. More precisely, there appears to be a homomorphism from the language of Python blocks of the form 

Add $n-2k$ extra vertices, each connected to all the original vertices with zero-weight edges, and add a large enough number $W$ to each of the original edges to make their weights all positive. Then look for the minimum weight perfect matching. 

The process you seem to be looking for (merging two descriptions of labeled trees) is called unification. According to the linked Wikipedia article it can be solved in linear time. 

Re your first question, re whether this type of graph has a name: it is the line graph of a bipartite graph (the bipartite graph that has a vertex for each nonempty row or column and an edge for each point). Line graphs of bipartite graphs are an important subclass of the perfect graphs. They are generally not planar (e.g. if you have five points on the same row or column you get a $K_5$ subgraph). 

Observe that a $k$-ary comparison suffices for $\: \left\lfloor \frac{k}2 \right\rfloor \:$ simultaneous (binary) comparisons. For $\: 2\leq k \:$, $\:$ $\;\; \left\lfloor \frac{k}2 \right\rfloor \: \in \: \Theta(k) \;\;$. By the AKS network, for $\: 2\leq k\leq O(n) \:$, $\:$ $O\left(\frac{n\cdot \log_2(n)}k\right)$ $k$-ary comparisons are sufficient to sort. When $\: n\leq k \:$, $\:$ $1 \:\: k$-ary comparison is sufficient to sort. $\quad$ $1\in O\left(\frac{n\cdot \log_2(n)}k\right)$ 

Fix some such randomness for each update, giving a deterministic algorithm, which I'll call DSSEA, whose error probability on that input distribution is at most 1/6. ​ Consider a guesser G which 

For each clause ​ $\color{grey}{w\vee \hspace{.02 in}}x\vee y\vee z$ , ​ create variables $a$ and $b$ and the terms $\color{grey}{\lnot a\land \lnot b\land w \hspace{.15 in},}\hspace{.12 in}a\land \lnot b\land x\hspace{.15 in},\hspace{.09 in}a\land b\land y\hspace{.15 in},\hspace{.15 in}\lnot a \land b\land z \hspace{.3 in}$. 

who's objective function can be computed in deterministic polynomial time, BotL can be implemented deterministically in polynomial time. $\:$ Furthermore, the value returned by BotL 

the strings represented by such pairs are equal, NC2 can build the deterministic finite automata from those representations which give one output per step, construct their product, and check