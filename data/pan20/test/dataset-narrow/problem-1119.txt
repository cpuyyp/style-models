Actually, the complete and general description of a problem that can be solved by a greedy algorithm is a matroid embedding, which generalizes both the concept of a matroid and that of a greedoid. The answer is no—a problem solvable by a greedy algorithm need not have a matroid structure, but it will have the structure of a matroid embedding (which is, alas, much more complicated). A mental model for some of this could be finding minimum spanning trees. The structure used by Kruskal's algorithm is a matroid, but that used by Prim's algorithm (which requires a start node) is not. (It is, however, a greedoid—and a matroid embedding.) Helman et al. (1993), in their paper An Exact Characterization of Greedy Structures define their notion of a greedy algorithm in terms of set systems, which is the same formalism that is used for matroids and greedoids. A set system $(S,\mathcal{C})$ consists of a set $S$ and a collection $\mathcal{C}$ of subsets of $S$, the so-called feasible sets. A basis for the set system is a maximal feasible set, that is, a set that is feasible but not contained in any other feasible set. An objective function $f:2^S\rightarrow\mathbb{R}$ associates each subset of $S$ with a value. An optimization problem, in this formalism, consists in finding a basis of maximum objective value for a given set system and objective function. The greedy algorithm, defined in terms of this formalism, is quite simple: You start with the empty set, and successively add a single element until you reach a basis, always ensuring that (i) your set is feasible at each step, and (ii) the element you add maximizes the objective function of the resulting result, wrt. all the alternative elements you could have added. (That is, conceptually, you try adding all feasible alternatives, and choose the one yielding the highest objective value.) You could, perhaps, argue that there might be other forms of greedy algorithm, but there are several textbooks on algorithms and combinatorial optimization that describe this set-system based algorithm as the greedy algorithm. That doesn't prevent you from describing something that doesn't fit, but could still be called greedy, I suppose. (Still, this does cover anything that could potentially have a matroid structure, for example, though it is much more general.) What Helman et al. do is that they describe when this algorithm will work. More specifically: 

EDIT: again, (part of) the basic idea is that pushouts act as a union with some glue. This allows defining "rewrite rules" for graphs — you match the left-hand side to the graph, and then glue the right-hand side to the (rest of) the graph in a corresponding manner. I'm afraid I can't add details because I've never gotten more than the intuition. 

I have to admit that I'm slightly fuzzy on formalizing the parametricity proof needed here, but this informal use of parametricity is standard in Haskell; but I learned from Derek Dreyer's writings that the needed theory is being quickly worked out in these last years. EDITs: 

A research paper contributes new knowledge in a field (novelty). To convince readers of this new information, it provides appropriate evidence to argue for it. What's appropriate evidence varies depending on the field and on what the paper claims. In fact, since a paper must be novel, it must also argue that its content is novel (though that's done somewhat implicitly). That's done by comparing to related works in the field; if another work looks similar to a submission, the author must explain what's the novelty in his work. Moreover, no paper is entirely novel, so a paper must also say what was already known by citing its invention (except for things that are standard). As a rule, technical articles don't do that; they typically don't claim to be novel. Sometimes the author invented the technique by himself, but it was already known. Good call for papers mention the criteria for judging papers — this one mentions novelty, importance, evidence and clarity. A good research paper should also contribute knowledge that is "interesting", and explain it clearly to the reader. What's interesting varies according to the publication venue. In fact, my favorite intro to research in book form is The craft of research, though what I've read is parts of this condensed version. I'd recommend to start reading it from the Amazon preview. 

You could get a Google Scholar profile, and it'll keep feeding you recommendations it thinks will be relevant to you, based on your publications. 

As has been pointed out, this problem is similar to the more commonly known edit distance problem (underlying the Levenshtein distance). It also has commonalities with, for example, Dynamic Time Warping distance (the duplication, or “stuttering,” in your last requirement). Steps toward dynamic programming My first attempt at a recursive decomposition along the lines of Levenshtein distance and Dynamic Time Warping Distance was something like the following (for $x=x_1\ldots x_n$ and $y=y_1\ldots y_m$), with $d(x,y)$ being set to $$ \min \begin{cases} d(x,y_1\ldots y_{m-1})+1 & &\text{▻ Add letter at end}\\ d(x,y_2\ldots y_m)+1 & & \text{▻ Add letter at beginning}\\ d(x,y_1\ldots y_{m/2})+1 & \text{if $y=y_1\ldots y_{m/2}y_1\ldots y_{m/2}$} & \text{▻ Doubling}\\ d(x_1\ldots x_{n/2},y)+1 & \text{if $x=x_1\ldots x_{n/2}x_1\ldots x_{n/2}$} & \text{▻ Halving}\\ d(x_1\ldots x_n,y) + 1 && \text{▻ Deletion}\\ d(x_1\ldots x_{n-1},y_1\ldots y_{m-1}) & \text{if $y_n = y_m$} & \text{▻ Ignoring last elt.}\\ \end{cases} $$ Here, the last option basically says that converting FOOX to BARX is equivalent to converting FOO to BAR. This means that you could use the “add letter at end” option to achieve the stuttering (duplication) effect, and the deletion at an point. The problem is that it automatically lets you add an arbitrary character in the middle of the string as well, something you probably don't want. (This “ignoring identical last elements” is the standard way to achieve deletion and stuttering in arbitrary positions. It does make prohibiting arbitrary insertions, while allowing additions at either end, a bit tricky, though…) I've included this breakdown even though it doesn't do the job completely, in case someone else can “rescue” it, somehow—and because I use it in my heuristic solution, below. (Of course, if you could get a breakdown like this that actually defined your distance, you'd only need to add memoization, and you'd have a solution. However, because you're not just working with prefixes, I don't think you could use just indexes for your memoization; you might have to store the actual, modified strings for each call, which would get huge if your strings are of substantial size.) Steps toward a heuristic solution Another approach, which might be easier to understand, and which could use quite a bit less space, is to search for the shortest “edit path” from your first string to your second, using the $A^\ast$ algorithm (basically, best-first branch-and-bound). The search space would be defined directly by your edit operations. Now, for a large string, you would get a large neighborhood, as you could delete any character (giving you a neighbor for each potential deletion), or duplicate any character (again, giving you a linear number of neighbors), as well as adding any character at either end, which would give you a number of neighbors equal to twice the alphabet size. (Just hope you're not using full Unicode ;-) With such a large fanout, you might achieve quite a substantial speedup using a bidirectional $A^*$, or some relative. In order to make $A^*$ work, you'd need a lower bound for the remaining distance to your target. I'm not sure if there's an obvious choice here, but what you could do is implement a dynamic programming solution based on the recursive decomposition I gave above (again with possible space issues if your strings are very long). While that decomposition doesn't exactly compute your distance, it is guaranteed to be a lower bound (because it's more permissive), which means it'll work as a heuristic in $A^*$. (How tight it'll be, I don't know, but it would be correct.) Of course, the memoization of your bound function could be shared across all calculations of the bound during your $A^*$ run. (A time-/space-tradeoff there.) So… The efficiency of my proposed solution would seem to depent quite a bit on (1) the lengths of your strings, and (2) the size of your alphabet. If neither is huge, it might work. That is: 

One approach is described by Georgios Fourtounis and Nikolaos S. Papaspyrou. 2013. Supporting Separate Compilation in a Defunctionalizing Compiler. SLATE 2013. As @gasche mentions: 

Scientists learn new knowledge for humankind, and then share what they learn through research papers. Moreover, we live in information overload, so readers only have time for the most important knowledge. 

I expect the answer to be "obviously yes", but to my inexperienced eye, that's not directly obvious, because the definition of infinite Böhm-reduction does not include a transitivity rule (it wouldn't work), and because I couldn't find a relevant lemma in the papers themselves. I'm referring in particular to the definition by Czajka [1] of the relation $\rightarrow^\infty_{\beta\bot}$, called infinitary Böhm-reduction. I've looked at [2], which however does not include Böhm-reduction (such that the defined relation isn't confluent IIUC, which is a problem for me). Rationale: Defining reduction for infinitary $\lambda$-calculus is tricky. In particular, you cannot create an "infinite transitive closure" which allows an infinite number of transitivity steps, but you need to be more careful. In particular, if you define multi-step reduction coinductively, you cannot include a transitive rule, lest your relation becomes total and thus degenerate. So one ends up doing transitivity elimination, which is not always trivial; and given how unintuitive coinduction is, I'm afraid I'd fool myself when attempting a proof. [1] Łukasz Czajka, 2014. A Coinductive Confluence Proof for Infinitary Lambda-Calculus. Proc. of Rewriting and Typed Lambda Calculi, Springer. $URL$ [2] Jorg Endrullis and Andrew Polonsky, 2011. Infinitary Rewriting Coinductively. In Proc. of TYPES, volume 19 of LIPIcs, pages 16–27. Schloss Dagstuhl. $URL$ [3] Richard Kennaway, Jan Willem Klop, M. Ronan Sleep, and Fer-Jan de Vries, 1997. Infinitary lambda calculus. Theoretical Computer Science, 175(1):93–125. $URL$ 

In other words: For structures such as these (which basically embody the kind of structures usually thought of when working with greed), exactly the set of matroid embeddings can be solved greedily. The definition of a matroid embedding isn't all that hard, so proving that a given problem is or is not a matroid embedding is certainly feasible. The Wikipedia entry gives the definition quite clearly. (Understanding the proof why these are the exact structures solvable by greed—that’s another matter entirely…) If your problem can be formulated in terms of selection from a weighted set system with a linear objective function, and if you can show that it is not a matroid embedding, then you have showed that it cannot be solved greedily, even if you haven't been able to find a counter-example. (Although I suspect finding a counter-example would be quite a bit easier.) This approach isn't entirely without problems, I suppose. As you say, the general idea of greed is rather informal, and it might well be possible to tweak it in such a way that the formalism of linearly weighted set systems doesn't apply. 

A really simple solution: Build a suffix tree for the first string, $S$, and annotate all nodes with $s$. Then insert all suffixes of the second string, $T$. Annotate nodes you pass through or create with $t$. The path label for any node that is annotated with both $s$ and $t$ is a substring of both $S$ and $T$. (See, for example, these lecture notes a quick web search turned up.) 

*Apparently, some (non-formalists) claim that set theory is not "just syntax", but something ontologically different. I'll ignore this subtle philosophical issue; the only reference I know on it is Raymond Turner's Understanding Programming Languages. 

A nice example is Tate et al.'s Generating Compiler Optimizations from Proofs. He uses pullbacks and pushouts as generalized unions and intersections, in categories where arrows are (IIRC) substitutions. Ross Tate claims (on the paper webpage) that details were overwhelming without the abstraction afforded by category theory. Personally, I'd like to submit as "suggestive evidence" (if there can be any evidence of such a claim) diagrams (6) and (7) in their paper — they look complex enough in diagrammatic form. Let me quote their comments inline. 

Background It's well known that, in a bicartesian closed category (BCCC), if the initial and final object coincide (that is, if the category has a zero object) the category collapses (with all types being isomorphic) by $A \cong A \times 1 \cong A \times 0 \cong 0$ for all $A$. This means, for instance, that the category of pointed sets, with its zero object, can't be bicartesian closed. But the category SCpo also has a zero object for the same reason: all objects are structured sets (CPOs) with a bottom element $\bot$, and arrows are strict (and ($\omega$-)continuous) functions, so they preserve $\bot$. Indeed, this is attributed to Smyth and Plotkin (1982), who describe this category as $\mathbf{CPO}_{\bot}$ and state it lacks categorical products; other categories they consider lack other features of a BiCCC (e.g., their $\mathbf{CPO}$ lacks sums). What is not clear to me is whether every way of handling $\bot$ falls into this trap. However, knowledgeable people on Reddit seem to repeat this claim without good sources (Filinski's master thesis was the best reference I got, and it doesn't lay out a generic categorical argument). 

As far as I can see, the value of each item depends on which bin it is added to. Its full quality if it is added to its primary preference, and a reduced quality (reduced by -0.5 and -1, respectively) for its secondary and tertiary preferences. Do I understand you correctly? In this case, this problem can be formulated as a min-cost flow problem, which resembles min-cost bipartite matching (but with the added twist of bin capacity). I.e., it is not an NP-hard bin packing problem at all (unless P=NP). Construct a flow network with a source, a sink, and two "layers" of nodes, corresponding to the items (first layer) and bins (second layer). Add edges from the source to the items (zero cost, capacity 1) and from the bins to the sink (zero cost, capacity equal to the bin capacity, i.e., from 1 to 3). From each item, you add an edge to each of its primary, secondary and tertiary bins, with capacity 1 and a cost of its adjusted quality multiplied by -1 (to go from a positive value to a negative cost). Now just run a standard min-cost-flow (or min-cost max-flow) algorithm to get your answer. Of course, not all items will be matched if the total capacity is less than the number of items, but the match will produce the matching that gives the greatest total (adjusted) quality. If you don't want to muck about with min-cost flow, you could split each bin node into multiple nodes (the number of nodes corresponding to the bin capacity), and duplicate the edges from the items. So if an item has an edge with a given cost to a bin node with a capacity of 3, you'd now have 3 bin nodes, and that item would have an edge to each of them with the given cost. You could then just use an algorithm for min-cost bipartite matching, such as the Hungarian algorithm. (You will now no longer have a source or a sink, of course.) This latter version is probably more practical to implement, and libraries for the Kuhn-Munkres algorithm are available in multiple languages. 

Then, to show that logical equivalence implies observational equivalence, one only need show that logical equivalence is a consistent congruence. However, the other direction requires some more work: in particular, to show that logical equivalence is a congruence, one does proceed by induction on contexts. EDIT: there's a big problem with this approach: you wouldn't get . Let be the type of vectors of naturals of length (assuming that can be defined or encoded), and consider using as a context (well, you'd need in fact a context ending in , but bear with me). Since $n + 1 = 1 + n$ is not a definitional equivalence, types and are incompatible, hence and are not observationally equivalent. 

You can "link" those types and handlers with a specialized linker. Unlike using open datatypes, you concatenate the list of constructors and the case functions. But the linker must add cases for partial application: Without whole-program analysis, you cannot predict which partial applications can be used for which function, so you add all cases. An $n$-ary function can be partially applied to $i$ arguments (with $0< i < n$) and produce a function of arity $n-i$, which can be partially applied again. 

I was reading about Call-by-Push-Value in the introducing paper from 1999, but I have some confusion, partially because of my unfamiliarity with domain theory. I might have figured it out, but I'd hope to get it confirmed. (If this is not the appropriate venue, advise is welcome on which one is). I was confused by the statement: 

You could use a search tree. Not a “standard” one, as is used for ordered universes (real numbers, strings…) but a more general type, such as the ones hinted at by the GiST project. There are search trees for spatial queries, as well as ones based on the metric axioms, for indexing metric (distance) spaces. The general idea (of which the “less than/greater than,” interval-oriented approach of ordinary, ordered search trees is a specialization) is to decompose the data set into subsets, usually hierarchically. This hierarchy of subsets is (obviously) represented by a tree, where the children of each node represent subsets, and each node has some form of predicate, which allows you to check whether there is an overlap between the the (conceptual) set of objects relevant to your query, and those found in that subtree (i.e., subset). For example, for a spatial tree in the Euclidean plane, each object could be a point, and the predicates could be bounding rectangles, containing all points found in or below that node. If a query is a rectangle (and you wish to find all points in that rectangle), you can recursively eliminate subtrees whose bounding rectangles don’t overlap with your query. In your case, you could build a tree where each node contains some set structure which would allow you to detect whether your query is a subset. If not, that entire subtree can be eliminated, as your query could never be a subset of any of the child nodes (and certainly not the leaves, which would probably represent the true data). As opposed to ordinary search trees, there is no search-time guarantee in general here—you’ll probably visit several branches, so even if you have a perfectly balanced tree, you’ll probably have a superlogarithmic running time. This is more of a heuristic approach, but it could be effective even so. What you need, in order to build this tree, would be some form of hierarchical clustering method that would fit your data. The GiST project actually has a tree very much like what you need, with a C implementation (although it checks whether the query overlaps, not if it’s a subset; should be easy to modify). The disk-based, B-tree-style balanced tree of GiST might be overkill, though. You probably just want to cluster similar sets together, hierarchically, and you could do that using any off-the-shelf clustering algorithm, using something like Hamming distance (or something more fancy). The more similar sibling nodes are, the “tighter” the bounding predicate of the parent (that is, the set representing their union) will be—and therefore, the more efficient your search will be. To sum up, my suggestion is: