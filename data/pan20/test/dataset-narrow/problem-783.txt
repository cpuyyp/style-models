I'm attempting to set up Cacti to monitor a router's interfaces, and I'm having trouble getting the graph templates to show the information that I'd like. Our interface configuration looks like this: 

What I use for this (and it certainly isn't windows specific) is a hardware load balancer to stand in front of the cluster. I've got a pair of Kemps ($URL$ in a cluster configuration, and the services are pointed at them. They relay the request from the source to one of the members of the pool of servers behind them. The order of servers queried can be decided one of several ways. Kemp isn't the only provider that does this, and using this method. Most hardware-based load balancers will work the same way. It's the most reliable way to provide highly-available services, short of tens and hundreds of thousands of dollars in network infrastructure. 

Nagios is great for small to medium networks. OpenNMS is supposed to be the gold standard free monitoring for large infrastructure (thousands of hosts) 

Yes, definitely talk to your internal IT people who are responsible for administering this It's possible that problem-domain.com has DNS that automagically responds to all DNS requests with a record pointing to their own IP block, typically to send traffic to a "domain not found" page with ads. This is called DNS hijacking. 

People are really creative in ways that you wouldn't believe, in order to get clusters running and reliable. When it comes to clustering (or at least HA-clustering), there are shared-storage clusters and shared-nothing clusters. Shared storage clusters typically use cluster-aware filesystems on a centralized array, like a SAN. They use OCFS, GFS, or something similar. Services running on these are sometimes Active/Active, where both machines are fully capable of providing a full range of services to clients, and typically use weighted or round-robin style load balancing, but could also be setup as Active/Passive, where the "preferred" machine acts as a server until it fails, in which case another cluster member takes over. Shared-nothing clusters have been typically Active/Passive, since there needed to be a state change to activate the passive member. This is changing with the advent of things like DRBD, which uses block-level filesystem replication over the network. Between one of these two methods, pretty much every service that I can think of can be replicated across an array of servers, especially if you don't care much where you put your state files. Even NFS can be replicated without clients freezing if everything including the lock files are referenced from the centralized storage. Enterprise computing in general has been very dedicated to the mindset that single-machine uptime doesn't matter as much as service availability. To that end, services have been engineered such that one machine failure doesn't mean disruption to the users. 

Now, when I start the VM, it immediately tries to PXEboot. I turn off the machine, and in the vSphere client, I edit the VM's properties, go to "CD/DVD drive 1", and verify, "Device Status" has a checkmark next to "Connect at power on". Here's the crazy thing. When I uncheck that box, then check it again, then start the VM, it boots from the ISO. I've done it again and again, with the console open, with it closed, and every time, I can set the StartConnected flag on the CLI, and the GUI reflects the setting, but only after I mark the checkbox manually does it actually boot from the ISO. Is there a step that I'm neglecting to perform in PowerCLI to get this setting to "take"? 

Then place your archive logs (or Sunday's, if you've got them segregated by day) where they belong and do 

We saw this in our production environment a few times, and it did end up being java's garbage collection halting further requests. The biggest tell for us was 100% processor usage on at least one of the cores for the duration of the unresponsive period. The answer in our case was to track down a memory leak in the application. I'm not certain this counts as an answer for you, but it's at least another data point. 

It isn't the kernel, I would bet. If you've got shell access to the machine, try to recursively grep in /etc for that string: 

Since I'm on an internal network, everything destined for 10.x.x.0/24 (the /24 comes from the "genmask" column) goes out the local ethernet card. Everything else (0.0.0.0/0) goes to 10.x.x.1, my gateway. My guess is that this line is probably absent or messed up on yours. If you have a relatively simple network configuration, and that line is missing, you can issue this command as root: 

The method will obviously differ based on the distribution. Some distros have easily selectable minimal installations (like the amazing new $URL$ while others, like Red Hat Enterprise (CentOS/Scientific) need custom kickstart files files created to craft the installation to your exact specifications. That assume that you know what the exact specifications are, though. I'm a huge fan of System Management by the Least Bit Principle. It's a good policy, and determining what that least bit is becomes difficult. Design your server to have a purpose, and ensure that the only software and libraries installed directly support that purpose. Ensure that you've got configuration management in place (like Puppet or cfengine) to make changes in the future. 

The best practice might be to use sudo, and modify the necessary files to make sure it happens automatically without being prompted for a password. If you don't mind the password being in the script, I think you can do something like 

In case that wasn't clear, / specifies 111 specifies x in the user OR x in the group OR x in the others. And those are not XOR, so we're looking for at least one but up to 3. Since the unix file permissions are 

Assuming it said the first, your DNS is fine. At that point, lets look into the routing: Here's mine: 

Well, that is great, but as for actually DEALING with the certs, I'm apparently out of luck. Of course, I'm not going to give up that easily. I ran "file" on the saved cert bundle and got this: 

Where "whatever" is obviously the device. Probably sdb. 1 is the partition that you just made. It shouldn't have a problem making this physical volume. Now, run 

I would also recommend calling ssh with the full path to make sure you're running the correct executable. 

In my experience, certifications are what you get when you don't have experience, because they show evidence to a base skill level. Once you've got a few years of experience, your certifications are less important than what you've accomplished and what skills you've got. I should say that I've really only worked for small companies, where getting things done is more important than impressing HR. I don't particularly care about college degrees for administrators, either. 

I try not to comment in 'holy war' discussions, but I'll try to be completely neutral here. As a sysadmin, I'm a pragmatist. I use whatever works best for what I do. In my infrastructure, I have somewhere between 75-100 Linux machines and two Windows servers. They're the domain controllers for my active directory domain, which I use to authenticate the rest of my Linux machines. There are directory servers available for Linux that would do what the Windows machines do for me, but it costs the company less to buy licenses and use Active Directory than it would for me to learn to install, configure, and administer those services. There is probably Windows software to perform the other 98% of what I do in Linux, too. It's cheaper for the company to use what works and what we know, rather than what might work and what we don't know. 

For me, it has been centralized authentication. I got to the point that I was administering 40 or so Slackware machines, and each one had local authentication PLUS local Samba authentication. I also had a VPN solution where each account needed to be setup, plus an internal jabber server and an internal email server. Everything had its own account. MAC (Moves, Adds, Changes) were insane. So I switched from Slackware to CentOS, created an Active Directory infrastructure, and used Likewise Open to authenticate all of my Linux machines against AD. It probably saved me 20 hours a month without joking even a little. Now, I've got everything authenticated through AD that I can, and it works tremendously. I can't recommend centralized authentication enough if you're still doing things the bad old way. 

Alright, sounds like great fun. First, do you have two identically up-to-date boxes (one of which is not segfaulting)? If so, check the md5sum on the binaries that are segfaulting, make sure they're the same. Next, run ldd on the binaries that are failing, then run md5sum on the libraries to see if they're different. Now, assuming the libraries are the same, something else is wrong, probably machine environment or configuration, but lets see if the strace sheds any light. To quote your strace, near the end, you have: 

You're in luck, because there's a GREAT blog called SysAdmin1138 Expounds that just covered this. The series of posts is called "Know your I/O". Intro The Components The Technology Caching I really recommend you check them out. I'm certain that it'll help you and your audience. 

This has already been answered, but what the hey. More and more, I find my non-physical diagrams use the "white box" model. In other words, I abstract everything away. If I've got a 48 port switch, then I've got a big white box with 48 smaller boxes in it, numbered. The pictures are pretty, but they just distract me from the purpose, which is to represent the logical state of a device. 

What you want to do is alter your clients to check authentication against two sources. In other words, keep your existing database while you are transitioning to Active Directory. Have the clients check the AD account and if that doesn't auth, have them check the legacy auth source. Any damage you do to make AD accept multiple usernames/passwords will be something you deal with for a long time. Just run parallel authentication sources until you're all transitioned. 

RAM is definitely faster than disk (if you need details, check numbers everyone should know). If you have the memory available on the webserver, you can make a RAM disk. If you've got an array of systems, maybe something like memcached would be better. 

Sure, you can join the domain with the host server, but it's a really bad idea. It's definitely a "chicken and egg" problem. As the people in John Gardeniers' link found, in order for the DC to be up, the host has to be up, and in order for the host to be up and function correctly, the DC has to be up. Someone found an answer in making the server start automatically, but I'd highly recommend against that solution, because there are too many things that can go wrong. Get another machine to function as the DC, make it independent of this stack of machines, and consider bare metal virtualization if you're looking to run production machines in VMs. 

There are many, many reasons to use a domain model, especially once you get past a few machines. How many machines are you talking about adding to the domain? 

I would use the same method that allowed you to take this image, but do it in reverse. The exact method would depend on how you made the image. Out of curiosity, did you use 'dd' to make the image, or did you just recursively copy the files, because if it's the latter, you may be out of luck. 

I'm building VMs in my lab and I want to replicate the situation that my fileservers will be encountering in production. Here's a brief overview of what I want to do. I have multiple ESXi 4 servers. They are accessing a SAN, and each virtual machine will exist in its own LUN on the array. I also have LUNs which house data that needs accessed by the VMs. My goal is to have a VM (lets call it fs, for fileserver) started on VMhostA. I want fs to be able to access a data LUN in raw mode, meaning that if fs goes away, I want to be able to mount it as ext3 (or whatever) from a physical (non-virtual) machine. This means that the data on the LUN must not be housed in a vmdk file. In addition, I want to be able to use VMotion to move that VM to VMhostB (and obviously, maintain access to the data LUN) It is my understanding that I need to present the LUN as a Raw Disk Mapping (RDM)? From what I have read, the RDM file created can be stored with the fs virtual machine or on another datastore. Would it be correct to assume that it should be stored with fs? Also, there exists two types of RDM, it seems. There are physical and virtual modes. I have encountered conflicting documentation from various sources, so I'm not sure what to think. Does it matter which I select in this case? What are the differences, as they apply to my situation? Thanks very much for reading all the way through ;-)