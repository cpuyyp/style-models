with --no-check-binlog-format you would not be able to checksum from master to cascading slaves aside from master's immediate replicas, --no-check-binlog-format also sets the tool's session to STATEMENT format but does not change the global status. there should not be any issues when you use the option but we recommend you test them first. 

If you want to backup database forum and all of its tables you should use this 'simplified' command: 

Create the server and client cert/key files on the master server and then copy only the client cert/key files to each slave server and configure SSL replication. So basically you have the same client cert and key on all slave servers. If a slave gets compromised change the replication user's credentials. 

You are migrating from 5.0 to 5.7, so the safest way is to dump and restore using mysqldump, you can find this link useful $URL$ 

Optionally, run ANALYZE TABLE on the affected table to re-calculate its statistics. Make sure you're aware of the effects of running ANALYZE TABLE on a busy server. 

No, you don't need to prep every backup. You only need to create the full backup on Sunday and then create the incrementals from Monday to Saturday. Full: 

Well afaik you can't if you rely on mysqlsandbox to orchestrate it for you. But you can create a master-master for M1 <-> M2 and then create two standalone instance and configure each one to slave off of one master. It's pretty easy since you have all replication user created "rsandbox". 

Percona XtraBackup 2.3 should be backward compatible, so you should be able to use PXB 2.3 with MySQL/Percona Server 5.5. There are some edge cases where you will need to use PXB 2.2 or even 2.1 but it is rarely needed if PXB 2.3 works. PXB 2.4 is recommended to be used on MySQL/Percona Server 5.7. In any case, you should always test your backups so backup using PXB 2.3 and restore it on another instance to see if everything works. 

This all depends on the size of the table. You're doing a full table scan without a where clause so an index isn't going to help. The use of the rand() function for sorting is going to produce a temp table and this will harm performance. If you have a primary key you could generate a random number in the app and perform a query where a single row is selected and can make use of the clustered index. You're likely to be serving this all from memory if you're using the InnoDB storage engine. 

Review indexing on cours table as you're getting a full table scan there. $URL$ Great indexing tips there ^ 

Slave_SQL_Running: No The above indicates that although the replica is connected to the master it is no longer applying SQL statements to the dataset. This is due to the problems exposed in the 'Last_Error' field. You should resolve these issues either by aligning the data or skipping the transaction and syncing the data thereafter. You've likely arrived at this scenario because of data or database object inconsistencies between the nodes. You should explore monitoring the replication health using any of the popular methods for example using Nagios and Percona's Monitoring Plugins for MySQL. There is a broad choice in this space. 

How large do you expect the data to grow? No harm in keeping older data in the same table. You could plan to use partitioning and implement date range partitions (per year). Using WHERE statements is completely natural in SQL so don't consider this as "messy" at all. 

UNTESTED - You did not provide any DDL or sample data scripts... IF is a flow control construct that determines which statement block will be executed. What you need are Conditional Expressions. In your case, the function should do the trick, also look at for non null condition predicates. 

You are absolutely correct that if you design your data model and the resulting schema very well to begin with, it will already be in at least 3rd normal form. That said, I believe that it is crucial to understand the relational model, and the normalization rules as helping guides. When you say: 

Here is an excellent article that discusses all the aspects of the new cardinality estimator. Bottom line - as with all changes, you win some, you lose some, and hopefully overall you win more than you lose. With the new cardinality estimator, it is a huge win overall, in spite of the occasional hickups like you are experiencing. You have several work arounds other than the TF, and they are discussed in the article. Easiest should be either using a query hint, not to lose the benefits of the new estimator. 

The COUNT technique that was suggested here should work, but the problem is that your table structure and data mess it up. You did not define any keys, and you have duplicates in the demo data - for example, course 101 enrollment by student 4 appears twice, on the same date, with different grades. Proper declaration of keys would have not allowed it. Changing the predicate to would also give you the result that you were looking for, taking into account that there may be multiple registrations. The generalized problem you are facing is called 'relational division'. Search here and you will find many discussions on the topic. Typically, the most efficient way to solve relational division problems is to use a negative formulation of the question. It seems weird at first, but once you get the hang of it, you can't go back... In your case the question is: 

You should consider some group ID within this or another relataed table. Marking accounts with the same group ID you can have 1 or more of them as an enhanced account type. Make sense? 

I would advise you to drop mysqldump and move to Percona Xtrabackup. It will give you a filesystem level backup that can be made online. Restoring those backups will be orders of magnitude faster. 

It could be related to indexes. Think about how a constraint could affect inserts. PK is unique by design and you have three columns and with your 3 col PK MySQL needs to check each time a new row is inserted whether that PK is unique. Why can you not simply use the 1st column as your PK? Secondary keys need to be updated when you add new data, ensure that you have expand_fast_index_creation turned on. FK relationships need to be maintained based on their inclusion in the table. What rippling effect does your INSERT have on related tables? On the other hand it might be due to your settings. Check and tune the following: innodb_buffer_pool_size innodb_log_file_size innodb_flush_log_at_trx_commit Some tweaks to these could spin you up a few notches. 

Slave_SQL_Running_State: Reading event... Suggests that it's working on a large event. If you have MIXED or Row based binary logging and have changed a large amount of data on the master you might end up with a large event or series of large events. Use the mysqlbinlog tool to see what's going on with your relaylog. The master might show a concentration of binary logs over a small period of time (on the filesystem) 

It is stated in the mariadb and even mysql docs: $URL$ systemd services are packaged on RPM and Debian based Linux distributions. When systemd is used, mysqld_safe is not used and settings in the mysqld_safe section of configuration files will not be read or applied. $URL$ On platforms for which systemd support is installed, scripts such as mysqld_safe and the System V initialization script are not installed because they are unnecessary. For example, mysqld_safe can handle server restarts, but systemd provides the same capability, and does so in a manner consistent with management of other services rather than using an application-specific program. 

Given that the other answer might be a problem for you, I would suggest you try pt-archiver instead specifically using the option, of course in combination with and option you may be able to pull this off somehow. Good luck! 

I don't think you have many options here. If you want to maintain high availability I'd say shutdown on node and configure it as a replication slave of one of the cluster nodes. Rebuild it from a mysqldump of the cluster node. Once slave catches up, restart it as wsrep node 1 of a new cluster and then promote it as master. Shutdown the other two nodes of the old cluster and empty datadir, and let each join one at a time to the new cluster. Others may have a better suggestion though. 

Manual states: You can also discard an input value by assigning it to a user variable and not assigning the variable to a table column: 

Get advice from the vendor. If you're using the commercial product you're paying for expert advice from Oracle themselves. 

You can write a simple script externally or use the MySQL event scheduler. Both will work just fine. The benefit of the MySQL event scheduler is that it will be part of your database backups. 

You should use the MySQL event scheduler to run a a query to update all rows where the age of the row is greater than your threshold and have it execute the update at a suitable interval. 

Mysqldump is single threaded so you’re bound by the speed that 1 core can run at to play all your SQL commands in serialised fashion. There are some config variables that can be changed to reduce the amount of disk flushes needed and permit larger memory buffers for speed. Innodb (if that’s what engine you use) variables (plus others) 

Joel, there is a lot of unknowns. If you post your my.cnf, queries, table definitions and queries, EXPLAIN plan output, then you might get more precise answers. You say "disabled query caching and other caching" what 'other caching' are you referring to? What steps did you take to disable QC? Did you restart your MySQL server between tests to flatten any buffers? If no and the KVP table was smaller and fit into RAM then InnoDB might have been able to serve queries fitting a certain criteria from data in the innodb buffer pool. You also refer to memcache. With MySQL 5.6 there is a memcache api. this means you can use memcache calls to access data stored in innodb. This will not be incidentally speeding up your queries. There is also another method of bypassing the SQL layer within MySQL called HandlerSocket. These are a couple of subjects that I would point you at should you continue to desire this type of access to your data with the added bonus of having MySQL persist the data to disk and also be crash safe through InnoDB's crash recovery process. 

You concern yourself for nothing. "it's a bottleneck since of each row processing" is an assumption you make, and not necessarily what the optimizer will choose to do, even if it looks to you that you write it that way. SQL is a declarative language, and as such you tell the engine what you want done, and not how you want it done, like you do in imperative languages. The query optimizer, especially in SQL Server, is a mind blowing wizard in moving things around, considering alternatives, and understanding what it is that you want, regardless of how you write it. It doesn't get it 100% right, but it is damn good. Write your query, use several syntax like you did is never a bad idea, and test it against a representative data set. If you see performance challenges, examine the execution plan, or post it here so people can help you. HTH 

You need to be looking at SET Operators. Something along the lines of the following query should do what you are looking for: 

Example 1: You shouldn't care. The process is much more involved than it seems, and I don't think you have any guarantee. The 3 commands go into the queue in order. The first one is pulled and executed, when the second is attempted, the engine may choose to put it back in queue if it waits for more than some threshold, and the third may be pulled and attempted. if it happens that the resources are now free, the 3rd will be executed first. if not, it may go back in queue again. Example 2: I think the above explains it, and the answer to both your questions is no AFAIK, unless you develop an execution buffer of your own. Don't. Rationale: Now you got to the core challenge, which should really have been the title - "How to deal with deadlocks". You assumed that your 'global lock table' solution is the only way to make things right, but I think the price of losing all concurrency is not worth it. It would be easier just to have the procedures executed serially using a scheduler of some sort. You will open a Pandora box with this approach that will be hard to close. Queuing and syncing is a very complicated challenge. If I were in your place, I would first investigate the root cause of the deadlocks. Sometimes, something as simple as adding or modifying an index can solve it. If you post your deadlock graphs here, along with the involved queries with their execution plans, people will help you pin point the root issue. HTH 

Webscalesql is a project born in order for those organizations that maintain their own MySQL patches to use, and review each other's code. So engineers from facebook can improve twitter patches can improve Google patches and so on. It is necessary for now at least, because its seemingly difficult for Oracle to accept, QA, test and ship external code. Note the use of "seemingly". Some would argue it is not. Webscalesql is not supported by anyone so I would advise against running it in production environments without your own MySQL engineering team. Oracle, Percona and MariaDB should be the direction to look to for an implementation. 

The right direction for this is to use an audit logging plugin. There are some choices out there. Please check out the following post for more information $URL$ My personal preference is the Percona Audit Plugin but you should be using Percona Server (MySQL Alternative) for this. 

This is an [info] message and displays due to the steps taken by the Debian mysql startup script. It is not an error. 

There are multiple audit logging products available. If you are using Percona or MariaDB flavour of MySQL you have the option of their plugin. If you are using Oracle MySQL you can pay for their enterprise version of audit plugin (as part of Enterprise Edition). There is also an audit plugin from McAfee that will fill this requirement and is generally available cross-alternative and from 5.1+. These products permit you to log both logins and queries. Finally there's a plugin to track logins only from a community contributor. Links to all below. $URL$ $URL$ $URL$ $URL$ 

An added bonus, is that it also works around the COUNT issue you experienced earlier. Check the execution plans with large data sets, and you will see that this one is easy to index, and will typically perform better than the COUNT technique. The reason is that EXISTS doesn't need to complete the operation - once a row is found, the predicate is TRUE and there is no need to keep processing. With the COUNT, all aggregates have to be calculated. Hope that helps~! 

Check out this article. I think it will answer your questions and give you the tools to deal with over-inflated number of auto-statistics. The author suggests dropping ALL auto-created stats occasionally, as the one that are required will be recreated when the first query that needs it is executed. It explains the risks / benefits. 

In a relational model every relation describes one 'thing' - the 'Fruits' table models fruits, fruits are not blends. Your Fruits table with a single column, is all key - 5NF. If you don't have a Fruits table, how will you insert a new fruit which has no blend yet? What happens to a fruit you have in stock, but its blend is removed? 

Depends on what data you store in it. If you store a null or an empty string, it will take around 2 bytes. If you use all 50 characters, it will take around 102 bytes. See the documentation here. 

The technique you are describing for representing task hierarchy is called 'Adjacency list'. Although it is the most intuitive to humans, it doesn't lend itself to very efficient querying in SQL. Other techniques include path enumeration (aka materialized paths) and nested sets. To learn about some other techniques, read this post or search the web for numerous articles on these techniques. SQL Server offers a native hierarchy representation for path enumeration. This is most likely your best bet...