What is the fastest algorithm known for factoring polynomials with $n$ variables and total degree $\leq d$? Here, $n$ is growing and $d$ is fixed. Most work seem to consider the case when $d$ is growing and $n$ is fixed. I am interested in results both over finite fields and over rationals. 

Alon, Matias and Szegedy, The space complexity of approximating the frequency moments, JCSS 58(1):137-147, 1999. This rather magical paper was the first one to formalize streaming algorithms and prove rigorous upper and lower bounds for foundational tasks in the streaming model. Its techniques are simple, its proofs are beautiful, and its impact has been profound. The work won Alon, Matias and Szegedy the Gödel Prize in 2005. 

Of course, it's true that Gauss didn't formally define exactly what he desired out of the factoring algorithm. He did talk in the same article though about the fact that all primality testing algorithms known at that time were very "laborious and prolix". 

A $(q,\delta,\epsilon)$-locally decodable code (LDC) is a map $C: \mathbb{F}^m \to \mathbb{F}^n$ such that there is an algorithm $A$, called the local decoder, which, given as input an integer $i \in [m]$ and a received word $y \in \mathbb{F}^n$ that differs from $C(x)$ for some $x \in \mathbb{F}^m$ on at most $\delta$ fraction of positions, looks up at most $q$ coordinates of $y$ and outputs $x_i$ with probability at least $1/|\mathbb{F}| + \epsilon$. The LDC is said to be linear if $\mathbb{F}$ is a field and $C$ is $\mathbb{F}$-linear. LDC's have many applications in complexity theory and privacy, among others. For $q=2$ and constant $\delta,\epsilon$, the situation is completely resolved. The Hadamard code is a linear $2$-query LDC with $n = \exp(m)$, and this is known to be essentially optimal, even for non-linear LDC's. But here, $q=2$ is the frontier! As soon as we make $q=3$, there is a huge gap between known upper and lower bounds. The current best upper bound is a linear $3$-query LDC over any finite field (and even the reals and complexes) with query complexity $n=\exp(\exp(\sqrt{\log m \log \log m})) = 2^{m^{o(1)}}$ [Efremenko '09, Dvir-Gopalan-Yekhanin '10]. The best lower bounds is $\Omega(m^2)$ for linear $3$-query LDC's over any field and $\Omega(m^2/\log m)$ for general $3$-query LDC's [Woodruff '10]. The situation for larger numbers of queries is even more dire. 

I have heard that there are heuristic arguments in statistical physics that yield results in probability theory for which rigorous proofs are either unknown or very difficult to arrive at. What is a simple toy example of such a phenomenon? It would be good if the answer assumed little background in statistical physics and could explain what these mysterious heuristics are and how they can be informally justified. Also, perhaps someone can indicate the broad picture of how much of these heuristics can be rigorously justified and how the program of Lawler, Schramm and Werner fits into this. 

Well done on your independent discovery. This is a Hadamard matrix of Sylvester type, written in a different order. There is a massive literature on this topic. It is used in coding theory, cryptography, is directly related to Reed-Muller codes of degree 1, it can be used to obtain best affine approximations of functions, etc. Ryan O'Donnell's notes on Analysis of Boolean functions available here which are now published as a book, Claude Carlet's chapters on Boolean functions here are easily accessible online. Just use the order $[1,X,Y,XY,Z,ZX,ZY,ZXY]$ which corresponds to lexicographic order over the integers, and apply the corresponding permutation to the columns as well. In that order your matrix is simply the 3-fold Kronecker product $$H_2 \otimes H_2\otimes H_2$$of the basic 2 by 2 Hadamard matrix. $$ H_2=\left(\begin{array}{cc} 1 & 1 \\ 1 & -1 \end{array} \right), $$ This paper considers efficient evaluation of Hadamard representation coefficients. 

Based on Sasho Nikolov's very helpful comment, it seems that both papers use similar models of complexity which lead to unreasonable conclusions, such as the implication that any problem in PSPACE or #P can be solved in polynomial time. I welcome any comments which may lead to a modification of this tentative answer. 

Because an optimal prefix free code, e.g. a Huffman code, can be shown to be within one bit of source entropy. This is certainly in Cover and Thomas, I am pretty sure. 

The answer is less than $2^{n-k}$ of the sequences of length $n$ have complexity less than $<n-k.$ Due to the uniformity assumption on these sequences we just count. Consider all short programs of bitlength $<n-k.$ Even if all represented sequences of length $n$ the total number of such programs (starting with the empty program) is $$1+2+2^2+..+2^{n-k-1}<2^{n-k}.$$ To clarify,the number of compressible strings by $k$ bits in a the set of length $n$ bits is $2^{n-k}$, their relative number is $2^{-k}$ and their percentage is $100\times 2^{-k}.$ So for each fixed $k$ and all $n$ you get a constant. For example the percentage of strings of length $n$ which are compressible by $k=3$ is $100\times 2^{-3}=12.5\%,$ those compressible by $10$ bits have percentage $100\times 2^{-10}=0.09765625 \%.$ 

Csizsar, I., and Korner, J., Information Theory: Coding Theorems for Discrete Memoryless Systems, 2nd Ed, Cambridge. Berger, T., Rate Distortion Theory, quite old probably late 70's or early 80s, can't remember the publisher maybe Wiley. Yeung, R.W., A First Course in Information Theory, Springer. 

The formulation on p.70 of the 4th edition of The Probabilistic Method by Alon and Spencer is along the lines you state. 

I'll try to show this, hope I interpreted the question correctly. Let $A_k=\{0,1\}^k.$ If $f(n)\leq n+\log n$ for $n$ large enough this implies $$\max\{\ell(c(x)):x \in A_1 \cup A_2\cup \ldots \cup A_k \}\leq k+ \log k,$$ for $k\geq N$, for some finite $N$, where $\ell(c(x))$ is the length of the codeword $c(x)$ assigned to $x.$ So this means that $$ \sum_{N\leq k ~~} \sum_{x \in A_1\cup A_2 \cup \ldots \cup A_k} 2^{-\ell(c(x))}\geq \sum_{N \leq k~~} \sum_{x \in A_1\cup A_2 \cup \ldots \cup A_k} 2^{-(k+\log k)} $$ which is equal to $$ \sum_{N\leq k} |A_1 \cup A_2 \cup \ldots \cup A_k|~ 2^{-(k+\log k)}= \sum_{N\leq k} (2^{k+1}-1)~ 2^{-(k+\log k)} $$ which is equal to $$ \sum_{N\leq k} 2^{1-\log k} - 2^{-(k+\log k)}=2 \sum_{N\leq k} \frac{1}{k} - \sum_{N\leq k}\frac{2^{-k}}{k}=2 \left(\sum_{N\leq k} \frac{1}{k}\right) -\ln 2 $$ which is unbounded, since the harmonic series diverges. 

Number theory has not been mentioned, but it's a very important tool for many cryptographic and complexity-theoretic constructions. 

If you are asking if the same statement holds true or not in higher dimensions, it does. Just project all the points to a random 2-dimensional plane. Another natural generalization is to consider hyperplanes instead of lines. Here, you have Beck's "other" theorem: 

Expander codes Gallager showed in the 1960's that random low density parity codes have good rate and relative distance with high probability. But it was Sipser and Spielman (1994), following work of Tanner (1981), who had the beautiful insight that it is the expansion of the natural bipartite graph associated with the parity check matrix of the code that leads to the code being good. They then proved that the following simple decoding algorithm runs in linear time for any expander code: repeatedly check if there exists a bit of the received word which violates more than half of the parity checks it is involved in, and if there is such a bit, flip it. Two footnotes: 

Are there interesting polynomial time solvable problems that we know of for which the natural convex relaxation has a non-trivial integrality gap? Note: Maximum matching doesn't qualify because I consider natural the exponential-sized poly-time solvable LP. 

Locally decodable codes and locally decodable erasure codes are qualitatively equivalent. Both imply $\Omega(m)$ many disjoint $q$-tuples from which one can recover a given message coordinate, where $m$ is the codeword length and $q$ is the query complexity. A formal argument appears in Section 3.4 of Kerenidis and de Wolf's paper. 

Hereditary properties are very "robust" in the following sense. Noga Alon and Asaf Shapira showed that for any hereditary property ${\cal P}$, if a graph $G$ needs more than $\epsilon n^2$ edges to be added or removed in order to satisfy ${\cal P}$, then there is a subgraph in $G$, of size at most $f_{\cal P}(\epsilon)$, which does not satisfy ${\cal P}$. Here, the function $f$ only depends on the property ${\cal P}$ (and not on the size of the graph $G$, for instance). Erdős had made such a conjecture only about the property of $k$-colorability. Indeed, Alon and Shapira prove the following stronger fact: given ${\cal P}$, for any $\epsilon$ in $(0,1)$, there are $N(\epsilon)$, $h(\epsilon)$ and $\delta(\epsilon)$ such that if a graph $G$ has at least $N$ vertices and needs at least $\epsilon n^2$ edges added/removed in order to satisfy ${\cal P}$, then for at least $\delta$ fraction of induced subgraphs on $h$ vertices, the induced subgraph violates ${\cal P}$. Thus, if $\epsilon$ and the property ${\cal P}$ are fixed, in order to test if an input graph satisfies ${\cal P}$ or is $\epsilon$-far from satisfying ${\cal P}$, then one only needs to query the edges of a random induced subgraph of constant size from the graph and check if it satisfies the property or not. Such a tester would always accept graphs satisfying ${\cal P}$ and would reject graphs $\epsilon$-far from satisfying it with constant probability. Furthermore, any property that is one-sided testable in this sense is a hereditary property! See the paper by Alon and Shapira for details. 

As pointed out in the comments if $u\in \{\pm 1\}$ then $x=x(u) \in \{0,1\}$ where $$x(u)=\frac{1-u}{2},$$ with $x(-1)=1,$ and $x(1)=0.$ This will then yield $$f(x)=2^{n-1}f(0)-\frac{1}{2} \sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i,$$ and if we denote the $\{0,1\}$ valued version of $f$ as $\tilde{f},$ $$\frac{1-\tilde{f}(x)}{2}=2^{n-1}f(0)-\frac{1}{2} \sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i,$$ leading to $$\frac{\tilde{f}(x)}{2}=\frac{1}{2}\left[1+ \sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i\right]-2^{n-1}f(0),$$ or $$\tilde{f}(x)=1-2^{n}f(0)+\sum_{S \in \{1,\ldots,n\}} \hat{f}(S) \prod_{i \in S} x_i,$$ if I haven't made any errors along the way. 

Disclaimer: This is based on generic information theory knowledge only. Too long for a comment. Summary: The pointwise product of your two plots should go to some limit, as the relevant blocklengths and sequence lengths increase. I don't know if this applies to DNA but in theory if your sequence is ergodic (stationary, and time averages are the same as ensemble averages, for long enough sequences) then the two entropies are related intimately. The whole point is, if you obtain a "block" entropy $H(X_1,\ldots,X_n)$ for a block of length $n,$ then $$ \lim_{n\rightarrow\infty} \frac{H(X_1,\ldots,X_n)}{n}=H_0 $$ which is the entropy per symbol, i.e., the entropy rate, what you call shannon entropy as opposed to block entropy. Now, the question of estimating these is a different question. In fact your program does this by a sliding window technique, which then means that it is averaging overlapping thus dependent samples. Define the shorthand $X_i^j=(X_i,X_{i+1},\ldots,X_{j})$ where $j\geq i.$ I am sure that under suitable conditions an estimator of the form $$ \frac{\sum_{k=1}^{n-\ell+1}\hat{H}(X_k^{k+\ell-1})}{n-\ell+1} $$ where $\hat{H}(X_k^{k+\ell-1})$ is a suitable estimator for block entropy, will converge to a multiple of the entropy rate, i.e., to $\ell \times f$ where $f$ is a correction factor. However, one may need to let $\ell$ grow with $n.$ 

The exact answer is unknown in general. One standard upper bound for $q-$ary codes is the Singleton bound, which gives $$|C|\leq q^{n-d+1},$$ and codes meeting this bound are called MDS. A lower bound is the Gilbert-Varshamov bound, given by $$|C|\geq \frac{q^n}{\mathrm{Vol}_q(d-1)}=\frac{q^n}{\sum_{k=0}^{d-1} \binom{n}{k}(q-1)^k}.$$ Using these keywords should get you started, there are many more results. 

Nice answer and proof. So, the bound in your answer can also be rewritten $$ p_{err} \leq 1-2^{I(X;Y)-H(X)}=1-2^{-H(X|Y)},\quad\quad(1) $$ since $I(X;Y)=H(X)-H(X|Y)$ by definition. This appeared in IEEE ISIT 1994, in a talk by Baumer, to the best of my knowledge. In a similar vein, one can obtain $$ p_{err} \leq 1 - \sum_{y \in {\cal Y}} P_Y(y) 2^{-H_2(X|Y)},\quad\quad(2) $$ where $$H_{\alpha}(Z)=\frac{1}{1-\alpha}\left( \sum_{z \in {\cal Z}} P_Z(z)^{\alpha}\right)$$ is the Renyi entropy of order $\alpha \in (0,1)\cup(1,\infty).$ Here, $\alpha=2,$ so the bound (2) is tighter than (1). 

In the recent preprint $URL$ it is claimed that $n$ real numbers can be sorted in time $$O(n \sqrt{\log n}), $$ and linear space. The paper seems reasonable, though I am not an expert in sorting algorithms. If correct, this would be a significant, I believe, at least theoretically. The presentation of the main argument is somewhat informal and nontraditional, however. Has anyone noticed/commented on this paper? It seems that the same author, Yijie Han, has published a related result on integer sorting, as discussed in Han's $O(n \log\log n)$ time, linear space, integer sorting algorithm 

Lenstra has shown that there is such an algorithm for finding a quadratic non-residue mod a given prime. Gat and Goldwasser have shown that there is such an algorithm for finding a generator of $\mathbb{Z}_p^*$, where $p$ is a given prime of the form $2q + 1$ for a prime $q$. 

For sampling-based algorithms, nearly tight upper and lower bounds are known for the query complexity of estimating the number of distinct elements in a sequence. Charikar, Chowdhuri, Motwani and Narsayya showed that multiplicative approximation of the number of distinct elements to within a factor $\alpha$ can be achieved with $q= O(n/\alpha^2)$ queries into the sequence. Here's the algorithm. Sample $q$ items uniformly at random from the sequence. Let $d$ be the number of distinct items in the sample, and let $f_1$ be the number of items that appear exactly once in the sample. Then, output $d + \sqrt{n/q} \cdot f_1$. The analysis is also very clean. Assume without loss of generality that each element in the sequence is an integer in $[k]$, where $k$ is the number of distinct elements. Let $p_i$ be the probability of selecting $i$ when sampling uniformly from the sequence. Then, the expected output of the estimator is: $$\mathbb{E}[d + \sqrt{n/q}\cdot f_1] = \sum_{i=1}^k (1 - (1-p_i)^q) + \sqrt{n/q} \cdot p_i q (1-p_i)^{q-1}$$ Now, the point is that for any valid $p_i$, the value inside the summation is between $\Omega(\sqrt{q/n})$ and $O(\sqrt{n/q})$. (This uses the fact that $1/n \leq p_i < 1$.) So, the expected output of the algorithm is within $O(\sqrt{n/q})$ of $k$. Charikar et al.'s estimator can also be used to get an additive approximation of the number of distinct elements. One can estimate the number of distinct elements upto $\pm \beta n$ by making $O((1-2\beta) n)$ samples. This was the state-of-the-art for a long while until STOC '11 when Greg and Paul Valiant showed that it's possible to reduce the sample complexity and time complexity to $O(\frac{n}{\beta^2 \log n})$. Their algorithm is based on a very general result that, given a probability distribution $D$, constructs a distribution $D'$ using $O(n/\log n)$ samples such that $D$ and $D'$ are close to each other in relative earthmover distance. So, one can make sure that the support size of $D'$ and the support size of $D$ are only an additive $\beta n$ away from each other. As for lower bounds, Charikar et al. showed that for $\alpha$-multiplicative approximation, $\Omega(n/\alpha^2)$ queries are needed. The proof stems from the observation that $\Omega(n/\alpha^2)$ queries are needed to distinguish $n$ identical items from the same sequence with $\alpha^2$ unique elements inserted in random locations. For additive approximation, however, this bound is sort of trivial; for constant $\beta$, this would only give a constant lower bound on the query complexity. Raskhodnikova, Ron, Shpilka, and Smith showed a nearly linear bound even for additive approximation. Specifically, they prove that estimating to within additive error $n/23$ requires $n/2^{\sqrt{\log n} \log \log n}$ queries. The Valiants in their above-cited paper showed a lower bound of $\Omega(n/\log n)$ for estimating to any additive error less than $n/4$, implying that their upper-bound is tight upto constant factors.