You never can be sure - what information fetched? And what if it official (government required) report? and You fetch wrong, not current address? Or You send package from Your internet store to wrong (outdate) address? 

from comments - duplicates, mean You have duplicates (answer with coalesce return the same), variant for avoid duplicates: 

Persona XTrabackup - $URL$ Stop - copy - Start (preferred) Attach Slave server with ROW based replication, than at any moment stop slave - copy files - start slave 

Size of bin-log will not affect to performance if You will use filter to replicate only selected tables, and even if You will replicate all - it also normally not a problem. For compare we replicate 20Gb (and more) between datacenter with real-time loading (telecom) Keep version of Source and Target Server same - it good practice even if You choose Variant B - it avoid many other troubles 

Uninstall latest MySQL - by windows tools Install 5.6.30 from previous step Copy folder c:/ProgramData/Mysql/data saved on previous step Start MySQL 

or just properly handle all queries: include in column list only really necessary column, request BLOB data after by second request with access by PK by the way add second biggest issue when BLOB stored in db - 

it give You idea - which index better to use in this case without information about data, general ideas: - index for created_timestamp - good candidate You can or create index separate for created_timestamp only, or create index for 3 columns: core_id, hit_status_code, created_timestamp - columns in index must be in same order as used in query and last SELECT * not give ideas about data size, even You request 25 records, but before server must sort records by DESC 

This topic is very interesting for me now, so because it Up, some last information: Replication is good, but what if You need replicate data to different technology Target System (for example Redshift) I found and test few implementations for change data capture: 1) MariaDB MaxScale CDC - my personal test was unsuccessful (and builded from source and binary distribution return link for missed plugin). Ask a questions, wait for answer. 2) StreamSets - simulate slave, ship changes to Kafka MQ. Tested on real loading, work perfect 3) Yelp 4) Maxwell Daemon - under test now, example with NiFi - NiFI examples 5) Python Library - also simulate a slave for remote server 6) Lapidus - NodeJS implementations Few other links also founded, some under active maintenance, some look like forgotten. From the tests, StreamSets look very good, Maxwell also very interesting MaxScale - sure it work, but may be available only under subscription support. From "other" side - Talend as primary data integration tools, Redshift, memSQL. 

Any tools and recommendations always depends from personal experience and of course on the market You can find more commercial and open source tools 

in case of Slave, ROW based replication - highly recommended. In case of mixed or statement based replication, You must be sure not have opened temporary tables on Slave, or replication will be broken, with ROW based replication - temporary tables not involved in replication process. Of course - same version of MySQL on all machines 

Looking on Your pictures, I can guess - for run subquery You also use MySQLWorkbench. Most of software - not return to You back all rows from dataset, by default it filter up to 1000 records (some less) So select without ORDER BY start work and immediately stop after first XXX rows. In case of create table: 

must not work - just because when index created no this table present because query work (not return error from Your description) - MySQL ignore this, but need test with full table structure also, JOIN with derived table - do not want to use index 

it will show You, what real indexes used or not You can add result in Your question and we could check, how it work 

If You already use MySQL Workbench, You can use menu: Database -> Synchronise with any Source Before apply changes it show You script, which You can copy and if need edit You can test it on copy of databases Also there are many other tools: 

it not use INDEX because no any WHERE condition in SELECT statement, so from SQL server point of view - it always make FULL-SCAN of table, so no reason for use index. possible make changes 

p1 - if You plan rollback exactly for 5.6.30, p2 - if You plan go till the win, best to have both. then You must have old installer for revert back at any time, You can download it from official archive - $URL$ Then You if You decide revert back: 

main reason for this - triggers created by one user (for example root) and select run from other, which not have full rights - You must have Trigger privilege for display triggers from database. in other case - result will be empty. 

It is possible to create views on Slave without collisions with replication, but ... but feature unpredictable and IT just try to avoid any potentials issues, and they are right. 

it work slower than UNION ALL of 2 queries what happens - native index for this construction is absent, so server need intersect 2 index (You can read - spend additional time and resources) when You use UNION ALL: 

If You check used indexes, You can see - PL_ProductCategoryId have cardinality 17, it mean it has +- 17 possible values for all table rows. It is very close to full-scan, client_id included in 2 indexes, both time as second part. You can change index PL_SkuName_ClientId - just swap them (client_id, sky_id), or create new index for client_id only. 

Playing with LEFT + INNER You can change logic of SQL query If You need all from restricted_component - LEFT JOIN But because groups must be for all - INNER 

What You make right - it is backup all files from /var/lib/mysql all other steps - wrong Your current error (in PHPMyAdmin with phantom tables) - because You have Your database folder with .frm files for table structure, and empty (new) ibdata. What You can do now - revert all back (restore files in it original place) Then You can: - check error log file, for error message - start change innodb_force_recovery from 1 to 6 

create user 'bhuvi'@'%' identified by 'bhuvi'; - You are create user with password grant all privileges on . to 'bhuvi'@'localhost'; - You create 2nd user without password 

because as unique identifier You choose Name - MIN() allow avoid errors when MySQL default settings - ONLY_FULL_GROUP_BY Specially for Rick James :) From original question, we can see - id, can not be used as IDENTIFIER so queries example with MIN and MAX for id - could be (and this realistic live scenario) - first or most resent contact with person Before we go to next form of query - first we need decide mike(id=1) for Jan 17 and mike (id=4) for Jan 17, it is same person? and salary for Jan - it MAX or SUM (as I include in my examples). 

Alternative way, which possible to use if You do not worry about updates of old rows (same restriction as other answer) because MySQL lock table for INSERT FROM SELECT and for SELECT INTO table, but at the same time - do not lock for: 

but - be careful, which name You need to take? FIRST/MIN/MAX? because id=2, in t2 can have name = four/seven/eight ... which right? 

based on result - add indexes, change queries when necessary in 99.99% cases after this - problem will gone 

By default (if it not disabled) MySQL will save file to same folder as database, in my case - sakila 

You are fix this situation Proper way for work with dates - have datetime type for date time columns in this case Your data would look like: 

continue work without errors, this query return all rows from Names related to Users In some other cases when You expect only 1 Name, You will need add GROUP BY conditions 

the logic of LEFT JOIN - take all rows from LEFT table because You have not any WHERE conditions - MySQL do not found any reason to use index for tb1 - with index or without index it still will read all rows. so, general answer for Your question - it will not increase speed, and possible will be slower (depending from data size), when sort operations will require use of temporary tables. You can make a few tests - for example change LEFT JOIN to INNER, it not correct for logic test, but it will show You different plan. Also You can add WHERE condition for tb1 and see - what happens. 

SET portion - is dummy, just for illustration, You need replace it for real procedure logic The logic will proper work with many numeric or dates - simple increment by numeric or date time interval. with tables where PK - combined from different columns without any visible logic, like telecom tables - (prefix, context, priority) it also possible, but little more complicated. 

in additional to the answer about support utf8mb4 from MySQL 5.5, the problem with collation You can resolve by edit(replace) all utf8mb4 to other utf collation. If file huge, You can use sed command for change string without open the file - 

mysqld_safe - not installed any more on MacOS and some Linux as well - $URL$ on MacOS (in example case Sierra, MySQL 5.7) You can edit (create and edit) - /etc/my.cnf 

$URL$ if strict mode not enabled, You can use "any" path (any enabled by windows) for output file, but not use user folders or root folder for avoid permissions collisions. create something like c:/exchange 

more related to other types of manipulation, like - manual edit of BLOB object with text or parameters, but change definer - it just change single column. 

this is really bad practice with applications, exactly because - if You add new column, some of Your code could be broken. For example: 

2 sec it is not significant huge time, but if it make collisions for other operations - You can to split operation for chunks 

with time difference will need adjust server settings but if we check size on disk (tables include only 1 column, PK) 

But first - You must be sure, all other code will not be broken by new column. Then You can use one of the tools from answer for Your other question: 

You need check - configuration of server, memory for InnoDB, for FullText compare size of index and FT buffer size, by default it only 8Mb, max value is 80Mb Than next step and profile query - how many records returned, what time of intialyze of index and upgrade 5.6.4 to something more new in 5.6.4 FullText was only implement for InnoDB, many bugs fixed from that time 

so, when You test it as described - it will use 30+- bytes per row, and this is will be similar with result additional source of information: 

General answer - No! First of all - mysqldump it is not a simple sql command, this is complicated logic behind it, such as: - show definition of all objects - tables, routings, foreign keys - manage export import in proper order, like disable constraints and etc Second (in addition to first) - mysql console (and SQL as language) do not support database level commands, like select all - You would need do this for each tables, objects, routines. Federated engine (FederatedX) also not resolve "problem" 

Data sorted by date (yellow column) and with scan mysql must fetch first NNN records rom green columns I not try explain - how indexes work, but in our case - mysql stop work after first any equal records and number of this first records dramatically less than total number of correct records this is simple visual example, real data could have different frequency, but logic will be same - with big data-set and small number of equal data, query could (or could not) work faster then on smaller. There are many other parameters could give affect, but this is one of possible scenarios what happens when index wrong. 

in original query - SELECTS inside VALUES included in double quotes it mean You try insert TEXT value into INT column but in any case, this form of query is not correct better try this: 

Stop mysql on Slave, change settings, Start MySQL on Slave Login to MySQL; Stop slave; exit from mysql client make dump from Master direct from Slave: 

So on huge dataset - it work as expected. Solution really only one - analyse query and reduce number of total records returned by query, how - depend from business logic For example - posts.created_at <= '2016-10-24 10:30:53' (it mean yesterday) return all records from 5 000 000, but if add posts.created_at >= '2016-09-24 10:30:53' it return only month of data and etc - any legal ways for reduce number of records. Second recommendation depend from Your table structure, some time replace SELECT * to the SELECT "WHAT REALLY NEED" could dramatically increase speed - for example one of column BLOB or TEXT, so exclude this column from SELECT - reduce size of data before sort for many. many times. Even if this BLOB column need for You - You can request it by ID (single record) by second query from the page