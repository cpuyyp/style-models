Redirecting port 80 to 443 is trivial with iptables, but from your description you clearly do not want to run http on port 443: You want people to be redirected from http to https. You are clearly aware that this can be done in Apache for each virtual host, but you prefer not to use that to avoid configuration oopses; otherwise sgtbeano's solution is what I would recommend. Based on that I would recommend you stop using Apache on port 80 and that you run another webserver on port 80 that does the simple redirect. You could even run a separate instance of Apache on port 80 with a different configuration dir. This will also avoid the configuration oopses, but may be harder to maintain (not many people expect 2 different instances of Apache on the same host with 2 different sets of config files). In this case you can use sgtbeano's config for the port 80 instance. 

I need a remote assistance tool (i.e. tool that makes it possible to control another persons screen) that: 

It should preferably survive if any of the machines change its internal IP address (e.g. as part of a DHCP change). TeamViewer delivers 1, 2, and 3, but fails 4 because of its license. Google Chrome Remote Desktop delivers 1, 2, and 3, but fails 4 because of its license. The different VNC tools deliver 3 and 4, but fails for either 1 or 2. 

Everything up to the last is OK. Only stuff written after the last sync is flakey. Add some spares and do the rebuild. When the copying of the existing disks finishes tomorrow I will test out the above. If it works, then the above is an answer. Otherwise a new copying of the original set will be started, and new ideas are welcome (but please test them on the test scenario). == Spares are now added and rebuild started. Every 1000th file was copied to a dir on the file system and this did not cause issues in the logs. So it seems the filesystem is OK. It remains to be seen if the users miss some files. == No users have reported missing files so far, so it seems to work. 

At this point do not use the directory: In the test scenario I have at least once had xfs complain and crash. So instead: 

It seems there is a solution here: $URL$ It gives 5 wan links, and 400 Mbps is plenty for my use. Finally the price seems reasonable. $URL$ 

NeilBrown (neilb (o) suse.de) answered this by email. The problem is the loopback-devices are too small. 1 MB is too little. If size is changed to 30MB it works: 

The workers are behind NAT. No access to a jump host. Access to firewall. If you can forward ports, so that port 2001 is port 22 on host 1, port 2002 is port 22 on host 2, 2003 is host 3 ... then you can use -p: 

I have changed the controller with an identical controller (SAS2008), reseated all cables, exchanged external SAS cables, reseated all disks. I have no problem reading disks individually using 'dd', but when used in a RAID6 the disks drop offline often. 

The first 3 are the 2 physical disks and the hardware RAID on top of those. The 4th is the external diskbox connected via SAS. As you can see there is no corresponding device in /dev for the external VessRAID. The VessRAID is an external SAS-RAID box that presents itself as a SAS device. Linux clearly sees the device, but somehow it does not make it into a device in /dev. There are two logical drives on the VessRAID. I have the feeling I need to tell Linux that this is a SAS disk device that Linux should use and scan for logical drives and make them show up in /dev. But I have been unable to find the the magical command that does this. Digging around gave this: 

I would like to monitor a service that depends on the server and a login service being up. So I would like an alarm to go off: 

The template runs a portscanning on the host to see which TCP ports are open. This all works very well, but portscans every single host every 7 minutes (my guess is that it takes in total 7 minutes to ping all hosts in the network and do a port scan of all the hosts that are up). I would like it to only run once per day or so. The discovery rule has 'Delay (in sec)' 200000. The 'Template TCP service discovery' has 'update interval' set to 86400. How do I tell Zabbix to cool it? Don't run the discovery+portscan every 7 minutes. 

GNU Parallel instead spawns a new process when one finishes - keeping the CPUs active and thus saving time: 

After a while Zabbix changes the status to: 'Not supported' and "Received value [] is not numeric". The command works fine when run on the command line as root or as zabbix. But the command is never run by the zabbix server (nothing is logged). How do I find out why Zabbix thinks the command returns [] while it clearly never runs it? The solution is in the comments: The script MUST be a bash script. So a perl-script must have a bash wrapper. 

says the disks are not 100% busy (but only 40-50%). This fits with the hypothesis that the max is around 80 MB/s. Since this is software raid the limiting factor could be CPU. says: 

I have made an external script (snmp_max_io written in Perl). When run with and IP-address, it logs input to syslog and it prints a single 64-bit numeric value and set the exit value to 0. Example output: 

As far as I can see Linux has RAID6 and RAID0, but not RAID60. So RAID60 is emulated by creating RAID0 on top of some RAID6s. It is possible to add a hot spare to each of the RAID6s, but is it possible to add a global hot spare shared by all the RAID6s? If so, how can that be added to existing RAID6s that are currently assembled automatically (no mdadm.conf)? 

My software RAID can write 800 MB/s sustained. I see that happening when returns > 2 GB. However, most of the time the writeback is round 0.5 GB which gives a performance around 200 MB/s. There is plenty of data to be written. says the dirty cache is 90 GB. As I understand Dirty is what needs to be written, whereas Writeback is what is actively being written to disk. So there may be blocks in Dirty that are located on the disk just next to blocks in Writeback, and these will not be written in the same go. This can explain why I get much worse performance if Writeback is small as the time spent seeking is much longer that the time spent writing a few extra MB. So my question is: Can I somehow tell the kernel to move more data from Dirty to Writeback more aggressively and thus increase Writeback? -- Edit -- This is during low performance: