To that data structure, add a list of effect references which point to the effects to apply to a character while that character is on the tile. Since a character is potentially larger than a tile, it can potentially "occupy" more than one tile for the purpose of having effects applied. It should be fairly easy to get the range of tiles covered by a character of known width and height given the character's current origin tile position and the width and height of a tile ( tells you how many tiles the character covers along the X axis; Y can be computed similarly). With that information you can collect the set of tiles covered by any given character, and then: 

Performance is generally the major reason these features can be switched on and off by the user. Generally, they are expensive, and by allowing users to toggle them off a developer broadens the set of machines that effectively run the game to include more lower-end machines. This translates to potentially more players, and thus potentially more sales (assuming those players can live with the decreased visual fidelity). These advanced features might also incur more of a hit to battery life or fan output, as @Artelius pointed out, and so users might want to disable them for those reasons. Finally, it simply to provide users with a choice about what they prefer. Some people may not like how anti-aliasing looks, or how disabling v-sync can cause screen tearing. Or for a more practical example, consider Pillars of Eternity, which has an option to enable or disable font ligatures. These probably don't cost that much in terms of runtime performance, but some users may find the text harder to read with them. Or (as @SeanMiddleditch points out), one might want to disable a feature because it's buggy on an individual machine. Or one might want to disable a feature to get a competitive advantage (such as by disabling noisy visual effects that distract from mechanics). 

This makes the relationship between those two interfaces more explicit and clear to the consumer of the code, and avoids your need for a singleton. Obviously the factory interface that products entities will need a reference to this "," (in case creating a new entity would require creating a rendering component) but you can just pass that explicitly to the entity factory as well: 

It sounds like you can implement this with a graph data structure of some sort. Each node of your graph is a story step, and each of the edges of given node are the answers which in turn lead to new story steps. In pseudocode: 

It is worth noting that there are no industry standards for either the test material or the evaluation thereof, so your mileage may (read: will) vary wildly from that described by the answers here (my own included). That said, what you describe doesn't sound like an uncommon test structure to me (the number of questions is a bit higher than any test I've ever taken or administered, but that's about it). Both the topics you specifically called out -- complex recursion and Big-O notation -- are topics that are typical of a solid computer science education and can be extremely useful in game development. Their presence on a test is a good way to measure if you are familiar with them, or probe for potential areas of discussion in an in-person interview. Remember that, as others have noted, a test for a job isn't necessarily like a test in school. Doing poorly may not immediately disqualify you for further interviews. Code tests are just another screening and evaluation metric to help see where you stand in terms of your technical ability and if that would be an appropriate fit for the available position or positions. Above all, don't be discouraged if you feel you didn't do well. Look at is as an opportunity to grow and perhaps try again later. If you're eventually told that you didn't do well enough on the test, it's worth asking for clarification (sometimes you won't get it, but it can't really hurt to ask) so you know where specifically to focus your continuing education. 

Yes. Technology never just vanishes -- once it has reached some critical mass, it's there to stay for a long time even if it never gets actively developed much any longer. It will be a very long time before the statement "it is a waste of time to learn OpenGL" is true. Although I find your motivation and reasoning somewhat suspect, there's nothing wrong with learning OpenGL and it will allow you to learn exactly as much about 3D graphics theory and programming as D3D would. Really, in fact, it might be worthwhile to use it as a learning medium even if it were useless as a practical platform because the fundamental concepts transfer between APIs, so as long as it helps you acquire those concepts it's a good choice. 

No, you don't. All current-generation commodity GPUs use (and have used for some time) triangle-based rasterization methods exclusively. Even though older version of OpenGL support the rendering mode, these were converted to triangles by commodity GPUs. It's likely that only resulted in actual quadrilaterial-based rasterization on esoteric academic hardware or hardware used for high-end offline 3D rendering in the early 80s or 90s (I have no evidence to support this claim, I'm just postulating). That said, looking at quadrilateral (and in general, polygon rasterization other than triangle rasterization) can still be educational, and thus useful, simply by providing a different perspective of things. The algorithms involved in doing so efficiently are interesting and sometimes still have applications beyond graphics programming. I once used a edge-walking polygon rasterization technique as a means of implementing a simplistic fake 2D water simulation, for example. It also may help you understand why triangle rasterization is preferable, in part due to the potentially non-planar nature of polygons other than triangles and because of some of the resulting optimizations you can make with triangles. 

Files on disk have some small-but-non-zero overhead in terms of book-keeping information for the filesystem. Thus, 16 files storing otherwise the same data as one may be slightly larger than the single file alone. Further, files are disk are typically actually allocated at some multiple of a disk's sector size, which can exacerbate that difference slightly (given the size of typical textures these days, however, it's not really a concern until you start getting into the hundreds or thousands of texture files). However, multiple files aren't necessarily located at the same spot on the disk. While less of a concern for SSDs, spinning-disk hard drives (and disc drives) are still commonly in use for games. To read a file off of such a drive, the read head must physically move (or the disk must move underneath it) to the appropriate location, which introduces seek time into the load operation. Games that ship on disc media (like console games) used to aggressively arrange the physical layout of their files on the media to optimize for seek times. It's still done occasionally. These disk IO overheads are also one of the reasons why many games pack all their "files" into a single monolithic archive: to amortize the cost, in physical storage and OS overhead, of reading a file across the entire file set instead of paying it for every tiny file. Similarly, you will have some setup and transmission overhead in sending the texture data to the GPU for use. So if you can pack things into a single / fewer textures, it will generally save you those costs (as well as the overhead of the state switch), so it's preferable if it makes sense and does not result in other compromises for performance (there is a point at which shoving everything into a single texture may save you the file IO performance hit, but cost you more in some performance hit elsewhere; you'll have to find that balance). Many of these issues won't be serious unless you are dealing with a high volume of files and constantly hitting the disk or the GPU bus to move them about. You can also hide a lot of the problems behind multithreaded design, if you can do something else during the IO that eliminates the IO from the critical path. But generally, if there is an overhead for some operation that you will perform N times, and that overhead can be paid once instead of N times, it's generally better to do so.