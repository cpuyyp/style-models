I've seen some experienced data scientists, who use Excel - either due to their preference, or due to their workplace's business and IT environment specifics (for example, many financial institutions use Excel as their major tool, at least, for modeling). However, I think that most experienced data scientists recognize the need to use tools, which are optimal for particular tasks, and adhere to this approach. 

Based on my cursory understanding of the topics, associated with your question, I think that Gephi ($URL$ the original gephi.org link redirects there) should be able to handle neural network dynamic visualization. It seems that, in order to achieve your goal, you need to stream your graph(s) with corresponding weights ($URL$ For streaming, you most likely will need this plug-in: $URL$ UPDATE: You may also find useful SoNIA software: $URL$ 

I think that is an overkill in this situation and doesn't match your requirement of dashboard reports to be static. I guess, that your use of the term "dashboard" is a bit confusing, as some people might consider that it has more emphasis of interactivity (real-time dashboards), rather than information layout, as is my understanding (confirmed by the "static" requirement). My recommendation to you is to use R Markdown and knitr, especially since these packages have much lower learning curve than Shiny. Moreover, I have recently run across an R package, which, in my view, ideally suits your requirement of embedding small charts/plots in a report, as presented on your picture above. This package generates static or dynamic graphical tables and is called sparkTable ($URL$ Its vignette is available here (there is no link to it on the package's home page): $URL$ Should you ever need some interactivity, provides some via its simple interface to . 

It is my understanding that random sampling is a mandatory condition for making any generalization statements. IMHO, other parameters, such as sample size, just affect probability level (confidence) of generalization. Furthermore, clarifying the @ffriend's comment, I believe that you have to calculate needed sample size, based on desired values of confidence interval, effect size, statistical power and number of predictors (this is based on Cohen's work - see References section at the following link). For multiple regression, you can use the following calculator: $URL$ More information on how to select, calculate and interpret effect sizes can be found in the following nice and comprehensive paper, which is freely available: $URL$ If you're using (and even, if you don't), you may find the following Web page on confidence intervals and R interesting and useful: $URL$ Finally, the following comprehensive guide to survey sampling can be helpful, even if you're not using survey research designs. In my opinion, it contains a wealth of useful information on sampling methods, sampling size determination (including calculator) and much more: $URL$ 

I was not sure about posting this question with mentioning the name of the company, which I quite respect and admire. However, I've figured that a wider exposure might help the team to fix this and similar problems faster as well as increase the quality of the machine learning (ML) engine of their website. The problem exposes itself by too many occurrences of a quite trivial misclassification error on Amazon's book categories classification (which I'm a frequent visitor of). In the following example, the underlying reason of such behavior is quite clear, but in other cases the reasons might be different. I am curious about what could be other potential reasons for misclassification and what are the strategies/approaches to avoiding such problems. Without much further ado, here's how the problem appears in real life. I was reviewing some books, related to transitioning from graduate programs (Ph.D., in particular) to work environment in academia. Among several other books, I ran across the following one: 

One needs to use an artificial intelligence (AI) API, if there is a need to add AI functionality to a software application - this is pretty obvious. Traditionally, my advice on machine learning (ML) software includes the following two excellent curated lists of resources: this one and this one. However, keep in mind that ML is just a subset of AI domain, so if your tasks involve AI areas beyond ML, you need more AI-focused tools or platforms. For example, you can take a look at ai-one's AI platforms and APIs as well as interesting general AI open source project OpenCog. In addition to the above-mentioned AI-focused platforms, IBM's Watson AI system deserves a separate mention, as quite cool and promising. It offers its own ecosystem for developers, called IBM Watson Developer Cloud, based on IBM's BlueMix cloud computing platform-as-a-service (PaaS). However, at the present time, I find this offering to be quite expensive as well as limiting, especially for individual developers, small startups and other small businesses, due to its tight integration with and reliance only on a single PaaS (Blue Mix). It will be interesting to watch this space as competition in AI domain and marketplace IMHO will surely intensify in the future. 

If you're working with R language, I would suggest first to try use R ecosystem's abilities to parallelize the processing, if possible. For example, take a look at packages, mentioned in this CRAN Task View. Alternatively, if you're not comfortable or satisfied with the approaches, implemented by the above-referred packages, you can try some other approaches, such as Hadoop or something else. I think that a Hadoop solution would be an overkill for such problem, considering the learning curve, associated with it, as well as the fact that, as far as I understand, Hadoop or other MapReduce frameworks/architectures target long-running processes (an average task is ~ 2 hours, I read somewhere recently). Hope this helps. 

Note for moderators/administrators: This question seems not to be a data science question, but purely an R question. Therefore, I think it should be moved to StackOverflow, where it belongs. 

I would suggest you to check this excellent presentation by Li Deng (Microsoft Research). Many of the slides contain references to relevant research papers and even several interesting books on the topics of interest (it should be pretty easy to find). It might be also helpful to check references, listed in this research paper by Prof. Andrew Ng and his colleagues at Baidu Research. Finally, a focused Internet search will provide you with comprehensive list of resources for further research. 

I would try to analyze and solve one or more of the problems published on Kaggle Competitions ($URL$ Note that the competitions are grouped by their expected complexity, from (bottom of the list) to and (top of the list). A color-coded vertical band is a visual guideline for grouping. You can assess time you could spend on a project by adjusting the expected length of corresponding competition, based on your skills and experience. A number of data science project ideas can be found by browsing the following webpage: $URL$ If you have skills and desire to work on a real data science project, focused on social impacts, visit projects page: $URL$ More projects with social impacts focus can be found at fellowship webpage: $URL$ Science Project Ideas page at site looks like another place to visit for inspiration: $URL$ If you would like to use open data, this long list of applications on can provide you with some interesting data science project ideas: $URL$ 

I'm not an expert on this, so take my advice with a grain of salt. It's not clear for me what is the relationship between server-side and client-side data. Are they both representative of the same population? If Yes, I think it's OK to use different data sets for testing/training and evaluating your models. If No, I think it might be a good idea to use some resampling technique, such as bootstrapping, jackknifing or cross-validation.