Yes, thats right, its computable. The issue is that your function really isn't producing the solution to an infinite family of problems, the way (say) a function computing a solution to the halting problem is -- so there is no issue about computation. Instead, you are representing in function-form some single mathematical fact with finite representation -- either an integer, or the fact that f is the constantly 1 function It is possible to encode the halting problem in individual real numbers, like Chaitan's constant $\Omega$, but integers always have finite representations and so can be encoded as Turing Machines. Finding the correct algorithm of course might be a hard problem. But finding correct algorithms is usually hard! 

The standard analysis of all of these algorithms actually shows that the resulting cut is at least as large as $\frac{1}{2}\sum_{e \in E}w(e)$, which is an upper bound on $1/2$ the weight of the max-cut if $w$ is non-negative -- but if some edges are allowed to have negative weight, is not! For example, algorithm 1 (pick a random subset of vertices) can clearly fail on graphs with negative edge weights. My question is: 

Bollobas, Fenner, and Frieze ($URL$ give a polynomial time algorithm for finding Hamiltonian cycles in random graphs, that has an error rate asymptotically tending to 0 in the size of the graph. If you wanted to generate n vertex graphs that were not Hamiltonian, you could select a random graph $G_{n,m}$ with $m$ such that the graph was Hamiltonian with constant probability bounded away from 1. You could then run the BFF algorithm to attempt to find a Hamiltonian cycle in it, and reject the graph if the algorithm succeeds. After a constant number of rounds, you would expect to find a graph for which the algorithm failed to find a Hamiltonian cycle, and although this graph might in fact be Hamiltonian, the probability of this will be diminishing in $n$. Of course, this does not select uniformly at random from the set of all non-Hamiltonian $n$ vertex graphs, but it does select from an interesting subclass -- one for which you would expect a nontrivial fraction of graphs to be Hamiltonian, as well as a nontrivial fraction not. 

First, I don't think it is true that at most universities you can only apply to one department or the other. I know many people who have applied to both math and CS departments, particularly at MIT where lots of theoretical computer science is done in the math department. There are also several joint programs between math and CS departments that seem well suited to your interests. Some that come to mind are the ACO programs at CMU (here) and GAtech (here). At MIT, it is reasonably easy for you to take an adviser from either department, so it does not make a big difference whether you are in EECS or applied math. 

Uniform convergence bounds should imply that agnostic learning with respect to a succinct distribution specified by a finite set S is no easier than learning with respect to the uniform distribution, at least for large enough $S$. Specifically, for any class of functions $C$ with VC-dimension $d$, the canonical algorithm which takes a randomly sampled collection of $m$ points for $m = O(d/\epsilon^2)$ and finds the function $f \in C$ which minimizes error with respect to the sample, will be within an additive $\epsilon$ of the minimum error function over the entire target distribution. So if you have an algorithm for agnostically learning with respect to an explicitly represented set of points $S$, you can learn over the uniform distribution by just taking a sufficiently large sample of points $S$ and feeding $S$ to your algorithm for concise distributions. In fact, this holds for any distribution, not just the uniform distribution. Now there are ${d \choose k}\cdot 2^{2^{k}}$ size $k$ juntas, and so they have VC-dimension at most $d \leq 2^k + k\log d \approx 2^k$. So for large enough $S$ (at least, larger than $2^k$), lower bounds for agnostically learning w.r.t. the uniform distribution (or any other distribution) should apply to the problem of learning w.r.t. a concise distribution. 

A fixed point of a best response function is a Nash equilibrium -- the fact that you do not have the payoff matrix cannot make the problem easier (since if you know the payoff matrix, you also know the best response function, but not vice versa). Unfortunately, there are not in general good algorithms for computing approximate Nash equilibria in multi-player games. It is known that in general settings, computing a fixed point of a function is equivalent in difficulty to computing a Nash equilibrium -- see for instance $URL$ There are not believed to be efficient algorithms for computing fixed points for general (best response) functions, and it is open whether they can be approximated well in general. 

Well, it is important to specify what exactly you mean by "privacy". If you want a formal guarantee that no attacker will be able to learn very much about any individual, even given the output of your SVM, your best bet is probably to try and guarantee "differential privacy". This is a strong formal guarantee that the distribution output by your learning algorithm is insensitive to any individual datapoint in your training set. Luckily for you, it is known how to train SVMs while preserving differential privacy. Here are the two papers you might be interested in: "Differentially Private Empirical Risk Minimization" by Chaudhuri, Monteleoni, and Sarwate and... "Learning in a Large Function Space: Privacy-Preserving Mechanisms for SVM Learning" by Rubinstein, Bartlett, Huang, and Taft. 

This is discussed in section 2.2.3 of Jeffrey Hartline's thesis: $URL$ Online problems are all about informational uncertainty: you don't know what inputs are coming tomorrow, and the difficulty is often information theoretic, not computational. On the other hand, there is no uncertainty in an incremental optimization problem as Hartline defines it: every parameter of the problem is known at the outset. Without computational restrictions, the problems can always be solved optimally. So perhaps your definition is wrong, since indeed yours sounds like an online problem. The problem of "incremental optimization" seems to be defined in this 2008 thesis, and differs from your definition in that there is no uncertainty. 

Yes -- both gradient descent and the randomized weighted majority algorithm (often called multiplicative weights these days) are instantiations of the "Follow the regularized leader" framework. Gradient descent is what comes out if you regularize with the euclidean norm, and multiplicative weights is what comes out if you regularize with Shannon entropy. See e.g. here ($URL$ for a derivation of the weighted majority algorithm from this framework. 

There is a polynomial time algorithm for finding Hamiltonian cycles on random graphs, that succeeds asymptotically with the same probability that a Hamiltonian cycle exists. Of course, this problem is NP-hard in the worst case. They also show that a dynamic programming algorithm which is always guaranteed to find a Hamiltonian cycle, if it exists, has polynomial expected running time, if the input distribution is uniformly random over the set of all $n$ vertex graphs. Reference: "An algorithm for finding Hamilton cycles in random graphs" Bollobas, Fenner, Frieze $URL$ 

A line graph has O(V) edges and diameter V-1. A star graph also has O(V) edges and diameter 2. So it seems that the constraint that a graph has O(V) edges places almost no restriction at all on what the diameter can be. Of course, 2 is the minimum, since only a complete graph has diameter 1. 

Main stream game theorists are, I think, becoming much more open to contemporary work in the computer science community, so it may be less necessary to "make the case'' for algorithmic game theory than it has been in the past. One of the texts that I know of that is most accessible to auction theorists with an economics background is Jason Hartline's "Approximation in Economic Design". Chapter 1 in particular tries to make the case for approximation algorithms, if not specifically for the importance of computational complexity. 

Transforming intuition into formal argument is difficult, and the degree to which it is viewed as necessary is sociological, and varies by community. In general, arguments tend to be less formal in AI conferences, reasonably formal in the "Theory A" community, and extremely formal (i.e. machine verifiable) in parts of the programming languages "Theory B" community. Probably the best way to develop this skill is to take rigorous courses that require formal argument on problem sets. You can also just read papers in the community that you would like to publish in, but you have to make sure to actually read the arguments carefully for this to be helpful. i.e. it is easy to read a paper and feel that you understand it, but be unable to reproduce key parts of the argument. A good way to do this is to plan to actually present the paper to a group of friends. If you actually have to present the arguments on a board and answer questions, you will be able to force yourself to really understand. Do this a few times and you should be able to construct formal arguments of your own.