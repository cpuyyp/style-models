You can create a DNS alias (pointing MyPhone to the DNS name of your web server) by selecting New Alias (CNAME) from the Action menu in DNS Manager. However, you'll also have to configure your web server to recognize the name MyPhone as a distinct web site, and create a redirection from that web site's home page to the URL you want, or, depending on your needs, put the content in question on the home page of that site. How you do this depends on what web server you're using. 

When you get to the screen that lets you select which partition to install Windows on, there is an option to load additional device drivers. Or you could press to get a command-line prompt and use . Also, if loading the network drivers doesn't resolve the problem, you could try using after loading the network drivers to explicitly establish an iSCSI session rather than using ipxe. I'm not very familiar with this technology, though, so I'm not sure what the recommended approach is. 

They are related in that UAC relies on the existence of ACLs (and all the related security apparatus) in order to function. UAC works by removing (technically speaking, disabling) the Administrators token from non-elevated processes. This means that if the ACL for a particular file or folder only allows access to Administrators, non-elevated processes won't have access. That's why an installer can't write to Program Files without elevating first. EDIT: See this article for more information about UAC. In particular: "When an administrator logs on to a computer that is running Windows 7 or Windows Vista, the user is assigned two separate access tokens. Access tokens, which contain a user's group membership and authorization and access control data, are used by the Windows operating system to control what resources and tasks the user can access." I suppose you could argue that UAC depends on the Windows authorization model, of which ACLs are only a particular component, and that therefore UAC is not related to ACLs. I don't think that's a useful way of looking at it. Let me put it this way: if there weren't any ACLs, UAC would be pointless. You should also read this article which addresses some common misconceptions about UAC, and in particular the misconception that it is a security feature: "The primary goal of UAC is to enable more users to run with standard user rights. However, one of UAC's technologies looks and smells like a security feature: the consent prompt. Many people believed that the fact that software has to ask the user to grant it administrative rights means that they can prevent malware from gaining administrative rights." 

How come I can still read any CF metadata? I noticed that in the client code, the script goes to use instance credentials ( is True) 

If a directory "foo" is owned by user A and contains a directory "bar", which is owned by root, user A can simply remove it with , which is logical, because "foo" is writable by user A. But if the directory "bar" contains another root-owned file, the directory can't be removed, because files in it must be removed first, so it becomes empty. But "bar" itself is not writable, so it's not possible to remove files in it. Is there a way around it? Or, convince me otherwise why it's necessary. 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

It's single availability zone no backups not in a security group that's reachable from the outside world 

I know I can use IAM roles, but I found it's just too many moving parts, and complicates any scripts that have to use the access key/secret key (e.g. rewriting /etc/apt/* lines when it changes). Not to mention there is no way to attach roles to existing instances, which makes it even more pain. It's also not possible to simply restrict access by using VPC subnet, because S3 bucket access goes via public EC2 interface. 

We were a bit worried about that, because on several of our servers LOM1 is attached to a dedicated management network and LOM2 is attached to the LAN, and we don't want the DRAC to be accessible from the LAN. The puzzling thing is that, experimentally, it doesn't seem to be true. (Details below.) With the DRAC configured in shared mode, the DRAC is only reachable from LOM1, not from LOM2. Is the article wrong? If not, how do I get the DRAC to respond on LOM2? (Short of using Failover mode which according to the article requires teaming, i.e., both LOMs must be on the same network.) Most importantly, what, if anything, do I need to do to be certain that none of our DRACs are accessible via LOM2? 

The approval setting for the child groups are not actually changing in this scenario; they remain "Same as Parent". It is just that when the approval is "Same as Parent" the console shows the parent setting with a note that the setting is inherited. If you have explicitly changed the setting for a child group to anything other than "Same as Parent" then changing the setting for the parent group will not affect the child group. The "Apply to Children" option changes the setting for all child groups to "Same as Parent". 

I have a Domain Controller that for some reason beyond me has ADCS installed on it. The domain controller is a 2008 R2 server and needs to be demoted, but first I need to do one of two things. This same server also runs DHCP, NPS (RADIUS server),and other third party software. My Options (according to me) 1. Migrate ADCS to a different server (it would be a new machine with 2012 R2). $URL$ 2. Spin up an offline non-domain joined root CA and have a live domain joined issuing CA. I would then backup ADCS on the domain controller, revoke any issued certificates and remove the server roles. It would seem that for option 1 to work i need to migrate the computer name and IP as well. If I go with option 2, what steps can I take to mitigate the possible impact. Currently the CA has issued 9 certificates. However, only 3 have not expired, 2 of those expire in the coming week, the third one a year from now. What would be the best way to get ADCS off this domain controller? 

Unless someone has blocked outbound/destination ports to (specifically 8530) your clients should not have any problems checking for updates from your WSUS server. You may inadvertently be blocking the client connections on the WSUS server itself. Even if by default port 8530 is open, I would verify by testing the WSUS server for port 8530. You can use Putty or another telnet emulator to connect via telnet, just change the telnet port to 8530. If the connection is successful then and clients are unable to connect to and get updates from WSUS then you have other problems. Give this link a try WSUS Troubleshooting. 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

But it's not ideal, as it doesn't handle restarts and remote endpoint downtime very well, because it doesn't have anything like 's pooling, so I'll get duplicate logs and/or drop logs. Given that CoreOS has no package management, is there a conventional way to solve this painlessly? 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

I have a script that scans a server for drives that contains shares. After that I need it to create a file screen from a template that is already created with the same script, that part works fine. Here is how I am getting a list of drives that have shares: 

Alright I just had to prevail a little longer but I figured it out. I changed my for-each statement to the following: 

It seems to me like the script is running but not capturing the screenshot when run through windows task scheduler. The saved image is just one white page. Does anyone know why this is not working? 

I have been trying this many different ways.I get errors like "cannot validate argument on parameter 'Path'. The argument is null or empty. 

I have a cisco asa 5505. The outside and inside IPs are private IPs so I do not want to NAT the inside IPs as they go out the Outside interface (there is a router upstream of the ASA 5505. Would I do static NATs? Or how would I pass traffic from the inside to the outside (like on a ping) without NATing the IP? Outside e0/0 VLAN 1000 IP: 192.168.0.6 255.255.255.248 Inside e0/1 VLAN 2 IP: 192.168.0.230 255.255.255.252 (this int has a Router downstream that i want to telnet into from the Internet 192.168.0.229/30) The Upstream router will have a 1:1 Nat mapping a public IP to the ip of the router downstream of the ASA5505. 

But it glosses over the specifics of how these policies are combined and when the "fall through" happens to the next policy in the list, i.e. under what conditions each policy fails and moves on to the next policy in the list. For example, I have a policy list in my group and yet after scaling up and then down, the scaling group proceeded to terminate by newest (and healthy) instance (newer by a large margin), and I can't figure out why. Additionally, according to the same doc, default policy is actually itself a combination of policies, and includes and as two of its steps. If I have a list that includes , does it evaluate and twice? Lastly, does the termination consider load balancer? For example, if my new instance failed to initialise properly and is not in-service with the load balancer, and is in effect, will scale-down action kill the unhealthy instance first even though it's newer? 

I am trying to use AWS autoscaling lifecycle hooks in a template that encapsulates the following things: 

Within plain EC2 environment, managing access to other AWS resources is fairly straightforward with IAM roles and credentials (automatically fetched from instance metadata). Even easier with CloudFormation, where you can create roles on the fly when you assign a particular application role to an instance. If I wanted to migrate to Docker and have a kind of M-to-N deployment, where I have M machines, and N applications running on it, how should I go about restricting access to per-application AWS resources? Instance metadata is accessible by anyone on the host, so I'd have every application being able to see/modify data of every other application in the same deployment environment. What are the best practices for supplying security credentials to application containers running in such environment? 

The administrator of the parent domain removed the DC object the next day using ADSIEdit. I was then able to remove the orphaned domain as described in KB230306. I believe that part of the problem was that not all of the information had fully replicated, perhaps due to some odd issues with DNS in the isolated environment. It is possible that, had I waited overnight, the procedure I originally tried (described in the question) would have worked. 

I suggest you look into the Sysinternals tools (now part of Microsoft) in particular AccessChk and AccessEnum. I haven't used them myself, but they sound appropriate to your needs. 

The only way to create the application data folder (and contents) is for the user to log in. If the user has logged in, but was not created, the most likely reason is that the application data folder has been redirected by group policy. In this scenario, your customer probably just needs to find the user's RSA folder (which is probably sitting on a file server somewhere) and make a copy of it in the location your software is expecting. 

You can also do it from the GUI if you prefer. You don't need to turn off "Include inheritable permissions from this object's parent" but I recommend you select the "Apply to files only" option. 

Is it possible to buy an intermediate certificate to use it to sign subdomain certificates? It has to be recognised by browsers and I can't use a wildcard certificate. The search turned up nothing so far. Is anyone issuing such certificates? 

As I understand it, an instance needs to be granted access to resources in order to do anything with CloudFormation. But when I run this on a Beanstalk web server instance: 

with associated scale up/down policies, launch configuration, IAM role, etc. 2 of for EC2 launching/terminating events. (in a simplified example) where lifecycle notifications get posted. role for the autoscaling group to post notifications to the SQS queue. 

I don't specify any access/secret keys in the command line. My instance role was manually created (by me) and definitely does NOT grant any permissions on resources. 

For some reason the CPU usage is at 20% CPU while I'm doing absolutely nothing, exactly every 10 minutes spiking to 28-30%. I thought there was something wrong with the instance, so I've re-created it, same thing. What does this? Is this an RDS phenomenon in general or is this specific to the burst capable instance classes? 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

I am having trouble running my command to create a file screen for each drive that contains a share. I have to run this on a lot of servers, any ideas? Here is where i am at now: I can create a file screen if i manually enter the path in this first line. But I need to automate that, each serer will have different shares. 

So i realized that all the space between c:\users\ and Public was just white space. So i edited my list to remove all the white space after each username and after that it works. 

I have exchange 2010. If I go to $URL$ and send an email by entering an email address that exists on my exchange server in the "from" box and send it to my gmail address, My exchange server goes ahead and sends the email. Gmail blocks it because its spoofed. Here is the deliverable message I got in my inbox from Gmail. 

I have this foreach statement that goes through a list of usernames and puts each name in the path listed below, then it copies and pastes a file to the individual users startup folder. For some reason i get an error that a portion of the path was not found. Any ideas what the problem could be?