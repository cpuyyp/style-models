Another possibility is to calculate the position of your fragment in Model Space, World Space or another space of your choosing. Once you know that, you set a threshold up to which you color the object red and over which color the object green. In your fragment shader you would do something like 

I'd say it is the other way round, $L(v) \leq E(v)$, since $E(v)$ would consider any light source emitting light onto a surface, whereas $L(v)$ only considers light form $v$ (if you consider $v$ as the light source to surface direction. In my writing earlier, this is $\omega$ and in Real Time Rendering specifically, this is $\omega$ as well, or $l$ in chapter 5 as well as in the BRDF Theory chapter). Also, remember that these two physical quantities don't have the same units and should not really be compared this way. 

This depends on your calculations really, since you can do it either way. However, probably when you reach tone mapping you will be in linear space (i.e. no gamma correction has been done yet and you don't assume non-linear space for your calculations). Assuming this, gamma correction should be applied after tone mapping, otherwise you have "linear" tone mapping on non-linear color values. To quote from Real-Time Rendering 3rd by Möller, Haines and Hoffman, p. 145: 

If you have the prerendered images, you can "simply" render them onto a grid of quads. This is done with texturing, i.e. you upload the image to your GPU as a texture and apply texturecoordinates to quads, such that when you render the quads, the textures are applied correctly to their surfaces. You will however need to learn OpenGL (preferrably with GLSL), Direct3D with HLSL or even Vulkan, all of which will take some time. Depneding on where your goals lie, you may be better of going with a preexisting rendering engine and just learning like Unity or Unreal. If you do want to learn one of the mentioned Graphics APIs, there are numerous tutorials on the web. For OpenGL I can recommend the youtube tutorials by ThinMatrix, for the other two APIs you'd need to ask someone else. Furthermore, there are splendid books online as well as offline. If you have some money to burn, the basics of rendering are covered in Real-Time Rendering and Mathematics for 3d Game Programming and Computer Graphics. In regards to OpenGL, there are the Online Tutorials of Learn OpenGL, as well as offline books in pdf and epub version. 

EDIT While the basic principle of vector calculation below is correct, as Sebastián Mestre pointed out, in lighting you would use the vector from the lighted fragment to the eye position. Therefore, you would in fact use $v = -(point - eye) = eye - point = -point$, so you were actually correct in your "negative" assumption. Still (what might have been more a typo than a misunderstanding), it is the negative position of the lighted fragment, rather than the negative eye (which would still be the origin). As Sebastián also said in a comment to you, you would usually normalize the view vector after you have calculated it. Sebastián is the MVP. 

So yes, since your new first point is inside the viewport, but your second point is outside, the situation is non-trivial (i.e. no "go render it all" or "go discard it all"), it will repeat with the second point. Afterwards, both points will be inside the viewport, therefore it is a trivial draw and no further action is needed. 

What you need is to think about what your kernel must look like. In a simple box filter, you would sample a texel and every neighbouring texel, giving you 9 values. You add up the 9 values, divide by 9 to normalize and write the output. Your gauss blur can work the same way, only that you have to properly weigh the individual values depending on their position. You can also sample more pixels, than the direct neighbour, say the second neighbour as well - this will give you a nicer blur but will also be a lot slower since you have to do more texture look ups. In any case, start with hardcoding that, like you did in your old code. In image processing, they also use these kernels, so you can look at them. The linked one would translate to your problem thus: 

If you want to display two colors next to each other, you would typically add two objects to your scene next to each other, draw one of them with your shader that only displays red color and the other with your shader that only displays green color. 

Yes, there are other physically based lighting concepts. You can have a look at image based lighting and Area Lights (such as Sphere or Tube lights). If you have access, have look into Physically Based Rendering, if i recall correctly, that has a chapter or two about non-point lights. Free on the internet is for example Real Shading in Unreal Engine 4. Furthermore, there is the Physically Based Lighting at Pixar paper, although I am not sure how real-time suitable this is. This should give you a starting point into the topic. You should read up on it and then perhaps come back, if you have more detailed questions. 

You can get the coordinates of C by using circle equations for one circle C_A with point A as center and AC as its radius and one circle C_B with point B as its center and BC as its radius. Now you can calculate their intersection points, which leaves you with 2 points (iff C is not on a line between A and B). I am not entirely sure how you want to determine which of the two candidates C_1 and C_2 is the correct one. If I understand correctly, you always label the triangle counterclockwise. There is a neat way to check this, so you should have everything to find your point. 

Yes, you can scale your blur filter with a depth value (where you should "invert" the value to make the blurriness scale to larger effects on closer proximities, i.e. smaller depths). For a box filter, you take ratios of the pixels neighbourhoods, but you need to make sure that those ratios add up to one. For this, you'd ideally increase the ratio for the center pixel based on the proximity and reduce the ratio for the neighbouring pixels accordingly. 

This is not quite correct, you need to apply the modelviewprojection matrix and thus transform your vertices into Clip Space Coordinates. You can apply the ModelView Matrix in certain instances, e.g. if you want to calculate light in Eye Space. However, the will then still not use the ModelView but the ModelViewProjection Matrix and your light/whatever- position will use the ModelView Matrix and be transferred to the fragment shader via the variables. For a more detailed discussion, you can look into the comments on this question, I had to be corrected about that myself (since not all literature is clear/correct about Normalized Device Space and Clip Space). 

Model (/Object) Matrix transforms an object into World Space View Matrix transforms all objects from world space to Eye (/Camera) Space (no projection so far!) Projection Matrix transforms from Eye Space to Clip Space 

Careful. We don't associate direction (other than considerung only surfaces perpendicular to the light direction) with irradiance. We do however with radiance. 

Be careful, if you're really in screen space. My guess is, you're doing picking or deferred rendering, in which case you're probably reading fragments in Normalized Device Space (i.e. x and y in range *). If so, you can do it like this: Read out Normalized Device Coordinates as with depth as z value and as w value and bring them into range by multiplying by and subtracting . Convert them into Eye Space with the inverted projection matrix. Divide your new vector by its coordinate (which is your undivide step). Now you're in Eye Space. There are more ways to do this, you can have a look at Reconstructing Position From Depth to dive further into how. *I'm assuming OpenGL, ranges might be different in DirectX 

To get the height intersection once you have found the polygon, you can build a plane of the polygon and calculate the distance to the plane. For example you can displace the point along the planes normal and calculate the length of the normal for when the point is displaced into the plane. Implicit planes are neat for this, they are build with the normal of the plane (which is the normal of the polygon) and one constant $d$. $d$ can be calculated by arbitrary point on the plane (e.g. one of your polygon vertices). $d = -\vec{n} \cdot q$ and $\vec{n} + d = 0$ (where $\vec{n}$ is the normal and $q$ the point/vertex). The neat part is: if you calculate the dot product of your point you want the distance of, you have the signed distance toward the plane, i.e. if $p$ is your point and $l$ is the distance between the point and the plane, then $l = (\vec{n} + d) \cdot p$ (for this you assume the fourth coordinate of a point as $1$) Now that you have the distance to the plane, you need to check, weather the intersection of normal through point with plane also intersects the polygon. You could for example take the vectors $\vec{v}_i = v_i - p$ where $v_i$ are your polygons, and then take the dot product $\vec{n} \cdot \vec{v}$. If they all have the same sign, your intersection doesn't hit the polygon (assuming the polygon to be convex). Iff it doesn't hit the vertices you need the distance to the closest two vertices. Since you have already done the dot product test (with the sign) for all polygon vertices, you know that the shortest distance can't be to the edge between the two vertices, and thus it suffices to take the shortest of the two point->vertex distances. As to which polygon is closest, I'm not sure if there is a better trick, but I'd just transform the point into Model Space of the mesh and calculate the distance to any front facing face.