Since the granularity of caching is done at a page level (both in InnoDB and de-facto in MyISAM due to filesystem block), having large 42-column rows means that you will fit fewer rows per page on average and there is no split where the hot sub-set of columns are kept in memory while the inactive ones can not. This results in a sort-of cache dilution, where you may require more memory than if you were to normalize the schema and split into a few different tables. (Note: InnoDB does overflow large text/varchar/blob columns to separate pages. I agree with Rick's comment that InnoDB is the way to go.) 

As a database administrator, you are part of a team whose goal is to make a job easier, or just possible, for someone. The team includes system administrators, database administrators, database programmers, application programmers, network administrators, and so in. A valuable team member knows not only their own role but a little bit about every role in the team. Specifically, as a database administrator, it will help you to know some or all of the following. 

After switching to a different master you must reseed the identity value, otherwise you will get duplicates in the id column. Use the statement. 

Two drives, RAID 1 - Operating system, executables, pagefile. Four drives, RAID 5 - All data files (alternatively, RAID 1+0) Two drives, RAID 1 - All log files 

You've got to put it in context - InnoDB only verifies the checksums when it reads a page from the block storage device, and updates it before flushing it back to the storage device. While in memory, the checksum is not maintained. For many years, an IO to a disk has taken something on the order of 5-10ms (1ms = 1/1000th of a second). Computing a checksum probably takes somewhere around 50us (1us = 1/1000000th of a second). I don't have the actual data what it is in InnoDB's case, but if you Google "Numbers everyone should know", you'll hopefully agree I'm correct within an order of magnitude. Enter an era now where we have things like Fusion-io flash devices, which on paper have a ~20-30us access time, and you can see that reevaluating the checksum makes sense. My general advice: Don't ruin your backwards compatibility with MySQL releases unless you really need it. Most people do not need it yet. 

A non-RDBMS suggestions: if you are doing these sorts of queries often then I would suggest creating a cube of the source data and aggressively pre-aggregating the cube. This does, of course, require an installation of Analysis Services and a DBI with Analysis Services experience but if you are doing OLAP queries often then creating the data warehouse may be cost-effective in the long run. If you have Excel 2010 or 2013 then PowerPivot for Excel is a cheaper alternative to a full Analysis Services installation. 

Both the default instance and SQL Browser maintain a list of instances running and their current port numbers. If there is nothing listening on those two ports (maybe there is no default instance, maybe the default instance is using a different port, maybe SQL Browser is stopped) then the only way to connect is to manually specify a port number in the connection string using a comma. For example, PRODDB\Payroll,14550. SQL Browser sends broadcast network traffic so a lot of administrators prefer to not run it. TechNet: Default Client Connection Behavior. As an aside, port tcp1434 is used by the default instance for the Dedicated Administrator Connection. 

There is no footprint, and you will likely not see overhead. Sure, there are hypothetical cases: If you have many virtual columns and from a client then more data will be sent across the network (since rows are sent in full; clients don't know they can reconstruct certain data from virtual column definitions). 

You have to understand where most of the tools you are using are getting their data from - and . This data simply is not available broken down on a database level inside MySQL. MySQL 5.5, 5.6 and Percona Server have been doing a great job of improving the diagnostics available to you with features like performance_schema and userstats - but I don't know of any GUI/monitoring tools that are taking advantage yet. I recommend installing Percona Server, and taking a look at table_statistics: 

No it is not, because identity does not guarantee a unique value. The identity property can be bypassed with (in SQL Server - you didn't specify what RDBMS you are using). A primary key constraint (and a unique constraint) uses a unique index to enforce uniqueness. 

I've just tried creating and dropping a table using Notepad to enter the command. It worked fine for me (Server 2012 R2, SQL Server 2014). In Notepad, I typed I selected all of that text, copied, and pasted into the create table and drop table statement in SSMS. In SSMS I could see that the nonprinting character was there, because using the arrow keys paused at the underscore. 

Fan in (multi-source replication) will be supported from MySQL 5.7. A labs release is available here: $URL$ 

I agree with @Remus' last point - most people use Hadoop for crunching and store the result in MySQL. You can also have apache write a custom log for you with only the fields you require, and export environment variables from the application for Apache to save (if required). For prior art, I would recommend taking a look at how OpenX (advertising server) has solved this problem through various versions: 

Small starting clarification: the article you linked to on InnoDB text/blob storage is a little out of date with MySQL 5.5, and the barracuda row format. This article is more up to date. On to your question: In the schema you have, each row will be less than ~8K, so you can guarantee across both antelope and barracuda row formats that all data will be stored in-line - not requiring external/overflow pages. If you were to require overflow pages, they are never de-duplicated (which is what I would probably describe your 'pooling' mechanism as). Even worse than they are never de-duplicated, they are never shared... If you could have a record too big to fit inline (~8K limit), each text/blob that needs to be moved out will take a minimum of a 16K page to itself. 

Look to see if the data is stored somewhere else. Perhaps the system sends email confirmations. If so, retrieve the emails (from sent items or from auditing on the email system) and hire some temps to retype the information. Perhaps the system prints out reports. If so, obtain the printouts and get the temps typing. Perhaps the system exports data and sends it somewhere else. If so, get the exported data back from where it went and get those temps typing. 

Alternatively, you could query the management view and find the table's object id. Use this to generate a dynamic SQL statement. Copy the object id from the results and paste it into the dynamic SQL. 

and then will show the rewritten form of the view. From here, it is easier to analyze performance characteristics. Sight checking queries is generally not easy to optimize. 

I believe the background threads are fixed in number (the actual count will depend on some configuration settings such as and the number of etc.) The FOREGROUND threads are one per connection + a potential of on top of that. 

I have a blog post explaining why this is here. The short version: The query cache causes scalability issues on multi-core machines. So it is now disabled by default. 

As noted, will change the system tables to include any new columns required. MySQL 5.6 includes support for microseconds in and , and as such uses a different format on disk for storage. Conversion to the new format does not happen as part of , but will happen on or and in which case you will not be able to start MySQL 5.5 and use this data directory. 

If you want to give principal Alice the SELECT, INSERT, UPDATE and DELETE permissions to all table-valued objects in all schemas then use the following. 

I know you asked about reading log files but I there are better strategies for dealing with slowly changing dimensions. If you have the Enterprise Edition of SQL Server then you can use Change Data Capture. It needs to be set up on the database and tables beforehand but will then track inserts, updates and deletes. You can use the lighter-weight Change Tracking. Microsoft have published a comparison of Change Data Capture and Change Tracking. You can use a column to implement high-watermark tracking. The nightly process records the current highest rowversion then queries the table for all rows with the rowversion greater than last night's highest value. This is for inserts and updates. If you wish to track deletes then you need to write delete triggers that change a "deleted" flag. You can use triggers on the tables that copy the inserted, updated and deleted data into tracking tables. The nightly process reads then empties these tracking tables. As an aside, this is similar to how merge replication and updating-subscriber transactional replication work. All of these methods require modification to the source database objects. I'm guessing from your question that this might not be a possibility. If that's the case, see VonPryz' answer. 

The minimal package was designed for use by the official docker images for MySQL. It cuts out some of the non-essential pieces of MySQL, but is otherwise the same product. 

It used to upset me how features were decided at MySQL... How was it decided that partitioning was a critical feature for 5.1, but backup totally missed the radar? There seemed to be a bunch of low hanging fruit (years old bugs) that were not being addressed, and I was always cynical that unless it could check off a box on a features grid, it would never be handled. There was a bit of talk, but no indication it was any better under Sun's management. However, now that Oracle is in control, several years old bugs are being addressed, performance has become a feature, and I actually find really compelling reasons to upgrade to 5.5 and 5.6. I feel awkward having to defend one of the world's biggest software companies, but they're really not getting enough praise. Instead everyone is making claims they are somehow screwing the project. Most of the projects they 'screwed' made no commercial sense to them... however they make a non trivial amount of money on OEM licenses and subscriptions/professional services for MySQL.