I'd like to be able to render a large population of small independently moving objects in real time. They may move in a swarm-like manner, but their relative positions will not be coherent - their position may change arbitrarily within a swarm and swarms may break up and reform at any point. What approach to building a bounding volume hierarchy would best suit this situation? Is there a way to maintain a hierarchy which is sub-optimal but good enough, that only requires a partial update each frame? Or is there a way of building a hierarchy from scratch each frame that is fast enough for smooth animation? The number of objects will be too large to render without a hierarchy, but for the same reason I expect building the hierarchy to be time consuming. 

Yes, this is to be expected. The difference between your two images is the difference between using and not using importance sampling. The first image shows noticeably more noise than the second. This is because instead of biasing the hemisphere samples towards the normal by using cosine weighting, the code instead samples uniformly over the hemisphere and then gives less reflected light to rays nearer to perpendicular to the normal. Note that since this uses the dot product, this is also cosine weighting: $$A\cdot B=\|A\|\|B\|\cos{\theta}$$ It's just weighting the contribution of each ray, rather than weighting the number of rays in each direction. The second image is less noisy because it doesn't waste many samples on near-perpendicular rays which contribute very little to the image. The image will still converge to the same result given enough samples - the approach taken in the second image will just get there much sooner. The effect in both cases is the same: 

Following the comment from John Calsbeek, if my focus on bounding volume hierarchies is misguided, and there is a better space partitioning approach for this situation, please answer accordingly. I'm looking for something that can deal with what I describe, including anything I haven't thought of. 

So provided it suits your purpose to model arbitrary shapes as collections of pixels, you don't need to make any adjustment to the algorithm. Simply feed in a texture that labels all pixels in an arbitrary shaped seed with the same seed number, but different locations. Possibly even weighted seeds, where the distance to a seed is adjusted by a multiplier and/or an adder to give it more or less influence For "weighting on seeds such as multiplicative and additive", the paper only mentions the possibility in passing in section 8, as potential future work. However, this should be straightforward to implement provided your desired weighting can be included in the seed data that is passed from pixel to pixel. The current algorithm passes to specify a seed and its position, and only one seed is stored per pixel at any one time. Extending this to store provides all the information required to weight the distance function and calculate whether the new seed being passed to a pixel is closer to it than the one it is currently storing. You could even include two weights, one multiplicative and one additive, and just set the multiplicative one to 1 and the additive one to 0 when not required. Then your algorithm would include the possibility of being used for multiplicatively weighted seeds, additively weighted seeds, or even a combination of both at once or some of each. This would just need 

This appears in two places so it will need to be fixed in both. is , presumably in world coordinates. appears to be in the local coordinates of the triangle being intersected (one of the triangle vertices being the origin). The distance between two points in different coordinate systems is meaningless, so that in different regions of a triangle it will appear to be behind or in front of another triangle for no immediately apparent reason. If is first converted to world coordinates then the distance will be correct and the intersection chosen will be the closest to the camera. 

The iris (the colourful ring surrounding the pupil of the eye) is covered in a layer of water, and appears to have opaque elements embedded in transparent and translucent elements. What effects do I need to model in order for the iris to appear realistic close up (the iris taking up over 20% of the image area)? Is sub surface scattering necessary or is transparency sufficient? Is it necessary to take into account light from within the eye that entered via the pupil, or can the back of the iris be regarded as completely opaque? Are there other effects that I haven't considered? I'm looking to produce still images offline - the approach does not need to work in real time. So far I've tried covering the eye in a thin transparent film to simulate a water layer, and treating the iris as a section of a transparent ball with coloured transparent radially arranged strands, offset as they are layered front to back. To prevent unwanted effects from light that gets all the way through, I have backed the iris with a matt black sphere section. This still seems to give an eye that looks somewhat artificial (inorganic) and disconnected though. I'm trying to make this work with geometric primitives such as spheres, cones and cylinders, but I'm open to an approach using triangle meshes if that opens up new possibilities. 

I'm looking to use my GPU for non-graphical calculations (artificial life simulations) but ideally I would like to leave this running for weeks at a time, 24 hours a day. Is there anything I should take into account before trying this? Is a GPU subject to overheating or shortening of its lifespan by continuous use? What about the computer in general (it's a laptop in my case, but I'm interested in the differences in case it's worth me getting a desktop computer for this)? Will running the GPU non-stop place any strain on connected parts of the computer? Are there known problems that result from this? 

I'm interested to know whether the demoscene has historically introduced new techniques that would have otherwise taken longer to discover, contributing to the progress of computer graphics. Has it become more or less relevant over the years? I know that some of the early demos took advantage of known hardware errors and went beyond what was considered possible at the time, but what I'm interested in is whether any of the new techniques introduced were then taken up by researchers or professional programmers to become part of the mainstream accepted way of doing things. 

Here I use as a placeholder for the id of the light source, since that part of the code is not included in the question. 

Line segment intersection To check whether any of the edges of a candidate rectangle intersects with any of the edges of the selection rectangle, you can either use a general line segment intersection algorithm and apply it to each pair of candidate edge and selection edge, or you can take advantage of the fact that the selection rectangle is never rotated,as follows: Parallel case If the candidate rectangle has edges parallel to the x and y axes (that is, it is rotated by 0, 90, 180 or 270 degrees), check each of its edges: If the edge is vertical, check if its x position is between the x coordinates of the selection rectangle, and if one of the following holds then there is an intersection: 

Is there a reason that the previous 2 lines need to appear before the line? If not, you can avoid the error by moving this line to the beginning, since the actions of the lines are independent of their order in this instance. The needing to be at the beginning of the file is not a new requirement. I don't know why you were able to run this code previously. 

As Dan Hulme points out in a comment, the variable is not updated each frame. Assuming this code is within a function, it is initialised to the current time once, the first time this function is called. Every time the function is called after that, the keyword prevents the variable from being updated. This variable therefore remains unchanged throughout the animation, always set to the initial time. If this suspected problem is indeed the case, you can correct it by either not using static variables, or by initialising the static variables and then separately updating them in lines that do not contain the keyword. In the current code, I can't think of a way that could already be set to the current time each frame unless there is more code that has not been posted, in which case we will not be able to help you without seeing it. There are more subtle problems with simply multiplying by delta, which will give a different trajectory for different delta sizes, but they may not be relevant depending on the accuracy that you require. I have focused on what appears to be the most significant problem, as nothing else can be tested until that is either addressed or proved to be not a problem. 

Since is not defined, the result of the code is undefined behaviour. In practice, the program is likely to act as if is zero, or for some compilers perhaps a constant arbitrary value, or even a different arbitrary value at each iteration. For your particular compiler it appears to be a constant value, and I strongly suspect this is the cause of the distinctive alignment of lighting angle on all the speckles and the sphere. If there is also another contributing problem it will be easier to analyse it in a separate question once this initial problem has been fixed. It may be worth considering using a compiler and/or flag setting that gives an error or at the very least a warning for undefined variables, as this type of mistake is both very easy to make, and very easy to overlook later. Additional contribution of light source There appears to be another problem that will cause results to be incorrect in a more subtle way, with no obvious distortion. Due to Next Event Estimation, the light source is contributing to each step along the path. This means it should not make a contribution if the path itself hits the light source directly. Otherwise, for that path the light source will be contributing twice. You can correct this by changing: 

Non-parallel case Check each of the edges of the candidate rectangle as follows: First find the points of intersection of the infinite line on which the candidate edge lies, with the infinite lines forming the boundaries of the selection rectangle. If either of the intersections with the y boundaries of the selection rectangle has an x coordinate between the x boundaries of the selection rectangle, then there is an intersection of one of the horizontal line segment edges of the selection rectangle with the candidate infinite line (so potentially with the candidate line segment too). Similarly, if either of the intersections with the x boundaries of the selection rectangle has a y coordinate between the y boundaries of the selection rectangle, then there is an intersection of one of the vertical line segment edges of the selection rectangle with the candidate infinite line (so potentially with the candidate line segment too). To then check if there is an intersection with the candidate line segment, check if one of the following is true: 

I'm trying to implement ambient occlusion in Python 3 and I'm seeing shadows beneath my reflective spheres but they seem very faint. I'm not sure if that means I've missed something, or if I just have a false impression of how much shadow results from ambient occlusion. The result looks like this: 

Cause of the problem with The function returns when an intersection is found, and updates an object called . However, the intersection stored here is not always the closest to the camera. The reason for this is here: 

Different screens can have different pixel geometry, so that the red, green and blue components are arranged in different patterns. Using sub-pixel rendering to give a higher apparent resolution is only possible if the pixel geometry is known (what will give an improvement in clarity on one type of monitor will make things worse on another). This is particularly relevant if an application needs to run on both a desktop/laptop and a mobile screen, as different pixel geometry is quite common in mobile screens. Is there a way to determine which geometry the screen uses at runtime, without having to ask the user? I'm interested in whether this is possible in general, but ideally I'd like to know whether this is possible when using JavaScript with WebGL. 

The long term sum is the same either way, but approach 1 involves many rays with negligible contribution, that still require just as much calculation as any other ray, thus wasting most of the calculation time. 

I don't have any experience with Gradient Domain Path Tracing, but here are my thoughts: There seems to be a different problem If you look carefully at the little spikes of distortion in the final image, you will see that they are all lit from the same direction - on their top left side at a consistent 45 degrees. The sphere also appears to be lit from this angle, rather than from above by the light source. This is unlikely to be explained by an incorrect probability estimation for a path. I would expect there to be a different problem with the code, that these distortions are hinting at. I will therefore address these two separate points: 

This is perfectly possible Although the difference may not especially noticeable, I would expect sampling taking into account the exact pixel geometry to give a slightly more accurate image. You just need to offset your pixel centres per colour component according to the (average) location of the subpixel(s) of that colour. Note that not all pixel layouts have a one to one correspondence between pixels and sub pixels. For example, penTile RGBG has twice as many green subpixels as red and blue, as this image from Wikipedia shows: 

So the short answer is that IIR is optimised for photographs, and RLE is optimised for images with areas of flat colour. The claim seems to be that the only difference is the time taken to calculate the blur, and that the end results are identical. I haven't tested this, so if you need to rely on that claim you should test both and check for any differences in the results. 

Note that even with this speed up, the convergence will still be very slow for scenes with small, bright lights, or if there are parts of the scene accessible only by narrow paths. Generally, any scene where there are regions for which only a narrow range of angles contribute most of the light. 

*Note that the order isn't quite the same as just reversed. In this example code, three random numbers are used for each pair of pixels to be swapped. So the order of each triple must be kept the same to avoid choosing different pairs of pixels. For example: [1, 2, 3], [4, 5, 6], [7, 8, 9] must be reversed to [7, 8, 9], [4, 5, 6], [1, 2, 3] in order that the same x value, y value and direction be chosen for each pixel pair when running in reverse. 

When used in the context of ambient occlusion, the term "ambient lighting" still means background light approximated as being the same in all directions, but the surface is lit based on how many of those directions are not occluded by objects in the scene. This gives subtly varying light levels that give much more realistic images than with a single level of ambient lighting. This gives the effects seen in real life such as the corners of a room being slightly darker than elsewhere. 

A very simple low memory approach If you really want to use as little memory as possible, it can be done with not much more memory than that required to store a single image (the first frame) provided it is acceptable to do some preprocessing in advance. If you copy the following jumbled image, this jsfiddle will take it as input: 

Although simulation models like Boids give good results for bird flocks or fish shoals on a small scale, simulating every single member in real time becomes unrealistic for huge numbers. Is there a way I can model a flock in the distance where only the density of birds is visible? I'd like to have that flowing, changing density gradient with a much smaller number of variables to process. I've tried using a much smaller population and displaying each boid as a blurred area with Gaussian density so that as they overlap the density rises and falls through their interaction. This is reasonably cheap but it never leads to sharp changes in density, either spatially or temporally, which makes it look too uniform. Is there any other way of getting away with a much smaller number of individuals? Or is the only way to get realistic results to prerender? 

I can model ice cubes as slightly misshapen transparent cubes with the refractive index of water, but they don't look convincing. They look like lumps of glass rather than ice. Looking at real ice cubes I can intuitively describe some differences but I don't know what physical properties to change to match them: