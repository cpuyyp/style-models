The way Haskell solves this problem is through Type Classes (see these papers for an overview). The idea is that it is not the $\_^{-1}$ operation that is special; it is the $\mathrm{\bf Array}$ type! To this end, you can define a (statically) overloaded method $map$ which works on each such type. In Haskell: 

There's no agreed upon "bible" for CT for computer science in the same way as for mathematicians (Mac Lane), probably because the field is younger and a bit broader. It really depends on whether you want to understand . Here are a few computer science concepts with category theory counterparts: 

As Andrej has said, the problem is undecidable if you allow replacing one term by another, extensionally equal one. However, you might be interested in optimal sharing of expressions, in the following sense: given the reduction $$(\lambda x:T.C\ x\ x)\ u\rightarrow_\beta C\ u\ u $$ it is clear that the occurrences of the term $u$ can be shared in memory, and every reduction applied to the one can be applied to the other. In this sense, it is known how to reduce untyped terms in an optimal manner, reducing sharing as much as possible. This is explained here: $URL$ and the relevant citation is J. Lamping's An algorithm for optimal lambda calculus reduction. There is little doubt that the theorem for the untyped calculus can be extended to the CIC. An other relevant question is the amount of type information that can be erased when performing type conversion, or indeed how to perform efficient conversion, which is an active field of research, see e.g. Mishra-Linger's thesis. 

I have no answer to your question but propose to consider a more restricted version of $\Pi_M$ for which we can show that it lies in P. Let us only consider families of graphs such that the number of edges grows logarithmically. I will formalize this by rephrasing your problem formulation, also to see if I have understood it correctly. An undirected graph $G$ with $n$ edges can be described by a $\frac{n^2-n}{2}$ long bitstring, simply concatenate the entries of the adjacency matrix of $G$ in the upper triangle. Therefore there are $2^{\frac{n^2-n}{2}}$ possible graphs on $n$ vertices. It follows that any function $f : \mathbb{N} \rightarrow \mathbb{N}$ such that $0 \leq f(n) < 2^{\frac{n^2-n}{2}}$ for all $n$ describes a family of graphs. For any efficiently computable such function $f$ we define $\Pi_f$ as $$ G \in \Pi_f \iff G \text{ is isomorph to the graph described by $f(|V(G)|)$} $$ For a natural number $x$ let $b_1(x)$ be the number of 1's in its binary representation. Now, let us only consider $\Pi_f$ for efficiently computable functions $f$ for which it holds that $$ b_1(f(n)) \in \mathcal{O}(\log n) $$ that is families of graphs for which the number of edges grows only logarithmically, as stated above. We show that $\Pi_f$ for this class of functions is in P. Let $f$ be such a function and $G$ be an input graph with $n$ vertices. Let us call $f(n)$ the reference graph. There are at most $\mathcal{O}(\log n)$ edges in the reference graph. Thus every MCC(maximally connected component) can consist of at most $\mathcal{O}(\log n)$ vertices of which there can be at most $n$. Note, that for any pair of graphs with only $\mathcal{O}(\log n)$ vertices we can trivially check isomorphism in polynomialy time w.r.t. $n$ because we can try all permutations. Thus using a greedy algorithm to assign each MCC of the input graph a MCC in the reference graph we can figure out whether the both graphs are isomorph. 

You can try the Coq user manual, in particular this section is pretty nice (the server seems to be down at the moment though). For meta-theory you can try some recent work of Benjamin Werner et al., see Proof-irrelevant model of CC with predicative induction and judgmental equality and On the strength of proof-irrelevant type theories for the most salient work. 

Alright I'll give it a crack. It seems what you are describing is close to contextual equivalence, which is defined in the following manner $t$ and $t'$ ($t\simeq t'$) are contextually equivalent iff: $$\forall E, E[t]\!\downarrow\ \Leftrightarrow\ E[t']\!\downarrow $$ where $E$ ranges over the set of term contexts (terms with one "hole") and $t\!\downarrow$ means $t$ is normalizing. Now this definition works for all untyped $\lambda$-terms and it is possible to show that if $t$ and $t'$ are normalizing, then $$ t\simeq t'\ \Leftrightarrow\ t=_{\beta\eta}t'$$ Here $\beta$ is the usual $\beta$-conversion and $\eta$ is the equality $$ \lambda x.t\ x = t$$ if $x\not\in \mathrm{FV}(t)$ This shows that $\beta+\eta$ is a rather strong form of equivalence for normalizing terms in the untyped setting. However in general the statement $$ \forall v:T,\ t\ v=_{\beta\eta} t'\ v\qquad (1)$$ in some (normalizing) typed calculus is significatnly stronger than $t=_{\beta\eta}t'$: the first one is usually undecidable, and the second is not. 

Andrej answer covers uses of extraction, but as far as expressiveness goes, I believe that having impredicative Prop leads to a system that is strictly stronger than Agda. In fact "Martin-Löf type theory with universes" is sometimes called "Luo's predicative UTT" One subtle issue is induction-recursion, which gives Agda significant power and seems to be absent in Coq. However there is a trick by V. Capretta (It's likely been independently discovered) which allows expressing such definitions in Coq. 

The statement is unlikely to be true since we can deduce NP-hardness for Graph Isomorphism from it (which would imply a collapse of PH to its second level) as follows: Let SGI be the Subgraph Isomorphism problem which is known to be NP-complete. Then GI is a subset of SGI. Now, take HAMCIRC and define a slightly different version: ISO-HAMCIRC = $\{ (G_1, G_2) | G_1 \text{ is isomorph to } G_2 \text{ and } G_1 \in \text{HAMCIRC} \}$ It is trivial to see that ISO-HAMCIRC is NP-complete as well. It follows that ISO-HAMCIRC $\subseteq$ GI $\subseteq$ SGI and therefore GI would be NP-hard assuming the hypothesis holds. 

What makes a language hard in a computational sense is neither simply that it contains very few words(e.g. is finite) or that it contains a lot of words(e.g. is infinte) but rather an intricate selection of a subset of $\Sigma^*$. I would try to formalize this intuition as the following statement: Let $A,B$ be two languages which are complete w.r.t to some class $\mathcal{C}$. Then for every language $C$ it holds that $$ A \subseteq C \subseteq B \implies C \text{ is complete w.r.t $\mathcal{C}$} $$ What can we say about the truth value of this statement or interesting special cases such as NP. Maybe it implies only lower bounds(hardness)? I would be rather suprised if even the lower bounds of $A$ and $B$ don't apply to $C$. One natural example to explore might be HAMCIRC and HAMPATH. If the hypotheses holds then we could state that any set of graphs $G$ for which every element has a hampath and every graph with a hamcirc is contained in it, it follows that $G$ is NP-complete. 

In the strictest sense, there is no real difference between syntax errors and semantics errors, at least as far as language theory is concerned: the only salient difference is the complexity of the automaton required to recognize that language, with, e.g. 

Note that $U$ must not itself depend on $x$. The intuition behind the equality is that this fact, for every possible $U$ is sufficient to characterize the existential quantifier. For 2), the set theoretic intuition is a bit difficult here, I'm afraid. It's not naively possible to think of types as sets of elements and arrows as the set-theoretic functions between them. However, intuitively, if some intersection $\bigcap_S A(S)$ is non-empty, then $A(\varnothing)$ should be non empty. In your case this gives $$ \left(\bigcap_R \left(T[x:=R]\rightarrow \varnothing\right)\right)\rightarrow\varnothing$$ (note the parenthesization) which needs to be non-empty. But the only way for that is that there is some $R$ such that $T[x:=R]\rightarrow \varnothing$ is empty, which means that $T[x:=R]$ needs to be non-empty. 

I think you have misunderstood the definitions: TERESE (9.1.1): A strategy $S$ for an ARS $\rightarrow$ is a sub-ARS $\rightarrow_S$ of $\rightarrow$ having the same objects and normal forms. This means that 

This is a tough question to answer, since simplicity is in the eye of the beholder. Certainly $\lambda$-encodings have their drawbacks, there are reasons why Coq decided to move away from them, the most obvious one is the inability to introspect the subterms during recursion. The second is the general awkwardness in defining deeply or mutually recursive functions. As you note, the "fix-encoding" avoids this but then it is hard to define non-constant functions (which is the point Damiano brings up)! You have to choose: introspection or recursion. Often we want both. I'm also a bit skeptical of Gabriel's proposal: how does he compute exponentials on "native" s without recursion? In addition, the fact that EAL can only express elementary functions seems intuitively like it should restrict "natural" ways of writing recursions. In particular this shouldn't be well-defined: 

I complete here Peter’s answer with a characterization of physical maps as CPTP maps. As you know, if the system is isolated, the only operations you can implement are the unitary operations. But, as you noticed, if you use an ancillary subsystem and throw a subsystem away, you can implement some other operations. The set of physical operations are the completely positive trace preserving maps. (Following most of the literature, I’ll call them CPTP maps), and Stinespring’s dilation theorem indeed ensures that any CPTP map can be written as the partial trace of a unitary acting on a larger Hilbert space. Since we are looking at non isolated subsystem, its quantum state has to be described by a density matrix $ρ$. In full generality, a density matrix has to be positive ($ρ≥0$) and having a unit trace ($\mathrm{Tr}ρ=1$). Let suppose we have a generic physical map $\mathcal{E}:ρ↦\mathcal{E}(ρ)$. To be physical $\mathcal{E}(ρ)$ needs to be a physical state for any input state $ρ$, that is, we should have $∀ρ:ρ≥0$, $\mathcal{E}(ρ)≥0$ and $\mathrm{Tr}\mathcal{E}(ρ)=\mathrm{Tr}ρ$. In other words, $\mathcal{E}$ has to be positive and trace preserving. However, it is not enough. For example, the transpose $\mathcal{T}$, is a positive trace preserving map, which is unphysical. The idea is the one behind the Peres-Hordecki criterion: suppose you have a channel performing $\mathcal{T}$ on the thystem $A$. You can add another system $B$ of the same dimension, where you perform the identity $\mathcal{I}$ i.e. wher you don’t do anything. The global map on $AB$ is then $\mathcal{T}_A⊗\mathcal{I}_B$, which is not positive any more: some linear algebra shows $\mathcal{T}_A⊗\mathcal{I}_B(Φ_{AB})$ has $d(d-1)/2$ negative eignevalues, where $Φ_{AB}$ is a maximally entangled state. For a map $\mathcal E$ to be physical, any extension of the form $\mathcal E⊗\mathcal I$ has therefore to be positive. This is the definition of complete positivity. The Stinespring dilation theorem then shows that this necessary condition is sufficient, since it gives a recipe to implement any CPTP map with an ancillary subsystem and and a unitary.