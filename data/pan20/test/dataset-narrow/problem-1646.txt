That'll get MediaTomb installed and started. Next you want to make sure that you media is under a directory accessible to the newly created user. I'll assume that your media is under . If this location is a Linux file-system (i.e. not a FAT formatted USB hard drive) then you can do: 

TL;DR: The problem is that you're trying to set the and properties while you're recording. The method you've written starts by setting which is fine (as the documentation states: "The property can be set while recordings or previews are in progress"). However, it then goes on to set which the documentation notes "...no recording must be active when the property is set". I'd guess you want to set the resolution and framerate at the top of your program and not every time the toggle button is hit. Incidentally, the same rule applies to and (and the property that's forthcoming in 1.11) but I don't think there's much else that can't be adjusted while a recording is running. Generally, a good hint for the way to debug these things is to look back through the stack trace and find the first line that exists in code you've written (this assumes that the mistake is in code you've written as opposed to an upstream library, but regardless of the coder's skill that's a decent assumption so it's a reasonable starting point). In this case if we work back from the end of the stack trace we get to: 

Given that latency is king, let's first explore the differences between streaming JPEGs and H.264 and what they mean for latency: When streaming JPEGs, the camera captures a full frame, encodes it, and dumps it over the net. The client now has a full frame to display (and decoding time is negligible) so the latency is down to the capture, encoding, and transmission time of the frame. In practice, capture and encoding are tiny too, so we'll say it's all down to transmission time. When streaming H.264 (or any MPEG-ish format in general) things are a tad more complex but not much so. The first frame output by the encoder will be an I-frame (a full frame), so this case is exactly like JPEG (except H.264 I-frames are better quality for the same number of bits): the latency is down to transmission time of the frame. The next frame output by the encoder will most likely be a P-frame (a "predicted" frame) which can't be decoded without knowledge of the preceding frame. All that really means in practice though is that it's much smaller; again, latency is down to transmission time. What if the prior frame got dropped (you are using UDP after all)? Assuming no re-transmissions, you can't (accurately) decode further frames until the next I-frame. This doesn't exactly affect latency, but in the JPEG scenario it would result in one corrupt frame; in the H.264/MPEG scenario it results in multiple corrupt frames (how many depends on the I-frame period - it's typical to set this quite low when streaming, e.g. once every second or two). So far, both schemes are looking exactly the same for latency (except H.264 suffers more with dropped packets, but provides better quality). When dealing with camera output that is indeed the case: there's no reason H.264 should have any worse latency than JPEG (and given the smaller transmission sizes it should actually be a bit better). So why does streaming H.264 give poor latency with things like this recipe? There's another frame-type we haven't considered in the H.264/MPEG scenario: B-frames ("bi-directional predicted" frames). These can't be decoded without knowledge of prior and subsequent frames. Obviously, in a streaming application this will incur latency as you have to wait for the next frame before you can decode the current one. However, we don't have to worry about that because the Pi's H.264 encoder never produces B-frames (I have heard it's common for most video cameras to only produce I-frames and P-frames, which makes perfect sense when you consider they're encoding live material, not pre-recorded stuff where they can look at the next frame to produce a B-frame). So, on the camera side of things B-frames aren't relevant... But what about the client side? H.264/MPEG players don't know that their source is never going to produce a B-frame. They also assume (not unreasonably) that the network is not reliable and that uninterrupted playback is the primary goal, rather than low latency. Their solution is simple, but effective: buffering. This solves uninterrupted playback and providing space for decoding B-frames based on subsequent frames all in one shot. Unfortunately it also introduces as much latency as there are frames in the buffer! So, your major issue is on the client side: you need to write/find/coerce client software into forgoing buffering to immediately decode and display incoming frames. Sadly, this is where my knowledge runs out. While mplayer can be cajoled into low latency playback by seeking forwards after playback starts, that's a nasty hack and I'm not aware of ways to accomplish this in other software (VLC, HTML5 tags, etc.), or ways to reliably automate it either. Incidentally, this is why I deliberately noted compatibility in the list of requirements at the start. Your problem is less to do with the camera (or even the format selected), and more to do with persuading the client to playback with low latency. Whatever solution you come up with is probably going to involve some specific software (or at the very least configuration) on the client. Or to put it another way: getting this working with a specific client sounds doable to me. Getting this working with generic web browsers on a wide variety of disparate platforms? That would be in the "all too difficult" category (at this time). Links for further reading: 

On the last line of that script, is a numpy array with shape (rows, cols, color-plane) with the color planes in BGR order - which is precisely how OpenCV represents image data. In other words, you can just pass that array straight to OpenCV functions: 

I think it's best to answer this question by giving some insight into how things work a little lower down. First a caveat though: I'm not a firmware expert by any stretch of the imagination; my rather rough understanding of how the Pi camera module works is based on my experience of writing the picamera library and interacting with the much more knowledgeable firmware developers on the Pi forums. If you hear contradictory information from the firmware devs, they're the authority on this, not me! With that out of the way... As soon as the Pi's camera module is initialized it is capturing frames. These frames are (as far as the end user is concerned) dumped but inside the camera's firmware there's a lot more going on. The frames are measured to determine the gain to apply to the sensor (AGC), the white-balance to feed to the AWB correction algorithm, etc. For example, if you start up the camera and immediately start recording you'll typically see the white-balance correct itself over the first few frames of the recording: 

VLC (or mplayer) aren't capable of playing back much on the Pi; they're not optimized for videocore GPU, so they're trying to do everything on the CPU (which is nowhere near powerful enough for video decoding). You can try playing back with omxplayer (which is) but I don't recall whether it's capable of playing back a network stream (probably possible with or something similar). I'll add a note to the docs for 1.7 that VLC/mplayer should be run on a "normal" machine, not the Pi. 

Hmm, evidently that portion of the docs isn't clear enough and needs re-writing for the next version (I should stress you're far from the only one confused by the analysis classes - I've had plenty of questions via e-mail about them). Do let me know if the following makes things any clearer (it'll form the basis for the next version's docs): The analysis classes (, , and ) are all examples of custom outputs. That is to say that instances of them are file-like objects and picamera will treat those instances as if they were file objects (i.e. calling their method with new video data). Now, in the case of the analysis classes (listed above), you don't have to worry about constructing the method, that's already done. The method takes the incoming video data (which is assumed to be in YUV format in the case of , RGB format in the case of , etc.) converts it to a numpy array and then calls the method with that array as the only argument. The method in each case is just a stub which raises . You're expected to override that method and perform whatever analysis you want on the array within it. Now, several things to note: the current docs contain a warning about the analyse method having to be fast. That's not true (and was written before I understood how the underlying firmware operated). The analyse method doesn't have to be fast, and nothing will crash if it's not. All that will happen is that buffers won't be returned to the underlying encoder/output port fast enough so it will drop frames, reducing the effective frame-rate, until sufficient buffers are returned for it to proceed. I'll remove that warning in the next version of the docs. Secondly, you've attempted (correctly) to override the method but your syntax is slightly wrong. Remember that in Python, the variable (implicitly the first parameter of any bound method call) must be explicitly specified in the method's signature. Thirdly, you don't want to construct an instance of directly; it's an abstract class, just there for you to override. You need to construct an instance of your subclass. Finally, you need to call the method with that instance of your subclass instead of a filename. The output from the recording is not going to be written to a file; it's going to be written to your analysis instance (you really don't want to record RGB video data anyway - the bandwidth requirements are enormous!). In other words, what you want will look something like this: 

Evidently UV4L does support stereoscopic mode (as you got it working), but I'm afraid I know nothing about configuring it for this mode. 

This isn't a terribly scientific test, but it may provide a rough idea. I ran the following script on a reasonably fast desktop system, causing it to print the current timestamp including microseconds to the console: 

Obviously you could use some other interface (network or even GPIO based) to talk to the process while it's running, but hopefully that demonstrates the principle! 

Sending data down a pipe to another process in Python is quite simple, especially if your script spawns that other process. First, you to import want the subprocess module. Then you want to use the class in there to spawn the other process, specifying as the value. As you might expect, that'll cause a pipe to be spawned for communication to the other process via its stdin file. Then it's just a matter of using the process' stdin as the recording output. The one thing to bear in mind is that you need to close the stdin pipe once recording is finished; picamera won't do it for you because it didn't open the pipe (you did via Popen), so it won't presume to close it for you: 

It's worth noting though, that the Pi rounds certain resolutions up a bit. The relevant info is in this recipe in the picamera docs. You'll need to adjust the formula above if you use a resolution that involves such rounding. Now, for reading the frames we just need to remember that OpenCV's image format is planar BGR so we need to read a YUV frame, convert it to BGR and pass it to OpenCV. Thankfully the code in that same recipe can help us do that pretty easily (actually it converts YUV to RGB, but we can easily swap the planes in the conversion matrix to make it produce BGR). The following code reads the fourth frame from the file, converts it to BGR and displays it with OpenCV: 

So, why do we refer to camera resolution (or image size generally) in order? I'm not so sure on this one, but I'd guess convention from mathematics. Either way, it's a well established convention (e.g. it's the way PIL and every other image library refers to image size), so I followed it in the construction of picamera's property. However, as noted, PIL (and everything else out there) stores images internally in order. Incidentally, this is also the reason that is at the top of the display rather than the bottom (again, in contradiction to mathematical convention): because that's where the graphics hardware starts drawing from. Why does it start drawing there? Before monitors there were teletypes which effectively print from the "top left" by printing left-to-right and feeding the paper upwards at a line end. The first monitors emulated teletypes and so we wind up with this mathematically topsy-turvy coordinate system (although in practice it's actually much more useful than the mathematical convention given that a lot of the time we're dealing with text). Finally: the is 1 because each element of the array (representing one color plane of one pixel) is a single byte, and therefore has a size of 1. 

As soon as the camera's initialized you should see those threads appear. In fact they're not spawned by the picamera library itself; they're spawned by the MMAL layer and are used for things like control callbacks. Why so many threads? Well, the camera actually has quite a lot of components that spawn by default (the camera itself, a camera info component, a splitter for the video port, a null-sink for the preview port), and each component has at least one thread for its control callbacks. If you start recording a video you'll see another thread appear. That's the thread that gets passed output callbacks from the firmware (when you stop recording, that thread will disappear and the rest will remain). As to why they're all marked "S" (interruptible sleep), that's because most of the threads are just sat there waiting to be woken with a message from the camera firmware. Most of the time (hopefully all the time!) there's no errors so the threads just stay asleep. If you're recording video, the output thread will probably blink into "R" (running) occasionally as it transfers data to the specified output, but otherwise most'll remain in S. Anyway, yes it's perfectly normal for the camera. If you run "raspivid -t 0 -op 128" (which simply brings up the preview transparently until you hit Ctrl+C) you'll see the same thing for raspivid: a whole bunch of PIDs under it with the same name which represent the background threads for the camera.