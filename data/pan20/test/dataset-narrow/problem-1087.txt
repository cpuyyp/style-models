Now I will leave it as an exercise to prove that if $f$ is computable in polynomial time, then $\bar{f}, \hat{f}$ are in P; and if $\bar{f}, \hat{f}$ are in P, then $f$ is computable in polynomial time. You do need the idea you mentioned, that $|f(x)| \leq |x|^c$ for some $c$. 

There seem to be lots of sort-of interesting statements we can make, though their novelty/usefulness is probably dubious. For instance, I think something like this is true: "$P = NP$ if and only if every polytime mapping can be preimaged in polynomial time", (the analogy is that "every continuous function has a continuous inverse"). Similarly, you could probably translate lots of complexity statements into this framework but with again dubious utility. For instance, one-way functions (although I haven't thought about randomized algorithms...that could be quite interesting). Final notes: I think the most interesting thing about this framework is that it seems not to relativize. One might of course think about other interesting complexity classes than $P$ and $NP$; one might also think of e.g. $DTIME(g(n))$ for a specific $g$; I'm not sure if that's interesting. 

Use Stirling's approximation to get $H(k/n) = \frac{\log(p_{n,k})}{n} + 1 + \text{something}$. Rewrite the original sum to get $\mathbb{E}[H(k/n)] = 1 - \frac{H(Binom(n,0.5))}{n} + \sum \text{something}$. Get that the entropy of the binomial, divided by $n$, plus the sum of "something", equals $\frac{\log(\sqrt{e})}{n} + O(1/n^2)$. 

We are usually only motivated by the second and third cases, so the VC-dimension term in your bound captures what we care about. For example in $X=\mathbb{R}^d$, you probably want to choose a hypothesis class that depends on $d$, for instance halfspaces in $\mathbb{R}^d$, and so $d$ shows up in the VC-dimension term (in the case of halfspaces, this is $d+1$). There are also some examples of classes with finite VC-dimension that are still interesting. Here even if $X$ is "large" or complex, we can get a small bound. 

For a fixed game and fixed $n$, whether for "small enough" $\epsilon$ the above condition holds (all $\epsilon$-equilibria are close to Nash equilibria). Perhaps the same question essentially, but whether the condition holds if differences in payoffs are bounded by a constant as $n \to \infty$. 

The size of a subtree at height $k$ is $2^k-1$. The probability of #1 is therefore $\left(\frac{2^k-1}{2^h - 1}\right)^2$. The probability of #2 is $2 \left(\frac{2^{k-1}-1}{2^h - 1}\right)^2$. The probability of the event that #1 happens and #2 does not (since #2 is a subset of #1) is thus \begin{align} \Pr[\text{$w$ at height $k$ is lca}] &= \frac{\left(2^k-1\right)^2 - 2\left(2^{k-1}-1\right)^2}{\left(2^h - 1\right)^2} \\ &=\frac{2^{2k-1} - 1}{\left(2^h -1 \right)^2} \end{align} once you work it out. (You can check that these probabilities sum to $1$.) There are $2^{h-k}$ nodes at height $k$. So the probability that the lca is at height $k$ is $\frac{2^{h+k-1} - 2^{h-k}}{\left(2^h - 1\right)^2}$. The expected height is thus \begin{align} &\sum_{k=1}^h (k)\frac{2^{h+k-1} - 2^{h-k}}{\left(2^h - 1\right)^2} \\ &\approx \sum_{k=1}^h \frac{(k)\left(2^{k-1} - 2^{-k}\right)}{2^h} \\ &\approx h - 1 - \Theta\left(\frac{1}{2^h}\right) . \end{align} Simulations bear this out up to three decimal points anyway. By the way, this is very easy to simulate: Suppose the nodes are numbered $1,2,\dots, 2^h - 1$. Pick two random numbers from this range. Now each node $x$'s parent is $\lfloor \frac{x}{2} \rfloor$. To find the lca of two numbers, continue halving (and rounding down) the larger number until they are equal. To find the height of a number, continue halving (and rounding down) until it is one and count the steps. So if your random number generator is comfortable giving you random numbers in the range $[1,2^{100}]$ (as python's seems to be) then you can simulate easily. 

Sketch. Consider two cases: $p_i < \epsilon^2/16$ and $p_i \geq \epsilon^2/16$. In the first case, the number of samples of $i$ will not exceed $X/8$ from either distribution: The mean number of samples is $< X/16$ and a tail bound says that with probability $e^{-\Omega(X/p_i)} = \epsilon^2 e^{-\Omega(M/p_i)}$, $i$'s samples do not exceed their mean by an additive $X/16$; if we are careful to keep the value $p_i$ in the tail bound, we can union bound over them no matter how many such points there are (intuitively, the bound decreases exponentially in the number of possible points). In the case $p_i \geq \epsilon^2/16$, we can use a Chernoff bound: It says that, when we take $m$ samples and a point is drawn with probability $p$, the probability of differing from its mean $pm$ by $c \sqrt{pm}$ is at most $e^{-\Omega((c\sqrt{pm})^2/pm)} = e^{-\Omega(c^2)}$. Here, let $c = \frac{\sqrt{X}}{16}$, so the probability is bounded by $e^{-\Omega(X)} = \epsilon^2 e^{-\Omega(M)}$. So with probability $1-\epsilon^2e^{-\Omega(M)}$, (for both distributions) the number of samples of $i$ is within $\sqrt{p_i\frac{X}{\epsilon^2}}\frac{\sqrt{X}}{16}$ of its mean $p_i\frac{X}{\epsilon^2}$. Thus, our test will not catch these points (they are very close to each other), and we can union bound over all $16/\epsilon^2$ of them. $\square$ 

This is not quite the same as the usual generalized-second-price-auction setting because you are assuming the items are identical, whereas in a GSP setting there is an order on the items (slot A is better than slot B is better than ...). But anyway, your auction is not dominant-strategy-incentive compatible. Suppose the bidders' true valuations are $v_1 > v_2 > \dots > v_k > \dots > v_n$, and suppose every bidder reports her valuation truthfully. The first bidder wins an item and pays $v_2$. If she had bid $v_k + \epsilon$, she would have still won an item, but only paid $v_k$. So she would have preferred to lie and bid $v_k + \epsilon$. 

Edit 2014-03-19 After reading the reference in Rahul's answer, it seems more reasonable to think in terms of $\ell_1$ distances between distributions rather than convergent sequences. So I'll try to rephrase the questions and also put some recent thoughts. 

(Edited from previous version, 2014-04-08.) I believe that the answer is $\epsilon_n \to \log(\sqrt{e}) \approx 0.7213475...$ where the logarithm is base 2. This seems to match simulation results. I don't have a full formal proof, but give the heuristic approximations/calculations. I think it's easier to note that your question is equivalent to: 

I think that in any equilibrium, the row player plays only C. But now if the column player uses a no-regret algorithm, every sample will either be action D or E, and the row player's best response will always be either A or B. So the row player will never play C in the entire empirical history. 

I've idly entertained the idea of an online arxiv overlay "journal" of negative results. The idea would be to allow negative results or counterexamples that don't seem suitable for a full publication in traditional venues. It would be lightly reviewed, so that we can guarantee that every paper has at least been looked over and approved by an expert in the area. It wouldn't necessarily be archival or "official", but it would at least be citable. I think such a resource could be very helpful for publicizing dead ends in some lines of inquiry as well as "folklore negative results" that may not be widely known. I can think of a few examples from my own experience. A main challenge would be in finding and connecting with reviewers, due to the variety of possible research areas involved. If there seems to be enough interest in this idea from the cstheory community, particularly people saying they'd be willing to submit and to review, then I'd be willing to take a lead in organizing. I'm pretty sure I can find some colleagues willing to join and help organize for the first few years. 

Proof. Now, again as Thomas points out, we have $$ H(X|Y,Z) = p\cdot H(X|Y, X \neq Y) . $$ Now, by plugging in to $(*)$, we have the inequality $$ H(X|Y) - p\cdot H(X|Y, X\neq Y) \leq H(p) . ~~~~~~~~ (**) $$ and we can make this tight for any $p$. Let $H(X|Y) = p\cdot H(X|Y) + (1-p)H(X|Y)$ and rearrange: \begin{align} H(X|Y) - H(X|Y, X\neq Y) &\leq \frac{H(p) - (1-p)H(X|Y)}{p} \\ &= \log\frac{1}{p} + \frac{1-p}{p}\left[\log\frac{1}{1-p} ~ - ~ H(X|Y) \right] . \end{align} and, again, we can make this tight using the examples from before (since we have only renamed things and rearranged the inequality). $\square$ 

This question gets more directly at representation and time required to reduce one problem to another.... The main answer I have in mind is an Integer/Linear Programming oracle. The decision version of that problem is NP-complete. There is a trivial "reduction" from linear programming because it is a special case. But an oracle for linear programming alone (let alone ILP) speeds up many problems that are immediately solvable by linear programming. They can be reduced to it in linear time by rewriting the problem as an LP. For instance, shortest paths and other flow problems, matchings. But I don't think ILP is the only one by any means, it's probably more that people haven't thought much about e.g. reducing shortest-path to TSP or so on. 

The key idea will be that the value we're interested in, $$ \mathbb{E}[H(k/n)] = \sum_{k=0}^n \frac{{n \choose k}}{2^n} H\left(\frac{k}{n}\right) , $$ can be related entropy of the binomial distribution. Let $p_{n,k} = \frac{{n \choose k}}{2^n}$ be the probability of $k$ heads in $n$ fair coin flips. The steps will be as follows: 

I think there is no paper solving that exact problem, but "Online Vertex-Weighted Matching" by Aggarwal, Goel, Karande, and Mehta (2011) is very close. If I understood correctly, they solve your problem only with all capacities equal to one. My best guess is that you will have to do some work to extend their guarantees and algorithm to your setting. On the other hand, a greedy algorithm probably should guarantee 0.5 of optimal by a standard argument: Consider the opt matching; for every "slot" in OPT left unmatched by greedy, there was some other match made that was more valuable (since the slot was available, its partner must have gone elsewhere). I hope you will forgive this discussion of methods contrary to your request. (edit: I want to mention that this argument wouldn't work if the weights are on the arriving vertices; it's good to think about why.) 

This is not a full answer and I am not very experienced in neural nets, but perhaps helpful. NNs essentially are given an input and produce a response. They are then trained via practice to produce similar responses on "similar" inputs in the domain, for example, the same label to images of the same animal, or high ratings to "good" chess positions where good means high winning chances. So as I commented, neural nets are a non-uniform model of computation that work in a totally different way than step-by-step algorithms run on Turing Machines. Instead, think of them as "soft" circuits that use continuous math rather than Boolean and can be tweaked or trained, and are allowed to err. 

Not sure how much of an answer this is, I'm just indulging in some rumination. Question 1 could be equally asked about P $\neq$ NP and with a similar answer -- the techniques/ideas used to prove the result would be the big breakthrough more so than the conclusion itself. For Question 2 I want to share some background and a thought. Pretty much all the techniques and ideas we have for BPP=P, as far as I'm aware, go via "derandomization": Given any probabilistic polytime Turing Machine, construct a PRG to feed it a bunch of deterministically chosen bits instead of random ones, such that its behavior is very similar to its behavior on truly random bits. So with good enough pseudorandom generators, we get BPP=P. (Goldreich's "World of BPP=P" gives evidence that any proof of BPP=P must equate to this.) This is pretty much along the lines of BPP $\subseteq$ P/poly, except there, the PRG is the advice string which is produced by magic. Perhaps the best answer to your Question 2 is that in P we have no magic and must come up with the advice string ourselves. Derandomization is also the idea behind the 2004 result SL=L, using tools like expander graphs. Now consider what such a proof would imply for just one particular algorithm, the Miller-Rabin primality test. It would show the existence of some deterministic generator that picks out a sequence of integers to feed to the Miller-Rabin primality test such that, if and only if all the integers passed, then the original number was prime. As I understand it (though I am no expert), the question of whether such a list exists and how small the numbers in it can be (in particular if it sufficices to check all the numbers below some bound) seems quite a deep question in number theory and is closely related to proving forms of the generalized Riemann Hypothesis. See this question. I don't think there's a formal implication here, but it doesn't seem like something we expect to get next week as an accidental miniature corollary of a much more general PRG construction. 

First, to be precise in your problem statement, we probably are promised that $A$ has at least one index $i$ with $A[i] = 1$. We want to show that no randomized algorithm can guarantee better than $\Omega(n)$ expected number of queries. In other words, we want to show that for any randomized algorithm, on its worst-case input, it requires $\Omega(n)$ queries. By the minimax principle, the expected number of steps required by any randomized algorithm on its worst-case input is lower-bounded by the following: for any distribution $D$ on inputs, the expected number of steps of the best deterministic algorithm against that distribution. Make sure that the preceding paragraph makes sense. Any randomized algorithm against its worst-case input has poorer performance than any randomized input distribution against its best-case deterministic algorithm. So now we have two steps: First, construct a distribution $D$ on inputs; and second, show that the best deterministic algorithm takes $\Omega(n)$ steps in expectation against that distribution. The nice thing about the argument is that any such $D$ will give you some lower bound. For instance, the distribution on $A$ where $A = (1,0,0,0,...)$ every time is a distribution on inputs. But, the best deterministic algorithm against this distribution just checks the first coordinate first, and always finds a $1$ there, so this gives a lower bound of $1$ query -- pretty terrible. Another distribution would have $A = (1,0,0,...)$ with probability $0.5$ and $A = (0,1,0,0,...)$ with probability $0.5$. Here, the best any deterministic algorithm can do is either check the first position first and then the second, or check the second and then the first. In either case, the deterministic algorithm finds a $1$ with an expected number of queries $0.5\cdot 1 + 0.5\cdot 2 = 1.5$ (because half the time it finds it on the first try and half the time it finds it on the second try). Still not great. Now, can you think of a better distribution $A$ to use? 

I think the question is not yet quite precise, but let me give a couple ways it could be made precise and an answer for each. 

Here is a writeup following up on Tsuyoshi's pointer to use DP. Given $p(x)$, we can write any (decent, deterministic) algorithm as a binary decision tree with nodes weighted by $p(x)$. The root of the tree is the first node the algorithm selects to test; the left child of each $x$ is the next node to test if $f(x)$ is zero and the right node is the next to test if $f(x) = 1$. An algorithm's worst-case performance on weights $p(x)$ is its weighted depth: the maximum over any path to a leaf of the sum of the path's weights. Therefore the optimal deterministic algorithm minimizes this value. The "minimum maximum length" or "cost" of a path starting at node $x$ is $p(x)$ plus the cost of the tree on the nodes remaining to be searched. The nodes remaining to be searched will always be an interval. This gives the DP relation: