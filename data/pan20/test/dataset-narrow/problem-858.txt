How are you confirm what headers are being sent when you access the / URL? Can you try this curl command? Something similar to this but with your URL: 

I always run these commands from a terminal first to see what's going on instead of trying to debug them in Nagios. It's just easier and you can typically provide switches to the plugins that make them more verbose. I'll usually become the user, whatever that username is, when doing this. 

There are 3 possible ways to solve this issue: 1. Add read only permission in the root of the site collection. 2. Deactivate the feature “ViewFormPagesLockDown”: 

NOTE: The 2nd one is nice because it let's you mix and match different sizes of HDDs into the RAID. Maximum size of RAID? I don't see why you'd be limited. The only limiting factors are the number of physical drives you can fit in the Dell 2900 (space plus open SATA ports), and the speed of the SATA bus (3Gb/s vs. 6Gb/s). Currently I don't think there are any 6Gb/s RAID controllers that the 2900 supports so that doesn't matter here. When adding the disks if they're > 2TB you have to set them up using GPT not MBR! Resources 

So the are part of the Python language. If you search for them you'll eventually find this page from the Python language's specification, specifically this section titled: 5.6.2. String Formatting Operations. There is an example on this page: 

For starters you can designate each site with a particular port 8080, 8081, etc. and then use something like autossh to maintain the connections. Then you'd be connected and would just need to go from 8080 to 8081 check each site that you've mapped to a designed local port. This type of approach could be scaled up so that instead of you looking at each sites' local port, you could have a script do the checking instead. Depending on what's on each site' Icinga site, you could periodically check it with your own Nagios/Icinga site that's been configured to check for the various things you've been checking for manually. It's really hard to provide more info without more specifics. 

I don't think there is any way to control upstart from the command-line. I did not see anything beyond on CentOS 6.x, while googling for this. You're only option appears to be to edit the upstart config files. 

Look in Apache's config files. The main file is located here: There should also be a bunch of files under here: . One of them in the 2nd location may even be called or some such. Look through these files and you'll see a section that is telling Apache to look in the . Probably something like this: 

EDIT #1 Turns out that the above command was valid, I had a typo in my path. If you have a path which includes spaces you can provide them to robocopy's /log switch like so: 

NOTE: That last bit is to handle the case where you run the above multiple times against the same servers. This will eliminate your pubkey from getting appended multiple times. Notice the single ticks! Also pay special attention to the fact that all these commands are nested inside of single quotes. That's important, since we don't want to get evaluated until after it's executing on the remote server. 

You'd then need to create a corresponding file under called, . You could also forgo doing this and simply override the file itself in the directory. Simply copy the pre-existing file to . NOTE: Be careful in doing the 2nd options, since you'll be overriding the for any jail that may use it. 

The above lines say connections coming into go to (RewriteRule) and that connections going back out from the proxied server get rewritten to (ProxyPassReverse). 

It sounds like a switch failing to me. You can use the app to scan your network or a single device and it will show you some additional information about the manufacturer of the NICs you're scanning. 

This was of course on a smaller HDD (1TB) so as was mentioned in the comments, anything over 2TB will require a different label, and yes you should be using GPT for that. 

Take care to make sure that the socket file has the appropriate level of access to perform the above. Mainly something like this: 

You don't say but if you're using Apache you can use the mod_substitute module to accomplish this. Check out the Apache documentation and also this example. The steps will be roughly: 

Rather than type your password multiple times you can make use of and its switch to prompt for it once, and then feed the password to all the servers in a list. NOTE: Using this method doesn't allow you to use , however, so you'll need to roll your own method for appending your SSH pub key file to your remote account's file. Example Here's an example that does the job: 

With that many drives I would use RAID 60. With RAID 10 you're vulnerable in different ways depending on which drive(s) fail.    In this scenario if you lose the 2 disks that share a common block (D1, D2, etc.) you're hosed. For your situation given you have 16 drives you have basically 8 groups of disks. So you can lose 1 disk from each group and still be fine. If you lose 2 disks that are part of the same group (mirror) then you're out of luck. References 

You can confirm this by ssh'ing into "remoteserver" just like you did originally and try and a file in this directory: 

Here are 2 ways to do it. Method #1: Using a "meta" tag You could just stick a file at the top of www.example.com that redirects to www.example.com/blog. This file would make use of the "meta" tags that are available in the tag: For example 

Using the shell you're options are limited to getting the list of files and then downloading them one at a time using a command like this one: 

Is this ? If so I got burned by this one as well. Check out the official CentOS 6 FAQ. The issue is that is now responsible for the device and by default it's disabled. During installation you can configure so that it will start up automatically, but the default is that it doesn't. If you need to configure this after you've setup the system you can follow the steps in the FAQ which tell you exactly how to setup the . The nuts of the fix are that you need to edit this configuration file, , and make sure that the line is enabled, i.e., . Here's a couple of screenshots of where to configure this during the CentOS installation. I totally missed it the first couple of times. 

Where 192.168.1.1 is my KVM headless server and myvm1 is one of my guest KVM instances. Alternatively you can remotely run virt-manager from the headless system and tunnel it through ssh with a command like this: 

One thing worth checking is what version of Jetty you're using. Depending on your version, there is a known issue with certain versions where accessing the server with URLs that do not include a trailing slash "/", will encounter the 400 error code as you've described. See here for more on this particular problem. Here's a link to the actual issue filed on Jetty's bug tracker, JETTY-1553. 

This seems to be an issue with apache2 on Ubuntu. I found several threads on other sites where the solution was to set ServerName to localhost. 

I think the keystore aliases need to be the same when you're loading the .crt files back into your keystore file. I'm doing essentially the same thing you are using digicert certificates and here's the process I used to generate my .csr and incorporate the results back. Also I find the .p7b version of the certs easier to deal with then the individual .crt files. This may help you as well. generate key pair and keystore file 

This is from a CentOS 5.x system but the technique is still apt. BTW, it can get a little confusing because in CentOS' case the files are organized physically here: 

By setting the xgid bit on the directory, you're forcing what ever group owns, said directory, to be propagated any time new files are created within it. This approach will only work for newly created files, it won't, for example, enforce the group ownership if someone were to move files into this directory from somewhere else on the system. However, given this is an SFTP server application, I can't think of any way a user would be able to move files into this directory, outside of SFTP. 

The above could be used to extract a list of IPs from my file. You can of course also use to generate dynamic content too: 

To only invoke the restart if a specific file's been updated, you can use a on your or tasks to save the state of whether the file was updated. The results of this can then be incorporated as part of the on the task. 

Ironically does not possess a method for performing a "dry-runbanaction` that's defined for a specific jail by overriding it for a given jail or do override the actions themselves. In newer versions of you can create overrides quite easily by making identically named files with the suffix instead of . You also have access to the and sub-directories where you can put overriding configuration files for a specific jail so that you can redefine values for variables that were previously set, such as . So say your or had this in it: 

There are some potential solutions you could try in this AWS developers forum. $URL$ For example: potential fix #1 

You could then grep the output looking for and files if you knew you were using those extensions to name your VM images. 

If you take a look at the rules that are included with you'll notice that they use these variables to make things neater and more parameterized. For example in the included they've used them to make general action rules that they can then use when defining the various jails. Example Here are some basic variables at the top. 

Notice that the entire contains a loop. So what does this all mean? I would say that if you have a heavily trafficked network with a lot of hosts and your DHCP leases are relatively short, then you may want to consider running 2 instances of dchrelay. If however, your network is relatively small and your DHCP lease timeouts are relatively long, then running a single instance should be fine. Some additional things to consider 

I just confirmed that this output is in fact from the tool. It's available from the the following website, $URL$ but I believe it to be available in most Linux repositories as a standard offering. It was available in the EPEL repository, for example, for CentOS and Fedora Linux distros. is just a script which makes connections to a MySQL DB and collects information such as and for example and then prints it out in a nicer format. There are the following resources available: 

Might I suggest using perlbrew. In general I've found over the years that if you have an application that depends on a particular interpreter: Ruby, Perl, Python, etc. It's usually a better idea to setup a dedicated installation of the interpreter for your application rather than rely on the ones included in a particular distro. Perlbrew maintains an entire installation of Perl in your directory. In fact you can have multiple versions of Perl along with its libraries so that you can perform testing, before you fully upgrade from one version to the next. By doing it this way your application is completely separated from upgrades that may occur when relying on your distro's version of Perl. Excerpt from the perlbrew webpage: 

I realize you're looking for a GUI way to do this but you can get the MAC Address from the command line like so: 

If you look in the man page for the CLI tool there are some examples which show how to accomplish this, though not directly using the subcommand. man page 

I maintain a single name server but have several sub-domains so I use the $ORIGIN trick mentioned in the O'Reilly DNS & Bind book referenced here. /var/named/chroot/var/named/data/db.192.168.1 

Also make sure that you do not have any firewall rules blocking this access, specifically you should have rules on the chain for . 

Here are a couple of websites that offer RAID calculators. I use these to get a rough idea of how much space I'll have with various RAID configurations. NOTE: Remember that this is the total space available after portions of it have been substracted out for use by the RAID. You'll still be losing more depending on which filesystem you lay on top of the RAID. 1. icc-usa RAID Calculator 

The certificates are associated to a specific hostname. So if you try and access your site with just it's IP address the certificate won't match. You can use a service such as SSL Shopper's SSL Checker to see what hostname is associated with a given certificate. example              

A co-worker ran into a issue with getty/multi-user systemd targets not starting. When he connected to the console there was a prompt that he had to acknowledge/answer a question so that things could proceed past which allowed for getty/multi-user to start. This is a screenshot from the prompt:        This issue is referenced here as well: “License not accepted” when CentOS 7 initiate. The EULA that you need to agree to can be automatically accepted via kickstart as described here - kickstart with eula - finish configuration. 

Some certificate authorities provide what's called a wildcard certificate. I think you're looking for the features provided by that. For example on the digicert website you can see this type of certificate: 

We have a xtm21-w and according to the docs in drop-in mode all the interfaces are on the same network. From the watchguard online docs 

I think all these answers aren't really answering the question. The root level can be determined by running the command . This will show you what options the Apache daemon was built with at compile time. This is what controls where determines where to look for it's config. files and .so modules by default. For example: