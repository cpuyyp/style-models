Tonight our server decided to install SQL Server 2014 CU7 for SP1. The update appears in the Windows Update history and I found downtime in our logs for it. The previous patch level was CU3. I was surprised by this because I am not used to Windows or SQL Server automatically installing CU's. I also never witnessed a service pack being installed automatically. What has changed? According to sqlserverbuilds.blogspot.de the CU7 was released today. It was installed right away after release. I am not categorically against this but I would like to understand what policies are driving the automatic update choices. Strangely, tonight was not a "patch Tuesday". No other updates were processed. The server did not reboot, either. I'm certain this was not a manual action. So far my plan was to keep the server at the CU3 level and upgrade to 2016 eventually. The new CU being installed caused unnecessary downtime and introduced risk which I normally was not willing to take. In particular I do not wish to install patches at a random time. So what's the automatic update policy and should I do something about that? 

I'm building up a data warehouse to be used by SSAS to create cubes on, and I'm debating between two possible schemas. In my warehouse, I've got two different fact tables that tracking daily changes in dollar values. Each entity in these fact tables have an underlying Sales Order and Line to which they relate. These SOs and Lines then have other related dimensions, such as customer, product, etc. About 12 sub-dimensions total so far. My question is if I should be rolling all these sub dimensions up directly into the fact tables, or if I should use a little snowflaking in my warehouse, and have them branching off the Sales Order and Lines dimension instead. The first option obviously follows a star-schema model better. However, if changes are made such as adding additional dimensions, it becomes more maintenance, basically having to do the ETL twice for each fact table, rather than just the once on the SO dimension. As well, if a new fact is added that relates to Sales Orders, I'd have to go through the whole process again. As this is my first DW/OLAP project, I'm not familiar on where the line should be drawn on snowflaking, and other people's thoughts would be highly appreciated. 

Is this feature documented and reliable? The only official mention of it is on the page but it's buried in a code sample. This suggest that it might be intended to be undocumented. There are remarkably few Google matches for this feature as well. 

The documentation calls out the behavior of and so I understand that part. But the operator seems to behave under a more complex set of rules. 

When you say in T-SQL and and are of a string type (e.g. or ): How is the resulting type determined? I found no clear rules for this when experimenting. It seems to depend on whether literals are involved. Also, how is the length determined? I found out that the type of is . Apparently, the length of the literal is being tracked. Here's a little experiment that I ran: 

Pull first probe row. Complete the operation if no row available (short-circuit). Match all probe rows against it. 

Watching queries execute with Live Query Statistics I noticed that it seems SQL Server is lazily constructing a hash table from the build input of a hash join. This is a meaningful difference in the case of 0 probe rows. It potentially saves the entire build side tree. I always thought a hash ran like this: 

The more I learn about about the great things you can do with mysql, the more I keep pushing to learn. I have two tables: Tests 

What I need to do is count the number of tests a user is signed into and are not complete. It must include all Tests they have not resigned from. (This is easy). BUT it must also include all the Tests they have resigned from and another User has not taken their place. (This is the tricky bit). Another user will have been deemed to have taken the place of the User in question if they have an entry in TestUsers with the same TestNumber and the same UserSlot (note both), and they either have not resigned themselves (ResignedTimestamp='1970-01-01 00:00:01') or they have resigned but their ResignedTimestamp is later. Here is my SQL. 

I have changed the password to 'abc' according to this link: $URL$ I have done exactly what is written here, even using localhost, not localhost:8080 in the things to type. There was a warning at one point, but it does not tell me what. It was at the point (I am version 5.7.9): 

On my dev box I sometimes need to run very IO intensive queries such as index builds and . This can put so much load on the disk that it hardly can process anything else. This causes enormous lagging in other programs. For that reason I sometimes need to suspend using Process Explorer. If I do that for longer periods of time I sometimes get IO timeouts such as 

I used the column to find indexes/partitions that have ghost records. I found a few that have one such record. I then tried to queue them up for processing by scanning all pages: 

I then waited at least 10 seconds to let the ghost cleanup task run. But the ghost records do not disappear. I also tried restarting the server. No ghost related trace flags are in use. This is not an actual problem that I'm having. I'm trying to understand ghost cleanup in general. Why do the counters not drop to zero in tables that have no writes? 

I'm not sure how to conclusively test which way it is. I'm not sure the Live Query Statistics output can be trusted in this way. Does anyone know how this works? 

With that complete, I added the duration unit dimension and the conversion rate measure group to my cube, and updated my total duration measure with a measure expression ([total duration] / [conversion rate]). And it's at this point I'm stuck. Because there isn't a bridge between the conversion rate table and my fact table (ie. the exchange rate date like in Adv.Wrks), I can't define the relationship in the dimension usage tab. As well, the lack of a time dimension is preventing me from using any of the semi-additive measures adventure works uses (AverageOfChildren, LastNonEmpty) on the conversion rate measure. Anyone know how I can get this to work? I've been searching and trying things, and can't seem to come up with a solution for what I thought would be a common problem. 

As you can see, my product numbers get reused and I need to be able to see what the product was at the time the order was placed. Easy enough to do with SQL, but I'm having trouble trying to build a cube in SSAS that takes it into consideration. Any help would be appreciated. 

I thought User should be or something like that. ORIGINAL QUESTION: When I try to log into my phpMyAdmin on localhost:8080 I continually get: 

However, a single row did change. So when I try to log in again it still says when I put in the user id and password . Something has obviously gone wrong, but what? In it has the following (after the password change to abc): 

I have another open question which at the moment seems to be a bit tricky to answer: phpMyAdmin - Cannot login even when changed password I am hoping someone can still answer it. I think one of the config files or ini files is wrong in some way. So I need to ask this question as a back up. I have a few databases in my present install of Wamp server on Windows 10. Can I reinstall, or do a repair-install, on this Wamp server without overwriting or deleting my databases? 

The Rank will only change about once a day. However, the page will likely be viewed maybe 1000 times a day. So instead of getting the database to calculate the rank for each of the 1000 visits, I thought it would be best to just fill the 'Rank' column once. How can I include an UPDATE to fill the Rank column, using my chosen way to rank people. (Note if two people are ranked 5, then the next rank is 7.) 

Row versioning maintains 14 bytes of internal versioning information for each row. But is this really a cost per row or is this a cost that also applies to each index on the table? It seems the 14 bytes must be added to all index records as well so that index-only scans (and other index-only accesses) can see the versioning information and perceive a point in time snapshot of the data. All information that I could find on the web only talks about a per row overhead of 14 bytes, though. 

I'd like to keep rows with the same together in the result set. The groups themselves should be ordered randomly, though. is supposed to be the secondary sort criterion. Like this: 

I learned that SQL Server can return the value of the (single) identity column of a table using the pseudo-column : 

This sometimes causes the query to abort. How can I disable IO timeouts on my dev box? Also, in case the query does not abort I sometimes get dump files. How can I disable those? 

does not order groups randomly. is totally random. comes to mind but I need a new random order each time. How can this be done? 

We are told to watch out for things being set to null (we dont want) by using the IN clause in the following: 

EDIT: Since posting this I have read some more and some people suggest changing within this file, but I do not seem to have this setting. Is this what is causing this problem? 

EDIT: After looking at the info I tried (where abc is my temporary password and mystuff is my database just below root): 

It gave the following (note I am localhost:8080 and not just localhost - could that make a difference? 

The result counts row (7, 25, 5, 1970-01-01 00:00:01) eight times, and adds to that (8, 25, 2, 2016-04-23 12:00:00) once. So it is giving me an answer of 9 when the answer should just be 1. 

The following SELECT with INNER JOINS works, but does not count it right. I have a tests table: Tests 

Surely this is possible? It does not like it but does really say why. It says: 'You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FROM (SELECT TestNumber, UserSlot ...' 

So I've been playing with this for the last day or so, and haven't been able to get an closer. I have a SSAS cube that has a measure group which displays total execution times for events in seconds. What I'm looking to do is give our users the option to select which time unit they want to display their results in (ie. minutes, hours, days, weeks, etc.). On the surface, it seems similar to performing currency conversion. So using AdventureWorks as my template, I created 2 additional tables. 

So this may be the wrong place, and may seem pretty basic, but I'm new to building up cubes, so please forgive me. Imagine a setup described by the following: 

I'm currently developing SSIS packages which include transferring data between two servers. The servers are in the same local LAN, but I'm remote to them. Is there any way that I can develop the packages on my local workstation and get the advantages of the visual debugger, etc. while having the actual data processing only happen on my remote SSIS server, saving me from having the data transferred over the WAN and back?