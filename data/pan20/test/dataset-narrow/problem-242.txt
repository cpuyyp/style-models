But best takeaway for doing that- test with more data - always makes sure you have data consistent with production and expected future production. Query plans start looking data when you give more rows to the tables, and try and keep the distribution what you'd expect in production. And pay attention to things like including Order By or not, here I don't think it makes a terrible bit of difference in the end, but still worth digging into. Your approach of comparing this level of detail and data is a good one. Subtree costs are arbitrary and meaningless mostly, but still worth at least looking at for comparison between edits/changes or even between queries. Looking at the time statistics and the IO are quite important, as is looking at the plan for anything that feels out of place for the size of the data you are working with and what you are trying to do. 

Update. I can see my answer being a bit unclear updated it to make it more clear *Short Answer: No * You don't need to check compatibility, yes it is something that can be applied to any database. It will be compatible, even if it is an 80(SQL Server 2000) compatibility mode database, you can enable Checksum, and likely should. This is transparent in that your tables don't look or act differently, your apps don't know about it, etc. As for the "database knowing" it depends on how you mean it. From the app connecting? No. But the SQL Server storage engine that interacts with the files knows it, so it does the checks on write, but again that is transparent to your app and users. One more consideration: You are correct, only pages changed after it takes effect is a good thing to keep in mind. Doing index rebuilds at some point after doing this is one good way to touch a whole lot of pages, and not a bad idea. And watch performance/CPU overhead. There is a "cost" associated with Checksum, though that cost isn't as high as you might fear. This article, though written for 2005, talks about some of those concerns, but again it is rare to have an issue from this there. 

The short answer is Yes. If you can do it in an AG, and you can do it in SQL Server standard - you should be able to do it in a Basic Availability Group. The only gotcha was the distributor which wasn't able to be on an AG - the Replication couldn't work with the listener right - until the releases described in your links. So you would basically have three AGs. Each with one DB. The distributor would be in one. The Subscriber in another. The Publisher in another. You have to have your distributor on a separate SQL Server instance. So this would be mean two instances participating in a Basic Availability Group for the distributor, and then two for the Publisher and, most likely, two for the subscriber. Because of the limitations in the Basic Availability Group. You could have more than on AG on an instance and have one DB in each and that would be good. but your limit here is 1.) You are doing replication, likely for a reason so your publisher and subscriber would likely be on different instances. 2.) The distributor has to be separate from publisher and subscriber. 

I suspect this is not a SQL Server 2000 database you are attempting to attach to SQL Server 2000. That appears to be a common cause of the error as indicated here also. Can you attach this to SQL Server 2005 or 2008 Express and see what error message you get? Do you have access to the underlying database that your files are from? I imagine if you attach this to SQL Server 2005 or 2008 you will be able to attach it unless there are other impediments to attach going on that that approach will reveal. 

I believe what the documentation author is getting at there is likely the problem your first question deals with. If you make "Domain\JoeBlow" the owner of the job, and that user is just has the correct least privilege for, say, an accounting user, then that login would certainly not have sufficient permissions to do what the job needs to do - the job fails in that case. 

Ensure all services that connect to SQL Server are off/stoped (this includes services like SSIS, SQL VSS writer, etc) Restart SQL Server in single user mode using the -m startup parameter (described here) Connect as the single user and then do your CHECKDB with the TABLOCK specified. 

Using multiple file groups is a great approach/answer for certain workloads. Separating TempDB is usually a best practice though I'd never put TempDB on RAID 5 typically. And if the environment is pushing no IO? Having no IO waits? I've seen and placed environments all one one big happy set of drives and things have been fine. It really depends on a lot. I'd spend some time profiling your workload. Look at the PAL tool and run some perfmon scripts on your existing environment. Look at your SQL Server File Statistics. Read about some best practices in SQL Server IO, and make informed decisions and monitor to see how they look as you go. 

Because of your code - Are you using TempDB a lot in your code purposefully? A lot of temp tables and table variables created and destroyed? Doing a lot of things in TempDB like this? That isn't bad or good necessarily, but you might look at that and understand your intentional TempDB usage pattern. TempDB is a shared workhorse - TempDB is one database that is used as a temporary space for user defined temporary objects and various work tables and operations used by your entire SQL instance. How many user DBs are there? What kind of workload do you see in general? TempDB is one resource for all things to share. Inefficient queries and insufficient memory - Perhaps there are queries that aren't using indexes tightly enough or are doing large scan and sort operations. Large hash operations, and the memory on the server isn't sufficient for these. These operations will "spill" to TempDB as worktables behind the scenes. Sometimes this can be avoided with looking at your query plans and indexing or query tuning. Sometimes it happens (more so on warehouse workloads, I find). If you have enough memory, this can help, but these queries can still spill at times. Look a this as well. Are you using Read Committed Snapshot Isolation level with a fair number of updates in your system? This can also result in increased TempDB activity. 

Figure out why you are doing DML live on a table you are trying to read from live and stop doing that or come up with a better process of offlining those reads someplace else. This is what I am suspecting. Try and find your blocking SQL (Try using SP_Whoisactive and catch a block - what is the blocking session_id? what is it doing? Look at that and see if that is really your issue). Deal with the true blocking cause. If this really expected behavior. You'll have to sort out some other way to grab data to a copy of the table. Lots of approaches here. Streaming it to two tables, ETL, Readable AG secondary, Replication, etc. But be warned - all of these techniques will involve a SCH-S lock. In short - the ONLY operation that should block a SCH-S held lock would be a SCH-M lock. So the only way this would be happening would be someone is modifying a table's STRUCTURE whilst you are querying the table's data. 

The SQL Server service account is ostensibly the account which will be creating the file. It isn't your account. But the service account. That said, sometimes I've seen issue where UAC can cause issues and you need to run as Administrator. If you use the SQL Server Configuration manager to change service account the necessary permissions should be granted but it could be you are trying to write to a drive not originally configured or that has bad permissions changed. Should be the service account, though. SQL Server service, not agent. 

etc. There are quite a few database level and connection information level Dynamic Management Views exposed to you in Azure SQL DB. 

This seems to be a pretty exhaustive list in your question. Service Packs are generally speaking safe affairs - but every so often things can go wrong. Ideally you would run on a test instance and make sure things are good. Since you can't do that - making sure you take backups is key. Stopping the SQL Server agent isn't necessary - but if you have many jobs and are trying to make sure a job doesn't kick off just before you start, etc - that is a good plan. Again not necessary, though. I also like to read the notes on the CUs released after a Service Pack and look for details about potential issues. It doesn't happen all the time, but sometimes a SP ends up breaking something and that then gets fixed in a later CU. In your case you are going with a fairly late release because Microsoft hasn't been generally releasing CUs - only security updates for that product with the current release now 3 versions higher. You can read the list of versions and builds and service packs and fixes post service packs here. 

Greenstone Walker is right. If you are granted SA rights, there really isn't much to do there to prevent it. I like that you are worrying about protecting from your ability to make such a mistake. I've seen people burned by an "oops" restore before. Not pretty. There are potentially a few things you can do, though. Your milage may vary but some ideas to get you thinking: 

Maintenance plans are certainly one way to perform database backups and a multitude of customers use them with success each day. My guess here, looking at the inclusion of the shrink job and the reorganize followed by rebuild that the maintenance plan was just quickly created without understanding the tasks. When you say you have a "backup database" task - what type of backup is it? My assumption in this answer is that you are taking Full Backups and then Taking Differentials and possibly Log backups. The problem likely isn't your backup, per se (though I suggest you spend some time understanding recovery models and the backup command at books online as you are likely not creating your backups in the most optimal order for your recovery intent). I believe you are likely having an issue because you are restoring in this order: 1.) Restore the full database backup (.bak) first keeping all the defaults 2.) Attempt to restore the differential or the log backup. Or you are just attempting to restore the log or differential on top of an existing database. If this is the case - the issue is understanding how recovery models work and the proper restore sequence. (One more link to read - about transaction log restores) but in a nutshell: If you plan on restoring any differentials or log backups, you have to first restore the full backup AND you must specify "" when restoring that full bakup. If you keep the default () then SQL Server performs the necessary recovery processes to make your database operational (basically rolls forward transactions that were done and consistent at the time of the backup, rolls back ones that weren't). If you want to restore additional backups (only the latest differential or each log file between the last full or differential backup and the time you want to restore to) then you must specify for each restore until the last one where you specify finally. I'd definitely spend some significant time with the Microsoft Books Online links I sent here and click on the links they refer to as well to best understand backup and recovery.