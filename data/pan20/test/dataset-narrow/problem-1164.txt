If $|x| \le k$, then $H(x) \overset{\text{def}}{=} G(x)$. If $|x| > k$, let $L$ and $R$ be the $(|x|-k)$-bit prefix and $k$-bit suffix of $x$, respectively. That is, $x = L + R$ and $|R|=k$. If $R = H(L)$ (where $H(L)$ is computed recursively), then $H(x) \overset{\text{def}}{=} R$; otherwise, $H(x) \overset{\text{def}}{=} G(x)$. 

In game 0, adversary A is given oracle access to a random permutation $\pi$. In game 1, the random permutation $\pi$ is replaced with a "stateful" random function, which preserves the "permutation" property. (In Bellare-Rogaway terminology, this is called "lazy sampling"). Finally, in game 2, the stateful random function is replaced with a "stateless" (of "forgetful") one. 

Do we have complexity classes with respect to, say, average-case complexity? For instance, is there a (named) complexity class for problems which take expected polynomial time to decide? Another question considers the best case complexity, exemplified below: Is there a class of (natural) problems whose decision requires at least exponential time? To clarify, consider some EXP-complete language $L$. Obviously, not all instances of $L$ require exponential time: There are instances which can be decided even in polynomial time. So, the best case complexity of $L$ is not exponential time. EDIT: Since several ambiguities arose, I want to try to clarify it even more. By "best case" complexity, I mean a complexity class whose problems' complexity is lower bounded by some function. For instance, define BestE as the class of languages which cannot be decided in time less than a some linear exponential. Symbolically, let $M$ denote an arbitrary Turing machine, and $c$, $n_0$, and $n$ be natural numbers: $L \in \mathbf{BestE} \Leftrightarrow$ $\quad (\exists c)(\forall M)[(L(M) = L) \Rightarrow (\exists {n_0})(\forall n > {n_0})(\forall x \in {\{0,1\}^n})[T(M(x)) \ge {2^{c|x|}}]]$ where $T(M(x))$ denotes the times it takes before $M$ halts on input $x$. I accept that defining such class of problems is very odd, since we are requiring that, every Turing machine $M$, regardless of its power, cannot decide the language in time less than some linear exponential. Yet notice that the polynomial-time counterpart (BestP) is natural, since every Turing machine requires time $|x|$ to at least read its input. PS: Maybe, instead of quantifying as "for all Turing machine $M$," we have to limit it to some pre-specified class of Turing machines, such as polynomial-time Turing machines. That way, we can define classes like $\mathbf{Best(n^2)}$, which is the class of languages requiring at least quadratic time to be decided on polynomial-time Turing machines. PS2: One can also consider the circuit-complexity counterpart, in which we consider the least circuit size/depth to decide a language. 

As usual, to have an idea of what is happening in a field, have a look at the major conferences: SIGMOD/PODS, VLDB, ICDT, to name a few. 

To answer in the large, the fact that the syntax of programming languages is not context-free is not news (see e.g. Floyd, 1962). To answer more precisely, in a context of scannerless parsing like yours, a way to implement maximal munch is to employ so-called follow restrictions (van den Brand et al., 2002) by forbidding some language to follow a given rule. In your example, you could write a restriction forbidding an $a$ after the token $X$. Provided your forbidden languages are regular, these restrictions can be compiled back into the grammar (however in van den Brand et al.'s formalism, the forbidden language can be context-free and $X$ can be any nonterminal, and this clearly leads to an undecidable emptiness problem). Another formalism for scannerless parsing is that of parsing expression grammars (Ford, 2004), which has a greedy, maximal-munch type semantics, and an undecidable emptiness problem. Now, it doesn't look like your specific maximal munch semantics would allow you to reduce from these undecidable problems. For a start, it seems to me that, rather than emptiness of the generated language, a tool should more broadly warn the user about any case where a token might "eat" a (non-empty) prefix of its follow language, i.e. whenever $(L_T\cdot\mathrm{Pref}_+(L_R))\cap L_T\neq\emptyset$, regardless of whether this prefix is mandatory of not. This would capture your $aba^\ast b^\ast ab$ example if I understand correctly how you would tokenize it. To conclude, here is an attempt to formalize your notion of maximal-munch-caused emptiness: let $\langle N,T,P,S\rangle$ be a context-free grammar with nonterminal alphabet $N$, terminal alphabet $T$, production set $P$ and axiom $S$. Each terminal symbol $X\in T$ is associated with a regular language $L_X\subseteq\Sigma^\ast$ used for its tokenization. For every occurrence of a terminal symbol $X$ in some production $A\to \alpha X\beta$ of your grammar where $\alpha,\beta$ are sequences of mixed terminals and nonterminals, construct the follow language $L_{\beta,A}$ of this particular occurrence (this is a context-free language) and consider the residual language of the token $L^{\text{max-munch}}_X=(L_X^{-1}\cdot L_X)\cap\Sigma^+$, which is a regular language of strings that will be "eaten up" by the maximal munch semantics. In fact the language $L^{\text{max-munch}}_X$ is the language one would put in the follow restriction for $X$. Then, if $$L_{\beta,A}\subseteq L^{\text{max-munch}}_X\cdot\Sigma^\ast\;,$$ or equivalently $L_{\beta,A}\cap (\Sigma^\ast\backslash(L^{\text{max-munch}}_X\cdot\Sigma^\ast))=\emptyset$, any string allowed to follow this occurrence of $X$ will be "eaten", and the language of the rule $A\to\alpha X\beta$ is empty. Using the classical algorithm for emptiness checking with this extra twist for handling terminal symbols might solve your emptiness problem. 

Description of Some Terms While obvious, I'm going to define several terms here. Let $f \colon \mathbb{N} \to \mathbb{N}$ be a function. Then: 

Number Theory I found several books frequently cited in many papers. They are excellent on the subject, but some of them are quite old. Here's a list of what I recall: 

PS: An idea which only satisfies the second requirement is called winnowing. It uses a sliding window over the bytes, computes some fingerprint in each window, and then uses another sliding window (the "winnowing" window) over fingerprints, and then selects the boundary to be the byte corresponding to the fingerprint which satisfies some property in each winnowing window. Edit: If no such algorithm exists, we can think of some relaxations. The one that best fits my case changes requirement 1 as follows: Let p(n) be the fraction of partitions of length n. We want p(n) to follow some bell-shaped distribution with mean $\mu$ and standard deviation $\sigma$. This way, I can reduce $\sigma$ to concentrate the partition lengths around $\mu$. 

If possible, find a survey, book, or a lecture note. This will help a lot to see the jungle through the trees! To find books, Google Books is your friend. Search the keywords in the Google Books, and try to find a relevant book on the subject. Take the top-down / bottom-up approach, and alternate. That is, read the most recent papers first (the top research), and references thereof (bottom). Why? Because the top research is likely to be better, and has probably surveyed bottom works and improved them. However, to understand top, it is usually the case that you need to skim the bottom works. Important: Don't get stuck at the bottom. After reviewing older works, take the bottom-up approach. Finally, alternate between the top-down and bottom-up approaches. Don't read a paper unless you are somehow sure that it is really relevant. The relevancy depends on several factors: The amount of citations, the fame of authors, the conference/journal it is published in, and so on. Be sure to read the title and abstract carefully, and skim the sections. Decide if you should ever read this paper now, and to what extent. If a paper has a conference version and a journal (full) version, select the first one. The conference version usually gives the general idea, and avoids cumbersome proofs. Read the full version only when you really need to learn the proof details/technique. 

I'm posting my comment as an answer, at the request of the OP. Arithmetic Hierarchy AH is a class of decision problems defined as below: Let $Δ_0 = \Sigma_0 = \Pi_0 = R$. Then for $i>0$, let 

An $(s,f)$- balanced separator in a graph $G$ is a set $S$ of $s$ vertices removing which yields connected compoennts of size at most $f|V|$. If the vertices of $S$ form a cycle of length $s$, $S$ is said to be a cycle-separator. 

For a graph $G$, let $\rho(G) = \frac{2|E(G)|}{|V(G)|}$ be the average degree of a vertex. We say that a class of graphs $\mathcal{C}$ is maximally $\rho$-sparse - written as $\rho = \rho(\mathcal{C})$ i.e. the maximum of average density (with maximum taken over all graphs from $\mathcal{C}$ with sufficiently many vertices) approaches $\rho$. Note: Planar graphs can be proved to be biparted because Euler formula holds. The same is the case with bounded genus graphs. What about bounded tree-width graphs, graphs excluding a finite set of fixed minors,...? 

Notice that I do not care about $f$ as long as it is a constant but $s$ (as a fraction of $\sqrt{n}$) is crucial for me. Some background: Lipton-Tarjan gave a $(c\sqrt{n},\frac{2}{3})$-balanced planar (vertex) separator for some constant $c$. Subsequently Miller gave a similar separator that is also a cycle but in triangulated planar graphs. If we omit the triangulated condition (say replace it by $2$-vertex connected) there might not exist an $(O(\sqrt{n}),O(1))$-cycle separator - case in point being a cycle on $n$ vertices. We can choose to ignore such easy (e.g. all bounded tree-width) cases. 

From the algorithmic proof of Lipton-Tarjan's planar separator theorem we can, in time linear in the size of the graph, find a partition of vertices of the graph into three sets $A,B,S$ such that there are no edges with one endpoint in $A$ and the other in $B$, $S$ has size bounded by $O(\sqrt{n})$ and both $A,B$ have sizes upper bounded by $\frac{2}{3}$ of the number of vertices. Notice that any triangle in the graph either lies entirely inside $A$ or entirely inside $B$ or uses at least one vertex of $S$ with the other two vertices from $A \cup S$ or both from $B \cup S$. Thus it suffices to count the number of triangles in the graph on $S$ and the neighbours of $S$ in $A$ (and similarly for $B$). Notice that $S$ and its $A$-neighbours induce a $k$-outer planar graph (the said graph is a subgraph of a planar graph of diameter $4$). Thus counting the number of triangles in such a graph can be done directly by dynamic programming or by an application of Courcelle's theorem (I know for sure that such a counting version exists in the Logspace world by Elberfeld et al and am guessing that it also exists in the linear time world) since forming an undirected triangle is an $\mathsf{MSO}_1$ property and since a bounded width tree decomposition is easy to obtain from an embedded $k$-outer planar graph. Thus we have reduced the problem to a pair of problems which are each a constant fraction smaller at the expense of a linear time procedure. Notice that the procedure can be extended to find the count of the number of instances of any fixed connected graph inside an input graph in $O(n\log{n})$ time. 

What you are looking for with "checking edges" are known as Petri nets with read arcs. In your question you also mention nets with a single read edge, those are called communication-free. It would be difficult (for me) to point to the original author(s) for these notions, but they are fairly standard. Relationships between communication-free Petri nets and Parikh images of context-free languages are investigated for instance by Javier Esparza, Petri Nets, Commutative Context-Free Grammars, and Basic Parallel Processes, FCT 1995, LNCS 965, pp. 221--232, and Fundamenta Informatica 31(1):13--25, 1997. 

Recall that, in the case of finite state automata, the notion of a minimal automaton is usually meant for deterministic automata only; you can define it for non-deterministic ones, but then you lose two important properties: 

Two more constructions: Brzozowski-McCluskey aka state elimination [1], and Gaussian elimination in a system of equations using Arden's Lemma. The best source on these is probably Jacques Sakarovitch's book [2]. [1] J. Brzozowski, E. McCluskey Jr., Signal ﬂow graph techniques for sequential circuit state diagrams, IEEE Transactions on Electronic Computers EC-12 (1963) 67–76. [2] J. Sakarovitch, Elements of Automata Theory. Cambridge University Press, 2009. 

One construction not listed on the SPOT website is given in a survey by Demri & Gastin, Specification and Verification using Temporal Logics, 2009. The construction is simple and yields reasonably small automata, so it can be carried out by hand for small formulæ, which is good for teaching (which is how I use it), but might also be helpful for debugging an implementation. I wouldn't bet on it being more efficient than the one used by SPOT though. About minimization, there is no canonical minimal Büchi automaton for a given $\omega$-regular language. In order to get smaller automata, one can quotient the automaton by some simulation relation. A classical paper on the subject is by Etessami, Wilke & Schuller, Fair Simulation Relations, Parity Games, and State Space Reduction for Büchi Automata, SIAM Journal on Computing 34: 1159--1175, 2005. 

A single alternation in Presburger arithmetic is enough to obtain exponential lower bounds, more precisely formulae as in the question with $m=1$ and $n$ not fixed suffice (Grädel 1989). 

About Q2: A regular grammar is a "one-sided linear" context-free grammar, where at most one nonterminal appears in any rule right-part, and where that nonterminal is at the last (in right linear grammars) or first (in left linear grammars) position. Such grammars are easily translated into equivalent finite-state automata (roughly by considering each nonterminal as a state), which are unambiguous iff the regular grammar is unambiguous. The class of unambiguous regular grammars and unambiguous automata has been studied in particular by Stearns and Hunt (1985), who show that they enjoy tractable algorithms for the inclusion problem.