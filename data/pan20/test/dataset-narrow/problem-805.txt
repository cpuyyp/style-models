I installed virtualmin and played around with the DNS settings. Now when I try to start bind9 I get the error: 

I'm in the process of recovering my server's files in a "Rescue" mode. Its a Ubuntu 12.04 with 2x3TB in RAID1. The instructions were to SSH into the server in "Rescue" mode, mount the partitions, chroot into and recover my files. I also have to change my root password after ing. Problem: I cant seem to mount the partition for which is where most of my files are at. When I tried mounting and 

I guess we really should worry about redis's memory usage getting higher over time! How can we troubleshoot this? 

I am trying to use pgAdmin on Windows to connect to 9.1.8 running on localhost's Ubuntu 12.04 VM. The host's port 5432 forwards to VM's port 5432. pgAdmin Error: 

I have an Ubuntu 12.04 server and the DNS looksups appear to be extremely slow. takes 40 seconds and takes 70ms. I reinstalled another dedicated server with Ubuntu 12.04 and it has the same problem too. Question: I have been asked to check my DNS configuration. So I added google's DNS servers but I am still getting slow DNS resolves. What else should I check for? /etc/resolve/conf 

I have a webserver (Ubuntu 12.04 with LAMP) using Virtualmin / Webmin. Because I just moved from a Cpanel system, I am having a nightmare configuring the DNS! Using intoDNS.com, the failed reports are: 

Later shows that the number of files remain constant, and shows that IO usage is now dominated by and . 

Problem: When I access the website via its IP address, by looking at the access/error logs, it appears that nginx is serving off the of . How does nginx decide which to serve? Can we force it to serve off the of when the site is visited using its ip address? conf 

Did it run the command then kill the screen? I would like the screen to be detached after running the command instead of killing it. How can that be done? 

I have no idea how they were hacked. What are the recommendations to reduce the chance of losing my servers again? 

I increased PostgreSQL's shared buffer (among other settings) to 4096M and now PostgreSQL fails to start, giving the error message below. Should I change the kernel's parameter to 4096M? The system has 16GB of RAM. How should this be done? What should I change to? I want the changes to be permanent and persists after reboots. 

However when we need to have 2 transparent HAProxy in front of our balanced servers (for redundancy), it seems like this wont work as we can only set one gateway for our balanced servers. What will be the correct way to setup the system such that we can have 2 transparent HAProxy infront of the balanced servers? The main reason for having transparent proxies is the need to find the client's IP addresses over TCP. 

On Ubuntu 14.04, an app communicates with clients at intervals of about 1 per sec to 1 per min. However the service have to be restarted regularly and this causes all the connections to drop/timeout. It takes only about 10-15sec for the service to restart. Is it possible to tune the system such that these connections do not timeout/drop when the service is restarted? 

It was noticed that the memory usage shot up tremendously at around 50k connections. is set to . Question: Is the amount of memory usage unusually high? How can we reduce the memory consumption? I've also read the following from another question, is it relavent? How should I check that the TCP settings are sufficient (for persistent TCP connections) so as not to cause a huge increase in memory usage? 

contains the files that powers a website, and contains many tiny media files like images in subdirectories named after their date of creation. Deleting/pruning an entire subdirectory of many tiny files hogs the IO and slows the server to a crawl for hours. 

and this network security group is assigned to the VM. Additionally, returns only the private ip address on the interface and the local loopback. There is no interface found for the public ip address . How can we fixed this problem? Is something setup wrongly? 

There is an Ubuntu 16.04 server running a service/daemon that listens only to 127.0.0.1. A second Ubuntu 16.04 server needs to query this service/daemon found only on the first server. Is there a way to do this? Will an SSH tunnel work? 

Problem: However, sometimes this step will shoot up to 2000+ msec, and Solr continues reporting 0-30msec . This makes me suspect that there may be a problem in the connections between PHP/Apache and Solr. How can I determine if this is true, or make this connection better? 

What are the roles and used for in Tomcat6? The role appears to give me access to . Which page does the role give me? In the file, the description for role is pretty vague. What is the ? 

When I installed pdo-pgsql, the extension was installed into and thus not automatically loaded. In , I have already defined. Snippet of 

Update eventually crashed due to out of memeory. and both hits 100% then is killed along with other services. From : 

When I perform an or , I notice that it can take almost a minute when connecting to a repo. For example, when doing a , it will be stuck for about a minute at 

I am trying to install Nominatim from its source on github. PostgreSQL 9.1 and PostGIS 2 has already been installed using . The system is running CentOS 6.3 with cPanel. 

I've just installed a Ubuntu 12.04 server and nginx 1.2.7, removed from sites-enabled and added my own file into and symlink at . Then restarted nginx. Problem: However going to the URL does not load the site. and both returns no results! also returns nothing. A from another server returns the correct ip address so it shouldn't be a DNS problem. I was able to connect to apache which I have now stopped its service. nginx logs also shows nothing. How should I troubleshoot this problem? /etc/nginx/site-available/mysite.com 

On a Ubuntu 14.04 x64 server, Haproxy uses 3.3 GB of memory and 6.8 GB of swap, while handling 52k connections. The CPU usage also keeps spiking to 100% before most of the traffic was redirected to another haproxy box. Traffic is mainly peresistent TCP connections. 

Restarted CSF using but the port remains open. Aren't all ports blocked by default except for those defined in , , , ? Why is port 25 still open? csf.conf 

I am new to linux and is trying to install PostGIS2 after successfully installing PostgreSQL 9.1. The machine is running CentOS 6.3 and has cPanel installed. Problem: When I tried installing PostGIS using yum: , I get the dependency error below. How should I solve this dependency problem and install PostGIS? Thank you so much! 

Updates squid.conf Default bloated config file is used. cache.log Nothing seems to be added here on failed 

I have already added apache2 user to the group , why is it still unable to write files to the directory unless I ? 

Next I created a Virtual server using that server template. This is what I've done but its still not working! Any ideas? I've been stuck for days, thank you for all your help! service bind9 status 

Question #1: If we run the puppet script again, won't the wget and echo be run twice? We will end up with duplicate repo in . Running twice doesn't attempt to install the package twice, it simply ensures that its installed. Question #2: Doing there are several promopts for user input. Will Puppet somehow know the default input to use, or will it crash? 

How can gzip be turned off for a particular location and all its sub-directories? My main site is at and I want to turn gzip off for both and . is turned on in . I tried turning off as shown below, but the Response Headers in Chrome's dev tools shows that . How should gzip/output buffering be disabled properly? Attempt: 

However I cannot find at , but only and , which I am not sure if they are the same. So I changed the lines in Step 5 to: 

Update Restarting (not reloading) haproxy lowered the CPU load to 30%. What could have caused the high CPU load previously? 

Problem: I believe disabling ext4 journaling can speed up the deletion, is that true? If so, is it advisable for the main website files at to have journaling disabled as well, since they both reside in the same ext4 filesystem . 

We can configure HAProxy to be a transparent proxy by using the guide here, where one of the steps says 

I just started trying out an Ubuntu 16.04 VM on Azure. A node.js app requires listening to a port 3000 on the network interface assigned with the public IP address so remote clients can connect to it over the internet. However, the node.js app is throwing the error 

I made a mistake in a and all the files are copied long with its full path. All the files that I copied are at 

Problem: This is when the error occurs as shown below. Any suggestions/idea how this can be fixed? Thanks!!! 

Hi have a LAMP stack connecting to a Solr 3.6.1 server running on CentOS 6.3. While benchmarking my app, I noticed sudden spikes in the time taken for PHP to send a search query to Solr and getting the results back. This time is usually about 20-40 msec and Solr's is typically 0-30 msec. My benchmark results: 

Step-by-Step of my Attempt In my domain registrar (Namecheap), I registered as a nameserver, pointing to the IP address of my web server which is running . The domain is setup with DNS and . is a secondary DNS server (SLAVE and pointing to the IP address of my web server) Webserver domain: Webserver hostname: Webserver IP: Under Virtualmin, I edited the default Virtual server template, 

I'm trying to use HAProxy as a fully transparent proxy using TPROXY in Ubuntu 14.04. HAProxy will be setup on the first server with and . The single balanced server has and as well. is the public facing network interface while is for the private network which both servers are in. Problem: I'm able to connect to the balanced server's port 1234 directly (via ) but am not able to reach the balanced server via Haproxy port 1234 (which redirects to 1234 via ). Am I missing out something in this configuration? Removing the line from , the haproxy works but not transparently. 

Few minutes later, shows the number of files dropping once more. Can anyone explain this behavior? Is this normal or is there something wrong with my server? Also, how can 3 processes all have 99.99% IO usage at the same time? 

I have been having problem installing for use with . This is the guide that I am following. According to the guide's Step 5, 

I'm using Ubuntu 12.04 with 2 x 2TB SATA3 harddisks in software RAID1. The filesystem is ext4 with . When using to delete a large number of files in a directory, doing at intervals show that: Initially a large number of files get deleted, and shows the operation taking most of the IO. 

On a Ubuntu 14.04 running Haproxy, after a , Haproxy is suddenly reporting all servers behind it as down. After some digging around, I noticed that ping is not working properly, sometimes it's able to ping successfully, then seconds later we get the error . It's also not able to resolve . does not show any rules in place. does not help. Any ideas? 

Can Squid3 use PHP helper programs other than for ACL and redirect? I need Squid to use a PHP program (with mysql access) to set the parent proxy that should be used for a particular connection, so the program should be able to set values for etc for that connection. I may have the entire approach wrong, please correct me if that's so 

I just installed tomcat6 on Ubuntu using and on CentOs using . When I visit , I get 2 different looking page. Not that it really matters if just the visuals, but did I miss out a package or setting that causes this difference? CentOS 

I'm using Digital Ocean's Ubuntu 14.04 image and needs the kernel to be compiled for . Is there a way to check whether the current kernel supports ? 

How can be used to start a new session and run a command within that session without killing that session? 

I am running a very intensive MySQL query and noticed that only 1 core is being used to 100% while the other cores are idle. The system is running Ubuntu 12.04 and MySQL 5.5.28 Is there a way to force MySQL to use all cores? 

Why did PECL install pdo-pgsql into the 2nd extension directory and not the first? Is it recommended to have 2 as shown in the 2nd code snippet above? 

I have set up Wordpress and MariaDB on a Digital Ocean droplet. Both docker containers use data volumes ( and ) to persist data on the physical host. Before I go experimenting on the site that may destroy my setup, how can I take a snapshot of the server setup or docker volumes so I can revert to them is something went wrong? Can I simply make copies of and and copy them back to their original locations to revert back to the working state? 

A redis server v2.8.4 is running on a Ubuntu 14.04 VPS with 8 GB RAM and 16 GB swap space (on SSDs). However shows that alone is taking up of memory! I do not think the database is this large, so why is it taking up so much memory? 

Haproxy is currently balancing the load between 2 servers, an . Clients are connecting to these servers via persistent TCP connections. Question: We want to upgrade from server to a more powerful server . How can we add a 3rd server and remove without disrupting/disconnecting the client? If is used to redirect connections from to , will it still work if we shut down at some point, or is required to stay online to continue redirecting existing clients to ?