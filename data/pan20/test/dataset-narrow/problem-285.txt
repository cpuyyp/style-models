I work on a relatively small development team that works with an Oracle (11g) database. It's recently been requested that all the developers be given the role so that we can utilize SQL Tuning Advisor when developing complex queries. Some have raised concerns that this may have significant performance and/or security implications, but I have not been able to find concrete answers to what these implications are. If this role were to be given to the various members of my team, what are the major pitfalls we should be watching out for? I've included the tag for Oracle 12c as well, since we'll be upgrading to that in the near future. If there's a significant difference between the two, I'd appreciate it if it was at least pointed out. 

to get the distinct program names, but I'm interesting in how often particular groups of applications are used in a given session. So essentially, how often are these three applications grouped together. Is there a way that can get me results in the form of 

I work on a team that has an SSRS setup with roughly ~1300 (and growing) canned reports. As one can imagine, this has presented problems when a breaking change is introduced to a table. Finding all the reports that may touch a table/field is error prone at best. I'm trying to programmatically build a dependency model. Getting the schema for and how views/tables relate is fairly trivial. What I'm struggling with is how to get a resolved list of fields from a given query. I can pull the query from the RDL files, but interpreting those files is far less so. Queries can contain aliases for fields and tables alike, plus there are problems such as . I'm trying to avoid a regexp hack, and I really don't want to write a SQL interpreter... My initial thought was to loop through the RDL files and parse the explain plan output. While technically possible, this doesn't give me a complete list of resolved fields. Is there any method in which I can use to get the DB analyse a given query and return a list of ? I don't mind having to do some text processing to pull out the results if necessary. 

When using the autotrace, query A had a cost increase of 30% and Query B of nearly 40%. Obviously, I should be using query b in both cases, but I don't understand what causes them to differ. 

If you have a bunch of random fields I would set that up as an . The table would include a reference key to the original account, a field that specifies what type of attribute it is, and the value of the attribute. Alternatively, you could have them each in a series of tables. If each country has a different style zip code, for example, a zip code table might not be a bad idea. Just make sure you have a field. 

An added bonus, is that it also works around the COUNT issue you experienced earlier. Check the execution plans with large data sets, and you will see that this one is easy to index, and will typically perform better than the COUNT technique. The reason is that EXISTS doesn't need to complete the operation - once a row is found, the predicate is TRUE and there is no need to keep processing. With the COUNT, all aggregates have to be calculated. Hope that helps~! 

Look at the and you will see the difference. Oracle uses a cost based optimizer that takes into account the actual values that are used in the query to come up with the optimal execution plan. I have not tested it in Oracle, but it is most likely that is evaluated at compile/optimize time, and the optimizer is aware that your predicate only covers one day, and therefor it will make sense to use an index for example. When you 'hide' the value in a function, the optimizer has no way to tell what value you use, as the function is only evaluated at execute time. If the general density of the column suggests that the average value will return many rows, and will benefit from a full table scan for example, you will get a full scan even for 1 day's worth of data, which may be less optimal for the value you are using. As a rule of thumb - don't 'hide' predicate values from the optimizer by placing them in functions. 

Depends on what data you store in it. If you store a null or an empty string, it will take around 2 bytes. If you use all 50 characters, it will take around 102 bytes. See the documentation here. 

Check out this article. I think it will answer your questions and give you the tools to deal with over-inflated number of auto-statistics. The author suggests dropping ALL auto-created stats occasionally, as the one that are required will be recreated when the first query that needs it is executed. It explains the risks / benefits. 

Example 1: You shouldn't care. The process is much more involved than it seems, and I don't think you have any guarantee. The 3 commands go into the queue in order. The first one is pulled and executed, when the second is attempted, the engine may choose to put it back in queue if it waits for more than some threshold, and the third may be pulled and attempted. if it happens that the resources are now free, the 3rd will be executed first. if not, it may go back in queue again. Example 2: I think the above explains it, and the answer to both your questions is no AFAIK, unless you develop an execution buffer of your own. Don't. Rationale: Now you got to the core challenge, which should really have been the title - "How to deal with deadlocks". You assumed that your 'global lock table' solution is the only way to make things right, but I think the price of losing all concurrency is not worth it. It would be easier just to have the procedures executed serially using a scheduler of some sort. You will open a Pandora box with this approach that will be hard to close. Queuing and syncing is a very complicated challenge. If I were in your place, I would first investigate the root cause of the deadlocks. Sometimes, something as simple as adding or modifying an index can solve it. If you post your deadlock graphs here, along with the involved queries with their execution plans, people will help you pin point the root issue. HTH 

Suppose that I have a table with 1000 records and I have the below query, which would return 7 total records: 

If that's the last statement in the clause, does SQL Server use the SARGability in the first two WHERE statements to first filter, get the 7 rows, then look to see if each row of the 7 are not equal to 12, or does it apply the to every row in the original data set of 1000 rows? The reason that I'm asking is because I'm aware that I could use a subquery or CTE to do the first part of the SARGable filter, then of the 7 rows, look and see if each is not equal, but is the query optimizer already doing that behind the scenes, or is it best to do it myself? 

Since the two columns have up to date indexes, SQL Server can make assumptions and find the values faster and both queries are SARGable (ideally fewer reads). However, suppose that I need to make sure that the value is not equal to another value, let's say 12 for a different column, so I would have to add 

I have a job in SQL Server Agent (2012 Instance) that deletes rows from a table. When I manually run the job, it succeeds without issue and in reviewing the table, does exactly what I want it to do. When I've set it to an automatic schedule, the history indicates that the job has successfully run each step, yet in reviewing the table, the records are not deleted. The job is set to enabled, and the history shows that it's run on the schedule, even with no records being removed. If I run it manually, suddenly I see the correct results in the table. Each step is run under the same login, for instance , whether I run the job manually or it's run on its schedule. The only difference I see between running it manually or the automated schedule is what invokes the job. When it's the schedule, it reports vs. when I run it manually, it reports, Note that this standard setup exists on other servers and doesn't create the same issue, so even though this is the only discrepancy I can find between how the job is run (manual vs. schedule), this doesn't look to be the issue. Any other idea what could be causing this? 

This question is completely predicated on the assumption that you're wanting to run an environment where Server A matches Server B and that the application can connect to either at any given moment and that what happens in one should also then be sent or updated in the other. I based this on the above quote in your question (the comments might indicate some other goal). Depending on the version (2005, 2008, 2008R2, 2012, 2014, 2016), you can look at merge replication as a possible solution here. Provided that the database structure is identical (lots of assumptions), if the application connects to Server A, then the data added, updated, or deleted will be sent Server B, if Server B is online to receive. Vice versa is true.