In contrast, MySQL Cluster has the technology to allow multiple management nodes. You will have to look for multiple-instance usage of MySQL Fabric in future releases. That would have to be on Oracle's roadmap. VERY WILD SUGGESTION If you use VMWare or Amazon EC2, you need to make an OS instance with the following: 

MySQL has no particular SMTP setup, mechanisms, or drivers whatsoever built in. However, there are two basic things you can do the kind of monitoring you want. Option 1 : You Could Monitor the Binary Logs If binary logs are enabled, you could write a shell script to call mysql and do SHOW MASTER STATUS; If either the filename or filesize changes, something changed. Once detected, you could send out an email expressing that something changed !!! Try something like this: 

In many examples I see used as a parameter with the definition of custom types. Also, the documentation tells me that "[t]he default assumption is that it is variable-length", if this parameter is omitted. Is this also true for types like the one above where the size can be calculated (3*4 bytes + 4byte padding)? So should I add 16 as internal length to the definition? Also, are there other parameters that could improve the performance of handling many (~10,000,000) entries using this type? 

As you can see, this takes about 1.7 seconds. This isn't too bad considering the amount of data, but I wonder if this can be improved. I tried to add a BTree index on the user column, but this didn't help in any way. Do you have alternative suggestions? 

I am about to split a large database into a bunch of federated instances. Currently, most of the primary keys are auto-generated identity ints. Obviously, that's not going to work in a federated setup. I've read people using auto-generated GUIDs to replace the ints, but that data type has well-known performance sapping problems on its own. What are some of the strategies I can use for having unique values for primary keys across federated members? 

I have an SQL Server 2014 Enterprise Edition with a lot of data. I would like to mirror the data from that to at least 3-4 servers. The mirrors are all SQL Server 2014 Standard Edition (no more money is available for Enterprise licenses). How do I mirror the data from my main box (with the Enterprise Edition license) to other boxes? I tried the mirroring feature, but it seems that it only allows single mirror. I could you use Always On Availability groups, but that would require that all mirrors also be Enterprise Edition (unless I am reading the docs wrong). At least one of the mirrors needs to be there almost real-time (1-2 minute delay is fine) data replication. The other mirrors could have 1-2 hours delay. So what are my choices? P.S. All the secondary servers are just read only. P.S. The purpose of the mirrored boxes are partially to off-load readonly queries to them. These mirrors need to have near real-time data replication. Another purpose is for analytics, which is a heavy load. Today everything is on the same box and we are forced to do analytics at night so as not to disrupt users and there is just not enough time. P.S. The servers are nearby each other - on the same subnet, connected via a 10Gb link. P.S. Our license also allows a no cost upgrade to SQL Server 2016 when it becomes available. Does that change anything? 

If you would like to detect which databases actually contain MySQL data, you can run the following query: 

How does this apply? You should err on the side of caution. You should always check the charset beforehand because you do not know the neighborhood (client program, internet browser) the PHP connection will be entering and if there is a risk of a carjacking (putting invalid data into the database, requesting too much data for retrieval). QUESTION #3 

This changes the query because the query will look for the specific and scan all days for 1 only. The reason is included in the index ? The query will retrieve the from the index only file rather than the table. That way, all 3 fields are retrieved from the index file instead of 2 fields from the index and 1 from the table. InnoDB If is InnoDB, this is the index you need 

With SQL Server 2005, you could look at the Task Manager and, at least, get a cursory look at how much memory is allocated to SQL Server. With SQL Server 2008, the Working Set or Commit Size never really goes above 500 MB, even though the SQLServer:Memory Manager/Total Server Memory (KB) perf counter states 16,732,760. Is there a setting where it will actually show the server memory in the Task Manager? Or is it a result of them changing how memory is used in SQL Server 

What does Table Scan (HEAP) mean for a partitioned table? Does it indeed use an index, perhaps behind the scenes? Is there anything I need to do to improve efficiency? 

I've setup a test SQL Server 2016 server. I've installed 2 instances, restored a backup on the primary. Then restored a backup on the secondary with , then restored the transactional log on the secondary, also with . I then followed the prompts in the Mirroring Wizard off the Database Properties page and ended up getting an error: . What am I missing? 

Based on this, I would suggest, not do binlog_do_db in PXC, but, do replicate_do_db on the Slaves instead. 

According to your question and its comments, your buffer pool may still be too small. Since the Buffer Pool is < 1G (1024M), innodb_buffer_pool_instances drops to 1. This, in turn, would force innodb_page_cleaners to drop to 1. You can verify than with 

I cannot say for sure what running time impact there will. However, I can say this: There may be some additional disk I/O because the indexes will no longer contain the needed column info. Queries will not have to turn to the file to retrieve additional column information. Please keep in mind that MyISAM is suitable for heavy-read queries. The only tuning I can further recommend if there is supposed to be INSERTs and DELETEs in the middle of the day during heavy-read periods would be to enable concurrent INSERTs. 

I am partitioning a table based on a column that is not a primary key? I've read some conflicting information today on whether the partition column must be a part of the primary key. My gut says no, but I am not 100% sure. So questions... 

It states Table Scan (HEAP). Not quite sure what it means in the context of a partitioned table. I also don't see that it uses any kind of index. And yet, it must, because the query comes back fairly fast (e.g. the table has 6 million rows). So, my questions are: 

I have an app that's local to the SQL Server and thus has a Shared Memory connection to it. I was wondering whether the RAM taken by the connection (including data transfers) counts against the max memory limit set for the SQL Server. The reason I am asking is that the SQL Server is maxed out on memory (e.g. Target Server Memory = Total Server Memory and other metrics). If the RAM taken up by Shared Memory connection counts against it, wouldn't I be better off using TCP connection to the SQL Server? 

The first SELECT can return at most one row. The second SELECT can return at most one row because of the . In the worst case scenario, the UNION will have two rows. Now look at the whole query: 

Give it a Try !!! CAVEAT : Once you are satisfied with this operation, you can drop the old table at your earliest convenience: 

You could limit the number of connections per hour per user. Restarting mysql would not be necessary for this. For example, suppose all your web servers connect to mysql from the 10.1.2.% netblock. You should already have a user in mysql.user named something like 'myuser'@'10.1.2.%'. To set the maximum number of queries per hour at 1000 on a given connection do this: 

after doing that mysqldump ? My guess would be yes. If neither of these are the case, then your table has some corruption in the . Try installing MySQL on another DB Server and loading the SQLDump there. If nothing errors out, the mysqldump is fine. This verifies some corruption. Try dropping the problem table, create it from scratch. Then, load it back. You could run myisamchk against the MyISAM table. 

Edit 2: This is the result, when I use an on (but no schema optimization, yet) as @erwin-brandstetter suggested (the query runs with 1.5 seconds at the same speed as my original query): 

For the sake of completeness, this is the complete table definition with all it's indices (without foreign key constraints, references and triggers): 

Edit: This is the result, when I use the query (and index) proposed by @ypercube (query takes about 5.3 seconds without ): 

I am running PostgresSQL 9.2 and have a 12 column relation with about 6,700,000 rows. It contains nodes in a 3D space, each one referencing a user (who created it). To query which user has created how many nodes I do the following (added for more information): 

In Postgresql 10.1 with PostGIS 2.4.1 I select line geometry distributed in space with the help of a 3D bounding box. These edges are then joined against the respective node information table to get more information of the points involved. Both the edge table and the node table have about 18,753,793 rows and will likely grow tenfold in the next year. In my bounding box query I use a to compute all intersected edges, but the in the query plan below takes longer than I want it to. I also find that the estimates involving it (and also the the PostGIS edges) are often off by a factor of 10 or more. My statistics target is 1000, increasing it doesn't change things. Statistics are up to date. I wonder if the the planner could come up with a better plan with more accurate statistics. When I use a instead of the , the estimates are better and the outer (see query plan below) is faster, the overall performance is roughly the same. I noticed though that the outer sequential scan on the temporary table says and the version does say . Is there a way to have the outer behave similarly in the CTE version or is this not really comparable? And Is there any way of improving the estimates, if they can help with performance at all? A typical query takes about 115ms and looks like this: 

The file contains both data and index pages for . If you have another disk (says an SSD or HDD) and you want the file to be moved over there, you may have to use the directive for the command. Here is what the MySQL Documentation for says on this: 

I don't see anything along those lines in the Percona Toolkit Documentation. There is something you can do to cheat Scenario 

In your case, you must have innodb_file_per_table disabled. The actual table was inside the system tablespace file (usually located in ) My Guess 

You can shorten the running type of queries by indexing certain columns. Since you mentioned sorting DATETIME fields, one of the best ways to bypass sorting of temp table results is to use a covering index which includes the DATETIME as the last column of any given compound index you need. I know this is entirely possible because in a link about Covering Indexes By Ronald Bradford, he gave a list of future topics he was giving. Here is that list: