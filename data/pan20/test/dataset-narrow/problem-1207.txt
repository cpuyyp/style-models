So taking "efficient" to mean "polynomial time", then the answer to your question is "no", or more precisely, "only if P = NP" 

Here's the (updated) program for generating G.Bach's graph. I added indicators at the end to check that the graph is a well-formed tournament graph: 

The longest path is just the path from to Now to show that the number of deepest-branch changes is amortized constant with each add, just imagine that each time we compare two depths and get d(a) == d(b) (when we stop), we reserve one time unit for later. When a node changes deepest branches, the paths must have first been equal so we use up that time unit. So now all that remains to prove is that the path-compression step is amortized fast and I unfortunately don't know how to do that. Please note I haven't tested this code and it is almost certainly buggy. Also, I may have left out some edge cases such as how to insert the first node or what to do in certain situations if a node already is on the primary path. It shouldn't be too hard to account for these. 

If a CDCL SAT solver only selects negative literals as decision literals (but can set positive literals through unit propogation) but has a perfect heuristic for determining which literal to select next, what kind of claims can be made about its running time? Is it necessarily exponential on some infinite class of satisfiable instances? 

Now when we add a new vertex $v$, we traverse upwards through each (3) updating the deepest vertex until we find one that doesn't update. If we encounter a vertex $w$ which is on the primary path (this is easy to check because $w$.deepest() will be the same as root.deepest()), then we check if the path between the newly inserted node and the current deepest is longer than the currently known longest path. Then that becomes the new diameter. This check can be done in constant time since we know the depths of the current deepest, the newly inserted node, and also the node $w$ at which the paths pointing to the two diverge. Then the length (number edges in) the new path is just $d(v)+d(deepest)-2 d(w)$ It's easy to see that this algorithm is correct because any diameter must have an endpoint at the deepest node from the root (This is generally true of trees. If you don't believe me you can verify it for yourself) and with every node we insert, we check that either a) It's the deepest node (in which case, we've just increased the currently longest path by one) OR b) There exists an ancestor containing a deeper (or equal) node which is not on the deepest path (so the diameter can't have increased) OR c) We check the newly inserted node against the deepest node and compute the length of the new path to see if it's longer than the current longest. To finish the Python pseudo-code for the algorithm: 

There are several standard NP-complete problems, like satisfiability (SAT), Graph-Hamiltonicity, and Graph-3-Colorability (G3C), for which zero-knowledge proofs exist. The standard way of proving any NP-theorem is to first reduce it to an instance of the aforementioned NP-complete problems, and then conduct the zero-knowledge proof. This question pertains to such reduction. Assume that the P vs. NP is settled in any of the following ways: 

I also suggest looking at the relevant paper: Beigel, R., Buhrman, H., and Fortnow, L. 1998. NP might not be as easy as detecting unique solutions. In Proceedings of the Thirtieth Annual ACM Symposium on theory of Computing (Dallas, Texas, United States, May 24 - 26, 1998). STOC '98. ACM, New York, NY, 203-208. DOI= $URL$ 

Consider an array of bytes. I want to partition the array, such that the following two conditions hold: 

The Hamiltonian Cycle Problem asks to decide, given a graph $G$, whether $G$ is Hamiltonian. This is the decision version. The function version(FNP) asks to actually find a Hamiltonian cycle in $G$ (if any). I suggest looking at FNP, and related classes (such as NPMV and NPSV) in the Complexity Zoo. In addition, a quick search in Google Books reveals many books in which you may find more info. Please be more specific if you have more questions in mind. 

Here's a small variation on the original question. Let $O$ be an oracle which, on input $(f_1,f_2)$ outputs 1 if CNF $f_1$ has more solutions than CNF $f_2$. Given this oracle, we build a poly-time machine $M$ which can solve the #P-complete problem of computing the number of solutions to a given CNF $\varphi$. Note that that $\varphi$ can have an exponential number of solutions. $M$ works as follows: It generates formulas with known number of solutions, and using binary search and by asking at most polynomial queries to $O$, it finds a formulas $\varphi_i$ which has the same number of solutions as $\varphi$. It finally outputs the number of solutions just found. This shows that $M^O$ has complexity #P. 

I am confused whether $\mathsf{APX-hard} \subseteq \mathsf{NP-hard}$. My confusion stems from a result on graph pricing from this paper which says the following : "Unlike the general case of the graph pricing problem, the bipartite case was not even known to be NP-hard. We show that it is in fact APX-hard by a reduction from MAX CUT". It seems like the authors are implying that APX-hardness is a stronger property than NP-hardness Since $\textsf{APX} \subseteq \textsf{NP}$ by definition, the above statement is true for MAXCUT as it is in $\textsf{APX}$ and also in $\textsf{APX-hard}$ but I am not sure if $\mathsf{APX-hard} \subseteq \mathsf{NP-hard}$ in general. Any pointers/counter example problem is appreciated! 

Consider the following linear program, $$\min y \\ xc_1 \leq c_2 + yz,\\ x = x_1 + \dots + x_n,\\ z \leq x_1 + x_2, \\ z \leq x_2 + x_3, \\ \vdots\\ z \leq x_{n-1} + x_n, \\ x,x_1, \dots, x_n,y,z \geq 0 $$ where $c_1, c_2$ are constants. This is an example of quadratically constrained linear program where I have 1 quadratic constraint. I wish to find out if this problem is NP-Hard or not. The quadratic constraint can be expressed in the form $\vec{y}M\vec{y}^T$ where $M$ for my problem is not positive semidefinite (and thus, non-convex) which is perhaps evidence of hardness Listing specific questions below: 

Apologies in advance for this a soft question which has no closed, correct answer. This is probably the best forum to ask my question. I am a third year graduate student in theory group of a top-15 school in US. So far, I've been doing decently well. I have a first author theory paper and a first author practical paper so far. My advisor has been superb in helping me honing my skills but I feel stuck (and helpless). So far, my advisor has been a helping hand and a guiding force to always point me right directions and pose the right questions (not too abstract, not too concrete) which have been instrumental in me finding the answers to research problem. As I try to be more "independent", i.e, ask the right questions and prove results, I feel I have failed miserably and quickly feel overwhelmed. I feel that I don't have the maturity of a theory PhD student yet where I can ask myself the right questions and come with answers. In other words, given the right definitions and some hand holding, I am able to do things but otherwise, it becomes very hard. My sloppiness when it comes to writing proofs doesn't help either. I am looking for advice on how can I sharpen my skills and be more "theory-minded" where I can grasp things quicker and require less hand holding. While one answer is simply to keep working and hope that experience makes me wiser, I am not sure how this will work out. The only solution I have is to actually go through come recent papers in my area and write down proofs by hand in as much detail as possible to help me nail proof writing skills and build intuition. Any advice would be extremely helpful. Please let me know if question is unclear or needs more detail. 

Thanks to Tsuyoshi's comment, I have the solution. It's always possible to take some subset of lefts (of size k) and construct a set of k bicliques covering all the edges connected to those lefts (assign each left to it's own biclique together with all its right-neighbors). This means that for any subset of k bicliques in the minimum biclique cover, there are at least k lefts contained in the union of those bicliques. Now we can construct the bipartite graph which contains a left node for each biclique in the minimum cover and a right node for each left node in the original graph. There is an edge between a left and right if the corresponding biclique contains the corresponding right in the original graph. The lemma above translates exactly to the criteria for Hall's theorem in this new graph which means there exists a matching which saturates the left side. This corresponds to an assignment of bicliques to lefts in the original graph. 

Ok, I figured out what graph @G.Bach was describing and coded it up in clingo (see the clingo description below. It starts with a description of the gadget graph and proceeds to describe how to join copies of it together to get the full 34-vertex tournament graph G.Bach is describing. I've attached the grounded graph description as well). I then proceeded to run clingo on that graph and it claimed to have found a TFAS with 241 edges. But I made a mistake in the graph encoding. I fixed the mistake and clingo now reports unsatisfiable (ie there is no TFAS). Here's the program for finding TFAS's on a graph 

In fact for any given vertex (1), (3), and (4) may be outdated, but using path-compression we can bring it up to date in (hopefully) amortized O(log*n) running time using path-compression. 

In this paper $URL$ they claim to have a practically fast persistent union-find data structure for most use-cases, but it's still not polylogarithmic in the worst case (the worst case being, I have two variants which I alternately update between); only in a backtracking search. Are there any implementations of a persistent union find datastructure which guarantee amortized polylogarithmic time complexity per operation regardless of how it's used? 

The technique of "hopping" from one game to another, and proving the security through a "sequence of games" is not new. In particular, there are several papers which discuss these techniques, and exemplify them through various security proofs. The famous examples are: 

Coloring Problems The best book on the subject is The Mathematical Coloring Book: Mathematics of Coloring and the Colorful Life of its Creators by Soifer et al. There is also another book Graph Coloring Problems, by Tommy R. Jensen and Bjarne Toft. 

The beauty of this law is that it does not depend on the distribution of arrivals or the service time (whether it is Markovian or not, etc.). More technically, and in Kendall's notation, it is true for the general GI/G/m queues. We now assume that service time follows an exponential distribution (with parameter μ), and the arrivals follow a Poisson distribution (with parameter λ). In addition, we assume there's only one server. That is, our queue is modeled as M/M/1. Using Little's Law, it can be shown (see formula (6.15) on page 247 of this book) that: $W = \frac{\lambda/\mu^2}{1-\lambda/\mu}$ Note that the book uses different notations than here. It also states the formula holds for M/G/1-PS and M/G/1-LCFS queues. Using Little's Law, we have $L = {\lambda^2 \over \mu^2-\lambda\mu}$. In your case, λ = 1/(KT), and μ = 1/T. Hence L = 1/K(K-1). 

I'm posting my comment as an answer, at the request of the OP. Arithmetic Hierarchy AH is a class of decision problems defined as below: Let $Δ_0 = \Sigma_0 = \Pi_0 = R$. Then for $i>0$, let 

I recommend reading Savitch's paper. It basically states that, for any function $f(n) \ge \log(n)$, $\text{NSPACE}\left(f\left(n\right)\right) \subseteq \text{DSPACE}\left(\left(f\left(n\right)\right)^2\right).$ The result establishes, for example, that $\text{NPSPACE} = \text{PSPACE}$; a surprising result which its "time" counterpart ($\text{P}$ vs. $\text{NP}$) is a long-standing open problem. Savitch, Walter J. (1970), "Relationships between nondeterministic and deterministic tape complexities", Journal of Computer and System Sciences 4 (2): 177–192. 

It is required that for all $k$ output by $\mathsf{Gen}$ and all $m \in \{0,1\}^*$, we have $\mathsf{Verif}_k(m, \mathsf{MAC}_k(m)) = 1$. The security requirement is defined via the following experiment, between the challenger and the adversary $A$: 

This problem is NP-hard. As proof, the maximal clique problem (or rather the decision variant find-a-K-clique) can be reduced to this problem as follows. Start with a problem on a graph with N vertices where we wish to find a clique of size K. The set containing these original vertices we'll call S. Add a clique (we'll call it C) of size (N - K)*K which is joined to every vertex in S. Additionally, adjoin one more vertex V which is joined only to the vertices in S (not the ones in C). Now we have an instance of your problem (never mind the edge weights) where we want to divide the resulting graph up into N - K + 1 cliques of size K+1. I claim that there is a solution to this problem if and only if there is a clique of size K in the original graph. only-if follows from the fact that V must belong to some clique of size K+1 which is only the case if there are K vertices which form a clique in S. Furthermore, there will be enough leftover nodes in C that every S-vertex not in the solution clique can be assigned to a separate set of K vertices from C. So once we've managed to find a clique for V, finding the other N-K (K+1)-cliques is always possible (and indeed trivial). 

Run it with (Using clingo 4.2.1) (the n=7 indicates graphs of exactly 7 vertices) It should return satisfiable if and only if there exists a graph with no TFAS on 7 vertices. 

Is there a better term for "complete k-partite graph" in the case where k is not fixed? If I say "complete k-partite graph", people tend to assume "for some particular k". In other words, what's a term for any graph for whom each connected component in the complement graph is a clique? I asked this before, but it was as part of another question, so it was ignored. 

The maximum independent set problem gives a lower bound for the minimum clique cover problem. This is easy to see because given any clique cover together with an independent set, any two vertices which are part of the independent set must belong to separate cliques in the clique cover. Is there any way to put a worst case (hopefully constant) upper bound on the ratio between these two values? What about when the number of vertices grows large? I know it must be at least $\frac{3}{2}$, since we can have a graph composed of an arbitrary number of pentagons (each pentagon has a maximum independent set of 2 vertices, but a minimum clique cover of 3 cliques). Can anyone find a graph where the ratio is larger than $\frac{3}{2}$? larger than 2? 

Based on posts from or-exchange and some internet reading, the following algorithm works. We adapt the cutting plane method to binary search for variable $y$. For my problem, the upper and lower bounds for all variables are known (but if these are not known, one can find a bound on $y$ by solving the LP by removing the first constraint). Let $l \leq y \leq u$. Fix $y = (l+u)/2$. This converts the program to a linear program. If the resulting constraints are feasible, update $u(l) = (l+u)/2$. Keep performing the binary search until we reach the optimal solution with an additive $\epsilon_0$. The running time of the algorithm is $O(PTIME)\log\frac{u-l}{\epsilon_0}$. While this algorithm can find the optimal solution within a small additive constant, I am still not sure about the hardness of the problem for solving exactly. Any comments regarding complexity are welcome! 

Can this problem be transformed into a linear program by taking logarithms? Is there any literature reference or reduction showing that linear programs with non-convex quadratic constraints is an NP-Hard problem? 

I will begin by linking a previous post where I asked a general question for a stochastic setting which I describe below. It turns out that my "proof" for a restricted case had a mistake and there is a much simpler setting where showing hardness should be easier. Please let me know if I should amend the original question instead. Consider a graph $G = (V, E)$ with $n$ vertices and $m$ edges. Each vertex $v_i$ can take positive value $a_i$ with probability $p_i$ and value $0$ with probability $1-p_i$. We will restrict $G$ to be a cycle where every vertex has degree $2$ (and $m = n$). The challenge is to assign weights $w_e$ to each edge to maximize the objective function $E = \sum_{e = \{i,j\}} w_e \Pr[X_i + X_j \geq w_e]$ where $\Pr[X_i + X_j \geq w_e]$ denotes the probability that that the sum of values taken by vertex $i$ and $j$ is greater than $w_e$. The additional constraint is that the weights $w_e$ need to be sub-additive, i.e., for any two edges $e'$ and $e''$ that "cover" edge $e$ meaning $e'$ and $e''$ include the vertices that make $e$, it holds that $w_e \leq w_{e'} + w_{e''}$. Observe that the deterministic version where $p_i = 1$ is trivial. Any suggestions on possible directions for hardness or PTIME algorithm would be very helpful!