Where the dot product is cos(θ) But in this article on better sampling ($URL$ the author suggests the PDF is (cos(θ) / pi), and there is no evidence of the N dot L calculation. My question is - does that mean that I no longer need to perform the normal dot rayDirection because it is included in the PDF, or is it in addition to the pdf? 

I made no special case handling for clamping the back-side of the basis in the tool - my experiments and research lead me to conclude that the entire hemisphere should be represented for each basis, even if intuitively the backwards-facing samples seem like they would 'subtract' colour because of a negative dot product. 

What performance can I realistically expect from that? If only 50% of the pixels(or should I say fragments here?) pass the stencil test, do I save 50% of the rendering time, less, none? If this is a poor example, my fallback question scenario is a static scene, where the previous frame's result is reprojected, with an error accumulator in alpha, and a stencil mask is computed based on misses or too high error rates(e.g. position of the previous frame's reprojected pixel is too far from the new target pixel). 

For reference, what I'm referring to is the "generic name" for the technique first(I believe) introduced with idTech 5's MegaTexture technology. See the video here for a quick glance on how it works. I've been skimming some papers and publications related to it lately, and what I don't understand is how it can possibly be efficient. Doesn't it require constant recalculation of UV coordinates from the "global texture page" space into the virtual texture coordinates? And how doesn't that curb most attempts at batching geometry altogether? How can it allow arbitrary zooming in? Wouldn't it at some point require subdividing polygons? There just is so much I don't understand, and I have been unable to find any actually easily approachable resources on the topic. 

When converting from uniform hemisphere sampling to cosine weighted hemisphere sampling I am confused by a statement in an article. My current indirect contribution is calculated as: 

Given that I am currently sampling on a hemisphere, do I simply project each sample onto the bases and store those accumulations? If I do that, how do I handle the fact that data within my original hemisphere, when projected onto the bases, could create a projection that actually subtracts energy, given that a sample ray pointing away from the bases gives a negative dot product. 

I have had luck with cosine weighted hemisphere samples, and I know how to generate stratified uniform samples, but I wanted to experiment with combining the two. However, how do I correctly stratify the hemisphere when performing the cosine weighted sampling? Do I cosine-weight the stratification? How many rays do I cast in each stratum? My application is the collection of indirect samples in a lightmapper. 

with α>0 per y>0 , and α<0 per y<0 (then 2 transformations)? And if it's right how can I calculate the parameter α? Otherwise if it is all wrong as I do? 

The scenery is the following: I have a polygon in the space, defined (the polygon) from your vertices. By 3 vertices of the polygon (non-aligned) revenue equation of the plane containing the polygon. Later I have a ray of light, represented by the parametric equation of the line in the plane (the parameter is $\textstyle t $). I must determine the intersection between the line and the plan and see if the point of intersection (if they line and plane are not parallel) is internal or external to the polygon. Written the equation of the plan then, it's simple find the point of intersection: I replace the parameter values of $\textstyle x $ $\textstyle y $ and $\textstyle z $ of the line in the equation of the plane and from here I determine the value of the parameter t. Then I replace t in the parametric equation of the line, and I have the coordinates of the intersection point. The text that I am following (3D COmputer Graphics) tells me that if I get from the calculation that $\textstyle t < 0 $ the point will stay in a part of plane that does not contain the polygon. I did not understand why. Than intuitive-geometric meaning has $\textstyle t $? I point that t <0 is sufficient but not necessary condition to say that the point does not belong to the polygon. If it isn't t <0 I'll have to explicitly test whether the point is inside the polygon or not but the book says that if t <0 I can avoid all this job because the point is certainly outside the polygon (in a part of the plan that the book calls half plane ). I would like to know why, what mean the parameter t. 

For example, let's assume I'm rendering cascaded shadow mapping, but for whatever reason, instead of one of the typical approach, I do the following: 

Whew, that was a long title. Either way, I'm asking this question, since I like to think about various things, and it occurred to me that there isn't really any simple, open source layers on top of GLSL, even if only to add simple things such as includes, or commonly used functions. As research of sorts, I'm asking this question, since my own awareness of such languages is minuscule to say the least - I know of bgfx's shading language, and Unity's ShaderLab, but I don't really know what they accomplish - or why - being a relative newbie to computer graphics. Alternatively, what would your wishlist for a shading language like this be? Mine so far is includes, some compatibility between versions, optional "hidden" inputs that allow easily accessing textures at pixel offsets, or image sizes, etc. and probably passes - for, say, two-pass gaussian blur. Thoughts? 

I'm learning webgl, and I'm using a book that use a old version of gl-Matrix (1.2.4). The newest versions of gl-Matrix aren't backward compatible, thus I'm looking for the documentation of gl-Matrix 1.2.4 (or 1.3). In the official site: $URL$ I not found the documentation of these old library but the latest version documentation alone. Where could I find that I looking for? 

I have to calculate the modeling transformation that transforms a cube of side=1 centered in (0,0,0) in a rectangular truncated pyramid of height=2,sides of basic rectangle equals to 3 and 2, sides of up rectangle 1.5 and 1. Moreover the pyramid has the center of basic rectangle (down rectangle) in (0,0,0), the axis of symmetry lies on y=-x and the major side of the the basic rectangle must be rotated 45 ° towards the direction of -z. I'm not able to realize this transformation. I thought that to transforme the cube in a trucated pyramid is analogous to transform a square in a isoscele trapeze and thus I can't use an affine transformation because it preserves parallelism and this isn't the case. I need, may be , a projection transformation that I know more o less how works but I'm not able to apply in this case. But I know the transformation that map a 3D figure in a 2-D plane, but in this case I have to associate a 3D figure (cube) to a 3D figure (truncated pyramid) I believe. How can I realise this transformation? 

For example, while it's the current top-of-the-line GPU, the GTX 980 has a staggering 72.1 gigapixels/second fillrate, which with back-to-front rendering and/or Z buffer checks, seems almost ridiculously large, possibly even at 4k resolutions. As far as polygon counts go, modern GPUs can do tens to hundreds of millions of textured triangles without a hitch if you batch and/or instantiate them properly. With forward rendering, the amount of fragments that shaders will run on can quickly become overwhelming, but with deferred rendering, the cost is usually more-or-less constant, depending on the resolution, and we long since passed a point where most shading or post-processing effects can be done in realtime in 1080p. Either way, the limiting factors nowadays are most commonly draw call counts, and shading costs, both of which are kept relatively low by proper deferred rendering and geometry batching, so with that in mind, is culling more than just backfaces and out-of-frustrum polygons of any substantial benefit? Wouldn't the costs(CPU/GPU time, programmer time) outweight the benefits, a lot of the time?