As you can see, it must be in 'recursive' mode or non-local names will not be resolved. If the requested hostname is not found in the dns-database, if 'recursive' is specified the request will be forwarded to the Fortigate's System DNS which can be a Fortiguard DNS (like in your case) or your provider's DNS. Now you can resolve a local hostname like 'namea.mycompany.local'. At last, if you use the FGT as your DHCP server, specify the Fortigate's LAN address as the DNS to use so that all your local hosts will know whom to ask. 

This message means that all high ports (>1023) are used up for outbound NATted connections. This is very rare and signifies that your FGT is processing way too many sessions. Check the number of open sessions in the GUI or CLI (get sys perf stat), "average sessions" and "average sessions setup rate". Two main reasons may apply: either you've hit a firmware bug or some host(s) on your internal network start sessions wildly, possibly malware induced. High (outbound) bandwidth consumptions hints into the malware direction, too. Remedy: reboot your FGT. Watch the session count and buildup rate. Depending on the firmware version used, check FortiView to determine where the majority of sessions either originate or are destined to. Then follow the trails. You may find more hints and help on the Fortinet forum (forum.fortinet.com). 

It might well be that it's not the AC blocking Skype but the WF. 1- Do you have a valid UTM licence? If not, WF will block. 2- Enable logging on that policy. 3- Make sure it's the first policy from top down so that it gets hit by traffic. 4- Disable WF. Disable AC. If Skype now is connecting, enable AC. If that is working, enable WF. If not, check the AC blocking rules, especially the 10 blocking entries. 

There is hardly ever a good or mandatory reason not to specify the interfaces in a policy. There is always a price you pay for convenience: loss of control and time while maintaining and debugging. Try to determine the flow of traffic through, say, 80 firewall policies most of which use the 'any' interface. Which one is 'shadowing' others, and are explicit policies hit at all, if not, why? This is near to impossible. Of course you can sniff but that's in realtime. Firewalls are a lot about planning and foreseeing. To put it differently: policies reflect the flow of data (and it's protection) in the way you have planned and foreseen it. Firewalls are built to prevent deviations from that plan. That's why there are implicit DENY policies for each interface pair in FortiOS, as an example. You do have the information about which network exists (or is routed to) behind each interface. In not using this information you prevent the firewall from attaining it's full potential. The loss of RPF is IMHO only a side effect of the 'any' interface. Usually, if the protected networks are connected to the internet you need a default route. This route is used all the time if you use an 'any' interface, even for traffic of unknown origin. Effectively, this eliminates RPF. 

For local name resolution you need to set up 3 things: 1- a DNS zone 2- at least one A record in that zone 3- a DNS on the interface where your internal hosts are A zone in a nameserver is a container for name/IP pairs, the records. You create a DNS zone in : 

There are 2 ways to do this: 1- create 2 policies, one where the destination is the exclusion range, and one following it with the whole destination range. Enable SNAT just in the second policy. Note that policies are matched top-down so no traffic destined for the excluded range should ever hit the second policy. 2- create one policy with the exclusion range, and in the CLI add "". Now this policy will be matched by any traffic except the exclusion range. Enable SNAT on it. Of course, you have to take care of the non-matching traffic now. So add a second policy with destination of the exclusion range and disable (rather, do not enable) SNAT in it. This is nearly the same effort as #1 but less visible and therefore prone to misunderstandings. The 'negate' parameter is best used for more complicated cases, e.g. where you have several 'holes' in an address range and want to block traffic matching those. 

The paradigm of a DMZ (as I've learned it) is "consider all hosts in the DMZ as being hacked". The network protection by a firewall should ensure that even in this ("worst") case the data on the internal network is safe from external access. If you have to expose some of your data to the internet you publish it on a server in the DMZ. In a DMZ malicious attacks from outside directly hit the hosts whereas attacks against internal resources hit the firewall - and firewalls are designed and equipped to withhold attacks of various sorts, in contrast to multi-purpose servers (or operating systems). Following this line of thought you would implement one rule that data from the inside will always be pushed into the DMZ and never actively pulled from the DMZ. In case a DMZ server is compromised it cannot pull data or use services you have not intended or explicitly denied in the first place. This has consequences though. For instance, a server might query an external time source which has to reside on the internet then. Or if the server needs to resolve internal names via DNS one would install a second DNS in the DMZ with a subset of the data. Regarding your questions: Q2 Firewalls usually have multiple ports and control traffic between those ports with 'rules' or 'policies'. A DMZ would always be connected to a port of it's own, likewise the internal LAN and the WAN. So you have all means to allow certain services, to/from certain sources or destinations, at certain times etc. Everything not mentioned explicitly is denied. Today's firewalls will try to protect the servers and hosts behind it by various means like IPS or Application Control, that is, content inspection. They do more than just allow or deny traffic and route it, and are then called UTM firewalls or NGFW (next generation FW). Q1 From the above one would only allow pushing data onto the DMZ webserver and never allow access from it to internal resources. In your case though I would not even consider to put sensitive data into the public. If HQ has data that a branch needs it should access the branch server via IPsec VPN. The data will be transfered in private, to authenticated partners and will be monitored for integrity. 

Assuming you have set up the VPNs in interface mode (a.k.a. route-based), you need 2 things for this to work: 1- two routes to the other 2 networks 2- policies to allow the traffic Let's call the 3 sites A,B,C and the firewall FGT. If A can ping B, and B can ping C then you already have a route to network B in FGT A, and a route to network C in FGT B. You also have return routes in place, on FGT B to network A and network C. Reversing the traffic: if you can ping from B to A, and from C to B then you have 2 policies on each FGT. The key point for the necessary additions to your routes and policies to enable traffic from A to C is that the FGTs will drop traffic from "unknown" sources, that is, from networks they don't have a route to. FGT A doesn't "know" about network C, and vice versa. So you need an additional route on FGT A to network C pointing to the tunnel interface to FGT B (the only tunnel interface here), and also one more route on FGT C to network A pointing to the (only) tunnel interface. Then you have to add network C to both policies on FGT A (as destination and as source, respectively). The same applies to FGT C, allowing network A in the policies. This does sound like a lot of effort for a WAN network with only 3 nodes. Having 4, 5,... nodes in between creates a substantial amount of configuration needs UNLESS you design your networks in a suitable manner. If all networks behind the firewalls could be supernetted then policies and routes only have to be created once. Example: create networks A,B,C as 10.121.1.0/24, 10.121.2.0/24, 10.121.3.0/24. The only route needed on any FGT would be 10.121.0.0/16, and in the only 2 policies needed on any FGT you would have one destination 10.121.0.0/16 and one source 10.121.0.0/16. That's how hub-and-spoke VPNs are set up ideally.