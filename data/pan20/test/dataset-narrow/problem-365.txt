If that works, we can go back to working out why your first script failed. Edit: Just in case... you mentioned checking reads for the new filegroup. What are you using to check this? Are you looking at sys.dm_io_virtual_file_stats and seeing no change as the tables are fully cached in the buffer pool? 

We've all been there. Those that haven't are going to run into this situation one day! There is a text book approach to dealing with third party horror software: 

If the current file is for example E:\MSSQL.1\MSSQL\LOG\log_200.trc, the previous files should be log_199.trc, log_198.trc etc. Get the contents of the trace with: 

Reading between the lines, I guess the expectation/plan was for the SQL Server Data Tools (SSDT) project type to be extended to SSMS for the release of SQL 2012. Evidently that didn't happen, so we're stuck with the (frankly dreadful) SSMS interpretation of a project. If you want something to manage your database source, you should be diving deep into SSDT. I'm currently using it for one greenfield project and also moving a large brownfield database to it. Minor niggles and pain points but no show stoppers and we're getting payback on the time invested already. Jamie Thomson, who blogged extensively on VS2010 Database Projects (the predecessor), has started to publish articles on SSDT and appears to be as big a fan as I am. Bob Beauchemin is another early adopter worth following. If you want to organise a collection of random scripts (rather than a full database build) you can but support isn't quite "first-class". To do so, create a standard SSDT project type and change your scripts to compilation type "None" in the file properties. 

A SQL query is not procedural in nature, there is no top to bottom processing of the join operators. The ordering of tables in your example queries has no influence on the execution plan as they are logically equivalent and will generate exactly the same plan. You've sort of evaluating two of the options the query optimiser might consider when generating a plan for this query. The primary factor that influences plan choice is the statistics for the tables involved and the costs associated with the operator choices in any candidate plans. A very simple two table join such as your example could be satisfied with any one of hundreds of different execution plans. The optimiser decides which will be best way to answer your query by comparing the costs of these plans. It sometimes gets it wrong and you can help it make better choices through improved indexing, keeping statistics updated and applying hints. In very rare cases, you might want to force the order of execution by using the FORCE ORDER hint but that should be used sparingly. It's a hammer to crack a nut, the optimiser can usually be teased in to generating better plans by feeding it better information. 

Capture SQL:StmtCompleted, with a column filter on TextData %TableName%. Capture Lock:Acquired event, with a filter on ObjectID with the id of the table. 

The alternative to dynamic SQL is SQLCMD, which can be invoked from the command line, an agent job step, the Invoke-Sqlcmd Powershell cmdlet or enabled in SSMS. Your example in SQLCMD syntax would be: 

ExchangeEvent & e_waitPipeNewRow suggests you've run into what Bart Duncan refers too as Annoyingly-Unwieldy Term: "Intra-Query Parallel Thread Deadlocks". 

+1 for the answers from @CadeRoux and @ChrisS, they make valid points. Your comments to those answers highlight that this is essentially a proof-of-concept venture at this stage and you want to minimise your capital investment. If that's the case, forget spending $1000s on hardware and licenses, rent. You don't appear to need to store a vast quantity of data, so your early build-deploy-test cycles may target subsets of the 100GB (after 9 years) that was mentioned. Even at 100GB you should still fit comfortably into a wallet friendly Amazon EC2 instance. You can technically/legally deploy a developer SQL license to a bare EC2 VM while you're pre-production, cutting costs further. Brent Ozar posted an interesting analysis of SQL Server on EC2 last year that would help you weigh the pros and cons. I'd also weigh the merits of SQL Azure, especially now that the tooling (SSDT) is better geared toward the platform. 

Edit: 2011-11-12 13:00 As TokenAndPermStore looks to be a "normal" size, it's unlikely to be a factor. Next place to look is probably spinlock stats. Can you also (as I mentioned in chat) determine if the problem persists if you remove the explicit drop statements and let the tables fall out of scope instead. Edit: 2011-11-12 01:00 As you're on 2005SP2, I'm starting to wonder if this might be a TokenAndPermUserStore issue. It would explain why the problem is alleviated by a restart of the instance and why the problem is prevalent on servers with large numbers of database. Can you run the following query and add the output to your question. 

Don't. Design your API according to RESTful principes, design your database according to normalisation principles. One does not need to impact upon the other. Your database should not contain a table, it should contain a (or purchase/order) table. That table will include a primary key that uniquely identifies a Sale and foreign keys to related User and Customer tables. You REST api will translate a request for the resource identified by to the appropriate database query, retrieve the row, construct the resource that represents a Sale and return it to the client. Be mindful that you are currently exposing what appears to be internal database identifiers (UserID/ClientId/SalesID) to the outside world. It may be appropriate in your case but generally feels off in a RESTful API. 

Setup a profiler trace that will track lock:acquired and lock:released events, filtering on the DatabaseId from the previous script, setting a path for the file and noting the TraceId returned. 

There is no best practice. Your choice of concurrency control must be based on the application's requirements. Some applications/transactions need to execute as if they had exclusive ownership of the database, avoiding anomalies and inaccuracy at all costs. Other applications/transactions can tolerate some degree of interference from each other. 

Index maintenance (rebuild/reorganize) and DBCC CHECKDB activity most likely, possibly statistics updates. Any scheduled maintenance configured? If there is no user access, bin them. Just be mindful of the time frame over which you decide they are no longer used. Are there any weekly or monthly reporting tasks for instance? While you're looking, dig around for duplicate indexes as well. Edit: regarding SSC link From a quick scan through the thread, looks like the SSC folk had similar thoughts. They are however taking a more cautious stance on the possible "occasional" use of these indexes, taking the position that someone put them there for a reason, a perfectly reasonable argument. The counter argument is that all too often it's the exact opposite, someone put them there because they thought it was the right thing to do but through a lack of understanding or lack of testing, it wasn't. I've brought a couple of systems back from the brink by doing nothing other than dropping unused and duplicated indexes. Over indexing can cause chaos. It's your system, you need to understand and weigh the risks of leaving these indexes in place or dropping them. If you decide to go ahead with the drop, document what you do, why you're doing it, script the indexes and publish to all interested parties.