I am configuring IPv6 between a Juniper MX80 and Cisco 7206VXR which already happily run IPv4 over ISIS and iBGP. ISIS is carrying IPv6 routes between the two devices but I have what I believe is a problem. If we imagine a network like the following; 

With measurements of file size or memory I have always used multiples of 1024 (binary prefixes) such as 1KB file is 1024 bytes in size; 

I find my self in a situation I was in not too long ago, but I can't remember how I resolved it :) The Scenario I have a Cisco IOS router with a LAN interface (fa0/0) and a WAN interface (fa0/1), and 2nd WAN interface (fa0/2). 

The Kernel could be sending a packet over an Ethernet link so it will build Ethernet headers for the IP packet, or it could go over PPP, L2TP, IPSEC, GRE, MPLS, ATM, dial-up etc. The Linux Kernel is very flexible and can do many things, which means it has to make many checks and burn CPU cycles trying to accommodate for the many different networking scenarios it supports (A Linux host could be a router, firewall, switch, proxy, etc). DPDK supports IPv4 and IPv6 over Ethernet and that's about it (yeah OK IPSEC too, but not ATM for example!). DPDK provides only a sub-set of features the Kernel does so the code is more focused on it's task. Also on a side note, the process for getting updates approved into the Linux Kernel is more tricky but improvements are being made to the Linux Kernel all the time (see below). 

I think you have missed something more fundamental that has lead you to ask your question, but in fact the question isn't really valid. ADSL isn't for hosting providers or people with large uploads requirements. It’s a cheap service aimed at home users and small businesses that can be quickly and cheaply deployed, it has low overheads, and works over a variety of physical line qualities and lengths. So that makes it great for domestic roll out, home users can't afford to have their street dug up to lay fibres. You mentioned frequency division multiplexing, the division of frequencies can be moved with ADSL. With ADSL2+ for example Annex M reallocates some of the downstream frequency range for use as upstream frequencies sacrificing download speed to improve upload speed. Also with ADSL2+ the line speed can and will fluctuate through the day or weak as the physical line is susceptible to interface and the attenuation rate fluctuates. As a result the frequencies used by an ADSL2+ line for upload and download will be changing (more acccuractly, they will reduce to cope with periods of high interferance). But like I said ADSL/SDSL/ISDN/3G/4G/LTE are cheap services (if you live in a developing country they maybe the best money can buy, I'm of course talking relatively within the scope of available technologies, relatively these are for home users or small offices at best). There are products that bring symmetrical bandwidths to home users like EFM (bonded SDSL). VDSL brings asymmetric high bandwidths to home users here in the UK, up to 80Mbps download and 20Mbps upload delivered over the same telephone line that would have been used for ADSL/ADSL2+ previously but with a new modem. If you are a hosting provider for example and need symmetrical bandwidth you can pay for, and this is the crux of my argument, FULL DUPLEX connectivity (probably Ethernet based). ADSL and as per you example use FDM so that it can be full duplex. These days 100BaseTX, 1000BaseTX, 1000BaseSX/LX etc 10GBaseSX and so on all have and transmit paths in both direction so they have full duplex communications and there is no need for FDM or TDM so “split” the physical bearer channel into an “upstream” and “downstream” path like ADSL. The old saying, you get what you pay for. If you pay peanuts which home users tend to do, you get a single copper pair that has to use FDM to achieve bidirectional communications that is only asymmetric in available bandwidth. If you pay more money you get bidirectional high speed low latecny connectivity. 

I have a looked through the config and sure enough, there is no "mls qos" in the global configuration. What potential issues can happen from enabling this global config command on this production device? I can obviously perform this during scheduled maintenance period for safety, but should it even be attempted at all? If it makes any difference, there were no class maps or policy maps defined on this router until I logged in, to make one to rate limit the previously mentioned server ports (all ports are Gig, I have been tasked with limiting this SVI to 100Mps for a short period). I trying to use the following configuration; 

It's important that we shape here not rate-limit or police, so that traffic isn't dropped, it's "shaped" to the bandwidth available. Have a read of this Cisco page for some additional info. 

A Gibibyte is this many bits: which is more. So a link that runs at 111.11Mbps for 1 hour hasn't transfered a Gibibyte. Purely from a networking and technical view point here (not legal regarding fair usage policies or T&Cs etc), what is a best practice, always used mega/giga/tera or mibi/gibi, IEC or SI notation for measurements and monitoring etc? How do you keep everything uniform? 

Sometimes they also mention a 7 level QoS model. Can anyone provide me with a Cisco IOS config example of what is meant here. So by this I mean a config example showing how to implement this (assuming there is such a thing!). I am more familiar with IOS 12.whatever rather than 15.whatever but either would be great. I assume that these people are referencing a 5 class set up matching traffic that is one of the 5 standard classes below and processing the traffic appropriately; 

Yes there are hardware improvements, such as new copy instructions on x86 CPUs which are more efficient, and DPDK support these (e.g. the SSE3 instruction set). The major benefits are from how efficient the software is though and how well the software makes use of good hardware (e.g. making good use of fast CPU L1/L2 I-cache/D-cache). DPDK has support for various IPSEC and crypto features so if you have an Intel CPU with the AES-NI instruction set ("Advanced Encryption Standard - New Instructions") DPDK will use those instructions for faster AES encryption/decryption. 

I am configuring some policy statements on JunOS 11. I am filtering inbound BGP prefixes using the below syntax. It's fairly simply and self explanatory, you can guess whats it's doing just by looking at it. However I am unsure of the keyword. Routes come in from a BGP neighbor and go through the various tables and processes etc (too much to expand on here), is the a part of the OS that all received prefixes are passed through before hitting a process RIB (like BGP RIP of ISIS RIB) or the main FIB? By creating this policy statement does it create hooks inside the ? 

The configuration I have given is only applied outbound on the WAN interface (so I have guaranteed 1000Kbps of traffic to the voice VLAN for their upstream path towards the VoIP provider). What can I do to guarantee the downstream bandwidth of the voice VLAN on this 1841 only? Is it possible to create an inbound policy on fa0/1 (WAN) that prioritises packets that match access-list 145, as they pass through the router, so that other packets inbound on WAN don't affect the downstream speed of the voice VLAN (peraps queuing packets that don't match ACL-145)? Or is it possible to create a policy on the physical fa0/0 interface, even though it is split into two sub interfaces, that will guarantee 1000Kbps of the traffic out of it, towards the voice VLAN, when it needs it? 

I haven't applied any QoS policies anywhere yet, I have simply shown above how and where I intend to enable them. What I want to know before I enable them though, is how can I monitor the 'strain' (computational and resource overhead) placed on the router after applying these policies? My understanding here is that as we add more subscribers to the router with QoS enabled (not all would need QoS if they don't have VoIP services) more memory resources (buffers) and CPU cycles are going to be required because our ADSL users are often congested (it probably won't be so bad for Ethernet customers as they typically have faster access circuits). The question is two fold really; how can I monitoring buffer pool usage for QoS policies, and should I? Should I be measuring something else? I am already graphing CPU usage and total memory usage via SNMP (Cacti/Observium/etc), is that all I need, is just watching those figures sufficient? I don't want to pile on subscribers and QoS policies and then exhaust the routers resources and drop packets even though it still has plenty of bandwidth to spare.