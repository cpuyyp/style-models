You could try using a lerp (linear interpolation) function using the current client rotation and the last rotation received from the server. Because you always want to use the same time period to update the rotation you could reset a variable each time the server receives a message and increment it by delta time each frame. For example, if you want the rotation to be corrected over 0.5 seconds, each time you receive a message from the server reset the timer to 0.0 and increment it up to 0.5. 

I believe that what you're talking about is a large number of individual draw calls to the rendering api vs a single large draw call to the rendering api. There is CPU overhead associated with setting up the rendering state (binding vertex buffers, textures, shaders etc) as well as the actual draw call itself it is generally more efficient to render a large amount of geometry in a single call. Note that Panda3D most likely attempts to reduce rendering api state changes such as bound texture and shaders by grouping renderables together so these need to be set the minimum number of times. But even with this optimisation if you are drawing many single objects, you must call draw in the rendering api which incurs unavoidable CPU overhead while communicating with the GPU. The CPU-to-GPU overhead is something we want reduce as much as possible when using rendering apis such as OpenGL and DirectX. This is done in many ways but some optimisations that generally yield good results are to minimise unnecessary state changes and to batch geometry together to reduce the number of draw calls. This is what the flattenStrong command is doing for you. To say it again, it is generally more efficient to render 1,000,000 vertices in a single draw call than it is to render 1,000 vertices in 1,000 draw calls. Therefore you could call this a fundamental law of the rendering apis which most game engines use under the hood. 

Yes, you can use OpenGL to render 2D stuff and it gives you some flexibility. I regularly does this, but it is a bit trickier than using a simpler 2D only API. What you gain is accelerated scaling and rotating, and the step to 3D is not that big if you want to take that later on. The API itself is C, and so is the GL ES API (for iPhone), but you can use it from Objective C and C++ (and most other languages). The consoles usually use custom low level API:s, but the Xbox360 one looks a lot like DX9. 

OpenGL is used quite a lot for 2D games. We used it in Teeworlds for example. Using it as a pure 2D language is ok, it might give you some quirks when setting up camera and especially if you want stuff pixel perfect. But you will learn a lot by doing it. You can use OpenGL with SDL, it's very common. Most people doing it that way only uses GL though, and skips most (all?) of SDL:s drawing functions. 

Same thing for index data, but with GL_ELEMENT_ARRAY_BUFFER as target instead. You can change GL_STATIC_DRAW to something else depending on your usage patterns. 

OpenGL know nothing of fonts, so you have two choices. Either use bitmap fonts (a font written into a texture) or use Canvas/HTML to display your text. Most regular OpenGL games uses bitmap fonts. 

I usually keep my own format internally, and a standardised format externally. The artist only sees the standard format and my engine can only load the optimized internal format. This way you won't have to have model loading code in engine, it can all be done in some nice high level language as a compile step. I usually use a existing build system that rebuilds an art folder on demand, will only rebuild changed content. Pros: 

Firstly find the most efficient texture format that you can while still being happy with the visuals of the game whether this is RGBA4444, or DXT compression etc. If you are not happy with the artifacts generated into a DXT alpha compressed image, would it be viable to make the images non-transparent using DXT1 compression for the colour combined with a 4 or 8 bit grayscale masking texture for the alpha? I imagine you would stay on RGBA8888 for the GUI. I advocate breaking things up into smaller textures using whichever format you decided on. Determine the items that are always on screen and therefore always loaded, this might be the terrain and GUI atlases. I would then break up the remaining items that are commonly rendered together as much as possible. I don't imagine you would lose too much performance even going up to 50-100 draw calls on PC but correct me if I'm wrong. The next step will be to generate the mipmap versions of these textures as someone pointed out above. I would not store them in a single file but separately. So you would end up with 1024x1024, 512x512, 256x256 etc versions of each file and I would do this until I reach the lowest level of detail I would ever want to be displayed. Now that you have the separate textures you can build a level of detail (LOD) system that loads textures for the current zoom level, and unloads textures if not used. A texture is not being used if the item being rendered is not on screen or is not required by the current zoom level. Try to load the textures into video RAM in a thread separate to the update/render threads. You can display the lowest LOD texture until the required one is loaded. This might sometimes result in a visible switch between a low detail/high detail texture but I imagine this would only be when you perform extremely fast zooming out and in while moving across the map. You could make the system intelligent by attempting to preload where you think the person will move or zoom to and load as much as possible within current memory constraints. You can cache where the user has been until they move away. That is the sort of thing I would test to see if it helps. I imagine to get extreme zoom levels you will inevitably need an LOD system. 

Unless you are pre-transforming all of your vertex data, vertices should come into the vertex shader in object/model space. This is typically where the vertices are converted into screen space by transforming the vertex by the world, view, and projection matrices. If you want to perform operations on the vertices in world space, then either pass through the objects world transformation matrix in addition to a pre-multiplied world-view-projection matrix. Or pass in the world, view, and projection matrices separate from each other. You would then transform the vertex by the world matrix, perform all necessary operations on it, and then transform it by the view, and projection matrices before passing it through to the pixel shader. 

If you want alternative lighting calculations or shading effects (beyond color shift) you can do it in a few ways. The first way would be to create a separate object that uses the vertices you want to be rendered, and either render the original object without these vertices, or simply overdraw the same vertices with the new effect. The overdraw is likely to be more efficient than the time it would take to sort out vertex buffer and index buffer organization. You may also need to remove z-testing for the effects pass. The second way would be to modulate specific vertices color, and render them a very specific color that is not present in your game in any other fashion (such as a vibrant pink). After that, you could perform glow effects or whichever as a post processing step, that only acts on anything rendered using this particular color. The color of course is only being used as an obvious identifier. This method would likely be best if you are rendering many different objects in this way. Edit, also for this particular effect, you will have to avoid texture sampling in this situation one way or another, otherwise it will be blended with the modulated vertex color, producing inconsistent color variations. The last way I could think would be to actually implement branching code in whichever shader is applied to objects that could have this effect. I highly advise against this, as branching code is problematic and generally much slower on GPU's, and you REALLY dont want a default shader to have branching code in it. You could perform a shader swap though whenever this effect is enabled, and test the performance characteristics of it. This would also require additional data overhead, as each vertex would need a flag indicating whether it is currently part of the desired effect, or rendered normally. This is the worst of the three by far, but it is an option. 

We used Computer Graphics with OpenGL (Hearn & Baker) when I was in university and I liked it a lot. Despite its name, it gives the fundamentals of 2D and 3D graphics and rendering. OpenGL is used as example when doing 3D graphics, but only after the fundamental maths and algorithms have been presented. $URL$ After that, Real Time Rendering as already suggested. 

You might want to rethink motion blur and only do it for one frame, not blending between several. This will help you avoid a lot of edge cases where your frame rate is stuttering and you have to decide on which frames to blend and how much. A common way to do this is using motion vectors, where you render your scene again, with extra data for each mesh, and render screen space velocity into a separate buffer. This buffer can then be used as a blur direction control. to compose the final blurred image. You can also build depth of field information at the same time using this technique. 

Don't roll you own engine. Roll you own game. If you happen to write an engine at the same time then good for you, if not you can always refactor whatever parts you might want to reuse to make it more "engine" like. People often over estimate what it takes to write the "engine" part of a game. If you only do what you need it won't take that long. The hard part is to not get stuck writing infrastructure and to only write what you absolutely must to solver your problem. I would use an existing engine when: 

Box2D worlds use damping and friction, consider that the world may be applying a damping to your objects as they are simulated. 

To understand how 2D animation works, you should understand how textures are mapped to a surface. When you pass in vertex data along with a texture to the graphics card, UV coordinates are passed in with it, to specify a (usually) normalized position on the texture that the vertex is located at. For instance, when rendering a sprite to a single billboard, you would usually pass in uv coordintes like so. Vertex01: (0,0), UV: (0,0) Vertex02: (0,1), UV: (0,1) Vertex03: (1,0), UV: (1,0) Vertex04: (1,1), UV: (1,1) This would map the entire texture to the billboard, which is fine for a single image, but if you want to use a sprite sheet for animation, then you would want to map to a portion of it only. So lets say you have 4 sprites of even size on a sprite sheet, and you wanted to map the first frame of the sheet to a billboard, these UV coordinates would allow you to do map only 1/4 of the texture to the billboard. Vertex01: (0,0), UV: (0,0) Vertex02: (0,1), UV: (0,0.25) Vertex03: (1,0), UV: (0.25,0) Vertex04: (1,1), UV: (0.25,0.25) So if you wanted to change the frame on this billboard, you would update the UV coordinates based on the dimensions of the frame. On top of this system, you would need to implement the animation framework that updates the animation, at the frame rate you want. Keeping track of the time since the previous frame update, you can ensure you are only changing frames every n milliseconds to achieve the framerate you want. 

Box2D was written as a tutorial on how to write 2D physics engines. The site contains links to a lot of presentations on how the algorithms work and how to make it efficient. You might also want to look into this presentation done by the Pixeljunk Shooter team, describing how they did 2D fluids. If you are interesed in 3D, read Realtime Collision Detection. For more cutting edge offline physics this might be a good presentation. It describes how a new solver was developed for Maya, and I remember our physics programmer going nuts over it some years ago. 

There are a couple of ways to reduce data. First see if you can drop from float to some other format. Can you use 16bit integers for position and store a scale per mesh? Can you compress normals? Two floats might work, or two integers, or use a format like GL_INT_2_10_10_10_REV. Compress other vertex data in similar fashion. How do you render your data? Triangles? Try using triangle strips and reuse vertex data as much as possible. Use primitive restart if you cannot render your whole mesh in one strip. Use something like NVidia Mesh Tools to generate this data. At least something to start with. =) 

The Art of Demomaking from the old flipcode archives might be a good start $URL$ The whole site has a lot of stuff, but is mostly PC centric although pre 3D hardware. $URL$ 

You seems to render without depth test on. This might make triangles further away render on top of closer triangles since they might be rendered afterwards. The solution is to turn on depth testing in your render setup and set the depth compare function to Less or LessEqual. Remember to turn on depth write too if you happen to have that turned off. 

Unless you are leveraging the tan/bitan for normal mapping, and you have some really strange normal maps, that is unlikely to cause the issue. It looks to me like you are performing per vertex lighting calculations in the top frame, and the bottom is per pixel. Or, the position data you are using to perform attenuation calculations is a bit odd. Notice each block seems to have a flat shading, as if every pixel for each block is being attenuated using the same distance calculation. 

If the memory footprint of the individual map segments is small enough, then you could easily get away with simply loading them in when the world loads. If on the other hand you are limited on memory, then I would instead implement a streaming system. The streaming system would allow you to divide the world into separate zones, and load only the appropriate resources depending on which zone you are in. You could also set it up to simply load in the neighboring tiles in addition to the current tile, and then update the loaded tiles whenever there is a transition. 

This is the approach i've seen implemented in a number of games, and while it isn't a quick and dirty approach, it is a relatively good design I feel. When reacting to input within the game, you want to provide the ability to bundle functionality into particular events or actions, as well as provide access to raw values for something such as mouse positions. Dispatching raw input is relatively straightforward, and providing the ability to react to specific events allows you to decouple your application code from the input itself. For instance, if you have your "CharacterController" class (which would implement an input listener interface) listening for jump events, then the character controller doesn't care in the slightest bit whether jump is space bar, or left-ctrl. Without this event based decoupling, any changes to your input map would require additional changes to your game specific code. Writing an input manager that acts as a central dispatcher, and uses an input map (a list of keys mapped to actions) to distribute events to classes that implement an input listener interface is the summary of my proposal. At its simplest the listener is just a pure virtual function accepting an input event, and the input map a set of key/event-list pairs. 

A lot of architecture information hare hidden behind NDA:s so it might be hard to give a good answer accurate to the current state. You might want to read this nvidia article that describes how the GeForce 6 works. Of course a lot has changed since, vertex/fragment units have been unified and can be load balanced, but the overall architecture (that you need to know about for writing general apps) is pretty much the same. More shading stages has been added, and a few more fixed function steps (tesselator etc.) These works very much the same way as the steps in the nvidia article, except that they pull computation resources from a pool of units. As said by seanmiddleditch in his answer, AMD has released a lot of specifications of their chips. You can find them here. Especially the Radeon R6xx/R7xx Acceleration PDF should be of interest to you. 

Ambient lighting is just a constant term modulated by surface color. Sky light can mean many different things, it might depend a bit on engine/library/app: 

One quite efficient representation is an array of matrices combined with an array of parent indices. If you keep things sorted, and updates is just a loop over the array. See Practical Examples in Data Oriented Design by the BitSquid guys. You might not need to resort that much depending of your change patterns. 

As you are just doing this for yourself and are not aiming to get employed as a game programmer, I would suggest sticking to what you know: Use C#. There seems to be a lot of movement doing gamedev in C# with tools like XNA and Unity3D. There has also been some cross platform games like SpaceChem delivered using C# on mono. You can always try to do stuff in C or C++ just to learn something new, or fresh up on old knowledge. Low level programming is not going away, but it might not be your main focus.