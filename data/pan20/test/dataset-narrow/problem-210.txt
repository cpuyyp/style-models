The Full Text Indexes are solving a different problem from creating plain text. However, my experience is that large text bases usually benefit from Full Text Indexing anyway, since someone is always trying to find something I understand that once you get the text you will redundantly store it in a SQL Server table so as to expedite your use of the plain text data. Automating the Extract and Load This will require some work. But, since your documents are currently in a FileTable you should be able to access them from the file system using file tools. The "off-topic" answer to plain text extraction below includes several tools that are being used by others. Perhaps some of these tools would be useful to you. 

The KILL command cannot kill a UOW of {00000000-0000-0000-0000-000000000000}, so that leaves you hanging on what to do next to get rid of it. To get rid of spid -2 with the unit of work {00000000-0000-0000-0000-000000000000} you can do something, but it has side-effects: 

How your organization views your (person or businesses) will help you determine what is the best approach for your business. What would I do? I would tend would separate the from the more complex . When needed a summary of the totality of Orders, Subscriptions, and (returns, reshipments, etc) can be produced for your company and for your customer. 

Use your computer's mouse to move the cursor to the first character to be included. Next Hold down and continue holding down the key, then move the mouse to include the last character to be included. Continue holding the key, then press the key which will execute the highlighted text. 

This was posted in a discussion of log files some years ago at: $URL$ EDIT: As a further option, if you are using Enterprise Edition you can use database snapshots for reporting. The database in this case remains in restore mode and the snapshot(s) read only. If this is your environment, you could indeed use log shipping to keep your Reporting Server data up to date. 

The fundamental issue is: Which server processes the query? When using a linked server query, the query is executed on the LOCAL server and uses the linked server to access data on the remote server. (Linked servers are subject to several limitations, some caused by running through the Distributed Transaction Coordinator (DTC).) When using OPENQUERY, the query is sent to the REMOTE server and is executed there in order to return data to the local server. (You will notice that in OPENQUERY the query uses a three-part name, since it all runs on the remote server, rather than the four-part name used for the linked server.) A simple exposure to the limitations can be found in this thread: $URL$ It mentions network topology, data types, type of remote server, the code in the query, etc. EDIT: See more on the Guidelines for Using Distributed Queries at: $URL$ The subheading Other Guidelines mentions: "To create the best query plans ..., the query processor must have data distribution statistics from the linked server. ... If the linked server is an instance of SQL Server, to obtain all available statistics, the user must own the table or be a member of the sysadmin fixed server role, the db_owner fixed database role, or the db_ddladmin fixed database role on the linked server." If your connection to the remote server does not have access to those statistics that might cause a poor plan. Ideally not an inaccurate plan, but the query processor is imperfect. 

It would be interesting to understand why you want Level = 100 when you do not want the code to change. If you install SQL Server 2008 R2 the SQL Server features available for the system (such as compressed backups) will work without regard to any database's compatibility level. So, keep your database (a little while longer) in compatibility level 80. EDIT: Responding to your comment on Mirroring. Mirroring is a server feature first introduced (for production use) with SQL Server 2005 SP1. Since this is a feature of the server, it will work for all compatibility levels running on your SQL Server 2008 R2 server. Therefore, keep your compatibility level at 80 (SQL Server 2000) level and use mirroring on the databases where you need it. There is a lot of documentation on mirroring, but since this is new to you, you should start by reading Ron Talmage's white paper at: $URL$ Please note, however, that SQL Server 2008 R2 is the last version to support SQL Server 2000 (80) compatibility. And SQL Server 2008 R2 is no longer under mainstream support since 7/8/2014, though the limited extended support will continue into 2019. This means that for your SQL Server next upgrade you must upgrade your T-SQL code. END OF EDIT Since you mention DDLs and EXEs, you seem to be indicating that your SQL queries are in the client not existing as in the SQL Server. If you decide to write a wrapper code for use in your EXEs and DLLs and include it in all the locations. That sounds as disruptive as just changing the SQL Queries. Since you are using the *= syntax your code should look like this: 

require . Perhaps you need to bill them so as to be paid. have no . Perhaps subscriptions are automatically paid through a credit card, PayPal, etc. 

I do not have specific experience with this, but I see a few posts that have not been answered successfully, so just pointing you to other resources. 

Mladen PrajdiÄ‡ also wrote a solution some years ago using Service Broker, which is available in SQL Server Express. See the links at: 

There are doubtless many blogs that discuss this, but this post by Kendra Little (at BrentOzar.com) discussed the issues that concern you. See: $URL$ Kendra discusses some of the problems that arise and how you can test for potential problems. After going through what problems SNAPSHOT isolation may cause, you need to also realize that READ COMMITTED isolation level is not as 'simple' as some might think. 

In other words, the transaction completes successfully or else (in a failure) all the in-flight changes are undone since the transaction is incomplete. 

According to: How to reduce paging of buffer pool memory in the 64-bit version of SQL Server ($URL$ makes it appear that this is asynchronous: 

All password authentication for is at the server level for a login. Therefore the database will have users but does not have per database password for those users since the login is the authentication point. If you have enough rights, you can change the passwords of the logins and use a different password, but that password would be the same for all databases on your server. If you need a special login for a database, then you should create another login, e.g. or some such pattern that suits you. Then you grant that login rights to the database. 

Retain the OLD NAS until the expiry of your retention period for those backups. Create your directory structure on the NEW NAS and redirect your backups there. Once your retention period on the OLD NAS expires, then drop use of the OLD NAS. 

You are correct that the BEGIN DIALOG CONVERSATION does not specify using a TRANSACTION for starting a Service Broker dialog. But many developers use them as a standard practice. For example, the 2008 R2 TechNet page Beginning a Conversation and Transmitting Messages seems to recommend and (or ) as a good practice. This topic Configuring Service Broker for Asynchronous Processing also shows transactions being used. These multi-step examples that use transactions do so for the same reason transactions are used in other code. Namely to ensure either a complete transaction or else a rollback to a consistent state. Of course, you can decide if your Service Broker dialog is simple enough that you do not need a transaction. But that is up to you to decide. 

If I understand you correctly, your major problem is the log files during the several hours of backup. From your opening statement I understand that the 1.5 TB database normally runs in SIMPLE recovery, and thus no log backups to do. Disclaimer: I have never done log shipping on this scale. Of course, you should ask whether you can get more space allocated for your log files. If you can, then great. However, I think a small modification of your plan, providing that you already run in SIMPLE recovery model and/or the risk of SIMPLE recovery model for a few hours is worth it, would ease some of your worries. 

If relatively few of the XML columns are actually needed in your query, then you could join the candidate XML columns to the Unique Clustered Index, perhaps greatly reducing the total amount of data being read. This may well result in a faster execution plan. Of course, if you actually need every XML column then there would be little benefit. But that depends on the factors described in the linked article.