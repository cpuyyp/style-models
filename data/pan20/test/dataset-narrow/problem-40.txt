Literature on rendering volumetric materials and effects tends to use a lot of mathematical physics terminology. Let's say that I have a decent handle on the concepts involved in surface rendering. What concepts do I need to understand for volumetric rendering? (Both real-time and offline rendering.) 

I think you may want to take another look at the iOS user interface if you consider real-time blurs to be out of range of mobile hardware: 

Here's an example of a downsampling filter that takes pixel geometry into account: Increasing image resolution on portable displays by subpixel rendering. Image (a) below is an image downsampled using pixel-based downsampling. Image (b) is downsampled using direct subpixel-based downsampling, which (as far as I can tell) effectively downsamples the R, G, and B planes of the image independently. Image (c) is downsampled using diagonal direct subpixel-based downsampling, which uses a diagonal pattern to improve apparent resolution in both horizontal and vertical directions. 

The near clipping plane is a fundamental feature of projective rasterization. To borrow a diagram from Eric Lengyel's Projection Matrix Tricks presentation: 

Each vertex of the sphere can have a randomly generated gradient for gradient noise. To get this information to the pixel shader (unless you want straightforward interpolation like value noise), you may need a technique like this article's wireframe rendering with barycentric coordinates: do unindexed rendering, with each vertex containing the barycentric coordinates of that vertex in the triangle. You can then read from (or the OpenGL equivalent) in the pixel shader, read the three noise gradients from the vertices based on what triangle you are on, and use whatever noise calculation you like using the interpolated barycentric coordinates. I think the most difficult part of this method is coming up with a scheme to map your triangle ID to three samples in order to look up the noise values at each vertex. If you need multiple octaves of noise or noise at a finer resolution than your sphere model, you may be able to do a coarse geodesic grid with vertices and do a few levels of subdivision in the pixel shader. i.e. from the barycentric coordinates, figure out which subdivided triangle you would be in if the mesh was further tessellated, and then figure out what the primitive ID and barycentric coordinates would be for that triangle. 

Doing math with uniforms is a shader won't usually get you any performance over doing it on the CPU. A CPU isn't slower than a GPU at doing matrix math, it just isn't structured so as to do large amounts of math in parallel. But you have to actually do that large amount of math to get a win. Sending extra data to the GPU just to have the GPU multiply two matrices together will rarely net you anything. Now, say you have a buffer of skinning matrices. It might start to matter whether you transform them all into world space on the CPU or just pass an extra model-to-world-space matrix to the GPU. But even then it depends on your ratio of vertices to bones. 

$D(\omega)$ is defined as the area ($m^2$ unit in the numerator) of the microsurface with normals pointing in the direction $\omega$. $\mathcal{M}'$ is defined as the portion of the microsurface with normals point in the direction $\omega \in \Omega'$. So it's natural that the integral of $D(\omega)$ over $\Omega'$ gives the area of $\mathcal{M}'$. 

If you are rendering a sphere, not just evaluating noise on the surface of a sphere, and are fine with tessellating your sphere to the resolution of your noise lattice, you can create a geodesic grid on the surface of the sphere (a subdivided icosahedron, usually): 

Consider using spatial hashing, especially if your objects are similarly sized. Basically, divide your world into uniformly-sized grid cells (2D and 3D are both valid possibilities depending on the amount of vertical motion). Each update, assign your object to each bin that it overlaps—if the cells are decently sized relative to the objects, most objects should end up in a single bin. Each bin is inserted into a hash table, with the key being the coordinates of the bin. (You can also think of it as a hash table with multiple values for the same key, and inserting an object once for every cell that it overlaps.) There's no hierarchy to rebuild in this scheme, which makes it well suited for dynamic scenes. You can still test the cell's dimensions against the frustum or against occluders at a coarse level and discard many objects at once. Also, it's easier to manage this structure incrementally—you can keep the hash table the same from frame to frame and only move objects from one bin to another when they cross the boundary of a cell. 

The result, using wording from their slides, is that "the ray can never go through the Smith volume; the model creates an opaque surface-like interface." Is this model compatible with traditional diffuse subsurface scattering, where a path can travel a perceptible (macro-scale) distance through a surface before exiting? Or is this simply a specular BSDF with no intention of modeling long paths internal to a surface, to be combined with a diffuse BSDF that adds that component? (Does the microflake model of volumetric rendering normally separate diffuse and specular, or is "diffuse" simply a path that bounces around so many times that the outgoing direction is uniformly distributed?) 

Generally speaking, path tracing removes a number of assumptions that ray tracing makes. Ray tracing usually assumes that there is no indirect lighting (or that indirect lighting can be approximated by a constant function), because handling indirect lighting would require casting many additional rays whenever you shade an intersection point. Ray tracing usually assumes that there is no such thing as non-glossy reflection, because while it is fairly easy to handle glossy/mirror reflection (you reflect the ray and continue with the raycast), if you have non-glossy specular reflection you have a problem very much like indirect lighting: you need to trace a bunch more rays and then do lighting wherever those rays hit. Ray tracing also usually assumes that there are no area lights, because it is very cheap to cast a single ray towards a light to determine if an intersection point is in shadow, but if you have an area light you generally need to cast multiple rays towards different points on that light to determine the amount that the intersection point is shadowed. And as soon as you need to break any of those assumptions with ray tracing, you now have a sampling problem: you need to decide how to distribute the many additional rays you need to trace in order to get an accurate result that doesn't have noticeable aliasing. 

Pixel screen-space derivatives do drastically impact performance, but they impact performance whether you use them or not, so from a certain point of view they're free! Every GPU in recent history packs a quad of four pixels together and puts them in the same warp/wavefront, which essentially means that they're running right next to each other on the GPU, so accessing values from them is very cheap. Because warps/wavefronts are run in lockstep, the other pixels will also be at exactly the same place in the shader as you are, so the value of for those pixels will just be sitting in a register waiting for you. These other three pixels will always be executed, even if their results will be thrown away. So a triangle that covers a single pixel will always shade four pixels and throw away the results of three of them, just so that these derivative features work! This is considered an acceptable cost (for current hardware) because it isn't just functions like that use these derivatives: every single texture sample does as well, in order to pick what mipmap of your texture to read from. Consider: if you are very close to a surface, the UV coordinate you are using to sample the texture will have a very small derivative in screen space, meaning you need to use a larger mipmap, and if you are farther the UV coordinate will have a larger derivative in screen space, meaning you need to use a smaller mipmap. As far as what it means in less mathematical terms: is equivalent to . is simply the difference between the value of at pixel x+1 and the value of at pixel x, and similarly for . 

I'd consider just going with 3D noise and evaluating it on the surface of the sphere. For gradient noise which is naturally in the domain of the surface of the sphere, you need a regular pattern of sample points on the surface that have natural connectivity information, with roughly equal area in each cell, so you can interpolate or sum adjacent values. I wonder if something like a Fibonacci grid might work: 

Antialiasing of 2D shapes boils down to computing the fraction of a pixel that is covered by the shape. For simple non-overlapping shapes, this is not too difficult: clip the shape against the pixel rectangle and calculate the resulting shape's area. But it becomes more difficult if multiple shapes overlap the same pixel. Simply summing areas can cause the computed coverage to be too high, if it neglects the amount that one shape covers another shape. For example, see the Limitations section of this article on font rendering. You might also be in a situation where the two curves come from different objects with different colors (so it's not about the total coverage for the union of the two shapes, but the coverage of each one separately). How would this be computed if you cared about complete accuracy? Even trickier, how do you compute coverage accurately for overlapping non-polygonal shapes like curves? Is there some point when you have no choice but to fall back to multisampling or stochastic techniques?