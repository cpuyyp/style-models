- Bonus To complete the picture, here's how a uni-directional BFS looks based on the same building blocks (, ): 

Post Scriptum These are random notes. Specifically, I didn't mention important things (like brace style, const-correctness, use of raw arrays etc.) 

The isn't actually much more complicated. I dropped the in favour of more obviously readable code. It has the added benefit of making the code short (except for the constants of course). Functionally, we show 

Note this is still basically exactly the same algorithm (modulo the bug you had in the condition), but it is now much clearer what is happening, and why. Note that I moved the logic to check for previously-visited actors inside to avoid duplicating that logic: when reviewing your code I had to think really hard whether this block of code was actually necessary/correct: 

Big ideas I noticed that "Stopwatch" was a bit of a "God Object" antipattern. It does too many things (you can't really usefully do a countdown and a lap time simultaneously).ยน I reckoned it would be nice to simply have tasks (without any UI) that expose duration measurements, and views that can display them as they update. This is akin to a publish/subscribe pattern. To demonstrate this, I made not only the views generic, but also the tasks. The UI will be an interactive terminal application that supports the following short cut keys: 

Okay, after I worked my way through the original code, a few things have become clearer. Since I have never done programming with I was eager to try my hand at a better design. Here it comes. It's a sketch only in the sense that I didn't create separate translation units. That is basically a tedious exercise and left for the reader. However, it does implement stopwatch (including lap times and reset), countdown and a bonus "random timer" task. 

The BFS State I've defined this as simply the "grouping" of things you have prefixed in and flavours. 

Using Memory Mapped Files And Spirit Taking the approach from my answer over at [SO]: $URL$ I changed the implementation of into the equivalent: 

Wow, my machine is noisy. I don't really know why it's that bad, but moving on.. The version that explicitly shares tails performs better in the small test, by a notable margin. At those sizes, the memory use doesn't have a significant performance impact. But in the larger test, it's notably slower. At that size, the cost of the additional memory use is visible. A side note from the larger test is that the performance difference between the naive version and my hand-fused version has become significant, and the hand-fused version is slower. I'm not as good of an optimizer as GHC! (I suspect that the issue is that I'm counting up and comparing against a non-zero value in my loops. I could test, but that's getting pretty far-afield.) In summary, I'd use the naive version. It might be a little slower in small cases, but when things get big, it starts to get ahead. And it definitely has better properties about memory use at any size. Sometimes duplicating work is faster than caching it. 

As for being idiomatic, the only thing I'd change is the use of the variable to . Using when is available as a name is like using when is available. There is significant redundancy in , though. A sufficient implementation looks like this: 

I was suspicious that your initial code was basically as good as possible. It's already algorithmically better than your second example. is immutable linked list. There's no way sorting values is better than just generating the correct values in the first place. So, I decided to benchmark your initial version vs a hand-fused version that is guaranteed to allocate only exactly what it needs. 

So is this better? Well - it depends. It should be better when the cost of allocating the suffixes is higher than the cost of keeping them around. The memory use of keeping them in memory isn't insignificant. Eventually it starts to really make a difference. Here's a new set of benchmarks: 

And in fact, that's exactly what leaning on the monad instance for iteration looks like. A couple of binds and a return. 

If you need your function to be non-destructive of the original matrix, at your function end, to undo this, you could loop on: 

A single simple in-place 2D matrix is much faster than what you have coded (20x faster in fact). Thus, it's all about performance. And, the algorithm matters. IMO, it's appropriate to talk about alternatives because yours is not in the ballpark range as to what is possible/required in performance. It doesn't meet the "most efficient" criterion. In an actual interview situation, this would probably be flagged by the interviewer. Note that, if you were [much] closer, say within 10%, this wouldn't be an issue [and I wouldn't have posted]. I'm not sure that you can meet the objective without a full refactoring of your code. Your solution puts an additional strain on the processor's cache and seems to have more complexity than would be needed. It also seems to use more memory than is required as well. And, a number of the STL primitives you are using, by their nature, appear to access the heap a lot (i.e.) they're slow. I'd say that simply clearing out cells as you traverse would be better than adding the complexity you have [or see below]. Also, for your algorithm, do you have benchmarks on and analysis of how much performance is taken up by each of the STL components you're using. That would probably be required when discussing the time/space tradeoffs. As it is, they are a bit of a black box. Is there a better alternative to for your matrix? I think it adds an extra [internal] pointer dereference [that the optimizer might be able to elide] for each cell access. And, for example, I see a better alternative to using a separate to keep track of visited nodes. When a node has been visited [used], simply OR it with 2. Eliminate your altogether. Then, you can replace: 

Anyway, here's a full program that does comparison benchmarking. Ignore most of it, and compare the and functions against your function. They are largely agnostic. The primary intent here is to back up the 20x performance benchmark above, rather than just stating that without proof. It will also allow, if you so choose, to provide a baseline reference for any recode/tweaks you may wish to do 

The effect of sharing Some comments on the question reminded me that sharing is possible in this problem, to some small extent. In particular, the suffix of characters can be shared from one list entry to the next. 

I changed the definitions to be functions that take the max size instead of infinite lists in order to prevent memoization from breaking the benchmarks. Here are the results I got: 

You could argue that your second pattern was an optimization in the case that the second list is empty, but the tradeoff is adding O(n) overhead every time the second list isn't empty. There's a reason doesn't do that check for . The third case was pure overhead, and always an improvement to remove. 

And the results say... They run in basically exactly the same time. The differences are well within the standard deviations in the measurements, and my computer is apparently a very noisy system to benchmark on. The conclusion I draw from this is that the naive code is basically as good as possible. The optimizations present in GHC and its libraries make it exactly as good as the far-more-complicated fully hand-fused implementation. 

It turns out that this is exactly what does in the monad. For the sake of completeness, let's look at exactly how works. 

If you want the laziness of , you'll need to write this using . Doing that requires a trick - the same trick as is involved in writing as . I wrote in some detail of my derivation of how that trick works here. For what it's worth, using that trick to write using looks like this: 

The big thing I can see is that you don't seem to know why to use a mutable structure in Haskell. They're not automatically faster. In fact, they have some GC-related overhead that exceeds that of immutable structures in GHC. Mutable structures in GHC are only a win if they can actually reduce the amount of allocation, and this implementation looks like it allocates a lot. When you consider using a mutable structure, you need to examine how mutability will reduce total allocation. If it doesn't reduce allocation (and usually by an asymptotic factor) it's unlikely that mutability alone is going to help performance. 

Edit: This is probably what I should have said at the outset to make things more clear: Since your interview question is concerned with performance: 

If the statement (e.g. ) is wide, this burns a lot of horizontal space and the comment will probably exceed 80 columns. Consider such a comment to be the "topic sentence" of the paragraph. When it's a sidebar, it's effectively like putting the topic sentence at the end of the paragraph. Most style guides recommend keeping the width to 80 columns or less. I prefer ANSI comments (e.g. ) over K&R (e.g. ) as they're much easier and faster to edit. Try to avoid "if-else" ladders if possible. I replaced one of yours with a and the other, in the loop, using a I prefer to put the return type of a function on a separate line [in column 1] and so the function will appear in column 1 on the next line. This makes it easy to find them with a regex like which can make things easier when editing or analyzing source. Never put multiple statements on a single line. This is very hard to see visually when looking at a lot of code: 

Edit: This was my original opening section, which has been getting dinged. I'm leaving it in, for reference, but after rereading it, although it might have been tightened a bit, it does talk about the performance issues Caveat: This isn't a critique of your code style [as Zeta has already done that], but rather an alternate algorithm that can be 20x faster. A single simple 2D matrix can be much faster than using the primitives. As you were doing a interview question, demonstrating proficiency might be paramount and this might be a moot point. But, when the performance difference is an order of magnitude faster, that may make the difference. I've had a few related interviews and speed sometimes matters more. Assessing such a tradeoff may, in fact, be part of the requested/desired solution. To eliminate boundary checks, I've created an oversize matrix that has a border of zeroes on all sides. The actual data matrix is inlaid from . This is a technique used in some video/image processing. By using pointers instead of index variables, this also eliminates a number of multiplies within the loop.