There is, in fact, a subcategory of chess problems, the goal of which is to determine whose turn it is to move in the given position. Examples here. So while you need the "who moved flag" in general, for all the reasons given above, it seems to me that classification of all types of positions where you don't need the flag is a genuine research problem. Another question: Given a mover-determinable position in chess on an n by n board, what is the complexity of determining who moved? So you might want to look at things like that, if you want a theoretical project out of the problem. 

In terms of searching by text meaning, arguably the most advanced search apparatus is IBM's Watson. There is a lot of information here. They don't keep a databse of meanings. Rather, they search for answers to questions where the information of the answer is arranged so that it has a high probability of corresponding to the information of the question. So they don't assume they know the meaning of either the question or the answer; the objective is to look for a likely connection. 

Depending on what you mean by "capture," Soft Linear Logic and Polynomial Time by Yves Lafont may be of interest. There is a 1-1 correspondence to proofs in this logic and PTIME algorithms that take a string as input and output 0 or 1. The Wikipedia article on Linear Logic is here. It's not a fixpoint logic. The intuition of "classical logic over $C^*$ algebras instead of boolean algebras" is the easiest for me to grasp. 

In Sparse obstructions and exact treewidth determination, Lucena states that in the PhD thesis of Sanders, "75 or so minimal forbidden minors for treewidth $\leq 4$ are given, and it is believed though not proven that this may constitute the entire obstruction set." 

An association scheme is defined as a pair $(V, R_0,R_1, \ldots,R_{n+1})$ of a set $V$ and relations $R_i$ on $V$ such that 

If the only thing that can happen is that characters disappear, I think you only need to solve the longest common subsequence problem. (A subsequence is a generalization of a substring: there can be multiple locations in the subsequence where material was removed from the larger sequence, not just at the start and/or end.) Beyond that, have you seen this list of metrics? I may be misunderstanding your problem statement, but it seems to me that if you define precisely how errors can occur (deletion, transposition, etc.) and then build a suffix tree, it should be possible to have a pretty fast algorithm, after paying the memory and preprocessing cost of building the suffix tree. 

Computability and Recursion, by Soare. $URL$ This paper is the first of the history of computation papers available here: $URL$ 

I'm especially interested in their use in model checking applications. I have Open, Closed and Mixed Networks of Queues with Different Classes of Customers by Baskett et al. Any other suggestions for reading material? Thanks. 

I am also interested in implementability. My not-very-serious-yet review of Dorn's paper makes me think his algorithm might be usefully implementable. Motivation here: most (empirical definition of most) molecular graphs are planar. The subgraph isomorphism implementations for molecular graphs have been optimized over a period of years, so they would be hard to beat in practice. Still, if this is a fundamentally new approach without a lot of hidden constants, then there might be something really exciting here. 

I would like to ask for help in compiling a list of as many TCS-related conferences and workshops as possible. My main motivation for doing this is to plan possible blog coverage of more theory venues -- finding correspondents attending these events who would be willing to write either brief or in-depth blog entries about events they are attending. Beyond that, I hope a list like this would give everyone a better sense of the lay of the theory land. I'll seed the question with an answer containing a few "obvious" conferences. Please feel free to edit my answer and/or post additional answers of your own. 

Is computability theory part of TCS? If so, then Computability Theory and Differential Geometry by Bob Soare, which exposits applications of results he obtained with Csima, is an example. Don't know why the link isn't showing up.... Here: $URL$ 

Here, C is "simple" Kolmogorov Complexity, $\chi$ is the characteristic sequence of $L$, and $c_L$ is a constant depending only on $L$. Intuitively, this constant is the number of bits required to describe the minimal DFA for $L$, plus some overhead. Finding such a minimum Turing program for $L$ might be hard....... This is discussed in detail in Section 6.8 of Li and Vitanyi's book on Kolmogorov Complexity. 

The Wikikpedia article on polygons provides a partial classification, but I would like something more complete. Also, I am not concerned about how to name a 90-sided figure. Rather, I am trying to find a list of classes that contain infinitely many figures each (examples: star polygon, isothetic polygon). For example: actual words for "looks like a spider," "would look like a football if it were smoothed out," and so on. I've already checked The Geometry Junkyard and several pages of a Google search, but perhaps I didn't know what to look for. Thanks very much. 

There are two main types of processor failures in distributed computing models: (1) Crash failures: a processor stops, and never starts again. (2) Byzantine failures: processors behave adversarially, maliciously. My question is: 

One of the main things people use computers for is searching. Programs like Google are even called "search engines," and they are used millions of times a day. A computer recently beat the humans on Jeopardy because it was able to search through tons of data, super fast. But some things are hard for even computers to search. Sounds weird, doesn't it? One example is reverse multiplication. Of course if I say "What's 5 times 3?" you can say "15" in a nanosecond, whooosh! But what's the answer to, "What two numbers mutliplied together equal 21?" (Wait for the answer, 7 x 3.) Right! Now, what two numbers multiplied together equal 23? (Wait for the answer, or for frustration.) The only two numbers multiplied together that equal 23 are 1 and 23 itself. That took some thinking, didn't it? And 23 is a small number. Think if the number were hundreds of digits long. And the thing is, the best programs in the world can't reverse multiplication much better than a 7-year-old might try to, just testing one number, and then the next, and then the next. Computers can do it faster, but we don't really know how to tell a computer to do it smarter. People get PhD's in this stuff, and they only know how to tell computers to do reverse multiplication a little bit smarter. So maybe there is no smarter way. But maybe there is, and we just haven't found it yet. That's the P/NP problem in a nutshell: if I can recognize an answer right away -- 1 times 23 is 23, duh -- does that help me search for the answer faster? People think it's so important that the person who figures out the answer, yes or no, will win a million dollars. 

Sure. Write routines that compute each of Godel's primitive recursive functions, and write a routine for an unbounded search operator. The set of functions you can compute this way is equivalent to the set of functions Turing machines can compute. More information here. The downside is that any input to your universal program would need to be framed in terms of those simple operations. Of course, that is what compilers are for. 

Regarding your "edit question": it makes a big difference if you are asking about computability or complexity. If there are complexity bounds on the TM, then you obtain the so-called random oracle model. If the TM can use arbitrarily large-but-finite resources, then you're in the world of relative randomness: there are randomness hierarchies of oracles, much as there are Turing degrees. (Side point: one of the (in)famous critiques by Koblitz and Menzes was about the use of the random oracle model, so your meta-question is touching on recent academic debates.) 

What are the best ways to solve problems, and what problems are too hard to solve? There's a word in European languages -- including English! -- "informatics." The science of information. In the USA, we call this theoretical computer science, because of the strong computer industry here, but think about problem-solving without computers for a minute. Consider the human body. It solves problems in almost a miraculous fashion. Light comes into our eyes, and we can see things we recognize. Sound comes into our ears, and we hear words we understand. These are information problems we solve easily, thousands of times a day, that the best computers in the world still struggle with. It took the process of evolution millions of years to solve those problems, using a strategy of trial-and-error, and killing off the unlucky. Imagine what we might accomplish if we took a more rational approach, and invested as much human creativity into this new science of problem-solving as we have invested into geometry, theology, or calculus. What I do is one small part of that investment. 

In Planar Subgraph Isomorphism Revisited, Frederic Dorn obtains an improved algorithm for Planar Subgraph Isomorphism, by using a technique he calls Embedded Dynamic Programming. This technique appears to me to be a generalization of dynamic programming, in that it builds up from the smallest possible instances, but then, to obtain the next stage, one considers only a relevant subset of the already-computed subinstances, instead of considering all of them. I am interested both in the planar subgraph isomorphism problem, and in the embedded dynamic programnming technique. A search on the phrase "embedded dynamic programming" is unhelpful, and it seems to me that this idea is close enough to dynamic programming that (some variant of) it might have been used in the past, for another problem, under another name. Does anybody know anything about this?