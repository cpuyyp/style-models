When you use the nodes() method with the XML column from the table, you must use the APPLY operator. Example: 

There are couple ways to achieve what you are after. I will show you two, but there are other ways as well. The first one is signing a procedure, and allowing other users to use it with specifying only. First of all you need a master key, which i assume you already have, and you can check it using DMV, and looking for record that has '##' as a prefix. If not simply create a DB master key 

As specified ,returns table therefore SQL server checks the underlying structure of the table because it needs to know what kind of table is begin returned. How many columns it has,if any of the columns are without names(in aggregate cases) and inform you about it. You can test this by running the query above without referencing the column name( 'as user name' ) in case username table does not have a specified column name. Note: I had to tweak this two functions a little bit, to make them work. Hope this clarifies what is going under the hood for you 

rather than creating the table and adding an index afterwards you would be able to benefit (at least potentially as that does have some issues) from temp table object caching. I wouldn't bother making it conditional on row count unless you've proved that the presence of the index with few rows somehow makes a significant deterioration in performance (unlikely). 

That was actually my question at AskSSC. I should have just tested it myself as I accepted an incorrect answer. With the following test table 

When running various versions of your test code I only saw this pattern with and tables. Not permanent tables in nor tables. In order to get the slower performance it is not necessary to have previously added and removed a large amount of rows from the table. Simply adding a single row and leaving it in there is sufficient. deallocates all pages from the table. will not cause the last page in the table to be deallocated. Using the VS 2012 profiler shows that in the faster case SQL Server uses a different code path. 36% of the time is spent in vs 61% of the time spent in for the slower case. 

You could also use window functions which are usually faster,to get OrderNumbers that contain two rows, meaning contain both 'ghi' & 'abc' 

When you are all set and done, you can transfer those views from ViewSchema to dbo or some other more meaningful schema with command: 

There is a workaround for that issue. It will require an additional table,a trigger, and an agent job to be ran before transaction log backup job. So if you think its worth going through that than bare with me: You should create a table where you should add Database name,whether database was updated or not (you will need this for an agent), and some other info needed for backups such as : 

No. If you try the following from two different connections then the second one will be blocked by the first (visible in ) but neither will result in any transaction log activity and running will report "No active open transactions" (assuming no other activity). 

(Side note: Both versions of your query will return incorrect results if same table name in different schemas) As you can see the plan for this is pretty horrible. 

This issue is called parameter sniffing. Later versions of SQL Server give you more options in dealing with it such as or hints. You might try declaring variables in the stored procedure, assigning the parameter values to the variables and using the variables in place of the parameters as it sounds as though most of the time you are getting a reasonably satisfactory plan. Normally the most catastrophically bad plans are those compiled for parameters with very high selectivity but ran with parameters with low selectivity. Assuming the plan generated is more robust with this approach and satisfactory for all parameter values then the advantage of this approach over that suggested by JNK is that it does not incur a compilation cost for every call. The disadvantage is that for some executions run time might be greater than for a plan tailored specifically for those parameter values so it is a trade off of compile time vs execution time. 

I used examples on value and node, because you provided the code with those two functions only, if you want to read more about it please visit this Hope this simple examples give you an idea of how to query xml types 

Once this is done, the next step is to actually sign this procedure, so whoever has rights to 'execute' the procedure will inherit rights from the certificate user. 

See if you can find SOS_SCHEDULER_YIELD & CXPACKET waits. If SOS_SCHEDULER_YIELD waits are high you might have some very CPU extensive queries, which you should pay attention to. This: 

After delete trigger is executed after the record has been removed from the table. Joining a table Emp will yield no result because record with that ID does not exist in that table. Also note that inserted table will always be empty in after delete trigger. 

Now add 3,000 additional batches to T1 (with batch numbers 2 to 3001). These each clone the existing thousand rows for batch number 1 

A CLR aggregate will almost certainly be the fastest way of doing this. But perhaps you don't want to use one for whatever reason... You say that the source for this is an expensive query. I would materialise this into a table first to ensure it is only evaluated once. 

This issue is to do with corrupt/invalid statistics. A specific case where it can occur is mentioned in this connect item 

As long as all the variables involved are of datatypes compatible with (they are in this case - basically no LOB datatypes, CLR types, or user defined datatypes) then you can use 

Havent really tried to debug the script, but i can see just from the parameters what it essentially does. There are many scripts online that you can use to do just that The one that i use pretty often is this: 

Like i said there are other ways, including roles but these two could get you a job done. Just remember you cannot track a user that executed procedure by specifying 'execute as'(login is possible tho). Also creating a certificate/database keys can be a headache when you`re migrating DB, or simply restoring it somewhere else. 

You can also use extended events and capture high resource consuming queries, with included user names 

Depending on transaction level, those queries will block writes. Serializable & Repetable Read transaction isolation level will hold S locks (for the whole duration of the transaction), which are incompatible with X locks that are required for writes(inserts/updates) . And yes it makes no difference, if you check execution plan you will see that they are exactly the same. So in order to prevent locking and blocking implementing different kind of objects wont give you no results, but performance boost(if used stored proc). Instead you should change isolation level 

Yes. At least in current versions of the product. SQL Server will not pick apart the statement and reverse engineer it to discover that if the result of the computed column is then must be . You need to make sure that you write your predicates to be sargable. Which almost always involves it being in the form. . Even minor deviations break sargability. 

The first thing you should do is change the datatype of the column to , The and datatypes are intended for data that is more or less fixed length. Your example shows that you are not dealing with fixed length 200 character strings. Currently each instance of the 'Logged in' string takes 400 bytes. Stored as it will take 20 bytes (and 11 as varchar). Then you can in an statement to remove the trailing spaces. NB: In some circumstances you might find that creating a new table from scratch actually works better than the above as the insert can be minimally logged and it doesn't copy the data once and then trim it. It also ensures that no space is consumed by the dropped column. You can use the SSMS table designer to generate the script for this and then add a to the result for something like the below. 

As Kris mentioned, you could create a stored procedure and run it as a job Here is a sample script that will accept Table name and Threshold(in KB) and send an email if table size exceeds its threshold 

Just to expand on previous answer that was posted. Just like George said, shrinking in general is not something that should be part of maintenance job. Rather log size miscalculations, some unexpected scenarios (such as uncommitted transaction, large and intensive DMLs etc etc) or insufficient amount of log backups can cause excessive log growths. If your log size does not seem large enough, you should monitor it during busy hours, or during night time ETLs(If you have some) to see the average log size and change it if needed. Also make sure to set log size auto growth in specific MB size, which will mostly depend on your initial log size. More info could be found here Database log VLFs Now to answer your questions: 1) No in general. But in scenarios i mentioned above, it could be helpful. Which is the only time when it should be used - out of ordinary situations. 2) If you determined you want to shrink your log file, you should be aware that the log file is made out of VLFs(Virtual log files), which are gradually filled one at the time. Once all of them are filled,if you reached your log maximum size, log auto growth will happen and depending on size will grow in 4/8/16 VLFs. Once the log is backed up, these VLFs will become empty again (you will always have some in use, so it can track current LSN). To keep it short, once you backup you log, you can check used and unused VLFs using command 

No. Rebuilding the table is the only way. See this Connect Item for confirmation. You could use SSMS to script this for you if you trust the somewhat buggy table designer. Apart from that you could declare a view with the desired column order as a way of grouping logically related columns together. 

Notice that the result set has 3 rows and CountOverResult is 3. This is not a coincidence. The reason for this is because it logically operates on the result set after the . is a windowed aggregate. The absence of any or clause means that the window it operates on is the whole result set. In the case of the query in your question the value of is the same as the the number of distinct values that exist in the base table because there is one row for each of these in the grouped result.