Other ways I have tried or thought about seem to be even more redundant, and in the cases that they don't, the performance loss is untolerable for my needs. Ah and I forgot to mention that nodes can have hierarchy, but this is not a problem if the frame data is reduced. The structure instances don't get duplicated unless any value changes. When constructing a node object it's done referencing a pool of structure instances to prevent allocation for 2 equal frames. Also the lights are something that I'm not sure on how to handle as I need them to be attached to the node in most cases, even if I allow setting global lights to the scene. @Nicol: Yes that's what I'm trying to figure out. You can see the code doesn't rely on any hardware in particular or contains any game attributes, it's just keyframed animation data and some IDs for them and to reference API objects. I use objects such as CFrame to hold pointers to frame data and to store runtime values. The thing is I don't want to create layers unless it's really worth it, to prevent excessive distinction for small values, such as a making a frame object to store only the Diffuse color variation or making it store any value that can change but making it so random that I woulnd't know how to relate the data to the other frames, or need a complex system for it. That's why at this level I prefer to have well-defined data so I can store and work with fast during runtime. It's this complicated because it must support a lot of rendering and animation techniques, must support being accessed by game entities and other objects, and must allow (somewhat)easy optimization for them if needed. Anything that helps me make a decision is a good answer for my question, thanks for the reply. 

The reference manual is up to date in the nvidia site Cg 3.0 Reference Manual, which i believe is where you can get most of the information you want. It does contain very detailed data of everything you can do with Cg, I don't think there's something missing there, but it may be hard to read at first as it's just a reference for all the technical aspects of it, you will hardly find something like a tutorial in it, perhaps there is some very small examples to explain some concepts (I'm gonna read some now it looks interesting). EDIT: The manual doesn't seem to be that tough as I expected, if you have some experience with Cg or something similar already. I downloaded everything from the Cg Toolkit Download page. There it also mentions the users manuals and the demo tutorials were not updated, I believe you may find more updated tutorials on the web. and this is The Cg Tutorial book, it may be useful for people interested in Cg 

This is very fast for the CPU as setting an index, but for the GPU to do the same, it's a lot more complicated. Because the power from the GPU comes from executing the same instruction at the same time (they are SIMD cores), they must be synchronized to be able to take advantage of the chip architecture. Having to prepare the GPU to deal with branches implies more or less: 

because if you substract a vector (v1=ball position) from other vector (v2=mouse position) you get a third vector in the direction from v1 to v2 (i think) then you multiply the vector by -1 because you want to go the opposite direction (which you were doing already) You may want to make this a unit vector for then be able to multiply by the distance you want to move the ball (or the velocity/force to apply to it), you then divide it by its magnitude like this: 

This method may vary based on a lot of things (ie. some very small branches are able to run without the need of this distinction) but now you can already see why branching would be an issue. The GPU caches are very small you can't simply execute a program from the VRAM in a linear way, it has to copy small blocks of instructions to the cores to be executed and if you have branches enough your GPU will be mostly stalled than executing any code, which makes no sense when it comes when executing a program that only follows one branch, as most programs do--even if running in multiple threads. Compared to the F1 example, this would be like having to open braking parachutes in every corner, then get out of the car to pack them back inside the car until the next corner you want to turn again or find a red semaphore (the next corner most likely). Then of course there is the problem of other architectures being so good at the task of logical operations, far cheaper and more reliable, standarized, better known, power-efficient, etc. Newer videocards are hardly compatible with older ones without software emulation, they use different asm instructions between them even being from the same manufacturer, and that for the time being most computer applications do not require this type of parallel architecture, and even if if they need it them, they can use through standard apis such as OpenCL as mentioned by eBusiness, or through the graphics apis. Probably in some decades we will have GPUs that can replace CPUs but I don't think it will happen any time soon. I recommend the documentation from the AMD APP which explains a lot on their GPU architecture and I also saw about the NVIDIA ones in the CUDA manuals, which helped me a lot on understanding this. I still don't understand some things and I may be mistaken, probably someone who knows more can either confirm or deny my statements, which would be great for us all. 

Depending on your OS and video mode and drivers, the front buffer might be anything that your OS/Driver tells you it's the front buffer, it could be offscreen memory that the OS/Driver then copies to the actual "Display memory" if there is such thing in that particular video card. OpenGL tries to create a framebuffer, but it relies on the drivers, and probably in the way the OS exposes them, so the OS or the Driver may well decide to handle your final image, for example to apply custom filters configurable from the control panel of the video driver. You can never assume that if you access the framebuffer you are pointing to the display memory. Most likely will be a pointer to the "offscreen VRAM", or some mapping for a memory closer of both the CPU and the VRAM such as the bus memory, which is far away from the display memory, if there is such distinction in the videocard in question. 

You have to consider that GPUs when fetching the data for feeding into the vertex shaders requires a lot of small calls because they can only cache a few triangles at the same time, and fetching data from the VRAM is a very slow operation. So I suppose that if you use half the bytes for the indices, the GPU will be able to fetch twice the indices for the cache to render triangles. 4MB Can be a small amount of memory for storage space, but if you have to make a complex call to only process a few bytes from them it may affect performance in a noticeable way. Nowadays video cards are powerful enough to make it irrelevant in games which don't have to process a lot of geometry, you... could do some benchmarks rendering a model with both index sizes if you really want to know. Loading times are also faster with 16 bit indices, and actually every write or read, unless you can effectively control memory alignment and proper paging. So it's not just about the memory it requires but how much it takes to handle it. If you are working with dynamic buffers that need to be moved over the bus to the GPU at some point between Frames, then you should use 16 bit indices when possible. If you are rendering even smaller geometries, you can batch many indices in a single 16 bit index buffer for them all to get copied to the GPU in a single call. Here you may find some information about how the newer ATI GPUs work: ATI Stream Developer Training Resources (sorry I couldn't find the exact document) I didn't read much about the NVidia ones, but I think they work more or less with a similar approach. 

The tex2D comes later with the pixel shader, but by then you already have the scaled coordinates, so you don't need to do anything else other than get the colors as or something like that. 

"I've read that F1 cars are faster than those we drive on the streets... why people don't use F1 cars then?" Well... The answer to this question is simple: F1 cars can't break or turn as fast as most cars do (the slowest car could beat an F1 in that case). The case of GPUs is very similar, they are good at following a straight line of processing, but they are not so good when it comes to choosing different processing paths. A program executed in te GPU makes sense when it must be executed many times in parallel, for instance when you have to blend all the pixels from Texture A with pixels from Texture B and put them all in Texture C. This task, when executed in a CPU, would be processed as something like this: 

That error is just because either you're not setting the right path for the SceneManager headers (wherever CSceneManager.h is located), or you do but you're using <> instead of "" in the #include line. It has nothing to do with the inheritance, it should fail either you use CSceneManager or ISceneManager, because you're not including the header properly (or at least that's what the error is saying). For extending the scene manager you must derive from CSceneManager, as ISceneManager is just an pure virtual interface with no function definitions. Note the at the end of the function declarations in the ISceneManager class, that means the function has a null definition which the implementation class must overload to comply with the interface declaration. If you do derive directly from ISceneManager, you must provide an implementation for every pure virtual function it declares. 

My problem is as follows: I want to use a single render target texture for drawing 16 viewports where I want to display the 16 different combinations of the marching squares algorithm with my 3D tiles. Each viewport it's supposed to have its own camera and be located at a custom distance from the center of each tile set and this camera should be able to rotate around their center, zoom in and out, etc. My current issue is that my current projection setting is placing the center of the camera view in the center of the target texture, instead of locating the center of the view in the center of the rectangle corresponding to that viewport. I assumed I just had to push a translation to the final transform in order to translate the image within the normalized screen coordinates. However, when I tried to do this, it didn't work. How should I do this? NOTE: Currently I achieved it by using the direct3d viewports, but I'd like to know how should I do if I want to use my custom transform pipeline. 

You may need to update some viewport config so the framework/dx updates your projection transform attributes (it seems to be working with the older view/projection matrix). I don't know how XNA works, but DirectX works that way, with normalized screen coords, which are subject to pipeline transforms. If the view/projection transform is not updated to match your new window size, the image enlarges to sides proportional to the size change, instead of keeping absolute "pixel coordinates" (because normalized coords are relative to window size). ID3DXSprite behaves this way at least (it seems to build matrices from viewport sizes, or swapchain sizes, not sure which one), and I believe SpriteBatch is no different in this situation, so you will have to find out how to make XNA properly apply it's screen-to-normalized-to-screen-cord transforms. This is because either the DX pipeline expects normalized coords (emulates sprites with 3d vertices and shaders) or the GPU expects to work with normalized coords, or both, instead of raw pixel coordinates. 

When you detect a collision with the ground and the speed Y is low, you disable the gravity for that object, set the Y speed to 0 and skip the collision reactions against that floor (store a temporal reference to it) until its Y axis acceleration gets disturbed somehow (because of an external force such as a collision reaction or the character jumping or the floor getting out of range). You can have many floors then you know that when you are over one floor you don't have to check collision against others, you only check for collision with the floor you know you were last frame, and if you don't collide with it then you wait until a floor object reports its collision and you can start the process again. EDIT: I forgot to mention that if you're dealing with more realistic physics such as rigid bodies, checking the Y speed may be not enough for non-spherical objects (non-circular in 2d) as an angular velocity may affect your vertical force depending on the shape and momentum of the object. Note that by identifying the ground objects you can exploit their mutual exclusivity properties in many ways, such as for collision with other objects (an object over a floor doesn't collide with objects that rest over another floor) similar to the way a quadtree works. You may also find place for some prediction there, depending on the rules of your game (more realistic physics will always be harder) you may be able to predict the floor where your object will (most likely) be next, then set that floor as a priority for ground collision checking. Ah, and for if handling slopes is harder, it depends on the variety of angles for all slopes (in some games the slopes are always 45 degrees or so) and on the way you make your objects slide over a ground, ie you may want that if the Y velocity is high, you slide when falling on a slope, but if your Y velocity is low enough you could stop it. You may also want to add an effect to have different floor materials, so you would slide less over grass than over polished metal. 

and then it populates all the cores with this program (essentially copying the program to the core), assigning a value for for each. Then is where it comes the magic from the GPU and make all cores execute the program at the same time, making a lot of operations much faster than the linear CPU program could do. This way of working is ok when you have to process in the same way a very lot of small inputs, but is really bad when you have to make a program that may have conditional branching. So now let's see what the CPU does when it comes to some condition check: 

then you will have a final vector of length 1 so if you want to move it for example 15 units far in that direction you can do: 

well... I'm building the animation system of my game engine (the skeletal and skinned animation stuff), and I came to a point where I added so much functionality in the frame and node structures that it seems to be excessive for some applications, and it ends up adding too much complexity when working with simpler objects. Objects like boxes or simple meshes that are meant to be static objects would end up with complex properties such as the ability of having a node hierarchy and transformation frames, animation type checks, and also require a lot of unnecessary creation parameters that make really hard to work with when coding some functions that use them. I was thinking of making ie. SimpleMesh and HierarchyMesh objects, which will also require that the renderer can deal with different types of objects in the same scene. I was also thinking about making a MeshNode class and then make a Mesh object that contains them, but then I have some conflict on where to store some data, as some meshes will have per-node shaders and other will have a single shader for the whole mesh, then storing the shader references in the node will be a redundance for complex meshes that share the same shader in every node. Other option I was thinking was making some helper functions to deal with the simpler cases, which would set some default parameters and would ask only the most basic ones, for example 

I believe is a bad idea to have antagonist and neutral and protagonist as different classes, unless your antagonists are multientity spaceships and your protagonists are mages, of course. I agree with Nicol that this is not really language agnostic, and I believe is a really bad idea to try to resume the game architecture like that. Just make a comment in the beginnig of the file or a companion .txt if you want to give insight. Even so, I would do something like this (following the kiss rules): 

You can take a look at eathena project, a mmorpg server clone. I used to practice with it because it contains a simple scripting language to design the whole game and you can find tools for free for designing the scenes or just make them yourself, there is a lot of information out there about it. When I was newbier I wrote a reader for some of those files such as .gat containing tile description and the like. You can take a look at the sources at google code and also you can see screenshots about tests of the functions at picasa There may be other free sources of mmo servers but that's the one I know most :). I hope it helps.