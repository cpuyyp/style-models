I am formalizing the type system for a small language, and thus writing inference rules. Taking for example, its entry may be a number as well as an array of number, thus there is an overloading: $-^1$ and $-^a$. Two of their typing rules are $-^1: \text{Integer} \rightarrow \text{Integer}$, and $-^a: \text{Integer Array} \rightarrow \text{Integer Array}$. Actually there are a lot of operators like , which can take one element as well as an array of elements as entry. I just don't know how to generalize this formalization. For instance, when I introduce the typing rules for operator, one of its rule is $\text{FACT}^1: \text{Integer} \rightarrow \text{Integer}$. I just would like to say $\text{FACT}^a$ exists, and follows "the map of type" logic as $-^a$. Does anyone know how to formalize that elegantly? Going further for arithmetic binary operators, one rule is $\text{+}^1: \text{Integer} \rightarrow \text{Integer} \rightarrow \text{Integer}$, how could I say $\text{+}^a$ exists, and follows "a map of type" logic, which implies naturally that $\text{+}^a: \text{Integer Array} \rightarrow \text{Integer Array} \rightarrow \text{Integer Array}$? 

I am interested in the set of the variables that satisfy the following properties. I would like to find a proper name for them. We assume that a program $\phi$ has a set of variables $v_0, \ldots, v_n$. I am interested in a subset $S$ of $\{v_0; \ldots; v_n\}$, that informally speaking impacts the execution of the program (i.e., any execution of its statements). For instance 1) For the statement $v_0 := v_1$ that may be executed, $v_1$ impacts the execution of the statement. Thus, $v_1 \in S$ and $v_1 \in SY$. 2) For the statement $v_0 := v_1 * 0$ that may be executed, $v_1$ impacts the execution of the statement, because if $v_1$ is a string value, the multiplication of string and $0$ raises a type error. Thus, $v_1 \in S$ and $v_1 \in SY$. 3) In the statement "If true Then $v_1$ := 1 Else $v_1$ := $v_2$ End", $v_2$ does NOT impact the execution of the statement, because the else branch of the statement will never be executed; the value of $v_2$ will never be read here. Thus, $v_2 \notin S$, but $v_2 \in SY$. I will define the semantics properly, but before that I would like to find a proper name for the set $S$ that fits the convention. I am thinking of ? ? ? ? ? or ? Could anyone help? Edit 1: Thanks for all the comments, they are really helpful. I decide to edit the OP to make the question clearer. Actually, I planed to introduce the notion of a set about semantics, and then another set about syntax that is always a superset of (because is hard to analyse). Then, I will make a static analysis to over-approximate , thus it over-approximates also . My question here is about a consistent terminology about and (for the case of read/input). I have added the case for in the 3 examples above. You mentioned another 2 examples: 4) For "If factorial(3)==6 Then v_1 := 1 Else v_1 := v_2 End" (babou), we have $v_2 \notin S$ and $v_2 \in SY$. 5) For "x := x" (Martin Berger), one understanding is that the value of is first still read, then be assigned to , so $x \in S$; another understanding is that the value of does not impact the execution of this statement, so $x \notin S$. I will be OK with both of them (though personally I prefer the latter), as long as if I give a terminology and a definition to , it is consistent for all the examples. In any case, based on the syntax, we always have $x \in SY$. Regarding "live variables" (Klaus Draeger), indeed, as babou understood, I am not concerned about "a point in the program regarding what variables are still be needed to finish execution from that point". Regarding "use-define or use-definition" (babou), from that web page, my understanding is UD is about syntax, and it is what I am going to do with (e.g., see what is on the left-hand/right-hand side of an assignment). If I call used variables, what should I call ? 

Yes, this variant, and in fact a further generalization has been considered in the literature. See the paper below for the problem they call capacitated facility location. J. Bar-Ilan, G. Kortsarz and D. Peleg, Generalized submodular cover problems and applications, Theoretical Computer Science, 250:179-200, 2001. 

The relaxation of Calinescu, Karloff and Rabani for the undirected Multiway Cut problem is one my favorites. Had a big influence on subsequent work. $URL$ 

These problems are studied but with different terminology such as drop-off. See below and references therein. The Finite Capacity Dial-A-Ride Problem, M. Charikar and B. Raghavachari, in Proceedings of the 39th Annual IEEE Conference on Foundations of Computer Science (1998) Algorithms for Capacitated Vehicle Routing M. Charikar, S. Khuller and B. Raghavachari, in Proceedings of the 30th Annual ACM Symposium on Theory of Computing (1998). Dial a Ride from k-forest ACM Transactions on Algorithms, 6(2):2010 Anupam Gupta, MohammadTaghi Hajiaghayi, Viswanath Nagarajan, and R. Ravi 

I strongly disagree with the last paragraph. Blanket statements like that are not useful. If you look at papers in many systems areas such as networking, databases, AI and so on you will see that plenty of approximation algorithms are used in practice. There are some problems for which one desires very accurate answers; for example say an airline interesting in optimizing its fleet scheduling. In such cases people use various heuristics that take substantial computational time but get better results than a generic approximation algorithm can give. Now for some theoretical reasons for studying approximation algorithms. First, what explains the fact that knapsack is very easy in practice while graph coloring is quite hard? Both are NP-Hard and poly-time reducible to each other. Second, by studying approximation algorithms for special cases of a problem one can pin-point what classes of instances are likely to be easy or hard. For example we know that many problems admit a PTAS in planar and minor-free graphs while they are much harder in arbitrary general graphs. The idea of approximation pervades modern algorithm design. For example, people use data streaming algorithms and without the approximation lens is hard to understand/design algorithms because even simple problems cannot be solved exactly. 

I know that it is undecidable to determine if a set of tiles can tile the plane, a result of Berger using Wang tiles. My question is whether it is also known to be undecidable to determine if a single given tile can tile the plane, a monohedral tiling. If this remains unsettled, I would be interested to know what is the minimum cardinality of a set of tiles for which there is an undecidability proof. (I have not yet accessed Berger's proof.) 

I would appreciate a formal definition of the languages accepted by these modern "regex" modules. They don't reach context-free, do they? What is the superset of regular languages achieved by these recursive-backtracking additions to regular languages? 

To more directly address your question, perhaps this is the most natural interpretation for $m=1$: Pollack, Sharir, Rote, "Computing the Geodesic Center of a Simple Polygon," Discrete Comput. Geom. 4:611-626 (1989): 

Computing the Cheeger constant of a graph, also known as the isoperimetric constant (because it is essentially a minimum area/volume ratio), is known to be NP-complete. Generally it is approximated. I am interested to learn if exact polynomial algorithms are known for special classes of graphs. For example, is it still NP-complete for regular graphs? For distance-regular graphs? (I have not studied the existing NP-completeness proofs to examine their assumptions.) Literature pointers appreciated—thanks! 

Here is a figure from the first:     Note that the shapes mentioned in Yoshio Okamoto's post appear in this hierarchy. And if you are wondering what a (Barbados-induced?) "palm" polygon looks like...              

I wrote a paper long ago that detailed an linear-time algorithm for finding the smallest area triangle enclosing a point set (or a polygon): 

"Two-Convex Polygons," O. Aichholzer, F. Aurenhammer, F. Hurtado, P.A. Ramos, J. Urrutia, 2009.            

You may want to look at nowhere dense graphs. $URL$ One of the reasons why minor-closedness is natural is the following. We typically want to work with families of graphs rather than specific graphs. And we want to solve problems with arbitrary weights/capacities on edges/nodes. Suppose we want to solve the shortest path problem in a family of graphs. Then, if we allow for zero length and infinite lengths then basically we are allowing minor operations on the family. In some settings it makes sense to work with unweighted graphs where positive results can be obtained for larger families of graphs that are not necessarily minor-closed. 

The "dual bin packing problem" is more commonly referred to as the Multiple Knapsack problem. One can show that $ALG \ge (1-1/e) OPT_b$ assuming an optimal algorithm for the Knapsack problem is used in each bin. If the bins are of different sizes then one can show that $ALG \ge OPT_b/2$ irrespective of the order in which the bins are processed. See Section 4 in the paper below for these results. $URL$ 

See the following paper by Khanna etal on syntactic vs computational views of approximability. MaxSNP is a syntactic class while APX is a computational class. dl.acm.org/citation.cfm?id=298507 Made comment into answer as per Suresh's request. 

One should be able to get a simple constant factor approximation as follows. Partition the nodes of the graph randomly into two sets $V_1$ and $V_2$ - each node decides with probability half to go to $V_1$ or not. Ignore all the edges with both end points in the same side. We are left with a bipartite graph. Now solve a bipartite matching problem where the nodes in $V_1$ have capacity $1$ and the nodes on the right have no capacity limit. This gives a collection of stars. Analysis is left as an easy exercise :). 

Not an answer, just a few references. First, I wrote a paper (long ago!) on the case where every point in the given set $V$ must be a polygon corner. In that case, it is not surprising that there is (at most) one polygon, and it is easy to find: "Uniqueness of Orthogonal Connect-the-Dots," in Computational Morphology, Ed. G. T. Toussaint, Elsevier, North-Holland, 1988, 97-104. Second, there is a beautiful update to this work by Maarten Löffler and Elena Mumford, in a paper, "Connected Rectilinear Graphs on Point Sets," Journal of Computational Geometry, 2(1), 1–15, 2011. From their Abstract: 

Does anyone know of work on computing the Voronoi diagram of a set of points on a polyhedron, where distance is measured by shortest paths on the surface? I am particularly interested in convex polyhedra. I have a vague memory that this has been explored, but my memory is too vague to locate any papers. Thanks for pointers! 

This is not a serious answer, just an opportunity to mention that Godfried Toussaint introduced sail polygons in his 1985 paper "A simple linear algorithm for intersecting convex polygons":        I don't think this particular class found many subsequent uses. :-) 

Python / Java / Perl / Ruby / etc. extend regular expressions to permit look-ahead and look-behind, e.g., LookAround: 

This does not answer your (interesting) question, but it may be worth mentioning that this variant (which does not use geodesic distance) has been studied: Thomas Shermer, "Hiding people in polygons," Computing, Volume 42, Numbers 2-3 (1989), 109-131 (Springer link): 

(2) I also found a 2007 German Ph.D. thesis, "Facility Location and Related Problems," by Martin Romauch (PDF link), that includes a chapter on the "Vertex Guard Double Cover problem," showing that it is NP-hard for polygons with holes. He also shows that the right combinatorial bound is $\lfloor 2n/3 \rfloor$ (disappointingly obvious!). I have only skimmed through this, but it is certainly worth a look. 

There is a nice book by Gartner and Matousek on SDPs and their applications to approximation algorithms. It covers a lot with the added benefit of giving a good introduction to the theory of semi-definite programming. See $URL$ 

See the following paper. There may have been other developments since then. Maximum Quadratic Assignment Problem, Konstantin Makarychev, Rajsekar Manokaran, Maxim Sviridenko, ICALP 2010 $URL$ 

The $k$-disjoint path problem for fixed $k$. Given an undirected graph $G$ and $k$ node pairs $s_1t_1,s_2t_2,\ldots,s_kt_k$, are there node-disjoint paths in $G$ connecting the pairs? Polynomial-time algorithm follows from the work of Robertson and Seymour and relies on very non-trivial and difficult graph theoretic results. There are more general problems than this one but the disjoint paths problem is highly non-trivial even for $k=2$. For instance it is NP-Complete in directed graphs for $k=2$. 

If the demands satisfy the no-bottleneck assumption, that is $\max_i d_i \leq \min_e c(e)$ then a constant factor approximation is known even for trees. See $URL$ which gives a 48-approx for trees. For paths a better bound can be obtained and may have been shown but I am not sure where or whether it was published. For the general case it is not quite clear what the best known result is. One can get an easy $O(\log n)$-approximation as follows. Consider the problem of maximizing the number of requests that can be feasibly routed; there is a constant factor approximation for this problem, see $URL$ Using this as a black box one can do a greedy set-cover like algorithm to repeatedly pack as many requests as possible to get the desired $O(\log n)$-approximation. A paper by Chalermsook on coloring rectangles may have some implications including giving an $O(\log \log n)$-approximation; the paper is available at $URL$ However it may require figuring out several technical details. One suspects that there is a constant factor approximation for the coloring problem. 

Godfried reports that his hierarchy paper (cited by Aaron) never got written, but some of its ideas appeared in these two papers. 

Is there a polynomial-time algorithm to find—if one exists—a spanning spider of a given graph $G$? A spider is a tree with at most one node with degree greater than 2:            I know that various degree conditions on $G$ (essentially, sufficiently large node degrees) guarantee the existence of a spanning spider. But I am wondering if there is an algorithm for arbitrary $G$. Thanks! 

After hearing Emo Welzl speak on the subject this summer, I know the number of of triangulations of a set of $n$ points in the plane is somewhere between about $\Omega(8.48^n)$ and $O(30^n)$. Apologies if I am out-of-date; updates welcomed. I mentioned this in class, and wanted to follow up with brief, sage remarks to give students a sense for (a) why it has proved so difficult to nail down this quantity, and (b) why so many care to nail it down. I found I did not have adequate answers to illuminate either issue; so much for my sageness! I'd appreciate your take on these admittedly vague questions. Thanks! 

The question has been answered in the comments by Tsuyoshi & Chandra! I am adding this CW answer so I can accept it to indicate the question is closed. Thanks, everyone! 

I am interested in learning connections between "chaos," or more broadly, dynamical systems, and the $P{=}NP$ question. Here is an example of the type of literature I am seeking: 

I feel I should know this... But I am not finding a definitive reference. Is it $\Omega(n^d)$? How about the $d{=}2$ specialization: The largest area bounded cell in an arrangement of lines? 

If one restricts Turing Machines to a finite tape (i.e., to use bounded space $S$), then the halting problem is decidable, essentially because after a number of steps (which can be calculated from the number of states $Q$, and $S$, and the alphabet size), a configuration must be repeated. 

One standard view of dynamic programming is the following. Start with a recursive algorithm and then memoize it. By memoization we mean that solutions for intermediate problem instances/sub-problems are stored and not recomputed. If one can argue that the number of sub problems generated in the recursive algorithm for an instance of size $n$ is polynomial in $n$ then memoization leads to a polynomial time algorithm. A recursive algorithm for a problem instance $I$ generates, naturally, an associated DAG $G(I)$ whose nodes are the sub-problem instances and the arcs are the dependencies generated by the recursive algorithm on $I$. It is possible to interpret several different computational problems as solving some kind of shortest path problem on this DAG $G(I)$ (not all though). In these cases it is possible to take out the scaffolding of the recursive algorithm and memoization and directly use a shortest path algorithm in an associated graph. This should not lead one to conclude that all of dynamic programming can be reduced to shortest path computation. 

Here is a formal reason why the problem is not poly-time solvable unless P=NP. We know that finding the treewidth of a given graph is NP-Hard. Given a graph $G$ we can add a disjoint clique of size $V(G)+1$ to create a new graph $G'$. A min-width tree-decomposition of $G'$ can be obtained as follows: it has two nodes with one bag containing all the nodes of the clique and the other containing all the nodes of $G$. Now making this tree-decomposition lean would require finding a lean-tree decomposition of the original graph $G$ which would, as a by-product, give the treewidth of $G$. 

An algorithm with a run-time of $n^{poly(k)}$ should be easy where $n = |V|$ and $k = |T|$. Think of an optimum solution which is edge-minimal. It will be a tree and one can see that the number of nodes with degree $\ge 3$ in this tree will be $O(k)$. We can guess those nodes $A$. Once we guess those nodes we create a graph on $A \cup T$ with edge-lengths between two nodes $u,v$ equal to the shortest node-weighted path between $u$ and $v$ in $G$ (we do not include the end points in this calculation). Then find a min-cost spanning tree in the resulting graph.