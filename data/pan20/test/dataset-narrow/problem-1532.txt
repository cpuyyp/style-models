There is a linear relationship between the dependent variables and the regressors (right figure below), meaning the model you are creating actually fits the data. 

$y\hat{}_{i}^{t-1}$ is constant as it is prediction in a step before $(t-1)$ that we know already, thus goes to the constant term. The other two terms remain as it is. 

As you can see has an implementation for feature_selection using chi2 (perhaps according to ) as was shown very briefly in the above-mentioned blog post. If you want a more thorough explanation and details how test ranks features based on statistics according to chi2 distribution and p-value etc., and also how to build your own chi2 class for feature selection in Python see this great post. Obviously one can read about the basics of chi2 distribution and test in wikipedia. 

In addition to what Lupacante conceptually and nicely showed such that the added feature(s) has(have) to be informative for the model otherwise it can get ignored by majority of models (perhaps easily by regularized models), I would like add that you also increase the dimensionally of the feature space synthetically using many simple mathematical expressions as well. Concertedly, let's say your only feature (column) is: x You may easily construct other features like (it is common practice actually in physical sciences): x$^2$, x$^3$, x$^{0.5}$, $sin(x)$, x$^2$sin(x),... Till you hit the so-called the curse of dimensionality for your exercise. Although I am not yet sure what features/num_samples ratio exactly causes the curse of dimensionality! What I have gathered so far about the curse of dimensionality has been very subjective. 

Perhaps you need to look at this self-contained blogpost on Machine Learning with Signal Processing Techniques on how to prepare your time series data and extract useful statistical estimate and feature for machine learning models. At the end an example is given for classification. I found it super useful and straightforward. Somewhere in the middle of the post, this great method for the Detection of peaks in data is introduced as well. 

Sidenote: Boxplots usually are used for easy and quick outliers detection as well. Said that I totally agree with jshep that it is the matter of taste of both user and audience, and usually academics lean towards less fancier presentations like Boxplots. 

Updated (Not an assumption, rather a good check): There is minimal multicollinearity between explanatory variables which can leads to less stable parameter fits (thanks KT. for pointing out this to me). Multicollinearity occurs when the independent variables are not independent from each other. There are tests like correlation matrix (Pearson's Bivariate Correlation), Variance Inflation Factor that can be used to verify this assumption. 

Chi-Squared test For Feature Selection goes under the Univariate Selection method for non-negative features. My favorite explanation of chi-Squared in one photo taken from this blogpost is: 

Why don't you consider Gradient Boosting Decision Trees (GBDT) for Regression which you will find many Python implementation for (XGboost, LightGBM and CatBoost). The good things about GBDTs (more relevant to your problem) are: 

I do not see any problem with your analysis. As you said you see an inverse correlation between humidity vs. temperature in your plot as well as via your -.32386 weight. While the weight of -.02219 for pressure is nearly zero and negligible and obviously you won't see such relationship between pressure-temperature. 

At the end (as we speak), I sampled only 20% of the images to have at least a model prototype up and running, although accuracy is not impressive!! 

(Edited after @D.W. suggestion). To the best of my knowledge, there is nothing wrong with what you have in mind; thus it is certainly valid.. As you said, you have to try out all possible ways you can think and see which one works better. The most important point adopting a particular encoding is to be aware of you have certain amount of information getting lost and it varies one case to another depending on the problem at hand. For example in the case of binary coding based on high/low frequency sublevels, there will large of information (details) lost which could help the algorithm to do classification. I liked your idea of percentile coding based on cumulative density distribution. Maybe you want to look at Quantile-based discretization which is available in pandas.qcut. The rest is my earlier answer as it was (below). I intended to suggest trying other techniques as well on top of what you have had in mind; but apparently the message was not clear. Please note that I do not seek to get my answer marked as final answer, as I aware that still it does not fully answer your question; it is simply to brainstorm and exchange ideas in length. ;-) Perhaps you have already digged out enough about ways to convert categorical variables into continuous data. In case you did not and missed checking this answer, check it out. As discussed, they are many ways to convert cat-to-num, and your problem is one of the hardest yet the most common across many domains. You have a high cardinality in your categorical variable, and as far as I understood imbalance distributions of those sublevels and you are not sure whether messing up the order of those sublevels matters or not. You may need to try Ordinal Encoding (if order really matters) or weight of evidence (WoE) transformation (see this blog post for instance) that I have heard but not tried or even going beyond mixing them in a meaningful way to represent your categorical data properly. What I have learned, despite all efforts in the field, that this problem is still an open challenge in data science and machine learning. Thus there is no best solution, or well-established method as far as I checked. Please do let me know if you come across one. 

Now using argrelextrema function you will find the index of relative extreme values (either minimum or maximum) 

These will return the peak values. Updated Answer: If you want local extreme points (e.g. maximum or minimum ) around each peak, check scipy.signal.argrelextrema in Scipy. A concrete example: Let's make a artificial random data with random spikes: 

The rest should be pretty straightforward, just add things that are left and clean it up and you see the second equation comes out beautifully. 

While Tensorflow is a super powerful numerical computation and optimization library with lots of features for building neural networks, YET it is a bit tedious and nontrivial especially for beginners to use. Here Keras comes very handy. In short, Keras is Tensorflow abstraction. It allows to quickly and easily define a neural net even complex ones with a few lines of codes. Also note that Keras not only runs on top of TensorFlow but also on top of Microsoft Cognitive Toolkit, Theano, or MXNet. Look at at this post where the blogger compares defining a typical neural network via directly in Tensorflow and Keras. You can see that with Tensorflow it is minimum 17 lines of code, whereas Keras reduce it to 10 lines of code. 

Well, I would not call this "Test" set you defined as "Test"! I would say: Train, Validation, Test. And this "Test" is the dataset that your model has not seen yet. This is pretty standard even explained in details in Andrew Ng courses like this very useful and important lesson on splitting data and the significance of their distributions. 

Task: Build CNN Model (preferably Keras or TensorFlow) to Predict Labels Associated to Each Image in CelebA Dataset (Multi-label Image Classification) In past, for majority of multiclass/binary image classification problems, I used to feed images efficiently using and in Keras after images are properly organized in a separate directory for each class. Therefore, I have never bothered converting images to numpy.arrary prior to feeding to the model, unless I had to and of course datasets were small so that I could do it easily in my local machine. However, CelebA is a Multi-label Image Classification with each images having 40 labels (attributes like Smiling, Eyeglasses, Young etc.) meaning that I can not organize them in subclasses as I used to do, so is off the table (as far as I know!). Still I've managed to convert the images to numpy.array by the following simple loop: