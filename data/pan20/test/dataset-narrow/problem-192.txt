You should not ONLY rely on buffer cache hit ratio it does not accurately measures or give signs of memory pressure the reason has been shown by Jonathan Kehayias in this Blog. The reason is concept called as Read Ahead in SQl Server. Due to read aheads more than required pages are brought into memory every time a single scatter gather read happens and this mostly keeps buffer pool populated with pages giving indication that their is no memory pressure due to churning out of pages. If you read the blog you would find other important counters to look for memory pressure. 

It is fully logged operation and is severely bad process so ALWAYS try to avoid it. What it does is, when you hit the shrink button, quoting from Paul Randal blog 

I believe you want to say taking transaction log backup and shrinking ?, If not in full recovery model transaction log backup would truncate transaction logs subject to few restrictions. And please stop shrinking log file daily this is big performance killer. And shrinking transaction logs DO NOT break the log chain so you are safe. 

12 PM Sunday full backup Differential backup taken before the disaster strike. Note this is just one diff backup you have to restore no matter how many you have taken because its cumulative The log backups after diff backup (in point 2) stopping just before the disaster struck. 

I guess this is a known issue and Microsoft has released information about it in below links Cause: This issue occurs because the "tran_sp_MScreate_peer_tables" transaction was left open by a replication upgrade script during the upgrade. This open transaction prevents usual log truncation. Transaction log for database is growing with system SPID as open transaction There is also a documented support article which will help you in understanding the issue Database transaction log continues to grow after upgrade 

Well in such case you have differential backups a very nice way to lower down RTO. This will surely decrease your total time taken to restore all backups after disaster. The restore sequence would be full backup followed by LATEST differential backup followed by all log backups in sequence taken after latest differential backup. Suppose you do daily full backup at 12 AM diff backup every 4 hr and every 1 hr log backup. Lets say database faied at 3:50 PM. You would restore 1.Full backup at 12 AM. 2.Differential backup at 12 PM 3.Log backup taken after 12 PM differential backup ( 1PM, 2PM and 3PM log backups) I dont think it would be big deal. This is minimum you have to do Logshipping is not a solution here. You can create your own script to automate the restore process. There are file system backups which can also do this job, they take file system backup of mdf anf ldf file and after disaster you just need to attach those mdf and ldf files but poiint in time recovery is not possible with file system backups 

First whenever you post question related to SQL Server please post 'version and edition of SQL Server', you wont believe answer might change completely with different versions. 

This is not an official statement but I got this from my MS friend. It would be supported subject to condition both CU and SP are supported. But the risk and burden lies on you. I have worked with such environment for very short period, to be precise for 4 days and after that I made sure all the replicas are on same SP/CU level. If you really need it, I would suggest to try to limit this to couple of days to be on the safer side. The correct approach would be to first test the SP/CU on UAT and then if you are happy proceed on production. But sometimes due to constraints this may not be possible. If replicas are on different SP/CU failover may cause primary replica to be on node which is not at latest SP/CU and you may see issues which was fixed in recently released SP/CU so this is very risky. The solution here is to keep such environment as short as possible. 

Stop there, this is not correct approach just because you did a full backup and log backup please don't be in idea that you can safely remove old log files and full backup. Do you have a plan to test old backup by restoring to actually see that in case of disaster this backup files would work. Remember only successful restore guarantees that your backup is totally consistent. Do you have a option to check backup integrity.If not please include it into your backup plan. At least keep 4 day old backup files( this is what I do on local disk) before if you delete backups This strategy is upto you keep backups for time where you are sure they wont be useless if disaster strikes. Sometimes business want only specific data change to which was made couple of days. I also have my database backup up on tape and that tape is stored for 6 months. 

Which means maximum amount of memory which can be granted to the query in "current" situation if the query, during processing, requests memory. This value is calculated when query ran considering memory requirements of other queries running and the system memory used. This is useful in troubleshooting a slow query which is running slow because statistics are skewed and optimizer prepares sub optimal plan which forces query to request memory different from what it would actually need while running. When it requests much higher than what is asked for then query becomes super slow because it was not granted memory which it needed. This also helps to understand if there is a memory pressure on the system. OTOH means Total amount of memory "actually" granted in kilobytes. Can be NULL if the memory is not granted yet. The signifies what maximum can be granted. A worth reading blog. Understanding Query Memory Grants 

Plus and fact being that since index is small pages are most likely to be in memory so no physical I/O would be required to get page and hence fragmentation would not come in picture. So the small indexes which have or for that sake < 3000 is hardly going to cause any performance issue, it is quite likely that pages would be in memory and physical I/O would not be required hence fragmentation would actually not matter. If I look at your output only two indexes with should really cause some performance issue if SQL Server has to scan index pages. People can argue that page_count of 5000 could also be deciding factor and to which I would say "not really", considering the definition from old books online quoted above. You must also note that the fragmentation we are talking is logical fragmentation where leaf level page order does not matches that of clustered index key. So performance issue, if any, could only come from large fragmented index not the smaller ones. 

I can see from the question that you have . There was Bug in SQL Server 2012 which forced PLE to plummet but that was fixed in . Now since ** SQL Server 2012 SP3** has been released I suggest you apply SP3 and see if the issue subsides. 

No it should first go to node 2 because the Node 2 is read only replica and Node 1 is primary replica which will server to read write operations. As per BOL 

Any backup restored on primary database will break logshipping and you would have to start from scratch to configure logshipping. 

As already noted AWE is an Windows API , which does memory allocation for SQL Server process if account running SQL Server has LPIM. You can use 

If you restore a database which has index defragmnated you do not need to run index defragmentation again after the restore. Restore creates exact copy of the database 

For upgrading SSIS packages you should read BOL document I guess pretty much every thing is documented in Upgrade Integration services packages. If you want reference document please download 5 Tips on smooth SSIS upgrade to SQL Server 2012 Basically you have to read a lot and you have to look for features that has been deprecated. You might as well need to make changes in your code. As you can understand it wont be possible for me to tell you changes, you have to look at your code taking doc as refernce as do necessary changes. 

SQLServer:memory Manager--Target Server Memory: This is amount of memory SQL Server is trying to acquire. SQLServer:memory Manager--Total Server memory This is current memory SQL Server has acquired. ( Ideally Target value should be less than or equal to Total) Page reads/sec – Number of physical database page reads that are issued per second. This statistic displays the total number of physical page reads across all databases. Because physical I/O is expensive, you may be able to minimize the cost, either by using a larger data cache, intelligent indexes, and more efficient queries, or by changing the database design Free Pages – Total number of pages on all free lists (free lists track all of the pages in the buffer pool that are not currently allocate to a data page, and are therefore available for usage immediately). Undoubtedly this value should be high Page Life Expectancy – Number of seconds a page will stay in the buffer pool without references> if you have NUMA system analyze PLE for each node as mentioned in this article Free List Stalls/sec – Number of requests per second that had to wait for a free page. Ideally stalls should be as zero or as minimum as possible SQLServer:Memory Manager--Memory Grants Pending: If you see memory grants pending in buffer pool your server is facing SQL Server memory crunch and increasing memory would be a good idea. For memory grants please read this article: If you see a non zero value of memory grant pending with Low PLE and High free List stalls you definitely have a memory pressure and should consider providing more RAM. 

The size which you are pre-allocating is 1048576KB which is approx 1G. The same is with Log file 1048576KB which is 1G. Can you alter your script and put it to few MB's and see if you again get the locking. You can later run to increase size. This is something I have witnessed to telling you. This is not a definitive answer but I am sure it will reduce the creation time which exactly is your problem. Did you also noted if any process was blocking the table creation script. Plus first run create database script and then the alter database commands. 

The number of such machines are high There is no application owner, so we are not sure where exactly changes needs to be done(a bit tricky situation). SQL Server cluster does not supports renaming if database is replicated. 

So you can see you can take transaction log backup on any replica no matter whether it is synchronous or asynchronous replica. 

So what this means is of all committed memory SQL Server is not using 40 MB of the memory. . A committed memory is one that is backed by physical RAM. When a process starts it can address any physical memory address in its VAS but memory will only be committed if that VAS region is backed by physical memory. 

One cannot accurately say SQL Server has sufficient memory or not by just looking at BCHR, there are other counters to be taken into account 

First please note log truncations and transaction log backup are not same thing.Its transaction log backup only which is responsible for log truncations in full recovery mode. For DB in simple recovery mode checkpoint causes log truncation or when log grows 70 % of its size. Regarding how much time transaction log backup of 60 GB log file would take its hard to predict there is not such formula. If you have faster disk where log file resides and faster disk where you are dumping the backup it would take normal time. Backup reads data from disk and writes it to disk so if disk is not facing I/O contention it would be quick. You must also read Optimizing backup and restore Performance in SQL Server Understanding SQL Server Backup 

Your research is correct you have to first apply service pack on mirror instance and then failover the mirroring so as to make this mirror instance as primary. Now you have to apply patch on instance which is nor mirror(which was previously primary). I have done it many times and I do not see much Gotchas. 

The credentials for which are not stored in SQl Server database and managed by windows/AD. There would be entry for windows autheticated logins in master database with respective SID but password would be with Active directory. 

The main objective of differential backup is to reduce your Recovery Time Object(RTO). In more simple terms in event of disaster you can quickly recovery database using differential backup. Supposing you do not have inbuilt script to restore backups so restoring full and lot of transaction log backups after that is time consuming as compared to restoring full backup then latest differential backup (since it is cumulative) and few log backups after that. In two scenarios which you have shown above in case of disaster you would have to restore 

The Log you posted seems incomplete, also it is not the log for SQL Server installation but for patch update. You can see in the log 

So basically it does page movement and frees empty space from the pages. This movement of page is logged in transaction log file. Due to this movement the logical ordering of pages changes and hence logical fragmentation comes in. The point to note here is this is not an atomic operation. If you stop the shrinking operation in between the changes done so far is not lost it is maintained and when you start next time it will continue from where it left. See this Blog from Paul Randal for data file size management if you are worried about growing data file size. 

Huge delete operation is bound to impact othere transaction, degree of impact may vary from the process you choose. May be impact would be very little or may be impatc would be high. In any delete operation you should always follow process of deleting in batches. This avoids long blocks, lock escalation (which ultimately hampers concurrency) and filling up transaction logs. You should also make sure you find a suitable time to run your delete operation by suitable time I mean time at which load on server is relatively less. After you have executed your delete statement you should look at sys.dm_exec_requests dmv to check blockings if any. If your delete operation is blocked find out who/what is blocking and take appropriate action. If you break your delete operation in chunks of 200K-300K you might get lucky but amount of rows to be deleted in one go depends on number of rows present in table. Find out your number of rows Other option which is much faster than delete is truncate. Truncate has few limitation please read This Doc before proceeding. If almost 80-90 % of records need to be delete from table you can transfer rest 10-20 % of records from source table to new table which would be created by taking script from original table and then move 10-20% records in new table and then truncate old table. Truncate is very minimally logged and very quick. PS: This is just a startup I am sure you would get more good suggesstions 

The database would not be changed at all, whatever would be changed is backup size. So if you have 2TB of database on source then when you take backup with compression it might be around 1 TB(A simple guess, size may vary) but when you restore the same backup on destination the size required would again be 2 TB. Compression is just to make backup size small, it has NO affect on database original size 

To track login failure reason SQL Server Profiler would be a good option. One can also use extended events trace to capture login failed events and reason behind the failure. You need to first launch SQL Server Profiler.Create a new template or you can use existing one if you have already. Please include below when creating the trace 

For more details and explanation about above steps please read Bob Dorr's I/O presentation blog Pagelatch* waits are non I/O waits and I have seen most of the time these wait are prominent because of allocation contention. My hunch is that it has to do something with how . So how is your tempdb configured ?, how much tempdb data files are present ? Make sure they have same autogrowth and same size. When new page is created system pages like GAM,SGAM and PFS pages needs to be updated or are accessed and when SQL Server finds contention in accessing these pages such wait come into picture.