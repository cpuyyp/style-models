Although this question is rather old, someone may find it while googling the very same problem, so here comes our "solution": We couldn't figure out the source of the problem, as the problem was non-reproducable and only happened about once a day on varying access-ports, with about 500 access-ports with the exactly same config in use. After updating all 2960 to a release from the 15.x line, the problems disappared. The config didn't change, so it seems to have been silently fixed in one of the releases in between. 

Forwarding is the term used for the action done by any station participating in the distribution of a packet that is neither the source nor the destination. Host A is the source or sender, Host B is the destination or receiver. All routers in between them will only be forwarding the packet, as they are neither source nor target of that communication, but just relays. I have to correct myself on that point. Forwarding, in networking terms, usually refers to "relaying data that you received for somebody else on Layer 2". Switches forward frames they receive, by determining if their Forwarding Table/CAM Table (Cisco) knows on which the port the destination MAC address is, and then sending it there, or flooding it on all ports if they don't know where the destination MAC address is located. Routers route. (Ha!) They do the same as switches, but on Layer 3 (IP). They take a look at their routing table to determine where to send a specific packet on it's journey to it's destination IP. Often, forwarding and routing are mixed up (like I did), but in network terminology, forwarding is Layer 2, routing is Layer 3. 

Equal-cost multi-path routing is simply a way to distribute traffic with the same source and destination over several links, which all have the same "cost". Imagine it like this: You're going from Los Angeles to New York (by car). Checking Google Maps, you find out that there are two possible routes, one via Denver, the other one via Albuquerque. Both routes are exactly 2800 miles long and take exactly the same time. You, as a driver, would now have to equal-cost routes to choose one from. However, as a network packet, you don't make those decisions, but the routers make them. The router in LA could now choose to send the first packet via Denver, the second via Albuquerque, the third one via Denver and so on. It would result in both links/routes being equally used, without having a traffic jam on one route and no traffic on the other one. In short - it's just a way to distribute traffic over to equal links to avoid congestion. 

We are operating several Cisco 2960S stacks here and, for some time, are experiencing regular Spanning Tree TCNs (Topology Change Notifications). When using show spanning-tree detail to get to the source of the TCN, they originate from different 2960S stack members. As the config has been transferred from older switches, the ports are configured as trunks, with the data VLAN being the native VLAN and the VoIP VLANs being included as tagged in the trunk. I know, today the correct configuration would be switchport mode access combined with switchport voice vlan - but at the moment it is at it is. Part of the problem was based on the fact that some ports were only configured as portfast, not portfast trunk, so this has to be fixed, as the portfast only command is not applied to ports in trunk mode. However, some ports are configured as portfast trunk, which as of my understanding of Cisco documentation, should avoid generating TCNs on link changes. Sadly, they do. Executing show spanning-tree interface Gi1/0/x portfast shows portfast enabled for all VLANs on that interface, but show spanning-tree detail together with show logging shows that these interfaces are indeed the source for the Spanning Tree changes. It is made sure that there are no "special" devices connected to those ports, just VoIP phones and desktop workstations. The version run is 12.2(55)SE3, the devices are 2960S-48LPS-L and 2960S-48FPS-L. Looking for known bugs, there was a bug in a 12.1 release for 3550 switches that caused such behaviour, but this is the wrong device and a much older version, and I guess somebody should have experienced that bug before. Any idea where to look, what to try? 

From my perspective, in keeping with the KISS principle you should only seek to differentiate those ISP services into your network as far as the devices that need to know the difference. In other words, if you are using the ISPs interchangeably for generic Internet access, keep it simple on the internal network and just have a default route to the firewall, and let it (or the router beyond it) sort out how to split traffic. If on the other hand you are using the different ISPs for different traffic types, you may need to "extend" some notion of that differentiation into the internal network through routing, tagging, NAT-ing or whatever. So getting back to your original question, only create 3 different VLANs if you have a good reason to. Otherwise you're just complicating things without a corresponding benefit. 

If you had several IP addresses assigned to the same NIC without using VLANs, you would most likely end up in the situation requiring what MS DHCP calls superscopes. See here : $URL$ This allows you to group several DHCP scopes into one super-scope, and the server will then check to see if there is a match within any of those scopes, and not just the one associated with whatever IP it thinks is the primary NIC IP. So in your situation, assuming I understood it correctly, if you did not setup superscopes, you would not be getting an IP. Removing the second NIC IP then gets you out of that situation and back into traditional DHCP behaviour. 

I believe you are over-complicating things. A VPN Instance in the HPE Comware world is essentially a VRF in the Cisco world... and you don't need a VRF to keep a few VLANs from talking to each other. Neither do you need Policy Routing. ACLs should work just fine. You can find the relevant HPE documentation here : $URL$ Essentially, you would create an IPv4 advanced ACL and add two rules to it, one allowing traffic to VLAN 101, and the other denying everything else, and then apply it outbound on the vlan interfaces for vlans 102 to 105. 

RSVP reserves resources along a certain path in the network, but the destination could only be specified by a single address (unicast or multicast) so as far as I know you can neither reserve resources towards a CIDR block, nor send traffic to a different host in the same subnet as the one you initially reserved resources for and assume that the reservation will apply. The path can be reused so long as you keep refreshing it periodically, and so long as the network doesn't send back an error when you attempt to refresh. 

There is one way to approach this with just an ASA by using a feature called tracking. Essentially what it does is use some sort of test (typically a ping) to track the availability of a resource, and add/remove a static route from the ASA's routing table depending on the status See : ASA Config Guide: Configuring Static and Default Routes I assume you are trying to do this for a set of servers. Let's say you have two servers, A and B, and you want them to be accessible through a shared IP IPV. In this case you could create a loopback interface on each of the servers with the IPV address, and then use tracking to manage two static routes towards IPV/32: 

You can query the switch via SNMP to read the ifLastChange field from the standard MIB-2, which gives you the value of the sysUpTime counter when the interface last changed state. You have to read the current sysUpTime value as well to make sense of it, of course, and I recall there are some caveats if that last change is too far in the past because the field is a 32 bit counter counting hundreth-of-a-second ticks. I don't believe you can do the same directly from CLI, but you can implement object tracking on the interface state to get the same result. 

The problem with this is that you have the router providing the byte count when it receives the poll request (and it may be counting bytes as they are queued, not necessarily as they are emitted), and the tool counting time when it sends the poll request. This allows for all sorts of fun problems with buffering in various places that can lead to unexpected spikes in the data because you end up violating the unstated assumption that time delta between byte count values is the same as between the polling requests. You can also get issues with counter resets and rollovers, though most tools are good at detecting that. 

Actually if your access switches connected to the so called "core switches" have just one uplink, either to the HP or to the Juniper, full redundancy is not an option given your constraints of no additional cabling. If also an additional cabling from the WAN router to the HP switch is not an option, even resiliency to the Juniper switch failure is not achievable as far as I can see. Back to interVLAN routing, any FHRP protocol (VRRP etc.) can effectively work only assuming that a client is able to reach the backup gateway if the active one fails, but in your case if the Juniper switch goes down there is no way for the clients (connected via the switches with a unique uplink to this Juniper) to reach the HP switch. Furthermore, even in the case you have VRRP in the example just above, the clients connected via the HP would only be able to establish interVLAN communications only with clients connected via the HP (no WAN or anything else). Hope it helps. 

If you're transferring files in your local - reliable and deterministic - LAN you will probably require less time with TFTP than FTP, but if something goes wrong the result will be a corrupted file or a connection abort (requiring you to restart everything and lose more time). Even if questionable from a real point prospective, no one will stop you from using UDP and than write an application protocol that implements the features of TCP, like retransmission, windowing, congestion avoidance, etc. Nowadays, btw, I would say that the most promising protocol is WEBDAV (HTTP), that runs on TCP/IP. But going back to your question my answer would be: TCP can be considered a better choice for developing an application protocol for large file transfers, because it has some built-in features that we can use out of the box, like: 

With sub-interfaces, the sub-interface number doesn't need to match the VLAN ID (even if it's a best practice). However, many other possible answers exist, for instance: 

About the mentioned "Proxy ARP", I don't think it is the case, because the packets from one VLAN are not allowed to jump in another VLAN; "Proxy ARP" might work in the case of a matching VLAN ID with a mismatching IP addressing, but this is not the case (even if no 802.1Q support, "Proxy ARP" is not needed as the gateway IP and the host IP match). 

QoS takes action when there is a congestion. So, yes, your team mates might be right when saying that a link used at 50 to 70 percent doesn't need QoS. First, let's think to a theoretical link of 1 bit per second with a clock rate of 1 second (meaning that there would be 1 wire that transmit either 1 or 0 for 1 second, because destination wouldn't be able to catch the value if the signal is shorter): until the traffic that we need to send is of 1 bit per second, we just put that bit into the wire. No QoS is needed. But if we receive 2 bits per second from a faster link (a LAN for instance), these 2 bits need to be forwarded to the 1bit/s link, and so we need to either queue or drop 1 of the 2 packets we received, while forwarding the other 1 bit. Here QoS should be used to decide what bit must be forwarded first, and what should we do with the other one (basically, drop or queue). Second, in a real world situation, we have links that have a fixed bandwidth and that can transmit only at that bandwidth; for instance, an Ethernet 100M full duplex can send and receive data at 100Mbps only. If we're connected to an ISP with a 100M Ethernet link but we pay for 50Mbps, our link must send frames/packets at 100Mbps. To achieve the 50Mbps we need to do something like transmitting at 100Mbps for an half second, and than wait another half second without transmitting anything, obtaining the average of 50Mbps in the time of 1 second. In this example, a burst may allow to transmit at 100Mbps for 1 full second if we didn't transmit anything in the previous 1 second. With these concepts in mind, we can understand that the link used at 50%, that hasn't any burst above the link capacity, will never be congested and QoS won't be used. On the other side, in a real world, it's rare to spend a lot of money for a WAN link that is never fully utilised (but it might be not in a LAN); also, peaks of traffic happen in a usually unforeseeable manner. Consequently, moments of congestion should be taken in account in a good plan, in order to permit the flow of the critical traffic while sacrificing the non critical one. QoS is quite complex anyway, if you're interested I wrote this column: $URL$