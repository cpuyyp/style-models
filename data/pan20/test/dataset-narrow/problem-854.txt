I'd be willing to bet that it's a lack of reverse DNS. Try disabling reverse DNS lookup in the server and see if that helps. 

I think it depends on what you're using for a monitoring system. Nagios has NSC++, you can always setup a Windows based snmpd, and various commercial monitoring software packages provide other agents for you to install. Do you have a monitoring system up and running yet, or is that what your goal is? 

If you have an $80 budget and VPS III costs $60, then that looks like the best deal. When you get $500/month, get the dedicated server. 

Do you care at which layer in the stack the encryption happens? Short of massively modifying the source code to decrypt all DB queries (and that's ignoring the key management aspect), the best bet might be to encrypt the partition that the database lives on. What is the reasoning behind it? There may be a better way. 

In an environment where you're using ACLs in addition to traditional UNIX permissions, no, probably not, other than convention. A user does (to my knowledge?) have to have a GID assigned to them, so you might as well make it restrictive, rather than assigning them to a generic "users" group. ACLs don't replace permissions, they augment them. 

Have you considered using an email retention service like Global Relay? We use them to archive our email for compliance reasons, but that has also had the side effect of shrinking my users' email boxes, since they know that if they want a deleted message, they can log into the Global Relay site and retrieve it. 

[root@a-sys1 ~]# yum search "php" | grep -i gd php-gd.i386 : A module for PHP applications for using the gd graphics library [root@a-sys1 ~]# yum install php-gd Loading "dellsysidplugin2" plugin etc etc 

First, you probably know that you're never actually going to hit 2Gb/s. The overhead of TCP/IP will limit you to probably 90% of the max. 2nd, even if you use a TCP offload engine, the stack above layer 3 definitely affects where the bottleneck is. In other words, how are you transmitting the data? I could have 10Gb/s NICs and a crossover between them and I'd not get above a few hundred Mb/s if I was using rsync over an ssh tunnel. What else can you tell us about the topology? You said that the server is connected to a couple of switches, and that the remote clients are all over the world. Do you have > 500Mb/s (aggregate) of WAN connections? 

Assuming you have a monitored Windows server, and you want to check to see if it has connectivity to a remote machine, I would recommend the following: Use this PowerShell code (or one much like it) to write a script that checks a remote port. Then set up the CheckExternalScripts module to run and interpret the output of the PowerShell script. 

Step 1 Securely configure the hosting web server to have a "staging" directory, which will hold the clients' uploaded files until you can retrieve them. 

As long as they're the same architecture, then you should be fine shutting down then rsyncing the files. 

I keep some of my writing in a subversion repository, and I use tortoiseSVN as a subversion client on my Windows machine. Have you considered having them checkout the repository there, and maybe giving them a "laptop development" branch or something, into which they can merge changes and then folding them into their other development efforts? 

The "Server Info" string is really just the machine name, and a short relevant description, such as "PolarSprings vmnic2". The important part appears to be that it follows the hashmark. When I run snmpwalk, I get the proper output: 

When you moved from CompanyA to CompanyB, did you move your database with it? If you did, and it's running and everything, make sure that you updated your application's configuration to point to the new host at CompanyA. If you originally configured it with the IP address instead of domain name, that would explain the symptoms. 

(or wherever PHP is installed for you) That should print out diagnostics on what's going on. Also, it should be noted that if you just run php, it doesn't print anything, and it waits. Also, it won't print anything if your script doesn't have any output. Edit Check the code in the index.php script and make sure it's doing what you want it to be doing. If php is exiting immediately, treat this like a bug in your code. 

I think the biggest risk to providing public interface to Nagios is that the Nagios CGIs haven't been written in a hardened fashion (at least they've never claimed to be). A raw Nagios interface leaks internal information like a sieve. There's nothing inherently insecure about sharing the information (as long as you take into account the value of the information you're giving away), and you're aware of decreasing your security-through-obscurity. The best solution may be to use NagVis ($URL$ to create a user-friendly page where people can see status updates in a way that's more meaningful to them than the raw service list from Nagios. If you do decide to just display the output, make sure to read the Nagios security considerations page ($URL$ 

Dia ($URL$ is the preeminent free opensource diagramming software. It's also pretty ugly. If you're on a mac, I recommend Omnigraffle ($URL$ It's my favorite diagramming software on any platform. It's neither free nor open source. There are also online tools that actually look pretty good. My favorite so far is Gliffy: $URL$ 

I would guess power button as well, but it's possible that something else is causing it. Is this machine connected to a UPS? Are there any other people who have root access on the machine? Can you check lastlog to see if anyone else was connected when it was shutdown? What services is this machine running, and are there any published vulnerabilities on these services? 

Well, it's certainly not a typical way of doing it, and I really wouldn't suggest it for...well, anything other than an experiment, but you might be able to get OpenAFS to do it. 

If you're looking for something cheap and quick (which it doesn't sound like, but just in case), try MRTG. If you'd like an actual NICE graphing solution, I'd recommend Zenoss (they have a free and a commercial version) or Cacti, which is free. Both of these have great interfaces, pretty graphs, and send alerts based on thresholds. 

What this means is that your MX record HAS to point to an A (or AAAA) record to be valid. If the A record that you want to remove is where your MX is pointed to, then you can't remove it (and expect to get your email) 

Igor, your question is great, and like so many innocent questions, there are many, many answers, all at different levels of details. The piece of hardware is a web server. Obviously ;-) The piece of hardware is actually a cluster of load balancers, all of which are configured to pull from shared storage so they're all identically configured with identical material. The piece of hardware is actually one of several clusters of load balancers, geographically dispersed, and you were directed to the one closest to you, a decision made by the DNS server. 

Here's what I do: My primary database has transaction log shipping turned on. The logs are written to /db/archive. Every hour, a cron job runs as the oracle user. This cronjob does the following things: moves the contents of /db/archive/ to /db_archive/YYYYMMDD/ (using the following script (that I didn't write, and so don't hold me accountable for ugliness)) 

I agree with Zoredache, it depends a great deal on what you used to write the data. If you used tar (or amanda, which uses freely available tools), Richie's suggestion of using cygwin may be good, because you may be able to get whatever tool running under it. I don't think that 'dd' will give you much usable information, since it will essentially create tape images. The exception would be if you were to make images of the tapes and present them to a Linux virtual machine running whatever software you used to create them. More information will get you a better answer. 

BLARGH* If you disable it when you're developing it, you'll regret it when you turn it on and it doesn't behave the way you want it to. Getting fencing to work right is one of, if not the, hardest parts of setting up RHCS. At least it was for me. I'd recommend biting the bullet and getting that working first before you add any services. * - because of this: $URL$ 

If you open your IIS manager and browse to "Web Service Extensions" under your server, do you see "PHP ISAPI" in the right pane? (from here: $URL$ 

I've got to think that the performance of the underlying filesystem probably has a lot to do with what you're getting out of tar. What is the filesystem you're reading from, and how tiny are the files? 

Not being a puppet user, I'm sure others will chime in with more appropriate answers, but it sounds like a wrapper script could be implemented with relative ease that checks before starting the service. 

I recently started playing with Spiceworks, and I've got to admit, I suspect it works by the FM principle. As long as your machines are domain-authenticated, it will take care of everything. At the very least, use it to create documentation in a form that you're more familiar with. 

I'm configuring a laptop that will function as a "floater", or spare, for whoever needs one in the office. Because I want people to be able to work pretty quickly and not spend a few hours setting up their profiles, I was wondering if there is an equivalent to Unix's /etc/skel in Windows, where I can set initial new-user configurations? The OS is Windows XP SP3. I'm not to the point where I can use centrally managed profiles yet, but that will come along shortly. I'm hoping for a band-aid until then. Any suggestions? 

That should expand the logical volume to fill the volume group. Now the tricky part. Assuming you've got an ext3 filesystem, you should be able to resize live, on the fly: 

It should probably be noted that RFC 1178 is devoted to this topic: $URL$ (even if I disagree with a lot of it, and much in there is out of date). 

There's no redundancy to be had from NFS itself. As for network speed, AFAICT (as far as I can tell) there's no speed increase...unless the NFS server isn't available at mount-time. If it's unavailable, you'll have to wait for 7 NFS timeouts to pass...in other words, pack a lunch. CPU speed won't be a big issue, nor will memory usage. Make sure that you exclude the NFS mounts from the updatedb.conf so you aren't indexing them across the connection. If all servers are going to have to mount all shares, there's no reason (that I can think of) to make them individual. As soon as you have one server that shouldn't have all of them mounted, they should really be separate. It's dependent on your situation. One question I do have. You said that you have 7 large disks. Do you mean that you have 7 individual disks, or do you have 7 slices on a RAID volume? Because if you want redundancy, that should be your first step.