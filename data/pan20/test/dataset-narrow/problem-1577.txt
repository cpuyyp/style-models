If you look deeper in LSTMs or GRUs, we observe that the gates(input, output, cell or forget based on the RNN) are calculated using an equation like you specified. Example, according to deep learning tutorial of lstm, it=sigma(Wi xt + Ui ht-1 + bi) In this, h is the hidden state vector and x is the input state vector as specified and W and U are the corresponding weights for the input gate it. SImilarly, there are gates for output and forget. So in the paper, they recall a gist of RNNs and sum it up as a general equation. It is a common computational block in RNNs despite their minor differences. Refer Colah's blog or wildml, I think they are one of the best to understand RNNs. 

Yes, you can definitely try that. I suggest you try Character level LSTM as it helps in fixing certain spelling mistakes. Even if not that, you can try a simple trie. LSTM would probably need a lot of data so I suggest maybe using some basic rules first as there are a lot of unknown words. Maybe it is worth using a keyword based rule to eliminate the meaningless words. Maybe use some Wordnet and get words in English dictionary to get existing words. Then, check if a word is very close to a dictionary word using Levenshtein distance or another metric. 

I know there is f1_score metric to get all types of F1 scores(micro, macro and weighted). But I want to be able to print micro averaged F1 score using classification_report of sklearn. By default, it seems to be returning weighted micro averaged F1. But I want the micro averaged F1 in the classification_report. How do I do that? Also, I know the difference in formula between weighted and microaverages, but what are the instances where one would be preferred over other? And, what information do they convey? 

I have a basic multilabel topic classifier(Tfidf vectorizer with OneVsRest Classifier) built on some customer reviews. I observed that there are some classes with right features but it still predicts with very low confidence. 

Get a dataset which contains all kinds of tweets which you desire essentially similar to what your task ahead is. Look into Transfer learning. 

The basic task here I get is sentiment analysis of tweets. So for this, we can first extract features and then use a classifier. Stop words are those which are present for grammatical purposes but do not add any value/meaning to the sentence. So, yes you can remove the stop words from the tweet, extract the features and then pass it to the classifier. You can get already existing stop words from 

You have not given your enough information about your input and expected output. So let us assume that hospital name or anything for that matter which is the input for your model is , you would like to remove it from the dataset because extracting features from '' wouldn't make sense. Apart from that if they are just other peripheral features, then it might be alright. In that case, if you wish to convert them into blank then use . Else if you wish to remove that frame, you can check for nan using this. 

It is observed that the accuracy of training dataset decreases but the accuracy of validation dataset increases. I am not able to justify this behavior. 

Reinforcement Learning uses a simple logic of learning in which the network tries to learn from the feedback it obtains. This tries to optimise the overall reward in the long run instead of the current reward. This is one of the best platform to read about it. It also contains some useful links. As stated by the wiki, The basic reinforcement learning model consists of: 

Normally, as training dataset increases, the training accuracy is supposed to increase right? Also, assuming that the dataset is very noisy and hence training accuracy is decreasing as dataset size increases. But this doesn't explain why validation accuracy increases because noise is supposed to affect that too. 

I think Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras is one of the best tutorial available for this task. It does sentiment analysis on IMDB Dataset and is the simplest to understand. Also, it uses Keras framework which I use right now because of it's simplicity and it's online support and documentation available. The RNN of keras has multiple options which you can explore based on your requirement. Also, keras make it real easy to use an optimizer from a wide variety of options. 

Instead of averaging and getting a single vector for the tweet, you can instead get vectors for each word and for different length vector sizes, padding can be done with zeros. 

What are the ideal activation methods and error functions to use? I felt that, using softmax layer will give the sum of probabilities as 1 but in this case since it belongs to different labels, at each instance we might have high probability for each class. So I believe that Softmax is not an ideal option. Am I right? I find data for images but nothing prominent for text. Can anyone refer me to some source? 

The reason behind this is the default value for min_count is 5 in word2vec. Since my words have very less frequency, they are not being added to the vocabulary. 

It turns out the Adaboost or VotingClassifier doesn't work for multilabel classification data. We get a NotImplementedError. I believe they are working towards it and might be out in next version or so. 

Removing the stop words doesn't always improve the accuracy. Take a look at this as it is a very similar problem statement as yours. Hope it helps! 

Sigmoid helps in controlling the activation unlike ReLu which blows up it up. Sigmoids don't overfit as much. Have a look at this. I still would ask you to start with ReLu for training as it performs better most of the time. 

I observed that my GPU's memory is being consumed but the Utilisation stays 0. Because of this, my model is taking forever to load. I have tweaked this code to handle multilabel data. The only changes being-using y as a matrix instead of vector and different methods of calculation of cost and errors(Sorry that I am not able to provide the actual code). We can observe that the theano function train_model is taking a lot of time. I am a beginnner at Theano and also this method internally seems to having cascading variables and methods as well which is making me heard to debug. What might be the possible reasons for such a scenario? What debug methods should I use for this? 

Multilabel classification can seem to be a tough one in nlp. Recently there have been many techniques developed for this purpose. ML-PA-LDA algorithm seems to be working well with multilabel approach. PA stands for presence absence where they consider the correlations because of absence of a class in a document. 

The confusion matrix as the name suggest helps to identify how many of the predicted classes are being confused with true classes. For instance, in this case, the rows indicate the predicted classes and the columns indicates the correct classes. So if we consider the diagonal values, the predicted and the true class is the same. Hence, the diagonal values do not indicate any confusion. When we check the upper triangular part of this matrix, the value 11 in your example indicates that 11 examples have been predicted as A when the correct values are actually B. Also, looking at (2,1)(the first value of second row) the value is 10. This implies that 10 values predicted as B actually are A. This shows that A and B are confused. When two classes are closely related, it becomes harder to predict. For instance, an example I come across in my work is this. When I want to classify customer reviews, and classes seem to get confused because they have almost similar keywords in many of their examples. Since they both refer to speed, they are closely related and this leads to the confusion 

I just stumbled across Kur. At first look, it seems to be making deep learning easy in almost similar lines like Keras on a high level. What are the pros and cons of each and what is suggested to be used for beginners? 

In Text Analytic Tools for Semantic Similarity, they developed a algorithm in order to find the similarity between 2 sentences. But if you read closely, they find the similarity of the word in a matrix and sum together to find out the similarity between sentences. So, it might be a shot to check word similarity. Also in SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation, they explain the difference between and which is probably the reason for your observation as well. For example, coffee and a cup. They are not similar but they are associative. So just considering similarity would give a different result. The authors suggest various models to estimate them. 

I have a graph which plots training datasize on X axis and accuracy on y axis. I plotted the curves using sklearn's learning_curve. 

I observed that Adaboost or Bagging ensemble classifiers present in sklearn only work for single label training data. How do I use these for multilabel data? 

For LSTM, the documents should be at word level. Hence, sentence vectors are not that useful for a document but word vectors are. You can use an embedding layer if you want do it though. in the 3D tensor, the first dimension is number of sentences. so 1000 is correct. The second one is the number of time_steps which is the number of words for each sentence. The third one is the word vector dimension. Hence, taking your numerical example, the input dimension of the LSTM will be (1000, 10, 100). 

Keras does give a chance to add custom layers. I do not know about the previous versions but Keras 1.1.3 is flexible to do this. Have a look at this link. This also contains a good example for a custom layer. Here, you can try to change the weight matrix according to your need. I'd also suggest you to try reading source code of some basic layers to understand it better.