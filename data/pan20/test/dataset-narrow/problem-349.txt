Remember, async commit replicas cannot have automatic failover. Thus, this makes sense; our current role owner is the only possible and preferred owner. Since SQL Server is setup so that the only way to fail over is by forcing it then we don't want any other replica to pick up and start running with this, thus enforcing this at the clustering level. Two Synchronous Manual Failover, One Async - Demo The results will be the same as the Three Async demo. Why? This is the same explanation as the three async replicas, we have no automatic failover yet so we don't want anyone AUTOMATICALLY taking over. Thus, the settings are the same. Two Asynchronous Automatic Failover, One Async - Demo Here is where things change and get interesting! Since we're now introducing automatic failover - and that's one of the roles of WSFC (along with health checking, metadata distribution, etc) - we're going to want to have preferred and possible owners. 

None of the services know about each other This can severely impact your FCI RTO "Phantom" performance issues Non-Scalable setup Overly complicated troubleshooting (as a result of #3) 

AOAGs aren't going to help you in this respect. The transaction log is basically replayed from the primary to all of the secondary's. The secondary's could be readable (if you allow them to be) but won't be able to have any write activity (including ddl changes) made to them. A synchronous secondary is a mirror copy of the primary. Could these changes be made when the application changes are made to minimize downtime? It may make more sense to use a load balanced front end using something such as an F5 and keeping the data in sync through something such as peer to peer replication than use AOAGs. I'm not a replication expert, but it definitely sounds like you will need two distinct systems that have some type of load balanced front end so that changes can be made to a single system at a time to move resources in and out while keeping them in sync. 

No, there is an initialization vector that is used so the keys won't be the same. It's not based off of the password used. 

The FCI doesn't work without shared storage, so I'm going to assume you meant to say, "I have setup with FCI+AG solution but with asymmetric storage". 

Mount points are just incredibly useful. There are many ways to use them, the most common is to get around the drive letter limitations (as in, there are only so many) of Windows. The next most common use is to have smaller manageable sized drives (think LUNs, virtual disk[VMDK, VHDX]) to help get away from insanely large and rarely manageable monolith volumes (it's really becomes a problem to manage drives in the 10TB range as a single LUN, virtual disk, etc.) especially on older versions of NTFS where the implementation was less than the possible usage... for example, in older versions of Windows the maximum NTFS size was 2TB. Workload segregation is another great use. You can definitely see, there are many uses and it depends on your individual use case. There are also improper ways to use it... such as making a blanket statement that everything needs to be a mount point. That's just crazy administrative overhead at that point. 

The issue was fixed in SSMS 17.3, however you'll need to load the "applocal" assembly and not the one from the GAC. In addition, in order to be "backwards compatible", the property will stay a string and act/behave just like it did before. The newly added (and a getter/setter with it) will have the correct representation you're looking for. 

While the IP may or may not need to be public (different architectures at different companies' may or may not require different things), it seems in your case it does... though I'm not a networking architecture guru so I can't comment on how/why this was implemented or any technical challenges observed. 

You should ALWAYS use unless the client driver does not support it... in which case, upgrade the client driver and/or application... or deal with the application timing out. 

This does not make things simple. It actually creates much more manual of an environment. It is possible to automate things, but most of the common automation tools or powershell scripts aren't going to work as most assume either a standard login on each server (security issue) or implicitly use Windows Authentication which you now, won't have. Additionally there is the aspect of SQL Server where you'll lose all the support of AD. No Kerberos, all SQL logins (or local windows logins - which are widely not useful in these scenarios) and no groups, etc. Setting up the AG will need to be completed through certificate endpoints if you want to keep things secure and you won't be able to use any GUIs in order to set it up end to end. To reiterate, it doesn't make anything simple - only more complex. These types of configurations should only be used where they are expressly needed. If you want to continue going this way, that's your prerogative... I can only give advice and answers. 

Again, we're at the mercy of the interop + disk based table performance. Additionally, sorts are not cheap on HASH indexes and a nonclustered index should be used. This is called out in the Index guide I linked in the comments. To give some actual research based facts, I loaded the in memory table with 10 million rows and with 100,000 as I did not know the actual size or statistics of it. I then used the select query above to execute. Additionally I created an extended events session on wait_completed and put it into a ring buffer. It was cleaned after each run. I also ran to simulate an environment where all of the data may not be memory resident. The results weren't anything spectacular when looking at them in a vacuum. Since the laptop I'm testing this on is using a higher grade SSD, I artificially turned the disk based performance down for the VM I am using. The results came in with no wait info after 5 runs of the query on just the in-memory based table (removing the join and no sub-queries). This is pretty much as expected. When using the original query, however, I did have waits. In this case it was PAGEIOLATCH_SH which makes sense as the data is being read off disk. Since I am the only user in this system and didn't spend time to create a massive test environment for inserts, updates, deletes against the joined table I didn't expect any locking or blocking to come into effect. In this case, once again, the significant portion of time was spent on the disk based table. Finally the delete query. Finding the rows based off just ID1 is not extremely efficient with a has index. While it is true that equality predicates are what hash indexes are proper for, the bucket that the data falls into is based off the entire hashed columns. Thus id1, id2 where id1 = 1, id2 = 2, and id1 = 1, id2 = 3 will hash into different buckets as the hash will be across (1,2) and (1,3). This won't be a simple B-Tree range scan as hash indexes are not structured the same way. I would then expect this to not be the ideal index for this operation, however I wouldn't expect it to take orders of magnitude longer as experienced. I would be interested in seeing the wait_info on this. 

It was added in SSMS 2016... Not sure what you were looking for here as items are added to new versions. 

Honestly, you're basically saying "my application is using all of this data, how do I make it stop doing that?". Tell your end users or application to stop. Not going to go over well? Didn't think so. There is an algorithm that is used to keep pages in cache, obviously these pages are used more often and aged out less. If you want to keep other pages in cache longer, use them more. If you want a table to be in cache, setup an agent job to run a select query against that tale every 2 minutes, that'll keep it with a high last used value and reference count. If the problem is disk thrashing I would suggest asking for budget and installing more RAM. This is normal per how SQL Server works. You could also ask for faster disks if DAS/Local or have your storage team investigate to the slow nature of your disks if iSCSI/SAN/NAS. Either way the crux of your issue is either slow disk do to thrashing, slow disks in general, or not enough memory. Lastly, I would check your plan cache to make sure it isn't bloated with a bunch of single use ad-hoc plans that aren't parameterized correctly. That could bring back a few hundred MB. 

I would explicitly set any permissions outside those stated in the msdn link I have above (also given by @joeqwerty and in your OP). For example, on a "backup" folder on a network share, on a new drive added to hold new databases (where setup was already run but the drive didn't exist), etc. 

Correct, that isn't the case. Moving the services manually isn't a failure - you've told it to go ahead and change the owner, this isn't a failure. Rebooting also isn't viewed as a failure as long as it was controlled - for example, I run the shutdown -r command or click start->restart. You're telling the server, "Hey, have a nice controlled reboot - no worries."