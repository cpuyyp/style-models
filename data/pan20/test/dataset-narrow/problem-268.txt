In Azure we have a dedicated service for landing telemetry streams like this: Event Hubs. It's whole purpose in life is to capture the events and store them for long enough for your stream processors, which can be running in Spark, or Azure Functions, or Azure Stream Analytics, to reliably process them. Your stream processors would do things like archive the event streams, perform any realtime analysis you need, and load (perhaps sampled or summarized) data into a DMBS. You can also load a DBMS by batch processing against the archive of the raw telemetry, using the so-called "lambda architecture" see, eg $URL$ 

Concurrent MERGE statements will deadlock or produce PK violations by default, as the "scan" phase of the MERGE is performed without a restrictive lock. You need to add a lock hint for this to work. See $URL$ for MERGE locking details. Or just fix it like this: 

Here's the pre-Extended Events way to do this. Instead of using Profiler interactively, use it to generate a Server Side SQL Trace that writes to a file. After running the trace for a bit, you can query the trace files and/or load the trace files into a table and query those. Find a session that has the rollback, and then query for all the events for that session, in order. 

A Database Snapshot is a NTFS copy-on-write snapshot of the target database. There is only one copy of the unchanged pages shared between the database and its snapshot. If you use a backup/restore instead of a Database Snapshot, then you could put it on a different disk. 

No. That is separate. All partitions should go to a single filegroup, unless there's some good reason to separate them. You could potentially have a separate filegroup for some special tenant, but I wouldn't recommend that. 

The secondary doesn't apply the logical operation. The log record indicates exactly what modifications are made to the database pages. 

The meta-answer to this is to use SSMS to publish the article and examine the replication scripts. The answer is that the @source_owner parameter to sp_addarticle has the target schema. This dates from before schema/owner separation in SQL 2005, where an object owner and a schema were the same thing. And the script you list uses does not publish dbo.Product, it publishes Production.Product 

You could pass the data using a Table-Valued Parameter instead. Or you could try something like this: 

Other than that: What is the point of the TenantIsolationID? You should include TenantID in every clustered index that contains tenant data. It should be the leading column, unless you're using it for partitioning, in which case it can be a trailing column. You must plan for splitting your single database into smaller databases as a scale plan. But split can be a one-way operation. Whether TenantID is an INT or UNIQUEIDENTIFIER only matters for index size. All your secondary indexes will be bigger if you use UNIQUEIDENTIFIER. But that's not a huge cost. Fragmentation and page splitting won't be a big deal here. See Good Page Splits and Sequential GUID Key Generation for details on the performance implications of having multiple insert points in a table. 

The SQL Server Log File is not publicly documented, but there is a partner program for 3rd parties that want to build solutions that use it directly. EG Attunity, Quest, etc. 

Database-per-tenant is the best-practice here. There are scenarios where it is impractical, but should be your strong preference in designing any multi-tenant system on SQL Server. Database-per-tenant gives you: 

Putting aside the dubious utility of single user mode here, you need to be connected to the database when you put it in single user mode to prevent another session from connecting. EG 

I think that is the most restrictive lock locks currently held on the resource, as opposed to the locks requested. This is often the lock that is incompatible with the blocked session's request, but not always. 

if @@trancount <> 0 then that SELECT will run with the current transaction isolation level. Which (unless it already happens to be SNAPSHOT) will require S locks for reading data. If you want all SELECTS using the default READ COMMITTED isolation level to use row versioning instead of S locks, set READ COMMITTED SNAPSHOT on the database. 

You'll see that the query plans in the first batch are both compiled, then the first query runs, then the second fails. The third query succeeds without a permissions check. 

The DELL PERC H730 is a RAID controller, and is responsible for the RAID configuration for your server. Once it configures disks in a RAID configuration, it presents each RAID set as a single Disk to Windows. You'll need to figure out how to configure that controller, or ask for help from someone who can. 

There is no rule of normalization that prohibits null values or would require storing the adjacency list in a separate table. Both approaches are common, and there are not significant performance implications to your choice. Whichever design you choose, remember all foreign key columns need to be supported by an index. So you'll need an index on ParentID to support efficient traversal down the hierarchy. 

Would normally use the clustered index. But scanning the whole clustered index to count the rows, or to fetch a page of rows from deep in the sort order is expensive. So it's a separate non-clustered index on the same key is sometimes created to support these queries. 

Yes. All AG replication traffic goes over TCP/IP connections made to the database mirroring endpoint on the instance hosting the primary replica. The mirroring endpoint is, by default, listening on all IP addresses. If you configure the mirroring endpoint to listen on only one of a server's IPs, then when you create the AG you must register the ENDPOINT_URL using that IP address or a hostname that resolves to only that IP address. But, in your desired scenario you must have the ATL servers' database mirroring endpoints listening on both IP interfaces (or else NY wouldn't be able to connect). So if you currently have the ATL nodes joined to the AG with their ENDPOINT_URL set to the 192.x IP addresses, you'll have to change that to add the NY node. But you can ensure that the ATL-ATL traffic goes over the 192 network by using host file entries on the ATL servers, as @Tony Hinkle suggests. 

Since you have not applied any CUs for SP2 you can get the fix without taking all the CU fixes by applying SP2 GDR KB4057120 12.0.5214.6 or you can apply the latest Cumulative Update and the GDR hotfix at once with SP2 CU10 KB4057117 12.0.5571.0 

No. Some .NET data types map exactly to SQL Server data types, and int is one of them. And this method is not deprecated. See the docs here The issues caused by not specifying parameter types exactly are usually limited to bad plans. If you pass a parameter with a higher data type precedence than the column it maps to, then the column values must be converted to the parameter's type for comparison. And this will prevent index use, and require a conversion per row. Another issue with deriving the string parameter type from the value is that .NET's SqlClient will set the length of the parameter to the actual length of the string passed. This may minimize the network traffic and memory requirements, but can change the query plan caching and reuse behavior in SQL Server. For stored procedure invocations it won't matter, but for ad-hoc SQL batches, you can end up with a different cached plan for each string length. If you pass a numeric/decimal parameter with excessive precision this could potentially cause the loss of scale in the result, per the rules here, but parameters don't typically appear in calculations.