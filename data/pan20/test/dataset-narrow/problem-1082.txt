Grover's algorithm can indeed viewed with this notion of hitting time. You need to decide when to measure the system, and even though T is constant for all results, it is still important to calculate. Here T is certainly not $O(\mbox{dist}(v_0, v^f))$ (which in this case is 1), but rather $O(\sqrt{n})$, so your assumption that $T=O(\mbox{dist}(v_0, v^f))$ is not valid here. I assume the author is taking an entire packet to do the random walk. Obviously this requires a somewhat more complicated unitary, but I don't really see an issue. Alternately, Burgarth and Bose have a very nice scheme for encoding information across identical graphs which would work too if you simply replace their 1d chains with the network of choice (quant-ph/0406112). Well, you don't quite need this notion of hitting time. Hypercubes have perfect state transfer (see for example quant-ph/0309131 and quant-ph/0411020), so you can view transport on a hypercube as an interferometer with the Mach-Zender interferometer corresponding to the 2d case. 

As Tsuyoshi points out in the comments, there are a number of such bounds. However, for the sake of actually giving you an answer, let me point you to the Singleton bound, which states that for a $(s,N,d)$-code over $\mathbb{F}_b$ that $N\leq b^{s-d+1}$. 

It is well established that there exists a noise threshold for quantum computation, such that below this threshold, the computation can be encoded in such a way that it yields the correct result with bounded probability (with at most polynomial computational overhead). This threshold depends on the encoding used and the exact nature of the noise, and it is the case that results from simulation often give thresholds much higher than what can be proved for adversarial noise models. 

Suresh's answer probably answers the question for you, but I thought I would point out a special case. You can always work out the result for the less significant digits for any base. Take $p$ as our base. Clearly, every pth term in the factorial is a multiple of $p$. Every $(p^2)$th term is a multiple of $p^2$, etc. Thus the highest power of $p$ that is a factor of $N!$ is $X_p = \sum_{i=1}^{\lfloor \log_p(N!) \rfloor} \lfloor \frac{N}{p^i}\rfloor$. $\log_p(N!)$ is easy to approximate by Stirlings approximation: $\ln N! \approx N \ln N - N$. Further, $p^{N \lceil \log_p (N) \rceil} > N!$, so the sum can always be computed efficiently by summing instead for $1 \leq i \leq {N \lceil \log_p (N) \rceil}$ (since $\lfloor \frac{N}{p^i}\rfloor = 0$ for $i > {\lfloor \log_p(N!) \rfloor}$). Thus the last $X_p$ digits of $N!$ are zero in base $p$. 

As all gates in $G$ are unitary, all of their eigenvalues are roots of unity, which for simplicity I will parameterize by real angles $0 \leq \theta_i < 2\pi$. As $G$ has infinite order, either $G$ contains gates for which at least one value $\theta_k$ is an irrational multiple of $\pi$ or contains an arbitrarily good approximation to such an irrational multiple of $\pi$. Let us designate one such gate $g$. Then there exists an $n$ such that $g^n$ is arbitrarily close to, but not equal to the identity. Since $g^n$ is unitary it can be written as $\exp(-iH)$. Since the Pauli group as defined in quant-ph/9802007 forms a basis for $d \times d$ matrices, you can write $H = \sum_{j,k = 0}^{d-1}\alpha_{jk} X_d^j Z_d^k$, where $\alpha_{jk} \in \mathbb{C}$ and $|\alpha_{jk}|\leq \epsilon$ for any $\epsilon > 0$ (by [3]), with at least one such $\alpha_{ab}$ not equal to zero. We can then choose $C$ an element from the Clifford group which maps $X_d^j Z_d^k$ to $Z_d$ under conjugation. Thus $C g^n C^\dagger = \exp(-iCHC^\dagger) = \exp(-i(\alpha_{ab}Z_d + \sum_{(j, k) \neq (a,b)} \alpha'_{jk} X_d^j Z_d^k))$, where $\alpha'$ is just a permutation of $\alpha$ and $\alpha_{ab} = \alpha'_{01}$. Note that $Z_d$ satisfies $Z_d (X_d^u Z_d^v) = \omega^{u} (X_d^u Z_d^v) Z_d$. Let us define $g_\ell = Z_d^{-\ell} C g^n C^\dagger Z_d^{\ell} = \exp(-i(\alpha_{ab}Z_d + \sum_{(j, k) \neq (a,b)} \omega^{j\ell} \alpha'_{jk} X_d^j Z_d^k))$. By the Baker-Cambel-Hausdorff theorem, since all $\alpha$ have been made arbitrarily close to the identity, we can evaluate the product of $g' = g_1 \times ... \times g_d$ to first order as $\exp(-i(d\times(\sum_{k} \alpha_{0k} Z^k) + (\sum_{\ell = 1}^d \omega^d)\times\sum_{j \neq 0}\sum_k \alpha_{jk}X_d^j Z_d^k))$. Summing over all routes of unity, for $d>1$ yields $g' = \exp(-i(d\times(\sum_{k\neq b} \alpha_{0k} Z^k))$. This is basically a decoupling sequence which decouples the non-diagonal elements. As only diagonal matrices remain in the exponential, $g'$ must be diagonal. Further due to the restrictions on $\alpha'$ it necessarily has eigenvalues which are non-zero but proportional to $\epsilon$. By varying $\epsilon$ and repeating the above process it should be possible to generate $d$ linearly independent gates: $g'_1 ... g'_d$, such that their product results in a diagonal gate with with irrational and incommensurate phases or an arbitrarily close approximation to one. By the reference given in Mark Howard's answer this, together with the Clifford group, should suffice for approximate universality. 

Valiant gates as mentioned in Joshua's answer Clifford group gates (see arXiv:quant-ph/0406196) Match gates (see arXiv:0804.4050) Commuting gates, etc. 

$f_n(x)$ is injective $f_n(x)$ is surjective $f_n(x)$ takes strictly less resources (either space/time/circuit depth/number of gates) to compute under some reasonable model than $f^{-1}_n(y)$, where $y=f_n(x)$. The resource difference for $f_n(x)$ vs $f^{-1}(y)$ scales as some strictly increasing function of $n$. 

As far as I can see, an efficient algorithm for factoring semiprimes (RSA) does not automatically translate into an efficient algorithm for factoring general integers (FACT). However, in practice, semiprimes are the hardest integers to factor. One reason for this is that the maximum size of the smallest prime is dependent on the number of factors. For an integer $N$ with $f$ prime factors, the maximum size of the smallest prime factor is $\lfloor N^\frac{1}{f} \rfloor$, and so (via the prime number theorem) there are approximately $\frac{f N^\frac{1}{f}}{\log(N)}$ possibilities for this. Thus increasing $f$ decreases the number of possibilities for the smallest prime factor. Any algorithm which works be successively reducing this space of probabilities will then work best for large $f$ and worst for $f=2$. This is borne out in practice, as many classical factoring algorithms are much faster when the number being factored has more than 2 prime factors. Further the General Number Field Sieve, the fastest known classical factoring algorithm, and Shor's algorithm, the polynomial time quantum factoring algorithm, work equally well for non-semiprimes. In general, it seems much more important that the factors by coprime than that they be prime. I think part of the reason for this is the decision version of factoring co-primes is most naturally described as a promise problem, and any way of removing the promise of the input being semiprime is to either