The Score bar, with different fonts The ship itself A satellite attached to the ship, detachable Secondary Weapon Projectiles Primary Weapon Projectiles Moving objects (can be enemy ships) The Status bar, featuring an image of the ship, the number of lives and three sections that can fill The primary weapon charge bar, can fill to the end A scrolling starfield in the background (not numbered on the screen) Solid objects like rocks or metal walls that are part of the background 

I'm working on a (Windows-only) game in XNA, and I have multiple music tracks. I don't want to load them all into memory, but play them on demand. To properly dispose them, I have a separate ContentManager just for music and a method that looks like this: 

When writing a C64 game in 6502 Assembler and loading the game using , I can choose the address where the game is loaded to. The most popular address I saw is , closely followed by , but I also saw and used a lot. Why would I choose one over the other? I understand because it's close to which you have to write the instruction to, but the other addresses all seem arbitrary. Can someone shed some light into this? 

I'm trying to figure out the basics of C64 game development past BASIC, and I could use some advice on something very basic, but important: What is all that stuff on the screen actually, and how to draw it? Take a look at Katakis/Denaris: 

Nowadays, most music seems to be prerecorded, but that means they are rather static and take up a lot of space. I wonder if MIDI is still a viable option (especially considering consoles or iOS) and if I can expect General Midi 1 to be available everywhere? (Note: Referring to the software instruments for sound generation from notes) Alternatively, are there vendors of embeddable software instruments that can generate the desired sound at runtime? The whole point is to re-create a dynamic soundscape in which instruments can change their volume, speed or pitch - this can be partially simulated with individual tracks, but it doesn't offer the same flexibility. 

(MusicTracks is an enum). Is this the correct way to make sure I'm not using more RAM/Resources than required? Or should I be looking into XACT? To be clear, I don't need to play multiple Songs at the same time (I do need to play 1 Song and a few SoundEffects though) and I don't need any crazy effects apart from setting the Volume of the music independently from the soundeffects, which should be straight forward with MediaPlayer.Volume. 

Your language looks like CSS, but it isn't. It's your own Domain Specific Language. Ask yourself this: Do you really want to create your own file format, for which you need to write a parser. This parser needs to be fast and robust. You also need to document your file format and teach people how it works - after all, it's a customized language. You then need to be aware that you may run into issues you never thought of - maybe someone thought it's a great idea to create a 50 MB file and your parser crashes, corrupting a savegame or so. Or you decide you need to add a feature that can only be implemented with a breaking syntax change, thus breaking all the existing files. The reason why formats like XML are so popular is because XML has these problems solved. You can find many great XML parsers that are proven to be robust, fast and leak-free. Also, many people know XML files and how to edit them, and because it's such a generic but well defined syntax you can be sure that you can extend later. Your motivation seems to be easy editing by hand, which is common in community SDKs. Now, the reason people edit those files by hand is that they don't have the tools the Game Devs use. You can assume that a company developing a game for several years has some graphical tools for editing such files - they may not be great and bugfree, but they edit the files for you and developers only rarely edit them by hand in order to tweak something. Those tools are rarely released in 'Modding SDKs', so most modders edit by hand. So my initial reaction would be: Instead of spending time developing a new, unproven format that is easy to edit by hand, I'd rather use a battle-proven format and spend the time writing tools that make editing easy. But blanket statements don't work like that. It always depends. If you spend 3 or more years writing a big game with a sizable dev team, then you can spend a lot of time developing, testing and finetuning your system. If you have 15000 files to parse, then finding ways to reduce memory usage becomes important. But for small/medium/indie games, I'd go with a known and well-supported format like XML and a nice tool to edit it. 

I have a typical Phong shading with specular light. I have all the parameters tuned so to give appropriate highlights from a point light source. Now the problem I have is a new object that has a large flat surface which tends to face directly towards the light source (which is a bit offset from the camera). It also has a normal map which is primarily in one direction (steel panels). This results in a pure white object. I have a lot of options to try and work around the problem, but I was wondering if this is a common issue and perhaps there is a clean workaround. The various options I see (and am trying) are: 

In teh tangent space vertex attributes why is the bitangent (aka binormal) provided as an attribute instead of calculated? If the three vectors of the tangent space must be perpendicular then the bitangent can be calculated as the cross-product of the other two vectors. There are technically two perpendiculars here, but surely we could consistently calculate the one we want, even if it requires a boolean marker to negate it. Similarly the bitangent itself doesn't need a full vector to be represented. From the normal alone we know the plane on which the tangent must reside. In theory a simple angle would be enough to calculate the tangent vector. Given that GPU memory is at a premium why do we provide the full vectors for these values rather than calculating them (with reduced data input)? Or am I wrong in assuming the tangent space must be a cartesian system? 

Pretty much the vertex shader has to run on all the vertexes you provide to the pipeline. The reason is simply because the vertex shader can alter the position of the vertex. Before the vertex shader runs the renderer will have no idea whether the vertex is in or out of the frustum. I suppose more correctly it doesn't even know what a position is until told via ; prior to this point the pipeline has no knowledge whatsoever where the vertexes are -- thus it couldn't possibly filter them. The fragment shader will of course not be called if none of it vertexes lie within the display (this isn't guaranteed of course, but we assume the card is at least half-intelligent). So if your question really is do you have to do your own culling of vertexes, the answer is yes. Of course there may be extensions which could alter this flow, but I think this covers the general case. 

Essentially you'll need two data structures (logical, intrusive, or real, depending on the rest of your code). The first will track the chains of objects, and the other the path. Chain Simply you need to know which objects are following other objects. In the simplest case this will simply be A follows B, but could include more followers. There is a designated leader in the chain. Path For each chain you'll need a path. Depending on how your game works will determine how this is structured. In most cases it will be some kind of linked list. This will track the positions that everybody in the chain needs to follow. Now, the leader in the chain will be adding items to the path. Each time it moves it will add something to the head of the list. Each object in the chain remembers where on the list it is. When it comes to moving it simply moves to the next item in the list (interpolated appropriately if necessary). As the last item in the chain moves past an item in the list, that item can be dropped (it will be at the tail). Metaphorically the leader leaves a breadcrumb trail for its followers. The last follower in the list consumes the breadcrumb. Whether your list contains individual points, or just the vertices of a path, or something else, is determined entirely by your game engine. But in any case I don't see that you'll be able to avoid the list itself. 

The ContentManager in XNA 4.0 only has one Unload() method that Unloads all Assets. I want to have some "global" Assets that are always loaded, but then I want per-Level Assets which should be Unloaded when the level is changed. Should I create a second Instance of the ContentManager as part of the Level.cs Class? Or should I use Game.Content and then call .Dispose on the Assets I load? Or should I create my own ContentManager on top of the ReadAsset function as outlined here? 

The C64 only supports 8 sprites per horizontal scan line, so I don't think that everything in the middle of the screen can be a sprite? I would assume that anything requiring a collision would be a sprite (since I can get hardware collision detection with sprites), but even then I quickly hit the limit of 8 sprites. Also, my weapons can fire much more than one projectile - my ship, the satellite and 6 bullets would already be 8 sprites on a row (look at about 50 seconds into the video). Also, which graphics mode would a game like this use? The Programming Handbook lists Bit Map Mode which essentially modifies screen memory directly. Is this the mode I should usually be working in? How would I compose all the non-sprite elements together to get them on screen? A lot of the stuff in the score and status bar is static ("Area: 01" or the "frame"), so I guess I'll just populate them once when the level starts. Things that need updating - the score, the charge bars at the bottom - would be updated by filling the screen memory with black and then drawing the new score every frame? Or do I have to draw the entire screen on every frame? 

I'm interested in learning about Shaders: What are they, when/for what would I use them, and how to use them. (Specifically I'm interested in Water and Bloom effects, but I know close to 0 about Shaders, so I need a general introduction). I saw a lot of books that are a couple of years old, so I don't know if they still apply. I'm targeting XNA 4.0 at the moment (which I believe means HLSL Shaders for Shader Model 4.0), but anything that generally targets DirectX 11 and OpenGL 4 is helpful I guess. 

Generically speaking, how would you handle a huge 2D Map of which only a part is displayed? Imagine the old top-down racing games like Micro Machines. I would know how to do something with a tile-based map, but I want to create completely custom Maps. Target Devices are iOS, Windows Phone 7, Android, Mac and PC. Assume the Map is too big to fit to fit into a single Texture. Would I have multiple textures, 4096x4096 each and load them all into RAM? Seems wasteful, and if textures are uncompressed I might actually run out of graphics memory. Would I only load the max. 4 Textures that I need at any given point (when I'm at the intersection)? Or would I have one huge image file and load parts of it? In that case, are there any image formats that make it easy (O(1)) to find the file offset and length which I would have to load? Or is there a completely different algorhithm? Are textures the wrong idea? How would you implement a game like Micro Machines? 

id Tech 3 isn't really free - it's GPL, so your entire Game has to be GPL. You used to be able to license a non-GPL Version for $10,000 but that options seems to have disappeared after the Bethesda-acquisition. Apart from it's age, id Tech 3 doesn't have much in the realm of Single-Player AI. So if you want to make a "traditional" shooter, you have a lot of AI work to do yourself. Another problem is the tooling. There are a lot of Map Tools, but that's about it. There is very little assistance in the Asset Pipeline or Shader creation. On the upside, the Team Arena source contains an early version of MegaTexture and supports relatively large outdoor environments.