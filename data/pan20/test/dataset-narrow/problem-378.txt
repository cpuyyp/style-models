Relational databases have to accommodate missing or irrelevent values. For example, the list of customers may include a mobile phone number or a gender. What if a corporation wants to become a customer - what gender should it have? A special indicator is used to show a value is missing. This special flag is NULL. So sloppy programming may cause the missing value indicator - NULL - to be confused with Jennifer Null's surname, perhaps interpreting her surname as missing data rather than an actual value. It's a bit like opening a bank account but not depositing any money. The balance is zero. This does not mean you have no balance. It means you have a balance and its value is "0". But sloppy programming could misinterpret the zero as a "does not exist" and determine that you do not have an account at all. 

Multi-row rollback is one of those things which are foregone in the NoSQL world, sadly. That same link mentions the $isolated operator. This may be of some help, depending on the use case and topology. Also documented is the so-called "two phase commit" pattern. This may require application changes, however, so may not be practical. Five million rows isn't a silly-big number. Would it be possible to arrange an application outage until the writes succeed, with continuous retries? Maybe run the write application on the same hardware as Mongo so the network is no longer an issue. 

These two schemas represent different business rules. The first, with idFactor as a FK in Sanad, allows each Sanad to be connected to at most one Factor (if the foreign key column is nullable) or exactly one Factor (not nullable). The second, with a separate table, allows a Sanad to connect to zero, one or many Factors. Of course the second model is a superset of the first and can be limited to zero-or-one. There is no trivial way to do this in SQL Server, however. It will required additional constraints and triggers, meaning extra programing, debugging and maintenance. You should implement the model which matches your business rules. This will force the data to be correct. It also self-documents the intent of the application. Sufficient performance can be achieved through sensible SQL and indexes. To answer the actual question, it is likely to vary according to the ratio of reads to writes and whether Sanad field1, field2 etc. are returned. In schema 2 each row inserted in the intersection table will require two FKs to be checked i.e. two table reads. In schema 1 there will be 1 FK read. To check only the existence of a Sanad/Factor connection may be faster in schema 2 because the rows are shorter, there will be more rows per page and hence a better chance that the required page is in memory. This only applies if there is a small working set of "hot" rows. To read the data columns in scenario 2, however, will need reads of both interesction and Sanad tables; double to IO. So, if working set size is bigger than RAM, and the vast majority of reads only check for existence of a link, and there are relatively few writes, and you're confident to do the extra programing to ensure data integrity, or you do actually have a many-to-many relationship, go with schema 2, otherwise stick with schema 1. 

Joins you either pre-compute and store the denormalised data in the document. Or the application implements a join algorithm for runtime. Indexes can be defined for whichever values inside the documents you'd like to have them. The documentation adequately describes the options. As currently, they help reads but have a write overhead. Keys: there are no keys. Each row has a unique identifier which you can assign or let the system generate a value. This would correspond to a surrogate primary key. Foreign keys, uniqueness, check constraints, assertions etc. are the appapplication's responsibility. 

There are many good schema comparison tools out there. Redgate Schema Compare and SQL Server Data Tools for Visual Studio are two that come to mind (I'm not affiliated with either). In a development environment, create two databases which match the the current "live" schema. Deploy your changes to one database only. Use the tool to compare the updated DB to the current DB. Call the generated script "Deploy.sql" or something similar. Now use the tool to compare current to updated -- NB the order has switched. Call the generated script "Rollback.sql". An important aspect of both roll-forward and roll-back scripts is that they should be idempotent i.e. running the script once or multiple times against the same DB should produce the same output. Generally this requires a lot of "if exists .." statements. The tools can generate these for you. If space is tight you can use the actual production database as the "current" DB. I prefer not to because of the (small) extra load and the (small) risk of mistakes. 

If you are trying to measure this by, say, running it in SSMS and looking at the elapsed time in the status bar, be aware that is also measuring the network time to transfer the results from the database engine to the client and can include a significant amount of network variability. You could and . This will show how much work the DB engine itself is doing. Minimising these numbers, and analysing the query plan, should give faster-running queries. 

I'm looking at Resource Governor on 2014 and how it relates to volumes. I can see where to set up MAX|MIN IOPS PER VOLUME on a resource pool. That's not keyed by volume, however sys.dm_resource_governor_resource_pool_volumes is. Should I read that as the pool's definition applies to every volume but the DMV reports what actually happened on each? My understanding is this: say I have three volumes - E, F and G. I set min = 10, max = 300 on the pool. Then there can be, at most, 300 IOPS against each drive for a max of 900 for the pool. The DMV shows what IO actually happened for each volume. If the table/filegroup definition were such that the majority of the activity happened on one volume (let's say F) the numbers from the DMV would show large and for that volume and smaller numbers for the others. Is this a correct understanding? Neither BoL nor any of the blogs I've read make this explicit. 

Most often the CASE is written with the column name given and various values listed. I've inverted this so multiple columns can be checked against a single, known value. This could be achieved just as well using the longer form of CASE i.e. 

One of the features of a relational database management system is that it separates the logical presentation of information to clients from the physical storage. The former is columns in a rowset. The latter is as files on a storage volume. It is the DBMS's job to map one to the other. It is a concern only to the DBA or sysadmin which hardware is supporting the information need. The consumer need not, indeed should not, be aware of those concerns. Neither should the client be concerned about how a rowset is constructed. A base table, a view or a function are all, in this respect, identical. Each simply produces a set of rows and columns which the client consumes. Even a simple scalar can be considered a one-row, one-column table. The row / column model is the contract between the client and the server. By prefixing the artefacts' names with their implementation type this separation of concerns is broken. The client is now aware of and dependent on the internals of the server. Change in the server's coding may require client re-work. 

BIML is designed for this kind of semi-repetitive task. One defines a "template" that includes placeholders for the file and table names. The tool will then generate the corresponding 150 SSIS jobs. 

There may be similar concerns for dates, depending on the chosen character representation. Again type casting can cure this with the costs mentioned previously. Occasionally you will find strings which contain digits only. Examples are national identity numbers, bank account numbers, phone numbers and such like. The common characteristic of such values is that it does not make sense to perform calculations on them. Other things being equal it may be OK to store these as columns, especially if they have optional embedded alpha characters, but the above considerations still apply.