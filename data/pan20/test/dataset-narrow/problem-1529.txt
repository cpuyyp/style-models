Like Word2vec is not a single algorithm but combination of two, namely, CBOW and Skip-Gram model; is Doc2Vec also a combination of any such algorithms? Or is it an algorithm in itself? 

Abbreviations used: 1) H.B. - Health Block 2) H.F. - Health Facility 3) H.S.F - Health Sub-Facility 4) P.H.C. - Primary Healthcare "In terms of a political entity, a state is any politically organized community living under a single system of government." "A district is a type of administrative division that, in some countries, is managed by local government." As per the problem statement I get an intuition that it is a classification problem but the data set lacks output variable. What other kinds of ML algorithms could be applied to this data? Does the data makes any sense? Do I need additional attributes? What kind of data analysis could be performed and the intuition behind it? 

I'm building an NLP question-answering application using Doc2Vec technique in package of Python. My training questions is very small, only 20 documents and I am getting very inaccurate and different similarities even for same document while running at multiple instances. Almost all the sources which I referred trained data set containing thousands of documents. So I infer the reason behind my model's inaccuracy is the size of my data set. Is there any way to improve the similarity between documents, maybe by changing parameters or feature engineering? If yes, what are those parameters and by what ratio should I change them? If no, what are other ways or perhaps other neural network models to tackle the problem? 

I have a that contain around 3000 observations. Every observation falls in one of the five categories (these are pre-defined). I am using and from the package for classification. Before I run the algorithms i optimize the of each algorithm using the . For model validation I am having a look at the for both algorithms, for both and . The issue is that in sample the algorithms perform relatively well for all 5 categories but out of sample they they only perform well for one category. I guess that is very common. I have 2 explanations: the 5 categories are not equally divided, this means that for a couple categories I have a few observations. Second, is that the algorithms are overfitting. The "1m dollar question" is how can someone avoid overfitting ? I thought of running a preliminary analysis on my data set, to see which variables (in my case most of them dummy variables) in my data set are the "most important" for the classification, and then use those, instead of the whole data set. This could be tested by taking the means for the variables per category and which variable has the highest difference across the categories. Could this be a good idea ? I understand that the subject is too broad, but I think some ideas from experienced people on this topic could be useful for everyone ! 

I know that i use as argument , but still for instance the optimal combination for gbm using is the "50th optimal combination" for the gbm using , and the "2nd optimal combination" for gbm using , is the "14th optimal combination" for gbm using 

Popularity Based Approach - Popularity Based Approach will recommend items, in your case, mutual funds, based on the total count of purchases. It will rank the mutual fund with maximum purchase in the past at the top. The item at the top will be recommended to every user/client. 

I assume you want to build a Recommendation Engine which will recommend mutual funds to the clients based on their profile. Recommender System is a special class of Machine Learning and make use of Supervised/Unsupervised Machine Learning techniques. According to Wikipedia, 

Content Based Approach - In Content Based Approach, features of both the item as well as the user/client is used to offer personalized recommendation. According to Wikipedia, 

So essentially, the item here in your problem statement is the mutual fund of which you need to predict the preference a client would give. Higher rating/preference of a Mutual Fund, let's call it X for a client A will mean X should be recommended to A. Now, how do you give rating/preference to a list of items? Recommender Systems uses a number of approaches listed below: 

I am trying to implement Doc2Vec model to convert a corpus into vectors using a pre-trained model (). I want to return the resultant vectors and save it in a text file. This is my code: 

Popularity Based Approach Content Based Approach Collaborative Filtering Based Approach which composes of Nearest Neighbor and Matrix Factorization Hybrid Approach 

Without going into technical jargons, Collaborative Filtering Approach is of two types -- User-based Collaborative Filtering & Item-based Collaborative Filtering. In User-based Collaborative Filtering, let's say user/client A bought two items X and Y and user/client B bought the same items X and Y. So these two users are similar to each other based on their purchase history. Now if the user/client B purchases a third item Z, Z will also be recommended to user/client A. In Item-based Collaborative Filtering, let's say a user/client A bought items W, Y and Z. User/client B bought W and Y. User/client C bought only Y. What will be recommended to user/client C? Answer is item W because item W was co-purchased with item Y by user/client A and B. These comes under Nearest Neighbor techniques of Collaborative Filtering because we are trying to classify a user or an item by determining its nearest neighbor. An algorithm known as kNN (k-nearest Neighbor) is used to do the task. There is one another technique known as Matrix Factorization as well. Hybrid Based Approach - Hybrid Based Approach, as name says, is a combination of Content Based Approach and Collaborative Filtering Approach. It is used largely used by Netflix and Amazon to give best recommendations. 

I could not understand how the global gates are computed and why would they be equal for each word for first pass. They are going to be computed as a function of question vector (same) and word vector (different for each word). This is how vague i am on what is going on here. Can somebody explain how the piece in the paragraph mentioned come together? 

Run a BiLSTM over the question, concatenate the two end hidden vectors and call that the question representation. Run a BiLSTM over the context paragraph, conditioned on the question representation. Calculate an attention vector over the context paragraph representation based on the question representation Compute a new vector for each context paragraph position that multiplies context-paragraph representation with the attention vector Run a ﬁnal LSTM that does a 2-class classiﬁcation of these vectors as O or ANSWER. so a final softmax(ReLU(affine)) layer over LSTM output of output size 2 (O or ANSWER). 

Dynamic Memory networks are described here . I understand what is going on for question answering task but when it comes to sequence to sequence modeling, they describe it in 4th paragraph of 2.4 answer module. 

I understand to check only a few dimensions but what does the part after be careful mean. I fail to understand that warning. can somebody explain it? 

@shimao has answered it Here on stats stackexchange To quote shimao "Yes, it would be more logical, although equivalent both ways. I was able to find a bit of discussion on this here It looks like the original authors later implied that both training and test time should use the unbiased variance. Also the actual implementation in many libraries is not completely consistent with what is described in the paper." 

Indeed is a parameter for Tree Booster. There are 4 choices, namely, , , and . The default is set to which heuristically chooses a faster algorithm based on the size of your dataset. Approximate algorithm () is chosen in your case since your dataset is huge. To change the tree construction algorithm, you have to pass in the function as follows: 

So, let's say if one of the client with features UF1, UF2 purchased a mutual fund X with a set of features F1, F2 and F3 in the past. New mutual fund products, lets say Y and Z will only be recommended to this client if their features matches with that of F1, F2 and F3. It is also called Classification Based Approach because you are feeding in list of features of products and user into a classifier to predict what item should be recommended. To build a Content Based Approach model, you can use Machine Learning models like Naive Bayes Classifier or a Decision Tree. 

I have a data set which is distributed across various web services. The problem statement given to me is 1) Is it possible to determine which pregnant women are prone to an abortion? 2) Is it possible to determine which pregnant women are least likely to go to a hospital for delivery? 3) Is is possible to determine which pregnant women are most likely to give birth to an under weight child? I have made a comprehensive list of all the web services and the attributes within and it is given below: 

If it is a categorization problem then you should look for a classification algorithm, not a regression technique. The simplest classification algorithm is Logistic Regression. But by the looks of it, seems like you do not have a labelled data-set and if that's the case you should look for Clustering techniques. Clustering is a part of Unsupervised Learning Technique in ML which create clusters or groups of similar data points. 

Then I am trying to do the same but with different . So in the above i use , and in the following i use , so i run the following code: 

What I would like to do now, is to plot a 3D contour plot (so that I can actually see the "mountain" that is created after plotting the histogram of $y_{1}$ against the histogram of $y_{2}$). So in the z-axis I would like to have the frequencies of the values. Any suggestions ? 

I am not aware about Instagram, or if there is a ready-to-use dataset for your purposes, but in general in order to get data from (almost) any online source you should a method called "web scraping" (This might help you while you google). It might also be useful to you, if you decide what kind of tools you are going to use for your analysis. I have tried something simple, but it was for Twitter though Here is a rather simple piece of code that gets twitter tweets concerning using 

I also get the whole data table as output in my console, which floods my console. Any ideas how to fix it ? (It might also be a setting in R, but I am not aware of it. 

The , , and can be obtained as it is explained here in Also in that site you can see a guide of how to web scrap data from Twitter using Python. You might also find useful this (Python), this (Python) and this (R) 

Here is the code I wrote to answer my question. It might not be the most efficient one but it works. Sharing is caring :) 

Temporal data is across time. Data points are correlated and you model them using a variant of RNNs (LSTMs, GRUs, Tree recursive NNs etc). example would be text script of speech. Notice how each data point would be correlated with the previous data point local data is ambiguous and could mean a lot of things. local to a particular location? local data as opposed to server data? local to a particular task? etc etc. Please provide context 

If memory is not the constraint, should i not always keep its value equal to the number of processors available with me on GPU. Here i have my first question - Am i right in thinking that number of processors on GPU mean number of parallel floating point computations it can undertake (i don't suppose GPU's too implement 2 threads per processors as in CPUs)? Now, GRU machinery is: (please note the biases are not showcased in the image but are to be used for better performance) , To me the value 32 seems too low to be kept as default. If i have a GRU with 250 dimensional hidden state and 100 dimensional input i would have (2 x 250 x 350 x batch_size) floating multiplications that i can run in parallel (computing r and u) followed by (250 x batch_size) floating point additions that i can run in parallel followed by (250 x batch_size) sigmoid applications which are at least (250 x batch_size) potential parallel computations. Then it is a (250 x batch_size) parallel computations in application of reset gate followed by (1 x 250 x 350 x batch_size) parallel computations followed by (250 x batch_size) parallel computations for biases. Now in the final step it is (250 x batch_size) floating point computations to calculate (1 - update gate) followed by (500 x batch_size) parallel floating point computations for output state. To me it seems the bottlenecks in parallel implementations using GPU would be presence of too many low sized layers which, for instance here, are bias additions after matrix multiplications (at any rate they are still 250 x batch sized big here so will anyway use GPU to the maximum) Is the way i am thinking in the second paragraph correct while analyzing a deep network for a vague idea on how much the hardware would be able to impact its running time 

I want to choose the "optimal" hyperparameters for gbm. So I run the following code using the package 

I have 2 vectors $1000 \times 1$, lets call them $y_{1}$ and $y_{2}$. Each vector represents a normal distribution with certain mean and variance. I plot the contour plot using the following R code: 

Assuming that we have a dataset with the respondents on each row ( respondents) and their respective characteristics as columns ( characteristics). Each respondent has also a . In case of high number of respondents, is it a good idea to remove the duplicate respondents and sum their s ? Will this lead to different results ? So my initial data would look like this 

I have 2 matrices, A (1000x21) and B (1000x7). Matrix A has individuals(=1000) in the rows and their consumption in 21 days at the columns. Matrix B has the SAME individuals(=1000) in the rows and some weights for each day of the week(=7) in the columns. What I would like to have in the end is 1000 (with the dimension of 2x21) matrices (one for each individual), lets call them $X_{i}$. In the first row of each $X_{i}$ I would like to have the consumption of the individual $i$ each of the 21 days (this will come from matrix A), and at the second row of the $X_{i}$ I would like to have the respective weight of that day (this will come from matrix B). So matrix A looks like $[cons_{1,1} \ cons_{1,2} \ ... \ cons_{1,21} \\ \ cons_{2,1} \ cons_{2,2} \ ... \ cons_{2,21} \\ . \\ . \\ . \\ \ cons_{1000,1} \ cons_{1000,2} \ ... \ cons_{1000,21}] $ Matrix B looks like $[weight_{1,1} \ weight_{1,2} \ ... \ weight_{1,7} \\ \ weight_{2,1} \ weight_{2,2} \ ... \ weight_{2,7} \\ . \\ . \\ . \\ \ weight_{1000,1} \ weight_{1000,2} \ ... \ weight_{1000,7}]$ And I would like the matrix $X_{i}$ to be like $[cons_{i,1} \ cons_{i,2} \ ... \ cons_{i,21} \\ \ weight_{i,1} \ weight_{i,2} \ ... weight_{i,k}]$ Any ideas how to do this in R in a loop ? 

In lecture notes for cs231n while discussing checking analytical gradient with numerical gradient the paragraph says this: 

Take the first batch of data as first S (Number of time steps) (S1, S2, .., Ss) words from each article (for the sake of simplicity let us assume batch size = m) Set the initial hidden state H0 = [0,0,..,0] Calculate loss and gradient on this batch and update the parameters We move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration Do this to the end 

I am training a model on squad dataset for question answer. The problem is as follows In the SQuAD task, the goal is to predict an answer span tuple {as,ae} given a question of length n, q = {q1,q2,...,qn}, and a supporting context paragraph p = {p1,p2,...,pm} of length m. Thus, the model learns a function that, given a pair of sequences (q,p) returns a sequence of two scalar indices {as,ae} indicating the start position and end position of the answer in paragraph p, respectively. Note that in this task as ≤ ae, and 0 ≤ as,ae ≤ m The approach i am taking is: 

*In this scheme we would have to use zero padding on the last batch (at the end of articles) Scheme 2 Same as Scheme 1 but in step 5 we reinitialize H0 to a vector of Zeroes Scheme 3 Same as scheme 1 but in step 4 we move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration Scheme 4, 5, 6 Same as Scheme 1, 2, 3 but instead of taking s consecutive words we take first sentence from each article and zero pad them to the length S What is the right way to go through the data in feeding it to a RNN, Please give reasons as well. I think it should be Scheme 1 but it could be really slow