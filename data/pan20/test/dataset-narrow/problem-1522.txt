You would then turn the text into features using some sklearn pre-processors. Count Vectorizer or TF-IDF Vectorizer are popular choices. You can also create your own features from keywords from the Attribute key and values you already have, such as creating a Brand indicator column if the words Samsung, Nokia or OnePlus appear in the text, but I would never use only manually specified features when modeling with text features. 

Your line just returns an array of scores for each run of the cross validation, so the error is telling you that it's just an array of numbers, not a model itself and has no defined method for fitting things. To use predict, you need to call it on a model object, which I'm assuming you're going for the VotingClassifier. Your variable has a fit method, so you want to call 

However, if you see the performance on model one does better overall, you might shift the average to 

Is there any reason you have to use Naive Bayes? While Naive Bayes does handle multi-class modeling quite nicely, sometimes the "naive" assumption that each word in the text is independent of the others is too naive, especially given the size of your training set. While you may be able to increase the accuracy a little bit by doing more text processing, I wouldn't expect to see significant improvement, especially given the training size. Since Naive Bayes itself doesn't have much parameter tweaking, if you want to try more processing of the data itself to improve performance, you can try things like text count vectorization or TF-IDF vectorization to represent the text data more holistically than just keyword flagging. If you are able to implement other models, I would take a look at the scikit learn multi-class models for ideas. Tree based methods are also inherently multiclass, and have more parameters to tweak, so implementing something like Random Forests may suit you better. As for the training size itself, that does seem a little small. Especially if you are using text data, the more features you include, the more data you need to be able to stand a chance of detecting a signal in the data. 

Word2vec works in two models CBOW and skip-gram. Let's take CBOW model, as your question goes in the same way that predict the target word, given the surrounding words. Fundamentally, the model develops input and output weight matrices, which depends upon the input context words and output target word with the help of a hidden layer. Thus back-propagation is used to update the weights when the error difference between predicted output vector and the current output matrix. Basically speaking, predicting the target word from given context words is used as an equation to obtain the optimal weight matrix for the given data. To answer the second part, it seems a bit complex than just a linear sum. 

I'm working on Seq2Seq model using LSTM from Keras (using Theano background) and I would like to parallelize the processes, because even few MBs of data need several hours for training. It is clear that GPUs are far much better in parallelization than CPUs. At the moment, I only have CPUs to work with. I could access 16 CPUs(2 Threads per core X 4 cores per socket X 2 sockets) From the doc of multi-core support in Theano, I managed to use all the four cores of a single socket. So, basically the CPU is at 400% usage with 4CPUs used and the remaining 12 CPUs remain unused. How do I make use of them too. Tensorflow could also be used instead of Theano background, if it works. 

Given just one line of the data, it's a little hard to go off of, but I'm assuming you're trying to get at the number after each colon, and the number before it refers to the column name? If so, you can use read_csv with a little tweaking: 

The problem type you're dealing with is referred to as multiclass classification. Not all algorithms are suited to handle it, but tree based methods and neural networks are popular choices. If you need it to run quickly and probability calibration isn't too important, Naive Bayes also works quite well for some data sets. To see an example of a dataset of this type, check of the Kaggle Spooky Author Identification competition. The published kernels give some good examples of feature engineering and modeling choices. (I'm assuming the category label is unique. If there can be more than one label per record, it's called multilabel classification, which I would handle by building a separate binary model for each of the 39 labels in your data set. For an example dataset, check out the Kaggle Toxic Comments competition.) As for modeling with your data in specific, the structure you have it in now seems a bit odd to feed to a model. A training data set should have each row represent one record, and each column represent a feature with the value in the column describing the record, whereas in your format, any cell phone should have information regarding each value in the corresponding mobile Attribute_Names. When you get the raw input 'Samsung Galaxy On Nxt 3 GB RAM 16 GB ROM Expandable Upto 256 GB 5.5 inch Full HD Display', how is that transformed into a format that can be fed into the model? Also, where are the Attribute_Names, Attribute_Values coming from, are they manually specified? If so, that is limiting the performance potential from a model since there could be additional words in the data the model could detect if left to generate features on it's own. For a good modeling flow, the training data should contain the inputted raw text, then process the text to generate features, then feed into the actual model to output a label. So one row of raw training data set would be: 

Obtain all the word vectors of context words Average them to find out the hidden layer vector of size Obtain the output matrix ( or ) which is of size Multiply by , the resulting vector will be with size Compute the probability vector with size , where the highest probability denotes the one-hot representation of the target word in vocabulary. denotes size of vocabulary and denotes size of embedding vector. 

Start with , it gives an idea which part of posting is requirements and what part is description and then you could try just with bag of words. As the topic structure and vocabulary is standard and no weights are needed for word importance, I dont find a reason for TF-IDF to be used here. 

In a draft copy currently being written by Andrew Ng, he discusses about the amount of data in train-test dataset. My understanding from the book, The traditional and most common value is 70-30 or 75-25. If you have 10k or 30k samples, it is fine to go with 70-30 split. But when dealing with Big-data, for example if you have 1 million samples, it is not recommended to have 30k samples as test data, so in that case, 90-10 is actually okay. Because 10k test samples can pretty much provide an intuition about the model. in brief: for less samples, go with recommended 70-30 split, for much higher samples, go with number of samples Draft copy link : ML Yearning 

If instead you want to create a second model, you can try using any machine learning model suitable for classification. I would probably stay away from larger models like Random Forests, but doing something like using the predictions as features into a Logistic Regression model can work well. But again, I would emphasize only trying to use the predictions from a holdout data set into the logistic regression model to make sure you aren't over fitting. 

The ultimate end goal of your modeling is going to affect the way you want to format your data. It's a good practice whenever you start a machine learning project to ask yourself, what is the precise question you want to answer, because whatever model you generate, it's only going to make sense if used in the context of the question asked. If in your case you want to predict loan approvals, then first you need to check whether that information is even present in the data you have. The JSON you have just shows historical data, but do you know the outcome for each data point? Is that another feature, maybe recorded in the "someValue", "title" area of the file? Without it, you really can't do anything. If you can get that information, then it's perfectly fine to generate a variable for each year: assets_year1, assets_year2, ... assets_year8, investments_year1, ... etc. True, year8 might have a lot of Null values, but that's not necessarily bad. For example, most models working with text data consist of really sparse training matrices, yet they do very well in practice. Depending on the algorithm, it might weed out those variables anyways. With Null values, you just have to try imputing the Null values differently, and you can also create an indicator variable for whether the column has missing values, and see how different settings change performance. To take it a step further, you can also create additional features to encapsulate the nuances that are occurring over the years. Create features like the historical average, average change per year, total number of years of history, etc. Creative feature engineering is the hardest part, but can lead to big changes in performance. 

In my work, I have done the same way by averaging the word vectors. But there is another idea I wanted to try. It is with the help of POS tags. First construct a most complicated sentence with all the POS tags as possible and set these POS tags as a template. For each sentence in the twitter corpus, POS tag all the words in it and apply those word vectors respective to the POS tags in the template. So, the unseen POS tags will have zeros. For example: is the template. Thus each position will contain 300-dimensional vector totally a 3000-dimensional vector. And if the first tweet's POS tags are , then the word vectors are applied on these positions and have vectors on positions and 300-dimensional zero vectors on other positions. This method not only solves the problem of representing the sentence by a single vector but also preserves the syntactic structure of the sentence by positioning in the right POS. Ofcourse, there will be problems when there are more than one POS tags or jumbled positions of the tags. This is just one of the possibility. 

In paragraph vector, the vector tries to grasp the semantic meaning of all the words in the context by placing the vector itself in each and every context. Thus finally, the paragraph vector contains the semantic meaning of all the words in the context trained. When we compare this to word2vec, each word in word2vec preserves its own semantic meaning. Thus summing up all the vectors or averaging them will result in a vector which could have all the semantics preserved. This is sensible, because when we add the vectors (transport+water) the result nearly equals ship or boat, which means summing the vectors sums up the semantics. Before the paragraph vector paper got published, people used averaged word vectors as sentence vectors. To be honest, in my work these average vectors work better than document vectors. So, with these things in mind, in this way it could be compared. 

The regression line is almost exactly between the population means and is almost vertical because both ordinates of the population means are zero. If I take a couple of thousand points then it will be vertical. The code for this picture: 

Some approaches when there is a small amount of labeled data and a large amount of unlabeled data: Semi-supervised learning $URL$ - mixtures of supervised algorithms on labeled data and unsupervised algorithms on unlabeled data. One of them (label propagation) is even implemented in scikit-learn $URL$ Active learning $URL$ - algorithms that actively choose the data from which they learn, so they can achieve better performance using less labeled data. These two approaches are complementary. Therefore, there are combinations of active + semi-supervised learning algorithms. 

The x-component of the sample mean is overestimated (it should be = 1) and standard deviations are underestimated (they should be = 1). As a result, the line separating the classes is different from the true line on the previous picture, so a lot of new data will be classified incorrectly. Let's take a couple of more samples randomly from the same population. Again, the size of the minority class is 10, and I resample them using the same two methods. 

Lateral connections exist so that the update of a neuron forces the neighboring neurons also to be updated but to a lesser degree. Think of each neuron that has n inputs as a vector, a point in an n-dimensional space. It's coordinates are the values of the weights on its inputs. When the network receives an input - also an n-dimensional vector, each neuron calculates the distance between its weights and the input vector. The neuron whose weights are the closest to the input is the winner, and is allowed to update its weights. It updates them by moving one step towards the input vector. The size of the step is equal to the learning rate. While it moves, it uses the lateral connections to pull its neighbors (or push some of them away, depending on their distance to the winner and on the form of the function that you use for the lateral connections). As a result, the neighbors also move but they move less than the winner. Only the winner and its neighbors are allowed to update their weights. All other neurons don't move. The neighbors are those who are closer to the winner than a certain radius. This radius is a hyper-parameter. This type of learning allows to chart the space of the input vectors. The result is the weights of the neurons are distributed more or less uniformly in the set of the input vectors. After training, each neuron can be thought of as a representative of some region of the input space. 

If selecting the most important and relative questions is the question and you choose the option: option 1.) find the closest document to each answer(30 docs) and after the documents are validated relative or not. You will have the results of each person's evaluation (persons x 30 scores). Now order the questions in descending order from most scored question to less scored one. option 2.) Summing up all the answers and finding 20 docs. After the validation of relativeness by the persons, it is not possible to find the important features(questions) because they are summed up and the information is already lost. From this result you could only find which person validates closer to your model among all, which is not the aim of the work. 

how about Doc2vec. Mikolov released the journal after the successful attempt of word2vec. A single feature vector for a whole paragraph or sentence or document (depending upon your use case) could be generated. Personally, I didn't get quite good results with it for sentence classification problems. Rather I went with averaged word vectors for sentences. 

I've tried to explain the logic behind labels used in Document vectors in Doc2Vec - How to label the paragraphs (gensim) To answer your questions. 1) when two documents share the same label, then doc2vec algorithm determines the semantic meaning of the label from both the documents. Note that doc2vec learns the semantic meanings of labels not individual documents. 2) Again, you are not learning documents. you are instructing doc2vec to learn the embeddings for the labels. So, if multiple labels are given for a document, all receives the same semantic meaning from the document and when a part of labels when used in other documents, keep on learning more semantic meaning from them. For instance. doc1-> doc2-> . It is clear that doc1 is about animals and doc2 is about human, the label will have semantic meanings from both of them. 3) If your goal is to find similar question such as this, then you should probably give just a single label for the whole question and then find a question with a label having closer cosine distance. 4) Never confuse labels with words. In word2vec, words learn embeddings and in doc2vec, labels learn embeddings from the words used in documents. If you would like to add some more semantic meanings to a document, then you could add it to the particular document as words. If you want to add semantic meanings to a label, the words have to be added to each document which carries that label if you want the label to have strong affinity (but adding words manually, doesnt sound like a good option to me personally).