Here are two variations on the definition of NP. They (almost certainly) define distinct complexity classes, but my question is: are there natural examples of problems that fit into these classes? (My threshold for what counts as natural here is a bit lower than usual.) Class 1 (a superclass of NP): Problems with polynomial-size witnesses that take superpolynomial but subexponential time to verify. For concreteness, let's say time $n^{O(\log n)}$. This is equivalent to the class of languages recognized by nondeterministic machines that take time $n^{O(\log n)}$ but can only make poly(n) nondeterministic guesses. 

[I'll answer the question as stated in the title, leaving the litany of other questions about GCT for other threads.] Proving the conjectures arising in GCT seems like it will crucially use the fact that the functions under consideration (determinant and permanent, and other related polynomials for P/poly and NP) are characterized by their symmetries. This necessity is not a formal result, but an intuition expressed by several experts. (Basically that in the absence of characterization by symmetries, understanding the algebraic geometry and representation theory that arises is far harder.) This should bypass Razborov-Rudich because very few functions are characterized by their symmetries (bypassing the largeness condition in the definition of natural proofs). Again, I have not seen a proof of this, but it is an intuition I have heard expressed by several experts. Now, over the complex numbers, it's not clear to me that there's an analog of Razborov-Rudich. Although most of GCT currently focuses on the complex numbers, there are analogs in the finite characteristic (promised in the forthcoming paper GCT VIII). In finite characteristic, one might actually be able to prove a statement of the form "Very few functions are characterized by their symmetries." 

Note that the generalized version of Ladner's Theorem implies that there are infinitely many poly-time degrees strictly in between $\mathsf{\Sigma_i P}$ and any poly-time degree strictly above it. In particular, if the hierarchy collapses to the $i+1$-st level but not the $i$-th, then there are infinitely many p-degrees between $\mathsf{\Sigma_i P}$ and $\mathsf{\Sigma_{i+1} P} \cap \mathsf{\Pi_{i+1} P}=\mathsf{\Sigma_{i+1} P}$. If I recall correctly, constructing an oracle for which $\mathsf{PH}$ looks like the arithmetic hierarchy is still an open problem. By "looks like the arithmetic hierarchy," I mean that $\mathsf{PH}$ does not collapse, and $\mathsf{\Sigma_{k} P} \cup \mathsf{\Pi_k P} \subsetneq \mathsf{\Delta_{k+1} P} = \mathsf{\Sigma_{k+1} P} \cap \mathsf{\Pi_{k+1} P}$ for all $k$. This at least suggests that the answer to your question may not be known. Ker-I Ko gives oracles in which he separates the levels of $\mathsf{BPH}$ from those of $\mathsf{PH}$. As these two hierarchies intertwine one another, this gives you at least some information about relativizable collapses of problems between the levels of $\mathsf{PH}$. This next reference is the wrong direction, but you might also be interested in the result and its techniques. Chang and Kadin showed that if the Boolean hierarchy (which lives entirely below the second level of $\mathsf{PH}$, but extends $\mathsf{DP}$ to a whole hierarchy) collapses to its $k$-th level then $\mathsf{PH}$ collapses to the $k$-th level of the Boolean hierarchy over $\mathsf{\Sigma_2 P}$. 

Although this is not quite what you are asking about, it is in the same vein and hopefully you (and other readers of your question) will find it of interest. You should definitely read up on the Curry-Howard correspondence, which says that the category of programs is, in a specific sense, isomorphic to the category of constructive proofs. (This is discussing proofs and computability at a different level than the other answers.) 

Are there any natural problems in $NP \cap coNP$ that are not (known to be/thought to be) in $UP \cap coUP$? Obviously the big one everyone knows about in $NP \cap coNP$ is the decision version of factoring (does n have a factor of size at most k), but that is in fact in $UP \cap coUP$. 

I believe all known GI-completeness results are functorial (definition in the paper), and Babai has recently shown (ITCS 2014, free author's copy) - based on bounds on the structure of automorphism groups of strongly regular graphs - that there is no functorial reduction from GI to strongly regular GI. 

Depending on which version of $\mathsf{NP}_{\mathbb{R}}$ you use, yes or it's open. When one considers BSS machines that only use addition and subtaction, and only branch on equality, the answer is yes. If one includes branching on $<$, I believe it is still open, and the same if one allows multiplications. For details, see Cucker, Koiran, and Matamala "Complexity and dimension," Inform. Proc. Lett. 1997 

The following (earlier) paper shows that solving a system of linear equations reduces to computing the rank, and thus, together with the above result, solving the system (in particular, finding a basis for the nullspace) is also in $\mathsf{NC}^2$: 

Lance Fortnow wrote an article that explains quantum computing without using quantum mechanics. He presents it essentially the same way one would present probabilistic computing. I suspect this may be a quicker starting point than something like Nielson and Chuang (though I agree that if you want to really go into this then Nielson and Chung should definitely be on your reading list). L. Fortnow. One complexity theorist's view of quantum computing. Theoretical Computer Science, 292(3):597-610, 2003. Special Issue of papers presented at the second workshop on Algorithms in Quantum Information Processing. 

Your idea generalizes as follows: given an algebraic circuit (over the finite field) or Boolean circuit (computing the bit-wise representation of your finite field elements) computing $P$, then maintain the value at each gate in the circuit. When you change the $i$-th bit of $y$, simply propagate that change along the DAG of the circuit, starting from the input $y_i$. If the circuit has size $s$, this takes $O(s)$ time and space. This could be much smaller than the number of monomials (which corresponds to the size of algebraic circuits of depth only 2). 

The answers of John Watrous and Andy Drucker are excellent for understanding some of the issues involved. I'll just add to Andy's answer that, not only is no such implication known, but showing such an implication requires non-relativizing techniques (of which essentially only one--arithmetization--is known). Lance Fortnow and John Rogers [1] constructed an oracle where $P = BQP$ but $PH$ is infinite (and in particular that means $PSPACE \supseteq PH \supset\neq AM$ in such a world). We know that $IP = PSPACE$ doesn't relativize, but it does algebrize. I do not know if the Fortnow--Rogers result shows (or can be extended to show) that such an implication would require non-algebrizing techniques. [1] L. Fortnow and J. Rogers. Complexity limitations on quantum computation. Journal of Computer and System Sciences, 59(2):240-252, 1999. Special issue for selected papers from the 13th IEEE Conference on Computational Complexity. Also available here. 

Far from it. Indeed, any countable distributive lattice embeds as a sub-partial-order of $\leq_p$, even if we only consider those degrees in between two given fixed languages (K. Ambos-Spies, Sublattices of the polynomial time degrees, Inform. & Control 65(1):63-84, 1985). 

I don't know about a method for analyzing an arbitrary given algorithm to come up with a cache policy in general (this sounds quite hard), but this is essentially what's been done (optimally, in an asymptotic sense) on a case-by-case basis for most known cache-oblivious algorithms, by analyzing their divide-and-conquer structure. Cache-oblivious algorithms are known for FFT, matrix multiplication, sorting, and a few others. See the Wikipedia page and references therein. 

Regarding Question C: Number of monomials is essentially the same thing as circuit size of depth-2 algebraic circuits (unless the polynomial is a product of linear polynomials, in which case one could also consider a depth-2 circuits with a multiplication gate at the top). Degree is very closely related to the depth of algebraic circuits, as in the results of Hyafil and Valiant-Skyum-Berkowitz-Rackoff. 

It is known that Ford-Fulkerson or Edmonds-Karp with the fat pipe heuristic (two algorithms for max-flow) need not halt if some of the weights are irrational. In fact, they can even converge on the wrong value! However, all the examples I could find in the literature [references below, plus references therein] use just a single irrational value: the conjugate golden ratio $\phi' = (\sqrt{5}-1)/2$, and other values that are either rational, or are rational multiples of $\phi'$. My main question is: 

Here's a suggestion of why it might be difficult to come up with an example of such, though I agree with Kaveh's comment that it would be surprising if it didn't exist. [Not an answer, but too long for a comment.] Suppose that someone, say me, comes up with such a language $L$. A natural way for me to prove that $L^{=n} := |L \cap \{0,1\}^n| = 2^{n-1}$ is to explicitly build a bijection between $L \cap \{0,1\}^n$ and $\{0,1\}^n \backslash L$. Since I personally am not able to decide instances of $\mathsf{NP}$-hard problems, most "simple" bijections that I will come up with will have the form "$f\colon \{0,1\}^* \to \{0,1\}^*$ is a length-preserving bijection, and $x \in L$ if and only if $f(x) \notin L$." Furthermore, I'm likely to come up with such an $f$ that is computable in polynomial time. But then $\mathsf{NP} = \mathsf{coNP}$, for $f$ is a reduction from an $\mathsf{NP}$-complete set to a $\mathsf{coNP}$-complete one. Of course, this objection can be gotten around by "simply" having the bijection be harder to compute than that. If your bijection takes exponential time - say it and its inverse might both be $\mathsf{EXP}$-hard - then I think you're pretty safe. But if it only takes, say, quasi-polynomial time, then note that you still get the consequence $\mathsf{coNP} \subseteq \mathsf{NTIME}(2^{(\log n)^{O(1)}}) =: \mathsf{NQP}$, from which I believe it follows by a simple induction with padding argument that $\mathsf{PH} \subseteq \mathsf{NQP}$. Now, if you believe the preceding containment is simply false, then no such quasi-poly-time computable bijection can save you. But even if you believe it might be true, then by coming up with such a bijection you would prove $\mathsf{PH} \subseteq \mathsf{NQP}$, which seems to be beyond current knowledge... The objection can also be gotten around by simply not having such a bijection, but then it seems harder to see how to prove that $L$ has the desired property in the first place... And in fact, even if your proof isn't a bijection, you'd need it to be the case that no such easily computable bijection even exists. Of course, this is also the type of thing where someone will come along with an example and we'll easily see how it gets around this objection, but I just wanted to throw this out there to say how anything with a simple enough bijection can't work (unless widely held beliefs are false). (Related question: is there an oracle relative to which there is no such $L$?) 

The fundamental reason there are such limitations on communication complexity is that there is only ever a linear amount of total information that needs to be communicated (the inputs). Although Hartmut Klauck already essentially pointed this out in his answer, I wanted to highlight an answer to the other OQ regarding the underlying reason for this fundamental limitation, namely, that the players are computationally unbounded. If one would like to consider "higher" communication classes, a natural thing to look at (instead) is combined communication/computational complexity, which people are definitely aware of and has been studied in various guises, but I think hasn't really been systematically studied. For example, in the study of interactive proofs, it is common to consider the effects of the computational limitations of the players, though not quite as common to consider the total number of bits communicated. The latter is more common in studying PCPs, where e.g. a PCP of poly size requiring $d(n)$ queries only needs $O(d(n)\log n)$ bits to be communicated. When $d(n)=O(1)$, I think the converse is also essentially true, so that query complexity in PCPs is closely related to this issue of combined communication/computational complexity. 

(Now, of course, it's possible that these problems all have subcubic algorithms, and then there might be a polynomial difference between computing and verifying, but for these problems there can't be a cubic difference. And it seems plausible to me that in fact they all require essentially cubic time.)