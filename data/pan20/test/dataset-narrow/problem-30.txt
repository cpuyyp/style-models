Cross-process Vulkan interop is not something external code is expected to define. There are extensions that provide various aspects of such interop. 

They are not "very similar images". They have quite different aspect ratios. The first one has a 5:6 aspect ratio, while the other has a 3:4 ratio. If you try to print a 5:6 image at 8x10, what you find is that if the width is 8, then a 5:6 image would have a height of 9.6, which is less than 10. So that means the 5:6 image can fit into the 8x10 region with its full width; some of the height will go unused. However, a 3:4 aspect ratio image at 8x10 doesn't do that. If the width is 8, then the 3:4 aspect ratio would require a height of 10.6. That would exceed the maximum available height, so the width cannot be full width. Instead, it uses the full height of 10, and the width used will only be 7.5. You will find that this is the case for all of the ones where you noted such a difference. The "DPI" its computing is essentially the number of pixels in the dimension that isn't the maximum. For the 3:4 image at 8x10, it takes the image width (1000) and divides by the area width (8). For the 5:6 image at 8x10, it takes the image height (1200) and divided by the area height (10). This number only makes sense if we assume that the image is being scaled independently in width and height to fit the page. That is, the aspect ratio of the image is not being preserved when printed. So a 5:6 image is being transformed into a 4:5 aspect ratio image when printed at 8x10. Since each dimension is being scaled differently, each dimension has its own DPI. The lowest DPI will be for the dimension being stretched the most. And that's what this formula seems to be computing. Of course... who cares? Stretching images creates obvious visual distortion, and should pretty much never be done when printing something you actually want to have a paper copy of. So I would declare this computation algorithm to be bogus and move to something that actually makes sense with a reasonable printing methodology. 

Where refers to wrapping in the S component of the texture coordinate. Unfortunately, OpenGL picked S, T, and R long before GLSL and swizzle masks came around. R, of course, conflicts with R, G, B, and A. To avoid such conflicts, in GLSL, the texture coordinate swizzle mask uses S, T, P, and Q. In GLSL, you can swizzle with XYZW, STPQ, or RGBA. They all mean exactly the same thing. So is exactly the same as . However, you're not allowed to combine swizzle masks from different sets. So is not allowed. 

You seem to be using additive blending against its purpose. Additive blending is supposed to represent light from multiple sources being combined. It is not physically possible for one source of light to eclipse another. Furthermore, even if you hack an alpha of exactly 1 to mean "opaque", you will get a strange circumstance where an alpha of 0.99 is quite far from "nearly opaque". So if you're animating "opacity", then you're going to get a sudden jump from appearing to be transparent to completely opaque. Lastly, opacity with blending can only be achieved if you render the objects in the correct order. So even if you use this hack on the pink square, it will only eclipse the sphere if the sphere is rendered before the square. Using this hack will lose you the order-independency of additive blending. That last part makes what you want essentially impossible. However, if you wish to persist in this, there is a way to do what you've asked for. You need to use dual-source blending (which means you can only be rendering to a single render target). This is a technique whereby you output two values from the FS to the same render target: 

Extensions can change the visible behavior of the implementation, not to mention the implementation of it. Consider KHR_external_semaphore and its various sub-extensions. If you don't specify them, then the 's implementation knows that you're not going to need to be able to turn a VkSemaphore into an externally-accessible mutex and back. So the implementation may be able to use a more efficient implementation. If the extension is always on, the device must assume that you're going to use it. And thus, any possible efficiency is lost. 

The fields of are specified by the or functions. In the case of the latter, it also specifies the corresponding data. Note that stores a buffer object. That's where that binding point reads its data. This structure is not magical; it simply explains how to read vertex data from its associated buffers. 

Everything you're trying to do will "add the workload of GPU". The only question that matters is how much. Your choices are: 

However, given that you've effectively designed yourself into a corner, I'd say that the simplest approach is to develop a way to encode uniform values, rather than relying on callbacks. Uniforms can be scalars, vectors, or matrices (arrays and structs use multiple uniform locations). So you need the equivalent of a "variant" type that can store all of such data, of the types supported by GLSL. And then, for each object, you walk through each uniform location in that object's data block and set that value. Sure, "visitation" is not the most instruction-cache friendly approach, but it's a lot better performing than calling a . At least with visitation, you're executing the same block of code each time. So what you'd have is a (possibly dynamic) array of uniform data, where each uniform datum is a location, a type, and the variant data. I'll leave the storage optimization of the "variant" as an exercise for the reader. Generally speaking however, non-block uniform usage should be fairly minimal, dealing only with values that change frequently while rendering with a program. So even in that case, things shouldn't be too bad. 

The black-and-white squares represent our texture. It's just a checkerboard of 2x2 white and black texels. The orange dot is the texture coordinate for the fragment in question. The red outline is the fragment's footprint, which is centered on the texture coordinate. The green boxes represent the texels that an anisotropic filtering implementation might access (the details of anisotropic filtering algorithms are platform specific, so I can only explain the general idea). This particular diagram suggests that an implementation might access 4 texels. Oh yes, the green boxes cover 7 of them, but the green box in the center could fetch from a smaller mipmap, thus fetching the equivalent of 4 texels in one fetch. The implementation would of course weight the average for that fetch by 4 relative to the single texel ones. If the anisotropic filtering limit was 2 rather than 4 (or higher), then the implementation would pick 2 of those samples to represent the fragment's footprint. 

With modern hardware, morph targets probably should not be VS inputs. They should be SSBOs or the D3D equivalent. The vertex shader should fetch from these arrays of positions and act on them appropriately. You can pass the number of targets as a uniform or something. 

D3D12 has the same separation between compute pipelines and graphics pipelines that Vulkan does. However, when issuing commands, D3D12 has only one pipeline binding point, to which you can bind any kind of pipeline. By contrast, Vulkan has separate binding points for compute and graphics pipelines. Of course, Vulkan doesn't have different descriptor binding points for them, so the two pipelines can interfere with one another. But if you design their resource usage carefully, it is possible to invoke a dispatch operation without disturbing the needs of the graphics pipeline. So overall, there's no real difference in pipeline architecture here. 

Yes, there's a time not to use GL types when interacting with OpenGL: . There is absolutely no reason to use this type. With everything else (assuming you're using a "standard" OpenGL loader), you should use OpenGL's types when talking to OpenGL. That is, the values you pass to OpenGL functions should be in OpenGL's types. This may mean that a function that talks to OpenGL may need an OpenGL data type in its function parameter list. This should be avoided where possible, to prevent you from creating dependencies between OpenGL code and non-OpenGL code. However, if you don't mind having large portions of your application have dependencies on OpenGL, you may use OpenGL's data types throughout your code. OpenGL's data types are specified to have a particular bitdepth. As such, they are (conceptually, if not strictly C++-legally) pointer-interconvertible to certain types defined in . Indeed, many OpenGL headers will check for the existence of this header and use it wherever it is available to define OpenGL's data types. As such, if you want a fixed bitdepth integer, you have a choice besides using OpenGL's data types. 

Desktop OpenGL requires implementations to convert pixel data between the internal format of the image and the format you specify in the pixel transfer command. OpenGL ES does not. Indeed, it is so serious about not allowing this that it actually changes the meaning of the parameters to functions like . In ES, you cannot use sized internal formats (well, outside of texture storage calls). Instead, the internal format is kinda there. What really defines the internal format is the pixel format and type parameters. That is, when you tell OpenGL ES that the pixel data you're providing looks like X, it requires the implementation to create a texture whose real internal format exactly matches that. And if you're using or , you must provide pixel transfer parameters that match those you provided to the call. So OpenGL ES doesn't allow any of that pixel conversion stuff. 

How? If you have two s, unless they are both attached to the same , there's no guarantee that "everything" is the same thing for both devices. 

For consoles (and D3D12/Metal/Vulkan), you know everything you need to know to develop an allocation strategy. You know: 

With this blend function, if you want a fragment shader's output to be opaque, you set to 0. Otherwise you set it to 1. The other components are irrelevant. Of course, you must still suffer the limitations of dual-source blending. Namely, that you can only render to a single render target on most hardware.