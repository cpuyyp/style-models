You need to use the "Route" command. route ADD (site ip) MASK (site umask) (gateway) IF (number of the interface you want to use) So to get to 200.200.200.200 on interface 2: 

Original (circa 2011) ebury was pretty lame...You could clear it by doing: yum update sshd. Pretty trivial. If you trust your repo's (and rpm), you can do rpm -vVa and it'll show you every installed rpm where the md5 doesn't match the one stored on the server. Problem is that the attacker can be assumed to have acquired root level access, and that means you could be screwed past the ability to diagnose it from the local machine. If you can't run a file integrity scanner off an unaffected machine, then you're never going to be sure unless you rebuild from scratch. 

Only if you're running graphically intensive stuff on the server (e.g. playing games in the server room during maintenance downtime). All those terminal server things are set through the client. The server won't do graphical processing for the clients. The there are some edge cases where people write software to use the GPUs on high-end graphics cards to do actual processing, but 99% of the time, the graphics card runs the monitor that is attached to the computer. That's it. I haven't seen a server with more than basic onboard graphics...Well...Ever. Sun used to do some that could do fancy graphical crap, but I think that was more about looking pretty than actual functionality. 

As along as you're checking to make sure that your file-processing script isn't already running, then you should be fine. /tmp is fine for this sort of thing, unless you're experiencing random reboots, and that's a pretty big problem on it's own. 

Not enough data. How are they being packed? How bumpy is the road? If you're using reasonable caution, you probably won't see any problems, with reasonably new servers, but you can always lose a hard drive. I'd make sure my backups were up to date, and the servers were well padded and secured. @lombardm I'd be worried too. Almost all of those things can shorten the life of your HDDs. Definitely make sure of your backups. Still, in all likelihood, this will only (again) shorten the life of those drives. Most of them are going to weather it fine. 

Hmmmm. Well, 6 gigs is a decent amount of ram, even for a big MSSQL install. You might actually want to look and make sure that your code really IS efficient. A 6 gig transaction is a bit unusual...I've worked on state-wide payroll systems that didn't top a gig on year end 1099 processing...And to have one running often? I don't know. What kind of data are you working with? That being said, you can stuff as much RAM as you like in a 64 bit box, and ram is dirt cheap, so might as well put as much in there as you possibly can...Can't really have too much RAM on a database server. Edit: This is wildly out of date now. I have MSSQL boxes with 256 gigs of RAM. 

The client is mstsc.exe; search for it (it'll be in Windows/system32, probably) right click on it, and go to the version tab. @Zoredache: Depends on which version of Windows you're running. 

Make sure you're not over-riding the value in another file. You can set that value in any one of a hundred places. 

Technically, you can't fix the php.ini file while it's running (iirc it's only read when apache starts), but your don't actually have to change it to make your changes. ini-set allows you to change the option within the code. It's a bit tedious to set one option at a time, so they added parse_ini_file to...well, parse an .ini file. You can also over-ride the values with an .htaccess file, using the php_flag directive. 

I don't know. My gut says wipe it, and pretend it never existed. If they know you had it, they may make a stink over it, regardless of how often you say they have the only copy. As a freelancer you need to keep a very strict separation between your systems and their systems, especially where things like backups are concerned. In the future, make sure they know what you're going to do with their data before you actually touch any of it. Tell them your retention policy, and when your contract is up, dump it on 'em and delete the traces. Write it up, and make 'em sign it. You can buy cheap boilerplate NDA forms at various places online. It's worth it, and it tends to calm down the people you're working with. 

Well, if you're redirecting to port 8901 and you haven't marked your lo interface as "trusted" by accepting all from it, then you're probably just blocking yourself at your own firewall. Assuming you've set up the site to correctly listen at 8901, you should add another line: 

Well, the defaults aren't too bad. Might need to check minFreeThreads and minLocalRequestFreeThreads: both of those get set to "8", whereas most of the other parameters scale based on the number of cores. In the end, it depends on your application. The server doesn't take into account how processor intensive your code is, so if you have a really processor/memory intensive page, you might want to go in and tweak the default values down so that fewer processes are running at the same. Conversely, if your pages are really resource light, you might be able to tweak the defaults up. This is all fine tuning. If you set autoConfig="true" and you don't see any problems, then it's fine. If you have slowdowns and errors, you're going to need to revisit the configuration. 

(Though I tend to use tds version = 8.0 when I'm working with MS SQL Server, and it works fine) Edit: (Going to put my responses here so I can code-format them) Your /etc/odbcinst.ini should look like: 

Well, speaking as a guy who works for a company that, until earlier this year, had external ips for every...single...computer...in the whole company. I mean, the crappy pc in the break room? External ip. Junky laser printer in an office that hasn't been used in 5 years? External ip. Gotta get your money's worth out of that /16 block... This is not a problem...There is nothing inherently wrong with having an externally routable ip...Hell, ip6 is based on the idea that that is the right way to do things. The real question is how much protection do you have on that machine? Because a domain controller is a fragile beast, and it shouldn't be allowed to play on the internet unsupervised. If it's compromised, your whole domain is compromised, all trusts are compromised, it's just a nightmare. So make sure it's got plenty of firewalls and filters, and don't worry about the external ip. 

For my money, I'd do two three-disk arrays, with one shared hot spare. If you don't have an need for a single block of space larger than a 3 disk array, then there is no reason to cram all 6 disks into 1 raid. You're not going to gain anything in performance or over-all space, and, given a two disk failure, you're probably going to be in a better place. @Dayton Brown: The total space will be the same...3 1TB drives in a RAID5 is what, 1.8TB? 6 will be 3.6 by the same measure, so in that sense you'll have more space in that particular RAID volume, even though the total space will remain the same. The difference is, RAID5 only allows for 1 parity drive, whether there are 3 drives in the RAID or 300, so splitting the drives into manageable groups adds protection against multiple failures. Even if you lost 3 disks, for example, you'd only lose half of your data. If you moved to RAID6, a six disk array would make more sense, because you could lose two and be okay. But most people jump straight to RAID10, and skip 6. 

There probably isn't as much latency as you think. Remember it's the military. It's probably one hop to the satellite, and one hop back down to the drone, and military satellites tend to be in reasonably low orbits, so you wouldn't even have the light-speed latency of a trip to the Lagrange points (Or HEO) and back. Edit: How do you figure 26,000 miles? You're assuming the satellites are in MEO, which isn't a valid assumption. Also light travels 26,000 miles in like .13 seconds, so even if it was traveling that far .5 second latency is about twice what you'd be seeing. You're looking at the whole thing backwards. We know they're doing it. The question is how? My thought is that it's perfectly possible that they're bouncing off a series of LEO satellites, or that they're taking a fibre hop to Europe and bouncing a satellite from there. Or alternately, they're flying with much greater latency than you'd think. It's not like they're dog fighting at high speeds or anything. Edit2: Lot of people are talking about switching latency, and that's really only a factor in two cases: 

Check your (could be tomcat(versionnumber).conf and make sure you're not over-riding the system value there. 

I think the INFORMATION_SCHEMA.TABLE_PRIVILEGES is only a list of what is allowed to be granted, and by whom. There is a table, but it's wildly ugly to look at, and I'm not wholly sure what it's storing. Edit: It's none of the above. Instead it's the aptly named table. Now I feel stupid. 

There is no magical number of servers that encompasses "high availablity." If your needs are modest, you can do it with a single box, though (obviously) as soon as you have to reboot it, there go your 4 9's. As long as your site isn't being heavily utilized, I'd say that two (decent) machines with a mirrored database set up (which is supported by MSSQL web, as you know), and NLB should be fine. Again though, this is a very general question. I run about 50 HUGE sites off of two "servers" but the servers are big honking Sun blade servers. My worry wouldn't be about the machines (I'm assuming you'd be running both IIS and MSSQL on each machine, because there is no other way you could go about attempting high availablity otherwise) but about what you have between them and the internet. Having IIS exposed to the internet isn't the problem it once was, but IIS and MSSQL? I wouldn't do it if I had to be PCI or HIPPA compliant. @Tom: This is your standard "defense in depth" build. Almost everyone will suggest separation of web and database servers, and on top of that, you should put heavy restrictions between them as well...Ideally there should be nothing on your webserver that you can't just restore from backup...Your code is all mirrored on your development machine, so just dump it back up to the website and you're fine. But database servers change all the time, and your setup needs to reflect that. If you're restricted to two machines, you're going to have to install web and database on both, and do your best to harden them. I'd completely lock the databases down so that they can only be accessed by each other and the local webserver. NLB takes the place of a hardware load balancer, and should automatically failover. Obviously YMMV: the hardware solutions have a lot to offer, but the price point is much higher. If you're not expecting a huge amount of traffic, NLB should be okay, but the hardware load balancers tend to have superiour protection against things like DDoS attacks. Again, it's all about what you're going to need. @Tom: 1 IP is fine. NLB uses a "virtual" ip address that resolves to all the local machines. When you turn on NLB, go to the connection properties, and you'll see a "Network Load Balancing" tab. That tab needs the address that applies to the whole cluster (your static ip). Then you go in to the regular TCP/IP tab, and set up the local ip, which is the address that is specific to the individual machine.