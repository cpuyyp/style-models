Here's a proof that it's impossible. Suppose you could build such a data structure. Build it. Then choose $n/\log n$ items at random from the list, add $\epsilon$ to each of them, where $\epsilon$ is smaller than the difference between any two items on the list, and perform the queries to check whether any of the resulting items is in the list. You've performed $O(n)$ queries so far. I would like to claim that the comparisons you have done are sufficient to tell whether an item $a$ on the original list is smaller than or larger than any new item $b$. Suppose you couldn't tell. Then, because this is a comparison-based model, you wouldn't know whether $a$ was equal to $b$ or not, a contradiction of the assumption that your data structure works. Now, since the $n/\log n$ items you chose were random, your comparisons have with high probability given enough information to divide the original list into $n/\log n$ lists each of size $O(\log n)$. By sorting each of these lists, you get a randomized $O(n \log \log n)$-time sorting algorithm based solely on comparisons, a contradiction. 

You appear to be positing a universe where (a) the fine-structure constant has an exact value and (b) we can measure as many digits of it as we want. Thus, if a Turing machine cannot compute the exact value of the fine-structure constant, it cannot predict the outcome of an arbitrary experiment. I don't believe (b) is the case. Generally, the way that constants are measured is by measuring the ratios of some physical quantity that we can measure directly, like time, and using this ratio to compute the constant. The smallest unit of time that exists is believed to be on the order of the Planck time, which is $5.39 \times 10^{ −44}$ s. The age of the universe is $4.32 \times 10^{17}$ s. Thus, if we constrain our experiment to take no more time than the age of the universe, we can measure time to at most 61 digits of accuracy, so this is the limit of accuracy for the fine structure constant. This constant can easily be hard-coded in a Turing machine. Is (a) the case? That may be a matter for philosophers. If changing the 100th digit of a physical constant has absolutely no impact on the dynamics of the universe, does that constant really have an infinite nuber of decimal places? UPDATE: The best you could hope for (and I don't believe physicists currently know how to do this) is an experiment that measured $k$ digits of the fine structure constant in time $2^k$. In this case, the strongest version of the physical Church-Turing thesis would imply that the fine structure constant is computable. But you would be able to simulate an experiment that took $2^k$ years to carry out by using an oracle that gave you $O(k)$ bits. 

What do you mean by von Neumann architecture? I'll use the Wikipedia definition, and say that it's a computer which keeps its program and its data in the same random-access memory. This works well as a theoretical model of a quantum computer, but there are severe drawbacks to implementing such a device. In particular, for a quantum computer, the data has to be quantum. If the data and program are stored in the same memory device, this would imply that the program has to be quantum, as well. So the computer would then have to be able to implement a superposition of instructions in its instruction set. This is orders of magnitude more difficult experimentally than just having the memory in superposition. In fact, even indirect memory retrieval, including RAM, pointers, and so forth, appears to be quite difficult to implement experimentally. This has resulted in the default model of a quantum computer being the uniform quantum circuit model, which is both easier to imagine implementing experimentally and tractable to deal with theoretically from a TCS perspective. The circuit model is able to simulate the von Neumann architecture with only a polynomial slowdown, so quantum complexity classes are not changed by this change of model. 

Several algorithms for simple stochastic games work well in practice, even though they have exponential worst-case running times. Of course, this problem is in some sense related to linear programming, although it is not known to be in polynomial time. 

You seem to have the idea that a quantum gate is a physical thing rather than just a conceptual thing. It doesn't necessarily work that way. While CMOS gates are usually actual physical devices, quantum gates may be just conceptual. Consider an ion trap. The ions represent qubits by using one electronic state as a $|0\rangle$ and another as a $|1 \rangle$. A quantum gate is performed by applying a $4 \times 4$ unitary matrix to two of these ions. This is done by shining a sequence of laser pulses on the ions. It's not a physical device into which two ions are input, in which they interact, and out of which the ions come with their states changed. Quantum gates are performed by shining laser pulses on the ions, but there's not necessarily one laser per quantum gate—the same lasers are used for many different quantum gates. Thus, you can't point to any specific piece of hardware and say "this is the quantum gate". And if you use flux qubits in superconductors for your quantum computer, the quantum gates don't use lasers at all, and they look completely different (and slightly more like CMOS devices). 

In his answer, domotorp suggests analyzing a special case of the game. This special case arises when the permutation is a series of increasing sequences, each of which is larger than the following one, such as (8,9,5,6,7,4,1,2,3). In this game, you start with a collection of heaps of stones, and players alternately remove one stones from a heap. The player who leaves a single heap wins. We will say the $i$th heap has $h_i$ stones in it, and assume that the $h_i$ are given in decreasing order. For example, for the permutation above, the $h_i$ are 3,3,2,1. I tried giving the analysis of this game in the comments to domotorp's answer, but (a) I got it wrong and (b) there isn't enough room in comments to give a real proof. To analyze this game, we need to compare two quantities: $s$, the number of heaps containing single stones and $t=\sum_{i\geq 2, h_i>2\,} h_i -2$; note that we ignore the largest heap in the sum. This is the number of stones you would have to remove to ensure that all heaps but one contain no more than two stones. We claim that the losing positions are as follows: 

Can't this problem be used to compute the permanent of a 0-1 matrix? Since permanent is a #P-complete problem (harder than NP), there is very unlikely to a polynomial-time algorithm. Suppose you have an $n \times n$ matrix $M$ that you want to compute the permanent of. Construct a graph on $2n+1$ vertices, where there is a special vertex $v$ which has one copy, and $n$ of the vertices $x_1$ $\ldots$, $x_n$ have only one copy, while the remaining $n$ vertices $y_1$, $\ldots$, $y_n$, have $n$ copies each. On the $i$'th copy of vertex $y_j$, connect it to $x_i$. Connect the $i$'th copy of $y_j$ to $v$ if and only if the entry $M(i,j) = 1$. Now, for $x_i$ to be in the spanning tree, you must have chosen the $i$'th copy of $y_j$ for some $j$. So we must have a permutation $\pi$, where we have chosen the $\pi(j)$ copy of vertex $y_j$. For this permutation to be a spanning tree we must have $M(\pi(j), j) = 1$ for each vertex $j$. This shows that the permutation $\pi$ yields a spanning tree if and only if it contributes $1$ to the permanent of $M$. Thus, the number of spanning trees is the permanent of $M$. 

Consider the plane through the points $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$, $(1,1,1,1)$. If you add the origin to this set of points, the hyperplane is a facet of the convex hull. The equation is $x_1 + x_2 + x_3 - 2x_4 = 1$. The coefficient on $x_4$ is not $0$ or $\pm 1$. 

You're confused because the example didn't cover all possible cases, and you're extrapolating improperly. Negation is the operation 

These bit strings give the same sum in binary, but not in vector addition. Now, we have carries in the 1, 2, 4 places, so we need to add three sets of three vectors to the equation so as to perform these carries. 

If you take the standard Ising model: min $E= -\sum_{i \neq j}J_{ij}S_iS_j$, where $S_k =\pm 1$, and replace $J_{ij}$ with a non-symmetric matrix, it remains the standard Ising model. This is because you can rewrite the terms $- J_{ij}S_i S_j -J_{ji}S_j S_i = - \frac{J_{ij} + J_{ji}}{2} (S_iS_j + S_j S_i)$. A more interesting asymmetric generalization of the planar Ising model is planar MAX 2-SAT (the problem planar MIN 2-SAT is equivalent). This is the problem MAX 2-SAT where the clauses are on the edges of a planar graph. Planar MAX 2-SAT is NP-hard: look at this paper by Guibas, Hershberger, Mitchell and Snoeyink. Thus, it appears that there may be no interesting polynomial-time generalization of the planar Ising model algorithm. I should note that for planar MAX 2-SAT to be a generalization of the planar Ising model, you need to restrict the Ising model to polynomial-size integers $J_{ij}$. The obvious reduction also requires a planar multigraph, but there's an easy proof that planar MAX 2-SAT on a graph and on a multigraph are equivalent problems. 

You could; the algorithm works fairly fast if you do. If you want to reduce the expected number of quantum steps, you could also do some other tests; for example you should check whether $r$ is a small multiple of one of the convergents. But if you don't find $r$ after these extended tests, you need to start again. 

I can think of two cases where this (more or less) happened. Valiant published his first paper on PAC learning in Communications of the ACM 27 (1984). Charlie Bennett got Stephen Wiesner's paper on quantum cryptography published in SIGACT News 15 (1983). I believe that at the time of publication, the editors had more latitude in accepting articles without extensive peer review than in the standard model of refereed journals. In both cases, the idea turned out to be enormously valuable, and I am sure the prestige of Valiant and of Bennett helped the work get published. In both cases, the research was quite a bit better worked out than "barely a sketch", but it was still very sketchy, and not completely convincing to contemporary researchers. If you are an unknown researcher, in order to get anybody to pay attention, you probably need to work out your ideas at least as well as they were worked out in these two cases, and even then I am not sure whether you could get sketchy ideas published without having the backing of a respected researcher (as Wiesner did).v Ralph Merkle did not succeed in getting his sketchy ideas on what was to become public-key cryptography published until after Diffie, Hellman, Rivest, Shamir and Adelman had worked out the sketchy ideas in enough detail to convince everybody they were worthwhile. Fortunately, somebody put him in touch with Diffie and Hellman, who were clever enough to see the value of his ideas, talented enough to work them out in some detail, and honest enough to give him his fair share of the credit. See this article. 

The main thrust of Cao and Luo's argument is that in the variant of the algorithm that was implemented, the first register—that eventually contains the output—contains only 1 bit. And if you only get 1 bit of output from the algorithm, that's insufficient for factorization. (For one thing, although this isn't their argument, 1 bit clearly does not contain enough information to determine the factors.) What Cao and Luo seem not to realize is that for the variation of the Fourier transform with only one bit in the first register, the same value of $c$ is output as in the standard factoring algorithm; it's just output one bit at a time. This change doesn't affect the $O(\log^3 N)$ running time. To try to be fair to Cao and Luo, they say that they don't think this algorithm works, and if it does work, then it isn't Shor's algorithm because it doesn't exactly match the algorithm described in the original factoring paper. A quote from their paper: 

You could look at this paper, which gives a proposal for how to solve this problem for a practical application in radio astronomy. One of the authors is quite knowledgeable about computer science, so they're not rediscovering the wheel here. 

This is actually problem 5.12 in Cover and Thomas's information theory textbook; show that the probability distribution ${1/12,1/4,1/3,1/3}$ gives a counterexample. And if you want a really nice counterexample, consider the many non-isomorphic Huffman trees you can make when you have probabilities proportional to $$1,1,1,2,3,5,8,13,21,34$$ (the Fibonacci series with an extra 1). Figure out what the maximum and minimum depths are for a Huffman tree with this probability distribution. This calculation will lead to a counterexample for your follow-up question. 

A quantum Fourier transform is a unitary operation, so the number of basis states of the input and output must be the same. The number of basis states before the Fourier transform is 120, the number of group elements. The number of basis states after the Fourier transform is 120, in this case broken up according to the identity $$ 120=1^2+1^2+4^2+4^2+5^2+5^2+6^2. $$ This is because there are seven irreducible representations of $S_5$, which have dimensions $1$, $1$, $4$, $4$, $5$, $5$, and $6$. The Fourier transform of $S_5$ can be represented as the direct sum of $7$ matrices, where the matrix corresponding to an irrep of dimension $d$ has size $d \times d$. You can take the basis states of the output of the Fourier transform to be all the elements of these 7 matrices. In fact, if the dimensions of the irreps of a finite group are $d_1$, $d_2$, $\ldots$, $d_k$, the identity $$ |G| = \sum_{j=1}^k d_j^2 $$ holds in general, and the basis states of the output of the Fourier transform can be taken to be matrix elements corresponding to an irrep. 

The problem with this question is that it is way too vague. Suppose you have a casual conversation, and somebody says "I've been looking at Trudeau gadgets, and I think I can show that they all have a canonical commuting umbrella." If you then publish a paper showing that all Trudeau gadgets have a canonical commuting parasol, there is no question that this is unethical, even if you didn't hear all the details of his proof. (Things like this have happened occasionally, and sometimes the unethical author gets away with it, but I'm not sure doing this is good for your career ... people will probably hear about it). However, suppose you are having a casual conversation about your research, and you explain to somebody what a Trudeau gadget is, and she says "That reminds me of Smith's umbrella theorem." You then look up Smith's umbrella theorem, and it is exactly what you need to prove your result. Here, it would be nice to thank her in the acknowledgments of your paper, but you certainly don't need to make her a coauthor. So there is really no way to answer this question without knowing more details. 

How about Kelner and Spielman's randomized polynomial-time simplex algorithm? It finds the optimal vertex of a linear program. No deterministic simplex algorithm is known which is proven to run in polynomial time, and for many of them, pathological instances can be constructed that make the algorithm take exponential time. Of course, there are polynomial-time interior-point algorithms, so it's not exactly what you're looking for. 

Let $$P(0) = |a|^2 + |c|^2 + |e|^3 + |g|^2.$$ This is the probability of observing $0$. Then \begin{eqnarray*} w &=& a \big/ \sqrt{P(0)}, \\ x &=& c \big/ \sqrt{P(0)}, \\ y &=& e \big/ \sqrt{P(0)}, \\ z &=& g \big/ \sqrt{P(0)}. \end{eqnarray*} 

I don't have time to go into detail now, but here's a quick sketch of how to solve it. Let's assume that all of your polynomials look like $x^k-1$. What you have to do is use the identity $x^k-1 = (x-1)(1+x+x^2+\ldots + x^{k-1})$. You are starting with the function $\frac{\Pi_{i=1}^n (x^{k_i}-1)}{\Pi_{i=1}^m (x^{\ell_i}-1)}$. Now, factor out all the $(x-1)$ terms and take the limit. If $n >m$, you get $0$. If $n \lt m$, you get $\infty$. Otherwise, the $x-1$ terms cancel out, and what you're left with is $\frac{\Pi_{i=1}^nk_i}{\Pi_{i=1}^n\ell_i}$. 

You can't do it. Suppose you start with an array where everything is sorted to distance $k$. Then, you can use a modified QuickSort to sort this array in time $n \log k$. 

Factoring can be achieved by using a polylog $n$ depth quantum circuit, and ZPP pre- and post-processing; see this paper. If it were P-hard, any algorithm in P could be done with polylog $n$ depth quantum circuit and the same pre- and post-processing steps. I believe these steps are modular exponentiation and continued fractions, which to me seem unlikely to be powerful enough to solve P-complete problems, even with the addition of a polylog $n$ depth quantum circuit. 

is the only possible relation among these six vectors because the matrix formed by these six rows has rank 5. 

Suppose you have $n$ balls. Take a random ball from each non-empty bin. Your expected weight is approximately $(1-1/e)T$. This holds because the probability that a bin is empty is $(1-1/n)^n \approx 1/e$. Thus, the expected number of balls you get is $(1-1/e)n$, and these are a random sample of all the balls. Now, suppose you have balls whose weights are not all equal. Then the expected weight is at least $(1-1/e)T$, because you always take the largest ball from each bin. However, you never get a total weight larger than $T$. This means that at least half the time, you must get a weight of at least $(1-2/e)T$. You're never going to get $c > 1-1/e$. That's probably the right answer for $c$, but to get it you probably need to use probability theory that's at least a little more sophisticated than I did.