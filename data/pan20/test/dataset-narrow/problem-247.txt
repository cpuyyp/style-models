The incremental backups them selves seem to be being created fine and in the correct place, it's just the recovery that is failing. Is there any way to see what/where RMAN is looking for the datafiles during the recover command? Or is there a way to force it to look in the new location? If needed, my RMAN config below: 

Is it actually even possible to connect to the reports server from external clients? and if so; Where abouts do I need to configure the reports server so it listens for external connections? 

Then from within the actual exports, for Database B, the times reported by the LOGTIME parameter all give a time of a few seconds per table, but the datapump reported times are all 0 seconds: 

I'm having issues with Oracle 12.1 rman - specifically the MAXPIECESIZE parameter not being honored by my Level 0 backups. This value is set in rman as 

Due to diskspace concerns, a second disk was added and I tried to move rman to use this disk by changing the channel device eg 

but I can't see any options in there that would override the MAXPEICESIZE To be clear - I'm NOT concerned about the location just the file sizes. Is there another setting somewhere that I am missing? Do I need to remove the db_recovery_file_dest parameter maybe? Thanks Backup summary of the latest backup: 

After the same number of level 1 backups, the backup piece usage is sitting at a few meg, instead of almost 100G as is the misbehaving database. Why are the backup pieces on this database sitting in the recovery area eventually filling it up completely? Is this normal behaviour with the other database being the abnormal one? This is not a high traffic database - should I need to have the recovery area set to more than 3 times the size of the database? Thanks 

If I run a crosscheck archivelog all, this picks up only the recent archivelog files that I expect to be there: 

As mentioned, this is the same oracle home, same server, same disks etc - the two databases should be performing at least roughly the same I think. Any know what could be causing this? Or where I should be looking to see what could be causing the slow down? Thanks 

Where are these files likely to have have come/why weren't they eventually expired and deleted like the rest of the archivelogs? Why does crosscheck archivelog all not detect them and add them back? is it because there is a 4 month gap between them and the next most recent files? Is it safe to just delete these files off disk? 

On the other database on this server (that is behaving as I think it should), using identical config/scripts the recovery area usage looks like 

(in hindsight, symlinks may have been a better option but at the time I preferred to make the location of the backups on a seperate drive explicit/obvious). This channel change didn't seem to have any effect. Initially I suspected because I have a 7 day retention period set and the old image were still valid, however after 7 days datafile images were still being created in the old location. As diskspace was becoming increasingly tight, I changed the backup script to use a different TAG to try and force a new set of backup images to be create on the new disk. This seemed to work - the new set of backup images were created okay on the new disk that night, but subsequent recovery operations seem to be failing. 

It sounds somewhat like you are hitting a bit of memory pressure. Dirty Buffers are pages in memory that have changes in them that have not been written to disk yet. Normally they would stay in memory until one of the various processes clean them up and/or write them to disk. There are some things you will want to look into but with only the information you have given here I have to ask, "How much ram have you reserved for the OS?" You are running a 64 bit version of Windows. Now, the 32 bit kernel will use 4 GB of RAM. However, we have found (the hard way) that 64 bit versions of the OS will use 8 - 12 GB of RAM dependent on how busy the Sql server is. By that I mean are you doing a lot of reading and writing to disk. If the OS can't allocate the memory it needs for this (because the Sql server has sucked it all up) you will see the CPU being consumed for the additional context switching it needs to do to write those buffers out to disk. When you added the additional RAM to the server did you reset the Sql Server Max Memory setting? When you only had 16 GB of RAM your Sql server was constrained as much as the windows kernel was and so you were likely using virtual disk (hard drive space) quite a bit. When you added more RAM you opened things up a bit and now the Sql server is attempting to keep more of the database(s) in memory. 

What you're describing sounds like Microsoft Sql Server HADR commonly called AlwaysOn Availability Groups. It combines traditional Windows clustering with Database Mirroring to allow you a primary replica and up to 4 (Sql Server 2012) secondary replicas. Once set up correctly it will provide you with a single IP address and port # to use as a connection target for your application. Setting up the mirroring portion is a bit more than just deploying your database to the connection though there are ways to automate that also. The replicas may be geographically separated as well as residing on different sub-nets and domains. 

Both multiple files in a filegroup and multiple filegroups are tools for managing your database. The first lets you manage your IO and both will let you manage your backups. It is possible to backup a single file of a database as well as a single filegroup. Be sure to backup the tail of the transaction log when you do if you are planning on restoring it somewhere. Database files allow your multi-core CPUs to have multiple read/write streams to the database without hitting higher disk queuing values. It may help to think of the filegroup as a logical division and the file as a physical division. If you have multiple filegroups you will automatically have multiple files as a file can only belong to one filegroup. It can also help (if you have enough cores on your server) to have multiple files in your filegroups. That can give you the best of both worlds. You assign database objects to a filegroup not a file. You can put the files on different physical disk arrays. When I first started doing database work it was common knowledge that you put your data and log on separate disks. If you had the budget you would put your non-clustered indexes on another disk. It's tough to get that these days with SAN technology everywhere. However, SAN is a management tool not a performance tool. As you pointed out having different filegroups will allow you to isolate high traffic tables from each other and from lower traffic tables. It will also allow you a limited additional protection from a corrupted database potentially limiting data corruption to a smaller part of the data.