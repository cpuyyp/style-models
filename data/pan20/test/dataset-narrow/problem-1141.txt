(this is not specific to TCS conferences, but would work for better conferences in general) A nice idea I saw in a mathematical conference for young researchers is asking every participant to write a short "research statement" - in case of junior participants who don't yet have much results, a description of interests would be OK. Then, some time before the conference, the statements are published on the webpage. A working example of this: $URL$ (conference in geometric group theory) I think this would be especially helpful for junior participants suffering from the "I don't know anyone here" problem, but also the other way, since even students aren't anonymous mass anymore. Of course, this is rather feasible for smaller conferences/workshops, not 400-people events, but still I think it's worth implementing. 

Let $\mathcal{B} = \{B_1, \dots, B_k\}$ be a set of Mutually Unbiased Bases (MUB) in $\mathbb{C}^n$, i.e. each $B_i$ is an orthonormal basis and for $v \in B_i, w \in B_j, i \neq j $ we have $|\langle v\vert w\rangle| = \frac{1}{\sqrt{n}}$. We are interested in discriminating between arbitrary vectors from $\mathcal{B}$. Is the optimal (worst case or average with uniform prior) POVM measurement identified explicitly anywhere in the literature (e.g. using Holevo criterion), at least for some specific constructions of MUBs? 

This is equivalent to the biclique partition number of a bipartite graph. You can think of M as representing a bipartite graph $G$ on $[n] \times [m]$ in the natural way: $M_{i,j}$ is 1 if and only if there is an edge $(i,j)$ in G (where $i$ is an element of the left partition, and $j$ an element of the right partition). Then $M$ has binary rank $r$ if and only if the edges of the corresponding bipartite graph $G$ can be partitioned into $r$ complete bipartite subgraphs. To see this, take an optimal factorization $M = UV^\intercal$, denote the columns of $U$ by $u_1, \ldots, u_r$, and the columns of $V$ by $v_1, ..., v_r$. $M=UV^\intercal$ is equivalent to $M = \sum_{i = 1}^r{u_i v_i^\intercal}$, and $u_i v_i^\intercal$ represents a complete bipartite graph on the vertices $S_i \cup T_i$, where $S_i$ is the set of left vertices for which $u_i$ is the indicator vector, and $T_i$ is the set of right vertices for which $v_i$ is the indicator vector. Computing the biclique partition number is NP-hard, and hard to approximate. See these two papers for some results and references: [1], [2]. 

Although, to be honest, representation theory at the level required above can be learnt in two evenings (maybe three, if you want to learn representation theory of the symmetric group $S_n$), so there should be no pressure to learn it in advance. 

Representation theory of finite groups (also over finite fields) can be surprisingly useful for various tasks, including: 

Is there any software package allowing decomposition of unitaries from $U(2^n)$ into quantum circuits over a predefined universal gate set? 

It is well known that some major complexity classes, like P or NP, admit a full logical characterization (e.g NP = existential 2nd order logic by Fagin's theorem). On the other hand, one can also define complexity classes in communication complexity (where P = problems solvable with poly(logN) communication etc. - see Complexity classes in communication complexity theory for more). My question is - are any descriptive complexity characterizations known for communication complexity classes (or do any such results from standard complexity classes transfer to communication setting easily)? 

figure which tableau cell it corresponds to if it corresponds to the first $|x|$ entries of the first row or the first entry of the last row of the tableau, output the clause if it corresponds to any other cell, we're dealing with the same constant-size 3CNF, and we only need to compute the variable labels for it, which can be done based only on the index of the tableau cell 

The Lopsided Lovasz Local Lemma relaxes the mutual independence condition to negative dependence. We assume we have events $A_1, \ldots, A_n$, with a lopsidependency graph $G$ defined on $[n]$ s.t. for every event $A_i$ and every $S \subseteq [n] \setminus \Gamma^+(i)$, $$ \Pr\left(A_i \mid \bigwedge_{j \in S}{\bar{A_j}}\right) \le \Pr(A_i). $$ As you (sort of) suggest, the classical inductive proof of LLL works fine with the lopsidependency graph in place of the depdendency graph. This answers your question because the negative dependence condition is only weaker than the equality you wrote. See these notes by Vondrak for some applications. The tightness of the constant $e$ (in the limit) was shown by Shearer, who characterized the smallest probabilities that guarantee a local lemma-type statement for any given dependency graph. There are fascinating connections with statistical mechanics, covered in a paper of Scott and Sokal. 

Suppose I have an exponentially large graph $G$ ($|G|=2^n$) supplied with an efficient (of size $poly(n)$) randomized circuit $C_G$ implementing the random walk on $G$ - that is, $C_G$ takes a vertex index $i$ and outputs a random neighbor of $i$. Has this type of graph specification been studied and is it more powerful that the standard succinct representation, where $G$ is given as an efficient circuit that given $i,j$ outputs whether $(i,j)$ is an edge in $G$? I could imagine that being able to perform a random walk could help e.g. in detecting triangles in a dense graph (e.g. by choosing a random starting vertex and performing a random walk of length $3$; on the other hand, deciding triangle-freeness in the usual succinct model in NP-hard) 

Given a finite set of quantum gates $\mathcal{G} = \{G_1, \dots, G_n\}$, is it decidable (in computation theoretic sense) whether $\mathcal{G}$ is a universal gate set? On one hand, "almost all" gate sets are universal, on the other, non-universal gate sets are still not well understood (in particular, of course, it is not known whether every non-universal gate set is classically simulatable), so I imagine giving an explicit algorithm for checking universality could be nontrivial. 

Clearly $P$ has an integer point if and only if $\phi$ is satisfiable. Also the number of integer points in $P$ equals the number of solutions of $\phi$. So as long as the ratio between the diameter and the width of $P$ is polynomial, this answers both your questions. Let us then compute its diameter and width. $P \subseteq [0,1]^n$, so the diameter of $P$ is at most $\sqrt{n}$. By width, I am assuming you mean the standard $$ \min_{\theta: \|\theta\|_2 = 1} \max_{x, y \in P} \langle \theta, x-y\rangle, $$ i.e. the smallest distance between two parallel hyperplanes that sandwich $P$. Notice that any $x$ which satisfies $\frac13 \le x_i \le \frac23$ for all $i$ is in $P$. So, for a $\theta$, pick $x_i$ to be $\frac23$ if $\theta_i > 0$ and $\frac13$ otherwise, and pick $y_i$ to be $\frac13$ if $\theta_i > 0$ and $\frac23$ otherwise. Then: $$ \langle \theta, x - y \rangle = \sum_{i = 1}^n \frac{|\theta_i|}{3} = \frac13 \|\theta\|_1 \ge \frac13 \|\theta\|_2. $$ So the width is at least $\frac13$. 

First, prepare a state $\frac{1}{\sqrt{3}}((-1)^{f(0)}|00\rangle + (-1)^{f(1)}|01\rangle + |11\rangle)$ (which can be done easily using single black-box query and unitaries). Notice that two such states correspondng to different $f$'s have always inner product $\frac{1}{3}$. You can easily turn this observation into an algorithm succeeding with one-sided error $\frac{8}{9}$ or better if you allow two-sided error (note that the best classical procedure can achieve probability at most $\frac{2}{3}$). 

(also asked here, no replies) A $(d,\lambda)$-quantum expander is a distribution $\nu$ over the unitary group $\mathcal{U}(d)$ with the property that: a) $|\mathrm{supp} \ \nu| =d$, b) $\Vert \mathbb{E}_{U \sim \nu} U \otimes U^{\dagger} - \mathbb{E}_{U \sim \mu_H} U \otimes U^{\dagger}\Vert_{\infty} \leq \lambda$, where $\mu_H$ is the Haar measure. If instead of distributions over unitaries we consider distributions over permutation matrices, it's not difficult to see that we recover the usual definition of a $d$-regular expander graph. For more background, see e.g.: Efficient Quantum Tensor Product Expanders and k-designs by Harrow and Low. My question is - do quantum expanders admit any kind of geometric interpretation similar to classical expanders (where spectral gap $\sim$ isoperimetry/expansion of the underlying graph)? I don't define "geometric realization" formally, but conceptually, one could hope that purely spectral criterion can be translated to some geometric picture (which, in the classical case, is the source of mathematical richness enjoyed by expanders; mathematical structure of quantum expanders seem to be much more limited). 

Grothendieck's inequality, from his days in functional analysis, was initially proved to relate fundamental norms on tensor product spaces. Grothendieck called the inequality "the fundamental theorem of the metric theory of tensor product spaces", and published it in a now famous paper in 1958, in French, in a limited circulation Brazilian journal. The paper was largely ignored for 15 years, until it was rediscovered by Lindenstrauss and Pelczynski (after Grothendieck had left functional analysis). They gave many reformulations of the paper's main results, related it to research on absolutely summing operators and factorization norms, and observed that Grothendieck had solved "open" problems which had been raised after the paper was published. Pisier gives a very detailed account of the inequality, its variants, and its tremendous influence on functional analysis in his survey. Grothendieck's inequality is very naturally expressed in the language of combinatorial optimization and approximation algorithms. It says that the non-convex, NP-hard optimization problem $$ \max\{x^TAy: x \in \{-1, 1\}^m, y \in \{-1, 1\}^n\} $$ is approximated up to a fixed constant by its semidefinite relaxation $$ \max\{\sum_{i,j}{a_{ij}\langle u_i, v_j\rangle}: u_1, \ldots, u_m, v_1, \ldots, v_n \in \mathbb{S}^{n+m-1}\}, $$ where $\mathbb{S}^{n+m-1}$ is the unit sphere in $\mathbb{R}^{n+m}$. Proofs of the inequality give "rounding algorithms", and in fact the Goemans-Williamson random hyperplane rounding does the job (but gives a suboptimal constant). However, Grothendieck's inequality is interesting because the analysis of the rounding algorithm has to be "global", i.e. look at all terms of the objective function together. Having said this, it should not be surprising that Grothendiecks's inequality has found a second (third? fourth?) life in computer science. Khot and Naor survey its multiple applications and connections to combinatorial optimization. The story does not end there. The inequality is related to Bell inequality violations in quantum mechanics (see Pisier's paper), has been used by Linial and Shraibman in work on communication complexity, and even turned out useful in work on private data analysis (shameless plug). 

We say that a Turing Machine $M$ is mortal if $M$ halts for every starting configuration (in particular, the tape content and initial state can be arbitrary). Is every recursive language recognized by a mortal Turing Machine? (i.e. if there is a TM that accepts $L$, there is also mortal TM that accepts $L$) 

Consider the set $S = \{1, \dots, n\}$ and $n$ subsets $S_i \subseteq S$ of size $d$ each (think of $S_i$ as neighborhoods of vertex $i$ in some $d$-regular graph, although the graph structure is not important here). Each vertex can have label $0$ or $1$. Each set $S_i$ comes with 2 constraints: there can be at most $k_i$ zeroes in $S_i$ and at most $l_i$ ones in $S_i$ (assume that $k_i + l_i \geq d$, otherwise constraints are clearly inconsistent). The problem - is it possible to check in time $\mathrm{poly}(n)$ (with fixed $d$) whether this constraint problem is satisfiable by at least one labelling of vertices? It smells like something NP-hard, but I don't see an obvious reduction e.g. from $d-SAT$ since it's not clear you can implement negation by only this type of constraints. 

Such a collection of subgraphs is called a bramble of order $k$ Notice how "bramble number is at least $k$" is a $\exists\forall$ statement, with both quantifiers over exponentially large sets. So it does not suggest an easy to verify certificate (and if there were one that would be really big news, as I said above). To make things even worse, Grohe and Marx showed that for every $k$ there is a graph of treewidth $k$ such that any bramble of order at least $k^{1/2 + \epsilon}$ must consist of exponentially many subgraphs. They also show that there exist brambles of order $k^{1/2}/O(\log^2 k)$ of polynomial size. 

Let me finish with a sketch of the SDP rounding (which is fairly standard fare). Let $P = \{x: -b \leq Ax \leq b\}$ be a centrally symmetric polytope, where $A$ is $m \times n$. Define the vector program: $\alpha^2 = \max \sum_{i = 1}^n{\|v_i\|_2^2}$ subject to: $\forall 1 \leq i \leq m: \|\sum_{j = 1}^n{A_{ij} v_j}\|_2^2 \leq b_i^2$ Above the $v_i$ range over $n$-dimensional vectors. This can be written as an SDP in the standard way and is a relaxation of the diameter of $P$, i.e $\alpha$ is at least the euclidean diameter of $P$. I now claim that $\alpha \leq O(\sqrt{\log m})\cdot \text{diam}(P)$. To show this, I will give you an algorithm that, given $(v_i)_{i=1}^n$ of value $\alpha$, outputs $x \in P$ of length at least $\frac{\alpha}{O(\sqrt{\log m})}$. The algorithm is just a random projection: pick a random $n$-dimensional vector $g$ where each $g_i$ is a standard gaussian. Set $\tilde{x}_i = g^T v_i$. By standard properties of gaussians: $$ \mathbb{E}\ \|\tilde{x}\|_2^2 = \alpha^2 $$ $$ \forall i \leq m: \mathbb{E}\ |(A\tilde{x})_i|^2 \leq b_i^2 \ \ \Rightarrow \ \ \mathbb{E}\ \max_{i=1}^m{\frac{|(A\tilde{x})_i|}{b_i}} \leq C\sqrt{\log m}. $$ where the last bound holds for large enough $C$ (this is a standard fact about the maximum of $m$ subguassian random variables, and can be proven using the Chernoff bound). The two equations already imply there exists an $x$ such that $x \in P$ and $\|x\|_2^2 \geq \frac{1}{C\sqrt{\log m}}\alpha$. Or, using concentration bounds, you can show that with constant probability $\frac{1}{2C\sqrt{\log m}}\tilde{x} \in P$ and $\|\tilde{x}\|_2\geq \frac{1}{2}\alpha$. 

The syntactic monoid of a language $L$, which is $\Sigma^{\ast}$ quotiented by $\sim_{L}$, is usually bigger than the set of equivalence classes of $\Sigma^{\ast}$ quotiented by $\equiv_{L}$. Informally, the Myhill-Nerode relation $\equiv_L$ only cares about the prefixes of a word $w$ (since it reflects the processing of $w$ by a DFA), while the syntactic monoid has to encode the information about all possible infixes of $w$ (otherwise you won't get the algebraic structure $[x]\cdot[y]=[x \cdot y]$) 

I'm interested in examples of problems where a theorem which seemingly has nothing to do with quantum mechanics/information (e.g. states something about purely classical objects) can nevertheless be proved using quantum tools. A survey Quantum Proofs for Classical Theorems (A. Drucker, R. Wolf) gives a nice list of such problems, but surely there are many more. Particularly interesting would be examples where a quantum proof is not only possible, but also "more illuminating", in analogy with real and complex analysis, where putting a real problem in the complex setting often makes it more natural (e.g. geometry is simpler since $\mathbb{C}$ is algebraically closed etc.); in other words, classical problems for which quantum world is their "natural habitat". (I'm not defining "quantumness" here in any precise sense and one could argue that all such arguments eventually boil down to linear algebra; well, one can also translate any argument using complex numbers to use only pairs of reals - but so what?)