One way I deal with such stuff is I put a member in the base class that describes the type of the derived object. For instance, GameObject can have a member called type which is an enum or w/e. Then, you could do 

I'm implementing a software renderer with this rasterization method, however, I was wondering if there is a possibility to improve it, or if there exists an alternative technique that is much faster. I'm specifically interested in rendering small triangles, like the ones from this 100k poly dragon: 

Also, virtual methods tend to be slow as well. It may not be the most professional way of dealing with this, but I think it's the fastest. 

The third matrix (C) is the one that transforms from world space, into camera space. This matrix is a translation matrix with a translation of (0, 0, 10), because I want the camera to be located behind the object, so the object must be positioned 10 units into the z axis. 

where d is the distance from the eye to the projection plane, so d = 1. I'm multiplying them like this: (((P x C) x W) x I) x V, where V is the vertex' coordinates in column vector form: 

Why does adding these two threads slow down my application? Why doesn't it work when compiling for Release or with optimizations? Can I speed up the application with threads? If so, how? 

As you can see, the method I'm using is not perfect either, as it leaves small gaps from time to time (at least I think that's what's happening). I don't mind using assembly optimizations. Pseudocode or actual code (C/C++ or similar) is appreciated. Thanks in advance. 

I'm applying phong shading onto a single giant triangle, and I'd like the light's coordinates to coincide with the camera's coordinates in 3D space. In order to do this, whenever I update the camera's coordinates, I also update the light's coordinates. However, diffuse and specular lighting don't "focus" exactly at the camera position when I bring the camera close to the triangle, instead they do it a few units too far. The triangle is .5 units below the XZ plane and parallel to it. Here is a picture demonstrating this effect. 

Thanks to everyone for their responses, I have finally figured out the problem. Turns out I had to interpolate the positions, as they are transformed in view space, instead of the light direction. Apparently light direction does not linearly interpolate properly. Here are my modified shaders: 

I'm making a collage of lots 16x16 renders on a 512x512 texture, of the same scene, from various viewing positions and angles, preferably lots of times per second. I've profiled my program (which contained a glDrawElements call per mesh), and the multiple glDrawElements calls seemed to slow it down a lot. In order to optimize, I've resorted to instanced rendering. However, the main problem I'm having is changing the viewport between, say, every 3 instance renderings, or so. I was thinking of adding a fourth matrix, which would scale the perspectively-projected vertices of a would-be 16x16 picture, translate them so that the little images don't overlap, and based on the little picture's position and size (16x16 pixels) on screen, use the `discard' command in the pixel shader. How do I scale the projected vertices from the current large viewport into a 16x16 smaller version of it and translate them inside the former at a certain position? Or, more clearly, how do I change the viewport during a glDrawElementsInstanced call, every N instances? EDIT Here's a visualization of what I'm trying to achieve: Keep in mind, I can't change the viewport as I want to do this during a call to glDrawEleemntsInstanced, every 3 instances, or so. How do I compute a matrix or what do I have to do to get the post-projection vertices scaled and translated so that I'll have the full image scaled in a 16x16 portion of the screen? 

To answer your first question, yes it's quite possible to load an ETC2 texture on any OpenGL ES 3.0 device. In fact, it's required by the standard. In order to do so, you replace the function call with . For more details, read the documentation. Currently there aren't that many texture compression tools that support ETC2. Off the top of my head I can only think of one. That compressor uses the stock ETC2 compressor that Ericsson released a while back. I believe that the tool can output textures in a KTX file that comes with a codec implementation. If you want to do compression online in the app, you may be out of luck. A preliminary search on the internet shows that ports of etcpack exist but I've never tried them. In any case you should be able to expect a decent quality compression of a 1024x1024 sized bitmap within 60 seconds of any codec worth it's salt. 

If you're looking to have not a lot of water move over a terrain, then your best bet will likely be some form of Smoothed Particle Hydrodynamics (or SPH). In this formulation, you simulate a set of particles as they adhere to the laws of hydrodynamics (namely, the Navier-Stokes equations). There is a great series of articles written about this from Intel: $URL$ Rendering your particles as fluid will likely be done using some sort of metaball formulation. You can get a good intuition from this article. The basic technique is described here, and what you can expect it to look like is something like this although it will depend how you write your shaders. For larger bodies of water, I suggest looking at this tutorial which outlines some of the techniques used to generate a large body of water. You can combine this with the previous method to have fluid that flows into large bodies of water too. 

For each agent, assuming that it is stationary, calculate all of the velocities that would cause it to collide at any point in the future with any of the other moving agents. This can be represented in "velocity space" as a set of of intersecting half-planes (also known as a velocity obstacle). Determine the point in this space closest to , this is the new velocity of the unit. 

Now that we have a good understanding of how matrices work, for your problem, you will need to compute a separate Model Space matrix for each block that you want to render. This is usually done "on-the-fly" from the block's position and orientation. Alternatively, you could specify a single Model Space matrix for the entire terrain, but then each block's vertices would have to be specified relative to each other (in "terrain" space). 

First, if you've set freeze position on the rail tracks, why do you have them set to react to gravity? Second, make sure that your train and tracks are not positioned so that they are initially colliding. The way you can do this is to keep moving the train up until it simply falls to the tracks. Third, I'm not sure that metal is the material that you want here. I would suggest making your own physics material and experimenting with the properties. In general, your objects should have a very high coefficient of static friction, and a not so high coefficient of dynamic friction. Play around with the settings to see what feels right. 

I'm trying to get an object from object space, into projected space using these intermediate matrices: The first matrix (I) is the one that transforms from object space into inertial space, but since my object is not rotated or translated in any way inside the object space, this matrix is the 4x4 identity matrix. The second matrix (W) is the one that transforms from inertial space into world space, which is just a scale transform matrix of factor a = 14.1 on all coordinates, since the inertial space origin coincides with the world space origin. 

After I get the result, I divide x and y coordinates by w to get the actual screen coordinates. Apparenly, I'm doing something wrong or missing something completely here, because it's not rendering properly. Here's a picture of what is supposed to be the bottom side of the Stanford Dragon: 

I'm trying to make a real-time GPU (CUDA) ray tracer, and for now I'm tracing single rays, but I've ran into a problem: the BVH. This [PDF]paper has been my inspiration for the theoretical part, and as you can see, the BVH is composed of Axis Aligned Bounding Boxes, however, the stackless rope-based algorithm for the ray-AABB intersection does not take into account overlapping siblings, which occur quite a lot with the AABB creation algorithm I've read about in multiple places on the Internet, which is averaging the centroid of each triangle in the current triangle list and deciding in which child to place each triangle based on the projection of the average on the axis parallel to the longest edge of the parent box. The use of AABBs in the paper indicates that there indeed exists a method to efficiently (in terms of speed) make AABB trees without overlapping siblings. Unfortunately, I can't find such a method. Would someone please describe a fast method for creating an AABB tree without overlapping children? I'd also appreciate it if they'd post pseudocode too. Thank you. 

Texture coordinates are expressed as floating points values between the limits 0 and 1. What you are doing is sending 5 and 32 which get clamped to 1, resulting in the image becoming transparent between 0 and 1, which encompasses the whole thing. What you need to do is divide, either in the shader or outside (preferably outside, on the cpu) by the actual width and height of the texture. For example, say you have a 32x64 texture, and you want it to be transparent in the rect x1=0, y1=0, x2=5, y2=10. You have to divide x1 and x2 by 32 and y1 and y2 by 64, and then do the comparisons. Something like this: 

I'm making a software renderer which does per-polygon rasterization using a floating point digital differential analyzer algorithm. My idea was to create two threads for rasterization and have them work like so: one thread draws each even scanline in a polygon and the other thread draws each odd scanline, and they both start working at the same time, but the main application waits for both of them to finish and then pauses them before continuing with other computations. As this is the first time I'm making a threaded application, I'm not sure if the following method for thread synchronization is correct: First of all, I use two global variables to control the two threads, if a global variable is set to 1, that means the thread can start working, otherwise it must not work. This is checked by the thread running an infinite loop and if it detects that the global variable has changed its value, it does its job and then sets the variable back to 0 again. The main program also uses an empty while to check when both variables become 0 after setting them to 1. Second, each thread is assigned a global structure which contains information about the triangle that is about to be rasterized. The structures are filled in by the main program before setting the global variables to 1. My dilemma is that, while this process works under some conditions, it slows down the program considerably, and also it fails to run properly when compiled for Release in Visual Studio, or when compiled with any sort of -O optimization with gcc (i.e. nothing on screen, even SEGFAULTs). The program isn't much faster by default without threads, which you can see for yourself by commenting out the #define THREADS directive, but if I apply optimizations, it becomes much faster (especially with gcc -Ofast -march=native). N.B. It might not compile with gcc because of fscanf_s calls, but you can replace those with the usual fscanf, if you wish to use gcc. Because there is a lot of code, too much for here or pastebin, I created a git repository where you can view it. My questions are: