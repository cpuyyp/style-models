You may wish to try disabling and uninstalling any antivirus software you have installed and see if it resolves the issue (I of course don't recommend this in the long term). I have seen two different instances of antivirus interrupting SSL conections on Windows 10 recently. Chrome would report a similar error to "bad record MAC" and the issue was generalized to any SSL website or connection and any browser. Unstalling or changing A/V software resolved the issue. 

Yes, it is possible, but it takes a bit of work. I assume that the concern with duplicating the database is 1) time and 2) size/storage, otherwise it seems like it would be trivial to spawn databases for testing against. Based on the fact that you are targeting PL/SQL i'm guessing you are working with Oracle. You will either need: 1) a smaller-scale dataset that can act as a representative sample of you data which is sufficient for testing all of your corner cases or 2) to clone the whole production database. Honestly, I recommend option #2. There are numerous cases where things operate correctly at a smaller scale, but run into issues once scaled up - especially with database data. Ideally, you could store and reply a sample of queries for load testing using a database proxy like ScaleArc (if Oracle won't let you store your queries). Saying you need to clone your whole dataset may make me sound like a kook and like it will take forever, but you may find this answer in which I detail a methodology for cloning multi-terabyte databases in seconds without significantly increasing your storage costs due to duplication and delta-snapshotting useful. Be forewarned, the tech isn't cheap, but if you wanted cheap, you wouldn't be using Oracle. The best way to sell that to your upper management is to prove that the tech is cheaper than the outage/data mangling it would prevent. It's your insurance policy against outages - so how much insurance are you willing to buy? 

It sounds to me like you have some serious architectural issues with the configuration of application. It sounds like 1) XML files or SOAP requests are a terrible way to manage configuration of an application and 2) the application is not very agile and requires a restart every time a change is needed, which is disruptive. Using JSON is a far better way to manage the application configuration - especially if this change will allow the configuration to be dynamically updated without a restart using a REST API. You will find that configuration management tools, whichever you use will require some integration coding - Ruby for Chef and Puppet or Python for SaltStack and Ansible - for the REST API calls. While you could use something like Ansible to manage the XML files, this is not the best way to do this and will leave your application with several pain points. Rearchitecting your application in the manner described is definitely the right call. 

Scalability and High Availability While you asked about continuous deployment, DevOps is conserned with more than just continuous deployment - so I am going to include some bits about redundancy, scalability and high availability. I mentioned, JIT, immediate and eventual consistency. This is where varous RDBMS engines come in. Eventual consistency is relatively easy by simply configuring circular asynchronous replication. This can cause some collisions however *(what if your application layer updates data on one side of the cluster and on the other side of the cluster before replication is completed?) For immediate consistency, look at Galera cluster which will force synchronous replication, but causes scalability issues (how will you replicate to your Disaster Recovery site or load balance geographically without incurring significant latency due to propigation delay at the network layer?) You can also see if you can do synchronous replication within the datacenter and asynchronous replication between sites, but this seems the worst of both worlds. Typically however, most poeple do not need fully synchronous replication - this is usually only needed for very specific (and exotic) high-write environments where multi-master is needed with table sharding. Most apps can deal with Just-In-Time consistency using a database proxy. For example, ScaleArc will monitor replication status and track where writes just went (to send subesquent read requests until replication catches up) to provide Just-In-Time consistency and the appearance of database consistency. ScaleArc is compatable with Postgres, MySQL, MariaDB, Oracle and MSSQL and can use regular expressions to shard/partition your databases for applications that can't use shard keys. It also has a robust REST API for your configuration management software to interact with - and their support team is outstanding Similarly, you might wish to consider a free alternative, MaxScale developed by the MariaDB team for MariaDB. It lacks the GUI and some of the caching features of ScaleArc however. Finally, MySQL fabric (and the in-RAM only MySQL Cluster - if you can afford that much RAM) are other potentials - especially with MySQL's new proxy. This can provide the scalability and redundancy component to your environment. Postgres and Oracle should have the replication and sharding features you need, but ScaleArc will pair well if you need a proxy. Ultimately, all these peices add up to a highly flexible environment suitable for continuous deployment and development if you are unable to simply use a cloud based environment and let your cloud provider deal with the above problems for you. 

in the minion configuration will force the minion to use the prod state, but it will still use the default pillar data. To select a pillar environment you will also need: 

It seems to me that your customers should be dictating the versioning. Both teams seem to be heading towards the same thing but only partially understanding the customer impact. For development, when they significantly re-architect components, there is risk because this could corrupt data or leave core features in a malfunctioning state - this should be reflected by a major version number change. This doesn't mean much to marketing - everything looks the same, so why would the customer notice this change? No need to reflect this as a major change... unless you are a developer, in which case you risk a huge amount support interactions over those bugs. Similarly marketing is looking at the user interface with the assumption that with a significant overhaul to the GUI, customers might get lost or hate the new user interface. This could lead to significant support interactions helping customer navigate the new UX, so should be reflected as a major version number change to the customer. There is a lot of risks there... Unless you are developer. Then this is purely cosmetic. Everything is still functioning the same on the back end. So it seems you need to decide what it is that you feel version numbering is for. 

Some of the above even have sub disciplines, such as Windows Systems Admin vs. Linux/Unix System admin or maybe you use more than one coding language in your. No one person can possibly be an expert in all this which means that if you are adverting for a DevOps engineer, when the weakest area on your DevOps team is Networking, you are not doing a very good job of advertising your need for a networking specialist for your DevOps team. While no individual should be pigeon-holed to a specific role in the DevOps team, you would do your team a disservice by pretending there are no specialists or subject matter experts (SMEs) within the scope of DevOps. Swinging the pendulum from one extreme to the other - from silo-ing to pretending like every role on your DevOps team is the same - can cause just as many problems. While having team members cross-train in more than one discipline - particularly in the overlapping areas is good, expecting them to be able to be proficient in such a wide volume of knowledge simply isn't practice. This means that anyone who tells you they know all aspects of DevOps is probably lying to you. Hire a specialist in the area you are weakest in who has worked on a DevOps team - not a "DevOps Engineer." 

As I see it, the basic problem you have is that you aren't looking at the Total Cost of Ownership (TCO). While you may eventually save on the hardware costs, this is costing you in man-hours - either yours or your employees - a cost which you also pay out of pocket until you begin generating revenue. The reason for this is this: Instead of using a ready to go system, you have to set up, install and configure the following: 

For a laptop, desktop or standalone server, consider trying out Docker containers. These containers are designed to allow you to provide differing libraries, packages and shared objects uniquely to each application. You can have several different Docker containers running different and even conflicting libraries for the same application. You should be able to then use these to package and compile your RPMs. Because these containers run on a single kernel instead of having to power the overhead of multiple kernels, you will be able to run more simultaneously. Furthermore, because Docker uses the underlying file system and creates unions instead of keeping an entire second copy of a whole disk image, managing and cloning containers is faster, lighter weight, will require less storage and since there is no boot sequence to go through, they should start and stop faster. They are also compatible with Jenkins, TeamCity, Vagrant and several other build server software suites, AWS/EC2 and many configuration management systems. 

Less unused time on the wire - we aren't sitting around waiting for an ACK and we have successfully defeated the prop. delay (latency). Less time devoted congestion control = more time for data. Congestion control messages can eat into your transmission time and bandwidth. Even with TCP Window Scaling, Congestive Collapse can still occur in which an entire TCP window may need to be retransmitted and congestion control may be required. 

While the above considerations may not be a concern at smaller scales, at larger scales, these become huge problems. This means that it is extremely important that you define your requirements and forecast the size of your dataset. 

First, confirm that the container is actually listening for connections and has properly bound to the interface. If not, you probably need to fix the and/or directives in your my.cnf. Otherwise, you probably have an issue with your GRANTs. Check that the IP/subnet is correctly authorized in the mysql.user table with the command 

I recently learned about an outstanding little tool, jq that allows you to pipe unformatted JSON output into it and then have that output reformatted and output to the screen in a very nicely formatted color-coded json layout. Eg: 

These four basic resources will power your application or service at every layer of your application. This will be architected into up to 5 layers depending on your environment and you will want to monitor them at every layer. This might look like: 

This makes this question very difficult to answer. This is because you have two basic types of queries: OLTP and OLAP queries 

What DevOps means and entails What teams and individuals are involved in DevOps How DevOps can benefit a companies' workers and business Challenges and weaknesses of the traditional model