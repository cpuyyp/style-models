I am looking for a university that has a good PhD program in theoretical DevOps topics. Especially any program that is trying to advance the beyond Deming's Theory of Profound Knowledge, Goldratt's Theory of Constraints and other related topics. Do you know a professor specifically interested in running a program like that and currently doing advanced research in this field? I've found some companies, which do consulting in this area and try to push it forward, but I am having problems to find some program in academic setting with sufficient credibility. 

You can find many of the answers in the result of DevOps survey which you should ask the product owner to read. This is a document written specifically for business people with little technical knowledge talking in the terms he should understand. At average you will need 1 extra developer for every 4 people to keep same level of feature development (38% vs 49% time spent on new work). Your mean time to recover from failure will drop up to 25 times. Your work will be 20% less enjoyable and you will be 40% as likely to recommend your work to a friend. Just those three facts should be enough. 

Established corporations often aim at (1) and either omit or fail to adopt (2). They will still gain increased effectiveness and improved organizational key performance indicators, but they will fundamentally stay at the orange level of organizational development, failing to undergo the transformation. Such transformation always requires an existential threat to the organization and those are rarely recognized and acted upon until there is too late for the organization to transform. New organizations and organizations with new leadership have an easy way to focus on (2) and so they either are raised up at the new developmental stage or manage to successfully seed business units operating with new culture and values that either subsume the entire organization eventually or the old business units slowly decay and whither while the new ones take over a large bulk of the business. It is almost like a parent passing his family business to a child who has been raised with different values and more progressive ideas. Either way, you can say that DevOps as it is understood today, operates between the orange and green layer and has the potential to enable the transformation from Orange to Green under certain conditions. The core ideas behind DevOps are a different matter. They are in a way a method for an organization to pass from any developmental stage to the next, but as DevOps is being codified through specific cultural recommendations, concrete values, and actual practices and tools, the instantiation of those core ideas in form of DevOps is squarely aimed at the border between Orange and Green. 

TL;DR: It is the wrong thing to measure. By measuring and increasing utilization in employees across the board, you create problems in the system and reduce overall throughput. Throughput Accounting What you actually want to measure is throughput, inventory and operating expense all together and try to reduce inventory and reduce operating expenses while at the same time maximizing throughput. This method is known as throughput accounting. In software development, inventory is the work in progress that is not bringing benefits to the customer yet. Anything that has been done, but not released. Throughput is the amount of work useful to the customer that is being released. Any work done that is not directly useful to the customer is accounted as operating expense. Simple system In a simple system with single human or multiple humans working independently with independent equipment as much as each one does will directly increase the throughput of the whole system. This leads to the common misconception that is basis for this question that increasing human utilization will lead to increased throughput in all systems. But you still measure the throughput of the system, inventory and operating expenses. Complex System In a complex system, even with as little as two dependencies, increased utilization of one part of the system can directly lead to decrease of utilization in the bottleneck, which decreases throughput of the whole system. Any increase in productivity outside of the bottleneck is a mirage. Example: Team of software engineers has all code reviewed by the software architect, who also makes plans for new features. This person is a bottleneck, code not reviewed by the architect will simply increase inventory, if the architect does not have time, no new features will be properly planed. If you start measuring utilization of software engineers, they will each try to make more changes, rather than better changes. The time the architect will need to spend on each change will increase and the total time spent reviewing will increase further by the increased amount of changes to a point where no time would be left to plan new changes. Eventually the whole system grinds to a halt. If on the other hand they decrease utilization, even spend time idling, they spend longer on each change or peer reviews, this could lead to decrease in time required for reviews and eventually increase in throughput. This is just single team with 2 dependencies. Engineers depend on architect for new changes to be planed and for changes to be reviewed. Clearly the benefits are to be gained in properly managing the bottleneck and trying to gain productivity at the bottleneck, where hour gained, is hour of throughput of the entire system. This is the real message of The Phoenix Project and comes directly from the Theory of Constraints by Eliyahu M. Goldratt. You might also read an article on utilization thinking vs throughput thinking. I would also suggest to read more on Critical Chain Process Management. Remember: What you measure is what you get. And you definitely DO NOT WANT to get increased individual utilization. A road to Hell is paved with good intentions. 

Docker is running in a VM on MacOS. One of your options is to put the entire VM on a tmpfs. You could use this shell script to create and mount the ramdisk: $URL$ Here is a guide for resizing the machine. $URL$ The trick is to stop docker first, then you can make changes and move the 

TL;DR: Just use Ansbile, it is both configuration and deployment tool :) There are several types of deployment: 

This question perfectly illustrates the different approaches in adopting DevOps by organizations. While both approaches target the 3rd circle on the culture model - the traditional corporations - there is a difference in aim and results. 

Wikipedia has a very good answer to this question. Artifact, sometimes also called Derived Object, is a product of some process applied to the Code Repository. Originally they were called Build Artifacts, but as more processes were applied other than build to create them, the first word was simply dropped. The major distinction is that artifacts can be recreated from the code repository using the same process, providing you have preserved the environment in which the process was applied. As this process can be time consuming and the environment can be preserved imperfectly to be able to recreate the artifacts in the exact same way, we started to store them in Artifact Repositories. Storing them apart from Code Repository in an Artifact Repository is a design decision a DevOps engineer would make. Some companies, namely Perforce, suggest to use their Code Repository as Artifact Repository as well. There are different requirements in terms of access, auditing, object sizes, object tagging and scalability on each repository and so depending on situation it is often better to use two distinct products. For example Git repositories are copied in their entirety to every development machine and so storing artifacts in the code repository would increase its size beyond all reason, although lately there are ways to mitigate this. Another decision to make is which artifacts to store. Some companies store even intermediate artifacts as individual object files, to speed re-builds, others store simply just the final binaries. Not all artifacts have the same value. Artifacts resulting from a release build could have different requirements than artifacts resulting from a developer build. Most common artifacts are results of the following processes: Configuration, Preprocessing, Compilation, Linking, Automated Testing, Archiving, Packaging, Media files creation and processing, Data File Generation, Documentation Parsing, Code analyzing, QA, etc.