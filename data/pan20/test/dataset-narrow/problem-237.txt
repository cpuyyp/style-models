It depends on the configuration you have in place. Oracle uses log shipping (as default) to feed the transactions into the standby database. In order to make sure that ALL transactions are recovered on the standby database, we should make sure that the primary database is running in FORCE LOGGING mode. Switching FORCE LOGGING may cause a difference in performance on the primary database, where FORCE LOGGING mode would be a little slower than without force logging mode. Just shutting down could - depending on the configuration - bring the primary database to a halt (protected standby database). When the primary and standby databases are on different hardware they should hardly influence each other, other than that the transactions need to be saved and transported to the standby site. If you run in sync mode the primary could be slowed more than when running a-sync which is the default. The configuration that makes sure that the performance impact on the primary is the least, is the max performance mode, in which the transactions are only shipped when the redo logfile is full. It also gives the least data protection. 

This is the default behaviour. As long as you don't have the default service in your tns_aliasses that are in use it should not be a problem. 

Sure you can. Explicitly set the sort_area_size for your session and a lot of PQ. Adding a few tempfiles might be easier. 

This is a bit hard to tell, what is high-end hardware? Using Oracle on EMC storage 60MB/s is certainly possible. Using that same old Oracle on exadata can be slightly faster, 300MB/s and sometimes higher, again, depending on the configuration. A full rack is faster than a half rack ... 

Yes, Oracle RAC is built using multiple servers sharing their memory - Cache Fusion - and they all access the same shared storage where the database resides. In Oracle an Instance is just the collection of processes that acts as an interface between client connections and the actual database. Since all storage is shared, when a server crashes, a surviving Instance can perform the crash recovery on behalf of the crashed Instance[s]. You could call the storage a single point of failure but also one that can be taken away using for example Oracle Active Data Guard, where all transactions are transported to a remote standby database that is open for read access. Doing so allows you to build a very scalable system that has also a very high availability because you can activate your primary database in whatever datacenter that is part of your configuration and every datacenter normally has it's own power and storage. 

No matter what infrastructure we support, we have to support the users of it. A lot of users are developers, so we support the developers to enable them to make the best possible use of that infrastructure. To be able to do this we need to understand each other, with the different ideas and points of views in mind. Having insight to the views from both sides helps to make things better for the business and that is our combined goal. Make IT support the business as effective as we can. In many organizations we see some dba types running in god mode. Most of the times these are not the ones that score very well if competency is measured ..... Often they just hide their - lack of - knowledge behind a wall of words. To my opinion it has nothing to do with being 'programmer friendly' more with being professional. For a dba it means we need to be able to explain why we do the things we do and be prepared to at lease reconsider decision if it helps, without loosing the normal goals like availability, scalability, recoverability and performance. For the programmer it means he has to communicate to the dba, sometimes to teach the dba, sometimes to learn from the dba. My motto on this is: let the first day that I don't learn a thing be the day that the coffin closes above my head. Normal collaboration, having combined teams with developers and dba's certainly help make things easier. 

If indexes are in place: updates in indexes are always logged. An other thing is: nologging is only done for statements that copy data. will always be logged, can be unlogged. 

Use your working connection that connects using the SID to find which services are in the database. If you are connected run: 

to list the databases that are defined on the current host. This comes very close to the Windows services. You could pipe this into the next lines to filter out the database name and request that status of the database, where the instances are listed: 

yes, you can, you can read data from about any database using dg4odbc. Any odbc complient database can be connect to and Oracle database providing you can find a working odbc driver for out platform. .odbc.ini and $ORACLE_HOME/hs/admin are your configuration locations. 

Normally this does not solve a problem. There was a problem that made the instance crash .... check the alertlog, it could have some clues in it. 

No, Oracle won't add an extra datafile. Normally the SYSAUX tablespace is more or less stable so it would be smart to check what is eating the space in there. Connected as a DBA user, run the script to get the current usage of the SYSAUX tablespace and see how it will grow when you change certain parameters for which you are asked to enter values. See OracleÂ® Database Administrator's Guide 11g to get an idea of regular sizing. 

If this succeeds, your parameters are good enough to start an instance. Now see if your control_files parameter is OK: 

at this moment you have control_files that still refer to the original datafiles locations. They need to be redirected. 

When you have naming conflicts, there is a problem, combination not possible. You might want to take some downtime for maintenance/upgrades. If you can not get a downtime from all applications at the same time, you have a problem. Using Resource Manager you can give a certain performance guarantee for the specific services. Services are a smart thing to use, it is the easiest thing to see how resources are used, compared to one and other. Easiest is to run multiple instances on a single server, each serving it's own database. This is the easiest but not the smartest thing to do. Smartest is to have a single instance on a single server. This is because every instance considers itself as the master of the server. You can not very easy isolate their resource usage, as you can do in a single instance. If you want to give some performance guarantee in a multiple instance server, the setup will grow in complexity because in many cases you need to start multiple projects and users to run your databases under. A single database can easily support a few hundred applications, a lot cheaper than using a few hundred databases. This quickly saves BIG money on a yearly basis by making good use of Oracle features. 

Your best chance is to take a look at /etc/oratab or /var/opt/oracle/oratab. If you are in an installation where every PATH and/or ORACLE_HOME is hardcoded in scripts then you are out of luck, as is with any other option you mentioned. Even if you happen to find a listener.ora, it could very well be that is setup for dynamic registration in which case you won't find your ORACLE_HOME's in the listener.ora too. Also, don't forget that the listener.ora does not need to be in ORACLE_HOME/network/admin, it can be in any location pointed to by TNS_ADMIN, /etc/, /var/opt/oracle ... BTW: the oratab works for all Oracle versions, only, with v11 Oracle did include a call to a script (orabase) that only exists in v11 to set ORACLE_BASE. They even did this in such a way that you need OSDBA privileges to be able to run the script. Also see oraenv oddities in 11gR2 

You should use Oracle Resource Manager to throttle the load caused by scheduler jobs in such a way that they do not interfere with the online users. An other way to make such type of a switch is to tie your jobs via a job class to a service and disable that service. See CREATE_JOB_CLASS Procedure This is also very convenient if you want to run import from a database that had enabled jobs and you do not want them to start running at import time ... 

If any error: hire a dba to help you. If step-8 gives problems because of incorrect file locations: you can rename the files one by one, or create a text dump of the controlfile, edit that dump for the correct locations and use it to create a new controlfile. Oracle is very flexible and many situations can be handled. In your case, your database is 9.2.0.6 on Windows. You could try to request a download from oracle support and perform an upgrade. This is the easiest is your dbf locations are the same as on the original system (they are in the controlfile) 

rule #1) use rman for backup, recovery and cloning of databases. since this was a user managed backup, and you need your datafiles on a different location, there are a few things to do. 

This problem occurs when your auxiliary (the clone target) database instance is started using a static init file. Smarter is to use the more dynamic spfile so the control_files parameter can be changed by the cloning procedure to where it is restored on the target. I guess the question was: why this happened? ;-)