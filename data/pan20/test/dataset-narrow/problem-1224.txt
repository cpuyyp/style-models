the Turing Machine was introduced in ~1936, so Lambda Calculus predates the appearance of the TM by several years! 

the classic Post Correspondence Problem is Turing complete and remarkable in its simplicity, its just a string matching question albeit with potentially very long strings. but on the other hand, the Turing Machine itself is a very simple theoretical concept. some have said that Turing was inspired by [electric] typewriters. the 1st commercial electric typewriters appeared in the ~1920s and the TM was invented in 1936. also stock teletype machines, originally invented by Edison, had been in widespread use at the time. some plausible case can be made that all Turing complete systems are exactly as/equally simple. 

it is also natural to draw some at least loose analogy to another massive/complex physics project that so far has not achieved its ultimate goal after decades of attempts and optimistic early predictions, that of energy-generating fusion experiments. 

it has some empirical analysis via benchmarks. it says in general there is "no known measurement of locality" of cache requests and then proposes stack distance as such a measure. it does not relate it to random graph theory although you sketch out such a connection in your comments. (it seems like there stack distance could be related to markov chain mixing?) it appears that you are interested in modelling cache performance or optimization algorithms by considering cache requests as nodes of a graph and the edges as transitions between adjacent requests. have not seen papers that study the structure of this graph. it does appear to provably not to be a purely random graph in real applications due to the success of caches in practice and what is referred to as spatial and temporal locality in the above slides. ie some kind of "clustering" as joe sketches out in his answer. (maybe it has a small world structure?, which is quite ubiquitous in real world data) 

this is a recent breakthrough google research result where a large distributed network running stochastic gradient descent optimization (SGD) and working with unlabeled images (from random youtube videos and frames) was able to develop highly meaningful emergent feature detectors including facial recognition and other object recognition (cats, human bodies, etc). as the researchers note in the article, this was previously considered impossible by conventional wisdom. most prior experiments tend to focus on labelled data samples or supervised algorithms. note however the training phase is extremely CPU intensive. it seems likely the technology will have wideranging applications in the future including eg, most basically, for detecting similarity of images, but with much more advanced possibilities such as for cutting edge problems in AI. based on its operational similarity to an old biological evidence/observations/speculation sometimes referred to as "grandmother neurons" (ie high level "feature detectors") it is seen as not so merely "ad hoc" and may have some real relation to actual "algorithms" used by the human brain for image recognition (or perhaps even deeper or more general cerebral processing). presented at ICML 2012: 29th International Conference on Machine Learning, Edinburgh, Scotland, June, 2012. [1] Building High-level Features Using Large Scale Unsupervised Learning by Le et al [2] How Many Computers to Identify a Cat? 16,000 by John Markoff/NYT 

there is some theoretical/ scientific/ applied research into modelling music with CS formal grammars. see eg 

Dimitris mentions approximating fourier transforms. there is a wide use of this in image compression eg in the JPEG algorithm.[1] although I havent seen a paper that emphasizes this, it seems in some sense a lossy compression[2] (with derivable limits) can also be taken as a P-time approximation algorithm. the approximation aspects are highly developed and finetuned/specialized in the sense they are optimized so that they cannot be perceived by human vision, i.e. the human perception of encoding artifacts (roughly defined as difference between approximation and the lossless compression) is minimized. this is related to theories about how the human eye perceives or itself actually "approximates" color encoding via some algorithmic-like process. in other words the theoretical approximation scheme/algorithm is actually intentionally designed to match the physical/biological approximation scheme/algorithm (encoded by biological info processing ie neurons in the human visual system). so, the compression is tightly coupled with the approximation. in JPEG the fourier transform is approximated by the DCT, discrete cosine transform[3]. similar principles are employed over multiple frames for the MPEG video compression standard.[4] [1] jpeg compression, wikipedia [2] lossy compression, wikipedia [3] DCT, discrete cosine transform, wikipedia [4] MPEG, wikipedia 

The Symmetric Group Defies Strong Fourier Sampling by Moore, Russell, Schulman "we show that the hidden subgroup problem over the symmetric group cannot be efficiently solved by strong Fourier sampling... These results apply to the special case relevant to the Graph Isomorphism problem." with a connection to solving the Graph Isomorphism problem via QM approaches sec 5 Representation theory of the symmetric group 

my understanding is that while there are some candidates from the theory of unbreakability of cryptography and random number generators [eg some cited in Razborov/Rudich, Natural Proofs], most aspects of your question are acknowledged as basically key "still open" questions by experts in the field. from the introduction to the comprehensive survey, Average Case Complexity by Bogdanov and Trevisan (2006) has some related points. Trevisan's youtube lecture on findings and open questions of average case complexity may also be helpful. 

there is not a lot of widespread recognition of this concept so far in the literature, but the clause graph of the SAT problem (the graph with one node per clause, and nodes are connected if clauses share variables), as well as other related graphs of the SAT representation, seems to have many basic clues as to how hard the instance will be on average. the clause graph can be analyzed via all kinds of graph theoretic algorithms, is an apparently natural measure of "structure" and with strong connections to measuring/estimating hardness, and it appears that research into this structure and its implications is still at the very early stages. it is not inconceivable that transition point research, a/the traditional and well-studied way to approach this question, might eventually be bridged into this clause graph structure (to some degree it already has). in other words the transition point in SAT may be seen to exist "because of" the structure of the clause graph. here is one excellent reference along these lines, a Phd thesis by Herwig, there are many others. [1] Decomposing satisfiability problems or Using graphs to get a better insight into satisfiability problems, Herwig 2006 (83pp) 

there are many "spread" models, what you request does seem to have been studied. eg this paper builds a framework to analyze the difference & distinguish between random virus spreading and network- (graph-) based spreading. 

as in the comments this problem is generally closely connected to the same problem in a Hilbert space and algorithms there are nearly applicable. an example of this can be found in this paper by Arya et al [1] p29 where the authors benchmark their Hilbert space nearest neighbor algorithm using the boolean cube and the $L_\infty$ norm. their algorithm works on any $L_m$ Minkowski metric. as you point out (but wikipedia does not seem to nor do a lot of other refs) the Hamming distance metric is equivalent to the $L_1$ Minkowski space metric or "taxicab metric" on binary coordinates. their algorithm takes $ O(dn \log n)$ preprocessing time ($d$ dimensions) and logarithmic "query" time (per point). see also [2] [1] An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions Arya et al, 30pp [2] Effective nearest neighbors searching on the hyper-cube, with applications to molecular clustering Cazals 

there are many such references, they seem to be increasing, as some have noted we seem to be in the midst/living through a Golden Age of algorithms. some newer algorithm-focused refs [hence not so well known] not listed so far that may be interesting, some written by TCS researchers/scientists/experts (Cormen, Valiant, Davis), others by popsci writers: