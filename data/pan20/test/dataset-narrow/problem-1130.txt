Question: The big questions is which two odd vertices to use as the start and end point for the Eulerian Path. So far I am computing all possible Matchings using the Blossom algorithm and taking the one with the minimum weight. Is there a way that we can do this better? Potentially already integrating it into the Blossom algorithm? Are there any assumptions we can make about the vertices that are optimal? I.e. in a lot of cases two odd vertices with largest distance in the original graph are the best choice, however this is not always true (see Example 3). 

Edit (Aug 7 '17) This presentation seems very interesting: $URL$ Unfortunately I wasn't able to find a paper accompanying it. At least counting Eulerian Cycle is #P-Complete. Considering finding a minimum Eulerian Cycle for simplicity and using the idea of the Hierholzer's algorithm, we can find all simple cycles in the graph efficiently using e.g. $URL$ We would then need to find the edge-disjoint edge cycle cover. But this still doesn't guarantee a minimal path or Eulerian Cycle, because we still have to fuse the sub-cycles together. 

Problem: Given a planar (undirected and mostly sparse) graph with an Eulerian Path, we introduce a cost function f: (v, e1, e2) for all two edges e1 and e2 that share a vertex v. The function also fulfills f(v, e1, e2) = f(v, e2, e1) For an Eulerian Path we then define the overall cost as the sum of costs of all path-neighboring edges and the vertex in-between. The goal is to obtain an Eulerian Path that has a minimal total cost. This has to be done somewhat efficiently, so testing all paths is not an option. Ideally answers should outline an algorithm. Background: The goal is to find a path that is "smooth". An example here is the B point in the image below. The flow is a lot smother if we go clockwise through the loop when coming from C. Thoughts: I think it is important to understand how many variations exist for an Eulerian Path and how we would obtain them. 

Here are some interesting anecdotes about TCS giants. Some of them are dark, quite possibly the opposite of 'fun', but you also might get something you're looking for. This also might work best as a community wiki. Kurt Gödel: 

Shannon-Nyquist sampling theorem proposes a sufficient condition for information theoretic bounds on communication. Sampling theory is worked around examples where the incoming signal has a compact/random representation. Recent advances in sampling show that this abstraction does perhaps comes with a price - that the sorts of things we are interested in measuring generally have sparse representations so that these bounds are not tight. Additionally, information can be encoded in a much denser way than originally thought. 

The complexity of modular addition is known: $g + p \mod N$ (for $|p| \approx |g| \approx |N|$) can be computed in $O(n = |N|)$. The complexity of modular multiplication is open though some results are known: by Toom-Cook $g * p \mod N$ can be performed in $O(n^{1.465})$ and by Schönhage–Strassen in $O(n \log n \log \log n)$. Modular exponentiation can be performed in $O(M(n) k)$ where $M(n)$ is the complexity of multiplication and $k$ is the length in bits of the exponent by the square-and-multiply algorithm. I am looking for results about the modular arithmetic in general. Are there better complexity results for performing modular exponentiation (over the 'naive' square-and-multiply)? What results are known for modular exponentiation of power towers of height h? (For $h = 3$: $g^{g^{g}} \mod N$ and for general $h$: $g^{g^{g^{g^{...^g}}}} \mod N$) 

The Closest Vector Problem (and related problems) is random self-reducible and in general is NP-Hard, making it a useful tool in cryptography research and post-quantum public key crypto. For a variety of reasons (e.g. key size, protocol speed) it might be interesting to consider CVP instances where the vectors are sparse. In fact, in solving an algorithmic problem that has come up separately from cryptographic research, I have encountered what I am currently modeling as a sparse CVP problem. Suppose we have a high dimensional (N >> 0) closest vector problem where each of the basis vectors and the target vector all have low (log N) support. What is known in the literature in this case? Does the problem become harder, by the compactness of the input? Does the sparsity of the vectors enable solutions to be found or approximated more quickly in the limit? A rudimentary literature search was not able to find relevant papers. So, really two questions: 

It becomes more difficult when the cycles from the vertex overlap, because then we can swap between them as well. Or do we not need to consider this at all? It's getting complicated and I'm no longer sure about correctness. So I’m kind of stuck. 

I'm thinking maybe we can break the Eulerian Cycle into several "locally optimal" walks and then fuse them together again (efficiently we could find those walks using the Hierholzer's algorithm and following the lowest cost edge). Not guaranteed to be optimal though I think (or is it if we fuse them optimally?). What's next? I'll try to save up for a bounty to give this question some more exposure. 

Example 3 The maximal distance between two odd Vertices (the one at the top and the one at the bottom) is 50. However for the optimal path the distance is only 11. 

Given a connected road network on an Island without one-way streets, where should I para-shoot in and what route should I take to deliver mail to all houses on the island (being picked up again by helicopter)? Or in terms of Graphs: Given an un-directed, connected, weighted Graph, what is the optimal start vertex and shortest path that visits every edge at least once (return to the start vertex not required)? This question is an altered version of the Route Inspection Problem or Chinese postman problem. The solution I have working is and I'm trying to improve this. A Slow-ish Solution 

Edit (Jan 7 '18) I've started looking at solving this for an Eulerian Cycle since we have to consider fewer edge cases and the cycle problem is easily converted into the path problem (see here). There have been suggestions to solve this by optimizing for each node. However, as the next example shows (colors indicate Eulerian Cycle), the local optima "battle" for each other. The top node uses the local optima, however if we were to use the optimum for both nodes we would obtain two circles and no longer have an Eulerian path. 

Given a $k$-regular graph $G$, the number of acyclic orientations $Acy(G)$ is $\chi(-1)$ where $\chi$ is the chromatic polynomial of $G$. How many bipolar orientations does $G$ have? Is there an upper bound for it? I assume it should be exponentially lower than $Acy(G)$ but didn't succeed in finding a known result connecting these two numbers. 

My issue is what's the shape of $f(p)$. What I understand from the definition, is that first I need to see the class as set of boolean functions: 

I am self-studying in the area of query learning and having a difficulty in understanding the definition of closed under projection for concept classes discussed in several papers (for example, here (page 2), here page 18 and here page 8) with regard to learning from membership queries in Query learning. I have a set of elements X. Each concept is a subset of X and the concept class C is a set of concepts. For example the following is a concept class over $X=\{x_1,x_2,x_3,x_4,x_5\}$ with three concepts $f_1$,$f_2$ and $f_3$ represent respectively the sets $\{x_2,x_3,x_4\}$ $\{x_1,x_5\}$ and $\{x_1,x_2,x_4\}$. 

Given an undirected graph $G(V,E)$ and a bipolar orientation $s$ over $G$, consider the problem of identifying $s$ by finding the minimum number of edges such that when orienting them in a particular way they give arise only to $s$ as bipolar orientation. An orientation $s$ for a graph $G$ is bipolar if $s$ is acyclic and has a single source and single sink. That is, find the minimum subset of edges $\hat{E}\subseteq E$ such that if we orient them $\vec{E}$, the only bipolar orientation for the mixed graph $(V,E-\hat{E},\vec{E})$ is $s$. Is this problem known? has been tackled before? Any pointer is appreciated. 

Binary obfuscation is widely used today in a variety of applications (and is in fact even used in the MathJax.js file used on this website to render CSTheory's TeX typesetting system). The goal of binary obfuscation is to preserve semantics and disfigure syntax - i.e. to take an input Turing Machine $M$ and create an equivalent Turing Machine $M'$ such that $M'$ rejects a string $w \iff M$ rejects $w$, such that $M'$ accepts a string $w \iff M$ accepts $w$ and such that the run time complexity of $M'$ is the same as $M$ on all inputs. Finally, there is the additional stipulation that $M'$ must be difficult to both understand and to modify. Industry sees such obfuscation as a good trade off involving a sacrifice of slight (constant time) overhead for the benefit of increased difficulty for adversaries interested in understanding, breaking or stealing IP from $M$. There are many commercial companies that offer binary obfuscation solutions, and there are also several open source solutions. Exact methods for obfuscation, of course, are kept secret; the obfuscation paradigm prevalent in industry is heuristic, so knowing the algorithms used to obfuscate a binary in this context will generally guarantee some advantage in deobfuscation. Obfuscation in industry is known as "security through obscurity." There are theoretical approaches to the problem of obfuscation that formalize the desires of industry but rely on a strictly stronger notion of security based on computational intractability (imagine replacing integer and string equivalence tests with one way function equivalence tests). In particular, the study of composable point obfuscation has been advanced to try to solve the obfuscation problem interesting to industry. Unfortunately the most wide spread theoretical model for obfuscation based on a model inspired by tamper-proof hardware was given an impossibility result in 2001 by Barak et al in the paper "On the (im)possibility of obfuscating programs". (Several other models have since also been given impossibility results). Right now the theory of program obfuscation is in a state of flux, requiring a new (likely less restrictive) model. In fact, the main problem with the theory is its lack of an agreed upon model (and therefore formal garuntees). The recent advent of Fully Homomorphic Encryption may provide such a basis (this is purely speculation on the part of this author). To clarify, obfuscation matches your third example: "Examples of something in widespread practical use has little theory behind it." Obfuscation is used widely today by both the industry and by those with more nefarious purposes. Obfuscation in industry is not currently based on any rigorous theory despite attempts. 

The definition is different from restricting boolean functions to a specific range $A\subseteq X$ and can be stated as follows : 

A boolean function $f(x_1,x_2,\dots,x_n)$ is $k$-Junta if it depends on at most $k$ variables. Consider the class $\mathcal{J}_{\leq k}$ of all $k$-Juntas over $n$ variables, what is the VC dimension of this class? Or at least is there any known method to construct the largest shattered set for small values of $k$ say when $k=1$. 

Let $H$ be a hypothesis class with VC dimension $d$. In supervised learning, we need almost $O(\frac{d}{\epsilon})$ random labelled examples to return a hypothesis within $\epsilon$ from the target (separable case). Active learning potential lies in its exponential saving in terms of the number of queries required compared to the supervised learning. This big saving in known for some structures ( for example learning threshold functions requires $O(\frac{1}{\epsilon})$ examples while we can achieve the same goal with $log \frac{1}{\epsilon}$ in active learning ). What characterizes this exponential reduction in general? Are there certain properties such that if $H$ satisfies them then active learning would help? 

Let $C$ be a concept class with VC dimension $d$ exponential to the input size (i.e number of variables represented in each concept $c\in C$). I am looking for papers/resources/suggestions of how computational learning community handle the learnability of $C$. One thing I am thinking about is investigating the learnability of $C$ in active learning. Hoping that I will end up with less number of examples required. However, I prefer to stick with the passive learning for now and wondered what else I can do for this poor $C$? As I am new to the field, I also wish to have some examples of concept classes with exponential VC dimension.