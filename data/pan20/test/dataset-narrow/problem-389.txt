After the same number of level 1 backups, the backup piece usage is sitting at a few meg, instead of almost 100G as is the misbehaving database. Why are the backup pieces on this database sitting in the recovery area eventually filling it up completely? Is this normal behaviour with the other database being the abnormal one? This is not a high traffic database - should I need to have the recovery area set to more than 3 times the size of the database? Thanks 

I have an Oracle Reports server 10g (10.1.0.4.2) that for the last few years has been running okay with the rwclient.sh client application residing on the same host. Due to some performance issues caused by long running/heavy reports we are looking at trying to separate the reports server from the application server. From memory rwserver.sh should be able to accept connections from a client on the same LAN, but not on the rwservers localhost (but I'm going back many years since I saw that sort of setup and I was just a developer, not an admin, so I could be misremembering it). When I try and submit a report across the network I get: 

My understanding from the documentation is that this is normal on the first run (when there is no datafile image copy), and normal on the 2nd run (when there is a datafile copy, but no incrementals), but on the 3rd and subsequent run there should always be a datafile copy and an incremental to apply to it. This has now been failing for the last 6 nights. From $URL$ 

Then from within the actual exports, for Database B, the times reported by the LOGTIME parameter all give a time of a few seconds per table, but the datapump reported times are all 0 seconds: 

(The java processes above are all for ) I assume the 0.0.0.0/127.0.0.0.1 multiple ports things is why the server can see the initial probe, but the client then fails to connect properly. I suspect there's got to be an option somewhere that determines what interface the server tries to bind to but I can't find any option in the report server .conf to enable this. Questions: 

I have always found that trying to shrink a lot of space out of a data file in one go either takes an inexplicably long time (with low I/O on the files while the command is running) or completes after I've lost patience or blown through my timebox limit on when I can do the operation. Instead, first get an idea of how many MB you will want to keep (that's going to be data space plus some extra to handle reindexing--if you shrink out all of the free space, it will be painful when SQL auto-grows the file back out during the reindex that you will have to run to fix the fragmentation). Next, write a bunch of SHRINKFILE commands (to save time, you can use a text editor with a good macro or scripting facility or write a small script in tsql, powershell or vbscript) that will shrink the file in small chunks, maybe 1 GB or 500 MB at a time. Start with the current size of the file and slowly move down towards where you want the file to be. As a dumb example, if you have a database file that is 500 GB and you want to shrink it to 400 GB, you would want to use statements more like: 

You say that everything is fine, then after a couple of weeks, performance drops. (Usually, people claim that performance drops quickly, or at specific times, or at seemingly random intervals. That could mean bad I/O performance or lock storms or cpu-intensive queries running at wierd times, or a heavyweight scheduled job or lack of indexing or bad stats causing cpu-intensive queries or disk reads. Or other stuff.) Weeks is unusual. My hypothesis is that another application on your server is leaking memory. I have seen this with virus software (every DBA's favorite server software villain) and 3rd party monitoring software. I would double check the memory usage of SQL Server, over time, and I'd grab all of the memory usage of all of the other applications on the box as well. If you have hard limits set on SQL Server's memory usage and have it set to not allow paging, it might be other apps that are getting paged out and eating up I/O capacity. It's not hard to look for. If you are not already keeping metrics on the server, I would just start up Perfmon and have it grab a sample every 30 or 60 minutes. After a few days, you may see another applications memory usage creep upwards. Are there error messages in the SQL Server log stating that "significant portions of sql server have been paged out"? That would also be a big clue. 

If I run a crosscheck archivelog all, this picks up only the recent archivelog files that I expect to be there: 

On the other database on this server (that is behaving as I think it should), using identical config/scripts the recovery area usage looks like 

Due to diskspace concerns, a second disk was added and I tried to move rman to use this disk by changing the channel device eg 

I've got two databases running on the same 12.1.0.1.0 oracle home on the same machine, on the same disks. Each database has multiple schemas of approximately the same size. On database A, the datapump exports take on average of about 2 minutes. On database B, the exports take on average, about 20 minutes. These exports get run twice a day, and the performance has been consistent for at least as long as I've been paying attention. I've also tried different times of day incase it was related to system load, but that seems to make no difference. The configuration for the databases are largely the same, the only differences I can see are file locations, and the pga limit/target, both of which are big enough for the respective databases, and the log_buffer (which I don't think should impact exports, but maybe?) I've added METRICS=y and LOGTIME=all to the backup scripts - that revealed some interesting timings: Database A (fast) 

(in hindsight, symlinks may have been a better option but at the time I preferred to make the location of the backups on a seperate drive explicit/obvious). This channel change didn't seem to have any effect. Initially I suspected because I have a 7 day retention period set and the old image were still valid, however after 7 days datafile images were still being created in the old location. As diskspace was becoming increasingly tight, I changed the backup script to use a different TAG to try and force a new set of backup images to be create on the new disk. This seemed to work - the new set of backup images were created okay on the new disk that night, but subsequent recovery operations seem to be failing. 

The usual problem with selective copying of many tables (but not all tables, and not all rows) is that people run into foreign key violations because they forget something somewhere. If django isn't declaring foreign keys (and I don't know anything about django, but I am old enough to remember when ORM mean "run away quickly"), some functionality might just wind up broken, and you will have to troubleshoot those problems because it might be missing data or it might be code. In short, I suspect that this is more trouble than it's worth. 30KR doesn't sound like a lot of data, unless you have thousands of tables. I'd evaluate the size of the tables and see if I could just restore the database to a seperate machine running the dev edition. You could also restore a copy of the database back to the produciton server (assuming that you have enough space) but then you'd have to be extra careful to use the test database for testing and that whatever test load you have doesn't impede production. 

If all you need is a key/value store, you might want to look at some of the NOSQL options as an RDBMS may be overkill. If you stay with SQL Server, you should investigate the "table partitions" feature, which can help organize very large tables, and table/data compression feature, which can reduce the amount of space that data takes on disk and in memory. 

A few quick ideas: The local server initiates a connection to the remote server, runs a query and fetches appropriate rows from the remote server. This all takes time. Worse, if you trace such activity with SQL Profiler, you may find that rows are fetched from the remote server to the local server one at a time, or in very small batches. This is very inefficient, and the more rows it fetches the worse and worse it gets. If all data is local, none of that happens and the data can be retrieved much more quickly, especially if it is sitting in the data cache. Even though there may be indexing on tables on the remote server, SQL may not be able to take advantage of them while it can build a local query plan that does take advantages of indexing. For very large remote result sets (in terms of data size), you may run into issues where SQL needs to grow data or log files to accommodate the data. 7 seconds probably doesn't qualify, since it probably isn't bringing back much data. Lastly, if the remote query is to a production system and the local query is to a dev or test system, load on the production server (which does not exist on the dev server) may cause blocking of the remote query. This would also slow down the observed performance of the remote query. 

The incremental backups them selves seem to be being created fine and in the correct place, it's just the recovery that is failing. Is there any way to see what/where RMAN is looking for the datafiles during the recover command? Or is there a way to force it to look in the new location? If needed, my RMAN config below: 

As mentioned, this is the same oracle home, same server, same disks etc - the two databases should be performing at least roughly the same I think. Any know what could be causing this? Or where I should be looking to see what could be causing the slow down? Thanks 

Is it actually even possible to connect to the reports server from external clients? and if so; Where abouts do I need to configure the reports server so it listens for external connections? 

I have tried explicitly setting the CHANNEL DEVICE as part of the backup script (immediately before the BACKUP INCREMENTAL line) - this made no difference. Also maybe related, the location does not seem to match what I am specifying above - vs - this seems to match up with db_recovery_file_dest set in the database: 

I've enabled tracing on the reports server and can see requests on the server coming from the external clients (i think), but I only get one line per report run attempt. (But consistently get this same line so I think the rwclient is at least seeing the rwserver): 

but I can't see any options in there that would override the MAXPEICESIZE To be clear - I'm NOT concerned about the location just the file sizes. Is there another setting somewhere that I am missing? Do I need to remove the db_recovery_file_dest parameter maybe? Thanks Backup summary of the latest backup: 

Looking at the output of , there seems to be number of ports opened by the reports/java process - most of them open to 0.0.0.0, but a couple are bound to 127.0.0.1: 

I'm running a 12.1 SE database on Oracle Linux 7. I am getting nightly errors/warnings from my rman scripts about missing datafiles when trying to recover. I have been running rman to a local drive using the following script: 

Pulling data through a linked server is unlikely to be quick especially if you are joining remote tables to local tables. (Just use Profiler to watch what your local server sends to the remote server and I think that you'll be convinced.) Linked Servers are convenient and often "good enough", but not when you start blowing through your batch processing windows. I have seen overall processing go faster by pulling the remote data into local (temporary or permanent) tables and then "doing the joins" locally to figure out what to put into the production tables. If you have a large amount of data, I suggest looking into using a SSIS package or bcp and probably bringing the data into a local staging table first and converting the code to run off of local tables. This might involve a lot of surgery to your jobs since you need to have a job step (or steps) to run bcp code and/or packages. If you are using SQL 2008 or better, MERGE should be available and it could provide more efficient operations. I am unsure if MERGE works with tables on a linked server and, if it does, performance may not be much better than what you have. So, you are still stuck getting the data onto the local server. Also, you could look into partition switching, but I think would be the most costly thing to do in terms of time spent on getting things to work. You could view it as a learning experience. Alternatively, you might consider increasing the query timeout, which is a server-level setting on the local server. That might help with the timeout error (until your data volume increases, then you would need to tweak it again), but I doubt that it will help with the TDS error problem. If you want to minimize the length of time that your production tables are locked (due to the INSERT, not the SELECT with nolock), you would want to get all of the data from the ETL system onto the local box first, then go and insert it all. You want to avoid pulling data for one table, inserting that, pulling the data for the next table, inserting that, etc. (I'm presuming that you are doing all of this in a declared transaction that I'm not seeing.)