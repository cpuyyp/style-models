I'm not familiar with Sequel Pro but here's my guess at what happened: The records we dumped out in utf8 correctly but the import was treating the dump as if it was latin1. Ensure that both schemas are set to utf8 and both the export and import connections are told to talk in utf8. Using the built in mysqldump command is pretty good about making sure all variables get set properly to avoid character encoding issues. 

Then do that and you'll probably be sitting pretty. As a general rule though you'd want to leave at least 30% - 40% of total system memory for other mysql overhead and dis cache. And that's assuming it's a dedicated DB server. If you have other things running on the system you'll need to take their requirements into consideration as well. 

They didn't explicitly mention this, so I am only guessing here, but they are using some search index such as Lucene to perform the actual searches. They'll have a persistent database of some sort but their search index is periodically built from that data set. 

If you have access to restart the mysql daemon you can follow the mysql directions for recovering your root account. 

Is there a way to determine the hostname of the server you're running in w/ in a stored procedure? You can run system hostname from a client but you can't make system calls from SP. I don't see it any of the global variables or statuses. 

AFAIK you cannot configure mysql to inject this information for you. What you could do is setup some other system monitoring such as nagios to independently sample system values such as CPU util and load avgs. Then you can correlate the graphs with the times of your queries 

Does anyone know of any existing tools/products that accomplish what I'm trying to do? After searching around for a bit the only thing I could find surrounding what I'm trying to accomplish is someone else looking for the same thing $URL$ The idea is I'd like to capture all the traffic to my master to save for a replay log against a snapshot of the entire database taken when the monitoring started. Bin logs won't serve what I want since they only include writes. I want read activity to realistically view the effects of proposed changes with "real" production traffic. Real production traffic meaning everything from all applications that are hitting the database to be modified. If there's some application level change, tests running just that app don't account for other activity going on in the system at the time. Running all applications in a test environment aren't guaranteed to have the same state of the database. I could take a test snapshot as a starting point as I fire them up but the applications don't have the ability to do an exact replay of their own activity. I've use the tcpdump script from $URL$ to monitor activity but this doesn't tell me which queries are coming from which connections. Part of the playback I'm wanting is a multi threaded approach that replays the activity from the same number of threads that were actually in use. I can't afford to turn on general query logging b/c my production master wouldn't be able to handle the performance hit for that. The whole snapshot part of the process is to have a golden start point database to ensure everything is the same during the start of each test run. 

What version of the client are you using? I remember this being the older behavior but recent versions of 5.1 and 5.5 seem to just cancel the running command without exiting the client. Then again it may be that I'm using the Percona builds. You could try just running their client if that's the case. 

I was wondering if anyone had any pointers on migrating MyIASM tables from mysql 5.5 to 5.6, specifically using Percona builds, via an rsync? Is this safe? I know we need to do a full mysqldump/reload for a migration for InnoDB tables but I didn't see any notes about significant changes to the MyISAM format between the two. It would make things a little nicer for a rather large MyISAM db that would take several days to complete with a mysqldump. 

What leaves a shadow of a doubt that's the exact bug that's biting me is the schema and application have been running for years with out issue. It started happening shortly after upgrading mysql versions. That might seem like a 'well duh' thing but it was an upgrade from 5.1.5 to 5.1.59. The ticket indicates it's been around since 5.1.35, so should have bitten me much sooner, no? If you've run into this what version were you running? 

Why the jump in thread IDs? Just grepping out all the "Connect" lines there are bursts of incrementing thread ids +1 then varying size gaps for the next connect id. Is this expected behavior? 

It depends on how long you want to measure. If you won't be recording any times longer than 2^32, or 4294967296 ms than a regular unsigned int is just fine. That corresponds to about 50 days of clock time, btw. If you were storing unix time stamps or larger intervals then look at using bigint. This gives you 8 bytes of space, or the ability to record times as long at 1.84467440737096e+19 ms (which is roughly 500 Million years) 

Note Your actual value may differ. Then from your shell shut down mysql, back it up and copy it over. Something like: 

I seriously doubt you're being CPU bound. This can easily be confirmed or denied by watching top (or process manager? in windows). More ram and faster hard disks are likely the hardware facets you'd want to improve on a DB server. 

It appears to be a change in the optimizer. I haven't tracked down the exact setting that caused this yet but the explain on the 5.5 version was showing it using the Primary Key. On 5.6 It was using the secondary char1(8) index. Adding force index (primary) got it back to it's 3-5 minute count time. 

Check shell histories for mysqladmin calls cd /home; for u in *: do; sudo grep mysql /home/$u/.bash_history; done Check with people you know that either have sudo or mysql root access on this machine 

To get a feel for current storage requirements. What you'll be more interested in is growth rate over time though to ensure you're not going to run out of space in 3months down the road. MySQL doesn't give a built in growth rate metric so that's something you'll need to periodically poll and record yourself. For memory requirements, in a perfect world your innodb_buffer_pool would be big enough to accommodate all your data and indexes in memory. It's never a perfect world though and can become impractical or cost prohibitive to have such a machine. At the same time, strive to make the buffer pool as large as you can to keep most frequently accessed data in memory. If you are straight up dropping tables the storage space will be reclaimed to the file system if and only if you are configured with innodb_file_per_table = true. Other wise all your data is stored along with metadata and transaction history in ibdata1. 

The percona tools don't take the standard mysql client arguments. You need to specify a DSN in their format. See $URL$ You'll want something like 

If you want this to replicate through to your slaves you need to specify a --replicate option that tells it the table to place the checksums in (and consequently put the slave values into on each respective slave). If this is your first time you might want to have it create that table for you with --create-replicate-table. It won't need to directly connect to the slaves unless you want it to monitor the slaves for falling behind. It basically issues queries like replace into checksumsTables ... (masterV1, masterV2, (select blah...)) where the literal master values are written to the binlog along w/ the select portion of the replace. When that executes on the slaves the select does it's think to get the slaves checksums. I've spent a lot of time with this tool recently so start out with that and reply with more specific problems you need help with so I don't just rewrite the entire manual and every possible thing that can go wrong. Note: if you run 

I have a master-master setup where each master has it's own slave. They layout might be illustrated as: 

I was trying to evaluate the latest tokudb Hotbackup for percona's tokudb engine. This install is under debian (Ubuntu 14) I was able to get all the packages installed with out error and get all the base toku plugins installed. Attempting to install the backup plug i get 

I'm trying to figure out the difference between some warning messages related to data truncation. Consider the following table: 

We're running locally managed databases for our Atlassian suite (Jira/Confluence/Fisheye). I noticed something strange in the slow query logs. There are a bunch of selects of session variables showing up like 

It is a non goal of pt-table-checksum to show you the actual differences. However, there is a companion tool pt-table-synch which can read the checksum table generated by pt-table-checksum. It will take the bounds of chunks where master/slave checksums differ and then scan with in those to identify rows that don't match. It will even generate replace statements ready to execute on the slave to get it in synch with the master. 

I have a collection of documents I'd like to pull a subset created after a certain point in time. I understand the timestamp of creation is encoded in each documents ObjectID (assuming they are auto generated). I see the ObjectId has a getTimestamp method that returns that portion of the ObjectID as an ISOdate. I'm not very fluent in mongo and am having trouble constructing this seemingly simple query. For bonus points, once I figure out the "where clause", if you will, I'm wanting to select a single field from the documents using mongodump or what ever else might be available to export the results to a text file via a mongo shell. 

From what I've read and gathered from my tests the answers are: Yes. Yes. I'm not sure. Update I've been running w/ some Dynamic and some Compressed tables in various instances since this post with out issue. Further I neglected to read $URL$ at the time. 

Convert and cast docs. Aside from the syntax I believe they can functionally be considered synonyms, even when trying to change character sets. 

Rolando's solution has many caveats. The first being one replica stream is necessarily not replicating while the other works. This is going to give you periods of time where your slave is out of synch. You now have to play a delicate balancing act to ensure each has enough time to catch up when it has its "turn". As described you also have to play book keeper of log positions to switch back to. This really just seems buggy, opening the window for missing or inconsistent data or even breaking replication when it goes wrong (either being caused by even a just 'off by one' error in the log position) I would recommend just running multiple mysql instances. There's nothing stopping you from running two or more mysql's on the same machine. They cannot both operate on the same port of course. I don't really see this as being a problem though as every client and library allows you to specify something other than 3306. Just specify port=3307 (or whatever in one of the .cnf files). You will also want to take care in ensuring the individually configured buffer pools and other memory configurations aren't at odds with each other. This is actually a benefit though as you can more finely tune those settings to the specific requirements of the individual databases that are being replicated. This way you just have two replication streams running into the same server; never behind, no book keeping required, no "swapping" script required. 

You'll want indexes on domain_id on domain. for domain_settings a compound index across (domain_id, is_keyword_checked) would be beneficial. 

Then your create trigger you posted, then finish your statement with just entering your new delimiter in the prompt 

Doing a reload like this will give you the added benefit of defragging your tables. This example also implies you don't mind having the 5.1 DB locked up while the dump is occurring. 

What can cause reading a simple session variable like tx_isolation to be anything less than instantaneous? 

This turned out to not be a mysql issue at all. The network team recently installed a new security device that will block packets on certain rules. A legitimate database write contained a sequence of characters the device deemed nefarious. The overall connection handshake for replication was able to make it fine but then it just sat there asking the master for the next log entry whose packets were never making it back. As far as the broken slave was concerned, it was up to date b/c it had executed the most recent event in the relay logs it had received. 

Is there anyway to tell with SQL if an indexed column is the first part of an index. The closest thing looks to be information_schema.columns. But that seems to only tell me if it's a PRI or MUL key (not if it's the only member of that index or what position its in if it were a composite key). 

You'll see a slow query entry with a query time of 60 seconds and no lock time. 50 seconds is a standard lock wait time out setting. Run 

This question came back on my radar after a recent comment was posted. This issue at the time turned out to be this reporting table that was meant to be maintained by triggers was on a slave only. The insert on duplicate key statements were getting flagged as not safe for statement based replication and getting pushed through in RBR (the stream was in mixed mode). RBR events do not fire triggers. 

Playing around with TokuDB I'm finding even after "optimize table" for things such as changing row compression or other DDLs it takes a non deterministic amount of time for freed space to be reflected in the file system. Is there anyway to force this more immediately or otherwise view status of if the clean up is in progress or otherwise scheduled? 

The problem I forsee is the server crashing and you loose 500 gigs of data. I'll only use in memory engines for temporary things that aren't really production critical or queues that can be ultimately rebuilt in the event of a crash. It would be more prudent to go with innodb and make sure to set your key buffers accordingly and leave the rest for disk cache. It doesn't sound like money is the problem for you so invest in an SSD raid solution for your disks. 

This is just a hunch, but make sure your collation settings are the same between the dbs. I know you said they're synced, but check. If you can't find anything there run each select into their own outfile