Transforming columns to rows is sometimes referred to as an unpivot. Declare indexes on the keys and other predicate columns. Effective indexing is a separate subject for which we'd need the precise table design and the query. It may be possible to pre-calculate aggregate values from these normalised tables. Running a batch job after loading and writing aggregates to a further set of tables is one approach. Materialized views is another. 1.75 billion values is large but not outrageous by current standards. Any of the usual RDBMSs on reasonable hardware should be able to handle it. Unfortunately the devil is often in the detail. It is possible to defeat the best software with contorted queries and convoluted schema. To avoid that we would need much more specific information. 

The method must start at the root of your xml. It will then return one "row" per matching node for the clause to work on. It should look like this: 

Response: There are two types of model - logical and physical. In a perfect world the logical model is written without giving thought to any implementation considerations whatsoever. It can be to whatever esoteric normal form you desire. Like an elegant proof in theoretical mathematics it is "beautiful". The physical model, however, has to recognise that IO takes time, CPUs are expensive, memory is strictly limited and there is never enough of it. The physical model has to deliver a database which is actually useful to the application's users. One technique for accommodating the limitations which reality imposes is demormalisation. I would suggest that holding the last order ID in the customer table is an example of denormalisation for this purpose. It is done to allow the system to give quick response to users, with the knowledge that it adds complexity to the system, and therefore it is acceptable in the physical model. 

The two worlds differ because of "state". A web page has none; a database is nothing but. A web page is rendered and it is over with. Yes, a user can click here and there, type values and widgets may appear and go again, but nothing permanent has changed as far as the enterprise is concerned. Should the rendering computer fail no inconsistency can be introduced. The page can be completely re-created as it was with a single call to the server. A change to, say, some CSS does not have to account for every other piece of styling that has been applied to this page throughout history. A database, however, must maintain and protect state. If the DB server fails it must (should!) guarantee the integrity of the data it has acknowleded as successfully written. Rebuilding the current data from all past input would be time-consuming, assuming the previous files (key strokes?) exist and can be processed in the identical sequence. A change to a table's schema must take account of existing data's structure and meaning. Declared referential integrity (DRI) constraints, which further guarantee's data quality, must be honoured. NoSQL tries to side step this issue by allowing dynamic schema i.e. no to rows need have the same columns, data types or meaning. This allows very rapid evolution of the applicaiton without hinderance from database upgrade tasks. The responsibility for ensuring the completeness, correctness and consistency of the data is now in the hands of the application developer with little or no help from the persistence tool. Being able to Deal with all the existing "state" in the previous format(s) is now her responsibility, too. The difference between logical and physical models is a valid one, too. In my experience, however, they tend to be very similar. The mapping from logical to physical must be explicit within the modelling tools (if it is to generate DDL) so could be made transpartent to code written against the logical model. I don't think this is a great impediment to the world you hope for. I have used CASE tools where the logical data model can be entered into the tool, procedural logic written against that model immediately, and a functioning system generated at the touch of a button. It was a great tool to work with. It was, however, expensive and seems to have declined. There are tools which will take a "desired state" model, compare it to an "existing state" database and generate the delta DDL. This can be integrated into a continuous build & deploy workflow. Having seen a generated change script which was so awful it would take in excess of three days to complete, however, I'd recommend reviewing manually before submitting. 

1 SQL Server 2017 has implemented graph processing with precisely this form. It has added extensions to identify a table as containing nodes or edges. 

If you look at the two execution plans you will see that the estimated cardinality is different in each case. When the actual value is in the statement the optimiser can use the cardinality of that one, actual value to choose an access path. When the has a variable the optimiser cannot know whether you have provided a very popular or very unpopular accountId so produces a 'best guess' plan. Unfortunately for your situation this is awful. Stored procedures (SPs) would be good. Then you can put all the query hints you want in there. Also use to get around parameter sniffing issues. If you have good inexes already, and you don't have access to the source SQL or can't use SPs, Plan Guides may be a good option. If you end up using them be sure to schedule a review every so often to ensure the chosen plan is still optimal. 

You may be able to optimise further by referencing only once in each part; I'll leave that to you to decide according to your needs and business rules. Oh, and stop using and . 

Your prolem occurs because of the different steps involved in query execution. First comes parsing, then binding and finally execution. In parsing the submitted text is verified as valid SQL and converted to an internal representation. Binding resolves the names from the submitted SQL to objects inside SQL Server's catalogue. Unfortuately does not exist when binding is attempted so that part fails. There are two things you can do*. First, split the query into two batches, one to create the column, one to update it. Only run the second if the first succeeds. Second, put the in dynamic SQL and run it via . This effectively creates a second batch internally. *That I can think of right now. I'm sure wiser heads could suggest others. 

When you perform a restore you have to specify the database name the backup will be restored as. If a DB of that name exists it will be overwritten and all data previously in it will be lost. If you only need to transfer rows from the log table there are various ways to do this. If the two DBs can see one another over your network a simple INSERT statement using three-part or four-part name will suffice. Otherwise the bcp command line utility writes a table to file. It can also load the file to the destination, or a staging table if that is preferred. The "proper" solution, to my mind, is an ETL job. SSIS is the tool of choice in the world of SQL Server. It will not be difficult to learn how to do a data copy like this. There are other features which will automatically move data from one DB to another, such as mirroring, replication and log shipping. They are solutions for high availability, however, and are excessive for this scenario. You would have to ensure writes but not deletes are synchronised, for example. 

Storing the submissions is easy. Any persistence software with sufficient bandwidth will do. Heck, even the OS's native file system will do. From this perspective JSON in MySQL is excessive as you have the DBMS overhead. Similarly with an update. The system need only overwrite the current submission with the new one. The real constraint on your design will be on retrieval. You use the word "filter." I presume you want to see all submissions with a given value in a particular form field. To do that efficiently will require indexes. Capability in this area varies widely across the various NoSQL and relational products. You must learn what each can provide, gain a good understanding of your requirements, and choose. If you adopt a relational store you will likely gain a lot of advantage by shredding the JSON into table(s). This is how an RDBMS is meant to work. It may be difficult to settle on a schema, however, if there is great variabilty in your forms. There may be some advantage to holding, in addition to the submissions, a separate list (document / table) for each answer and pointing to all submissions which contain that value. This may become unworkable at scale because there will be a document keyed by answer holding an array of submission IDs. When any of those submissions is updated this document will also have to be updated. This will be a bottleneck when there are a large number of updates per second. It will require careful programming to keep the submission document and answer cross-reference document in sync during updates since Mongo does not have transactions that span documents. 

You could add an column to the base table () to indicate which of the sub-types any particular occurance writes to. I don't think that is necessary because the process which writes to this database has to know which type it is dealing with in order to capture the correct values. Even if that process dynamically builds its list of attributes by examining the DB, it has to know which ultimate sub-type it is looking for to bootstrap the process. It is tempting to have, say, and a separate foreign key . This is unnecessary. When retrieving values you will either be interested in a known occurrance of a sub-type or will be looking for all available values for an instance i.e. "Human 99 as Employee" or "All about occurrance 99". The former can be had by ing up to its ultimate ancestor using the defined keys. The latter by ing all tables in the model. If your attributes can vary at run time you will be forced into using some variant of an entity-attribute-value (EAV) model. This has been well documented in many, many blogs, papers, forums and SO questions. While being fractious, conceptually challenging and non-performant these models can be made to work (for an appropriate definiton of "work"). Your tables will be a lot like this: 

If the objective is to be able to sort the rows according to then don't bother updating values on a delete. The rows will come out in the same order after a row is deleted as they would before. It is not worth the effort to close up the gaps. In the worst case, if you delete the lowest-numbered row, you will have to update every remaining row in the table, in a transaction, with locking and all the concurrency problems that involves. Don't do it. If you absolutely must have some visual indication to users then let the application UI produce one, or use a function like ROW_NUMBER() at run time (SQL Server; other RDBMSs have similar). 

Your question is ambiguous. Do you wish for a user to see all rows in one table, but only one of the three tables? Should a user be limited to only certain rows within a table? Must this be enforced within the schema or is it OK to use other aspects of the application and infrastructure? Anyway, here are some ideas. Most DBMS have quite fine grained permission systems. It may be possible to GRANT just the permission required on each table. Create views that expose just the correct tables, columns or rows. Grant permissions on the views but not the underlying tables. Write stored procedures for each read & write. Grant access to these SPs but not the tables. Alternatively, let everyone execute all SPs but have the code reject unauthorised actions. Triggers can achieve a similar end. In the application have separate screens for each read or write action for each entity type. Limit access to the screens through the authorisation system. For a SQL-only solution for row-level access, add further tables, each keyed by userid and the store or business or corporation id. There will be one further column which is the permission granted. For each statement touching a base table join to this permission table and qualify on the userid and action. If the user does not have rights zero rows will be affected. These permission tables can get big. It may be useful to put users into groups and assign rights to the groups. If there are rules to control which, say, stores a user sees these can be modelled instead of the naive approach above. The precise table definitions will depend on how the rules normalise. This can get quite tricky and will require a lot of testing.