It looks like this is part of a device driver for a Matrox graphic card. Assuming that the serer is using that brand of graphic card, you could try uninstalling the driver completely (reverting to VGA) and installing it again. 

In such a case, the system will prompt for a password every time the private key for your user's certificate is accessed which includes every time an encrypted message is opened. 

Internal network can only talk to each other and to the host. You can't connect such a vswitch to an Ethernet adapter. The correct option is to use an "external" vswitch type. 

First question: you got the right answer already (WSUS) Second question: it depends a bit on how your thin client is distributed. If it's an MSI file, then you can stick it into the AD GPO for your users and it will be deployed on all machines. details will depends on what version of Windows you're running (both AD and on the clients). 

As far as I know, you cannot do that unless you write a script to do the lookup and adjust the rule. But in all honesty, you shouldn't do that: DNS isn't really a secure service unless you're using DNSSEC (which I doubt). In adition, you shouldn't need to do that, really, unless you have restriction on your outbound connections from an external machine. What are you really trying to do here ? 

If you want to expand beyond the Linux distro world yet still stay small-footprint, check out installing FreeBSD. It comes with Sendmail (general ugh, but good enough out of the box) as part of the base install. You didn't say how little your old computer has, but FreeBSD should run just fine on something as little as 64M. 

The "default" category is supposed to get all of the logging that doesn't happen in other categories. However, when I start named, I get a bunch of messages in /var/log/daemon.log, and then more in the named file. I would like them all to go into the named file so I don't have to look two places to figure out what might have gone wrong when starting. 

In Ubuntu, I have two ethernet interfaces eth1 and eth3 that I want to call "foo0" and "foo1", respectively. Basically, I want something like "ip link alias DEVICE SECONDNAME". Is there a way to do this without mucking in the kernel? 

I am creating a test system that will be run by others. It will have some kernel mods, and some of the default software in /usr/bin will be replaced. Because of this, I want to prevent the users from doing "sudo apt-get update" or "sudo apt-get install". These systems will not have any valuable data, and will mostly be behind corporate firewalls, and thus it is safe to completely punt on security-based releases. FWIW, moving apt-* and dpkg* to /usr/dontusethese does not work because some other processes like (gag) MOTD updating relies on them being in /usr/bin and whine if they are not there. 

Yes, you can perform this, but it isn't exactly trivial. First, you need to establish a certificate authority that trusted by the client. Once oyu have done that, you can use the sslbump feature of Squid to perform the decryption (see $URL$ While you do not explain the context of what you want to do, it is probably worthwhile mentioning that if what you want to do is debug outgoing HTTPS connections coming from a windows machine, you can perform this in a much simpler way by using the Fiddler web debugger ($URL$ 

No. You can't really disallow access to these files. What you can do, however, is hide and restrict the drive so they won't be able to navigate is through explorer. It's not bulletproof (because it relies on application supporting the limitation) though. 

In theory, you can query the DNS server for a zone transfer (AXFR) request to get the whole zone file. In practice, however, this is not going to work against a domain that isn't your own: this option is usually not enabled from Internet at large. 

I have written a very simple batch to perform that analysis. It will grab the appropriate symbols from MS's web site so you don't have to download them beforehand: 

This is probably a combination of two errors: the server sending an invalid response and the AV handling this wrong response by corrupting it. The server sends a response with code 401 but no header. This is invalid according to the HTTP standard: 

With this forwarder the connection to 192.168.1.17:1988 gets forwarded to 127.0.0.1:41952. The client will get the original certificate from the server because the forwarding is done at the TCP level. The server will see the connection coming from 127.0.0.1. EDIT: after lots of communication it is now clear that the aim is not to have the right source hostname es claimed and in the question and not the right Referer as claimed in a response but that the Host HTTP request header has the expected value 'localhost'. Since the host header is set from the URL you would need to make sure that the request gets forwarded to the remote system and the browser is not trying to resolve the URL by itself, because otherwise it would try to connect to server on the machine where the browser is running. To defer the resolving of the URL to the target system you need to run a proxy there, i.e. either something like Charles Proxy which you've tried or some SOCKS proxy. 

I host my FreeBSD image at Opus One ($URL$ They are a long-lived small ISP with a pretty impressive setup in their own facility. I had been on a pizza box with them for years, but have just transitioned to their VMware cloud. If you read Network World reviews, you'll know Joel Snyder's name. Highly recommended. 

This is a feature/bug of putty, not FreeBSD. putty always tells the other OS what its width and height are. You can show this easily by doing one of your truncating commands, then widen the putty screen. You can see that the lines were truncated. Now give the comand again: you will see more columns. In essence, what you are asking putty to do is to lie about the width of your window (giving a number larger than what it really is showing) and to wrap the longer lines intelligently. That will work fine for and and so on, and fail miserably for or any other full-screen editor. Such lying can also cause display of information that can be misinterpreted because of the wrapping. There may be a mode of putty that does this, but I could not find it. 

II am working on a project where I need to create a virtual machine that acts like a home gateway, so I am looking for the firewall/NAT that is most flexible. My hope would be that I could give some configurations that would make the VM act like some of the gateways typically installed in homes and small offices. Has anyone tried this before? Which software might be best for this? I can use any OS, so it could be pf, pfSense, iptables, ipfw, ipfilter, or even something more obscure. Any clues are appreciated. 

Several ways: The easiest is simply to create a web site bound on port 80 containing only a default page with a meta refresh in the header: 

Alternatively, you could also modify the source application so that it will write to both DBs at the same time (I used that solution in a similar case). Frankly, I can't see any (reasonable) solution that would be simpler than making sure that both DB are using the same version of SQL server. 

Your problem is that you're using an IPv6 address to send mail to google. In itself, that's not a problem except that: 

Routing only happens between two (or more) different networks. If you have only one NIC, then either your have a local virtual segment or you're using another kind of interface to connect to another network. Your network diagram doesn't make a whole lot of sense either: you should have (at least) two interface per router, not one. Could it be that you actually have bridged your "internet router" and your "linux router" ? That would explain it, then. Also, what's the internal IP range looks like ? is it a routable IP range or do you need NAT ? 

The problem can be solved by synchronizing the zoom factors of both screens but, of course, that is a major inconvenience when the user has several screens with very different DPIs (typically, a laptop or tablet with a QHD or UHD screen and a main display with a 1080p one). In such a situation, the user has more or less to stop using one of the screens. 

a) Yes, I have experience with this. b) The answers above about using hashes answer only the question you asked in the title of this thread, not in the body. To prove you had them before you got the CD-ROM, you will need to provide logs of when they were last touched, something you probably don't have because this kind of information is rarely kept. c) Having said that, your company probably does keep backups, and those backups have dates on them, and those backups can have files selectively restored from them for matching. If your company has a written backup policy, and the backups you kept match the policy, this will make it much easier to convince someone that you didn't fake the backups. If you don't have a policy but the backups are clearly marked, that might be sufficient (although the lawyer for the other side will question this up the wazoo). d) If your company didn't keep backups, and all you have is the described screen shots, forget about it. You will have a very hard time convincing anyone that you are in control of your data well enough to "prove" that you had those files first. 

First-hand recommendation: Rootbsd.net. They are part of a larger Linux hosting house, but their FreeBSD expertise is quite high. I have had a few in-depth conversations with tech support, and always come away impressed. 

Certificate validation will be done by the client. Use of wildcards is defined for POP, IMAP and SMTP, so a wildcard certificate should fit. But it might be that some older clients have problems, because the details for certificate handling in SMTP were only defined much later then for the other protocols so some old clients might not expect wildcards. 

HPKP does not replace the normal validation. Instead it is additionally to the normal validation. Thus, if the normal validation fails since the certificate is self-signed HPKP will not help. 

Because the browser does not know up-front that it will get the same certificate when connecting to another hostname, even if the IP is the same (because of server name indication) it cannot reuse the same connection. Even if the certificate it gets for example.org includes example.com it cannot be sure, that it will get the same certificate when connecting to example.com. 

You must make sure that the name you use to connect matches the certificate. That is either the certificate needs to be changed so that it includes the IP address or you must the hostname as seen in the certificate to connect to the server and not the IP address. Since the hostname probably refers to the load balancer you might change the hosts file on your client system (the one which connects to the SQL server) so it resolves the hostname to the IP of the server itself and not the load balancer. 

I have started a local Docker registry with and it works fine for pushing and pulling. In a script that is going to pull from this local registry, I want to first test whether the registry is up. Is there any good way to do this short of trying a pull and catching the "Error while pulling image..." error? 

In a lab setup, I want eth0 on a Ubuntu server box to get a DHCP address for talking to the outside world, and I also want to assign it a static 10.x.y.z address for talking to other boxes on the local network that have 10.x.y.z addresses. I want to do this by editing /etc/network/interfaces. The man page for /etc/network/interfaces is modeled after "here's a bunch of examples, I hope you can figure out the actual rules on your own". I don't see a way to give one interface two addresses using two different methods (static and dhcp), and initial fumbling didn't produce useful results. 

I have been using RootBSD ($URL$ for years and remain impressed. Low-cost, excellent tech support. Lots of different VPS plans. 

Is there a way to get all of BIND's startup messages to go into the log named in the config file? For example, in named.conf, I have: 

This is a combination of a bad server setup together with missing support for SNI (Server Name Indication) in python 2.7.6. SNI is needed to support multiple certificates on the same IP address. SNI support is in python3 and was added in version 2.7.9 too, but is not in version 2.7.6. If you connect to the server without SNI you get only the certificate, but not the trust chain: 

HSTS is used by the operator of a web server or web application as a better protection against man in the middle attacks. It tries this by addressing the following ways for man in the middle 

The first line enables listening on port 443 on IPv4. The second line covers IPv6 only. Since you have only a single (IPv4) configuration it is the one which gets used if you connect with IPv4. If you would try to connect with IPv6 instead SNI should show the expected behavior. Instead you might probably use for the default server: 

In summary: I see multiple problems at the server side which might result in the problems you see. If you have access to the server try to fix them there. First it looks like the SSL stack of your target host (174.47.225.118) is kind of broken: 

I have a couple of VMs that cannot have guest additions installed, and I pause and resume them with days between. I normally forget to nudge NTP when I start up the VM. Is there a good way to tell ntp to be very aggressive in how often it should check for changes? 

I need to make a quick-and-dirty NAT out of a Ubuntu 10.10 box. Just "this side is NATted with DHCP handing out 192.168.x.0/24" and "this side gateways to the Internet", and nothing else interesting. There seems to be many choices with varying degrees of documentation. What's the easiest way to do this? 

When booting into Linux, there are sometimes one or two lines that get quickly cleared. I think that some of them don't even appear in dmesg. If nothing else, I want to suppress the clear before the "login:" prompt. Is there a kernel command or sysctl that I can set to prevent this so I can read them on the console screen after booting? 

I need to set up an ESXi 5.1 box to run some pre-made VMs that will not run well under VMware Workstation. I have started testing this, but cannot figure out where the @#$% the control comes from other than vSphere Client, which runs only on Windows. I get strong hints that, at least under earlier versions of ESXi, you could do most of the necessary things (start and stop VMs, add files to the datastore, configure new VMs, and so on) from a command line. However, VMware's site is now so littered with related products, it is impossible to find whether this low-end scenario is supported. I would imagine that a lot of serverfaulters would rather not have some of their critical infrastucture running on Windows. Let us set up a hypervisor box, and we'll control it from SSH. If there is a guide for how to do this for ESXi 5.1, I'm not finding it. Clues appreciated.