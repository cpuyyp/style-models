There are some history topics that link really well with computing - for example a history of communication, taking in writing, printing, semaphore, the telegraph and Morse code etc, through to the internet and the web. The English history curriculum for 5-7 year olds suggests that pupils compare William Caxton and Tim Berners Lee. Another great topic would be cryptography, perhaps starting with the Caesar cipher, mono-alphabetic and poly-alphabetic substitutions, Enigma, Colossus and other work at Bletchley Park, public / private key encryption (with applications to SSL), some of the contemporary issues around privacy and perhaps a look ahead to quantum cryptography. YMMV, but I think context like these may make these topics more engaging than a straight history of computing unit. 

We're introducing some aspects of parallel processing quite early on in Scratch. Each sprite has its own script which appears to execute in parallel with those of the others. Scratch has a broadcast and receive message protocol, and support for shared as well as private variables and lists. Children might encounter this in a maze game, perhaps programming a number of similar ghosts to chase the player's avatar. It's also useful for agent-based modelling, e.g. the spread of an epidemic through a population. Of course, it's not true multi-threading, as all of Scratch runs inside Flash, inside the browser, on just the one core, but I doubt those using Scratch will be aware of, or care about, the distinction. This does, though, lead to potential difficulties in 'graduating' from Scratch to a text-based language such as Python - young Scratchers who've been used to programming in parallel in Scratch can find it hard to adjust to doing just one thing at a time in introductory Python programming. 

I describe programming as a two-step process, of algorithm + coding. I.e. deciding how to solve a problem, and then implementing that approach in a particular language on a particular system. Can you have one without the other? Yes. There are occasions, particularly in the realm of unplugged approaches to computational thinking, when we'll think about algorithm-like solutions without any intention of automating them (e.g. the jam / PBJ / chocolate sandwich thing, or an effective way to play a board or card game). There are other occasions when we'll write in a code that a computer understands without their being any actual algorithm to implement - markdown, HTML, XML, json etc, etc. I think HTML is a great way to get students thinking of a formally specified language, and seeing some immediate effect from their coding, without the additional cognitive load of implementing an algorithmic solution. 

I said earlier in this, "even if there was such a thing". I don't believe there is such a thing. I don't exactly have evidence to back it up. I just have two reasons. 

That's exactly how a for loop thinks. It needs your conditions first, and then what you want to repeat while those conditions are true. (Yes, while - just like a while loop. I personally agree with thesecretmaster in that it makes a lot of sense to introduce while loops before for loops.) As you're pouring, you're constantly rechecking the level of the milk - "updating" a variable in your head as you act. So, if you were asking a computer to fill a glass of milk: 

First, tell them that it happens. I can tell you I have had one awkward conversation with someone on stack exchange in chat in terms of nerdy interests and gender; thankfully someone I know (from SE) stepped in and helped me out. And that was a minimally offensive sort of conversation. Letting your students know that they are not alone is a good first step, and will encourage them to reach out to others who probably have had similar experiences. Just talking about it, and not feeling like maybe it is just you is important. Second, tell them that it's not right. Reassure them that what's happening isn't because of them. There's people out there that for some reason (background, personal belief, whatever) are of the mindset that gender (or age, race, etc) does play a role in ability in certain fields. Solutions 

Neither. Biology has some physics and chemistry, but it also has some stuff all its own. Ditto for computer science - there's science and there's math. (Plus, if you're being really nitpicky, scientific theories are based on math, but, whatever.) There's also a dash of something different. Yeah, teach mathematics, just like you teach a scientist mathematics, or a biologist physics, and also teach science, just like you teach a biologist chemistry, but also teach how it all comes together - computer science! 

Starting languages for non-programers Vastly depending on what is the purpose, but for some people it may be beneficial to choose for a first language a scripting language, that is used only as snippets in a wider context. The main advantage is that you get useful stuff basically from the point zero, and not only after learning half a language. If you stop learning after 5 hours, you may still have some benefit from it. Some possible choices: 

Starting languages for wannabe-developers Now, to the other question: "do you agree that starting with C/C++ is beneficial for a deep understating of programming"? Yes. For a person planning to do any serious programming, I'd perhaps recommend starting with C++ (where "++" stands for "streams, strings, vectors and maps", maybe now also "smart pointers"; the rest is confusing and unnecessary noise) or Pascal/Delphi (that's what I actually started with) for less opportunities to shoot your feet, and a good GUI library with IDE support. It is also very useful to do some programming in a functional language (OCaml? Scheme?) early, to get some different perspective on programming, with better structured framing. I'm not sure it's a good first choice though. However, I'd be careful to not go into too far into high-level programming in C++. C++ "high level" structures are actually thin wrappers around explicitly low-level stuff. To write classes and abstractions, take a proper high level language, perhaps Java or Scala. C++ is not a high level language, and I tend to treat it's high-level features as a kind of a last-resort: if you are bound to using C++ for performance and low level features, that is how you get some abstractions. As for starting with higher-level languages, my intuition is that thinking about classes and this kind of abstraction is unnecessary noise at the entry level, which is why I look with great suspicion at starting with Java (in which even a "Hello world" is in a way OO). I don't have any data-backed point here though. Additionally, I find the Java memory model (i.e. references everywhere) to be both confusing and misleading. 

Firstly, I would say that some things do come before content. Student safety and well being, for one. If you are so focused on content you can't see that a student is having serious problems (bullying, home life, etc) then that's no good. A student must be able to "ingest" the content being presented, and they can't really do that if they aren't well fed, feeling safe, tired, sad, "antsy" or feeling like moving (seriously - ever tried going from P.E. right into a test?), etc. So let's say that first step is fulfilled, and a student is able to pay attention. The next thing I'd say is presentation. This is the difference between plopping up the quadratic formula on the board in Algebra I and having every student write it in their notes and move on, and explaining it, perhaps with some examples, or whatever. In both cases, the students "get" the information, but unless they are devoted textbook readers and question-askers, I doubt they'd actually understand it and be able to use it. Of course, this doesn't necessarily mean just explaining it, this means keeping the student's attention, it means slideshows, videos, activities, demonstrations - things other than just the robot droning teacher. Then, practice. What use is information if it can't be used? Problems, labs, and so on, are just as important as the content itself. You can't have a presentation or a problem about nothing - there has to be some substance to it. But the content won't get through with the presentations and it won't be useful without the problems. But safety and well-being of the students comes above it all. Also, study strategies are important, and so are the prerequisites for the content. These are things, which, if the students don't understand, will make things a lot harder for them. There's probably also things I haven't thought of here - this was supposed to be a general overview and then got a little longer than I thought =) So: is content king? No, content alone cannot do much. It must be coupled with a student ready and able to learn, a good presentation, and practice to solidify the content and make it useful. 

Harvard's grading policy for CS50 is worth looking into. There are four components for the grade on problem sets (each of which involves submitting code). The overall grade is calculated as scope * (3 * correctness + 2 * design + 1 * style) Scope: to what extent does the code implement the features required by the specification? Correctness: to what extent is the code consistent with the specifications and free of bugs? This is done by the check50 autograder, and it's essentially unit testing. Style: to what extent is your code readable (i.e., commented and indented with variables aptly named)? there's a component for formatting: I think in Harvard's case these marks are awarded by teaching assistants, but basic static analysis or linting might suffice. Design: essentially, is this good code in terms of clarity, efficiency, logic, elegance - again, Harvard use TAs to award these grades, and it's hard to see a machine (or an inexperienced grader) being able to award these marks accurately any time soon. If you were determined to use automatic grading, I guess you could do something with run times for test data or the more sophisticated forms of static analysis. A compromise might be the use of peer-assessment and a detailed, criteria based rubric: peer assessment might have other benefits. 

Some interesting projects for those working with block-based languages (such as Scratch, Snap! and Blockly): For Scratch, check out Dr Scratch, which takes a rubric approach to evaluating how much 'computational thinking' is evidenced by a project. Whilst the analysis might seem a bit reductive, it can be used independently by learners and includes some useful guidance on how to progress. The developers describe their approach in this paper. Dr Scratch is built on Hairball, a Python module which does static analysis of Scratch projects. A more conventional autograder, lambda, is being developed by Michael Ball for Snap! It's already integrated into UCB's Beauty and Joy of Computing MOOC, and I think there are plans to make this more widely available. Michael wrote about this for Hello World #3. Chris Roffey has developed an autograder for Blockly used in the initial round of the TCS Oxford Computing Challenge programming challenge, although I don't think the code for this is shared publicly. 

Breaks? I'm in highschool (not an adult) and I'd be kind of offended if someone told me to go "take a break", especially in an elective - I took this course to learn, not to get told that "sitting is the new smoking". You're wasting their time. (Also, don't assume your students spend their whole day sitting - a not insignificant number of them probably work out or do some form of activity during their day. No need to force it on them!) Now, that being said - you don't need to rely on breaks to get people up and moving! There's activities for illustrating different sorting algorithms by having people stand in a row and shuffle themselves according to the algorithm; there's activities for "programming" each other, there's activities for just about anything you want to do. These have the bonus of helping the information stick in the mind of the student ("remember that time we programmed Joe so that he walked into a wall?") and, most importantly, having actual substantive content - that is, not wasting student time. 

Tell them about role models - people who have experienced it, but pushed through. Think the movie/book Hidden Figures, or colleagues of yours, or even you yourself. Give them people to contact (like yourself, or other students) that are role models or who have gone through it. Encourage them to not care what other people think. They chose to go into computer science for a reason - because they enjoy it, probably. Don't let other people stop you from doing what you love. Of course, this doesn't mean that words or actions aren't painful - it's to encourage them to push through that and just do what they've trained to do. Give them ways to respond, i.e., going to HR/management/something within the company, documenting it, things to say, etc. Tell them that they are part of the group that's working toward stopping it. Get them angry, and ready to do something about it if they encounter it, not just blow it off Seek a good environment. With so many companies needing programmers, they can pick the good ones. 

One problem with these examples is that the base class is more of a mixin than a real base. But I don't think you can do much better without going big. So, ultimately, you may want to go big. Want really interesting inheritance? Go big, use a framework. While little examples like above are good for understanding the basics, the most practical examples of implementation inheritance are big, all-inclusive classes provided by frameworks, like Django's views and forms. You may consider using them to show some practical uses of inheritance. While writing a Django¹ application is definitely an overkill, modifying one may be perfectly good task even on (relatively) early level. If you prepare a working application that needs relatively small modification, like adding a view with different sorting, it may be a good hands-on experience. However, this is risky, and may be daunting experience if either students or exercise are not prepared well enough, so proceed with great care (if at all). 

A few tweaks to expression classes, and basic caching is there. Another idea to add common implementation would be listing free variables occuring in the expression (after adding variables ofc). First, make subclasses implement method (returning list of subexpressions, e.g. in case of sum). After that, implement method in Expression, and override it in Var/Variable (to return the single variable used there). The effect should be something like: 

Implement nested arithmetic expressions. Let me start with what it can look like, and then I'll explain why it's a great example. It starts very simple: 

Where is the inheritance? I'd like to point that I didn't use Python's subclassing at any point here, and it was on purpose: Python's subclassing is mostly about implementation inheritance, and here it's not necessary at all. Of course, if you use type hints, you should define an interface, but only in that case. Python is generally duck-typed and there's not much point in pretending it's not. However, you can also add some shared implementation to this example. Adding implementation inheritance There is at least one thing that all kinds of expressions may easily share: caching. Example base class could look like this: