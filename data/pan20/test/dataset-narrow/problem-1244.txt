This will be a bit long, but please bear with me. First, let me clarify the terminology a bit. I think what you are interested in here are in fact the facets of the polytope (i.e., "the minimum set of irredundant valid inequalities" or the "maximal dimensional faces still distinct from the polytope itself") rather than the faces. Plus, as you write, the object you study is much better called the "aggregate-flow polytope" or, as some authors refer to it, the "demand polytope" or the "throughput polytope". Call this polytope $T$. Here is what I know about $T$: 

Now, regarding the number of facets needed to describe $T$. First, I believe your conjecture "for any $N$ there is a graph for which $T$ has more than $N$ facets" is not true. What we can show is that $T$ has finitely many facets, because the projection cone $W$ has only a finite number of extreme rays, and these generate all the facets (unfortunately, usually many more inequalities too). Note, however that this alone does not disprove your conjecture. However, that this still allows the number of facets to be exponential, and curiously this is precisely the case. At least for directed graphs, I seem to have a proof somewhere that for the complete bipartite graph $K_{n,n}$ on $n$ nodes with $n$ distinct source nodes in one partition and $n$ distinct destination nodes in the other and capacities all equal to $1$, all the $\{0,1\}^n$ vectors $b$ except for the all-zero vector generate a facet $\sum_n b_n t_n \le q$ for some appropriate $q$, and this gives you $2^{n}-1$ facets. I don't know about the undirected case, the above may either hold or not hold. For $K=3$ in particular, this gives 7 facets (plus the non-negativity conditions, but that's trivial) for $K_{3,3}$ in the directed case. As far as I can work it out now, the same applies to the undirected case as well. I have fairly good reasons to believe that this is in fact the upper bound for the directed case, but I may be completely wrong here. You can find an awful lot of results on this topic in Schriver's heroic survey: Alexander Schrijver: Combinatorial Optimization - Polyhedra and Efficiency, Volume 3, Part VII, Multiï¬‚ows and Disjoint Paths Should you have any comments, results, or further reading (apart from my own papers) on this topic, count me very very interested. 

The inductive invariant separator problem for Presburger arithmetic is for a given 4-tuple $\left<\bar v,Init,Next, Bad\right>$ where $\bar v$ is a finite set of variable names, $Init$ and $Bad$ are Presburger formulas whose free variables are in $\bar v$, $Next$ is Presburger formula whose free variables are in $\bar v$ or $\bar{v}'$ (a primed copy of $\bar v$) does there exist a formula $\phi$ in Presburger arithmetic with free variables in $\bar v$ such that: 

The inductive invariant separator problem for Presburger arithmetic is undecidable. I am unaware of a proof in the literature to point you at. (It seems so straightforward a question I assume it is somewhere out there.) The proof I came up with follows roughly the same construction as the halting problem. Here is a brief overview. We first assume a decision procedure $D$ exists and then construct a machine $S$ with input $M$. $S$ uses $D$ to decide non-termination of $M$ on itself and then $S$ reverses the output. We then use the construction of $S$ to show that $D$ must give an incorrect answer on the execution of $S$ on itself. Instead of a reduction to the halting problem, the proof is for all intents and purposes a restatement of the proof of the halting problem. It is a bit verbose as will require that the exact strongest post condition can be expressed. (If a simpler proof is possible, I'd be very interested in hearing it.) Now on to the gory details. 

What you need is called a "distance oracle". Unfortunately, I am not very familiar with distance oracles, so I can only refer you to the seminal paper due to Thorup and Zwick: Mikkel Thorup and Uri Zwick. Approximate distance oracles. STOC '01, 2001. Here is an excerpt from the abstract: Let $G = (V, E)$ be an undirected weighted graph with $|V| = n$ and $|E| = m$. Let $k$ be an integer. We show that $G = (V, E)$ can be preprocessed in $O(kmn^{1/k})$ expected time, constructing a data structure of size $O(kn^{1+1/k})$, such that any subsequent distance query can be answered, approximately, in $O(k)$ time. The approximate distance returned is of stretch at most $2k - 1$, i.e., the quotient obtained by dividing the estimated distance by the actual distance lies between 1 and $2k - 1$. [...] The space requirement of our algorithm is [...] essentially optimal. According to their results, what you request is basically doable even for weighted graphs: choosing $k=1$ yields a distance oracle of size $O(n^2)$ obtained in expected time $O(mn)$, which can answer your shortest path queries with $1$-stretch in $O(1)$ time! Distance oracles is a very well researched field, so you will be able to dig further I believe. 

Doing this exercise really made me appreciate Jerome Leroux's work on separators for vector addition systems. 

We now show $D$ cannot give a consistent answer on $S$ with the input $S$. Start executing $S(S)$. $C$ terminates with $Init$ and $Next$ that can simulate $S$ with the input $S$. We now setup a correspondence between state $i$ of the execution of $S(S)$, $s_i$, and the $i$th variable assignment $\bar v_i = f(s_i)$ of the execution of $Next$ starting from $Init$. $S$ is deterministic by construction so by the properties of $C$, $Next$ must have unique successors on variable assignments and $Init$ is a variable assignment. (For reference, $f(s_0)=\bar v_i=Init$.) Suppose $D$ says a separator exists. Let $\phi$ be such a separator. The execution of $S(S)$ then reaches $term$ in $k$ steps. (This includes executing $C$ and $D$.) Now $\bar v_1 \ldots \bar v_k$ is a counter example to $\phi$ being a separator as it reaches $pc = \left<term\right>$. $D$ gave an inconsistent answer. Now suppose $D$ says no such separator exists. $S(S)$ then reaches the control state $loop$ in $k$ steps. All states after $k$ are then identical $s_{k+1}=s_{k+2}=\ldots$. The corresponding variables assignment sequence after $k$ must then assign each variable to the same constant. Let $\phi = \bigvee_{i=1}^{k+1} \bar v_i$. (Note: $\phi$ is now exactly the reachable variable assignments by $Next$ starting from $Init$.) Then 

Associated with any capacitated graph $G$ and set of source-destination pairs $(s_k, d_k): k \in 1, \ldots, K$, there is an "aggregate-flow set" $T$. This $T$ is a polyhedron and it is well-defined. You can see this for yourself by considering the following construction: take the multicommodity-flow polytope (i.e., the polyhedron of the feasible arc-flows or path-flows) and apply an affine mapping that to each commodity orders the sum of the individual flows. As affine maps of polyhedra are again polyhedra, you get the required result. If $G$ is connected and the edge capacities $c$ are strictly positive, then $T$ is a full-dimensional, compact, convex polyhedron in $\mathbb{R}^n_+$ (so it is in fact a polytope). $T$ is down-monotone: $t \in T \Rightarrow \forall x \in [0, t]: x \in T$. You can obtain all the valid inequalities that describe $T$ using the Japanese Theorem due to Onaga and Iri: for any non-negative weight set $w_{ij}: (i,j) \in E$ on the edges $E$ of $G$, the inequality: $$\sum_{k=1}^K \beta_k . t_k \le \sum_{(i,j) \in E} w_{ij} c_{ij} $$ is valid for $T$, where $\beta_k$ denotes the shortest path distance from $s_k$ to $d_k$ for commodity $k$ over the weights $w$, $t_k$ is the aggregate-flow for the $k$th commodity and $c_{ij}$ is the capacity of edge $(i,j)$. In fact, you get all the valid inequalities for $T$ this way. Any weight set that is not an extreme ray of the below projection cone $W$ generates redundant inequalities for $T$, where $$W = \{(w,\beta): \forall k, \forall P \in \mathcal{P_k}: \sum_{(i,j) \in P} w_{ij} \le \beta_k, w \ge 0\}$$ where $\mathcal{P_k}$ denotes the set of all paths between $s_k$ and $d_k$. This sort of answers your second question. 

Now construct the Turing machine $S$ that takes a Turing machine $M$ as input and does the following (in pseudocode): 

Not knowing anything about the input problem, I suspect that if the blow up is coming from quantifier elimination of Presburger formulas. Cooper's algorithm can introduce a lot of redundancies and duplication during case splitting. My suggestion comes from a trivial observation: Any tool that can find "very simple" Presburger formula also has to be able to find "very simple" pure boolean formulas. (Just encode each propositional variable $x_i$ with some trivial Presburger atom $s_i < 2$.) BDDs are a fairly natural candidate for trying to get simple formulas (or at least unique wrt a variable order). The closest work I am aware of to extending BDDs over Presburger is the Linear Decision Diagram (LDD) work of Chaki et al: slides and the project's homepage. Links to the papers are on the project's site. This work combines linear real/rational arithmetic with BDDs. The goal of this work was to have a compact representation in order to do Fourier-Motzkin quantifier elimination. Already LDDs do not try to be canonical, just "canonical enough". The main idea is to add local conditions to eliminate certain kinds of redundancy during construction. These local conditions are things like: put the constraints on $x$ next to each other in the variable order, if $x < 5$, then $x<7$ always holds on the high branch, etc. They do mention support for UTVPI constraints over integers in the paper and slides (like what you have above). The tool's API does not seem to support divisibility constraints coming from quantifier elimination or other more elaborate kinds of integer specific reasoning (gcd computations, conversion of $x < 2$ to $x \leq 1$, etc.) If the formulas have a lot of propositional redundancy introduced by quantifier elimination, this might be a reasonable thing to look into. (I doubt this includes all of the reasoning you used to get down to the smaller formulas so it might not work out of the box.) I've used tricks like BDDs + unate implications (eg. $x < 2 \implies x < 3$) to reduce Presburger formulas that were blown up by a different algorithm to "very simple" equivalent ones. But I did this for debugging purposes only. It has never made sense to stick what I did into a real implementation so I do not have a tool I can point you to. Good luck.