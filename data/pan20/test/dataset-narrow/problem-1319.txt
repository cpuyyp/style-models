The key distinction is that of who the 'customer' is. For gameplay programmers, it's the design team - their task is to make the design into a reality. For engine programmers, it's generally other programmers. Engine programmers make technology and systems, which gameplay programmers and content creators can then use to realise the design. Engine programmers write code which is largely game-agnostic - the same technology can often be used in multiple games. Gameplay programmers tend to write code which is very specific to the game being made. And there's always overlap - sometimes to implement the gameplay you need some new engine code. Gameplay coding isn't any harder or easier than engine coding. It is however definitely a slightly different skill-set; you often need to be more creative, and be able to bodge solutions. Your goal is not to create the perfect re-usable system (the ideal for engine programmers), but to create the best game implementation. So what can be an asset in engine programming (attention to detail and well engineered structures) can be a hindrance in gameplay programming (where adaptability and iteration time are more important). A good gameplay programmer knows when to do it right (when the design is solid and the code will be used in many places), and when to do it quickly (when the design is still being worked out, and your code has a good chance of being discarded once tested). Most of all, gameplay programmers need to be able to collaborate with the design team on a level that engine programmers do not. Aside from the obvious communication skills, that means you need to be familiar with other game designs much more than engine programmers do, so that when a designer says "make it work like Zelda," you know what they're talking about. To learn to be an engine programmer, you'd drill down on a technology and learn how it works, and maybe implement your own version of it; the game would just be an arena in which to show the technology. To learn to be a gameplay programmer, you'd take someone else's engine, and use it in as many different ways as you can. In other words, if you want to be a gameplay programmer, make games, not technology. 

A straightforward cylindrical mapping should suit your purposes readily, if you look into the equation of a circle then you can come up with a 3D vector for any point on the pipe surface using all the data you have already, and your heightfield variation simply becomes variation of the radius parameter of the function. If visualisation is your goal, then most likely what you actually want is a shader which highlights deviations from the 'normal' plane of the pipe. Whether you do that in 2D or 3D isn't really relevant, but I can imagine a simple shader that colours in a heatmap style based on the normal of the surface at any given point would quickly let you pick out deviations in the surface. Basically you would calculate the 'ideal' normal of the surface at any given point (i.e. point directly out from the centre of the pipe). Then pick your output colour based on how much the actual normal deviates from that ideal normal. If it doesn't deviate at all, then colour it black, if it deviates along the horizontal axis then raise the R component of the output colour, if it deviates along the vertical axis then raise the B component. Scale that appropriately, and you should see dents picked out as red/blue areas on the surface, with brighter colours indicating larger / more severe dents. 

Sounds like you are looking for something like a sliding window effect, centred around the camera position. I'm going to gloss over how you're spawning the spheres initially, because that will very much depend on how your scene is setup, but rather than destroying and creating spheres it may be more efficient to simply move them to where you want them to be. Here's some off-the-cuff C# code that should get you started. It assumes that you have a list of which is pre-populated with a bunch of GameObjects that you're going to render. Also key is knowing where the camera moved from to where it is now (because you need that information to decide where you're going to spawn the new spheres. 

C++ is a C superset, not a C subset If you're teaming with inexperienced programmers that don't understand inheritance, then it doesn't really matter what language you choose, they're still going to have problems. Compilers can do an amazing job with optimisation. Chances are, apart from a few edge cases, you'll have a hard job writing faster assembly language versions of your functions. And even if you do, it'll take you far longer than writing it in C/C++. Trust that the compiler writers know what they are doing, and worry more about whether or not your team knows what it is doing. 

Lack of time and expected reward. There's a lot more to skinning than just adding vertex / bone weights to a geometry format, there's not much point in having that unless you also have the skeleton definition. And that's a lot more work, even for a basic implementation. Not to mention, none of the tools which export to OBJ format will support it. Why bother, when you already have other animation formats already defined, formats which are much better suited to storing animation data? There's nothing stopping you or anyone else from extending the format yourself, both on the export and import sides. But an organised extension to the spec would be work for whomever took it on. Better to treat OBJ as what it is: a static geometry format. 

All 3 platform holders provide forums / newsgroups which are strictly for registered (licensed) developers only. In general they provide a second tier of support for developers, allowing folks to share problems and solutions in a shared forum with the developer support teams also read. It is a specific part of the licence agreement that the development SDKs and any information about them is not shared outwith the developer or those secure shared forums/newsgroups. XNA on X360 development and PS3 Linux are allowed by Microsoft and Sony. Anything else (Wii, PSP, etc.) is home-brew only and not sanctioned (since it involves hacking the hardware); although that's not to say there aren't plenty of forums out there for that. 

My bet would be that your performance is not bound by GPU operations, and that hardware acceleration makes no difference because the amount of time spent doing the GPU work is dwarfed by 'something else'; be that marshalling on the driver side, something in the OpenGL libs itself, or even possibly your own code. The trouble is, figuring out what overhead is getting you isn't straightforward unless you know what's going on under the hood. That diagnosis would be consistent with your reply to mh01's answer; increasing the number of draw calls to render the same primitives adds yet more overhead, while keeping the number of primitives drawn the same. Have you tried making the task less complex to see if there's one element of the problem that causes a massive drop in performance? E.g. does disabling textures but rendering the same amount of primitives work fine? My advice would be to dig out a sampling profiler (something that breaks into your process every X microseconds and grabs the callstack), and see which module it's spending all this time in. 

All credit to API-Beast for this, but since they didn't add it as an answer and this question remains technically unanswered... Check out this link for a blender script to actually output the individual frames. That's doing just a rotation, but fundamentally it's the same process to iterate over multiple frames of animation and output a snapshot render taken of each frame. 

That should extend to 3D as well, except instead of a boundary ring, you'd have a boundary surface. So when you are trying to determine whether the two recently disconnected voxels are still connected, you can exclude all the internal voxels from your traversal, because by definition if a voxel is connected to any one of the boundary voxels of a group, it is also connected to all the internal voxels in that group. It's sort of the reverse effect of the hub voxels I talked about in my answer to the other question - you don't have to find your way from every voxel to every other voxel, you only have to find your way to the interesting voxels. 

How you detect events can either be with the animation code calling you back (i.e. when it detects a new event because the animation has played to a certain frame, it calls your game code to tell it about the new event), or by polling the animation like so: 

I'm going to leave my other answer in place, because I think it's more valuable general advice, and people can consider it separately. Looking in more detail at your code raises the following questions, all of which need answered before you'll get working code. 

Gdx.graphics.getDeltaTime() will return a float, and it will likely be a very small float. At 60fps, the result of that equation will be somewhere near 6.6666666... But if offset is an integer, then it will either increase by 7 units each update. That .6666666 will get thrown away, and your background will move 0.3333333 further than it should have done. Over time that error will accumulate. If the frame rate was perfectly constant, you might not notice, because it would always be 6, or always 7, and would appear perfectly constant as well, even though it's actually moving slower or faster than it actually should be. If your framerate varies up to ~62 fps, then offset will suddenly start advancing by 6 every update instead of 7, a decrease in speed of almost ~15% even though the frame-rate has only increased by ~3%. This will manifest as a kind of jitter, moving slower or faster as the frame-rate varies. The obvious solution there is to make a float, so that any movement less than 1px is preserved, and when accumulated will give the correct rate of motion. That may not be the only extra granularity in your system though, but you've not given us enough information to diagnose any better than that. 

What you're seeing is probably made worse by the inaccuracy of the Bresenham traversal meaning that some cells appear blocked when they're actually not. I suspect what's really happening is that your visibility drop through a block isn't actually taking account of how much of a cell the ray covers. Bresenham's doesn't give you an anti-aliased line, it either hits a cell or it doesn't. But if the calculation was done at the proper resolution, you'd see that some cells have a long stretch of line passing through them, and other cells have only a short stretch (because the ray line just clips the corner of the cell). You need to make the visibility drop proportional to the length of the line in the cell with that degree of opacity. If the line just clips the corner of the 90% opaque cell, it should lose much less visibility than if it goes from corner to corner across the diagonal. To get that information you need the fractional portion of the ray traversal, i.e. you need to do the floating point math to calculate the line properly. Even then that won't fix your problem, because you're still calculating visibility at the centre of the cell rather than across the visible faces of your cell blocks, but it'll get you closer. If you don't want to do that, I think you can get a decent workaround that won't impact on your system too much. Every time a ray terminates in a grid cell due to loss of visibility, and the grid cell is not fully opaque, then mark the grid cells just beyond as visible, at the same visibility level as the ray was when it terminated. If you later visit one of those cells with a direct ray-trace and find they're more visible, then you use the higher visibility value. For 'just beyond', I mean one more in each direction. If the ray you're casting has any northward component, mark the cell to the north as visible, if it has any eastward component mark the cell to the east as visible, etc. What that will do is cause a bit of visibility bleeding, such that a visible partially opaque cell will cause others nearby to also become visible, at a higher level than they should properly get. 

No. Unless you are doing something very naive or brute force, the sorts of numbers you're talking about here will barely be noticeable. Even if your map format was some sort of ugly ASCII file or XML file that needed parsing, I'd be very surprised that loading would even show up as a blip in the profile. You most likely won't even notice loading time until you get up into the hundreds/thousands of tiles. 

The only inputs into this section of code are the screen width and the cameraOffset. As you continually run right, cameraOffset will increase. To begin with, will remain , and will increase. Once reaches the width of the first image it will reset to 0, and will increment. This pattern will repeat until you reach the right-hand edge of the last image in the set, at which point it will start again with 0 and 0. It can increase forever, and the modulus logic will make sure there are always images underneath the camera. NB: this code will work even if the backdrop images are smaller than one screen in width, because it will loop and draw as many images as it needs to, even if they repeat, until it has moved past the right screen edge. If the backdrop images are larger than the screen width, then this loop will at most go round twice (when the boundary between images is on-screen), and usually only once. 

Which should work, even for files under Program Files. NB: If you're just using the 'plain' versions of the functions, i.e. omitting the FileAccess.Read parameter, then the system will default to FileAccess.ReadWrite. You will not be allowed to open files using FileAccess.ReadWrite, if they live in the Program Files hierarchy. Even if you never write anything to those files, the open call will fail unless you ask for the right sort of access. 

I feel that you are mistaking use of the extended feature set of C++ (inheritance, classes, templates) with a) writing slow code, and b) writing hard to understand code. You can write hard to understand and slow code in C as well. You can write it in any language. Your choice of language does not automatically lead to good or bad code. C++ simply gives you more language features that might cause problems (i.e. enough rope to hang yourself with). It is a mistake to think that if you simply avoid using the features that make C++ different from C, that you will not encounter those problems. All that said, there are complex structures (multiple inheritance, complicated template code, and various other anti-patterns) which you absolutely should avoid using unless everyone on your team understands them. While they are possible, they are certainly not advised, both for performance and maintainability reasons. But you can write some incredibly clean, structured, and above all fast code using C++'s features, even when you are a programmer of only medium skill. The language is not what causes the problem. With any language, you should be aware of the consequences of using particular language features, both in terms of performance, and in terms of maintainability and readability. 

Since you can't guarantee anything about the type of the argument you're passing between the states, essentially what you're looking for is to serialize / de-serialize the object. Fortunately Programming in Lua covers this: Chapter 12 - Data Files and Persistence. Essentially you are converting your Lua object from one state into a stream of bytes, passing it to your C++ layer as a stream of bytes, then passing it to the second state still as a stream of bytes, and then de-serialising it into table/object form again on the other side. That's the only type-safe, generic way to transfer data of arbitrary types between lua_States, although if you have constraints on the types of data then you might be able to make assumptions about what sort of data you need to serialize. 

Your problem here is essentially that A* is an algorithm for finding the quickest route to a target. If that is your primary criteria for a 'good' path, then it's unsurprising that all your actors make the same decisions. What you need to do is modify your quality criteria for the path, so that 'shortest-is-best' isn't the only factor. The element of randomness is key in this, but not so much that it detracts from the path-finding intelligence (i.e. actors take stupidly roundabout paths to the target). A* pathfinding is innately naive, as it usually assumes that the actor has perfect knowledge of the entire route before it starts out. That's always going to look unrealistic. The solution suggested that picked intermediate goals is a step away from that - the AI is trying to get closer to the target, but only tries to navigate in small sections at a time (this is analogous to real life where you can only navigate as far as you can see, and as you traverse more of the path, you can see further ahead). I'd perhaps recommend a simpler way of looking at it. When you're pathfinding, don't just maintain a single best-path-I've-found-so-far. Instead, collect a set of the best 5 or 10 paths. Use a threshold to discard obvious outliers. E.g. if the best path traverses 20u to get to the target, the next best traverses 21u, and the next one after that traverses 50u. Set a threshold of 20% larger than the best path, and so discard the 50u path because it's stupidly longer. Now you have several paths to choose from, and by randomly selecting from that set of paths, your actors will make different decisions. However you won't get this sort of information with standard A* searching, so I think you'd have to modify the algorithm or use something else to gather the set of possible paths.