is quite good. It has a surprising number of great results that I have found especially helpful in research. 

$R^n$ "stands for" the fact that the (data) points to be classified lie in a $n$-dimensional Euclidean space. For example, points in our familiar $2$ dimensions (with, say, latitude and longitude coordinates) lie in $R^2$ -- an $n$-dimensional space generalizes this notion. A hyperplane (in $R^n$) is a flat of dimension $n-1$. It is what an SVN uses to classify the data (after they have been mapped to the higher dimensional space by the chosen Kernel). For example, if your space is $R^2$ (or a plane) then a hyperplane is line, and so on. How to transform the space? This is done via the Kernel function. A big part of SVM design is choosing a proper Kernel for the problem at hand. 

If no class is too simple, then here are some agnostically PAC learnable classes. In response to the comments, the classes with polynomially many hypotheses are crossed out: 

If TCS is something you enjoy, and you want to pursue a PhD, I wouldn't let the fear of failure stop you. You can also mitigate some of the risk by enrolling in a program that awards you a Master's after a year or two -- that way, if you realize research isn't for you, you won't come out "empty-handed". And if you don't succeed in landing a research job afterwards, as long as you enjoy grad school, it's not a big sunk cost. To answer your question about "having what it takes" more directly -- I think you need to both have talent and put in the hard work to succeed. My feeling is that most people who fail to get a PhD, however, fail not because they were not talented enough, but because they weren't disciplined enough. 

Just a quick point: knowing how to program gives me an additional tool in theory research. When I have an algorithm that I think will work, if it's easy enough, I might code it up and check if it actually does. If my idea doesn't (even) work in practice, it's not very likely to work in theory, and this approach often saves me from sinking an enormous amount of time trying to prove something that's false. 

So here is the right paper for my purposes: Joe Kilian, "A note on efficient zero-knowledge proofs and arguments." $URL$ To get the strongest result, we need to accept zero knowledge arguments rather than proofs (computationally bounded prover); these are what I am interested in but did not know the terminology. Assuming sufficient cryptographic assumptions, the paper gives zero knowledge arguments with total communication $O(b \log^c n \log (1/p))$ for $c = O(1)$. This result is tightened to $O(1)$ rounds by Ishai et al., "On Efficient Zero-Knowledge PCPs", $URL$ 

Unfortunately, this algorithm is $\Omega(n^2)$ even if no unnecessary vertices are generated. If the $S_i$ form a convex polygon inside $T$, the BSP tree is degenerate regardless of the order in which segments are inserted. 

Do Delaunay triangulations satisfy any extra topological conditions over normal planar triangulations? In other words, if $T$ is a topological triangulation of the plane, when does there exist points $x_i \in \mathbb{R}^2$ s.t. their Delaunay triangulation has topology $T$? 

Here's some weak empirical evidence that the answer is yes. Let $G_n$ be a random graph on the $2n+1 \times 2n+1$ lattice defined by picking each diagonal randomly. Here's a plot of reachability probability estimates vs. $n$ (ignoring the vertices which are always unreachable due to parity). If we rescale the square to $[0,1]^2$, the probability appears to converge to a smooth function independent of scale, which would mean even more: that the probability of the origin reaching infinity is positive. However, it's also possible that I haven't computed far enough out to see the downwards trend (the $n = 800$ plot does seem a little smaller than the others). Code here: $URL$ 

Update: This answer is obsoleted by my other answer, with fully polylogarithmic bounds from appropriate references. On second thought, there's no need to reveal the Merkle tree gradually, so the lower communication version needs no extra rounds. The communication steps are 

Given David's bound it's unlikely you can do better worst case, but there are better output sensitive algorithms. Specifically, if $m$ in the number of medians in the result, we can solve the problem in time $O(n \log m + m \log n)$. To do this, replace the balanced binary tree with a balanced binary tree consisting of only those elements that were medians in the past, plus two Fibonacci heaps in between each pair of previous medians (one for each direction), plus counts so that we can locate which Fibonacci heap contains a particular element in the order. Don't bother ever deleting elements. When we insert a new element, we can update our data structure in $O(\log m)$ time. If the new counts indicate that the median is in one of the Fibonacci heaps, it takes an additional $O(\log n)$ to pull the new median out. This $O(\log n)$ charge occurs only once per median. If there was a clean way to delete elements without damaging the nice Fibonacci heap complexity, we'd get down to $O(n \log m + m \log k)$, but I'm not sure if this is possible. 

The closest things I'm aware of in the literature are extensions of Chernoff bounds to negatively correlated random variables, e.g. see this or this. Formally, your condition could be satisfied without the negative correlation, but the idea is similar. Because your generalization isn't difficult to prove, it might be that nobody bothered writing it up. 

I have 3 answers, all giving somewhat different hardness results. Let $f: \{0,1\}^n \rightarrow \{0,1\}$ be some function. Answer 1 Given a decision tree $T$ computing $f$ and a number, it is NP-hard to tell if there exists a decision tree $T'$ computing $f$ of size at most that number. (Zantema and Bodlaender '00) Answer 2 Given a decision tree $T$ computing $f$, it is NP hard to approximate the smallest decision tree computing $f$ to any constant factor. (Sieling '08) Answer 3 Let $s$ be the size of the smallest decision tree computing $f$. Given a decision tree $T$ computing $f$, assuming $NP \subsetneq DTIME(2^{n^\epsilon})$ for some $\epsilon < 1$, one cannot find an equivalent decision tree $T'$ of size $s^k$ for any $k \ge 0$. I think that this stronger answer (relying on a weaker assumption) can be made from known results in the learning theory of Occam algorithms for decision trees, via the following argument: 

When $f$ is convex, even if it doesn't have a closed form, you can use search methods (on a bounded domain) to find a point as close as you'd like to the local minimum, which will also be the global minimum -- this will work for finding the minimum of the sum, as the sum of convex functions is also convex. There are many other better numerical methods with varying guarantees (depending on the properties of the function) for optimizing convex functions -- this book is a good (and free!) reference. 

Sanjeev Arora has a nice document for a grad course (for 1st year students) he taught called the "theorist's toolkit," which has a lot of the basic material a theory student should know. A lot of this stuff you can wait until grad school to learn, but it will give you a good idea of what you'll need to know and some of the prerequisites. 

If $HT(n)$ is the set of halting times of $n$-state Turing machines on a binary alphabet with empty initial tape, then $BB(n) = \max HT(n)$. What can we say about the second largest number in $HT(n)$? Call this $BB_2(n)$. $BB_2(n)$ is trivially uncomputable, since it lets one compute $BB(n)$: just wait for one more machine to halt. Naively, I would expect the gap $BB(n) - BB_2(n)$ to be "busy beaver-like", growing faster than any computable function. Is this provable? 

If randomness is in bounds, one rough idea would be to generate a bunch of "random monotonic signature" functions and use them to approximate the subset relation (a la Bloom filters). Unfortunately, I don't know how to make this into a practical algorithm, but here are some estimates that don't immediately prove the idea impossible. This is very far from a useful solution, but I'll write it out in case it helps. Assume for simplicity that the sets are all nearly the same size, say $|S| = s \pm O(1)$, and that $s = o(u)$. We can assume $1 \ll s$, otherwise we're done. Define $$\begin{aligned} q &= [s/2] \\ p &= \left[\frac{u \choose q}{s \choose q}\right] \end{aligned}$$ Note that $p \gg 1$. Here is the wildly impractical part. Randomly choose $p$ subsets $A_1, \ldots, A_p \subset U$ with replacement, each of size $q$, and define a function $f : 2^U \to \{0,1\}$ by $f(S) = 1$ iff $A_i \subset S$ for some $i$. With $S$ fixed and $A_i,f$ varying randomly, we have $$\begin{aligned} \Pr(f(S) = 0) &= \Pr(\forall i. A_i \not\subset S) \\ &= \Pr(A_1 \not\subset S)^p \\ &= \left(1 - {s \choose q}/{u \choose q}\right)^p \\ &= e^{-\Theta(1)} \end{aligned} $$ Since $f(S)$ is monotonic, $S \subset T$ implies $f(S) \le f(T)$. If $T \not\subset S$, fix some $t \in T-S$. The probability that $f$ detects $T \not\subset S$ is $$\begin{aligned} \Pr(f(S) = 0 < 1 = f(T)) &= \Pr(f(S) = 0) \Pr(f(T) = 1 | f(S) = 0) \\ &= e^{-\Theta(1)} \Pr(\exists i. A_i \subset T, A_i \cap T-S \ne 0 | f(S) = 0) \\ &= e^{-\Theta(1)} \Pr(\exists i. t \in A_i \subset T | f(S) = 0) \\ &\le e^{-\Theta(1)} \Pr(\exists i. t \in A_i \subset T) \\ &\approx e^{-\Theta(1)} p \Pr(t \in A_1 \subset T) \\ &\le e^{-\Theta(1)} p {s \choose q-1} / {u \choose q} \\ &\approx e^{-\Theta(1)} p \frac{q}{s-q} {s \choose q} / {u \choose q} \\ &= e^{-\Theta(1)} \end{aligned}$$ Some of those steps are pretty tenuous, but I don't have time to improve them tonight. In any case, if they all hold, then at least it's not clearly impossible to randomly generate signature functions that have reasonable likelihood of distinguishing subsets from nonsubsets. A logarithmic number of such functions would then distinguish all pairs correctly. If generating a signature function $f$ and computing $f(S)$ could be reduced to $\tilde{O}(n+u)$ time, the result would be an overall $\tilde{O}(n^2+u^2)$ algorithm. Even if the above calculations are correct, I have no idea how to generate monotonic signature functions with the desired features quickly. It's also likely that this technique doesn't extend to significantly different set sizes. 

Let me give a partial answer from a learning theory perspective. As your question isn't well specified, this answer won't be either. In my answer, I'm assuming your question was inspired by your blog post, linked from your profile. Say that you are thinking about programs that are just functions (so they have to halt, etc.). You can ask whether certain classes of such functions can appear randomly by, perhaps, looking at the probability a random program (from some distribution that you think is likely) lands in that class or not, with the hope that probability is polynomially large. I haven't really thought this argument through. You can also ask whether such a class is efficiently evolvable according to Valiant's model of evolution (also in @Artem's pointer in comments): luckily what is efficiently evolvable is known to be the class learnable by correlational statistical queries; taking "crossover" into account, you get parallel correlational statistical queries. One thing to note is that just because evolvability is characterized, it is still a separate and sometimes difficult task to determine whether a particular class is evolvable (learnable with CSQs) or not. If you find a class of "programs" that is neither randomly occurring nor evolvable, perhaps you can conclue it has a "creator/programmer," though that conclusion may still take a leap of faith. 

We know something close to what you want. If you look at Ke Yang's "Honest Statistical Queries" -- there is no noise at all, but only "sampling error". In this model, you pass in a parameter $t$, and the Oracle takes $t$ samples, honestly evaluates the passed-in function (onto {0,1}), and returns the average value of the function on the samples. In effect, this estimates the expected value of the function. It is easy to see that this Oracle samples from the Binomial distribution $B(t, E_{x \sim D}(f(x))$. For small skew and large $t$, this is approximated by the Normal. We extended this to explicitly consider variance here. The punchline, basically, is that SQ lower bounds still hold in this non-adversarial noise setting. Actually the SQ lower bounds are even harsher because they work for sample sizes $t=1$, where the algorithm sees the result of its function evaluation on every random sample (and can average or do anything by itself).