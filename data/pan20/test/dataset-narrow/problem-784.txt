You're on the right track with the cookie-based approach, but the initial check needs to be done using a process a lot cheaper than an Apache/PHP thread. I suggest an nginx proxy in front of your web host that forces redirecting and setting a cookie. Than only requests bearing the appropriate cookie are even allowed through to your PHP host. And as long as you're setting up a proxy, this relatively new piece of bot-detection software is pretty impressive: $URL$ I also highly suggest the DDoS presentation linked to from that page: $URL$ It covers anti-DDoS concepts overall and describes why they wrote Roboo. 

I'm a big fan of the relatively inexpensive KEMP LoadMaster series. They are full-featured load balancers with ASIC SSL offloading. They are non-OSS linux-based appliances. Support is outstanding and the feature set keeps improving, frequently in direct response to user requests. 

One approach would be slicing off the first character of the variable's content, using "Substring Expansion": 

If the problem is failure to resolve, then it pretty much has to be a problem with the DNS servers or configuration. It's possible, albeit unlikely, that Pingdom and the unnamed other service have broken caching resolvers. It's also possible, ableit unlikely, that theplanet has broken DNS servers. It seems more likely that the DNS zone for your domain might be mis-configured, resulting in random failures. Lame delegation from the GTLD servers, for example, can result in intermittent failure. If you post the actual domain, we could tell you quite a bit more. Some things to check: 

It's possible, but The Right Tool is expect. There are also expect-like libraries available for most scripting languages. 

I use thermite. Of course it's a little hard to donate them to charity, but they sure are thoroughly unreadable. 

I'm not aware of a way to tell if a directory has been indexed, but you can ensure that they all get indexed by running this after setting the option: 

The important column in your case isn't but and . It's normal and healthy for Linux to swap unused stuff out. If, however, and showed constant activity, that would imply that you did not have adequate resources for your usage pattern. As others have pointed out, that would mean you had either a leaky app or just not enough RAM. Frankly, I don't see anything disconcerting about the info you've posted. 

I think you're being too paranoid. User-driven password changes allow users to select passwords they find memorable, change passwords when they suspect their boyfriend / girlfriend / mother is spying on them, etc. There's no reason for a password-changing app to be a security hole. Separate the hell out of the UI and the back-end logic, perhaps writing a simple CLI tool to perform the database manipulation and calling it from the web app. This way your CLI tool can perform its own sanity checks on every request it receives. Some user/password databases have well-tested, reliable password changing mechanisms already, like OpenLDAP's Password Modify extended operation. Yeah, I recommend keeping webmail on a different host. I've managed several large email sites over the years and the only compromise I've experienced was when I failed to keep the webmail app up to date. Webmail apps are large, complex, and frequently not designed with an eye to security. 

Normally I'm a big advocate of rsync, but when transferring a single file for the first time, it doesn't seem to make much sense. If, however, you were re-transferring the file with only slight differences, rsync would be the clear winner. If you choose to use rsync anyway, I highly recommend running one end in mode to eliminate the performance-killing ssh tunnel. The man page describes this mode quite thoroughly. My recommendation? FTP or HTTP with servers and clients that support resuming interrupted downloads. Both protocols are fast and lightweight, avoiding the ssh-tunnel penalty. Apache + wget would be screaming fast. The netcat pipe trick would also work fine. Tar is not necessary when transferring a single large file. And the reason it doesn't notify you when it's done is because you didn't tell it to. Add a flag to the server side and it will behave exactly as you'd expect. 

If it's true server-grade hardware (not a re-purposed desktop) it will likely have built-in IPMI controls. For example, my Penguin Computing and Dell servers can all be rebooted or powered on/off remotely regardless of the state of the OS. 

Then executing will execute your debugging command as root but still preserve the benefits of sudo (audit trail, not operating out of a root shell). Example: 

If the Apache user does not have access, every directory in the path, starting from will need to have read and execute permissions either for the Apache user or for "other": 

You are correct to suspect that your $5 network tester is inadequate to the task. It's fine for verifying that your wire map is correct and that the you have connectivity, but it won't detect any of the other problems, like the many varieties of crosstalk. If you are fine with learning about problems as they occur, you're probably fine with the testing process you described. Another good test would be to push some actual traffic through the wire (iperf on both ends of the wire) and see if any of them run dramatically slower than the rest. The downside is that when some weird network issue occurs, you'll never be sure if it's your wiring or something farther up the stack. There are companies that lease test equipment. If you could get your hands on a good Fluke meter for a day, it would go a long way to identifying any of the more esoteric wiring problems. 

OS X server uses Postfix for mail. It's reasonably safe to back up a host while Postfix is running. After restoring the queue may be a little messed up. These queue problems can be repaired by running after restoring. In fact, is required after restoring even if you don't shut down Postfix. Postfix relies on certain files in its queue having a name that's the same as their inode number. Obviously, this can no longer be guaranteed after restoring from backup. Regarding other services: Hard to say without knowing what other services you're running. Databases, for example, frequently require special care when backing up. 

I have a RAID 1 + hot spare that I would like to reconstruct, online, into a RAID 5 set. I know this is possible with OpenManage, but I has anyone figured out how to pull this off using the MegaCli tools? 

If a file is deleted while another process holds it open, that process can continue to write and, eventually, invisibly fill the disk. As soon as the process holding open the file exits, the blocks are made available. Try evaluating each of the running daemons. If possible, restart them. If you can't figure it out, rebooting the box should clear it up. 

Our web nodes have an NFS cluster behind them. We symlink the default config file locations for both Apache and PHP to the shared copies on the the NFS share. A very simple shell script then does some basic housekeeping (rebuilding Apparmor rules, etc.) and then iterates over the web nodes triggering first an and then . Other ideas: 

If you substitute your office subnet for 192.168.0.*, users will be able to use passwords to connect, but only from the office subnet. You, however, will be able to use your SSH keypair to connect from anywhere. When an ssh client connects to the server, it is presented with a list of authentication mechanisms it can try. Typically the list is "publickey, password." In this case, when someone connects from outside, they are only presented with "publickey" so their client will not even attempt to send a password. If they attempt to authenticate with any mechanism other than an SSH public key, the connection is immediately shut down by the server. So no possibility exists for brute-forcing the passwords. 

Debian-based distributions leave a blank in . All of the actual configuration is in . Is that, perhaps, what you're seeing? In either case, many distributions keep example config files in . 

Basically you're globally allowing public key auth, globally disallowing password auth, and then specifically re-allowing password authentication for anyone in the 192.168.1.0/24 subnet. Edit: You probably already have the first three lines elsewhere in your config. If you do, they do not need to be added again. However the "Match" block must be at the end of the file. Also, the keyword is specific to SSH v1, which you don't allow, right? 

You seem to be confusing "qualification" with "resolution". Resolution is converting the name to an IP address. The client absolutely needs to do this. The only way to avoid this would be to create URLs with the IP address, like so: but that's generally a bad idea and, in many cases, will not work. Qualification is expanding to . This also technically needs to be done by the client. However, if all you're trying to avoid is typing the long name, you could use server side includes to expand something shorter into the full URL. Ugly hack, but doable. Probably the most logical workaround, if you absolutely can't stand the thought of typing the long URLs, would be to write a Makefile that runs your source HTML through sed to convert the short versions to long versions. 

Basically they're the same except when the variable is enclosed in double quotes. Then expands to a single word and expands to separate words. I don't see any merit to claims that is a security risk. From the bash man page: 

I am assuming that by "But the subdomains are not accessible" you mean they are not accessible remotely. If that is the case, then adding them to will not help, as only overrides local hostname lookups. If you want the subdomains to be accessible from the internet, you need to add them to the DNS for your domain name. Your hosting provider should be able to help you with that. 

The output of apachectl is sent to stderr. The commands you are using attempt to filter stdout. To use grep in the way you are describing, redirect stderr to stdout, like so: 

As I understand it, most of the recent development has been behind the OpenAFS project. I can't pretend to be familiar enough with the project to know if the "preferred locality" feature is available, but otherwise it sounds like a good fit. 

I still say LDAP is the easiest / best choice. Insanely reliable, very easy to maintain. I've been running master/slave LDAP pairs in production for several years now without a fault. At my previous job they provided authentication for 20 - 30 servers and hundreds of workstations. To the best of my knowledge, they never failed over as a result of a fault. When I would deliberately fail over (reboot / upgrade / etc) no one noticed. That being said, there is a solution that does almost exactly what you describe but with the advantage of being centrally managed: NIS. It distributes passwd, hosts, group, etc. via a a client-server protocol but is, as I understand it, fully capable of continuing to function if the server goes away. It's a little complex but is supported by every *nix-like OS that I can think of. 

On their box, they would run . Then they would configure their web browser to point to "localhost:4444" as the proxy server. The local ssh client would accept the proxy request, forward it through the ssh tunnel to your server, where it would exit out to the internet. 

I run a backup script on an off-site server every night. Since supports incremental dumps, it's pretty straightforward to make nightly incremental dumps and weekly/monthly full dumps. 

Enable mod_rewrite using whatever mechanism your distro prefers ( on Debian-based distros). Add these directives to the for example.com: 

After much wrangling, I wasn't able to get the xfsdump solution to work. So I did what I should've done in the first place: tar. 

We use OTPW for this. Simple implementation. Easy to replicate the password list. The system requests passwords by number, so no problems trying to keep the lists in sync. 

This enables b-tree hashes of directory index data, dramatically improving lookup time. Of course, it's possible your install already has this enabled. You can check by running this command and looking for in the output: 

Well, the base package list (assuming this is a desktop system) is built by the combination of the and packages. These are metapackages that exist solely to depend on other packages. The problem, of course, is that their dependency list is not the complete package list. The packages they depend on may have other dependencies. So... I would suggest playing around in . Perhaps try flagging all installed packages for removal, then specifically selecting your active kernel, ubuntu-minimal, and ubuntu-desktop to be installed. With a little finagling, you should be able to reach a point where the only packages flagged to be removed are the ones not required by the two metapackages. I haven't tried this. I haven't even experimented with it. You may hose your system by following my suggestions. Have fun! 

It does not reference a option other than to mention that historically it was used to request bzip compression. 

It then calls the script. So they both accomplish the same thing, except calling the script via just ensures that some environment variables in your shell don't screw up the script. 

I'm seeing the above mentioned SMTP error when a couple different senders attempt to send mail to accounts hosted locally on my server. My ideal solution would be to override the restrictions for just specific sender domains or hosts. I do recognize that their DNS is not set up properly. A previous discussion suggested this was triggered by the directive, which I removed. I still saw the same error, so eventually I commented out all directives, yet the rejections continue. This is the state of my section at that point: 

You can't use wildcards if you explicitly list the destinations in your Postfix config, but fortunately the option accepts table lookups. So try setting to a pcre map like this: 

Easiest solution would be to use mod_vhost_alias instead. It does not do exactly what you described, but it's very close. 

If you click through to the documentation for each of those commands, you will find the permissions required by each of them. Most require both SELECT and INSERT, some only require SELECT. So GRANT ALL should not be necessary. 

On Ubuntu/Debian, you can install the shell to restrict them to scp/sftp only. Just install the package and change their shell to : 

If your users are logging in using key-based authentication (and they should) you can add ``command="/usr/local/sbin/validate_svn'' as the first field in their public key in . In order for this to be safe, though, you'll have to disable password auth. Otherwise they can log in with a password (bypassing the forced command) and edit to remove the restriction. Add a "Match" stanza to the end of for the user(s) that should only be able to use svn. It should look something like this: 

The first time you connect it will authenticate you against "a" and open a persistent, backgrounded ssh tunnel. Subsequent calls to "s" will open almost instantaneously through the pre-authed tunnel. Works great. 

So if there is mail being injected for 30 different domains, it may try to deliver a message to all 30 of them at once, but never more than one at a time to and never faster than 1 per second to .