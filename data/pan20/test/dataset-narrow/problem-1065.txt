If you don't feel too bad about some guessing, let me offer an explanation. And I don't mean "someone set the value to nonsense", that's obviously always possible :) Unix time usually uses the number of seconds since 1970. Windows, on the other hand, uses 1601 as its starting year. So if we assume (and that's a big assumption!) that the problem is wrong conversion between the two times, we can imagine that the date that was supposed to be represented is actually sometime in 2011 (1970 + 41), which got incorrectly converted to 1640 (1601 + 41). EDIT: Actually, I made a mistake in the Windows starting year. It's possible that the actual creation time was in 2010, or that there was another error involved (off-by-one errors are pretty common in software :D). Given that this year happens to be another of the tracking dates associated with the file in question, I think it's a pretty plausible explanation :) 

First, this has nothing to do with RAM, really. We're talking about address space here - even if you only have 16 MiB of memory, you still have the full 32 bits of address space on a 32-bit CPU. This already answers your first question, really - at the time this was designed, real world PCs had nowhere near the full 4 GiB of memory; they were more in the range of 1-16 MiB of memory. The address space was, for all intents and purposes, free. Now, why 0xFFFFFFF0 exactly? The CPU doesn't know how much of the BIOS there is. Some BIOSes may only take a few kilobytes, while others may take full megabytes of memory - and I'm not even getting into the various optional RAMs. The CPU must be hardwired to some address to start on - there's noöne to configure the CPU. But this is only a mapping of the address space - the address is mapped directly into the BIOS ROM chip (yes, this means you don't get access to the full 4 GiB of RAM at this point if you do have that many - but that isn't anything special, many devices require their own range in address space). On a 32-bit CPU, this address gives you full 16 bytes to do the very basic initialization - which is enough to setup your segments and, if needed, address mode (remember, x86 boots in 16-bit real mode - the address space isn't flat) and do a jump to the real boot "procedure". At this point, you don't use RAM at all - it's all just mapped ROM. In fact, RAM isn't even ready to be used at this point - that's one of the jobs of the BIOS POST! Now, you might be thinking - how does a 16-bit real mode access the address 0xFFFFFFF0? Sure, there's segments, so you have 20-bit address space, but that still isn't good enough. Well, there's a trick to it - the 12 high bits of the address are set until you execute your first long jump, giving you access to the high address space (while rejecting access to anything lower than 0xFFF00000 - until you execute a long jump). All this are the things that are mostly hidden from programmers (not to mention users) on modern operating systems. You usually don't have any access to anything so low level - some things are already beyond salvage (you can't switch CPU modes willy-nilly), some are exclusively handled by the OS kernel. So a nicer view comes from old-school coding on MS DOS. Another typical example of device memory being directly mapped to address space is direct access to video memory. For example, if you wanted to write text to the display fast, you wrote directly to address (plus offset - in 80x25 text mode, this meant if my memory serves me right - two bytes per character, line by line). If you wanted to draw pixel-by-pixel, you used a graphics mode and the start address of (typically, 320x200 at 8 bits per pixel). Doing anything high-performance usually meant diving into device manuals, to figure out how to access them directly. This survives to this day - it's just hidden. On Windows, you can see the memory addresses mapped to devices in the Device manager - just open properties of something like your network card, go to the Resources tab - all the Memory Range items are mappings from device memory to your main address space. And on 32-bit, you'll see that most of those devices are mapped above the 2 GiB (later 3 GiB) mark - again, to minimize conflicts with user-useable memory, though this is not really an issue with virtual memory (applications don't get anywhere near the real, hardware address space - they have their own virtualized chunk of memory, which might be mapped to RAM, ROM, devices or the page file, for example). As for the stack, well, it should help to understand that by default, stack grows from the top. So if you do a , the new stack pointer will be at - in other words, you're not trying to write to the BIOS init address :) Which of course means that the BIOS init routines can use the stack safely, before remapping it somewhere more useful. In old-school programming, before paging became the de facto default, the stack usually started on the end of RAM, and "stack overflow" happened when you started overwriting your application memory. Memory protection changed a lot of this, but in general, it maintains backwards compatibility as much as possible - note how even the most modern x86-64 CPU can still boot MS DOS 5 - or how Windows can still run many DOS applications that have no idea about paging. 

There's a lot of layers to this. And importantly, many of them are interchangeable. For example, you can have a coax-cable network, an ethernet, or a Wi-Fi down at the physical level. HTTP works on top of all of those, but each of them has a slightly different handling of the payload being sent around. HTTP works on top of another protocol, called TCP, which in turn more or less runs on top of yet another protocol, called IP (nowadays mostly in two variants - IPv4 and IPv6). So the HTTP server registers an IP address (like , or most often "any"), along with a TCP port ( being the default for HTTP, but in general anything from to ), with the operating system. Now, the HTTP server tells the OS to ping it when data (or another message) comes. The OS knows when that happens, because the network interface card driver tells it that. And the NIC driver is told by the NIC itself, which actually has its own software to interpret the electrical signals on the network cable (or the wireless signals in the air etc., you get the idea). Side note: If you want to know more about how the NIC can initiate communication with the driver / OS, you might want to lookup some basic info on hardware interrupts - basically, whatever the CPU is currently doing is stopped, and the program flow switches to an interrupt handler routine - an extremely simple piece of code that takes care of notifying the system, and then immediately returns control back to the original thing the CPU was doing. In fact, it might answer you a lot of questions about the inner workings of the OS and the computer itself - like how an operating system can "steal" CPU from running applications and shuffle the CPU resources between different applications running at the same time, even if they do not coöperate. Back to business: In your manual telephone analogy, imagine that your phone doesn't actually ring. To know if you're having a phone call attempt, you'll have to look at the screen periodically and check. To make this easier to manage for the HTTP server (since there's already quite a few layers who do that periodical check), you can actually block on the check attempt. So instead of checking, seeing there's nothing there and checking again, you basically keep looking at the screen all the time. However, you've basically got a whole separate system for handling this (in your case, the hearing center, which checks the air vibrations for useful information, the ring), so it doesn't actually require your attention (CPU time). This is further improved by techniques that allow you to monitor many connections at once (IOCP). This gets closer and closer to the phone ring system - you have a room with ten thousand phones, but you only care about those that are ringing at the moment, the others aren't taking any of your attention. 

Recent updates to Windows 10 brought back Windows 8's feature where the window/UI color was derived from the current wallpaper: "Automatically pick an accent colour from my background": 

I have a pie chart in Excel, my data labels are overlaid directly over each slice in the pie - but I want my labels outside the area of the pie (but within the general chart area). I can drag them individually, but I remember that older versions (2003?) had the labels like how I want them, but 2013 doesn't. Is there a GUI function or button for this, or a VBA feature? 

I have a Synology NAS at home, which has a web-based user-interface which I expose to the public Internet. Inside my LAN my NAS has the hostname and on the public Internet I have the domain name pointing to my home router (which then forwards port 7000 to my NAS' HTTPS server on port 443). My NAS comes with a self-signed certificate which isn't really fit for purpose, I would like to install an SNI certificate that supports both the hostnames in addition to - however all of the major CAs perform some form of domain-validation (as they should!) but they can't verify my domain name so they wouldn't issue that certificate. I don't want to issue my own certificate as, for expediency in use, I would like browsers to trust the certificate. 

From a developer's perspective, no. When you use the DWM you take advantage of the composted desktop which means application windows don't have to repaint as much, which has certain performance benefits. Under Windows XP many GDI/GDI+ operations are hardware accelerated, such as BitBlt, filling in shapes, drawing text, etc - however Windows Vista removed most acceleration, Windows 7 brought some acceleration back, but not as much as XP's had - however this setting is (I don't believe) controlled by the System control panel. God developer/blogger Raymond Chen wrote a good article about this setting, actually, and how it's largely irrelevant today: $URL$ 

Observe how, for the top 3 processors, as the core-count increases, the CPU frequency decreases. While the L3 Cache increases - as it's shared between all cores it works out at 2.5MB/core. Most of my computing needs are not many-core optimized, such as games - I don't do things like video-encoding, rendering, or bitcoin-mining - which means whatever is in cache would be suited to however many cores are being utilized, so the larger L3 cache would definitely help (and so be effectively larger than 2.5MB/core). So my question is: if I got the i7-6850K, would the extra 600Mhz (300Mhz in turbo-boost) in single-threaded performance offset the smaller L3 cache compared to the i7-6950X? But contrarywise: would the 500Mhz turbo-boost in the 6950X be in-effect if I was running low-threaded programs and augment the larger L3 cache? To be sure for certain, I'd have to wait for the Broadwell-E benchmarks to come out - though ideally I'd like to get my pre-order in first. 

(This script uses instead of because when runs, my NVMe drive is remapped to , but when I run the commands interactively instead it's mapped to . I don't know why or how this happens). When I boot up and let the shell execute I get this output: 

I have a table in an Excel worksheet which logs my incoming invoices and outgoing expenses - and estimates the tax I owe the IRS. It looks like this: 

I want to get the actual Date and Currency values from this table into a format that Excel understands (note that my computer is set to "English (United Kingdom)" and I custom-set my Windows Short Date format to ) and my default Excel currency is £ GBP. I create a new column in Excel, , and set the cells to and a Currency column set to . However for both columns, D and E, Excel gives me the error (because it won't parse as a date as it expects , nor as it expects - so I need to get it to ignore both the initial currency symbol and the trailing 3-character name ) I don't want to have to write an ugly string-parsing function in VBA (or worse, Excel formula syntax) to convert into or into , but is there any other way? 

You're running Windows 10 Enterprise. This is relevant because the Spotlight Group Policy settings only work on Enterprise and Education editions, so it's possible you have a policy set that disables it. 

These are the formulas I have right now, so I can compute fine, but I don't know how to compute or on a monthly basis because only lets me do criteria on a single column (in this case, the column, but I want to only sum rows for and for ): 

That's a lot of HDDs. My problem is that my job is in a country with a bit of a repressive regime and there is no uncertainty in my mind that any computers in my company-provided air-freight crate will have their HDDs examined, so I've decided to remove all HDDs from my computers before they go into shipping (except for my laptop, which will be in my carry-on). This leaves at least 8 3.5" drives, roughly 5TB of data, that I need to sneak into the country. I'm paranoid about my data because it's almost everything to me, including my personal diary, and I also frequent on sites like 4chan and Fark so I have a lot of "unsavoury" (but far from illegal!) content on my drives that I'd rather not part with. I think I also have stuff like The Anarchist Cookbook on my drives too - I'd rather not answer questions about those, of course. One possibility I thought of is consolidating as much data onto a single drive, for example I could mount my desktop computer's HDDs on my server and copy their contents onto a huge 2TB drive and leave only the System HDD (which contains no confidential or personal information). I could do the same with the server. I could wipe the DPM backup drive and leave it in the server, which means I'd only need to smuggle two 3.5" drives: my Exchange VM server's dedicated HDD, and "the drive with everything on it" - all of the drives left in shipping are just system disks. I could fit the 3.5" drives into fairly compact external HDD caddies that go with my laptop. Obviously I'd then make another backup of that "one drive" and leave it with a trusted friend at home. Does that sound like a sound strategy to everyone?