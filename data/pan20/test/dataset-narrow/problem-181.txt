Fortinet has an ALG (proxy) for SIP (a VoIP protocol) since this protocol uses dynamic ports (like FTP) and thus need to be handled in a specific way. Port 5060 is the default port for SIP. This means that you are connecting to the SIP ALG on the Fortinet Firewall. And, since this is an application level gateway and not only some packet filter rules it will first accept the TCP connection before it tries to forward the connection to the original target system. Therefore you get a full TCP handshake between your system and the Firewall even though the final target does not listen to port 5060. 

Because it the ACK to the FIN which be definition should be the last sequence from the data plus one to distinguish it from an ACK to the last data. 

The pcap-ng format just describes the container format which encapsulates the captured packets. It does not describe the content of the captured packet. Thus if you have captured an IP packet you need to refer to the specification of IP to get the source and destination IP address. And if the captured data also include the ethernet frame you have to remove this first before you get to the IP part. If you want to get to the TCP ports you have to understand the TCP header format in order to find where the port numbers are in the TCP header inside the IP packet etc. None of this is part of pcap or pcap-ng. 

Xmas "attack" is a XMas Tree Packet, aka a TCP packet with every flag set. It makes no sense to set all flags, and systems back in the '90s used to crash when they see one. That's what it's called an "attacK". These days they are used for OS fingerprinting as different operating systems respond differently to nonsense packets. 

If you need to electrically disconnect a port via command line (doing electrically the same thing as physically removing the cable from the port) you'll need a Layer 1 switch, which is a very expensive device (since it can connect any port to any other port). If you only need to stop the port communicating, then you might be able to do it with a cheaper managed switch by issuing interface disable/enable commands. Depending on what you're doing, you could also manufacture a device that does it, for example by putting 8 Mosfets between the pins of two outlets, and then trigger those with the I/O of a Raspberry Pi. This won't satisfy the requirements for Ethernet when it comes to shielding and frequency attenuation, but for test purposes it might work. If you're only using FastEthernet, then you can get away with only doing it on 2 pairs of the cable as well. 

You can use the Axis IP Utility to find the camera on the local network, even if the IP address is wrong for that network. You can then use it to change the IP to the right setting. The tool can be downloaded from here $URL$ Also, as Ron Trunk said, Wireshark can help you find out what IP is configured in it by looking at the traffic going over the network, even though the IP might be wrong it'll still send some traffic. The MAC address on Axis cameras is the same as the serial number, just add : in between every 2 numbers and letters. The serial number is printed on a small silver sticker on the camera, on the bottom on that model if I remember correctly. You can also set the IP using ARP through the command line if you prefer, but you have to know the MAC (aka. serial number) of the camera to do so. It's exactly the same thing as the IP Utility does, but it might be useful in a pinch. See the cameras manual on how to do that, or just google "Axis ARP" or something along those lines. Finally, you could reset it using the reset button on the camera, this will reset the camera to DHCP, and allow you to get connectivity back. I've seen some cases where the camera jams itself somehow, and you can access it even if you know the IP, and you can't reset it using the IP utility either. Then a reset is the only way to get the camera to behave itself. 

This route-map will set the next hop for all permitted traffic, regardless of the routing table. (Use of the "default" keyword can get around this but doesn't sound like you need to) This means that your static route is probably not needed for this purpose, but you can leave it in there as a fallback in case BGP fails. FYI - the way I did this was to enter the entire configuration, live, during the day, but with no entries in the access list. Since nothing matches, obviously, all traffic behaves normally, following the routing table. This provides for a very easy, testable, reversible framework for switching internal subnets over, since all it takes from this point is just to add them to the ACL. 

I would recommend that you look into First Hop Redundancy Protocols like HSRP or VRRP. Actually, having two gateways can be a very good network design, because if a router were to fail, the other router can take over somewhat seamlessly. However, as you're aware, it's not easy to make this transition if you have to make a manual reconfiguration of each client on a subnet. Protocols like HSRP (or VRRP if you have non-Cisco gear) allow you to have two (or more) routers (or L3 switches) on a subnet share a single IP address. You'll have your first router with an address of .2, the seond with an address of .3, and a "virtual IP address" of .1 that both routers are aware of through configuration. When the primary router fails, the secondary is able to detect this and take over the virtual IP address, meaning that your clients just need to have .1 configured as their gateway and you're good to go. In terms of routing design, that would largely depend on the current setup. It's possible that both gateways lead to the same internet edge, in which case you may not have a problem. Asymmetric routing can be bad, mainly because you risk packets being delivered in the wrong order, but again, depends greatly on the topology you're talking about. Lots of design principles implied in what I just said. I suggest you research both protocols, and determine what's best for your environment. If you're using Cisco gear, HSRP is a widely used and well understood method of solving this problem. 

This is mostly a terminology question, so here's my 2 cents. Purpose-built routers do not have NICs, they have physical ports/interfaces, which can be used for many different applications, and internal (logical/virtual) interfaces that can be defined in software. Servers and VMs however have NICs, they are what connects the server/VM to a switch/router. 

It's a trade-off made by the provider; Say that this potential server is connected to the Internet with a 100 Mbps link. Now, if you were to download the video, which is 100 seconds long, and each second is 1 Mb of video data, it would take 1 second to download the whole thing. But, during that time you either consume all the bandwidth available, or you can't download it with "maximum speed". Aka. you'll consume lots of bandwidth for a short period of time. Now, if you on the other hand stream it, you can at the most watch 1 Mb of video every second. Therefore, the server only has to provide that amount of data, plus some extra for buffering, for you to be able to watch it live. Therefore, ignoring a lot of real world problems, say that you need 2 Mbps to get a reliable live stream, then 50 people can watch the same video at the same time with the same available resources. Net result is the same, you get your video just as fast, but for the provider it's much more manageable to throttle connections so that the video buffers "fast enough" since you get a much smoother load on your network. YouTube and others probably do this on a quite grand scale so that they buffer fast enough for you to be content, but slow enough so that they can serve as many users simultaneously as possible. 

A better term would be "port mirroring". This concept is generally universal among vendor products. (Cisco would call theirs SPAN, but it's essentially the same thing). You take a range of ports or VLANs, and simply copy all traffic to a destination port, where you'd hook up a PC with wireshark. Try this thread - it's not a complete exhaustive walkthrough but it will point you in the right direction. Regardless, searching for terms like "HP Port Mirroring" should yield better results. 

Whenever you're configuring a FEX switchport, you must ensure that the configuration is consistent across both parent switches (i.e. Nexus 5K). You can do this manually or use the config-sync feature to simplify your configuration. For instance: 

This implies that - while meant to only be a transitory state, I'm sure - it is at least possible to form an HA pair between a 1000v VSB on a 1010 appliance and the ESXi-hosted VSM. Since you're using the newer 1110 model, I'd refer to the new guide, but the aforementioned statement has been changed to only reflect migrations from the 1010 to the 1110, and does not include a migration directly from the hypervisor to the 1110: 

I will say this much - I wouldn't recommend mixing the two permanently (i.e. not as part of a transitory migration state). Technical issues aside, you're undoubtedly going to get an earful every time you call TAC. If there's budgetary constraints that prevent you from getting a second physical appliance, just go all-virtual. 

Because it is easier to understand the output then, i.e. one sees that this packet continues after (relative) sequence number 1 and goes to 205, i.e. length 204. 

If only a simple predictable function is used to compute the servers ISN from the clients SYN then the attacker might create the final ACK without seeing the servers actual SYN. In this case the attacker could IP spoof not only the SYN but also the ACK and would thus cause the allocation of even more resources on the server. 

If the port is closed since no listener is on this port at the server the OS kernel will not be able to find a listening socket for the arriving packet and will discard it. Given that the port is involved to determine if something is listening but no application data are transferred yet (initial SYN from client) the rejection is done at the transport layer (TCP). If the initial SYN gets discarded or rejected by the server the initial TCP handshake will fail on the client side, i.e. no TCP connection will be established. Therefore no application data will be sent and the failure is propagated at the transport layer too, i.e. the connect will fail. 

When just sending a SYN to allocate resources on the server the attacker can spoof the IP address and thus hide its real IP address. To create a full handshake the attacker cannot use IP spoofing because the final ACK must be in response to the servers SYN which the client does not get if the original SYN was IP spoofed. 

Your design is valid, but you need to carry the management VLAN1080 as well to all switches. As your network isn't that huge, I'd just tag out every single VLAN to every switch (which is how Cisco's VTP works by default by the way) as you'll only see a limited amount of extra broadcast traffic that way. The reason you can't get RSTP to work is that you have enabled almost every single feature of STP on every port. Disable Root Guard, TNC Guard, BPDU Protect and especially BPDU filter on all ports, as these are meant to be used for very specific purposes (read up on what these do before you enable them on select ports). BPDU filter is likely the problem, as it basically tells the switch to not send BPDU (special packets that's used to determine the STP topology) or process the ones it recieves on that port, essentially disabling STP alltogether. It's basically a way to tell the switch to "what I'm going to do looks stupid, but run with it regardless.) You should also tweak the priority of your root switches to make sure that the spanning tree builds it's topology from the root switches instead of from some random switch with the lowest MAC address. Please read through my guide on STP on HP switches, it's meant for the higher end HP Procurve models, but it goes through some core concepts of STP that are useful for you. $URL$ 

Honestly if you're concerned with best practices and keeping your LSDBs nice and neat, ready for fast convergence, I would recommend ensuring that your OSPF areas are constructed well. Keep LSAs that don't need to be in an area summarized, and don't let a single area grow too large. 

NBMA networks require a special OSPF network type on the connected routers' interface. This setting affects quite a few things, including the need for a Designated Router, and the timers like hello timers and dead timers. I would recommend researching OSPF network types, as the question as you've stated is way too broad to be answered on this site. You should read the OSPF RFC, or even vendor-specific books. On this topic, Cisco Press has a few resources, namely Routing TCP/IP Volume 1. 

To my knowledge, the ASA platform is only able to classify based on existing DSCP markings, not apply or change them. It does preserve existing markings, though. 

I found only one reference to this, and it was the software configuration guide for the Nexus 1010 appliance. Read through and you'll see the following statement: 

EDIT: I neglected to read the section titled "Migrating a VSM". I instead linked to "Migrating a VSB" which of course is an appliance-only concept. Refer here and you'll see a very promising, similar statement to before: 

I don't have a lot of experience with Brocade, but I do know on the FC side that it's not uncommon (i.e. Cisco Nexus, MDS) for Fibre Channel switches to require a re-activation of the active zoneset when zoning changes are made. A will show a new zone, for instance, but will not until the zoneset being referenced is actually activated. It's one of the only cases I'm aware of (at least on Cisco gear) where the output isn't actually live and running on the system yet. Though I wouldn't know of any technical reason for one or the other (pertaining to TCAM or switch logic) I do know that this provides for a little better handle on the changes that are being made. It is an extra step but it allows you to make the changes, show the changes, but not apply them until you've reviewed and verified that they're good. Zoning from the SAN world being a rough analogy to ACLs in the IP world. 

I'm having issues with a HP 8206zl switch not delivering PoE+ to a PoE+ device, a Meraki MR42 WAP to be exact. The WAP reports that it's operating in 802.03af fallback mode, which restricts it's capabilities. The port it's connected to is part of a HP J9534A 24p Gig-T PoE+ v2 zl Module. The switch says that the device requested class 4. Attached output of the PoE port info. EDIT 1: Here's the output of the PoE supply status on the switch, ample amounts of power available: 

A fat tree topology "ensures" bandwidth by utilising thicker links closer to the core or root, compared to a normal "skinny" tree where you have the same links everywhere. So for an example, imagine that you have a network with access, distribution and core layers. The 4 access switches each connect 4 hosts using 1 GigE and you're designing a fat tree. The access switches connect to 2 distribution switches which are connected to a single root switch. For a fat tree, designed 1:1, you'd make sure that the access <-> distribution trunks are 4 GigE, and the core <-> distribution trunks would need to be 8 GigE. As you can see, you're not ensuring anything, you're just making sure that along every path of the network, every host has it's own 1 GigE of bandwidth. Here's a good article if you want to learn more; $URL$