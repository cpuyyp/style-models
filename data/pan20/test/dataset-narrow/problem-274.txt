Finally, are you virtual or physical? You should be physical and dedicated for loaded database servers. 

If I understand correctly, I wouldn't use SSIS. This is invoked on demand or scheduled to shift data whereas I read this as "automatic shift" of data So, I'd consider replication or database mirroring and the let the DB engine move data for you: 

will invalidate any optimal use of an index (it isn't covering) even if hash is indexed. Your index scan is most likely on the clustered index because of this. Personally, I'd start with 

So, the ordering of values for the simple list of numbers makes no difference at all For the semi-join, there should be an index anyway because it is a type of JOIN. Note that the ORDER BY inside the subquery would give an error on SQL Server because it makes no sense (unless you use TOP). Not sure about MySQL but I suspect it will simply ignore it. 

@CoderAbsolute's answer gives you a good design for your tables. Since he or she did not go into detail about why this approach is better, I thought it was worth adding another answer. First of all, design your table structure in accordance with how your data fits together. Don't try to smoosh different types of things into one table, don't add several tables for the same kind of records. Try to 'normalize' - if a table will have the same info repeated many times, then move that info into a different table, and link it from the first table using a foreign key. You should be aware of what first normal form, second normal form and third normal form are. Many real-world databases do not match these standards completely, but being aware of what you should aim for will help you make a much cleaner design. I would say, don't worry about optimization until you've already got a correct design. First of all, you don't yet know how many entries you will have in your tables. Secondly, database engines are designed to make queries as fast as possible, even if you have a lot of entries. Don't second guess the developers of your database software. When you've figured out that you really do have a bottleneck, you should look first at indexing. We think of a database table as being something like an array. In reality, it could be stored as a bunch of 'blobs', one for each row, which might all have different locations on a disk, not necessarily in order. (This is not a super accurate technical description of DB internals. But it should give you a clearer picture of how the pieces fit together.) How does the database find all the blobs? It has a somewhere, a list of 'pointers' which tell it where to find each blob. So typically, finding every single row of a table is an efficient process. It goes through the list and records each one. Now, suppose that you most commonly retrieve all the photos for a given user. This might be a slow process, since it has to go through every single row of the table, and look at the field. In this case, you should add an Index on that field. An index is something like a lookup table, which allows the software to quickly find the location of all the rows with a given . So it doesn't have to check every row in turn. There are different kinds of indexes. Some are optimized for matching a particular value. This is probably the type you want for the column. Some are optimized for finding things greater or smaller than some value. This might make sense for timestamps. (Give me all the photos from the last month.) You can have an index on multiple columns if this is what you regularly query on, or even on some function of one or more columns. Some indexes mean that similar items are stored close to each other on disk, to take advantage of caching to retrieve them more quickly. You should familiarize yourself with the possibilities for your DBMS. Experimentation can also be very valuable here, since the exact speedups depend on your settings and also your hardware configuration. 

Jobs are DB agnostic: the steps that comprise a job may have a database context sysjobsteps has a database_name column 

A foreign key does not have an index by default. You have to create one. In this case, I'd suggest either of these. It depends on relative selectivity of the 2 columns 

Arguably there is no such concept as a backup strategy: you have a restore strategy because this determines how long until you are back in operation*. All strategies require a full backup to base any subsequent restores of differential and/or log backups. In practice, you can have a full backup from 6 months ago with 15 minute log backups: however you have to apply every log backup from the last full. As a random example, one scenario could be full weekly, differential daily, log 15 minutes. The backup interval determines how much data you'll lose in the worst case: 15 min log backups gives you a data loss between 1 second and 14 mins 59 secs, average 7.5 minutes. Is this acceptable? Log shipping is warm standby with manual failover: it isn't backup but a high availability option. 

The only situation where I would not do it this way, was if 1 to 8 is something which could never ever change. For example, you might be sure that there will only ever by 7 days of the week (but who knows?). In this case I would create a TYPE with this constraint. Then you can use the type in multiple tables. If the days of the week (or whatever) DOES change, you can change the TYPE. 

We are using Postgres 9.3 and making use of the JSON column which was new to that version. Version 9.4 added a LOT of utility functions for handling JSON, as well as the new JSONB column. You can see this by comparing $URL$ with $URL$ 9.4 has a function, which returns the type of a JSON object (as a string: 'string', 'number', 'array', 'object', etc.) I would like to know if there is any practical way in 9.3 of retrieving all rows which are scalars (or, equally useful for us, which are not objects). We would like the column of a given table to always be a JSON object. This means that we can do queries like: 

If you have locking problems then you have a problem with your code: it isn't the database engine It isn't a magic bullet You may add more problems 

Standard to Workgroup is a downgrade This is the BOL page on upgrades Unfortunately, you'll have to uninstall and reinstall. On the bright side, the master, msdb etc will simply restore into the "lower" version as long the version number is the same. 

"it depends" Using a table variable or temp table with requires overhead of creating populating this object However, if the you require multiple processing steps then this is small compared to querying the same data over and over, especially as the query gets more complex. Also, for multiple steps, using a table variable or temp table means working on the same set of data for all steps. Typically the underlying data usually changes as a result of other processes. Finally, have you tried both techniques and benchmarked for your real situation? I use both approaches depending on all of the above factors. Sometimes it doesn't matter of course... 

should catch most cases. However, unlike the column, columns store JSON in its original form, not in a canonical form. So this method doesn't exclude the possibility that there will be some weird JSON, for example starting with another space character, which won't be matched. 

This is a pain, as we have had strings written to this column some of the time, mainly due to errors. Is there any reasonable way of either filtering these out in a query, or identifying them all so that they can be removed in one go? 

that one day some other part number such as 9 would be acceptable. You can quickly and transparently add it to the table, rather than changing a bunch of constraints on different tables. that you might want to store some extra data alongside each part number, for example the name of the part. You can add a column to the table which you already have. 

Did you rebuild indexes and statistics after setting up the database on SQL Server 2008? It's mentioned on MSDN (see "Next steps") 

Why insert into tblusers directly at all? I always use staging tables. You can use SSIS of course for the same result at with greater complexity 

With no indexes or keys, you can't defragment it normally. You can add/drop a clustered index, or do something like this 

70,000 per 2 minutes isn't much: you can do that per second easily in SQL Server. Saying that, the question is otherwise too wide. The choice of edition doesn't really apply: your design and hardware setup matter more. Standard or Web should be enough. If you budget extends to a single RAID 5 volume, for example, then forget scaling up. Or you let nHibernate design your schema. 

Your colleague is an idiot. The solution won't be scalable, the UDF isn't concurrent (same reason as this). And how do you deal with multi-row inserts: this would require a UDF call per row And migrating to other RDBMS doesn't happen often in real life... you may as well not use SQL Server now and use sequences on Oracle and hope you don't migrate away. Edit: Your update states that moving data is for refreshing non-production databases. In that case, you ignore the identity columns when refreshing. You don't compromise your implementation to make non-prod loading easier. Or use temp tables to track the identity value changes. Or use processes: we refresh our test system every night from production which avoids the issue entirely. (And ensures our prod backup can be restored too)