Give it a Try !!! UPDATE 2013-08-06 16:57 EST If you are planning to do queries from the archive table, you need to get away from the ARCHIVE Storage Engine. Why? Again, according to the MySQL Documentation 

I have a very intricate solution based on the rowcount Here is the basic algorithm Run this line once 

As shown from the end of the error log, it performed a clean shutdown at 2:39 PM. MySQL came back up when Windows was started. 

It is not so much a deadlock in the traditional sense, but it safe to say that the option can get its loyalty displaced and lose it point-in-time window from the first victimized table and all other tables after it. SOLUTION Setup Master/Slave Replication 

ngram_key has 73 bytes 64 bytes for ngram (ROW_FORMAT=FIXED set varchar to char) 8 bytes for ngram_id 1 byte MyISAM internal delete flag 2 Index Entries for ngram_key = 64 bytes + 8 bytes = 72 bytes 47 million rows X 073 bytes per row = 3431 million bytes = 3.1954 GB 47 million rows X 072 bytes per row = 3384 million bytes = 3.1515 GB 47 million rows X 145 bytes per row = 6815 million bytes = 6.3469 GB 5 billion rows X 073 bytes per row = 365 billion bytes = 339.9327 GB 5 billion rows X 072 bytes per row = 360 billion bytes = 335.2761 GB 5 billion rows X 145 bytes per row = 725 billion bytes = 675.2088 GB 

Give it a Try !!! CAVEAT Since the default for innodb_file_per_table is ON, each InnoDB table being optimized will actually have its file shrunk. Any MyISAM tables (which I hope you don't have) will also have their and files shrunk as well. UPDATE 2012-09-18 18:19 EDT I just tried the following: 

Since Amazon RDS does not give away SUPER privilege, that's the only way to change a GLOBAL static option. UPDATE 2012-07-27 16:34 EDT Without SUPER Privilege, you can modify a DB Parameter Group for a given MySQL RDS Instance, and launch a reboot with one of the following: 

The IO Thread is what communicates with the Master. Killing the IO Thread on the Master Side using the command would abort the IO Thread on each Slave. That could corrupt the Clean Recording of Replication Coordinates. 

The script will convert all MyISAM tables in size order from the smallest to the biggest. Give it a Try !!! 

then goto . This needs to be in on both Master1 and Master2. If you did not, please add it and restart mysqld on both Master1 and Master2. STEP 02 Did you enable binary logging on Master2 ? You should have a line in Master1's like this 

Since mysqldumps were in alphabetical order by default, there might be some cases where the parent table of a foreign key relationship would appear after the child table rather than before. As a consequence, the mock table was created to have a parent to adopt the aclGroup table. Since a child cannot have more than one foreign key reference to the same parent on the same columns, then NDB is ignoring it and still trying to enforce Under the hood, this is what the NDB storage engine should be doing after 

It is no longer a case of EXPLAIN a query. If you can EXPLAIN the procedure, you can code. Obviously, if you can EXPLAIN a procedure faster, you can produce faster code. 

This will erase all binary logs and leave you with with 107 bytes. Next, if you comment it out like this 

If is an auto_increment column in the table, you may want to change the INSERT INTO command to exclude that column as follows: 

Under the hood, both of these commands will only check for the presence of .frm files in the currently selected database. In MySQLWorkbench, you must make sure you have a Default Schema selected. Otherwise, you should not get anything back. Here is what effectively happens from the mysql client's point-of-view: 

When it comes InnoDB and mysqldump, you can lower this number under two circumstances. CIRCUMSTANCE 1 : Set it permanently to 0 Just add this to my.ini: 

Since every column in the WHERE clause is mentioned as a eq_ref, aka equa-reference (using =), the index can be used to zero in on one column. Note this query : 

Here is an explanation on how works vs how works flushes all data buffers of a file to disk (before the system call returns). It resembles but is not required to update the metadata, such as access time. Applications that access databases or log files often write a tiny data fragment (e.g., one line in a log file) and then call immediately in order to ensure that the written data is physically stored on the harddisk. Unfortunately, will always initiate two write operations 

I would suspect you hit some weird bug when running the table in this instance. Why ? Look at the response to the signal 11 you got which I will itemize 

Experience 1 is virtually unavoidable. As for Experience #2, it could result in needed index pages being purged out of the MyISAM Key Cache in the face of more recent queries against other MyISAM tables. Those needed index pages more be brought back by querying the corresponding table. The two experiences togther could make for a slower-than-expected query on relatively small tables. However, you could minimize or neutralize any ill effects of swapping by assigning the index of by creating a dedicated MyISAM Key Cache. It will be a cache that will contain only index pages from . How do you do that ??? First, recall that you mentioned that the index usage for was 2.29MB. You can create that dedicated key cache with that size with a little headroom, say 2.5MB. Let us create that key cache like this: 

For more clarification on this, please see MySQL Documentation. BTW once you are done with the maintenance window for throttling user connections, simply set the values you changed back to zero(0) to remove the limits as follows: 

This might remove any need to access the table and search the index only. Now, in terms of the queries, both of them may improve with the new unique index. Creating the unique index also eliminates the need to insert and into the table because and would be the index anyway. Thus, the second query would not have to gore through the table to see if someone is a friend of someone else because another person initiated the friendship. That way, if the friendship is broken by just one person, there are no orphaned friendships that are one-sided (seems a lot like life these days, doesn't it?) Your first query looks like it would benefit more from the unique index. Even with millions of rows, locating friends using the indexes only would avoid touching the table. Still, since you did not present a UNION query, I would like to recommend a UNION query: 

So, it can be done. Just use to create hard links instead of symlinks. WOW !!! I learned something about MySQL for Windows. I doubt if Oracle will implement the and options since the default storage engine is now InnoDB. Notwithstanding, you create the empty table, move the .MYD and .MYI to different folders from the OS, create hard links, run and INSERT your data. Give it a Try, and let me know how it goes... 

This will import SQL from /device.sql into the fms database mysqldump only outputs and is not designed to input anything Doing this 

Then, add that line to in Master2 and restart mysql in Master2. Once log-slave-updates and log-bin have been enabled on both Master1 and Master2, you are ready to setup replication in the other direction STEP 03 On Master2 

The best way would be to enable binary logging on the Slave. Why ? With binary logging enabled on the Slave, each recorded in the binary logs comes with the server_id where the SQL statement originated. What if log-slave-updates was enabled ? If you have log-slave-updates enabled on the Slave, every from the Master comes with the Master's server_id. If any writes were executed on the Slave directly, you will have to separate the commands. EXAMPLE: suppose the Master's server_id is 10 and Slave's server_id is 20. Now, let's say you have these binary logs on the Slave: 

Please keep in mind that OPTIMIZE TABLE does not perform defragmentation. Internally, OPTIMIZE TABLE perform several operations (copying data to a temp file, recreate indexes, recompute index statistics). In fact, the example I have can be performed manually as shown. Example: If you optimize , you enter this command: 

The timestamps from these two coordinates helps you figure out . Knowing these things, here is what you can do: 

Here is one thing that caught my eye when you replied to my comment: The target table is InnoDB and you are using LOAD DATA INFILE. I see two issues ISSUE #1 : LOAD DATA INFILE While LOAD DATA INFILE can load InnoDB tables, that command can be tuned for loading MyYSAM tables. There is only one option to do this: bulk_insert_buffer_size. either setting to very large or setting it to zero to disable it. There is no synonymous provision for InnoDB. ISSUE #2 : InnoDB Storage Engine Let's take a look at the InnoDB Architecture 

The cleanest method I can think of is to examine only one table : If you can login to mysql at all, run this query: 

What really jumps at me is that 52% miss. It's possible your innodb_buffer_pool_size is just too small. Keep in mind that the InnoDB Buffer Pool caches data pages and index pages. How can you get a good size for it? Run this query: 

One of the silent killers of MySQL Connections is the MySQL Packet. Even the I/O Thread of MySQL Replication can be victimized by this. According to the MySQL Documentation 

It's not only table storage that is a consideration. If you use indexes where the int column is part of a compound key, you would naturally want the index pages as full as possible, this being the result of index entries being as small as possible. I would definitely expect to find that examining index entries in BTREE pages would be a little faster with smaller data types. However, any VARCHARs involved in index entries would offset (nullify) performance gains from using TINYINT over INT. Notwithstanding, if index entries have compound entries and all are integers, the smaller the integers are bytewise, the better and the faster. 

With that in mind, you would have to change myisam_data_pointer_size to 7 First, add this to under the group header 

This will rename the table, perform the defragmentation, and rename it back. You could do it in one line as well 

You are going to have to bite the bullet somewhere in this process. You should determine which databases need to be dumped first (smallest to largest): 

will perform a CHECKSUM TABLE against all tables on Master and Slave. You can configure it to do all databases on just specific ones. can be run on a Slave against any table. Using the --print and --sync-to-master options, you can see what SQL statements need to be executed on the Slave to have it perfectly match the Master. This tool does not work with table that lacks a PRIMARY KEY or UNIQUE KEY. I have used MAATKIT for years. I still do. I have not tried the Percona Toolkit yet, but I am sure it should be of the same quality as MAATKIT.