It was tempdb contention. Adding more tempdb data files and enabling trace flag 1118 solved the issue. 

One of the SQL Server 2008 R2 instances I administer is running on a two node Windows Cluster (Windows Server 2008 R2 Enterprise) When I run the DMV sys.dm_server_services, SQL Server Service Account shows as Y under the 'is_clustered' column. However, Agent Service Account is an 'N' for the same field 'is_clustered'. Is it something that I should be worried about and what might be causing this? Does this mean that Agent will not fail over? Thank you 

Please can anyone help me in resolving the deadlock of which details and screenshots are given below? Occasionally, two stored procedures are resulting in a deadlock. I was able to pin down the statements that caused the conflict. Only one table and one clustered and one non-clustered index is involved. For demonstration purposes, I renamed the table and indexes in a test environment. How can I avoid this deadlock? Thank you 

I'm experiencing high Signal Wait percent while CPU utilisation is very low at one of our servers. I have gone through numerous articles online about Signal Waits and understand that it is the time spent on the runnable queue however I'm still having difficult time to understand why CPU usage is staying so low (25%) while Signal Waits' percent is high (band of 75-90%.) If a session is waiting for an available CPU to run it, why is CPU utilised so low? 

Daily full backups start at 02:00 AM and takes around 1 hour to complete. Various metrics for the past 24 hours is as follows: 

We are on SQL Server 2008 R2 Standard Edition. Some tables are highly fragmented. I want to see if defragmentation will improve performance and is worth the effort and tables/indexes being locked during the process. Therefore I want to restore the full backup on a test environment and simulate a live environment. What is the best way to follow? How can I capture the events going on in a live environment for some period? What tools are available for this? Thank you 

To my knowledge no index method or statistics depend on the potential range of the type. They all depend on the actual values and their current min and max. (This also applies to columnar compression store) So this is not a reason for restricting your PK (and columns referencing it). The only difference it makes is if you have a combined index with many large rows, because when you create those Oracle will check the potential maximum length of each column to see if it fits into the page induced restriction for maximum bytes. You will hit in this case. But this is as DDL time not when you do DML. However that's not a very strong reason as you should stay away from operating with such a wide index which is close to the maximum. 

I am looking for information on recyclebin extents. When dropping a table its segments are renamed to 'BIN$...' system generated names and the extents are hidden. Will those extents still stop me from shrinking a data file (or tablespace)? If so I would like to account for them in the script. Currently I am using a query inspired by AskTom's maxshrink.sql: 

Check the management package $URL$ It will describe procedures to,clean up the audit entries according to your likings. 

You can use the table or view) which is a fixed table driven by the XML alert logs. This way you can use a SQL statement to filter the entries. Here are a few examples: $URL$ And just for completeness the Enterprise Manager offer HTML views and alerts. 

You either login as user Inventory before running the DDL, or you create the table by specifying the Schema like 

When using Oracle BasicFile and SecureFile one property of the implementation is that each LOB occupies multiples of the chunk size (and in case of SecureFile the smallest chunk size is the block size). This does luckily not happen for data which fits inline in the row, however for that data I could use a normal type. So my concern is more with data stored in the lobsegments - there the overhead can be up to a whole database block (per row) if the LOBs have unlucky sizes. Does this change when compression is turned on? I mean if I have a blob which is 7k and compresses to 5k It would still occupy 8k (if this is the DB blocksize). It would be somewhat unfortunate to have a tablespace with extra small blocks for (extra large) LOB segments. (But I understand that SecureFile with dynamic chunk sizes deals much better with this) 

After 30 seconds a notification needs to be sent out to all clients that a lock has expired and this employee is available again for updates. Now to identify if LOCK_UNTIL duration has reached, application makes a SQL call to database every 2 seconds to see if 10:00:30AM has reached. Performance Issue: This call every 2 seconds is causing lot of overhead on the database and on the application server. I am looking for a better ways where Oracle itself initiates a notification to the server when lock expiration time has reached. Is there any way I can achieve this? Possible solutions: 

Now this SQL is internally generated by Oracle to perform row migration for child PLAN table. To resolve the issue I tried following changes: 

I have confirmed that OrderID column (Foreign Key column) in the PLAN table has index on it. Tried increasing PCTFREE parameter on the table. 

But If I break the query into 2 parts and rebuilt indexes with ONLINE option separately, DML queries DOES NOT get blocked while indexes are being rebuilt 

There are many more child table where ORDERS is there parent table. Under heavy load, when ORDER status is changed which causes row movement between partition, following deadlock error is printed in the log ORA-00060: deadlock detected while waiting for resource In the Oracle trace log, I see following SQL causing deadlock 

So for some reason, ORACLE is not taking Index into consideration while running update query on PLAN table. Am I missing something? 

Current Setup: My application uses Java (Spring) and Oracle 11g and has functionality where logical locks are placed on an object before updates are made in the table. For example there are 2 tables EMPLOYEE and EMPLOYEE_LOCK. When any update is made to employee, an entry is inserted into EMPLOYEE_LOCK table to indicate that for next 30 seconds a particular employee is locked. So EMPLOYEE_LOCK table looks like below (as of 10AM) 

My ordering application uses Oracle 11g Database. This DB has a primary table ORDERS and multiple child tables like ORDER_DETAILS, PLAN etc. ORDERS table is LIST partitioned on STATUS column and all other tables are referenced partitioned with ORDERID as a foreign key. At peak load, when order status is changed and ORDERS table row is moved from one partition to another, Oracle performs row migration for all the child tables referenced partitioned by ORDERS table. Due to many tables that depend on ORDERS table, large number of row movements happen causing a deadlock in one of the child table. My question is, how to resolve a deadlock caused in the ORACLE's internal row migration step? Here is an example setup: ORDERS table: 

I'd like to know if it is possible to have a single passive node for two SQL Servers in a failover clustering deployment. For example, both active servers A and B will use server C as their contingent server. If it is apt to do so, will server C run two instances of SQL Server; one for A and one for B? As a side question; what is the benefit of "dynamic quorum" and "dynamic witness"? 

Record count is the same but there is a big gap between the two in terms of space used. sp_spaceused shows 7.691.344 KB as reserved while the report shows 4.340.216 KB. Which one is correct? 

I want to log the duration and time of user defined stored procedures each time they are invoked. I believe using extended events would be the way to go to achieve this. Can you please help me in defining the session and in querying the results? I am not quite sure about which event to add (sqlserver.rpc_completed?) and which target to choose (synchronous_event_counter,asynchronous_file_target or ring_buffer?). I also need help in querying the results (For example: grouping the result set so that each sp will show from top execution duration and execution time.) We use SQL Server 2008 R2. 

It was all down to collation of the column. It was different from the database's (and the table's) collation. Now changed the column's collation to database's and no more implicit conversion shows up. Have no idea about the internals and why it caused the problem. 

I set up a virtual lab environment simulating a clustered network where I unticked one of the nodes as the preferred owner. I can confirm that whenever the ticked node goes offline, the unticked one comes into play and the system failbacks onto the ticked node once it comes online again. 

We experience hundreds of sessions going in suspended state on rare occasions. When it happens there is no increase in terms of resource usage - CPU or memory. Rather there is a small amount of decrease 5-10% on CPU. There is no single error on the SQL log. I know this will be a blind question but what might be causing this? Any suggestions to check in SQL Server like plan cache etc..? 

The systemd unit file for the SQL server should be locally extended by a dependency on the this will make sure the start is done after the mount and the unmount waits for the stop. The most generic option to do this would be 

I am wondering about unused index in MS SQL Server. By the Index usage DMV I can identify an index which has not been used for seeks, scans or lookups. However I know from Oracle that an index might not be used in such a way in a execution plan, however it can still contribute statistics/cardinality information to the (Oracle) optimizer. This contribution is not monitored in the same way. So I am wondering if in MSSQL a Index can have a similar positive effect even when it is not directly used (in a representative time frame)? And specifically, if it can be better than a column statistic (I.e. dropping the index would be harmful). I haven’t seen this mentioned in any of the index tuning articles I have come along, so I assume MSSQL (up to 2017) does not have this concept, is that correct? 

Update: Initially my question also asked why I no longer see the 'BIN$' segments created by Flashback Drop Table anymore in 12.2 anymore. However that was an error on my side, my test tables had simply no segments to begin with. 

In a scenario where Oracle Golden Gate is used to replicate a primary site with an Oracle RAC database to a secondary site (and active/active back) we suspect unexpected changes from the unused secondary site. The issue is a bit hard to debug as we do not have direct DBA access. I wonder is there an easy way with unprivileged SQL access on the primary side to see if any changes are received from the other database? Can I see counters or timestamps of OGG activity which helps me to track down DML made? As I understand it I could see changes from the OGG user when setting up triggers or auditing - however both is not available in this situation. 

My primary goal (with this question) was to see if Oracle can give me some way to identify this expiration time trigger and initiate an activity rather than Application server initiating one. 

Failing Edge Case Since partition for inserting data is decided based on submitteddate which is a current date, there will be a situation where an order comes at 2016-11-30 at 11:59PM and data in ORDER table is inserted in NOV2016 partition but data in ORDER_LINE and PLAN table is inserted on DEC2016 partition as by the time inserts are done, date may change in the system. When I try to drop Nov2016 partition from all tables (child first due to FK constraint), ORDER_LINE and PLAN table drop partition might go through but ORDER table partition drop will fail as orderid from Nov2016 partition would be pointing to the DEC2016 partition data in other 2 tables. How do I make sure that the orders inserted on date change still goes to same partition across all tables? Added info (based on @dezso reply) Dezso's transaction suggestions makes sense. But to make question concise, I left some details. With those details, the suggested solution might differ a bit. Application supports 2 databases, Oracle and Postgres. For Oracle, partitioning has been implemented using Reference partitioning with partitioned ORDER table and child tables are referenced partitioned based on foreign keys. For postgres, since there is no reference partitioning option like Oracle, each table was supposed to be individually partitioned using submitteddate. The plan was not to add submitteddate to each table but to use inheritance like below 

I wanted to know if DBMA_REDEFINITON package allows a WHERE clause to filter contents before migration. I have a partitioned table and wants to copy data and constraint to another table using DBMA_REDEFINITON but while copying contents, I do not want to copy a particular partition from the original table. Is it possible to drop this partition using WHERE clause. The question came from following information given on Oracle Tips site 

One solution could be to use DBMS_SCHEDULER package and create a scheduled job. But I could not find anywhere in the documentation, some way for the job to notify application server. It can send an email but that wont help me much. Second option could be to use "Database Change Notification feature" but this is triggered on a DML or DDL change on the DB object which is not happening in my case.