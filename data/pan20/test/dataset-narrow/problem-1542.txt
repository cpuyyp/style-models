Take the first batch of data as first S (Number of time steps) (S1, S2, .., Ss)words from each article (for the sake of simplicity let us assume batch size = m) Set the initial hidden state H0 = $[0,0,..,0]$ Calculate loss and gradient on this batch and update the parameters Move a word forward and get next S words from each article (hence S2, S3, ... , Ss are the same words as in the previous batch) Use H1 calculated from previous step and use it as H0 in this iteration Do this to the end 

@shimao has answered it Here on stats stackexchange To quote shimao "Yes, it would be more logical, although equivalent both ways. I was able to find a bit of discussion on this here It looks like the original authors later implied that both training and test time should use the unbiased variance. Also the actual implementation in many libraries is not completely consistent with what is described in the paper." 

Dynamic Memory networks are described here . I understand what is going on for question answering task but when it comes to sequence to sequence modeling, they describe it in 4th paragraph of 2.4 answer module. 

Temporal data is across time. Data points are correlated and you model them using a variant of RNNs (LSTMs, GRUs, Tree recursive NNs etc). example would be text script of speech. Notice how each data point would be correlated with the previous data point local data is ambiguous and could mean a lot of things. local to a particular location? local data as opposed to server data? local to a particular task? etc etc. Please provide context 

Why we wont take any but Scheme 1 and Scheme 3 is because: H0 should only be initialized with a vector of zero when there is no context available (if you have some additional information, why not use it?!). The objective is to maximize the probability of each article and the sentences are not independent. Also, there is no reason to not treat period "." as a step in itself. We do calculate its word embedding too Why not Scheme 1 - Because of catastrophic interference ( As the network in Scheme 1 fed in same input again and again ) combined with slow learning it would result in 

This is a bit off topic for this SE, or maybe opinion-based, but, I work in this field and I'd recommend Scala. No I would not characterize Scala as a "stats-oriented" Java. I'd describe it as what you get if you asked 3 people to design "Java 11" and then used all of their ideas at once. Java 8 remains great, but Scala fully embraces just about all the good ideas from languages that you'd want, like type safety and closures and functional paradigms, and a more elaborate types/generics system, down to more syntactic sugar conveniences like case classes and lazy vals. Plus you get to run everything in the same JVM and interoperate with any Java libraries. The price is complexity: understanding all of Scala is a lot more difficult than all of Java. And some bits of Scala maybe were a bridge too far. And, the tooling ecosystem isn't great IMHO in comparison to the standard Java tools. But there again you can for example use Maven instead of SBT. But you can mostly avoid the parts of Scala that are complex if you don't need them. Designing Scala libraries takes a lot of skill and know-how; just developing run-of-the-mill code in Scala does not. From a productivity perspective, once you're used to Scala, you'll be more productive. If you're an experienced Java dev, I actually think you'll appreciate Scala. I have not liked any other JVM language, which tend to be sort of one-issue languages that change lots of stuff for marginal gains. Certainly, for analytics, the existence of Spark argues for Scala. You'll use it eventually. 

I think most people would simply average a statistic like accuracy or kappa over the several runs, rather than compute one overall statistic. While it's not necessarily true that an average (weighted by the test set size in each case) of these statistics equals the statistic computed over all of the runs at once, it happens to be true for accuracy and kappa. Hence it shouldn't matter. I think the reason you may be getting a difference is that you're normalizing away the test set size. I don't think you should. 

I could not understand how the global gates are computed and why would they be equal for each word for first pass. They are going to be computed as a function of question vector (same) and word vector (different for each word). This is how vague i am on what is going on here. Can somebody explain how the piece in the paragraph mentioned come together? 

If memory is not the constraint, should i not always keep its value equal to the number of processors available with me on GPU. Here i have my first question - Am i right in thinking that number of processors on GPU mean number of parallel floating point computations it can undertake (i don't suppose GPU's too implement 2 threads per processors as in CPUs)? Now, GRU machinery is: (please note the biases are not showcased in the image but are to be used for better performance) , To me the value 32 seems too low to be kept as default. If i have a GRU with 250 dimensional hidden state and 100 dimensional input i would have (2 x 250 x 350 x batch_size) floating multiplications that i can run in parallel (computing r and u) followed by (250 x batch_size) floating point additions that i can run in parallel followed by (250 x batch_size) sigmoid applications which are at least (250 x batch_size) potential parallel computations. Then it is a (250 x batch_size) parallel computations in application of reset gate followed by (1 x 250 x 350 x batch_size) parallel computations followed by (250 x batch_size) parallel computations for biases. Now in the final step it is (250 x batch_size) floating point computations to calculate (1 - update gate) followed by (500 x batch_size) parallel floating point computations for output state. To me it seems the bottlenecks in parallel implementations using GPU would be presence of too many low sized layers which, for instance here, are bias additions after matrix multiplications (at any rate they are still 250 x batch sized big here so will anyway use GPU to the maximum) Is the way i am thinking in the second paragraph correct while analyzing a deep network for a vague idea on how much the hardware would be able to impact its running time 

rhadoop (the part you are interested in is now called rmr2) is simply a client API for MapReduce written in R. You invoke MapReduce using R package APIs, and send an R function to the workers, where it is executed by an R interpreter locally. But it is otherwise exactly the same MapReduce. You can call anything you like in R this way, but no R functions are themselves parallelized to use MapReduce in this way. The point is simply that you can invoke M/R from R. I don't think it somehow lets you do anything more magical than that. 

Only square matrices have an inverse, and neither $X$ nor $X^T$ here are (necessarily) square. I guess that's the most direct answer. $X$ may have a left-inverse $A$, such that $AX = I$. In that case, if $X\theta = y$ then $AX\theta = Ay$ so $\theta = Ay$. The direct way to construct that $A$ is just where you started though; it's $(X^TX)^{-1}X^T$. The Gramian matrix $X^TX$ is (square, and) invertible if the features (columns) of $X$ are linearly independent, and, $X^TX$ is more importantly relatively quite small and so it's feasible to invert. You normally wouldn't compute the inverse anyway, but try to solve the system $X^TX\theta = X^Ty$ with QR decomposition or something, but that's not what you're asking. 

Your system isn't just trained on items that are recommended right? if so you have a big feedback loop here. You want to learn from all clicks/views, I hope. You suggest that not-looking at an item is a negative signal. I strongly suggest you do not treat it that way. Not interacting with something is almost always best treated as no information. If you have an explicit signal that indicates a dislike, like a down vote (or, maybe watched 10 seconds of a video and stopped), maybe that's valid. I would not construe this input as rating-like data. (Although in your case, you may get away with it.) Instead think of them as weights, which is exactly the treatment in the Hu Koren Volinsky paper on ALS that @Trey mentions in a comment. This lets you record relative strength of positive/negative interactions. Finally I would note that this paper, while is very likely to be what you're looking for, does not provide for negative weights. It is simple to extend in this way. If you get that far I can point you to the easy extension, which exists already in two implementations that I know of, in Spark and Oryx. 

I understand to check only a few dimensions but what does the part after be careful mean. I fail to understand that warning. can somebody explain it? 

So, the final loss would be calculated by calculating cross entropy between gold (the answer dataset provided) and predicted over each position in the final vector produced i.e m separate cross entropy for position in the context paragraph and for each paragraph in the batch fed. This should encourage each ANSWER position to increase in probability over class ANSWER and each O position to increase in probability over class O. The probabilities sum to 1 over ANSWER or O for each token Instead what i decided to do is i made the a final ReLU(affine) layer over LSTM output of output size 1 (probability of answer) and hence calculate only 2 cross entropy loss per paragraph aimed at increasing the probability outputs at the token positions where the answer begins and ends. This should encourage each ANSWER position to increase in probability over the class ANSWER and as the probabilities predicted would sum to 1 over the length of the paragraph But this doesnt seem to be training at all. What could be going theoretically? I am looking for arguments against the second approach. 

Let's say we have A1, A2, ... , Am different articles in the corpus and each of them has W1, W2, ....., Ww words. We are training a language model on them. Do we: Scheme 1 

Take the first batch of data as first S (Number of time steps) (S1, S2, .., Ss) words from each article (for the sake of simplicity let us assume batch size = m) Set the initial hidden state H0 = [0,0,..,0] Calculate loss and gradient on this batch and update the parameters We move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration Do this to the end 

I am training a model on squad dataset for question answer. The problem is as follows In the SQuAD task, the goal is to predict an answer span tuple {as,ae} given a question of length n, q = {q1,q2,...,qn}, and a supporting context paragraph p = {p1,p2,...,pm} of length m. Thus, the model learns a function that, given a pair of sequences (q,p) returns a sequence of two scalar indices {as,ae} indicating the start position and end position of the answer in paragraph p, respectively. Note that in this task as ≤ ae, and 0 ≤ as,ae ≤ m The approach i am taking is: 

To understand the coefficients you just need to understand how the logistic regression model that you fit uses the coefficients to make predictions. No, it does not work like a decision tree. It's a linear model. Really, predictions are based on the dot product of the coefficients and the values from some new instance to predict. This is just the sum of their products. The higher the dot product, the more positive the prediction. So you can understand it as computing something like (I don't know what coefficients go with what feature in your model.) That generally means that inputs with bigger coefficients matter more, and inputs with positive coefficients correlate positively with a positive prediction. That's most of what you can interpret here. The first of those conclusions is only really valid if inputs have been normalized to be on the same scale though. I'm not clear that you've done that here. You should also in general use L1 regularization if you intend to interpret the coefficients this way. 

Lots of questions here. First, for a truly new user with no data, there is no way to use a recommender model. If you have literally no information on the user, the only thing you can do is provide some default recommendations. Of course, once you have any data, and you can rebuild the model to incorporate the user, you can make recommendations. You can do that in Spark but you already know that. This will take too long if you need to add information about new users at runtime. The technique you want is called "fold-in", used to figure out what the new user vector is (approximately) given the items the user interacts with. It's just some linear algebra and does follow from the equation you give. I dug out an old slide which might help: 

This sounds like a classic frequent pattern mining problem, where you're trying to find sets of items that are frequently found together within users' sets of purchases. Start here: $URL$ 

In lecture notes for cs231n while discussing checking analytical gradient with numerical gradient the paragraph says this: 

Run a BiLSTM over the question, concatenate the two end hidden vectors and call that the question representation. Run a BiLSTM over the context paragraph, conditioned on the question representation. Calculate an attention vector over the context paragraph representation based on the question representation Compute a new vector for each context paragraph position that multiplies context-paragraph representation with the attention vector Run a ﬁnal LSTM that does a 2-class classiﬁcation of these vectors as O or ANSWER. so a final softmax(ReLU(affine)) layer over LSTM output of output size 2 (O or ANSWER). 

*In this scheme we would have to use zero padding on the last batch (at the end of articles) Scheme 2 Same as Scheme 1 but in step 5 we reinitialize H0 to a vector of Zeroes Scheme 3 Same as scheme 1 but in step 4 we move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration Scheme 4, 5, 6 Same as Scheme 1, 2, 3 but instead of taking s consecutive words we take first sentence from each article and zero pad them to the length S What is the right way to go through the data in feeding it to a RNN, Please give reasons as well. I think it should be Scheme 1 but it could be really slow 

LSTMs would run into problems beyond 500 time steps. original paper by Hochreiter and Schmidhuber: "LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps." Example: Your video is 60 seconds long. I would start by encoding 5 frames per second using a CNN (use resnet (SEnets have better performance but i havent read the paper) till the layer just before final affine layer which feeds into the softmax encode your images. Use these encodings as inputs in a GRU (Hence a 300 time steps for this GRU) and use its final state to make the class prediction (use a separate loss for each label?)