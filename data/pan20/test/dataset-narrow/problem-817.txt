DDOS is never an easy problem to solve but it sounds like the type of low level attack you are experiencing would definitely be helped by the queuing facility in HAProxy i.e. don't flood the Apache server with too many requests (maxconns). You could also try and prioritise your existing customers using a cookie facility, like some busy sites do black Friday protection. 

The normal way to handle multi-site fail over is to have a pair of load balancers on site A in a master/slave fail over config. Then put an identical pair on the remote site. If ALL the backend servers on site A fail then use the public VIP on site B as the fallback (high latencey of course across the WAN)... But back this up with a proper DNS based load balancing services such as Amazon route 53 (which has health checking and fail over) , So that fairly quickly no traffic would go to site A anyway. If you have a really fast stretched VLAN across two sites with the same IP scheme then you can do it with just a single pair of load balancers. But it sounds like you should just forget HAProxy and use Amazon Route 53 for the whole lot? (unless you have persistence issues.) 

First of all are you sure the connections are not just hitting the queue? i.e. you have reach maxconns? What does your stats page show? Also just disable conntrack (it sucks): 

Yes, Traffic is configured to hit a Floating Virtual IP, when the master load balancer fails the slave detects this, brings up the Floating IP and sends a load of gratuitous ARPs to the network. You can achieve this with VRRP, Keepalived, CARP etc. The Loadbalancer.org appliances use HA-Linux & HAProxy. 

On NPS, under NPS (Local) > Accounting, I see "Configure Accounting", is it that straight forward?? Is there anything else needing done for accounting to be sent to the host 10.10.4.25? It looks way too straight forward. Thanks for your input. 

We currently have a pretty simple setup. We have a 2008 Windows Server using DHCP for our clients. We have a remote site that has 10 people. The remote site currently get their IPs from the router, but we'd like for them to get their IP from our local DHCP server here. From what i've read this seems fairly straightforward. On the remote router turn on Ip-helper x.x.x.x to point to the DHCP server back here. I guess my concern is, how will the transition go? If i enable that right now, will the users who currently have an IP lease from the router drop and acquire a new one, or will it just allow those leases to expire, then once they do, it will then look at the new IP range from the local DHCP server here? Just curious if this is anything i need to worry about or is it really as simple as it seems, just set it and run. I've configured RRAS on my side as well per several online articles discussing how to set this up. Thanks for any insight you can give! 

We have just implemented RD Gateway for our own department in preparation for a push to the whole agency for telecommuting. It is all setup and working great, but I was trying to figure out how best to go about monitoring/reporting of users. I see third party software out there that will do it, but is there anything built-in or via powershell/scripting that I could use that would give me a report of the daily activity of users? Something to say, "User A connected at this time, was on for this long, sent/received this much data"? Basically some of the same stuff you can see in event viewer. Ideally I'd like to be able to have this setup so that once a day it emails me with the daily usage for when a supervisor asks about if their person is actually working (or at least online sending and receiving x amount of data), I'll have some metrics to give them. I realize that actual work output is relevant and more of a managerial issue, but I would like to be able to offer as much as I can from my end when asked. Thoughts? Thanks! 

I know this is a fairly old post but I was searching for load balancing remote desktop on AWS and came across it. Personally I prefer NOT using RD Gateway and just sticking to plain old session broker with a load balancer in front of it for high-availability). You could always use a VPN for extra security. 

Try removing parts of the system to find the bottle neck. 15 test servers sounds an awful lot! you should be able to get 1000's of TPS out of a single test unit. You are not asking HAProxy to wait for a response are you? i.e. utilising the maxconns functionality and queueing functionality? Like I said try simplifying, but if you do think it is HAProxy then please post the configuration. 

I'm glad that you seem to have solved the immediate problem with fail2ban, and it does make sense to block at the iptables level, but you can do the exact same thing in your HAProxy config: You can use acl's with src_http_req_rate() or even src_http_err_rate(Abuse): I've used them in some examples of haproxy configs for DDOS mitigation here. Longer term: It sounds like you might want to implement a double login for that page, if the users will accept it then you could just put an extra htaccess password in from of the login page, so that they need to separate logins (pretty hard to crack with a script). Or you could use mod_security to do the same thing or some kind of honey trap. 

Thousands should be fine, the total number of health checks on backends may become an issue but ports are not. For a definitive answer you might want to try the HAProxy mailing list. 

I've never attempted to restore a deleted individual mailbox and figure it's a good thing to have tried for when/if it's actually needed. I have a full backup from Monday (today is Wednesday), i have the GRT options checked in the backup for Exchange, I can browse through the backup job and find a particular employee's mailbox that I removed on Tuesday. My question is, is it as simple as selecting that checkbox and submitting the job? His account no longer exists on the network because he is no longer employed with us, i figured it was a good person to try this on. I see an option in Backup Exec that says "automatically recreate user accounts and mailboxes", so i assume i would select that as well. I guess it just seems too easy, and since this is a production server i wanted to ask around before attempting to restore this individual mailbox back. Anyone have any experience with this? Thanks. 

I'm currently wanting to implement a backup Exchange server, and from what i read it sounds like SCR is the way to go. I currently have one Exchange 2007 server, pretty small and nothing special going on. I built another server and set the drives the same (as was recommended on technet). Online i see several sites saying, "all you need to do is this" run these commands: Enable-StorageGroupCopy -Identity -StandbyMachine -ReplayLagTime 0.1:0:0 Suspend-StorageGroupCopy -Identity -StandbyMachine Update-StorageGroupCopy -Identity -StandbyMachine But my question is even more basic i guess... I logon to the secondary server, and obviously Exchange isn't on there so i assume i'd need to install it first before i can set it as a passive/backup server. I run the install but when i select Passive Clustered Mailbox Role it informs me it needs to be setup as a Clustered Server first (which makes sense). But i'm wondering, do i need to set up two servers in a cluster to make this work? I'm not wanting real-time failover, just the SCR ability to turn the passive server into the active server in response to a disaster. Would i just need to install Exchange on the second server as if it were the only one in the domain (Mailbox Role) and then once done, run the above commands? Any help explaining this woudl be greatly appreciated, thanks! 

Andy, The trick is to add another backend that you only use for the extra stick table. You can only have one stick table per backend - BUT you can use them in ANY front/back end... So I just add one called Abuse that you can then use as a global 60 minute ban for any backend... You will need to change my example but try something like this: 

I see the confusion now, you actually want to know how the backend server can read the x-forwarded-for header from the logs? Try these instructions for Windows IIS XFF or Apache XFF. 

Um - Yes? SSL sessionid can be used as an optional performance enhancer - i.e. don't re-negotiate keys by hitting roughly the same server each time.... Sometimes we use it if source IP is all from one proxy... BUT it should never be used for session persistence if the application is depending on it always being correct. You need to use SSL termination + cookies or just good old reliable source ip. 

If you have MySQL listening to BOTH ports then simply leave the backend server port out of the HAProxy config i.e. let the client decide the end port to connect to. OR if your MySQL is only listening on 3306 then force all the connections to go to that port by specifying it on the backend config. If still stuck try posting your config. 

You can use socket commands to clear the table i.e. echo "clear table Abuse key 192.168.64.12" | socat unix-connect:/var/run/haproxy.stat stdio The documentation is here: $URL$ 

Actually, I just figured out what it was. I was passing in -assecurestring but it wasn't actually returning the password I entered. Apparently I was barking up the wrong tree. :) Thanks for your time! 

We are currently looking at archiving emails and revamping our retention policy. The big question is (for the legal department), how far back do we want to save? Currently our users have a huge mailbox limit, and in the past have all been able to archive as they saw fit. So we have a couple hundred GB of data that isn't in the Exchange Database, but that we'd probably end up sucking into an Archive database for discovery. What I'd like to do is be able to quantify for the legal team how much that would be if we went back 1 year, 2 years, 3 years, etc. I found a fairly straightforward Powershell Script at TheDailyAdmin that does what I want for the most part, but it lumps it all in one pile. I'd like to be able to see the results but sorted by user to know that Sally has 47MB that is older than 2 years, Charles has 190MB over 2 years old, etc. Here is the script I've ran: 

We currently use Public Folders with Exchange 2010. We have a series of shared calendars throughout the organization. Some of the items posted require us to set up devices (video conferencing for example) at a certain time, but the end users who schedule these appointments aren't always the best at letting us know when it is (even though it's supposed to be several days in advance/notice). Other than the obvious way of just going out to those individual calendars multiple times a day to look, is there a way to have Exchange (or powershell, Outlook, etc) alert us via email when an event has been created? For instance if Powershell could go to the Public Folder database, search for any entries added within the last 12 hours, then email me to let me know. Even if I could set it to run once a day, each morning at 7.30 am, that would be a huge help. Thoughts? Thanks! 

Alexis, This sounds exactly like what agent-check is meant to be used for. Willy kindly added the patch we use for the Loadbalancer.org appliance and we have open sourced our Windows based server agent. The instructions including a basic Linux server example are here: $URL$ The Windows agent can monitor a combination of CPU, RAM & TCP connection counts (i.e. for RDP sessions). Feedback is always welcome. This reminds me - it would be nice if someone implemented some kind of server response time algorithm, then you wouldn't necessarily need a server based agent. 

Karl, Call me crazy bu don't you just need to remove the OR? redirect scheme https code 301 if !host_mydomain or !{ ssl_fc } should be: redirect scheme https code 301 if !host_mydomain AND !{ ssl_fc } i.e. if NOT the domain I don't want AND NOT SSL 

Either set up 2 * HAProxy instances as a master/slave pair. Or why not just use Amazon Route 53? Its cheap, fast, transparent and has health checks. Load balancers are most effective when the application is clustered on multiple servers. 

My first question would be why? If mod_security is on the actual server it will be transparent. If mod security is on a gateway then the client will only ever see the gateway server address. Just use x-forwarded-for to see the client IP in the server logs. It may be possible to use TPROXY in the linux kernel in a two subnet configuration where the servers default gateway is translated through the mod sec box (servers would NOT be able to have a public IP address). But I'm not sure if apache/mod_sec even supports TPROXY it needs code specifically to support it (like in HAProxy). 

I've read that it's fairly normal to have several ARP requests per second on the network, but was curious if someone could elaborate on the "why" I would have so many versus not? Over the course of 4 seconds I see about 40 arp requests, right about 10 per second, and all reqeusts seem to be from valid IPs on our network with a known server or PC. Is that many normal? If so, what is the purpose? I've never thought much of it thinking that since it wasn't several hundred a second it was "ok", but my boss recently looked at network monitor and saw them and said that at his previous job they didn't have that many requests, and the network was much larger than ours currently is. So I was just hoping someone could explain to me a bit more about this and if it's anything I should be concerend about or check more deeply into. I definitely want to avoid any network congestion that I can, to allow the network to run as smoothly as possible. Thanks in advance. 

This may be more than what you're looking to do, but here's how we use RDP for remote users who aren't using VPN. We recently started using the RD Gateway Manager with Remote Desktop Services, a role in Windows 2008. We have it setup to go through our TMG server and directly to a users machine. It uses NLA as mentioned above. The user connecting has to be a member of the right AD group, and a member of the right local group to be allowed access. Depending on how you want it setup, you can connect via a webpage which basiclly opens up mstsc and inputs the proxy setting for the RD Gateway, or you can set the settings on your machine manually so that each time you open it it attempts to go through that proxy. So far it has worked quite well and seems to be secure.