To understand the nature of anisotropic filtering, you need to have a firm understanding of what texture mapping really means. The term "texture mapping" means to assign positions on an object to locations in a texture. This permits the rasterizer/shader to, for each position on the object, fetch the corresponding data from the texture. The traditional method for doing this is to assign each vertex on an object a texture coordinate, which directly maps that position to a location in the texture. The rasterizer will interpolate this texture coordinate across the faces of the various triangles to produce the texture coordinate used to fetch the color from the texture. Now, let's think about the process of rasterization. How does that work? It takes a triangle and breaks it up into pixel-sized blocks which we will call "fragments". Now, these pixel-sized blocks are pixel-sized relative to the screen. But these fragments are not pixel-sized relative to the texture. Imagine if our rasterizer generated a texture coordinate for each corner of the fragment. Now imagine drawing those 4 corners, not in screen space, but in texture space. What shape would this be? Well, that depends on the texture coordinates. That is, it depends on how the texture is mapped to the polygon. For any particular fragment, it might be an axis-aligned square. It might be a non-axis-aligned square. It might be a rectangle. It might be a trapezoid. It might be pretty much any four-sided figure (or at least, convex ones). If you were doing texture accessing correctly, the way to get the texture color for a fragment would be to figure out what this rectangle is. Then fetch every texel from the texture within that rectangle (using coverage to scale colors that are on the border). Then average them all together. That would be perfect texture mapping. It would also be exceedingly slow. In the interest of performance, we instead try to approximate the real answer. We base things on one texture coordinate, rather than the 4 that cover the entire fragment's area in texel space. Mipmap-based filtering uses lower-resolution images. These images are basically a shortcut for the perfect method, by pre-computing what large blocks of colors would look like when blended together. So when it selects a lower mipmap, it is using pre-computed values where each texel represents an area of the texture. Anisotropic filtering works by approximating the perfect method (which can, and should, be coupled with mipmapping) through taking up to a fixed number of additional samples. But how does it figure out the area in texel space to fetch from, since it's still only given one texture coordinate? Basically, it cheats. Because fragment shaders are executed in 2x2 neighboring blocks, it is possible to compute the derivative of any value in the fragment shader in screen-space X and Y. It then uses these derivatives, coupled with the actual texture coordinate, to compute an approximation of what the true fragment's texture footprint would be. And then it performs a number of samples within this area. Here's a diagram to help explain it: 

Pixel transfer operations are governed by more than just your parameters to a pixel transfer function. They are also governed by parameters passed to the functions. Of particular interest to you is . This tells OpenGL what the byte alignment for each row of data is in your array. By default, this value is four, not one. This means that OpenGL will believe that the second row of your image starts at this address: , since 1025 is not aligned to 4 bytes. And that means it will run off the end of your allocated buffer eventually. If you are using data with 1-byte alignment for rows, you must inform OpenGL of this by calling . Note that you should not do this by default; do it only when dealing with byte aligned data. 

The complexity of this makes it seem like you don't really want to use transform feedback; you want to use a compute shader. Then, through indirect rendering and SSBOs, you can write whatever vertex commands data you want. The idea is that you have some number of indirect rendering commands (one command per set of instances). And for each instance you create, the CS invocation needs to bump the instance count of each of those commands (atomically, of course). Now, because there are state changes between the commands, you wouldn't be able to use a multi-draw indirect command. But by using indirect commands and a CS, you can at least keep all of the data on the GPU, without having to do things like buffer copies to multiple locations. Transform feedback is merely an ad-hoc mechanism for something that modern OpenGL allows you to just do directly. If you need to broadcast the "primitive" count to multiple locations (or really, if you're doing anything besides rendering the feedbacked data), then you should be using a compute shader, not TF. 

Each VAO, and shader for that matter, has a separate space of attribute locations. It's perfectly fine to have multiple shaders use the same attribute locations. Indeed, it's a good idea to do so if those attributes conceptually mean the same thing to both shaders. A VAO can be used with a shader if that VAO's attribute locations match those the program expects. Well, technically they don't have to match. It's OK if the VAO provides an attribute that a shader doesn't use. It's more complicated if a VAO doesn't provide an attribute for a location that the shader expects. So whatever it is that generates your shader prefix strings and VAOs should not be based on what has been generated for other shaders/VAOs. So if you're going to have attributes increase numerically starting from 0, then this should be the case for each independent shader. It should not be based on some constantly increasing global. 

OpenGL doesn't arbitrarily process data; it reads the data you tell it to read. If you want it to not read past a certain point, stop telling it to read past that point ;) 

The "all" extensions option means exactly what it says: all extensions. If it exists, then its in the generated header. But this doesn't mean that the code will fail to execute on an implementation that doesn't provide every extension. The extensions you select are merely what the generated files make available to you. But of course, the whole point of selecting specific extensions is that you don't have stuff you don't use cluttering up your header. If you're going to select everything, you may as well be using GLEW. 

The common names for the components of texture coordinates are U and V. However, because the next letter used (for 3D textures) is W, that causes a conflict with the names for the components of a position: X, Y, Z, and W. To avoid such conflicts, OpenGL's convention is that the components of texture coordinates are named S, T, and R. Thus, you have function calls like this: 

Image Load/Store is not intended to replace framebuffers. It's a feature that allows (relatively) arbitrary reading and writing to memory. So you would use images when you need to arbitrarily read and write to memory. You would use framebuffers when you're rasterizing primitives for rendering purposes. The fact that you can build scenarios where you would use the latter in place of the former does not mean that this is the purpose of the feature. 

You can set a stride of zero with OpenGL 4.3's separate attribute format API, or the ARB_vertex_attrib_binding extension: 

There are also the API functions and , which are analogous to and . If you don't do any explicit assignments, implementations usually will assign one of your output variables to location 0. However, the OpenGL standard does not require this behavior, so you should not depend on it either. 

I don't know what language that is, but I'm going to hazard a guess that the arrays and are not the same size. If you intend to provide per-vertex colors, then you must provide one color for every vertex. If you want to provide a single color that all vertices share, then that would be a , not a vertex shader put. Also, commenting out your declarations isn't helping. Though if you are going to declare a for the VS output variable, then the matching FS input needs to have the same declared. 

You must fully create a renderbuffer before you can attach it to an FBO. The pre-DSA OpenGL Object creation functions do not create objects; they create object names. It's like creating an uninitialized pointer variable; it doesn't point to anything yet. The actual creation of the object happens the first time the name is bound to the context. At that moment, the name is given a real object. Under normal circumstances, it was pretty much impossible to use an object without binding it to the context, so this distinction couldn't be noticed. However, once the ability to attach objects to one another came to be, this distinction became noticeable. Pre-DSA, the only way to fully create a renderbuffer object was to bind it to the context. However, with ARB_DSA/GL 4.5, the functions actually create objects (imagine that!). So if you use , you can attach the renderbuffer to an FBO without binding it first. 

I'd say that the fundamental problem here is your approach. You basically want the outside renderer to throw a bunch of random stuff at the wall, then let the internal rendering system sort it out without having any idea what any of that stuff actually means. Here's what I mean. Consider the case of skinned meshes vs. unskinned meshes. The internal rendering system has no idea which ones are skinned or not; all it knows is that some draw commands use a particular program and a large UBO/SSBO, while others use a different program and much smaller UBOs. And that kinda works... to a point. And that point is when you're dealing with state values that are no longer encapsulated in objects. Uniform values are program state. And you might reasonably have two different objects that use the same program have the same value for that uniform. They might have different values for other uniforms, but there would be different variance in which objects use which values. But here's the thing: in pretty much every case where this happens, someone knows that it happened. And the code that knows that is the higher level code. After all, if two objects have common programs and common uniform values, they probably have a bunch of other stuff in common too. So odds are good that the higher level code knows when such things would happen. By using this "throw stuff against the wall" approach, by separating the business end of your renderer from the source of that rendering data, you destroy the semantic information you could have used to detect such things. That is, it is the higher level system that should handle the "sort by state change" part, since it can do a much better job. 

This is not why the nearZ plane cannot be zero. The goal of perspective math is not to project the E onto the near plane. If you look at the actual perspective matrices, you'll find that the nearZ only applies to the computed clip-space Z coordinate, not the X and Y. Remember: projection is essentially transforming a position such that you lose one or more dimensions. You're projecting from 3D space to 2D space. So projection is removing the Z component, projecting a scene into a 2D region of space. Orthographic projection just discards the Z; perspective projection does something more complex, rescaling the X and Y based on the Z. Of course, we're not really doing that; Z still exists. Otherwise, the depth buffer wouldn't work. But the basic perspective projection math does not define how the Z is computed. Therefore, we can compute the Z however we want. The traditional depth equation (for OpenGL's standard NDC space) is where near/farZ come from: $$Z_{ndc} = \frac{\frac{Z_{camera}(F + N)}{N - F} + \frac{2NF}{N - F}}{-Z_{camera}}$$ and are near/farZ, respectively. If is zero, then the equation degenerates: $$Z_{ndc} = \frac{\frac{Z_{camera}(F)}{-F} + 0}{-Z_{camera}}$$ $$Z_{ndc} = \frac{-Z_{camera}}{-Z_{camera}}$$ $$Z_{ndc} = 1$$ Oops. So your question really is why we use depth computations (which again, don't really have anything to do with perspective projection) that require near/farZ values like this. This equation, and those like it, has some very nice properties. It plays well with the clip-space W component needed for perspective projection (the division by $-Z_{camera}$). But it also has the effect of being a non-linear Z projection. It puts more precision at the front of the scene than in the distance. And that's generally where you need depth precision the most. There are alternative ways to compute the clip-space Z. You can use a linear method, which doesn't forbid near Z from being 0. You can also use a method that allows for an infinitely distant far Z (but still relies on near Z); this requires a floating-point Z buffer, though. And so forth. You haven't really shown your code, so I can't say if it truly "works". The clip-space W in particular is important, as it is used for perspective-correct interpolation of vertex data. If you set it to one, then you won't get proper perspective-correct interpolation.