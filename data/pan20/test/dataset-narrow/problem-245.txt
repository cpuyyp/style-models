Note: This query assumes that (ItemCode, FiscalYear, FiscalMonth) is unique. I haven't tested these queries, so they may contain spelling (or thinking) errors. Best of luck. 

I've used "int" as a datatype in my example, but datetime2 will work as well. You can place more than one partition on the same file group if you want. Here, you'll have to do some planning with regards to how the load is distributed across different partitions, so you don't put all the I/O load on a single filegroup. A filegroup can have one or more .mdf files. A database can have one or more .ldf file. 

Assuming an item moves between some type of state or location (, ) and the timestamp of the move is in the column, you could put your query in a common table expression, along with a function that calculates a sequence number, like so: 

Basically, returns the value of , row(s) back (so n=1 returns the previous row), ordered by . Your CTE solution will scan twice, then join the two streams. This query will perform only a single scan, which is much, much more efficient. The function (and its cousin ) is available as of SQL Server 2012. Partitioning It looks like you've forgotten some type of partitioning term (iAssetID, pehaps?). The partitioning term serves to separate data points between different vehicles, in case two vehicles are out-and-about at the same time. Add this partition term to the query by changing the OVER() clause from to . Indexing To make this solution really fly, I would create the following index on : 

... however, the order of any recordset isn't guaranteed unless you provide an clause, so here's how to add a to an XML result set: 

What's probably going on is that you're running the query with different input values in your parameter. Let's look at this simplified example: 

You're basically talking about two entirely different technologies, used for two different purposes. A relational database stores information, commonly in a normalized fashion which is most suited for day-to-day processing and storage of business information. Given this storage model, you need joins to.. well, join those relational tables together when you want to compile data and properties from those different sources. OLAP cubes are not really used to store information, but rather duplicate and re-organize the information in a denormalized way so as to allow for typical analytical queries, using MDX for instance. What makes OLAP cubes so fast is that a lot of aggregates are pre-calculated, and this comes at a potentially great cost in storage. Excel is not a realiable OLAP tool, imho - it can act as an interface to cubes, but if you try to use PowerPivot with any larger set of data, you'll very quickly find your workstation out of memory. On a final note, I wouldn't say that cubes and tabular models are "omnipresent". I would say that they can often be found for specific end-user applications, and as such they are often stripped down to a minimum of dimensions, attributes and measures in order to avoid confusing the users. Power-users will often go directly to the relational database to get what they need. 

As @MarkSinkinson pointed out in another answer, avoid (user-defined) scalar functions for performance and parallelisation reasons. 

I think that this is a business-driven question that your client/management should decide. Most well-designed databases can scale a lot larger than you may think at first. It's good that you're thinking of an archiving strategy, but you should really only build this on an actual, outspoken business requirement. I'm almost certain that with a little index tuning, your server will cope for decades to come. On a side note, don't confuse archiving with data warehousing (sorry if you think I'm nitpicking). You can archive old data from your e-commerce database, perhaps into a database that has more or less the exact same schema as the production database. A data warehouse could function as an archive as well, but it has a different schema and purpose than the production database, in that it is optimized for reporting and analytics. Remember that whether a data warehouse functions as an archive will affect your choice of backup strategy, etc. 

There are a lot of common misconceptions when it comes to tempdb and shrinking databases. For starters, if tempdb grows beyond 350 GB, it's because it needs to. With that kind of size growth, I'd say there's an ETL job or manual batches that aren't behaving well. You could fix that in a number of ways, the best of which are (matter of my opinion) 

If every process accesses all the objects in the same order, the example above doesn't result in a deadlock: 

When calling , you can set and place the HTML source code in the parameter. Important: I'm not sure if there will be a plain-text version of your message for recipients that can't or won't read HTML formatted e-mail. Like Mark says, you should check the documentation for sp_send_dbmail for more information. Also, if you want the challenge, you could use the construct in a statement to generate an XML document where the elements are named and arranged like HTML tags. 

I also removed Status1 from your aggregate, because it would create a separate row for each occurrence of employee and Status1 - what you want is one row per employee only. 

.. but if you do, consider including any columns that the query may need, in order to create what's known as a covering index. 

If the cast fails, the result of TRY_CAST will be NULL and the comparison is false, but it won't crash. TRY_CAST requires SQL Server 2012 or newer. It's worth mentioning that you're potentially opening up a pit of bad things when you allow text values in a numeric column, but judging from the question, you already know that. :) 

Is there a practical way of doing this? I would use /, but this is harder to implement with the automated build process, and given that I would first like to inspect the results (from a different database connection) before committing or rolling back. 

No. To my knowledge, changing a column that is included in an index is not possible (you'll get an error message to that effect). You would have to drop the index before changing the column, and then re-apply the index again. The only exception to this that I am aware of is that you can change a column from to without having to drop the index (but not the other way around). However, if you use SSMS to change a table, SSMS may drop and re-create the index for you, but it shouldn't drop the index either way. On a side note, if you have an indexed view, modifying the view till drop any indexes on the view. 

The following solution uses a common table expression that scans the table once. In this scan, the "next" points level is found using the window function, so you have (from the row) and (the next for the current ). After that, you can simply join the common table expression, , on and the / range, like so: 

Actually, you can achieve the same thing by changing your to a . While the statement is actually a pretty neat way to do "upserts" in SQL Server, there's nothing to stop you from using it just for the purpose of inserting: 

"Non-blocking" in this context means that the query doesn't need to buffer any significant amounts of data (like, for instance, a sort or hash aggregate would), which means it (a) starts returning rows immediately, and (b) consumes practically no working memory. 

For the third one of those four to use an Index Seek, you're going to need a second index on , though. Here's how your query might look with these changes (including my refactoring of the query to make it more readable). 

The "last updated" columns in the remote tables will help you considerably, in that you won't have to download the entire database's contents every time you run the ETL process. Without being able to install or modify anything on the remote server, there's no practical way to synchronize in "real time". The frequency and speed that you can synchronize the databases will probably come down to: 

In my mind, that should give you the same result, but it's late here, so you'll have to verify the results yourself. :) Here's my query plan: 

Like @ypercube suggest, aggregate both sides separately, then join them. In the following code, I've intentionally used to cater for the possibility that a project may not have a benefit or cost. 

I added to eliminate key lookups. You can try the query with or without this hint. If I had a lot more time to work on this and it was a super-critical query, I would probably (a) remodel the tables and/or (b) consider storing the data in #FORMULAS as XML data and add XML indexes, which would eliminate the LIKE match. Footnote: Text matching like you do here ( and are pretty much the same here) will include formulas with the parameters "parameter10" and "parameter11" even when you're just searching for "parameter1". 

Most database platforms have different algorithms when joining tables. How tables (or streams of data, really) are joined depends on a number of factors, including the number of rows, the ordering of the rows and the type of join (for instance, if it's a so-called "equi-join" or not). Assuming the following example: 

The stuff that is probably taking a lot of time is a large number of operations in your query plan. You can preempt those by sorting the data yourself, in the form of indexes. Here are some index suggestions that I think would get you started: 

In the first query, we loop over a table, row-by-row. In the second query, the entire calculation is done with the entire table as a single "set", i.e. set-based. Obviously, most cursor-based patterns will be a lot more complex, particularly if they launch a stored procedure for each row in the cursor. There isn't a single trick for how to convert a cursor-based solution to set-based one. 

Generally speaking, in a SELECT operation, SQL Server will prefer a non-clustered index to a clustered index, all other things equal. The reason is that non-clustered indexes typically are "narrower", and as such generate fewer I/Os. Locking keys or pages in a non-clustered index instead of the clustered index may reduce locking conflicts (and, down the road, deadlocks) in specific circumstances. The downside, on the other hand, is that having to maintain both a clustered and a non-clustered index will make change operations (INSERT, UPDATE, DELETE) take more work and more time, prolonging any locks that they create, which can increase the probability of deadlocks. 

.. you can add a new partition by "splitting" the last range from (30 to infinity) to (30-39) and (40 to infinity). Here's the syntax: 

Simply for query performance, I would consider sorting the datetr column DESC in this index, although that may not be optimal for INSERTs into the table. Alternatively, you could just INCLUDE the datetr column: