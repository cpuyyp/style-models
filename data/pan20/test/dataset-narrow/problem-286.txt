I have a single instance database server with several databases. Some do not require log shipping and some do require log shipping. Is there a way to edit Ola's LOG backup job to ignore the databases I have configured for log shipping and backup all the remaining log files? The only way I can think to do this without turning up a new server or create a secondary instance is not using Ola's script and create my own custom maintenance plans. 

Further, does anyone know of a TSQL script or a (free) program that'll script all logins from the primary server? 

When I posted this question, I had all, but “NetFx3-ServerCore-WOW64” enabled. Further, my problem was an assumption on my part. I assumed that these features would be installed along with SQL setup, this wasn’t the case from the CORE installation, you need to make sure these features are installed/enabled BEFORE the SQL 2012 installation begins. When I performed the GUI-based installations, I didn’t have worry about enabling the aforementioned features. Moral of the story…DON’T ASSUME ANYTHING! ALWAYS check prerequisites if you’re having installation problems for any software/hardware. 

I know that there are times where the vendor would rather their builtin solution manage the maintenance. I'm sure I could find that feature in the application and turn it off, but this question is in case that I can not. How would I configure or edit this job to ignore a specific database or a list of databases in the same instance? 

I believe that I have most of the work done for whoever wants to tackle this question. I have the following query built: 

If anything else is needed, let me know. Edit #1 5-10-15 1:15pm CST When I execute the dir command in the folder suggested by Shanky, I get: 

What I've had to do to fix the issue is delete every instance of 'MyReportAcct' on the server, then recreate it, then add to each database. This doesn't happen all the time, but often enough that it is irritating. Further, we shouldn't have to do this at all. I think that we should change this account to an Windows Account instead of a local SQL Login in every instance. I have a feeling that this might fix the issue, but I need to prove why or provide a reasonable answer. I think part of the problem is the mirroring in some way, but I don't know how to prove that either. Security is a weakness of mine and I'm employing steps to change this, so any insight you can provide would be greatly appreciated. Please let me know if you need more information. 

I've never seen this before and wasn't able to find much during research. I've verified that the associated service account has permissions to the backup folders and there is more than enough space. Other details 

Our Process: We perform/make/implement our updates to the SSIS packages using Visual Studio (v 15.7.0) on the local SSIS SQL server (Microsoft Windows Server 2016 Standard) save the package to a file system drive not located on the server. We then open SSMS on the local SSIS SQL server (v 17.6) connect to the Integration Services on the Local (SSIS SQL Server) and import the File System Package into the “File System Stored Packages” using the GUI. i.e. Right clicking the “File System” note and selecting Import: 

I've searched the forums here at dba.se and other places on the Net and haven't found a specific answer. I know this question looks similar to many, but the ones that I've found only address how to shrink a log file, which I already know how to perform. I happen to be in one of those unique positions that a log shrink is necessary. My specific question is: When I perform a log shrink on the primary in a mirrored set, do I then failover and perform a log shrink on the "new" primary? Or does this somehow replicate the changes to the secondary, making the manually failover unnecessary? 

I'm in the process of building the lab for 70-462 exam. On page 49 of the text, it provides the command to install SQL 2012 on Windows Server Core: 

There are couple of ways that you can verify that restores are happening as intended. The following query will help find gaps in the log restore process: 

I take the LSAlert job with a grain of salt. Troubleshooting that issue made me pull my hair out! In the end, the log backup, copy, and restore jobs were functioning as expected. Here are a few things to consider when you're getting that LSAlert error messages on your monitor server: 

Questions: - Is there a Jedi trick to gain SysAdmin access or will I have to contact our consultant? - How does this happen? I thought there was some sort of failsafe to keep this from happening. 

We use Quest Spotlight for database server monitoring. Depending on when SQL server decides to fail over automatically, we will get an alert stating that a database has not been backed up in over three days (the factory default) if that server has not been primary in over 72 hours. My suspicion is that SQL Server does NOT replicate this data, but I wanted to hear from the community since a quick Google search didn't provide anything. Plus, I am travelling and do not have time to test and I need to provide a fairly immediate response. 

Note: We are only able to import into the SQL Integration Services (SQL version 2017 14.0.3023) is the SQL Server Management Studio v17.6, 17.5, 17.4. The older SSMS’ cannot import into the newer SSIS service. We think this problem is originating in Visual studio because we can create the package and import it using VS 9 but not (Since May 3rd) VS 15.7.0. . .If we create a NEW package. . and leave it "blank" it can be imported .. if we make the package with VS 9 and "upgrade it to VS 15 it is importable. . but the second we make "real" modifications it is unable to be imported? We also have an inclination that it might be our version of Integration Services that is unable to import the newer packages. Possibly an incompatibility issue? We have checked and there were no “visible” changes/upgrades to Visual Studio since early March. We also checked the server for Microsoft patches and updates for the period in question and found the following: 

I've done some searching and haven't come up with much. Further, I don't know how to access this "summary.txt" file as it isn't anywhere to be found on the C:\ drive, so I can't see what file is stating. Other details: 

Yes, you can do what you're asking. You should have your database in full recovery mode and then be performing log backups (via jobs/maintenance plans). $URL$ 

In short, put the OSes on slow storage and all the data/log & backup files on fast storage. I know that my environment is different that yours, but I think similar principles apply. We have a SAN and the storage pools range in different speeds, 7.2k, 10k, 15k. We install the OSes on the slower drive pools and have all backup files and data/log files stored on the 15k drives. There was a time when we accidentally put everything on the 15k drives and we didn't notice a performance difference. It was brought to our attention by our SAN admin after an audit that the OS was stored on the faster drives. We monitor SQL Server performance with perfmon and Dell Spotlight. For grins, we went and looked at the historical metrics that mattered, and saw nothing to note. 

I forgot to update this post with what ended up working for me. I don't know why, but running SSMS as Administrator allowed the backup to complete as you'd expect. 

The following is a script I found online that has most of the elements of what I desire in the result set. The only thing missing is how I can get an average of the run times over a decided time period, like a week, 10 days, 23 days, a month, etc. I've done some searching, but what I've tried has failed in some way or another. Further, my TSQL are weak, but I'm reading publications by Itzik Ben-gan to change this matter. I'd truly appreciate any help that anyone can provide. 

He told me that he used to be able to use his Active Directory account or a SQL Server SysAdmin account to create new sites, now he can't. I'm pretty sure that his AD account was not an admin of any kind on the SQL Servers regarding the SQL 2005 configuration. Further, he stated that he used a SQL Server SysAdmin account (in the SQL 2014 configuration) but it still failed. 

I've had a maintenance plan in place for years that has run without fail and the associate logs backup this claim. Also, the backups are time stamped and in the place expected. I did a restore in a test environment and ran a consistency check without any errors. I'm not sure that I really see this as a true error or cause for concern considering the aforementioned evidence. So, maybe this is just an anomalous event? Either way, I'd like to poke around and a assuage my sensibilities. Has anyone ran into this situation before? If so, what did you do to investigate? 

You're correct about how the VLFs grow in size. Check out the following video from Jes Borland for some more tidbits. How SQL Server Works: Log File (Video) 

I've setup log shipping between two servers (they used to be mirrored). What is the simplest or most efficient way to mirror the system databases between the primary and secondary? 

Situation: We recently had a consultant company build new Lync 2013 Servers in a mirror setup. There are two security principles set, SA (disabled), and a AD group that is only associated with the Public server role. 

I have performed an intensive search on this error message and have not found much of anything. The only gem I found was to use Process Explorer from the SysInternals suite by Mark Russinovich. However, isolating the SQLAgent.exe binary did not show me what other process is using it; the process is lone. I am either performing the incorrect steps or my process is completely wrong. Other items to note: 

I've ran into this same problem! As Mr. Vernon stated, the account needs admin access to the server. However, that is not the recommended way of providing access to the service account, especially if you're to follow the "least privilege" security mantra. Instead, perform the following: 

I'm migrating some TSQL code from one server to another. The server I'm moving it from is production, so I'm not easily able to adjust settings for obvious reasons. The code runs fine on the production server, but the firewall is off, we don't desire this configuration and want the firewall enabled on the new server. How can I leverage Extended Events to tell me which ports are being used during TSQL code execution? Or do I need to use something like WireShark? SQL Server Version: 12.0.4213 Windows Version: 6.3.9600 N/A Build 9600 (2012 R2 Standard) VMWare VM 

Having 10s of thousands of error messages per week makes it pretty difficult to peruse and check other possible issues 

I'm simply looking for what most folks do to keep their SSRS servers in top shape. Beyond performing backups of the databases, symmetric key, custom extensions, configuration files, IIS settings, RDLs, SSL certs, etc. are there other tasks I can perform? I'm really new to SSRS and I've been asked to do some research to keep our 4 servers in the best condition that I can. I know this topic may seem subjective, but there has to be a general list of things that most SSRS pros use to make sure everything is running in tip top shape.