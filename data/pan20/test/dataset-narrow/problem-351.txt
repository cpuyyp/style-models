The COUNT technique that was suggested here should work, but the problem is that your table structure and data mess it up. You did not define any keys, and you have duplicates in the demo data - for example, course 101 enrollment by student 4 appears twice, on the same date, with different grades. Proper declaration of keys would have not allowed it. Changing the predicate to would also give you the result that you were looking for, taking into account that there may be multiple registrations. The generalized problem you are facing is called 'relational division'. Search here and you will find many discussions on the topic. Typically, the most efficient way to solve relational division problems is to use a negative formulation of the question. It seems weird at first, but once you get the hang of it, you can't go back... In your case the question is: 

You concern yourself for nothing. "it's a bottleneck since of each row processing" is an assumption you make, and not necessarily what the optimizer will choose to do, even if it looks to you that you write it that way. SQL is a declarative language, and as such you tell the engine what you want done, and not how you want it done, like you do in imperative languages. The query optimizer, especially in SQL Server, is a mind blowing wizard in moving things around, considering alternatives, and understanding what it is that you want, regardless of how you write it. It doesn't get it 100% right, but it is damn good. Write your query, use several syntax like you did is never a bad idea, and test it against a representative data set. If you see performance challenges, examine the execution plan, or post it here so people can help you. HTH 

For some reason, MS decided to deprecate the feature, but it still works. See the GROUP BY topic in the documentation. 

Depends on what data you store in it. If you store a null or an empty string, it will take around 2 bytes. If you use all 50 characters, it will take around 102 bytes. See the documentation here. 

Look at the and you will see the difference. Oracle uses a cost based optimizer that takes into account the actual values that are used in the query to come up with the optimal execution plan. I have not tested it in Oracle, but it is most likely that is evaluated at compile/optimize time, and the optimizer is aware that your predicate only covers one day, and therefor it will make sense to use an index for example. When you 'hide' the value in a function, the optimizer has no way to tell what value you use, as the function is only evaluated at execute time. If the general density of the column suggests that the average value will return many rows, and will benefit from a full table scan for example, you will get a full scan even for 1 day's worth of data, which may be less optimal for the value you are using. As a rule of thumb - don't 'hide' predicate values from the optimizer by placing them in functions. 

There is an option to prevent some commands that contain specific words from being logged. One common practice is not to log commands that has the word . This option is called More info: $URL$ $URL$ 

You create an extra table with customer ID and all other fields that will possibly change over the time. In the orders table, you link to the id of this new table, not to the costumers tables. 

Queries that do "full table scan" are the ones that don't use indexes. Try to log them in the slow query log using this option log_queries_not_using_indexes Be careful though that small tables that have frequent queries running against will fill your slow query log files. You may want to enable this option for limited amount of time. To reduce this chance, you may set min_examined_row_limit variable to a reasonable value, depending on your small tables. 

Replication may fail because of different factors. You don't want to shot down the site each time replication fails. Instead, you should think of a solution for this situation: when replication fails, how to keep the site up. It seems that you have master-master setup for the replication. I suggest to let users (application) connect to the closer server when they are reading, and perform all writing operations on one master, call it "active master". This way, you gain the speed of reading, and you know that the replication is going in one direction. If the replication fails, you fix it while the site is still up. On the other hand, if the master fails, you switch masters: becomes the , and you fix the failed one, then resume the replication. Having said that, there are some situations where you may want to shut down the site. HTH 

You can run the following statement in a loop in a script that triggers the statement every 10 seconds for example. You can customize it to give you more or less info depending on the query you issue. 

In MySQL, or almost any other RDBMS, it is common to have an ID for the row (object), so you use it in other tables to make a link between the tables (Relations, or PK and FK if you prefer). This is part of the normalization process that is recommended for most types of DB design [there are exceptions though]. In MongoDB however, the general way or design is to denormalize. i.e. the document (row) is stored with all its related objects. To make it more clear: In MySQL you would have two tables: and . In cars you only store the ID of the type of this car. In Mongo DB, you would have one collection called s, and the document in this collection would have a sub-document that contains the type's info. (You still can have a separate collection for , but you don't store only in car's collection) HTH 

InnoDB tables tend to occupy more space than MyISAM tables do. While converting, new physical files are being created, so you have to have free space on the disk (Hard to estimate, but to be safe I would say 120G) If you use Percona tools to make the change online, all modifications on the original table will be stored in a temporary table using triggers. So depending on the DML operations,i.e. inserts/updates/deletes, you may need more space. Be careful that on high traffic tables, triggers may affect the performance. It could be that some InnoDB related configuration should be tuned. 

You are absolutely correct that if you design your data model and the resulting schema very well to begin with, it will already be in at least 3rd normal form. That said, I believe that it is crucial to understand the relational model, and the normalization rules as helping guides. When you say: 

In a relational model every relation describes one 'thing' - the 'Fruits' table models fruits, fruits are not blends. Your Fruits table with a single column, is all key - 5NF. If you don't have a Fruits table, how will you insert a new fruit which has no blend yet? What happens to a fruit you have in stock, but its blend is removed? 

You need to be looking at SET Operators. Something along the lines of the following query should do what you are looking for: 

An added bonus, is that it also works around the COUNT issue you experienced earlier. Check the execution plans with large data sets, and you will see that this one is easy to index, and will typically perform better than the COUNT technique. The reason is that EXISTS doesn't need to complete the operation - once a row is found, the predicate is TRUE and there is no need to keep processing. With the COUNT, all aggregates have to be calculated. Hope that helps~! 

Check out this article. I think it will answer your questions and give you the tools to deal with over-inflated number of auto-statistics. The author suggests dropping ALL auto-created stats occasionally, as the one that are required will be recreated when the first query that needs it is executed. It explains the risks / benefits. 

UNTESTED - You did not provide any DDL or sample data scripts... IF is a flow control construct that determines which statement block will be executed. What you need are Conditional Expressions. In your case, the function should do the trick, also look at for non null condition predicates. 

You are using the 3rd normal form. A relation in 3NF, must also be in 2NF and 1NF. You are asking if you need tools to validate your design, which is something no one can answer. If you design a car, will you use available tools to validate that it is safe, fast, and reliable? or will you just trust that your original design achieved these goals without testing it with any tools? 

The technique you are describing for representing task hierarchy is called 'Adjacency list'. Although it is the most intuitive to humans, it doesn't lend itself to very efficient querying in SQL. Other techniques include path enumeration (aka materialized paths) and nested sets. To learn about some other techniques, read this post or search the web for numerous articles on these techniques. SQL Server offers a native hierarchy representation for path enumeration. This is most likely your best bet... 

Here is an excellent article that discusses all the aspects of the new cardinality estimator. Bottom line - as with all changes, you win some, you lose some, and hopefully overall you win more than you lose. With the new cardinality estimator, it is a huge win overall, in spite of the occasional hickups like you are experiencing. You have several work arounds other than the TF, and they are discussed in the article. Easiest should be either using a query hint, not to lose the benefits of the new estimator. 

You may use REPLACE INTO. The disadvantage of it is that it creats high IO, as each existing record will be deleted and then inserted (as opposed to being updated). Try loading the new rows' IDs into a separate table on the destination server, then run a delete on the destination joining this new table with the existing table using the ID. After that you run your ETL with s only 

Collation names that are case insensitive end with "_ci". Click here for more info about Collations. 

I tried it and it is working as expected: $URL$ However, some clients deals with every one statement as a single transaction. Try to add all of your code to one transaction and execute it. I tested that in MySQL client (command line in Linux) and on SQL Fiddle (Link above) 

The result would be one of the following values: , , or . More info. In the case of format, every changed row will add info to the bin log. If there are statements that affect a table of 1000000 rows, all these changes will be written to the binlog. For example, suppose you have a huge table, with 1M rows, and you run a statement like this: 

It is essential not to close terminal 1 before you are done with step #2. Notice please that there is no need to stop the slave. This link would be very helpful. It explains with more details, using LVM. HTH 

The slowness you're experiencing could be because of several reasons, one or more. It is important to note that 20K rows is relatively small number of rows. You may want to check for the following: 

Therefore, for replication purposes, slave's binary files are not needed. And, yes, you can disable binary logging or purge these files if not needed for other purposes like "backup". 

I have the following way to get the IDs of the rows that have at least two not null values; however, I am afraid this is not efficient when it comes to few million rows per table. I'd like to share it anyway: 

Try adding a specific field to order by. That field is the value of the month if it is greater than the value of current month, and add 12 if it is less (or equal), then order by that field: 

Create a table with all the fields that exist in the csv file. Create a trigger (After insert on the table created in step 1) that calls your procedure that you already have, or let the trigger itself insert values to the corresponding tables. Load the CSV file to the table you created in step 1 The other tables will be populated automatically (Nothing to do here) Drop the table that you created in step 1 

While there is more than one way to do that (including the one you mentioned), if you can temporarily stop the traffic to the DB, I suggest this way: 

Alter the table to add an MD5 field: Update this field: Add the unique index: Either create a trigger, or do it at application level, to update this new field after each insert/update operation. 

I am thinking of two things to try. If you get faster execution time, then the slowness would be related to the distribution of the data. If you still get slow execution time, it maybe then related to the server configs.