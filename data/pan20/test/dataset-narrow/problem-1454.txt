A user keeps a game on his mobile phone on average 20 days. So for the most part, adding ads is not going to be noticed much after launch. However, it is going to affect your biggest fan the most, which is the segment you don't really want to piss off. There are ways to mitigate the effect however. #1 is to only show ads to people who downloaded the game after version X, so old player won't see any ads. Also, do not show ads to players who purchased something (so no ads in premium games and no ads to a user who dropped $5 on virtual goods). I have retroactively added ads into a popular game. The effect was some bad reviews, but for the most part people expect ads - as long as they are not over the top annoying. The rating didn't really drop when I added the ads. Maybe 0.1 points, but that's it. Also you can let the users know that you need the money to make another game... lots of people are quite understanding of the plea of the indie developer. 

Just like the particle system solution, you can use 2dtoolkit to create the same effect. Add 3 children game object to the character with only a sprite attached to them. Change the alpha and coloring as needed. Then you can vary the sprite's local positions depending on the speed of the character: 

Solved Thanks to everyone who helped me with the issue. There were a few factors in play. First, there was the scale. What I did to figure this one is create an empty scene and put 2 cubes in there. I varied the size and position of the cubes (from 68x35 to 0.65x0.35 for size and the position by factors or 10 as well) and attached a script to move them, with variation in speed by factors or 10 as well. So all else being equal (same size on the screen and time to cross it), the size of the objects elected a different response to collision: big object got interpenetration, small ones none. I am not familiar enough with Unity to explain the cause of that, but it is reproductible. A possible second factor was the shape of my colliders and how it may be possible that they screwed up the ejecting part. The colliders I had were not overlapping, so I simplified them to 2 boxes in the shape of a cross. Finally I was setting the position of the objects manually in FixedUpdate and that caused some problems with interpenetration at lower scale (at higher scale, it didn't appear to matter). I changed that to work with the body velocity and it helped. 

I'm in a bit of crunch time and I find myself spending way too much time tinkering with an algo, so I would like some help. In the game I am working on, there are some old-style, pixelated minigames. One of the minigame is a spaceship in a cavern. The caverns curves and narrows down over time until the ship crashes in a wall. I want them to be generated at random during runtime. I'm having a problem creating the walls of the cavern and narrowing them while keeping a smooth curve on them. For the narrowing, I keep track of it with a variable that decreases over time, but how about the curving and how to keep it natural? I've though of a keeping a target point that goes up and down randomly and have the wall try to reach for it, therefore smoothing the randomized number but it is not working great. Any ideas/algo? 

It wasn't specified if you want to do a game professionally or as a hobby. For a professional game, you need professional graphics. Minecraft is a great counter example, but an exception to the rule. As a programmer making games for a living, I get the graphics from a paid artist and from online websites that sells 3d models, images and sound. For a hobby, there are free online resources as well as doing your own art. However, if you are anything like me, my art sucks, takes too long to do and I don't like the end result of the game. That means I lose my mojo doing stuff I don't like vs stuff I do like and I am not motivated by my results... which means: I get it from good sources. Get at least decent looking placeholder for your graphics. Then figure out how to get a better version, if you reach the point when you are ready to release, etc.. Don't force yourself to do stuff that will demotivate you - whatever it is. Motivation is the only thing that will make you finish your game and finishing the game will be the most important thing for you to accomplish. Do not squander motivation on graphics if it is not your thing, but realize that a nice looking UI will motivate you. 

If you are not using OpenGL, then most games load sprite sheets with rotation already done (see other image).. You only need a quadrant worth of rotation pre-calculated. It saves memory and you can rotate on the fly to 90/180/270 without loosing quality to cover the other 4 quadrant (going back to the first figure, notice that the grids will align perfectly with rotations of 90/180/270 degrees). 

What you are trying to do is also fairly similar to what older MUDs did. I am more familiar with the Diku variety and I'll explain it briefly here. If you think that is what you are looking for, you can look up the source code, it is fairly straightforward. First, the different stats and skills are all hardcoded. Strength is a concept known at the code level. Same for hp, armor, stealth, fire resist and so on. All the skills like backstab, spells, hide, etc., also exist in code. All the rest of the data is loaded from a database, even races and classes. So a character structure would be 

To create an atlas (without scaling the lightmaps), create an image that is, say 2x larger than your lightmap on each edge. Then you've got a 2x2 grid of lightmaps. Dump the lightmaps into it, and adjust the texture coordinates based upon which lightmap they came from. To directly answer your question, yes -- group close objects in the same lightmap. If you reverse the situation (have far away objects in the same lightmap) then you introduce performance problems almost immediately and unnecessarily. Resolution is actually much less of a problem for lightmaps since it provides only subtle detail rather than the actual high frequency visual information such as bricks/concrete. Scaling: don't scale things outside of the editor after generating lightmaps. Generally, you only generate lightmaps for static geometry anyways, so I don't know why you'd want to scale it. If it must be scalable, then you need some other [dynamic] solution since scaling a model can entirely change where shadows would normally occur in the lightmap generation process. In other words, it will look not just "pixely" or "blurry", but straight up wrong. Any lightmap generation program that isn't brain dead should use area of the triangle as a way of generation "priority" for texel space. For example, if you had 4 triangles in your scene that had 2500, 300, 500, and 200 units of area, you'd expect the triangle with 2500 area to have more texel space in the lightmap allocated for it compared to the others, since increased resolution will actually be visible in the larger triangle. 

Enable blending, possibly set up texture environment. Set up the blending function Draw rectangle 1 with left vertices having alpha of 1, right side alpha of 0 Switch blend function to GL_SRC_ALPHA / GL_ONE (thanks Trevor) Draw rectangle 2 with right vertices having alpha of 1, left side alpha of 0 Turn off blending and do other stuff. 

Yes, in general, to it being a good idea. OpenGL generally has transparent loss mechanisms, that is, if a texture is lost, you won't notice it. The idea of making multiple textures into one large texture has been explored. This is called "Texture Atlasing". There are faults with it if you try to make an atlas that is greater than the maximum texture size of the graphics card (which on modern cards is quite large, 8Kx8K or so). There are pathological cases where texture atlasing will not improve performance, but with discrete graphics cards offering >> 512MB of video memory on low end cards, I don't think you'll run into any of these type problems. I wouldn't concern yourself too much with performance optimizations if you are developing for the desktop systems until it is clear that they are needed -- and what an epic title it must be then! ;) 

When you specify vertex colors, the alpha value is smoothly interpolated across the rectangle. Using blending, we can multiply the color by that alpha value Since we swap 1.0/0.0 alpha values depending on which rectangle we're drawing, for each pixel where alpha is some value for rectangle 1, it will be for rectangle 2. 

I can't read all of the source code (dang firewalls), but a display list is something that you compile once, then execute many times. What you're doing is telling OpenGL to regenerate an optimized execution list containing such and such triangle data every frame. That isn't what display lists were* used for -- they originally were more for things like old SGI workstations that could literally compile OpenGL commands into GPU command buffers and execute them. Even if you were executing this code on an SGI machine, this would perform terribly because you're re-creating this display list! Modern GPUs don't function like this. If your data is in a VBO, doing a display list means OpenGL must read the data back to create it. JUST SAY NO. Do a glDrawElements() and be done with it. Nothing fancy. As an aside, display lists are gone in later versions of GL, so definitely don't use them. It pretty much guarantees problems going forward. * "were", as in, don't do it now! Ever! No! Nope! 

Rendering on a separate thread is a common idiom. To quickly and directly answer your question about "Was I just doing something wrong or is it possible to load textures and buffers, while it's rendering?" - OpenGL ES has a rendering context bound to one thread and calling any OpenGL ES functions from any other thread is illegal. More on that in a bit. Quake is a single threaded program, and written for DOS originally. Its code is fairly specific to the game, but basically the menus are built-in and simple rendered on top of the 3D graphics (if any). It has been a while since I've read through the source, but conceptually, the game loop is this: 

You are taking a 512x512 image and smooshing it into an area approximately 120x120 pixels. Thus, you should expect that each pixel is about the average of a 5x5 block. You're using linear texture filtering so that would be expected. It might be better to use unscaled images if you're going to reduce the resolution by such a large amount. You can control the quality easily since what you see is what you get. Aside from that it will load faster and run faster on phones. A 512x512 image requires an immense amount of memory bandwidth to draw compared to say 128x128 (16x as much, in fact) which is a performance killer on mobile GPUs. 

Repeat for each of the four points, and then find max/min boundaries of the AABB. -- This is super tedious. The better way is to use 3x3 matrix representing rotation and translation. In effect, you take each of the four points, translate them back to origin by subtract the center's coordinates from them so that their new center is on (0,0), then rotate them. Finally, you translate them back to the real center. With the new set of points, once again, simply compute new the AABB for those points. Wikipedia has some great information about matrix math, especially how to do rotations, translations, and more using 3x3 matrices.