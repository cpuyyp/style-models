You can use a wildcard certificate with multiple subdomains across multiple IP addresses. While there is no technical limitation, often Certificate Authorities have licensing restrictions on thier usage. 

I have had incredible success with Barracuda's "spam firewall" product, which you could offer to your customers as you describe. It's a Linux appliance using a primarily Open Source stack. I have built similar stacks myself on several occasions but I have yet to run across an Open Source Web frontend for the end users to manage the spam portion of it. 

Have you looked at LVS? It will meet most of those requirements. You might have a more difficult time finding a solution for tcp session failover but I've not sought that specific requirement before, as my application architecture could reasonably handle failover without it. 

Protocols such as FTP, HTTP, NFS, and SSH. I typically only use SAMBA file sharing for conveniently transferring files between platforms. 

The command is included with many Linux distributions. In CentOS 5.5, it's part of the coreutils package. 

Others have successfully implemented HAProxy and it even helps run the StackExchange sites. Other popular Web front ends are Nginx and pound. Ultimately, most of these solutions will be quite effective for most Web traffic. If your goal is high availability and load balancing, sticky or persistent sessions are ill advised, as they reduce the effectiveness of both. Without knowing more about your architecture or type of traffic, I would recommend LVS, which is my preferred solution. You refer to the network layer, which is where this load balancing solution is more focused. It is able to be used with most protocols and is not limited to Web traffic. 

First, you should scrub real data out of your development environment. Also be aware that there are a lot of variables and options here. On the Linux development server, I would first prevent the mail from sending out by either filtering the traffic or stopping the mail daemon. To stop the traffic, this would work in most modern environments: 

There is no native .htaccess validator. If a third party tool exists, I am not sure if I would trust it. Since it is a script that modifies the file, you should be able to code the validation within your script. You can also include testing and verification processes for your deployment, which would help reduce mistakes. 

If you do not know how to build one, you will need to purchase a network device (router) capable of load balancing between multiple ISPs. With inbound traffic, one solution would be to use a dynamic routing protocol such as BGP. With outbound traffic, a solution would be to failover a VIP between multiple devices using a protocol such as HSRP. If you do not have the knowledge or resources available to build a more robust solution, you could seek a router capable of handling multiple WAN connections. You will likely find affordable SOHO equipment able to serve your needs. It is important to realize this router is going to be a single point of failure unless you introduce redundancy. Another option would be with a single server, you could potentially introduce multiple routes with different metrics. 

Without having the RHCE, it is awfully presumptive to say that it is not very advanced, especially with only ~2 years experience. You will likely find it to be more challenging than you anticipate and it is a requisite for the RHCA track. Currently, I like both the Linux Professional Institute and Red Hat's certification tracks. With LPI, I prefer it because it is distribution agnostic and has a good generalist focus. The exam style is well done and would be difficult to get through without having supporting experience. LPIC-1 and LPIC-2 are both well aligned with general Linux system administration. LPIC-3 is the higher-level track, which is more specialist focus. Historically, I have not liked its high weight on LDAP but that is changing somewhat. The "hands-on" approach with RHCE has a lot of value. Someone who is proficient in Linux but not as experienced may be surprised, as it is specifically designed to weed out people who do not have a good foundation of working knowledge. While you may be able to "figure out" all things eventually, the time limitation will prevent success without an existing proficiency. RHCA is expensive to pursue. It is something I am working on now and is time consuming as well as challenging. The specialist endorsements insure more specific knowledge of the topics they cover. With this, however, there is an unavoidable focus on Red Hat technologies and methods. There is not a lot of LPIC-3 or RHCA. They both take serious commitment and a working knowledge to successful acquire. Were I to see either of these on a person's resume, I would certainly give it additional attention automatically. For training, Red Hat has training for their exams. Third parties offer LPI training but you may find it difficult to find training specific to LPIC-3. Additional information is available on both of their Web sites. 

ntop is what I would use for what you describe. NMIS could also help too for switch or other network device monitoring. 

I am not aware of a solution to do what you accomplish, as MySQL does not natively support slave servers replicating from multiple masters. Circular replication is fragile and generally not recommended. If your main master failed, which is acting master for all the slaves, you could potentially re-point them to the secondary master. This typically involves digging through the binlogs, which can be quite tedious and is easy to make a mistake. You could use Maatkit's mk-slave-move utility to make this a bit easier. You could potentially run multiple instances of MySQL on each slave and then have heartbeat or failover logic, which would be able to switch in case of the primary master failing. This would have to have substantial logic to keep from being fragile. You could run dual master, have a slave off of each master and then load balance both slaves. Have availability verification in your load balancing to remove the slave in case of a single master failure, which would probably be better than having multiple slaves on each server. This would not scale well. Allegedly this set of scripts helps with this type of configuration but I have no experience with them. If you want high availability for readonly queries, I would recommend having those more important queries run against the dual master highly available servers. For queries that do not need to be realtime, it makes sense to have them run against the multiple load balanced slaves, which could potentially not have current data in case of the master failing. 

Altering a large table is going to take some time. You should have tested your changes before running them against production. I suspect it was a table that is written to. When you started the alteration, it became locked. When connections attempt to write to it, they wait for it to unlock. Eventually, all available connections filled waiting for the table to unlock. If you don't want to risk data corruption, wait for it to stop. Otherwise, you can kill the alter but rollback can potentially take a while as well. If you stop the database, you risk data corruption. It may recover but taking actions like that are further irresponsible. 

You will probably be able to SSH to it, as this is installed and enabled per default on most modern UNIX servers. Before that, telnet was the common method. If you are unable to connect by either of those protocols, you could run nmap against the server to determine what ports it is listening on. If you need to change the root password, you can boot into single user mode by appending "single" to the boot parameters in both LILO and GRUB. 

If the inode was dereferenced, you might be able to recover the data. If the blocks were overwritten, it will require forensics recovery like Matt identified. My preferred method is to use , which can be used to access inodes that have been dereferenced by not yet overwritten. The command is key. With some simple scripts, you can create hardlinks to all inodes output by lsdel and start groking the data. There's also other tricks, such as grepping the filesystem or finding the inode using a different method. Good luck and be careful. Low level utilities can easily produce disastrous results without caution. 

5.5 is a development release and not suitable for production. Unless you have a specific reason to, you should not even consider running 5.5 for any production use. 

eBay and Craigslist are your best options unless you want to seek out local resources outside of Craigslist. 

While fenix's auditd recommendation seems ideal, you may find a filesystem IDS such as AIDE helpful. Unfortunately, it's unlikely to be fine-grained enough for what you're attempting to isolate. I'll often write scripts as a solution for problems like what you describe. If you cannot accomplish what you want with solutions recommended, write something yourself. It's often not very complicated. 

expects the password hashed using crypt. Use the command to set the password interactively. More specifically: 

It is advised to use a protocol that uses encryption for any authentication that utilizes a secret token. FTP is largely considered obsolete for anything short of anonymous file distribution. In UNIX, we'd typically use scp/sftp. There's a Cygwin port for Windows, which I've used in production environments. I suspect in Windows environments protocols like CIFS are often used for code deployment as well. However, that would be in a segregated network and not over the Internet. 

A good way to get debug output with is to use . Typically, any results will be output to STDOUT or STDERR. How that is captured will depend on how your script is being executed. As per the bash(1) manpage: 

In more detail, the performance you describe is typically considered the directory in FTP land. For any special handling of the uploads, that can be configured in the daemon. As far as having it world writable, that's not entirely necessary. If you want literally everyone (such as anonymous) to be able to write to it, it is. However, if it's a limited subset of known users, you can create a group, add all the users to the group, set permissions 775 or 770 with the SGID bit set. 

The MD3000i is the older 3Gbps bus. I'd at least recommend the 3200 series to get 6Gbps if you're making a new investment. If you have a bigger budget, I'd go with EMC. Otherwise, the Dell, IBM and HP devices are perfectly acceptable. EqualLogic makes some very nice stuff and they fall right in the middle with cost. If you want the most cutting edge, consider 10Gbps. Something more than a switch, like what? I can't imagine what unless we're talking fiber channel and then it would just be a host bus adapter in addition to the fiber channel switch. 

What are the symlinks to? What are the permissions on the data being symlinked? What user and group does Apache run as in your config? 

Using RAID5 with this configuration, you introduce additional risk to your RAID. With the storage density introduced with modern disks, the likelihood of encountering a bad block on multiple disks is higher. If using 1TB+ disks, it's recommended to use a RAID6 as opposed to RAID5, as it has an additional parity disk. If you want greater speed as well as better guaranteed availability, you might consider RAID 1+0 as well. As far as speed, compare the seek time and various other specifications. Speed does not vary much with commodity disks that are higher density. When I bought disks a few months ago, the largest tier1 SAS disk I could buy was 450GB. 

You are best sticking with the dump and restore. When restoring, make sure your database is empty and you should not have any id conflicts if the dump was created properly. 

The MySQL manual is fantastic documentation. You would run the client if it is already installed in your system. Recommendations specific to your distribution package cannot be provided without your distribution being identified. If you want to use one of MySQL's releases directly, there will be a and other incredibly verbose documentation within the tarball. 

It is often a good idea to sync the BIOS clock if there is that much of an offset after changing. This can be done via: 

NFS uses the filesystem permissions across systems. A reasonable solution would to use a group that all users were members of and set the SGID bit on the directory as well. Public writable is for /tmp. If you must, at least set the sticky bit. 

The SLA reporting should accurately reflect reality. If you are measuring availability from a user perspective and only the server doing the measuring is experiencing issues, reporting that issue within your SLA would not reflect the user experience. I can understand wanting to hold the source information to a high standard, perhaps always reporting it even if inaccurate but with a note identifying why. If you cannot come to agreement, perhaps there is a technical solution to make the measuring server less fallible. If the information is reported as an outage and it was not, what value does the reporting provide? In my environment, we report from multiple sources. An external monitoring methodology to report availability from an external perspective as well as reporting our internal outage recording system, which is human entered and considers multiple factors that most accurately reflect the situation. 

With an older 2650 I was able to use afasnmp. Unfortunately, I will not be able to give you a definitive recommendation for all chipsets. Edit Hm, I get really frustrated with Dell sometimes and their tendency to default to Windows approaches on UNIX. Don't get me started on the MD3000. It sounds like if you run MegaCLI from command line, it produces the status as desired. I hack out scripts in Nagios all the time. I'll tie a script on the source server, often a simple shell script, it will be in SNMP. On the Nagios server, I'll use a PERL script to pull the mib and produce the results in a fashion that Nagios can use. Would this work for you now? 

If isn't located in a directory listed in your variable, will be unable to locate it. The is affected by the following circumstances: 

is the standard location for the system-wide bash configuration on most systems. From the bash manpage: 

Are you actually using InnoDB tablespace? within your DB will show the ENGINE. If not, I suspect disabling it will hasten startup time. Edit 3 Wordpress doesn't appear use InnoDB by default. If you can verify none of your databases are InnoDB, adding to your my.ini will likely increase startup speed. 

Create a database, create a user for the database, and populate a database. Additional details are going to be specific to the application. 

It's rather normal to change permissions after a deployment. I write scripts for code deployment. What's your deployment method now? 

I'd guess that it's not completing for whatever reason and eventually spawns multiple processes. What's the script in full? What's the log output? Provide complete and intimate details regarding why you believe what you do. If you're running a common cron daemon, I'd think it more likely that you're misinterpreting the situation. It sounds like you may be basing this theory off the access logs. If so, you could wrap wget in a script and enable additional system level logging, which could more intimately detail the behavior. But I'm guessing, as you don't provide full details. 

It's important to realize that certain content must be downloaded to be displayed, such as with graphics. Anything you add to "prevent" them being downloaded will be limited. Direct links, however, can be prevented in most cases but a clever script could still set REFERER. Flash streamed from Flash server makes it more difficult to download and hot link as well. For controlling Flash, this should probably be investigated. I like Mike's solution though, I gave him +1. 

Typically on a multi-user system you would simply set the directory executable and leave all files within the directory public readable. If you run the server yourself, you will have the ability to set the group to the same group as the Apache group and set the directory group writable. Additionally, you would want to SGID, so as that anything created within the directory inherits ownership.