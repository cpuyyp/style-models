If the point to be transformed is (x,y,z), then it has to undergo a series of frame transformations: from F2 to F1 you have a rotation against the x by an angle alpha, then from F1 to F0 = world system, you have a rotation against the y axis by an angle beta. Chaining the transforms together, you should get the final position of your point in the world system. You can see it as a double rotation joint system, the camera being attached to the x axis, which is in term attached to the y joint. 

The problem is that in your first snippet, the component got "dirty" before it could be used in your computation. It's simple matrix-vector multiplication. UPDATE If one wishes to devise a transformation chain in which the x axis moves together with the y axis, then this is achievable through the following set of transformations: 

As mh01 suggested ("Viewing the generated assembly in PIX or a similar tool can be very helpful here."), you should use a compiler tool to examine the output. In my experience, nVidia's Cg tool (Cg is still widely used today because of its cross platform capabilities) gave a perfect illustration of the behavior mentioned in the GPU gems condition codes (predication) paragraph. Thus, regardless of the trigger value, both branches were evaluated on a per fragment basis, and only at the end, the right one was placed in the output registry. Nevertheless, the computation time was wasted. Back then, I thought that branching will help performance, especially because all fragments in that shader relied on an uniform value to decide on the right branch - that did not happen as intended. So, a major caveat here ( e.g. avoid ubershaders - possibly the greatest source of branching hell). 

If an active program cannot find an uniform in neither places, you should decide whether you supply default values in the program/material file or if you somehow forbid such a possibility by throwing exceptions. 

Regarding your Mip selection, I am using a custom texture atlas shader. In my case, the following snippet is accurate enough. You do have to provide it your own texture size.. i.e. you need to know the width X height parameters of the texture you want to "mip". In my case, these are just defines.. If you use dds textures, direct x compatible, they should already have mip levels embedded. 

Scenario: several objects (o1, o2,.., on) have to be rendered with the z test disable, but the z values must be written to the depth buffer. In another pass, some other objects (t1, t2,..., tm) need to be rendered considering with the z test enabled, and considering the prior values from the previous pass. Is it possible to achieve this with Unity's material script? The goal is to support a custom order for the rendering of transparent objects (the o1,..,on in the scenario description) alongside with matte objects (t1,..., tm). I tried using the features mentioned here, but the results were incorrect, i.e. as if the z values were completely discarded inbetween passes. Can anyone, perhaps, share a code stub for this scenario? 

I'm wondering if it is possible to dynamically save / amend a current asset bundle during the running of my Unity application. I've been looking into this for a few hours, but can't find anything about how to do this. I know you can download / stream assets from a server at run time, but I'm interesting in knowing if it's possible to do the same but in the opposite direction. Has anyone ever attempted this before? Is there any good examples of how this could be done? 

Now, clearly I can't link to this as I don't have 2012.2 on my system. So, my thinking would be simply changing 2012.2 to 2014.1, but this didn't work and lead to the following error: 

I've probably messed up somewhere, but could someone please help fix the issues I'm facing. Everything else seems to running fine, it's just when I try to run the program that certain things hit the fan. 

I'm trying to make a match 3 game in C++ and I want to make a best move method to show the player the ideal position to move to get the most amount of points. The rules of the game are as follows: 

So, looking in the FBX sample directory I opened up the ViewScene sample to see what it lib it linked to and I see that it wanted the following: 

At the moment I am trying to create a tech demo in directX 9. I have a good understanding of basic lighting but, in my game/demo, I want to be able to create and light a scene like the following: 

I'm trying to convert a .fbx model to a .x for use in my DirectX 9 project. Does anyone know of any software to allow me to do this? Been googling for a few days and not really found anything that has allowed me to do this. 

Thank you so much for you in depth answers. These have provided a lot of useful information and I'm currently in the process of implementing the information you gave into my project. Though I currently have one issue I was wondering you could help me with. Please note, I know that I haven't really modified the code you have given me, I am just trying to get the build first, then modify it proper. If I can get at least a single particle to fall down with this, I'll know I'm on the right track. Right now, in my kernal class I have the following code: 

Both of which were / can be found inside my UpdateParticle() method. I had originally thought it would be a simple case of changing the h_position variable in the cudaMemcpy to m_particleList[i] but then I get the following error: 

Checking with the picture: the initial time step is t0, but when the puck still has t1 units to go, it hits the wall at P1. Now, the procedure is called again, this time t2=t1 is the time left to travel. When it hits the wall again, at P2, is still got some t3 units left to go. Again, we set the current position to P2 and reflect the velocity. Using t3 and the reflected velocity, we end up with the final position, P3. This is just one example depicting how the method should work, conceptually. Working sample Using Ogre as a render engine, I've written this terribly simplified update function: 

Is there any resource location on how to view a 3D scene from an application or a game on multiple windows or monitors? Each window should continue drawing from where the neighbouring one left off (in the end, the result should be a mosaic of the scene). 

First observation: The inverse of is not , that is completely wrong. Rotations with quaternions imply that these 4D complex number equivalents have unitary norm, hence lie on the S3 unit sphere in that 4D space. The fact that a quat is unitary means that its norm is and that means that the quat's inverse is its conjugate. If a unit quaternion is written as = (cos(t),sin(t)v), then its conjugate is =(cos(t),-sin(t)v), where t is half of the rotation angle and v is the rotation axis (as a unit vector, of course). When that Hamilton dude decided to play around with complex number equivalents in higher dimensions, he also stumbled upon some nice properties. For example, if you employ a completely pure quaternion (no scalar part w !), you can consider that crap as being a vector (it's actually a quat on what people might call the equator of the S3 sphere, which is an S2 sphere!! - mind bending stuff if we consider how technically impaired the people in the 19th century seem to us eyePhone cowboys nowadays). So Hamilton took that vector in its quat form: and did a series of experiments considering the geometric properties of quats.. Long story short: 

Well, let's try it in 3D: Assume the dynamics equation: r(t) = r0 + v0 * t + 0.5*g* t^2 where r is the position vector, v0 is the initial launch velocity and g is the gravitational constant acceleration vector (0,0,-9.8) . Now let's analyze the length functional: L(t) = Integral(0,t, ||r'(t)|| dt) . This means that the total curve length from its beginning (t=0) till this t time instance is L(t). In this formula, ||r'(t)|| is the norm of the time derivative of the position function r(t). We know that the norm is actually the square root of the dot product of its argument vector: ||r'(t)|| = sqrt(dot(r'(t),r'(t)) hence you can analyze this amount: r'(t) = v0 + g * t and therefore dot(r'(t),r'(t)) = v0ov0 + 2*v0og*t + gog*t^2 where is the dot product operator (some write it as an angular bracket < a , b >, others use a dot which I tried to imitate here with this ). This dot product, in your case, is a quadratic function of with nothing more than scalar coefficients in the name of those dot products. Hence, if you can compute a primitive for this sqrt(dot(r'(t),r'(t))=sqrt(at^2 + bt + c) where a=gog, b=2v0og, c=v0ov0 you're on the right path. Call this primitive F(t), then L(t) = F(t) - F(0). If you want to know what t0 instance corresponds to a length on your path of d0 units, then just compute the inverse of L(t) by, what else, computing the inverse function of F(t). as I am in a hurry, I can't compute the primitive, nor its inverse for you, but if you can't either, do it numerically (numerical integration and then numerical equation solving via Newton's method). Later edit: Use the Wolfram Alpha to find the integral, then perhaps solve numerically for its inverse.. or again Wolfra Alpha 

I have a strong hunch it's something I need to change in the int player and int realDepth values, but i can't be sure. 

I've just ported over and updated a bunch of FBX model loading code I had that worked with Direct X 10 to try and make it work with Direct X 11. Since that time, the FBX SDK I was using has been updated. In my old program I linked to the following lib: 

Is there something that I have forgotten to do when I build and run the game from the file menu? Has someone ever beaten this issue before? 

I'm trying to make a TextMesh appear when ever an enemy prefab is hit. I currently have this functionality working. However, the text doesn't face the direction the players camera is looking. At the moment my OnCollisionEnter method contains the following line of code: 

i know the above line is checking only one type of enemy at the moment, but its the ground enemies I'm having issues with at the moment. Does anyone know how I can delete the enemy prefab I'm spawning from my first section, once they've been hit, then have it respawn at the next section without the error: 

I am currently working with CAVE systems and I'm looking into hooking up a pre-exisiting game engine in one. I know this is possible through Unity and the Unreal Engine as there is already research out there showcasing that it has been done. Right now, I have not decided upon one game engine to use and I'm currently looking around and researching if it is possible with the likes of CryEngine and Valve's Source Engine. The one issue that I am going to face, however, is getting the image to correctly render across all four of the monitors / screens. Thusly, as a result I have two questions: 

I'm currently using the following MySQL example to make a server side high score table: Unity Server Side high score The above example allows me to create a top 5 player high score table that sorts itself based on the score amount. However, my knowledge of PHP and MySQL is limited to only that of the above example and I wish to expand upon it. What I now want to do is create a local score amount (the score the player accumulates during game play) that is updated on my server. At the moment, the server only calls the total scores once, and I want it to be polled so that it updates the moment the player earns some points. And displays this score. The reason for this, is because my high score, and local score, are going to be displayed on another device. Say your playing my game on a PC but the score is updating and showing you on an iPad (rubbish example, but you get the idea). Can anyone point me in the right direction as to how I would go about doing this, using the example I've already used? 

Although being a neophyte to network programming myself, I would like to share my limited experience by adding a few points: 

We know the user can drag the slingshot by defining a vector having its origin at the tip of the downward pointing triangle (like in my figure). The user can define thus vectors that point toward the bottom of the screen (restricted programmatically), and whose radii cannot be larger than a dMax threshold. Whenever the user shoots, the input vector has a length between [0, dMax]. This interval must map into a range interval: [rNear, rFar]. But since the range in real life is usually not a linear function of the launch velocity, we're free to suppose that the transfer function between [0,dMax] and [rNear,rFar] is looking somewhat like this: 

find the number of contact points (contact point = {pair, position, normal} ) divide the ball into balls with the same center of mass (basically sharing the same position and volume, of course not colliding with each other), but having the initial sphere's mass divided by now simply compute for each sphere the necessary impulse required to push it out of the wall when all contact points are solved by finding appropriate impulses, just add those impulses together and apply them to your big sphere Inside a frame, to assure convergence, you can perform 4 such iterations to find a solution (relaxation). 

I've seen the comment of Byte56, but the links there don't provide the exact answer, or aren't graphically detailed for this specific task, so if it's an answer the user wants, why not let him have one. Here goes. 

It's very probable that Erin Cato subtly justified the use of the Symplectic Euler over RK4 or another higher order integrator. The author has lots of slides and/or material (e.g. $URL$ related to the inner workings of the Box2D engine. The main reasons for why RK4 is not really needed when writing this kind of a Physics Engine are: 

If Delta <0 you have no intersection. If Delta = 0, the segment is tangent to the cylinder. If Delta > 0, you solving for t gives you t1 and t2. Plug t1 and t2 in the POINT ON SEGMENT equation and get your P1 and P2 points that are CANDIDATES. Remember, t1 and t2 must be between [0 and 1] in order for P1 and P2 to be points on your segment. If one of them is outside [0,1], it means your segment has an endpoint inside the cylinder. If both t1,t2 are outside [0,1], the supporting LINE of the segment intersects the cylinder, but not your segment. One last checkup, P1.z and P2.z should be between 0 and Cylinder.End.Z, because you work with finite cylinders. Wrap-up: you have to plug in your t1, t2 values in the INITIAL SEGMENT equation to recover the intersection points. That's it, I don't have the time to sum it up in a C# snippet, but I hope you understand what to do.