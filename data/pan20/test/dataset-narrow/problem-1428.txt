There is another option. You could frame for an intermediate ratio, and do both cropping and expanding (or bordering) on both normal resolutions. I believe TV filming is framed in this way, such that widescreen looks ok (everyone isn't crowded into the middle of the scene), whereas cropping for 4:3 isn't quite as drastic and there's minimal panning. 

To convert a cursor position to a ray, you can back-project the 2D co-ordinates onto two planes parallel to the view plane. Typically you might use the near and far planes. Two points define a ray. This is simply a matter of doing the maths used to project a 3D point onto the screen, in reverse. Typically from world space you would multiply by a view and projection matrix to get into clip-space, divide by Z to do the actual projection, and then scale the resulting values into pixel-values. So in reverse you would scale the screen-coordinates into clip space, multiply by Z (taken from the plane you're interested in), and transform by the inverse of the view-projection matrix to get back into world space. However as you've noticed, most engines and libraries can do this for you... 

A cross product ought to be perfectly reliable for any planar polygon. If you've got problems with that, it's almost certainly just a bug in your code. As others have said, it would be useful to know why you think your normals are wrong. 

Interpolating the matrices like that isn't going to give a useful result, especially for anything involving a rotation. Ideally you want to decompose your animation into some other format (quaternion for rotation, for example) and then interpolate in that form before building the matrix. 

I'm developing a new monster taming/hunting game and I wanted to make some changes to the battle mechanics. Which would you find more fun and can you offer any suggestions to improve on what I've done. I'm leaning more to just option 2 although it will be a bit more difficult to deal with the AI. Current base for the mechanics: 

Monster A uses Explode (its recovery is at 50) Monster B uses Poke (its recovery is at 20) a check is done to see which has lower recovery and they go next (20 "time units" goes by and A now has 30 recovery, B is down to 0 and can perform a move again. Monster B uses Poke again (recovery at 20) (A is still at 30) Another 20 "time units" goes by and A is at 10 and B is at 0 again. Monster B can attack again 

Intercept - forward monster takes the next attack aimed at the rear position. Force switch - forces the opponent monsters to switch positions Pierce - 1/2 damage is done to the rear position if an attack on the front is successful. Dragon's breath - all monsters (including your forward position if you are the rear) in front of you are hit with fire damage. (this would be a good move to switch your positions prior to using) Goblin Mortar - fires 3 spaces away (requires the attacker to be in the rear to hit the rear opponent) 

You don't need collision detection, but you will need to give your enemies a bit of intelligence, and have them avoid running in to each other. Collision detection without that will just make your enemies look stupid anyway - people avoid each other, they don't generally collide. Look up "flocking" for some simple behaviours. The basic idea is that things should head towards the player, but away from a close obstacle (each other, walls, that kind of thing). 

The texel co-ordinates, if you have bilinear filtering, are falling between the texels rather than being centered. You either need to to add half a texel to the UVs, or turn off filtering. 

These things can always be useful. Whether or not it's the prettiest or safest solution is another matter, but I think game development involves a certain degree of pragmatism. 

Having more maths knowledge/ability is never a bad thing. However don't take it as a bullet point for the CV - it's only going to be useful to you, and/or an employer, if you actually get something out of it. Which means actually learning stuff, and being able to demonstrate it in an interview and use it in the real world. That can be tough, because IME maths is quite often a subject taught without much context. You may have to provide your own - look at what each topic can be used for and you'll probably absorb it a bit better. I wish I had done this a bit more myself. There are topics in maths I know I covered at university, but didn't take in because I wasn't given any indication of what they were useful for. Now, 15-20 years later I'm coming across things where I need that knowledge... and it's not there ;) 

You seem to have had enough data to produce the handy animation above. Your simulation may need more accuracy than provided by my solution: For each frame of your animation above, record the pixel positions of the centers of each star. Enter these values into two arrays in your program. For a given time t, find the corresponding four consecutive entries in each array and do a bicubic filter on them to produce the position of each star. 

Procedural texture suffers from the problem that an Art Director can not reliably point at the work of an Artist and say "please make that part a little more X." because the procedural shading system may not support X cheaply or at all. For example, a brick shader may support clean brown brick, but may not support brick that was painted with an advertisement 80 years ago and graffiti 10 years ago. Or it may not support having one purple brick out of 1,000 brown bricks. In exactly that one spot, because that spot appeals to the Art Director's taste. A real texture can support all these things of course, and in this sense a real texture is superior to a procedural texture. The procedural texture exerts artistic control by virtue of its preference for certain use cases over others. A real texture exerts little such control. GPU hardware, however, has a strong preference for proceduralism, because texture memory is so many cycles away from the ALU units that do the shading. 

You could try some kind of simple distance function in the pixel shader. Perhaps something like barycentric coordinates (store different RGB at the vertices and the rasteriser will do most of the work for you). Then your pixel shader will have the information it needs about where on a polygon the current pixel is - near a vertex, near an edge, or in the middle... I suspect in practice it'd be easier to texture it though. 

You could search in both directions - from the current tile to the nearest neighbour, but also looking to see if another tile has the current one as nearest in the opposite direction. Move to the closest of those two if different. 

I don't know whether it's clearer than the original paper, but there is another implementation of the technique (or a variation on it) which I spotted in this year's Siggraph material. $URL$ As to whether or not you'd get a better answer over at the main site, I don't know - I'd say the majority of people interested in this technique are probably game devs... 

It's easier to model because there's separation between the limbs and body, and the bits that need to stretch when animated are already stretched - so they get modelled and weighted correctly. Same reason you'd model a hand as open and fingers spread, rather than modelling it as a fist. Oh, and as far as programmers are concerned, we don't care. It'll still look like something from "The Thing" when we get the skinning code wrong at the first attempt. 

Basically, I'm starting with the simple Pokemon style battle. I would like to add either one or both of these changes: Option 1: Add in a recovery stat for moves so that the "turns" are based on how much the monster has to recover. When a monster uses a move, the move's recovery stat is added to the monster's. When a monster's recovery stat reaches 0 it can perform a move. Explode - A high damaging move requires 50 recovery after being used. Poke - A smaller damaging move requires 20 recovery after being used. (Both start at 0 recovery, A goes first just because) 

This adds a timing element to the battle. Instead of each monster being able to attack once a turn, you can use smaller attacks or status changing moves quickly before your opponent recovers from their attack. Status effects may cause a monster to require more recovery each turn (like a slowing effect) or a hasting effect could be applied. Option 2: Use 2 monsters in the same battle with a forward and rear position. Moves have a range value that determines if you can hit the forward, rear, or both monsters. I think generally, melee style attacks would be higher damage but only affect the forward monster. You would often want a tank (high defense) monster to take these hits with a ranged attacker in the rear position like an archer or mage. Certain moves could damage both positions and you could have the option of having the monsters switch positions. I think the strategy wouldn't be limited to a melee tank in the front with ranged in the back, as a melee tank may be susceptible to special attacks and certain monsters would do more damage in the front while being vulnerable to attack. Example moves that take advantage of this mechanic: 

The advantage to instancing is to allow multiple objects to be drawn in a single draw-call, reducing the overhead of calling the API. Typically as well as the geometry, you would also have a buffer containing multiple transforms, and perhaps other per-object data. This could be used in the shader to vary the appearance of each object by modifying its colour, or using a different texture (generally the texture variations would be achieved by using a texture atlas to pack multiple textures into a single texture). 

Well as a simple hack on your current technique, you could choose between two up-vectors based on whether your view vector is tending towards parallel with one. It'd just be an abs, compare, and select in shader terms, so not much overhead - especially considering the sampling and blending... 

The result will be that the object's vertices will first be scaled (along the local XYZ axes), then rotated (around the local origin), and then translated into "world" space. Then the world-space co-ordinates will be translated such that the camera is at the origin, and finally everything will be rotated around to the correct view direction. This is probably what you want for a basic scene. As Richard Fabian says, you generally want to consider the camera transforms as an inverse, though you might equally replace the camera transforms with a lookAt() function or similar, to directly construct an appropriate view matrix. If you push the matrix stack after setting up the camera, you can pop/push for each object without having to set the camera up again.