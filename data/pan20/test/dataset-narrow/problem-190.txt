As an aside; Why on earth do you want to insert a two-character table identifier into every instance of this new field? The table that the row came form is immediately obvious from the table being inspected. If you really think that your data collision rate across a large number of sites is so extreme as to justify a scheme such as this, at least use a GUID, an already implemented technology, instead of this NIH design. Update The monotonic nature of the autoincrement key is made use of in some very efficient tally-table algorithms for running totals( at least) to enforce proper sequencing of running-total calculations. This scheme would invalidate the use of those algorithms. 

I believe these are more likely to give you incremental performance improvements than partitioning the data tables. 

Use a CROSS APPLY (or perhaps OUTER APPLY) to replace the case statement in your aggregates like this: 

the General Journal and General Ledger are an aggregated summary of all transactions used for generating Trial Balances and the Financial Statements. The Account Receivable subledger receives entries from the Invoicing Journal (debits to AR by Customer Number) and the Cash Receipts Journal (credits to AR by Customer Number). 

As Joe Celko never tires of noting, the concept of Primary Key is an artifact of pre-Relational database technology. As such it is wholly within the Physical Design of your database. At the Logical Design level it is necessary to have a Unique constraint on the column token to preserve the integrity of your data, as that column is a Natural Key for your table. Then the question is (a) whether or not to have an additional surrogate key to assist in performance of Foreign Key lookups; and (b) if yes, then should the table be clustered on the Natural Key; the Surrogate Key; or in some cases on neither (heap storage, using an implied RowID as surrogate Key). In only one case will it be necessary to specify as part of Logical Design that a Surrogate Key be present in the table - when it must be allowed for the Natural Key to be editable. These two questions are not part of the Logical Design of your database, so need not be addressed early if your development methodology allows for convenient schema updates to your DB. As an aspect of Physical Design, the answers to these two questions will be properly based on the estimated table sizes, and the frequency of FK lookups into this table. In order to make a sensible design one should be well read on the affect of index structure and construction over time. It is unfortunate that the term Primary Key, which originates as an artifact of pre-relational database technology, was first co-opted into the Logical domain by Date, Boyce, Codd, et al; and then co-opted back into the Physical domain by the major RDBMS vendors. I avoid the term completely when I can, and specify all my indices as being simply Unique or Non-Unique, and decide which one will be clustered as late in the development cycle as I can. 

You have forgotten the join criteria . If you had used the modern JOIN syntax this would have stood out like a sore thumb. 

The key concept here is that linked SQL Server tables are truly linked to ACCESS, not copied. If you desire a private copy of the data, copy the tables down instead of linking them. 

When two tables have the some column structure (as Location and Address do here) that is almost always an indication that they should be the same table (conceptually and logically, even if separated physically for performance reasons). So ask yourself, "What is the design assumption that leads me to think that Address and Location are separate entities rather than the same one?" Answer: The relationship between Employer and Address is 1-N; but the relationship between Post and Location is 1-1. Therefore, each Address must have a FK to the Employer, and each Post must have a FK to the Address, with a transitive relationship to the Employer via the Address table. 

The Shipping Address is a property of each and every order, not of the customer. Even if it was a property of the Customer, it would be necessary to know where past orders were shipped to if a customer relocated. Therefore all that can be stored as a Customer property are the Default Shipping Address (and Default Billing Address), for pre-populating each order and invoice as they are created. This is not a denormalization, because it directly reflects and captures the required business rules. Now that the historical accounting records are taken care of, there should be a CustomerLocation table with FK's to three Address records in an Address table: Physical Address; Default Billing Address; and Default Shipping Address. The first of these records should be associated with an EffectiveDate, so that again a historical record exists as changes occur. 

At 14 million rows in your largest table and 16GB of RAM devoted to SQL Server it sounds like your entire DB almost fits in memory. If that is true the problem is likely to be quadratic (or worse) behaviour in your queries, which can only be truly fixed by rewriting the worst offenders. That said, I would try one or both of: 

Note that while both sides of every transaction will balance in toto, in general it is not required for the two sides of a transaction to be the same number of lines with the same values. For instance a simple Retail Sales transaction (in most jurisdictions) will look something like this: 

Working from Aaron Bertrand's Script to create dynamic PIVOT queries in SQL Server and with a sample data set defined thus: 

First, you must ask the Controller (or CFO) what the appropriate costing model is for inventory of this sort - LIFO, FIFO, Average, or some less common pattern. This is very important as if you use a different calculation from that published in Financial Reports of the corporation, you will be engaged in fraud of the shareholders and be in violation of Sorbanes-Oxley. If the financial records are mailed to the shareholders, you might be in violation of RICO also. All of the methods I listed above are appropriate in various industries, at various times - you cannot make an a priori decision on one being better than the others. One absolute principle of GAAP is that when any change is made to the traditional accounting policies of a corporation, a number of prior years financial statements must be restated according to the new calculation in the current year's financial statements. This can only be done as authorized by the Controller and/or CFO, and possibly the corporate auditors as well. Never, ever program an accounting calculation of any sort without verifying the technique with an appropriate corporate authority. There are many aspects of GAAP (Generally Accepted Accounting Principles) that are non-intuitive to those without an accounting background. Update: Note that even when inventory items have serial numbers, and thus the actual cost may be known, it is not necessarily appropriate accounting practice to use the actual cost instead of a calculated cost based on all equivalent items in inventory. That is to say: The cost of selling a unit of product XYZ is a calculated accounting value that may not be the actual purchase price of the particular unit sold, even when that is known. I repeat: You must discuss this with your Controller and/or CFO. 

I have often found the IN operator to be a source of performance problems in the past, and avoid it whenever I can (which is almost always). 

Adding some SSD RAM to the server devoted to the indices of the largest tables; and Adding more disks (not more disk space, more actual disks) in order to have more heads and axles servicing the massive table scans. 

Put the list of values into a temporary table, and then perform an INNER JOIN to it. Most SQL optimizers and engines can handle joins much better than they can handle an IN operation. If the list is long, this also allows you to define an index on the temporary table to further assist the optimizer 

Amend your table definition as following to add two additional indices (My apologies for the SQL Server syntax - it's all I have here at the moment): 

The standard implementation of any criteria specified as is as Update: Using this CTE for sample data: 

If the SQL server table is modified will it be automatically reflected in the [linked] table and when? Yes, on next refresh or requery. If I edit the linked Access table will it be reflected in the SQL server table? Yes; as before, these updates will be visible on the other end on next refresh or requery. Trusted Connection only controls how the connection is made to SQL Server, not behaviour after a connection is made. 

A Bond Purchase transaction will have to handle the Discount(Premium) on Face Value as either a Credit or a Debit depending on whether interest rates have risen or fallen since the bond was issued, so you won't even know in advance which side of the ledger your single-entry is on. Once you decide to engage in single-entry bookkeeping you lose all ability to handle these transactions, which as seen here are by no means either rare or complicated. 

I have had success in general replacing lengthy IN operations with a join against a temporary table. This makes sense because RDBMS are optimized to perform JOIN's as efficiently as possible, and the handling of lengthy IN lists will most likely be done by repeating the query for every value in the IN list. The performance issue you see could easily be explained by the simple fact of the query being run 5,000 times. 

You are thinking about this incorrectly. An assignment submission from a student not enrolled in the corresponding course, if it happens to get marked, is not fundamentally different from a properly submitted and marked submission from a student who subsequently drops the course. Allow the entry of the mark to exist in the database, by noting that when the final report of assignment marks for each course is generated, the (inner) join will fail for all submissions for students not enrolled in the course at exam time. That is sufficient to properly implement the business logic.