There seems to be problem with the calibration or something, since I am seeing a bit flip occurring on the first qubit with roughly probability 2/3 that should not happen. The expected result should be 00000 in all cases, however 10000 seems to be twice as likely in almost all cases (prob ~0.1 vs ~0.2 over 1024 runs for each stabilizer generator). Below are the probabilities of obtaining 00000 for each stabilizer generator over 1024 runs each: XIXII:0.10840 IXXII:0.15625 IIXXI:0.11621 IIXIX:0.10645 ZZZZZ:0.09375 And the probabilities of obtaining 10000 for each stabilizer generator: XIXII:0.21777 IXXII:0.21875 IIXXI:0.17871 IIXIX:0.14648 ZZZZZ:0.19726 

Well, the standard text used is Quantum Computation and Quantum Information by Nielsen and Chuang. It covers quite a range of different aspects at a reasonable level. Nearly everyone working in the field has a copy of this on their shelf. The Kaye, Laflamme and Mosca book is also good, but covers less (though there is a little more focus on algorithms). While it is quite possible to explain quantum computing without going into much quantum mechanics, I don't think that this is necessarily a good way to approach learning quantum computation. There is quite a lot of intuition to be gained by having a feel for the physical theory, since many of the more recent models of quantum computation (i.e. adiabatic, topological and measurement-based models) are more physically motivated than the quantum Turing machine or the circuit model. That said, the quantum mechanics required to understand quantum computation is fairly simple, and is covered quite well in Nielsen and Chuang. Really, you can get a good feel for it reading through the relevant chapter and trying the exercises. It's the kind of thing you can get a fair understanding of with a couple of days work. My advice, though, is don't go for a standard intro text to quantum mechanics. The approach taken to model atoms, molecules and materials uses infinite dimensional systems, and takes quite a lot more effort to get on top of. For quantum information it is a much better start to look at finite dimensional systems. Also, traditionally, the problems studied by physicists tend to revolve around finding ground states and steady state behaviours, and this is what most introductory texts will cover (starting with the time-independent Schroedinger wave equation). For quantum computing, we tend to be more interested in the time evolution of systems, and this is dealt with much more succinctly in quantum computing texts than in general quantum mechanics intro texts (which are by definition more general). 

So my question is are there any major areas I am missing? My motivation is very simple: I'm a theoretical physicist who has come to TCS via quantum information and I am curious as to other areas where the two areas overlap. This is a relatively soft question, but I don't mean this to be a big-list type question. I'm looking for areas where the overlap is significant. 

In quantum information theory, the distance between two quantum channels is often measured using the diamond norm. There are also a number of ways to measure distance between two quantum states, such as the trace distance, fidelity, etc. The Jamiołkowski isomorphism provides a duality between quantum channels and quantum states. This is interesting, to me at least, because the diamond norm is notoriously hard to calculate, and the Jamiołkowski isomorphism would seem to imply some correlation between distance measures of quantum channels and quantum states. So, my question is this: Is there any known relation between the distance in the diamond norm and the distance between the associated states (in some measure)? 

This question seems a little confused. The class of decision problems solvable efficiently on a quantum computer is BQP, while on a classical computer it is either P or BPP depending on exactly how you define things. An interactive proof is something entirely different. It is a protocol which allows a prover to prove, beyond reasonable doubt, the outcome of some decision problem, and allows the prover and verifier to interact. QIP is the class of interactive proofs where the verifier, not the prover, has access to a quantum computer. It turns out that this doesn't help very much since you can prove exactly the same things to a fully classical verifier. I think what you have in mind is something different: an interactive proof where the prover is limited to quantum computation. This is something that has been studied in recent years (see for example arXiv:0807.4154, arXiv:0810.5375 and arXiv:1209.0449). The extent of our current knowledge is that you can basically prove anything in BQP if you have either some limited quantum power for the verifier (the ability to prepare single qubit states, for example) or if you have several provers who do not communicate, but share entanglement. The DWave machine does not implement anything like these schemes, and does not constitute any form of interactive proof. Actually, the problem of implementing such a scheme on DWave hardware has been something that has interested me for a while. There are a few hurdles to doing this. All of the known schemes are based in the circuit model or the closely related measurement based model, while the DWave machines use a very different model of computation. This introduces a few problems, as their system is not a general purpose quantum computer, and so cannot implement any of the known schemes directly. Further, it cannot really accept any kind of quantum input, which rules out many of the tricks used to construct the known schemes. However, I don't mean this to be a criticism of DWaves device: virtually no other quantum device has been used to implement such a proof. The closest you get are things like violations of Bell's inequalities and the recent experimental demonstration of blind quantum computing in linear optic (arXiv:1110.1381). At this point, no one has yet published an experimental demonstration of a quantum prover interactive proof, so DWave isn't any better or worse in this respect than anyone else. By the way, I apologise for linking to two of my own papers in the above, but this is a question I have been very interested in in recent years, and I think those papers are relevant to the question at hand. It's not intended as self-publicity. 

The answer is an unequivocal yes. Quantum computers would definitely still be useful. Quantum computers are not oracles for BQP, but rather devices which process quantum states, and can communicate using quantum states. Just as the ability to make non-deterministic queries is fundamentally more powerful than the ability to make purely deterministic queries, independent of the status of P vs NP (and indeed this is the root of the oracle separations), the ability to make quantum queries and to communicate using quantum states is fundamentally more powerful than the purely classical counterpart. This leads to advantages in a wide range of applications 

Do you already know about Shannon's noiseless coding theorem? This theorem establistes theoretical limits on lossless compression. Some of the comments from the others seem to assume you know about this theorem, but from the question, I think it may be the answer you are looking for. 

A more extreme version of Robin's example: Let the input size be $n$, with the first $n-1$ bits (call this string $x$) encoding a Turing machine $T_x$. Fix some function $f(n)$. Let the last bit of the string be $1$ iff the Turing machine $T_x$ halts in less than $f(n)$ steps. The problem is then to determine if $T_x$ halts in less than $f(n)$ steps and the parity of $x$ is even. Thus, by making $n-1$ queries the problem can be solved in time $O(f(n))$, while by making $n$ queries, the problem can be solved in time $O(n)$.