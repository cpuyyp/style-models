Yes. The following paper presents a candidate for a PRF that is implementable in $NC^1$, whose security is based on a lattice assumption (hardness of LWE): Abhishek Banerjee, Chris Peikert, Alon Rosen. Pseudorandom Functions and Lattices. EUROCRYPT 2012. It also has some discussion of related literature that might be helpful. 

Intuitive algorithm. Basically, we'll apply the following closure rules to the graph as many times as possible, until they don't produce anything new: 

A general approach is to automatically infer loop invariants. There are techniques for inferring loop invariants that can be expressed in Presburger arithmetic, i.e., linear expressions over the integers plus quantifiers. These typically rely upon the fact that there exist decision procedures for Presburger arithmetic, e.g., the Omega method (see also the web site). These methods should be enough to build a special optimization that can solve your first two challenges. For instance, you can use those methods to find loop strides. In particular, you can use those methods to characterize the values of that will reach the then-block of the if-statement: you can first check whether the sequence of values of lies in an arithmetic progression, then characterize the progression (deduce find the minimum value of , deduce the maximum value of , and deduce the stride). These methods have been studied in depth in the compiler literature on loop vectorization, where we want to know whether we can execute each iteration of the loop in parallel on a separate core. Similar techniques should be useful for your kind of problem as well. I don't know whether there's any reasonable algorithm for your third example, or whether anyone has studied that sort of thing in the compiler literature. 

Your problem is at least as hard as SAT: given a CNF formula $\varphi$ on variables $x_1,\dots,x_n$, add variables $x_{\bot},x_{\top}$ and the clause $x_{\bot} \ne x_{\top}$, and translate each occurrence of the literal $x_i$ in $\varphi$ to $x_i \ne x_\bot$ and each occurrence of the literal $\overline{x_i}$ to $x_i \ne x_\top$. In this way each clause of $\varphi$ can be translated into a constraint of your form. Then $\varphi$ is satisfiable iff the resulting set of constraints can be satisfied by a set $A$ of size 2 (i.e., $|A|=2$). Therefore, if we could solve your problem, we could solve SAT. So your problem is at least as hard as SAT. Consequently, solving your problem by bit-blasting to SAT is a reasonable way to solve your problem. I don't think you should expect a much better algorithm. This does leave some room for cleverness in how you encode your problem as a SAT instance. You could encode each $x_i$ either with a one-hot encoding (using $nk$ bits), or in binary (with $n\lceil \lg k \rceil$ bits), or by encoding which pairs of $x$'s are equal (with ${n \choose 2}$ bits, i.e., a boolean variable for each $i,j$ encoding whether $x_i = x_j$, with transitivity constraints enforced). Which you choose is likely to affect the running time in practice. I suggest experimenting a bit with different possible ways of encoding your problem as SAT to see what seems to work best for the types of problems you are dealing with. (I don't particularly expect a SMT solver to do any better than a SAT solver here, but you could try it.) 

If you want something fast and that you can use in practice, you might look at the cryptographic literature. For instance, poly1305 and UMAC are fast, and there are many others. Because 2-universal hashes are useful for cryptography, cryptographers have studied many constructions and found ones that are extremely efficient. Poly1305 works like your first type of hash (called a polynomial evaluation hash), working modulo $2^{130}-5$. The scheme shows clever tricks to make this run very fast on a modern computer. The amount of randomness is small: 128 bits. If you want to reduce the amount of randomness and don't care so much about practicality, you might look at the following research paper: 

Yes, it is NP-complete, by reduction from 3SAT. In particular, we will go through an intermediate problem, which I will define as follows: Definition. The 1-to-3 of $k$ problem is as follows: given $n$ sets $S_1,\dots,S_n$, decide whether there exists $y_1,\dots,y_m \in \{0,1\}$ such that $1 \le \sum_{j\in S_i} y_j \le 3$ holds for all $i$. Theorem 1. The 1-to-3 of $k$ problem is at least as hard as 3SAT. Proof: Suppose we have a formula $\varphi$ with $n$ clauses over $m$ variables, $x_1,\dots,x_m$. Introduce $m'=2m$ variables, $y_1,\dots,y_m$. The intent is that these will correspond to the $2m$ literals $x_1,\dots,x_m,\neg x_1,\dots,\neg x_m$ (e.g., $y_i=1$ if and only if $x_i=\text{True}$; $y_{m+i}=1$ if and only if $x_i=\text{False}$); we will introduce a few sets to enforce this intent. There are $n'=n+m+\lceil m/3\rceil$ sets, as follows: 

Your problem is at least as hard as bin-packing. In particular, optimizing objective (a) basically is the bin-packing problem (in particular, bin packing is the special case where all drums have equal length, so your problem is at least as hard as bin packing), which is known to be NP-complete. Consequently, your problem is NP-hard, too. All the usual hardness results about bin packing apply to your problem as well: there is no polynomial-time approximation scheme (unless P=NP); and bin packing is known to be strongly NP-complete, so there is no hope for a pseudo-polynomial time solution, either. If you want a pragmatic approach to solve this in practice, or at least get as good a solution as possible, I would recommend formulating it as an instance of integer linear programming (ILP) and applying an off-the-shelf ILP solver. In particular, you can formulate the problem of minimizing objective (a) as an ILP instance, in the same way as for bin-packing. Then, off-the-shelf ILP solvers might be effective: even if they don't find the optimal solution, they typically have a way to output the best solution found after a certain amount of computation time, and that might be pretty close to optimal. Of course, it is not a silver bullet; in the worst case it might take exponential time. There's no way to know in advance whether it will work well enough. The only way to tell is to try it and see. Objective (b) amounts to maximizing a sum-of-squares objective function (per your comment). This can't be expressed using integer linear programming, but it can be expressed as an instance of quadratic integer linear programming. You could try formulating it as an instance of mixed-integer quadratic programming (MIQP) and then applying an off-the-shelf MIQP solver. However, MIQP is substantially harder than ILP, so I don't know whether it will work well on problem instances of the size you have. 

Heuristically: Yes, I suspect this probably works, if the system of equations is committed to in advance (well before mining works), and if the system of equations is small enough. You'll need the random oracle assumption, ERH, and maybe other heuristic assumptions. In particular, Theorem 5 of their paper upper-bounds the number of prime numbers $p$ such that the system has a solution modulo $\mathbb{Z}/p\mathbb{Z}$. Suppose this bound is at most $2^{32}$. Then, heuristically (in the random oracle model), that the probability that there exists a prime $q$ that could fool you is at most $2^{32}/2^{72} = 1/2^{40}$. So, the probability you are fooled into thinking the system is satisfiable, when it actually isn't, is at most $1/2^{40}$. This assumes the system of equations is small enough that you can get a small upper-bound on the number of primes $p$, e.g., $2^{32}$ or so. That's a major limitation. Once the upper bound gets large, then you lose all guarantees and the proof probably becomes useless. This is in contrast to the more standard zero-knowledge proof, which doesn't have that problem, since there one can adjust the size of the cryptographic hash to be as large as needed to avoid this problem; with your scheme, we're limited to a 72-bit hash. Given that the upper-bound in Theorem 5 has an exponential dependence on $n$ (the number of equations in the system), I am suspicious that you might very quickly run into this limit and thus that your scheme might become useless in practice for all but the most trivial cases. Important caveat: This requires some way to verify that the system of equations was committed to in advance. Otherwise, a miner could mine a block $(r,c)$, then subsequently construct a system of equations that has a solution in $\mathbb{Z}/q\mathbb{Z}$ (with $q=2^{32}r+c$) but has no solution in $\mathbb{C}$; it's easy to construct such a system once $q$ is known. Ultimately, I'm not sure how this is better than Koiran's original proposal. I'm not immediately seeing an advantage that would make this more useful in practice. 

If you don't like relying on the special case $k=0$, here's a related construction. Let $\psi$ be a boolean formula on $n$ variables $x_1,\dots,x_n$. Introduce $n+1$ new variables $y_0,\dots,y_n$. Define the formula $$\phi := (\psi \land y_0 \land \dots \land y_n) \lor (\neg y_0 \land \dots \neg y_n).$$ Set $k=1$ and $\alpha=n$. If $\psi$ is satisfiable, the answer to your problem is no. If $\psi$ is not satisfiable, the answer to your problem is yes. 

The key phrases you are probably looking for are "information-theoretic cryptography" and "quantum cryptography". Searching the literature on these topics will turn up lots of work of the sort you are looking for. Some example highlights below: 

Now back to the hard case. If it doesn't fall into the easy case, then we must have $$p(x,\cdots,z) = c_x \cdot x^2 + \cdots + c_z \cdot z^2 + p'(x,\cdots,z).$$ where $p'(x,\cdots,z)$ has no squared-terms (it is multilinear). Let's look at what methods we can use for this case. Well, here's one situation where we can solve this. Suppose there exist a pair of variables $x,y$ such that $-c_x/c_y$ is a square (a quadratic residue) in $\mathbb{F}$, say, $-c_x/c_y=\alpha^2$. Then we can apply the change of variables $y' = y+\alpha x$. Conveniently, we have $(y')^2 = (y+\alpha x)^2 = y^2 + 2\alpha \cdot xy + \alpha^2 \cdot x^2 = y^2 + 2\alpha cdot xy - c_x/c_y \cdot x^2.$$ Plugging this change of variables into $p$, we get $$p(x,y',\cdots) = c_x \cdot x^2 + c_y \cdot y^2 + 2\alpha c_y \cdot xy - c_y \cdot (c_x/c_y) \cdot x^2 + \dots,$$ i.e., $$p(x,y',\cdots) = c_y \cdot y^2 + 2\alpha c_y \cdot xy + \dots$$ where the omitted part does not contain a $x^2$ term (since the $c_x \cdot x^2$ and $\alpha^2 c_y \cdot x^2$ terms cancel). Effectively, we have eliminated the $x^2$ term, so now we can apply the method above to $p(x,y',\cdots)$. Notice that $p(x,y',\cdots)=0$ is satisfiable if and only if $p(x,y,\cdots)=0$ is. When we find a solution that makes this zero, we can back-solve for $y$, and we obtain an assignment to $x,y,z,\cdots$ that makes $p(x,y,z,\cdots)=0$. Is it possible that no pair of variables allows us to eliminate a squared-term? If $-1$ is a quadratic residue (a square) in $\mathbb{F}$, and if we have at least 3 variables, then that is not possible: $-c_x/c_z = (-1) \times (-c_x/c_y) \times (-c_y/c_z)$, and since the product of a quadratic non-residue and a quadratic non-residue is a quadratic residue, we are guaranteed that at least one of $-c_x/c_z$, $-c_y/c_z$, or $-c_x/c_z$ is a quadratic residue so at least one of the squared terms can be cancelled. However, if $-1$ is a quadratic non-residue, or we have only 1 or 2 variables, then it might not be possible to eliminate a squared-term. That's the remaining difficult case. (The situation with only 1 or 2 variables is not difficult to handle, so really the difficult case is where $-1$ is a quadratic non-residue, and where each variable appears squared in $p$.) I have no general solution for this difficult case.