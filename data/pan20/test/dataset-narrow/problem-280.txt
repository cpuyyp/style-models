From the sample code I understand that #1 is what you want. In that case, do you really need a WHERE NOT EXISTS? Could you just do something like: 

The problem is, you should not be trying to create an ever-growing set of queries in this manner. Of course the first step in your example fails on the second run, since the column has already been dropped. There is nothing to restore and there is no error handling to cope with the missing column. I cannot fathom why you would want to build up your queries in such a manner. If your goal is to record every query that you run, then insert the code into a table that records the time and the code that you run. But do not expect to run all the commands without errors unless you have scripted the code to also deal with each exception that may arise. 

My experience is that 1,000,000 row insert will actually require more resources and take longer to complete than if you used a batch inserts. This could be implemented, as an example, into 100 inserts of 10,000 rows. This reduces the overhead of the batches being inserted and, if a batch fails, it is a smaller rollback. In any case, for SQL Server there is a bcp utility or the BULK INSERT command which could be used to do batch inserts. And, of course, you can also implement you own code for handling this approach. 

Then load the data file on disk into a new temporary database (e.g. RepairRecovery). Once that is done you can compare the data between the original, now "repaired", database and the data that you were able to recover from the disk file. (There are tools to help you compare data, or you can just look for missing IDs or something similar.) 

$URL$ The stored procedure only changes the name and does not manipulate the data. EDIT: Code corrected per shadonar. 

Several years ago Hilary Cotter responded to a similar problem. In his post he mentioned that synchronous replication is more reliable than asynchronous replication. Rows Do Not Match Between Publisher and Subscriber Part of Hilary's response is included: 

Create a empty database with the same file layout, including names, filegroups, location, etc, everything but data Take the missing files offline Take the database offline Use the data files you have saved to overwrite the empty data files Bring the database online 

In order to get the full path name, used the `DIR ... /B /S' In order to get the file name and size, used the `DIR ... /-C /TC /A-S /A-D /S' Since in step 2 the first column of a row with a file is the date and time of the file. Used this to filter out all rows that do not start with a numeric. Since step 1 and step 2 return different name formants, this results in needing to join a full path name with a file name only. A file name can appear more than once on a server, if it is in a different location. So there may be some ambiguity in the join if that case arises on one of your servers. 

was successful. In other words, can create NEW databases, but can only restore a database where it is already a user and still retain rights to that database. Hope that this helps you decide what to do. 

Note, I do not use merge replication. But by the nature of the issues with merge, there are more resources needed than in standard replication. 

First of all, depending not just on the number of rows but also on the total size of your table (based on the average row size) , it may not be excessive to create a new Foreign Key. Especially if scheduled properly. However, it may well be too heavy a change. In that case, you could consider creating an Intersection Table like: 

No, sorry. Once the log chain was broken the following differential cannot be restored because there are at least a few transactions that would be lost. You may feel that those 'unimportant' changes could just be ignored. But SQL Server insists on a complete restore chain. Though there are certainly some companies that will try to reconstitute a database for you. Not for free, of course. 

Another possibility is that you replicate both databases independently to your CityA server. Then each initialization of the database and the subsequent replications remain as simple as replication can be. Then in either one of those databases or in a third database that you create, use stored procedures and/or views that combine the results from the two servers into result sets used for analysis. Stored procedures could leverage temporary table (or some other work tables) to collect the needed data for a report or query covering both databases. Stored procedures would require some development,but it may be that you already have procs that just need to be copied and modified to include two databases. Views that UNION ALL (or UNION) data from two databases may not perform quite as well, but it will work. (UNION ALL performs better, if that fits your data patterns.) And it simplifies your replication landscape and preserves the idea of giving the boss data from both locations. Note: Perhaps in that case you have a CityBdatabase and CityCdatabase database replicated to your CityA server, but a separate CityAdatabase contains the objects that pull the results together for the combined report. 

I do not know how dynamic your queries are, but naturally each table needs a separate query for that table. Assuming that you are constantly querying this 4 table group, it might be worthwhile to create an indexed view that included the searchable fields you need from the 4 tables into a single view. This means the overhead of maintaining the indexed view, but may help in this case. Of course, if you have a widely varying array of tables that might be joined, you would need to consider whether this is a good idea for you or whether you should query each table separately as you are doing. 

We always run our Analysis Services on a named server, e.g. AnSrv123, and being run by a Domain Login such as DomainName\ManageAnSRV124. Regarding connecting to Remote Analysis Servers using SQL Server authentication please note the following. Connect from client applications (Analysis Services) $URL$ This says, in part: "Authentication is always Windows authentication, and the user identity is always the Windows user who is connecting via Management Studio." 

Restore a copy of the source database to a point just prior to when you experienced the failure. Name the database [dbnameEmergency]. Restore the logs incrementally to [dbnameEmergency], such as 1 minute, 5 minutes, 30 minutes, according to how finely you want to process the deltas in order to reduce the damage. Reading and comparing the incremental restores will require either creating snapshots, if you have Enterprise edition, or restoring the log to STANDBY mode so as to read the data. Compare the latest readable changes in [dbnameEmergency] against the state of the [dbnameReplica] (which is the database you are trying to set straight) and write code to apply the changes you are able to infer to [dbnameReplica]. 

If I understand your question, the "resilience to system failure" is handled by the server to ensure that the transaction either: 

Could this be a double-hop delegation problem since apparently Kerberos is not being used? Every machine in the chain, from the workstation to the database needs to use Kerberos authentication. You mention visiting the page, which then queries the database. Which could be interpreted as a double-hop, although I do not fully understand the topology from the description. I interpreted the topology as: Workstation / IIS Server / SQL Server. The following post explains the technology: $URL$ Basically, NTLM is not able to go through the middle server to the next server because it does not have the password hash for NTLM to work with moving forward. Using Kerberos authentication does not require access to the password hash, using instead the session ticket to define itself. 

If it was possible to connect to a SQL Server without setting up a security context on that server, that would be a serious security hole. So, there must be a security context. If you can ride on some existing rights that are already defined, then fine, but likely any existing security context was not tailored for your need. And since you want a specific account it is further complicated. Perhaps you could have a SQL Agent job created on the that would periodically PUSH the data to a staging table on your . (This would also allow the administrators to control what happens, just in case that is an issue.) Then you would grant rights on your to allow the 's job to insert data into the staging table on your . Note: When SQL Agent runs a job using a credential it is not impersonating the account, but should be viewed as logging in using the credential's account name and password. 

The stored procedure is not examining the total culmulative size of the rows in the database. It is reporting the size of space allocated to hold that data in the cumulative size of the extents allocated for the data. If there is significant freespace available, such as from many deleted rows, then a rebuild of the clustered index would compact the space in pages and extents to be more efficient (i.e. smaller) for performance reasons. So, no data should have been discarded, but the rebuild process made that free space which was embedded in the data pages available again. 

Basically User CALs are for people, not for Service Accounts. In the User CAL model you need a CAL for each person whether they directly access SQL Server from their account or they access SQL Server through some Service Account. This is a CAL per each and every person that uses the SQL Server, not a CAL per simultaneous user. However, your CALs will allow you to access as many SQL Servers that use CALs as you can reach within your organization. Edit: There are also Device CALs for when several people share a device at different times. Read about CALS at: $URL$ Regarding Service Accounts on the SQL Server I cannot find my old link on the subject, but perhaps this post will help you: $URL$ See the sentence that says 

Remember that this is a lite version of SQL and does not support all operations. See the following link: $URL$ SQLite supports a BEFORE TRIGGER which can delete rows before further action is taken. The link also provides some cautions about how that works: 

Gives the error: Cannot alter column 'Col1' because it is 'ROWGUIDCOL'. Acknowledgements to Martin Smith. I overlooked the ROWGUIDCOL operators, which are: 

You can certainly change a database to recovery model. The 'risk' is that you might need point in time restores, but you have said that is not a requirement. The huge log file is likely because the transaction log is not being backed up. If you switch to mode you will have to shrink the file. Like this: 

So it is pretty straight-forward and actually gives you some useful insight into your space utilization. Additional Notes: Yes, when you shrink a database, it actually compacts the data and then releases the no longer used disk space. Of course, SQL Server 2005 Express can have many databases that are 4 GB. (If you update to a more recent version of SQL Server Express, the limit is 10 GB per database, not including the log space.) Within a database the column in the example, defines how much space currently exists in the database file that could be used for adding more data. However, the database can be expanded to its maximum size or you can set autogrowth to allow it to grow in selected increments. (Use exact growth values, not percentages.) 

That assumes that you want to save the file name, begin time, and end time. That is up to you, of course. I would log the ImportBeginTime at the start of the import, then add the ImportEndTime at the end. That way you will have some tracking even if the import fails. And, assuming that you are importing from a file, I would actually recommend using because you can set the batch size and let handle the issue for you. Much better than writing it yourself. E.g. 

It has been quite a while since I personally used replication, so I am not current on practical problems. A publisher with multiple subscribers is pretty common. However, Microsoft suggests using multiple Distribution databases when you have multiple Publishers. The following post has direction: $URL$ "However, if multiple Publishers use a single Distributor, consider creating a distribution database for each Publisher. Doing so ensures that the data flowing through each distribution database is distinct." It sounds like a recommendation to me. 

However, you would be better just making a backup (which from SQL Server 2008 R2 supports compressed backups) and you will get a faster transfer than detaching and copying files. Not to mention the risks of detaching a file and not being able to reattach. Recommendation: Use and . 

Syncronous - The purpose is to make sure that the Primary server and the synchronously updated Secondary server are kept in synch. The transaction is only complete once the Primary and Secondary server transactions are both committed. Asyncronous - The purpose is to send the transaction to an asynchronous 'Tertiary' server without guaranteeing that both servers have committed the transaction. This is primarily to deal with slower communication due to bandwidth, network latency (as noted by Sean Gallardy), and so forth. Using an Asyncronous connection in this condition prevents the Primary server from being slowed down in its primary work while waiting for the slower connection to be committed on both servers. 

Regarding Connection Pooling, Thomas Stringer has posted: $URL$ The simple thing to know is: "The answer that enterprise DBAs need to be giving to these inquiries is that it is provider-specific. In other words, it is on the client/application side that connection pooling is handled." The CONTEXT_INFO, which is stored in binary, needs to be set by the application. CONTEXT_INFO is just a small bucket available to contain useful information such as the user's Name / ID / Login. (Or whatever else you want to stuff in the small space available.) The overhead is small, but it is an extra step to insert and extract the BINARY data in CONTEXT_INFO. (But useful for systems that run under a service account, so that you can know whose connection this is at the moment.) Thanks to srutzky for clarifying that a pooled connection is reset, but it is not dropped. At least not immediately. A post by Nacho Alonso Portillo offers clarification: $URL$ In part it says the "sp_reset_connection... does the following tasks: 1) cleans up the session context ... which includes ... resets the CONTEXT_INFO; ... 2) notifies them of the occurrence of a successful logout... 3) initiates the process of redoing the login." 

[EDIT] OK, so the query is more complex that can be embodied in a single query. This means that you will likely need to use a pattern much like the one you first posted. Note that EXISTS implies a DISTINCT and is often faster than a JOIN from a SELECT DISTINCT. But you can try different approaches and compare the behavior, timing, etc. Then choose the one that you like the best. 

A SQL Server 2008 R2 mirror is not itself readable. If you are using Enterprise Edition, you can create a named snapshot database from the mirror. This allows you to create a point-in-time readable database that can be used for pretty much any query that does not do any updates. (Though you could use another database (tempdb or a workdb) to do auxiliary work that would not fit the read-only snapshot.) See: $URL$ When the snapshot gets 'old' enough you create a new snapshot database. You will eventually need to drop the older snapshot, but it is possible to support several snapshots. (This depends on the amount of activity in your database and server.) If you use Standard Edition and have the same need, look into log shipping using RESTORE with STANDBY. You will need to manage when the next restores get made in order to use the STANDBY feature. See: $URL$ It says of the STANDBY option: "The standby file allows a database to be brought up for read-only access between transaction log restores and can be used with either warm standby server situations or special recovery situations in which it is useful to inspect the database between log restores." Therefore, this method requires you to delay the restore of logs to match the schedule for how long you want to keep this point-in-time in action. A LOG Restore will end the STANDBY and restore to the following point, which could be restored with a new STANDBY period. Depending on your needs one of these approaches can give you a readable point-in-time database. No updates become visible in the database for the period of (a) the snapshot, or (b) the period between restoring updates.