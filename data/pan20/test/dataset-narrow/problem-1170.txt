In general, we look at fixed-points of monotone functions over lattices, i.e. with some partial ordering over your elements. If your lattice is complete (it has a least and greatest element, called a bottom $(\bot)$ and a top $(\top)$), and the function whose fixed-point you're trying to find is monotone, then the Knaster-Tarski Theorem says that a fixed-point always exists. However, finding this fixed-point may be undecidable. For example, in Hoare logic, there is a function whose fixed-point is the weakest-precondition of a loop. But there's no guarantee that this fixed-point can be found, since finding weakest loop invariants is undecidable. The basic algorithm, which might not halt, is as follows: 

One simple consequence is $\mathbf{P}/\text{poly} = \mathbf{L}/\text{poly}$. Proof: For any language $A \in \mathbf{P}/\text{poly}$, there is a language $B \in \mathbf{P}$ and a sequence of polynomial-length advice strings $y_1, y_2, y_3, \dots$ such that $x \in A \iff (x, y_{|x|}) \in B$. By assumption, there is a language $C \in \mathbf{L}$ and a sequence of polynomial-length advice strings $z_1, z_2, z_3, \dots$ such that $(x, y) \in B \iff (x, y, z_{|(x, y)|}) \in C$. This implies $A \in \mathbf{L}/\text{poly}$; the advice string for $x$ is $(y_{|x|}, z_{|(x, y_{|x|})|})$. (A concise version of the proof: $\mathbf{P} \subseteq \mathbf{L}/\text{poly} \implies \mathbf{P}/\text{poly} \subseteq (\mathbf{L}/\text{poly})/\text{poly} = \mathbf{L}/\text{poly}$.) 

To prove the version of Azuma's inequality stated by Dwork et al., I figured we should take $X_0 = 0$ and $X_i = X_{i - 1} + C_i - \text{E}[C_i \mid C_1, C_2, \dots, C_{i - 1}]$. That way, I think $\{X_0, \dots, X_k\}$ is a martingale. But all we can say is that $|X_i - X_{i - 1}| \leq 2\alpha$ almost surely, right? That factor of two causes trouble, because it means that after substituting, we merely find that $\Pr[\sum_{i = 1}^k C_i > k\beta + z\sqrt{k} \cdot 2\alpha] \leq e^{-z^2/2}$, which is weaker than the conclusion stated by Dwork et al. Is there a simple trick I'm missing? Is the statement by Dwork et al. missing a factor of two? 

I am trying to imagine what would be the implications of the loopholes on Bell test on the random sequence generation scheme proposed by Vazirani and Vidick (VV protocol) in the paper titled 'Certifiable Quantum Dice Or, True Random Number Generation Secure Against Quantum Adversaries'. To my knowledge there are six loopholes to be discussed here. 

What does discarding the second register mean for the standard approach of hidden subgroup algorithm? Why does discarding let the first register end up in a mixed state? 

If the output of a quantum program is the eigenvalue of the ground state of a Hamiltonian and is an irrational number, how do we finish writing it on the output tape? How do we deal with the gates / unitary matrices when we are supposed to write the shortest description of a quantum program? We can always squeeze a deep quantum circuit into a single unitary matrix. 

(In fact, Hajnal actually uses a slight extension of the above lemma.) The same lemma is also used by GrÃ¶ger in this other paper and by Chakrabarti and Khot in this other paper. But I can't figure out the proof of Hajnal's lemma. Hajnal attributes the lemma to Yao and cites this paper. But Yao's paper doesn't actually claim that lemma in that form. Yao's paper does prove a closely related lemma. (Lemma 5 in Yao's paper, or equivalently Lemma 6 in this journal version of Yao's paper.) By adapting the proof of the lemma in Yao's paper, I see how to prove Hajnal's lemma under the extra assumption that $\delta_L(G) \geq \Omega(1)$. I'm having trouble with the case that $\delta_L(G)$ is subconstant. (I'll now assume that the reader is familiar with Yao's proof.) My understanding is that to prove Hajnal's lemma, the idea is to modify Yao's proof by essentially just replacing $\lambda(n)$ with $\delta_L(G)$ and $\mu(n)$ with $\Delta_L(G)$ everywhere. At a high level, with these modifications, Yao's reasoning still makes sense, and shows that to "discover enough missing edges", the query algorithm will need to make at least about $\frac{\Delta_L(G) - 4\delta_L(G)}{\lceil 4 \delta_L(G) \rceil} \cdot \frac{n}{2}$ queries. The issue is that when $\delta_L(G)$ is subconstant, because of the ceiling operation, that lower bound is only $O(\Delta_L(G) n)$, which is much smaller than the bound $\frac{\Delta_L(G)}{\delta_L(G)} n$ that appears in Hajnal's lemma. (Concretely, the issue is that each set $T_i'$ needs to have an integer number of vertices in it.) How can the proof be patched? 

My question: Why is it impossible to work with polylogarithmic-length encoding schemes for quantum circuits? 

Let me try to express my understanding one by one. Detection efficiency, or fair sampling If a very high efficiency detector is used it could be claimed that the loophole for Bell test is fixed. But the detector is a component of the device manufactured by the adversary. So, we should not be assured if the vendor claims that they are using very high efficiency detector. Communication, or locality Non-locality is an assumption for the VV protocol so I assume this loophole has impact on the protocol. Disjoint sampling I didn't get the whole thing but should I assume that this loophole will have impact if someone extends the VV protocol to a 3-party system? Failure of rotational invariance I have no clue. Anyone with help? Sources of error in (optical) Bell test experiments Is my logic used in case 1 applicable here too? Free choice of detector orientations I am not sure but is it related to choosing the basis of measurements? Are my understandings correct? Thanks in advance for your help. 

Yes, exactly what you suggested is true: if there is a sparse $\mathbf{P}$-complete set under log-space many-one reductions, then $\mathbf{P} = \mathbf{L}$. This was conjectured by Hartmanis in 1978 and proven by Cai and Sivakumar in 1995. See this paper. Hartmanis also conjectured that if there is a sparse $\mathbf{NL}$-complete set under log-space many-one reductions, then $\mathbf{NL} = \mathbf{L}$. This was also proven by Cai and Sivakumar in 1997; see this other paper. 

One non-uniform "space hierarchy" that we can prove is a size hierarchy for branching programs. For a Boolean function $f: \{0, 1\}^n \to \{0, 1\}$, let $B(f)$ denote the smallest size of a branching program computing $f$. By an argument analogous to this hierarchy argument for circuit size, one can show that there are constants $\epsilon, c$ so for every value $b \leq \epsilon \cdot 2^n / n$, there is a function $f: \{0, 1\}^n \to \{0, 1\}$ such that $b - cn \leq B(f) \leq b$. I think separating $\mathbf{PSPACE}/\text{poly}$ from $\mathbf{L}/\text{poly}$ would be difficult. It's equivalent to proving that some language in $\mathbf{PSPACE}$ has super-polynomial branching program complexity. A simple argument shows that $\mathbf{PSPACE}$ does not have fixed-polynomial-size branching programs: Proposition. For every constant $k$, there is a language $L \in \mathbf{PSPACE}$ so that for all sufficiently large $n$, $B(L_n) > n^k$. (Here $L_n$ is the indicator function for $L \cap \{0, 1\}^n$.) Proof. By the hierarchy we proved, there is a branching program $P$ of size $n^{k + 1}$ that computes a function $f$ with $B(f) > n^k$. In polynomial space, we can iterate over all branching programs of size $n^{k + 1}$, all branching programs of size $n^k$, and all inputs of length $n$ to find such a branching program $P$. Then we can simulate $P$ to compute $f$. 

I have been going through Eddie Farhi's 6-pages long pre-Adiabatic paper, An Analog Analogue of a Digital Quantum Computation. I guess I understand most of the math and physics but I am struggling with the intuitions used throughout the paper. Here are my questions. Introduction of $|r\rangle$ on page 3: I understand that $|r\rangle$ and $|w\rangle$ are orthonormal. But I don't see why the authors introduced this state. Use of Norm in equation 17 on page 4: The equation is as follows. $$\sum_w |||\Psi_w, t\rangle - |\Psi, t\rangle||^2 \ge N \varepsilon$$ There are more than one kind of norms. What kind of norm are we talking about here? Why? What is the physical significance of this quantity in this context? 

Another example is $\mathbf{ReachUL}$, the class of languages decidable by nondeterministic log-space Turing machines such that for any input and any configuration, there is at most one sequence of nondeterministic choices leading to that configuration. See "An unambiguous class possessing a complete set" by Lange. 

In Appendix B of Boosting and Differential Privacy by Dwork et al., the authors state the following result without proof and refer to it as Azuma's inequality: 

Nothing better than $\mathbf{BPP}/\text{poly} = \mathbf{P}/\text{poly}$ is known. On the other hand, better results are known in the space bounded setting. Fortnow and Klivans showed that $\mathbf{BPL} \subseteq \mathbf{L}/O(n)$ (see this paper for a refinement). It follows that $\mathbf{BPL}/O(n) = \mathbf{L}/O(n)$. 

In 1982, Barahona proved that finding the ground state of an Ising model is NP-hard. Later, in 2000, Istrail proved that it is NP-complete. When I look up the citations of these two papers using Google scholar, it appears that the previous and weaker result has 647 citations while on the other hand the more recent and stronger result has 109 citations. Shouldn't it be the other way round? Isn't a stronger result more useful? 

I am going through the paper, Solving the graph-isomorphism problem with a quantum annealer, by Hen et. al. In the last line of the second paragraph of the second column of page 2, it says,