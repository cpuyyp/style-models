In the same shell it will not work. Try to put the ulimit setting in you .bashrc of root, login as root again and try. You can verify with ulimit -a to verify. 

it would depend on who manages the dns servers. if your providers do, the presumably they would have to have separate zone files (it is better to organize things) etc. to manage your subdomains. if they delegate your domain to your own dns servers and you create your subdomains arbitrarily, then they should have no more overhead at all and have no reason to charge you more. not a legal expert so if they still charge you in that case, i am not sure if would be legal for them to do so. 

is your isp delegating your domain dns to you? to have your own server handling dns for your domain, they must delegate the dns for your domain to you. in that case, you have to give them the ip because your dns servers has not become resolvable yet before you set up your own dns servers at that point. otherwise there will be an infinite loop. these are called glue records and ips must be known to the party that delegates you the dns (your isp that is). either you or your isp have misunderstood something completely. in the latter case you should change the isp right away because they do not know what they are doing. in the former case, probably they are handling your domain in the first place and you don't need your dns servers in any case (though you can still set up forward or cache servers if you want). 

iscsi itself does not support concurrent access to the same lun without causing corruption, unless you have a clustering filesystem. are you sure that your setup is even supported by freenas? since iscsi lun is a block device like a hard drive so you cannot realy have it in multiple client servers. nfs allows shared access, however. 

use LOAD DATA INFILE is the complement of SELECT ... INTO OUTFILE. this should be much faster as there is no sql parsing involved. 

There is no such recording in general, unless the scripts systematically logs to system logs when they run, or they were run as cron jobs. You can look at shell history for all the users, but these have no time recorded. 

You can set up some sort of software load balancer and put your actual server(s) behind the lb virtual ip. Then when the sites are down (if you have no redundancy), the vip can point to a downpage, which can be hosted by something really simple such a tiny instance of amazon ec2 spun up just for this purpose. This is much faster than dns change propagation. You can also automate the process. 

the original files should be one without .rpmnew so for instance, you should have a /etc/ssh/sshd_config diff /etc/ssh/sshd_config /etc/ssh/sshd_config.rpmnew will show you the diff. rpm does this to prevent overriding your settings. if you are looking for the previously installed rpm, you need to find that rpm, and then extract it with rpm2cpio and then cpio to unpack the files. then you should be able to find the default configuration files in the unpacked files. 

you can, as the blocks are striped across all the pairs. however, it will be n x miminum pair size, where n is the number of pairs, because the smallest pair will stop the striping process, and you will then waste all the space in larger pairs, if you don't mind. 

disclaimer: you should read lvm manual carefully and understand what each step does. however, there should be very little risk unless you encounter errors. this is what I do usually i this case. if there is a chance that someone else might do something to mess you up, you want to block any login while doing the maintenance (touch /etc/nologin etc. per your maintenance procedure and company policy). pvmove /dev/md3 # make sure all used extents are moved away pvs -o+pv_used # make sure that no extents are used in /dev/md3 vgreduce vg1 /dev/md3 # now remove the physical volume 

ctime is controlled by kernel, not any userspace tool, tar or any other program, because to back up and restore, they have to create a new inode which must have a new ctime. one known way to preserve it is to image your filesystem to a loopback file, and save that image. unless your filesystem is quite full and you do want to save the whole thing, it is quite wasteful. there are more efficient programs such as partimage, but i have not checked if they preserve ctime of inodes. 

that is cpu socket that you can put cpus in. so it means that you can have up to 4 cpus, though you can leave sockets empty and fewer. 

it looks like but actually not an IDE cable. it is described in detail here: $URL$ I still have one if you really need it :-) thanks to Chris McKeown for pointing out my mistake. 

It is better that you fix the vm datetime. It is quite a complicated issue actually if by vm you mean vmware. $URL$ if it is some other vm system, i think the issue would be similar. turning off consistency check etc. is not advisable because they are there for a reason. a system with time being inconsistent will affect integrity in many other ways. 

there is no universal way at all. using interactive/noninteractive shell detection or tty detection is not reliable either, as other cases than cron can have these characteristics. just add a variable in your cron entry. say you need to run test.sh, then use this instead. 

see what it tells you, in addition to what the server says. you can do this in a powershell or cmd window. if you have already generated your default putty public file, you can convert it with ssh-keygen -i -f yourputtypubkeyfile > openssh.key to be added to your authorized_keys 

expect is just a tcl script app so you can do anything you can do in tcl, such as send [cat 1.txt] you can also open the file and read and write its content with tcl. 

it may not be reading your config, or your config is broken otherwise. use ssh -v to see which data config it is loading from, and paste your output here. the config entry you listed seems to be correct, at least for the current version of ssh. also, make that the first entry to avoid something else clobbering it. what is your distro and ssh version, by the way, so that others with the same can help you more. 

if you root filesystem supports snapshot, you can take a snapshot, and copy the /bin files from the snapshot which should be copied to a partition (forgot you cannot mount it). otherwise, if you have a spare partition, you can use dd to copy the root partition there and get /bin files back that way. likely with an external usb drive. last, if you network works, you can just copy the command back from a similar system. 

after you execute mount, the permission you see is the mounted directory, not the mounting point (directory) that you created. if you umount /mnt/foo successfully, you will see that the mounting point itself has not changed. therefore, if you want to preserve the same ownership after mounting the partition, you need to do a sudo chown desireduser /mnt/foo. 

is the certificate self-signed or signed by CA? if the former you need to generate a new one, and if the latter you should contact your vendor CA support. or you just have to purchase a new one from some vendor if you do need a CA-signed one. 

Quite sometime back, I have worked with servers in some ISP where I can access via ftp/sshfs etc, but no interactive shell access, to the server. One workaround they have is to allow you to set up a cron/at job via a web interface, or a copy of crontab file that you can edit, that will invoke scripts/commands. Ask the administrator if that is possible, if interactive shell absolutely cannot be granted. 

your configuration says: rotate 2 that means log files are rotated 2 times before being removed, so logrotate only cares about 2 files. my guess is that the configuration was changed at some point, because previously more log files were kept, maybe it was something like rotate 28. these older files you have to remove manually. 

Use file system acl (man setfacl for details) so that the owner can give read/execute access to these directory to myuser $URL$ please let me know if you want more help. 

the parent processes of these processes died but they themselves did not terminate so they have become orphans and thus adopted by process 1, i.e., init. the software usually has problems when this happens a lot, and you may want to consider upgrade to a newer version. 

No dynamic change of an existing connection is supported, because the forwarding is set up when the connection established, so you have to reconnect in order to enable/disable this. 

that means the key exchange did not take place or failed. please download the cli tool plink.exe (same place you downloaded putty.exe) and do some debugging 

Try 2 more things. 1. Lower the innodb buffer pool size. 2. Edit mysql initial script and add --innodb option. I wonder also if your package is buggy. Could you try a different minor version? Also, I assume your mysql server got upgraded as well? Maybe that version is broken? Precise is not final yet. 

You are likely running into this issue: $URL$ remove the /3GB switch from your boot.ini and try again, if you have one and if this is your problem. 

this is the correct behavior indeed, as whatever comes after your page will be in PATH_INFO cgi variable used by server side scripting often. 

if the appliance is shut down, you can mount (unless they somehow disabled that too but i doubt it) the root vmdk in another vm that can read/write the vmdk fs. you can then try to change or add some admin/root auth once you find where the info is. it is possible that the vmdk fs is encrypted. in that case you should contact the vendor, which probably is a good idea in any case.