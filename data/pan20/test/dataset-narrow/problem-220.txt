The quick hack fix for this is to create a job that monitors the size of the cache and runs when it exceeds a certain size. IIRC (I'll check tomorrow and update) I've set the limit at 100MB on servers where this caused a problem. SP3 introduces a mechanism to control the cache size, without having to brute force clear it. Original answer: Might take a bit of to and fro but can we start with a look at the IO and wait statistics? (scripts in GIST) Modify your create/drop script to run at the start, followed by the wait stats script after your final drop. Result will probably be easier to read if you post them as CSV (so they can be copied to Excel). Could you also run and record the number of rows returned. 

reports a count of statements, not active transactions. From a different perspective, it is reporting the depth of a nested transaction. 

I wouldn't delete it, just in case you run into problems. If space isn't an issue create a copy of the original files with the instance offline, rename the original (so they aren't found on restart) then try: 

Licensing is always complex. Microsoft licensing is at least as complex as other vendors. Unfortunately, my experience has been that they have not yet mastered the art of training their own, or their resellers employees about license models. There is much fun to be had by locking a Microsoft licensing specialist in a room with a reseller specialist and watching them argue about the finer points of SQL Server or MSDN license arrangements. If you think SQL Server is a minefield, try MSDN. NB: The "helpful" diagram at the top of the article explaining edition compute capacity limits neatly demonstrates why folk lose their way. 

Remember that a clustered index is the table. The only overhead that exists for a clustered index over and above that for a heap (which is generally undesirable) is for the non-leaf index pages and the cluster key's inclusion in all non-clustered indexes for that table. This is why narrow cluster key's are preferred. Kimberley Tripp's articles on clustered key choices are an excellent reference for this. 

SQLQueryStress is a great tool for small scale concurrency testing but it isn't really up to the job of load testing. A surprisingly little known toolset is available for free, from Microsoft, that can cope with most SQL Server stress testing scenarios, the RML Utilities. A very brief outline of the tools: 

You're heading in the right direction but I'd use different terms. An invite is a request to attend an event of some sort, so more appropriate naming would be: Event (EventId, OwnerUserId) Invite (EventId, UserId) 

Next, from a database agnostic perspective, is an understanding of what a database server is built to do; provide Atomicity, Consistency, Isolation and Durability to your transactions and data. Commonly misunderstood, frequently the cause of performance problems and the primary source of data inconsistency. For your chosen platform, get into the internals of how ACID compliance is implemented. Look for topics like what the transaction log does, what write-ahead-logging is, isolation levels and storage internals. Understanding key aspects of database internals makes other aspects of DBA work, performance tuning and resource management for example, much easier to grasp. 

You'll be hard pressed to find any modern software department that isn't operating some variant of Agile. DBA's by comparison are stuck in the dark ages, with the kind of thinking that @RobPaller's answer contains still common place. Modifying a database schema has never been as easy as modifying code, which is why there has been reluctance to embrace an agile approach to database development and maintenance. Now that we have the tools and techniques to operate in a similar manner to developers, we most definitely should. Just because it isn't easy to change schema, doesn't mean you can't and that you shouldn't. I'm not advocating a haphazard approach to database design (see comments), merely an approach which more closely mirrors that of an agile development team. If you're part of an agile project, you aren't going to have requirements for work that may (or may not) occur in the future so design for what you know is needed, not what might be. I guess that puts my vote with your option 2 and I suspect I might find myself standing in the cold on this one! 

and Google or a reference list of SQL Server versions. SQLServerBuilds is the popular unofficial list, KB321185 is the official list. I'm not aware of any way to obtain a complete list of all patches that have ever been applied to an instance. It's likely buried in the registry somewhere but I haven't fished for it. 

Kyle Brandt has an opinion on Broadcom network cards which echoes my own (repeated) experience. Broadcom, Die Mutha My problems have always been associated with TCP Offload features and in 99% of cases disabling or switching to a-n-other network card has resolved the symptoms. One client that (as in your case) uses Dell servers, always orders separate Intel NICs and disables the on-board Broadcom cards on build. As described in this MSDN blog post, I would start with disabling in the OS with: 

So you just need to execute the collection of schema scripts to recreate your database? You could execute them one by one or create a batch file and use SQLCMD to execute all script files in directory 

Yes, it is available in the installer for development edition. Without firing it up, I'm not sure if the installer has changed between 2008 and 2008R2 but in 2008 there are two options for installing client tools. 

You can't prevent the locking of rows that are being updated or inserted, else ACID compliance would be compromised. You can reduce the locking on rows that are being read but you may need to consider the opposite and increase the isolation level. If other processes insert or update records while this update occurs, you could end up with a miscalculation. As per Isolation Levels in the Database Engine... 

In reality, high availability and disaster recovery are a trade off between uptime, Recovery Point Objective, Recovery Time Objective and cost. Zero data loss and zero downtime is expensive, complicated to setup and complicated to maintain. There is no free lunch. 

Maybe virtualisation would be a better fit. Maybe cloud would work. Maybe hybrid cloud. Honestly, it really does depend. Call in an expert to help, they could save you a fortune. 

Assuming your intended option B is to have UserKey in Alerts and for UserKey to be an INT rather than VARCHAR(50)... you might save some space and improve performance marginally if you expected there to be a very large number of alerts per user. But it's going to be minimal so I'd probably stick with option A. The inefficiencies of GUIDs are due to their size (16 byte) compared to an INT (4 byte) and the fragmentation they inevitably lead too, unless sequential. This is magnified where a GUID is used as a clustered index key (as is the case for your User table?) because all non-clustered indexes contain the clustered index key. Kimberley Tripps series of articles on the topic are good grounding for understanding this. Option B 

Certainly it can be cheaper to scale out across multiple lower spec machines than to scale up a single machine. Although, the lower hardware costs may be dwarfed in the total cost of ownership when the soft costs of additional development effort and operational complexity are factored in. If this isn't SOA and you just have a case where the component services of this platform are being built by different teams/suppliers for logistical reasons, stick with a single database and completely ignore everything above! :) 

This is a purely academic question, in so much that it isn't causing a problem and I'm just interested to hear any explanations for the behaviour. Take a standard issue Itzik Ben-Gan cross-join CTE tally table: 

BCHR can be very misleading due to read-ahead: The Database Engine supports a performance optimization mechanism called read-ahead. Read-ahead anticipates the data and index pages needed to fulfill a query execution plan and brings the pages into the buffer cache before they are actually used by the query. This allows computation and I/O to overlap, taking full advantage of both the CPU and the disk.[link] You are very likely seeing the impact of read-ahead filling the buffer cache pre-emptively, which distorts the BCHR counter. Excellent and very in-depth article from Jonathan Kehayias titled Great SQL Server Debates: Buffer Cache Hit Ratio covers this. 

You can get this (and more) from Dynamic Management Views (DMVs). To get statistics for a particular stored procedure, try the following query. 

Of course, this may be a really bad idea. All we've achieved is swapping a clustered index seek with a non-clustered index seek. Without seeing the full query, its impossible to say whether its the "right" thing to do. My initial thought is that a clustered index seek (key lookup) for 500 rows in what appears to be an analytic type query is likely to be reasonable. You refer to this lookup as being slow but in what context? Is it just the highest estimated cost operator in the plan? If ultimately this query is looking at data across a timespan, it may be that DateForValue would be a better choice as the leading key: 

Shut down the applications accessing the database. BCP export the data, table by table to files. As disk corruption could be the source of your woes, export the data to a portable drive or to a fileshare on another machine. Restore your old backup to a known good server. Depending on the complexity of the database schema either truncate the data from your restored backup or script out the schema and create a new empty database. BCP the data back in. 

For SQL Server we have the option of the FORCE ORDER hint. The only comparable I'm aware of for MySQL is STRAIGHT_JOIN. 

What you're trying to do would leave the database in a (transactionally) inconsistent state, hence it isn't possible. The Partial Database Availability whitepaper is a useful reference guide and includes an example of how to check whether a particular table or file is online. If your data access were through stored procedures, you could relatively easily incorporate that check. One alternative (but somewhat hacky) approach that might be worth a look in your scenario would be to hide the table and replace it with a view. 

So yes, the maximum number of tables is limited by the 2,147,483,647 object maximum in Express. Compact Edition differs substantially from Express, with very different capacity specifications. Haven't found an explicit reference to the maximum table or object count in CE but the 4GB size limit would prevent you getting anywhere near 2,147,483,647. 

Or could be an issue with MSDTC configuration, which is a laborious and lengthy checklist to run through, documented fully in Recommended MSDTC settings for using Distributed Transactions in SQL Server. 

Edit: So what's your point then? @AndrewBickerton mentioned in a comment that I've not answered the original question so I'll try and summarise "Why should I use Visual Studio 2010 over SSMS for my database development?" here. 

That would depend on the queries against the table. If query patterns are such that they target a subset of the 500 or 100 rows that your filtered index covers, perfect. On the flip side, it's unlikely that the optimiser is going to choose a filtered index that includes 85% unless it is a covering index for a particular query. I haven't tested this but I would expect filtered index utilisation to exhibit the same tipping point behaviour as non-filtered indexes. 

Install .net 4.5 on a server running a .net 2.0 application and what happens? Nothing, you're application continues to use the 2.0 framework. Update your LastIndexOf function on a server hosting 3 applications databases (as part of an upgrade to one of them) and what happens? All three are now using the latest version. The alternative is the approach adopted by SQL#. This is installed to a schema in each user database, so you can safely upgrade the database for Application-X without risking the stability of Application-Y. If you're working in a tightly controlled change management environment you'll have no choice, you can't upgrade a shared component without testing all consumers of the component. If you're working somewhere a little more "fast and loose", you're free to take your chances with breaking something unintentionally with a patch/upgrade. 

If you want to add additional storage to the database by way of additional files, that is also possible. 

While there is variation across drive models, you can apply a rough and ready figure to each of the common rotational speeds. 

In an ideal world you would have two choices, SNAPSHOT and READ COMMITTED SNAPSHOT (RCSI). Make sure you understand the basics of transaction isolation levels before you decide which is appropriate for your workload. Specifically be aware of the different results you may see as a result of moving to RCSI. This sounds like it's not an ideal world as you don't have any control over the application that is generating the select statements. In that case, your only option is to enable RCSI for the database in question such that the selects will automatically use RCSI instead of READ COMMITTED.