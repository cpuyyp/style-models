Note that although this sounds like $\mathsf{ZPP}$, it is not since $\mathsf{ZPP}$ is the class of predicates, and not multifunctions. I really appreciate any pointer to the right sources. 

GNU grep is a command line tool for searching one or more input files for lines containing a match to a specified pattern. It is well-known that grep is very fast! Here's a quote from its author Mike Haertel (taken from here): 

[Mark tried to enroll in many courses (well above the limit), and explore different areas of Mathematics and Computer Science when he was an undergrad.] Try to attend lectures and seminars on different topics in your department. When you're in your upper years, you should also ask for permission to audit graduate courses related to your interest. Also depending if you're majoring in Maths or CS, you also have to plan courses you should take to prepare you a solid basic foundation. If you're a Maths undergrad, then you should take more CS courses in algorithms and complexity which give you a more "algorithmic" mind. If you're a CS or Engineering undergrad, then it's always a good idea to learn some basic Maths courses in: 

In general, whenever you study a certain algorithm or data structure, if somehow the exposition in your textbook is not clear enough for you, then the best way is to search on google for lecture notes of that particular topic. In may cases, different explanations of the same thing eventually give you the complete picture. At least, that's how it works for me. 

(This is an interesting question for me because I'm also reading about the Pfaffian.) I suggest the following references: 

I think courses on algorithm design and computational complexity are always challenging for students who not familiar with these subjects because they do require some degree of mathematical maturity and problem solving skill. In my first graduate course on "computational complexity", a friend of mine who had his degree in pure mathematics told me how surprised he was by the fact that although that course didn't require much maths background (at least that's what was told in the course outline), it actually required nearly all the skills he got through all of his pure maths undergrad degree! I found that I got to know about "the way" most (when I first start my graduate study) by reading and doing exercises from Sipser's book. Be sure that you do the exercises because problem solving skill and mathematical maturity is what you want to learn and not just a bunch of facts or definitions. However, Sipser's book is only good for complexity and NP-completeness stuffs, it won't suffice to substitute the CLRS book. The only problem with CLRS book is that its advantage of comprehensive coverage might become its weakness since the book might look quite scary or overwhelming for students. So my advice is that you should really go to the library and search for books on algorithms, scan through one by one and choose the ones that fit your thinking pattern most. And again don't forget to do exercises! For algorithms, I personally suggest the following books (besides the ones suggested by Sadeq and JeffE): 

I am also aware of Beating Exhaustive Search for Quantified Boolean Formulas and Connections to Circuit Complexity by Rahul Santhanam and Ryan Williams where it is proven that improvement on the complexity of QBF implies strong circuit lower bounds but their results cannot be applied directly to $H_k$. 

For $\Rightarrow$, you have "Communication Complexity", Eyal Kushilevitz in Advances in Computers, Volume 44, 1997 ($URL$ You can also find a section "Communication Complexity and Chomsky Hierarchy" in the book "Communication Complexity and Parallel Computing" by Juraj Hromkovič where it is proven. It may be that $\Leftarrow$ is also proven somewhere in the book but I fail to find it here. The closest thing there seems to be is Exercise 5.2.5.2 but, it is only an exercise (see the whole Chapter 5 too, which extensively study finite automaton but I do not think it explicitly answers your question). For what it is worth, the proof of both directions looks easy so I think that if you need it in a paper you can sketch it quickly: for $\Rightarrow$, take a finite automaton for $L$ and observe that it is sufficient for Alice to communicate the state she reaches after having read her part of the input. Then Bob finishes the simulation in the automata. For $\Leftarrow$, if you have a protocol bounded by a constant, then $L$ has a finite number of quotient $w^{-1}L = \{u : wu \in L\}$ which is a well-known characterization of regular languages. 

First, you have to know that enumeration complexity is not as cleanly defined as other fields of complexity theory. People are still looking for the right definitions. Notions of reductions or completeness for example do not seem to generalize well. You will then have plenty of different names and notations for the same thing depending on the authors. For general overview of these notions, you can read the preliminaries of several thesis: Wojciech Kazana [4], Yann Strozecki [5] etc. The preprint [2] you cite is still a work in progress. Now a clarification: Valiant does not use his Counting Turing Machines for defining P-enumerable. He uses them to define #P then reverts back to deterministic TM and defines P-enumerable. He is not really interested in enumeration complexity. He only vaguely defines it (he says that the set can be enumerated without defining it formally) to show that some #P-complete problems can still be enumerated (using fact 7, also known as "flashlight method" for enumeration). Finally the answer. EnumP and P-enumerable are different indeed but not comparable. EnumP only says that the predicate you want to enumerate behave "nicely", that is, you can verify in PTIME that the outputs of your algorithms are indeed solutions. It does not says anything on the complexity of the problem of enumerating. The notion of P-enumerable of Valiant corresponds to what is called Output polynomial algorithm, that is, you can enumerate the set in time polynomial in its size and the input size. This is what is called $OutputP^F$ in [2], as the "uniformity" condition (that the predicate should be in EnumP) is not present in Valiant's work. For example $SAT(\phi) = \{\tau \mid \tau \models \phi \}$ is in EnumP but not P-enumerable. See Proposition 7 of [2] for example to see a problem P-enumerable but not in EnumP. As for the choice of RAM over TM for enumeration, the polynomial time is usually not affected by this choice, so this does not make a huge difference for complexity classes. However, such a choice is common in enumeration complexity as people are often interested in constant or linear delay, which is more robust on RAM than on TM (see for example, Wojciech Kazana thesis [4] for constant delay or the work of Étienne Grandjean for consideration on the class of linear time solvable problem). [4] $URL$ [5] $URL$ 

$M$ returns a correct output of the function with high probability $M$ can either return a correct answer or "don't know", and never return an incorrect answer. 

If I don't misunderstand what you mean by AND&OR gate, it is basically a comparator gate which takes two input bits $x$ and $y$ and produces two output bits $x\wedge y$ and $x\vee y$. The two output bits $x\wedge y$ and $x\vee y$ are basically min$(x,y)$ and max$(x,y)$. Comparator circuits are built by composing these comparator gates together but allowing no more fan-outs other than the two outputs produced by each gate. Thus, we can draw comparator circuits using the notations below (similarly to how we draw sorting networks). 

We can define the comparator circuit value problem (CCV) as follows: given a comparator circuit with specified Boolean inputs, determine the output value of a designated wire. By taking the closure of this CCV problem under logspace reductions, we get the complexity class CC, whose complete problems include natural problems like lex-first maximal matching, stable marriage, stable roomate. In this recent paper, Steve Cook, Yuval Filmus and I showed that even when we use AC$^0$ many-one closure, we still get the same class CC. To the best of our knowledge at this point, NL $\subseteq$ CC $\subseteq$ P. In our paper, we provided evidence that CC and NC are incomparable (so that CC is a proper subset of P), by giving oracle settings where relativized CC and relativized NC are incomparable. We also gave evidence that CC and SC are incomparable. 

Note that these two books cover much more than just randomized algorithms, e.g. they cover Probabilistic Method, Markov Chain Theory, Martingales etc, of course with many applications in TCS. The first book is easier to read with many examples whose proofs were worked out in detail. The second book is really a classic, not very updated, but still very useful. They both have a lot of exercises, so you will have plenty of material to practice what you've learnt. 

In [1], the authors prove that MaxSAT parametrized by the clique-width (resp. neighbor diversity) of the incidence graph of the CNF formula has an FPT-AS (Fixed Parameter Tractable Approximation Scheme) but it is known that MaxSAT parametrized by clique-width (resp. neighbor diversity) is W[1]-hard. The theorem mostly relies on a result of [2] that roughly says that a graphs of bounded clique-width without large cliques also has bounded treewidth.They thus smartly trim the formula so that they do not have large clique in the incidence graph and solve the reduced formula in FPT time using a well-known algorithm for MaxSAT on bounded treewidth. I guess this approach may work in other problems as well. [1] Holger Dell, Eun Jung Kim, Michael Lampis, Valia Mitsou, Tobias Mömke: Complexity and Approximability of Parameterized MAX-CSPs. IPEC 2015 [2] Gurski, F., & Wanke, E. (2000, June). The tree-width of clique-width bounded graphs without K n, n. In International Workshop on Graph-Theoretic Concepts in Computer Science (pp. 196-205). Springer, Berlin, Heidelberg. 

In "Branching Programs and Binary Decision Diagrams" by Ingo Wegener [1] (very good, complete reference to check this kind of fact on branching programs), Section 5.7 deals with how you can transform a given $\pi_1$-OBDD $F_1$ into an equivalent $\pi_2$-OBDD $F_2$ by using syntactic rules. If you have a bound $B$ on the representation of $F_1$ by $\pi_2$-OBDD then you can do this in time polynomial in $|F_1|$ and $B$. He uses it to prove Corollary 5.7.11 which answers your question and a bit more: you do not need to assume the width is bounded. Corollary 5.7.11 (in [1]): Given $F_1$ a $\pi_1$-OBDD and $F_2$ a $\pi_2$-OBDD, one can check if $F_1$ is equivalent to $F_2$ in time $O(|F_1| \cdot |F_2| \cdot \log |F_2|)$. [1] Wegener, Ingo. Branching programs and binary decision diagrams: theory and applications. Society for Industrial and Applied Mathematics, 2000. 

So if you want to reduce the error to exponentially small, then the running time will also become exponential! Note that both of the results you mentioned show that approximating counting can be done within the second level of the polytime hierarchy $\mathsf{PH}$ or so. On the other hand, $\#\mathsf{P}$ is the class of exact counting, which is widely believed to be harder than approximate counting. Otherwise, $\#\mathsf{P}$ is also contained in $\mathsf{FP}^\mathsf{NP}$, which implies that $\mathsf{PH}$ will collapse at least to the second level. 

The sorting problem is actually complete for $\mathsf{TC}^0$ (under $\mathsf{AC}^0$-reduction). A standard source for this is Section 1.4.3 of Vollmer's book. Note that $\mathsf{TC}^0$ is the class of decision problems, but we usually think of sorting as a function problem, i.e., we want to output the numbers, say, in nondecreasing order. However, we can also define sorting as a decision problem as follows: 

I'm surprised that Petri Nets have not yet been mentioned! Extensions of Petri Nets like Coloured Petri Nets or Petri Nets with inhibitor arcs are Turing-complete. 

I would like to ask if there is a name for the class of multifunctions, each of which can be computed by a probabilistic polytime Turing machine $M$ satisfying the following two conditions: 

Any PP-complete problem is trivially in PSPACE, but of course not known to be PSPACE-complete. We also don't know whether or not PP is contained in PH either, though it follows from Toda's theorem that PH is contained in P$^\text{PP}$. I believe the same also applies for #P-complete problems. 

A nice reference is "Part C" of Handbook of Mathematical Logic edited by Barwise. Part C includes the following chapters: 

Probability and Computing: Randomized Algorithms and Probabilistic Analysis by Mitzenmacher and Upfal. Randomized Algorithms by Motwani and Raghavan 

I strongly recommend the book Computational Complexity: A Modern Approach by Arora and Barak. When I took computational complexity at my Master level, the main textbook is Computational Complexity by Papadimitriou. But, maybe due to my background in Software Engineering, I found the writing in Papadimitriou challenging at times. Whenever I had problem understanding Papadimitriou's book, I simply went back to Sipser, or read the draft of Arora and Barak. In retrospect, I really like Papadimitriou's book, and I often find myself looking up from this book. His book has plenty of exercises that are quite effective at connecting readers to research-level questions and open problems. In any case, you should have a look at both Papadimitriou and Arora-Barak. People also suggest Oded Goldreich's textbook, but I really prefer the organization of Arora-Barak.