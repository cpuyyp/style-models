There are some cases where LabelEncoder or DictVectorizor are useful, but these are quite limited in my opinion due to ordinality. LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space. One-Hot-Encoding has a the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction. I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature. Hope this helps! 

It will have very little effect The answer most will give is that it will have no effect, but adding one more feature will decrease the ratio of records to features so will slightly increase the bias and will hence make your model slightly less accurate. Unless, of course, you have overfit your model , in which case it will make your model slightly more accurate (a good data scientist would never do this because they understand the importance of cross-validation :-). If you normalize your data and then attempt some sort of dimensionality reduction, your algorithm will immediately eliminate the feature that you added since it is perfectly negatively (linearly) correlated with the first feature. In this case it will have no effect. Please also consider the following: I always see big red flags when someone asks a very fundamental data science question with the words included. Neural networks are very powerful and receive a great deal of attention in the media and on Kaggle, but they take more data to train, are difficult to configure, and require much more computing power. If you are just starting out, I suggest getting a foundation in linear regression, logistic regression, clustering, SVMs, decision trees, random forests, and naive Bayes before delving into artificial neural networks. Just some food for thought. Hope this helps! 

Also think about taking some sort of online MOOC as this will help you gain some footing in the subject. Andrew Ng's Coursera on Machine Learning is highly recommended as a starting point and include some linear regression during the first portion and scikit-learn is a great Python based library. 

@Olologin is correct for very basic direct solves with no optimization, but there are optimizations that exist, so I wanted to add a few points... A great many mathematicians spend their entire careers on speeding up and optimizing this problem. So, the solution depends a lot on the composition of the matrix (), whether the problem is , whether it is being solved or , and what algorithm is being used. For lots of data science problems $O(n^2)$ and better solves are readily achieved. Some common libraries include: 

I play around quite a bit with location data and have found examples both where k-means works fine and where k-means is a poor representation and DBSCAN is a great fit. If you've ever gone hiking or mountain climbing on a day with fog or a low cloud cover, there are times where you get to the top of the peak and can only see the surrounding peaks poking up through the clouds. I like to use this analogy when I think of DBSCAN. The density filtering allows one to select a threshold where data is kept, and all of the remaining data is filtered out. Take a look at this Seattle crime incident data. Suppose I want to cluster the data by location to form pseudo neighborhoods i.e. neighborhoods that are roughly defined the the geography of where criminal events tend to occur. This is an example of k-means working just fine in location analytics: Seattle crime data superimposing a primitive map with k-means clustering 

See how you can sum the values and get for both, but average the sentiments and get vs. . There are cases where people have seen better results measuring the total number of good sentiment words rather than letting zero value words cloud the picture. In this case, I would argue that the sentences have the same sentiment and averaging artificially pulls down the sentiment of the second sentence. In certain methods like , the word vectors are normalized against are relevant corpus, so this may change things. In other methods, the may have already been removed, so this has less effect. In twitter examples, users are limited to 140 characters, so one might consider this to be an inherent denominator which makes averaging really averages of averages. Its very dependent on the specific application and methodology. Without a more specific reference, it is difficult to be more specific in my answer. Hope this helps! 

Just unroll the matrices into a large set of features: If all of the matrices are the same size, you can just unroll them into a large feature set e.g. a 20 x 20 matrix turns into 400 features. You can do this with multiple different feature matrices. Its up to the learning algorithm to infer each feature's meaning, so don't overthink the lack of human readablility. Take a look at tutorials on digit recognition in scikit learn and you will see that the "image pixel matrices" have been similarly unrolled. You can then employ or a nonlinear dimensionality reduction scheme to select a subset of the feature space. Though it might go against your intuition, the dimensionality reduction will likely improve your classification algorithm. Accounting for different lengths of sounds is a difficult problem because normalization will alter the frequency and thus affect your Fourier Transformation. I suggest defining some sort of characteristic sound length and subsampling long sounds $n$ times and oversampling short sounds $n$ times. Hope this helps! 

And you can think of each one of those binary features as an orthogonal dimension in your data that lies in a 8-dimensional space. Hope this helps! 

Take a look at some examples of parametric and analogous nonparametric tests from Tanya Hoskin's Demystifying Summary: 

Supervised Learning: Do you have a saved time history of the data? For a supervised learning set you need some churned="No" cases and some churned="Yes" cases, but it sounds like you only have churned="Yes" and the unknown cases e.g. current customers who may or may not churn. With some time history you can go back in time and definitively label the current customers as churn="No". Then it is very easy to split up the data. And no, you probably don't want to predict on any data that you trained on since you can only train on it if you already know the solution so it will be a waste of time and throw off any metrics you might use to assess accuracy (precision/recall/F1) in the future. Unsupervised Learning: If you don't have saved time history of the data then this is an unsupervised learning set for which you have churned="yes" and churned="maybe". You could then employ anomaly or outlier detection on this set. novelty detection: The training data is not polluted by outliers, and we are interested in detecting anomalies in new observations. outlier detection: The training data contains outliers, and we need to fit the central mode of the training data, ignoring the deviant observations. You can do either one but novelty is more powerful. This is kind of a flip around as the novelty here is Churned="No" since all of your data is the confirmed Churn="Yes" cases. Hope this helps! 

This varies from 0 to 1 and accounts for magnitudes of the constituent vectors e.g.: $$s_{\nu}[(1,1),(1,1)]=1$$ , where as $$s_{\nu}[(1,1),(2,2)]=\frac{4}{5}$$ Hope this helps!