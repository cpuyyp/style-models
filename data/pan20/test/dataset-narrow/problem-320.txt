I am using Merge Replication with SQL 2012. I look in the snapshot directory, but the largest file in there is a prc file which is 646 KB. I know for sure that the biggest of my replicated tables is 25 MB in the database after replicating, so I am not sure I understand why there aren't larger files in the snapshot directory? Also is there a place I can look for the snapshot files as they are downloaded to the subscriber? For instance the merge agent outputs messages such as, 

I am running merge replication in SQL2012 using web sync. Has anybody done any experiments to compare the performance of web sync to connecting to replication using a straight TCP/IP connection? Just wondering if it might work to open up a port for a TCP/IP connection over the web and not to use web sync. If it could work is it faster? 

The problem is after step 2. We can force a software upgrade to the clients so that data is written into the new field. But some data may have been modified in the old field at one of the subscribers before they do the sync which makes the schema change. This would also be before they were forced to do the update. The migration could be done more than once, but by the time you get to step 4 its hard to know which modifications have been made to the old field, and which have been made to the new field. 

The answer at this point appears to be quite simple. I was using a different domain account to run the subscription agent. I had to log into my machine with that account and install the security certificate. 

I went through the same exercise deploying hardware along the lines of Microsoft's Fast Track Data Warehouse strategy. Their Fast Track Data Warehouse Reference Guide for SQL Server 2012 covers the topics you discovered and already tested, like MCR (maximum consumption rate) for a query with data already in the buffer pool and BCR (benchmark consumption rate) for a real-world query where some, if not all data, needs to come from the disk subsystem. Query complexity also affects the overall throughput (using a TPC-H benchmark dataset, their example shows throughput rates from 56 to 201 MB/s per core). 

I am looking to eliminate an extraneous data file in a filegroup in my DB but don't want to use the dreaded DBCC SHRINKFILE method, I am preferring to rebuild indexes into an alternate FG, perform the shrink with EMPTYFILE followed with removal of the file and then rebuild back into the original filegroup. Is there a method to identify which table/index objects are populating the utilized pages/extents in the target database file? 

I'm implementing PBM on a SQL Server 2014 instance and want to enforce a condition that all objects are created outside the schema and all object names are created in title-case (i.e., OrderDetail). I have the regular expression drafted but am not able to create a satisfactory object name with it. An example output is here. 

I'm running SSIS packages stored in a SQL Server 2014 catalog via scheduled SSA jobs. I'm reviewing my SQL Audit capture and noting calls to SSISDB are coming from the proxy I created when looking at the value for but the SID showing up in doesn't correspond to the Active Directory SID. My proxy has an AD SID prefixed with the typical 'S-1-5...' but the audit is returning a SID with a prefix of 'S-1-9...'. Is that indicative of a contained user for SSISDB dynamically created at job runtime to execute the SSIS task? 

I'm just wondering what happens if you do some kind of bulk insert for instance which inserted 5000 new records into this table. What would happen to the identity column then? 

The user inserts a record 'Shape' at a client The 'Shape' has a default constraint to set the NumberOfSides field to 4 The client user edits the value of NumberOfSides and sets it to 5 They sync with the server 

So it is definitely the case statement. And the issue does not resolve if I use ELSE 0 in the case statement. On my actual query I have a where clause (the where clause on the very first query above) which selects only 3 records as a proof of concept, and they all have 8022 as the value. The ELSE 0 never gets used if I add it and I still get the same error. 

Adding the new field Migrating the data from the old field to the new field Wait for a while, perhaps a month Delete the old field 

I am running merge replication with SQL 2012. There seems to be a nasty consequence of the delete triggers added for replication in SQL 2012. Inside the delete triggers are this, 

I am using SQL server, and I have been looking closely at the concept of key lookups, $URL$ So if you have a key lookup you can create an index with the 'include' columns to cover the non index columns you have in the select statement. For instance, 

I am using merge replication in SQL 2012 with web sync. When I insert records into the top tables in my filter hierarchy it causes a lot of locking on the tables, including the merge replication tables such as MSMerge_contents. It means that no other process can insert into the same tables at the same time. Because of my particular setup this is a problem. Is there a way to reduce the amount of locking / blocking that occurs in the merge replication insert triggers? 

But right now, it's been running for 90 minutes. I'm watching via Spotlight that the new data file is being populated. I spot-check the UTC dates on all the tables in the DB in Prod and confirm nothing is dated after 1/1/2017. I can understand the engine needing to seek/scan indexes to confirm nothing has to move to the FG, but if no records are qualified to move, why all the data movement? 

Min/Max Server Memory were set at defaults and the most resource intensive query was the backup history check by Quest Spotlight. Only 1.6 GB of RAM was utilized, 1.5 of which by buffer and procedure cache. After setting Max Server Memory to 3 GB, PLE started to climb and is currently at 21 minutes. I am wondering why Max Server memory would have this positive impact on PLE when less than half the onboard RAM is currently utilized. My first thought is O/S memory trimming was in play before the change in the setting but how can I confirm this? 

Is this particular expression not supported by PBM or is there alternate syntax I should use in the condition? 

I'm not planning on creating 2016Q3 and 2016Q4 quarters as I don't want to incur the data movement between the filegroups. I've elected to start with 1/1/2017 and create a 2017Q1 filegroup. I execute the following, anticipating that it'll be a quick meta modification. 

C:\SQLIO>sqlio -kR -t8 -s120 -o8 -fsequential -b64 -BH -LS -Fparam.txt sqlio v1.5.SG using system counter for latency timings, 2211143 counts per second parameter file used: param.txt file L:\testfile.dat with 8 threads (0-7) using mask 0x0 (0) 8 threads reading for 120 secs from file L:\testfile.dat using 64KB sequential IOs enabling multiple I/Os per thread with 8 outstanding buffering set to use hardware disk cache (but not file cache) using specified size: 40000 MB for file: L:\testfile.dat initialization done CUMULATIVE DATA: throughput metrics: IOs/sec: 25160.07 MBs/sec: 1572.50 latency metrics: Min_Latency(ms): 0 Avg_Latency(ms): 2 Max_Latency(ms): 8 histogram: ms: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24+ %: 24 33 12 7 7 9 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 C:\SQLIO>sqlio -kW -t8 -s120 -o8 -frandom -b8 -BH -LS -Fparam.txt sqlio v1.5.SG using system counter for latency timings, 2211143 counts per second parameter file used: param.txt file L:\testfile.dat with 8 threads (0-7) using mask 0x0 (0) 8 threads writing for 120 secs to file L:\testfile.dat using 8KB random IOs enabling multiple I/Os per thread with 8 outstanding buffering set to use hardware disk cache (but not file cache) using specified size: 40000 MB for file: L:\testfile.dat initialization done CUMULATIVE DATA: throughput metrics: IOs/sec: 153634.35 MBs/sec: 1200.26 latency metrics: Min_Latency(ms): 0 Avg_Latency(ms): 0 Max_Latency(ms): 1 histogram: ms: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24+ %: 100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 C:\SQLIO>sqlio -kR -t8 -s120 -o8 -frandom -b8 -BH -LS -Fparam.txt sqlio v1.5.SG using system counter for latency timings, 2211143 counts per second parameter file used: param.txt file L:\testfile.dat with 8 threads (0-7) using mask 0x0 (0) 8 threads reading for 120 secs from file L:\testfile.dat using 8KB random IOs enabling multiple I/Os per thread with 8 outstanding buffering set to use hardware disk cache (but not file cache) using specified size: 40000 MB for file: L:\testfile.dat initialization done CUMULATIVE DATA: throughput metrics: IOs/sec: 181107.89 MBs/sec: 1414.90 latency metrics: Min_Latency(ms): 0 Avg_Latency(ms): 0 Max_Latency(ms): 5 histogram: ms: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24+ %: 100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 

If more records are added to Table2 I would want the contents of Table1 to be changed accordingly when the sync process next occurs. I found out that this is not working. Is it because when I tried it Table2 was not an article in my merge replication publication, or is it because merge replication doesn't support subqueries like this and recognising when a filter has changed. What it would mean is that the sync process would have to scan each filter, and work out what tables the subquery relates to and figure out whether these have changed. Does it do something like this? UPDATE: I have tried a number of things, but they don't work. 

I am using merge replication in SQL 2012. I am trying out the TableDiff utility to show non convergence. I see a problem with this approach though. This is because I am using parameterised filters to filter the subscriptions. In this instance the TableDiff utility tells me that there are missing rows in the subscription and generates the SQL to insert them. If I swap the source and destination around it generates a whole lot of delete statements to remove the records at the publication. This is not a correct result. It should just check the rows that exist at the subscriber, or understand the filters and know which rows should be at the subscriber. Can I use this utility in this instance? If not is there an alternative? I find it surprising that none of the documentation about this utility mentions this problem. 

So it uses the indexes, but does an index scan across the whole index, so 50000 records it scans 50000 records in the index. 

This works 100%. I have omitted the rest of it because it is just server, user, password settings etc. If I change the query to this, 

What I learned on my own. Basically from each job I was hired for, I learned how each shop operated, listened to business requirements and learned on my own how to complete the assignments. A lot of reading the manual (now Googling) and trial-and-error is involved but it's ultimately how we all learn. What I learned from certifications. One time I picked a textbook for my favorite programming language and discovered that I was only using a fraction of the techniques described on the job. I wanted to learn as many aspects as I could. In the back of the book they had mentioned the certification program for my language (Microsoft Certified Professional in this case). It's a structured way to learn all the major features of the platform you're focusing on for your employment. There's a lot of studying and practice involved (My MCSD took four tests and my MCDBA and MCITP:DBA were comparable). You'll learn about features you might not end up using on a particular job site, but being aware of their existence helps shape you decisions on how to tackle a business requirement. What I learned from the community. This by far, is the most valuable. You learn firsthand that you're not alone in your profession. You also learn what your peers have learned on the job and through their own research, and it's great to share what you've learned. You'll find your mentors here as well. There's a large of community of masters, MVPs and fellow DBAs who share their knowledge in-person, in print and online. Search for them in Google, attend a local user group, have your employee fund a trip to your favorite products' annual convention. I've seen at SQL PASS twice and the knowledge shared there is amazing. 

Why is it suggested to regenerate the snapshots every 14 days by default? With web sync you are an anonymous subscriber, so how can replication know when to clean up metadata for a user who never syncs 

When they sync with the server won't it repeat the default constraint and set the value of 'NumberOfSides' back to 4? Am I missing something here? I don't want to disable the default constraints being replicated to the client either because there are valid default values that need to be set. The client side user needs to have these values set to use the software. UPDATE: The explanation about the default constraint being applied only once makes sense. So my only remaining question is if I have default contraints which use sequences I am in trouble because sequences cannot be replicated. Are these my only two options? 

Turn off all default constraints for a particular table (not ideal if some of them are not for sequences) Create the sequences manually at the client side, and then create a custom handler to apply the correct sequence values at the server when syncing 

I am using merge replication in SQL 2012. Why can't you mark default constraints as NOT FOR REPLICATION? You can disable all default constraints for a merge article, but it is all or nothing so it doesn't seem to offer enough control. How about this scenario, 

I am running merge replication with SQL 2012. When I first create my publication the MSMerge_contents table is populated with a large number of records with the colv1 being set to 0xFF. When I run the stored procedure like this, 

I am in the process of implementing merge replication in SQL 2012 with web sync. I am wondering two things, 

Two suggestions come to mind. One is that the clustered index on both ApplicationEvent and ApplicationFault above be on the LogTime column. Assuming the data is posted to the table in chronological order, you'll have reduced fragementation from page splits and benefit from range scans when purging out older time periods. The second builds on the first to implement parititioning based on the LogTime column. Instead of the relatively expensive I/O during the delete operations, you can implement a sliding window which would 'slide' older time periods out the main table with ALTER TABLE commands, effectively dropping unneeded dates via essentially meta commands. 

Is this expected for a FlashArray config or can it be configured at the controller or LUN level to be more optimal for reads than writes? The host will host a data warehouse with tables already in the 250 GB+ range. 

If there's enough space still left, the best approach may be to 1) create a new filegroup, 2) rebuild the index(es) for the table(s) into the new filegroup with the ONLINE=ON switch, and then 3) run a SHRINKFILE operation to reduce the footprint of the original filegroup. The rebuild operation is multi-threaded while SHRINKFILE last time I checked is a single-threaded process and inherently slower. You also mitigate the fragmentation side-effect by doing the rebuild up front. The catch is having enough space, but if you can rebuild enough indexes into the new FG before the volume capacity is reached, you'll have an overall faster SHRINKFILE operation to reclaim the space from the original FG. 

I'm trying to figure out the peak load I can sustain on new hardware I procured for validating backups and performing DBCC checks. I've been using Crystal Diskmark to get throughput stats which helped me benchmark sequential I/O for the copy/restore tasks. I'm having trouble gauging how much random I/O I can sustain for the DBCC check. I'm thinking about using iometer and sqliosim but want to know config would work best to simulate a DBCC check. The hardware I'm testing consists of one R720 with dual E5-2609s for 8 cores, 32 GB RAM, Windows 2008 R2 Standard, SQL Server 2008 R2 Standard with SP2, and a PowerVault 3620f with 24 15k SAS spindles hooked up to two dual port HBAs on the R720. I've been experimenting with 4, 8 and 12 spindle RAID 0 groups (I can afford to lose the fault tolerance as the DBs have a life expectancy of minutes as part of the testing process). I'm thinking I can run multiple simultaneous DBCC checks with the above hardware without hitting disk contention. I have the option to upgrade the RAM to 64 GB and the O/S to Enterprise but probably can't upgrade the SQL to Enterprise due to licensing costs. Any suggestions on how to determine the max random I/O for DBCC using iometer, sqliosim or another utility would be deeply appreciated.