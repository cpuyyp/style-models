If your algorithm is to be "explorative" (the goal is somewhere, but the minesweeper does not know where), finding a goal should have a positive reinforcement and visiting the same location more than once should have a negative reinforcement. Also: Think about explaining your problem more detailed. This seems to be a common problem of your questions... Really describe what kind of "rules" exists. For example: If you think it can be done well with Neuronal Networks, but you are free to use any machine learning algorithm you like, state this! What are your minesweepers supposed to do except for avoiding bombs? Else, they can just stay where they are = goal reached. Shall your minesweepers learn by examples given? Or shall they find out by trial and error (just given the result of their actions) = reinforced learned? 

Do only one of the changes at a time and measure the change by the benchmark to see how it effected your runtime! Maybe one thing is good while the other idea was bad (even I did propose them above!)... I think these optimizations will be much better than trying to achieve better performance by using multiple threads! You'll have much trouble to coordinate the threads, so they will not overwrite the others values. Also they will conflict when accessing similar memory regions too. If you use 4 CPUs/Threads for this job, you could expect only a speed up of 2 to 3 for the frame rate. 

Each state has a evaluation "value" and links to the next Gamestates when moving (0 = stop, 1 = up, 2 = right, 3 = down, 4 = left) by storing the array index within "tree" in moves[0] to moves[4]. To build your tree recursively this could look like this: 

You should also use speaking names for the properties of the tiles (what do "G", "H" mean? I have to look it up elsewhere or even have to guess what it could mean, which makes the code unreadable). Keep your functions short! When editing a function you should see the whole function on the screen, without scrolling. They should each implement only -one- algorithm. For example A* and nothing else! Everything else, as Manhattan Distance calculations should be put into separate functions/methods. 

Adapting A* As Philipp said, you should add costs when the direction doesn't change for long time. But, the function by Philipp may quickly lead to summing up additional costs, that are higher than the cost for traversing an additional tile. But his key idea is correct! It seems easy to adapt A* to calculate "all" optimal paths (with shortest length) and then selecting one of them by another heuristic. But there is a problem. If you have a long path, there might be a lot of solutions with optimal length. This causes the A* algorithm to take much longer to compute all these other solutions, too. This is because the grid. You can not walk 80 degrees instead of 90 degrees, which leads to multiple suboptimal solutions instead of one optimal solution. For imagination, imagine a map without obstacles. The x-distance is 2 the y-distance is 3. This means, all shortest paths have 2 diagonal moves and 1 straight move. There are 3 valid combinations: S-D-D, D-S-D, D-D-S (where D=diagonal, S=straight) for this simple path. The real "fun" starts already when you have paths with e.g. 3 straight and 2 diagonal moves: S-S-S-D-D, S-S-D-S-D, S-S-D-D-S, S-D-S-S-D, S-D-S-D-S, S-D-D-S-S, D-S-S-S-D, D-S-S-D-S, D-S-D-S-S, D-D-S-S-S (10 variations of the shortest path, if I didn't miss any). I think you should have got the idea... So we should fix this by adapting the cost function in a way that fewer solutions (or even only one solution) are "optimal". Adapting the Cost Function Doing the adaptation as Philipp suggests in his example formula will give you much better results, but has still some problems. It will not evenly distribute the shorter/longer "parts" along the path, meaning: the direction changes will be more often at the beginning of the path or vice-versa. Additionally, a path with endlessly having the actor to "turn" seems to be suboptimal when observed by a human. As it takes time (to show the turn animation) and therefore it must be slower. How ever, instead of using floats for the costs you can implement a "secondary cost" or secondary sort criteria. If the primary costs are the same the secondary cost is used to estimate which solution is to be preferred. This will not accidentally cause the primary costs (route length in grid measure) to increase. 

Prepare some patterns and do tests with all of them with multiple game testers (or just some friends) multiple times per pattern (e.g. 6 times per pattern). For each pattern, record the success rate and the learning rate (e.g. average success rate first and second half of the test). Also let the testers rate the boss movement pattern as "too easy", "easy", "okay", "harder", "hard", "too hard", "nearly impossible". The preparation of the patterns could be done by once controlling the boss yourself like the player character and record the actions. Implement some kind of "arena" where you directly enter the boss fight and one can control/record the boss. This way your boss will probably become more "natural", that means less "programmatic". 

I guess you mixed up "path finding" and "object state" (e.g. position, speed, direction, ..). Path finding is used to find a path from position A to position B. As far as I understand, the "Mesh based" algorithm is only used to simplify this "path finding" process, when an actor (e.g. a car) wants to move from position A to position B to reduce the maps complexity for the search algorithm. All this has nothing to do with the actual position of actors during the game. The actors will (in this case) usually move continuously in one direction until they turn at the next corner. So you can take two observed positions (first / second) of the opponent to calculate the movement vector ( movement = second - first). If you divide it by the time between the observations (duration, in seconds) and multiply it by a time span x (in seconds), you get the relative position of the opponent, if it continues to move that way for x seconds. The estimation is calculated by: estimated = second + (second - first) * ( x / duration), where first, second, estimated are vectors and x, duration are scalars (time spans in seconds). The time spans don't need to be seconds but need to have the same unit (e.g. frames, time steps, ...). Regards Stefan 

The problem is most probably the and then using (% 10 == 0). The window.setTimeout() callbacks are not "accurate". Thus, if you update your objects this causes the noticeable wobble. Also your timeout functions runtime is shorter or longer depending on "counter % 2 == 0" and "counter % 10 === 0" is true. So you should call window.setTimeout() for the next callback directly at the beginning of the method! 

This code isn't verified to work, nor is it compileable or complete. But it should give you an idea how to do it. The most important work is the evaluation function. The more sophisticated it is, the the wrong "tries" the algorithm will try (and have to undo) later. This extremely reduces the complexity. If this is too slow you can also try to apply some methods of two-person-games as HashTables. For that you'll have to calculate an (iterative) hash key for each game state that you evaluate and mark states that do not lead to a solution. E.g. every time before the Search() method returns "null" a HashTable entry must be created and when entering Search() you'd check if this State has been already reached so far with no positive result and if so return "null" without further investigation. For this you'll need a huge hash table and would have to accept "hash collisions" which could cause that you probably don't find a existing solution, but which is very unlikely, if your hash functions is good enough and your table is big enough (its a risk of calculate-able risk). I think there is no other algorithm to solve this problem (as described by you) more efficient, assumed your evaluation function is optimal... 

PREPARATION/BENCHMARKING: To surely know that this routine is the problem, write a small benchmark routine. It shall execute the method of the Gravity multiple times for e.g. 1000 times and measure it's time. If you want to achieve 30 FPS with 100 objects, you should simulate 100 objects and measure the time for 30 executions. It should be less than 1 second. Using such a benchmark is needed to do reasonable optimizations. Else you will probably achieve the opposite and make the code run slower because you just think it must be faster... So I really encourage you to do this! OPTIMIZATIONS: While you cannot do much about the O(NÂ²) effort problem (meaning: calculation time increases quadratically with number of simulated objects N), you can improve the code itself. a) You use a lot of "associative array" (Dictionary) lookups within your code. These are slow! For example . Can't you just use ? This saves one lookup. You do this everywhere in the whole inner loop to access properties of the objects referenced by e and e2... Change this! b) You create a new Vector inside the loop . All "new" calls implicate some memory allocation (and later: deallocation). These are even much slower than lookups of Dictionaries. If you only need this Vector temporarily, so allocate it outside of the loops AND -reuse- it by reinitializing its values to the new values instead of creating a new object. c) You use a lot of trigonometric functions (e.g. and ) within the loop. If your accuracy doesn't need to really really exact, you might try to use an lookup table instead. To do this you scale your value to a define range, round it to an integer value and look it up in a table of pre-calculated results. If you need help with that, just ask. d) You often use . You can pre-calculate this and store the result as or -if this is always an even positive integer value - you can use bit the shift operation to devide by two.