So you want to evaluate the polynomial $$ p(x) = \sum_{C} x^{|C|} $$ where $C$ ranges over all nearly-minimum cuts in a graph (say, all minimal cuts of size $\alpha c$ where $c$ is the edge connectivity.) Here $\alpha$ is a small constant $>1$. You can do any precomputation you want. Two obvious algorithms present themselves. This is a polynomial of degree at most $\alpha c$, so you can do this is $O(c)$. Alternatively, for $\alpha$ sufficiently close to 1, the number of such cuts is at most $O(n^2)$, so you can do this in $O(n^2)$ work. Are there any other algorithms available? For $\alpha$ sufficiently close to 1, the set of all such cuts has a very nice structure, and these cuts can be represented in $O(n)$ space. 

I am trying to find about algorithms that, given graph $H, G$, determine if $H$ is a topological minor of $G$ (and if so, exhibit this explicitly). Most of the literature on this topic seems to be focused on the case when $H$ is a fixed small graph while $G$ goes to infinity --- e.g. testing if $G$ is planar. What are the algorithms when $H$ and $G$ are comparable in size? If this problem requires exponential time, I would still be interested in the best algorithms and/or real implementations 

Suppose I have a code $C$ over $GF(2)$. I would like to count exactly the number of codewords of $C$ of weight $k$. Here $k$ should be thought of as small compared to the dimensions of the code. What is the best algorithm for this? Even exponential algorithms could be useful. 

In a previous question about time hierarchy, I've learned that equalities between two classes can be propagated to more complex classes and inequalities can be propagated to less complex classes, with arguments using padding. Therefore, a question comes to mind. Why do we study a question about different types of computation (or resources) in the smallest (closed) class possible? Most researchers believe that $P \neq NP$. This distinction of classes wouldn't be between classes that use the same type of resource. Therefore, one might think of this inequality as a universal rule: Nondeterminism is a more powerful resource. Therefore, although an inequality, it could be propagated upwards via exploiting the different nature of the two resources.So, one could expect that $EXP \neq NEXP$ too. If one proved this relation or any other similar inequality, it would translate to $P \neq NP$. My argument could maybe become clear in terms of physics. Newton would have a hard time understanding universal gravity by examining rocks (apples?) instead of celestial bodies. The larger object offers more details in its study, giving a more precise model of its behavior and allowing to ignore small-scale phenomena that might be irrelevant. Of course, there is the risk that in larger objects there is a different behavior, in our case that the extra power of non-determinism wouldn't be enough in larger classes. What if after all, $P \neq NP$ is proven? Should we start working on $EXP \neq NEXP$ the next day? Do you consider this approach problematic? Do you know of research that uses larger classes than polynomial to distinguish the two types of computation? 

The exponential time hypothesis asserts that an algorithm for SAT must take time $2^{\Omega(n)}$. If I am reading this right, this refers only to deterministic algorithms. Is it possible that ETH holds, yet their is a random algorithm in time $2^{o(n)}$? If so, is there any name for the hypothesis that any randomized (say BPP-type) algorithm takes time $2^{\Omega(n)}$ as well? 

Another typical case of $NPI$ problem is when there is a witness of length $\omega(\log n)$ but smaller than $n^{O(1)}$. The problem of the existence of a clique of size $\log n$ in a graph is a typical example -- in this case, the witness (the specific clique) requires $O(\log^2 n)$ bits. Assuming the Exponential Time Hypothesis, such a problem is easier than an $NP$-complete problem (which requires time $\exp(n^{O(1)})$) but harder than a polynomial time problem. 

A lot of approximation algorithms are based on relaxation. The way it usually works is this. You take the original problem and relax it some large class of efficiently solvable problem (e.g. relax an IP to an LP). Any solution to the original problem of cost $X$ gives a corresponding solution to the relaxation of cost $X$. You then convert a solution to the relaxed problem of cost $Y$ to a solution to the original problem of cost $\alpha Y$ for some $\alpha > 1$. This implies that you have an $\alpha$ approximation. But what if you could argue that the original feasible solution of cost $X$ guarantees the existence of a solution to the relaxation of cost $X/\beta$? Then you would get an $\alpha/\beta$ approximation algorithm. Has this strategy ever been employed? Is there ever a case when you can show that a feasible solution to the original problem corresponds to a feasible solution to the relaxation of strictly lower cost ? 

I assume that talking with someone about $\mathbb{P/poly}$ and $\mathbb{NP}$ means that person is familiar with the $\mathbb{P}$ vs $\mathbb{NP}$ question and the verifying-solving duality. Then, I would try to explain that $\mathbb{P/poly}$ is so powerful because for each different length, the TM is given advice that it can trust completely. Then I would mention that we can devise hard (non-TM-computable actually) languages that have 1 word per input length (i.e. unary), so they are in P/poly ! But maybe a polynomial long advice isn't enough to solve all languages in $\mathbb{NP}$, since there we are allowed a different hint for every different input. On the other hand, I would remind that person that $\mathbb{NP}$ has to verify the answer, not trust it completely. So, we cannot use the same advice for each input length, it may not be verifiable! Finally, I would mention that complexity theorists believe that there are languages in $\mathbb{NP}$ that require more than polynomial many hints for some input length, and thus cannot be in $\mathbb{P/poly}$. A critical point for giving a good understanding, which I think is also common when teaching the subject for the first time, is making clear that advice and "hint" (i.e. certificate) are different things, and how they differ. 

I don't know about the most widely used, but I believe I know of the oldest usage (for computer science anyway). In the 1965 paper by Hartmanis & Stearns "On the computational complexity of algorithms", Corollary 2.1 is: 

Let S be the search frontier, i.e. the set of nodes to select from next. Select the node so that (node_weight+maximum_edge_weight) is maximized. Remove the node from the graph and S. Add the node's "children" to S. If the graph is not empty, go to step 1. Halt. 

Let $H$ be some complex hash function (almost any function will do), mapping long bit strings down to a single bit. Then to decide whether $H( A \times B ) = 0$, you will basically need to multiply $A \times B$ and compute $H$ on the resulting product. Unless $H$ has very special properties, there won't be any short cut to this. 

Suppose you are given input $w = \langle M, x, t \rangle$ and are asked to decide if RAM machine $M$ terminates on input $x$ after $t$ steps. By the time hierarchy theorem, the optimal algorithm to decide this is to simulate the execution of $M(x)$ for $t$ steps, which can be done in time $O(t)$. (Note: for Turing machines, simulating the execution of $M$ takes $O(t \log t)$ steps; we only know a lower bound of $\Omega(t)$. So, this is not quite optimal for Turing machines specifically). There are some other problems which contain the version of the halting problem as a sub-case. For example, deciding whether a sentence $\theta$ is a consequence of the WS1S takes time $2 \uparrow \uparrow O(|\theta|)$ and this is optimal. 

The set of transcendentals is not open in $\mathbf R$ (in particular, it is dense and codense in $\mathbf R$. Hence it is undecidable. 

To maximize the expression, given $x_i$, should set $y_i = m$ for the value of $i$ maximizing $x_i - \alpha$. This implies you should set $x_2, \dots, x_n = \alpha$ and $x_1 = 1 - (n-1) \alpha$. (I assume here that $0^0 = 1$; otherwise you must set $x_i = \alpha + \epsilon$, and the resulting function does not realize its supremum) 

The idea is to traverse those subgraphs that will give as much gain as possible first, in order to be able to bear the cost of the negative weight subgraphs later. 

For a single query, there is of course query optimization , which allows us to plan in which order should we apply the operators that compute the query. There is also a multi-query optimization, although to my knowledge it's not as successful as single query optimization, which is seen as a vital component of any DBMS. A quick search in google gave me this paper , I believe there is extensive research in this problem and you won't have trouble finding related material to your question. 

Is there a quick reference for the definition of a sparse s-t flow? In the general case, having the max-flow it is quite easy to determine the min-cut, via the max-flow , min-cut theorem. The edges that are fully saturated form a cut set, so by selecting one vertex for each such edge, one can form a min-cut. Trivially, this is O(m) in the worst case, and also if one makes the running time output-sensitive, then the number of edges in the flow or even better, the number of saturated edges in the flow, always is an upper bound on the running time of the algorithm for finding the min-cut from the max-flow. So if you have a modification that finds those sparse s-t flows in linear time in the size of the flow, finding the min-cut won't change the algorithm's runtime asymptotically. 

The definitions of efficient reducibility are motivated in part by an analogy with recursion theory. In recursion theory, the m-reductions are closely connected to the arithmetical hierarchy. (m-reductions preserve arithmetical degree). Arithmetical classifications are important beyond mere computability. For example, one can say that true $\Sigma_1$ statements are provable in Robinson's $Q$. In complexity theory, there is also a notion of "polynomial hierarchy", though unlike the arithmetical hierarchy it is only conjectured to exist. This leads to classifications that are more subtle than "Is this problem as hard to solve as NP?" 

A very common technique in the analysis of algorithms involving random permutations is to have each element $x$ select a rank $\rho(x)$ uniformly at random from the real interval $[0,1]$. The permutation $\pi$ is then formed by sorting on ranks. Is it possible to invert this transformation? That is, given a permutation $\pi$ generated randomly in $S_n$, choose ranks $\rho(x)$ (with some additional randomness) such that 

Suppose you have a circuit which takes as input an advice string and a random string. (So this circuit would be in $BPP/Poly$ or something like that.) You can convert this into a purely deterministic circuit, which takes a somewhat larger advice string, as follows. There are $2^n$ possible inputs. By hypothesis about the circuit, each random string is good for any input with probability say $3/4$. (By good, I mean that the random string leads the circuit to output the correct value.) Suppose you select randomly select a set $S$ consisting of $c n$ random strings (chosen uniformly at random with replacement), where $c$ is a large constant. Then, for any input, the probability that the number of selected strings good for that input is below $c n/2$ is $e^{-c' n}$, by the Chernoff bound. By taking $c$ sufficiently large, one can ensure that the probility is below $2^{-n}$. By the union bound, the probability that the set $S$ is good for all the $2^n$ input is $> 0$. This means, that there exists some such set $S$. So, fix some such set $S$ and hard-wire it into the circuit. Instead of taking a random string, the circuit evaluates at all the inputs in $S$ and outputs the majority vote. Now the circuit is derandomized, and is correct always. Thus, $BPP/Poly = RP/poly = P/Poly$. So there is no need to consider randomness plus advice strings. 

Although your reasoning is flawed, I believe it is educational to see why. First of all, I have a rule of thumb when I am trying to prove something that seems extraordinary or difficult: "Assume that your approach is wrong and try to find out why". Arguing against yourself is not an easy task, but it is very important. One technique I use very often is trying to apply the same argument to different situations. In other words, I'm trying to find a counterexample. Assume that your reasoning is perfect. Then, apart from computability, you are disproving a good part of mathematic knowledge, which is based on contradiction proofs. For example, the statement that there is no natural number such that no other natural number is greater than it, uses contradiction. This result is much more intuitive than the undecidability of halting. Perhaps the notion of contradiction could be better understood if you think of every person trying to prove a statement as a little God: Assume that you have a theory, i.e. a number of axioms and theorems. Assume also that you have a universe that instead of physical laws, obeys only this theory. Now , if you wanted to prove that statement S is false,you could work in the following way (an analogy to the physical world and a statement such as the conservation of energy is highly educational): 

EDIT: I considered both points raised in the comments but elected not to include them both of brevity and time I had available to write down the answer. This is my reasoning as to why I believe these points do not diminish the effectiveness of Turing machines in simulating modern computers, especially when compared to finite automata: 

I have been having a great deal of difficulty finding a reference that gives simple and straightforward explanation of the following: Suppose we have $n$ random variables $Y_1, \dots, Y_n$, each of $b$-bits long. (I.e. with values in $\{0, \dots, 2^b-1 \}$). We want a probability space where each $Y_i$ is unbiased (takes on each value with probability exactly $2^{-b}$), and has $k$-independence. That is, for any $i_1 < \dots < i_k$ and any $y_1, \dots, y_k$ we have $$ P(Y_{i_1} = y_1 \wedge \dots \wedge Y_{i_k} = y_k) = 2^{-k b} $$ When $b = 1$ you can always get a probability space of size $n^{k}$ and sometimes you can get $n^{k/2}$ -- is there any clear statement about when these are possible? Can someone point me to references about what happens when $b > 1$? Thanks 

See the paper "A parallel approximation algorithm for positive linear programming." by Luby and Nisan. (Some kinds of) linear programs can be approximated in log^(O(1)) n time. 

The following fact seems to be used implicitly in cs theory, particularly algorithms. Given a RAM machine $M$ running in time $O(f(n))$, another RAM machine $M'$ can simulate $M$ in time $O(f(n))$. This differs from the case for Turing machines, where $M'$ may require $O(f(n) log(f(n))$ time. I say this is often used implicitly because many papers will simply say something like "run $M$, but keep track of certain auxiliarily information as you do so". This is really simulating $M$, but for RAM machines the distinction is not so important because running times are not (asymptotically) affected. Is there a reference for this theorem? I am summarizing the situation correctly?