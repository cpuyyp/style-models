Here's a different approach, based upon iteratively finding numbers that cannot appear among $\{a_1,\dots,a_6\}$. Call a set $A$ an over-approximation of the $a$'s if we know that $\{a_1,\dots,a_6\} \subseteq A$. Similarly, $B$ is an overapproximation of the $b$'s if we know that $\{b_1,\dots,b_6\} \subseteq B$. Obviously, the smaller $A$ is, the more useful this over-approximation is, and the same goes for $B$. My approach is based upon iteratively refining these over-approximations, i.e., iteratively reducing the size of these sets (as we rule out more and more values as impossible). The core of this approach is a method for refinement: given an over-approximation $A$ for the $a$'s and an over-approximation $B$ for the $b$'s, find a new over-approximation $A^*$ for the $a$'s such that $A^* \subsetneq A$. In particular, normally $A^*$ will be smaller than $A$, so this lets us refine the over-approximation for the $a$'s. By symmetry, essentially the same trick will let us refine our over-approximation for the $b$'s: given an over-approximation $A$ for the $a$'s and an over-approximation $B$ for the $b$'s, it will produce a new over-approximation $B^*$ for the $b$'s. So, let me tell you how do refinement, then I'll put everything together to get a full algorithm for this problem. In what follows, let $D$ denote the multi-set of differences, i.e., $D=\{a_i-b_j:1 \le i,j \le 6\}$; we'll focus on finding a refined over-approximation $A^*$, given $A,B$. How to compute a refinement. Consider a single difference $d \in D$. Consider the set $d+B=\{d+y : y \in B\}$. Based on our knowledge that $B$ is an over-approximation of the $b$'s, we know that at least one element of $d+B$ must be an element of $\{a_1,\dots,a_6\}$. Therefore, we can treat each of the elements in $d+B$ as a "suggestion" for a number to possibly include in $A$. So, let's sweep over all differences $d \in D$ and, for each, identify which numbers are "suggested" by $d$. Now I'm going to observe that the number $a_1$ is sure to be suggested at least 6 times during this process. Why? Because the difference $a_1-b_1$ is in $D$, and when we process it, $a_1$ will be one of the numbers it suggests (since we're guaranteed that $b_1 \in B$, $(a_1-b_1)+B$ will surely include $a_1$). Similarly, the difference $a_1-b_2$ appears somewhere in $D$, and it'll cause $a_1$ to be suggested again. In this way, we see that the correct value of $a_1$ will be suggested at least 6 times. The same holds for $a_2$, and $a_3$, and so on. So, let $A^*$ be the set of numbers $a^*$ that have been suggested at least 6 times. This is sure to be an over-approximation of the $a$'s, by the above comments. As an optimization, we can filter out all suggestions that are not present in $A$ immediately: in other words, we can treat the difference $d$ as suggesting all of the values $(d+B)\cap A$. This ensures that we will have $A^* \subseteq A$. We are hoping that $A^*$ is strictly smaller than $A$; no guarantees, but if all goes well, maybe it will be. Putting this together, the algorithm to refine $A,B$ to yield $A^*$ is as follows: 

I don't know how to solve your problem, but here is a technique to solve a simpler version of your problem: namely, given an edge $e$, test whether there exists any simple path from $s$ to $t$ that includes edge $e$. (This corresponds to the special case of your problem where $l=\infty$.) You can solve this simpler problem using "max-flow with lower bounds" as a subroutine. In the standard max-flow problem, the capacity of each edge gives us an upper bound on the amount of flow going through that edge, and we require that the amount of flow on edge be lower-bounded by 0. In "max-flow with lower bounds", we are allowed to specify both a lower bound and an upper bound on the amount of flow through that edge. It is known that "max-flow with lower bounds" can be solved in polynomial time. Now, suppose we have an edge $e \in E$, and we want to test whether there exists a simple path from $s$ to $t$ that includes edge $e$. We're going to set up a max-flow with lower bounds problem. In particular, take graph $G$ and add a new node $s_0$ with edge $s_0 \to s$ and a new node $t_1$ with edge $t \to t_1$. Make the capacity (upper-bound) on each edge 1. The lower-bound on all edges will be 0, except that the lower-bound on edge $e$ is 1. Now check whether there exists a feasible flow from $s$ to $t$ that satisfies all the bounds (this test can be done in polynomial time, as mentioned above). If there is no flow, then there is no simple path from $s$ to $t$. If there is such a flow, then tracing out that flow yields a simple path from $s$ to $t$ that includes edge $e$, so there does exist such a simple path. How do we solve a "max-flow with lower bounds" problem? In this case, only one edge has a non-zero lower bound. Therefore, we can use a standard approach to network flow, where at each point we choose an augmenting path by computing shortest paths in the residual graph -- except that here we ask (roughly) that one of the augmenting paths includes the edge $e$. I learned this idea from the following paper: 

Let $s \in \{0,1\}^n$ be a secret bitvector. Define $f(x)$ to be the Hamming distance between $x$ and $s$. Suppose I am given an oracle for $f$, and I want to find $x$. How many queries to the oracle are needed to determine $x$? I want an algorithm that is efficient (running time polynomial in $n$, say). I am fine with average-case complexity (over a uniform distribution on $s$). Information theoretically, $\Theta(n/\lg n)$ randomly chosen queries $x_i$ should suffice, but I can't think of any efficient algorithm to recover $s$ from the values of $f(x_i)$. Obviously $s$ can be efficiently recovered given $n$ queries or so. How much better can we do? 

Sure. These are known as homomorphic hash functions. There are many schemes: see e.g., $URL$ for one possible entry point into the literature. One example construction is to let $\mathbb{G}$ be a group with group operation $*$ and let $h:\{0,1\} \to \mathbb{G}$ be an arbitrary function, then extend $h$ to a function $h:\{0,1\}^* \to \mathbb{G}$ by associativity, i.e., $$h(x_1 x_2 \dots x_n) = h(x_1) * \cdots * h(x_n).$$ Then you can choose any group $\mathbb{G}$ of your choice. If you want $*$ to be non-commutative, choose a non-abelian group $\mathbb{G}$. One plausible choice is $\mathbb{G} = SL_2(\mathbb{F})$ over some finite field $\mathbb{F}$; see $URL$ 

Also, here are two trivial observations. First, there is a PRP that can be computed in $NC^1$ if and only if there is a PRF that can be computed in $NC^1$. The "only if" part is immediate, as any PRP with large domain is also a PRF. The "if" part follows from the Luby-Rackoff construction (i.e., the Feistel cipher), as that shows how to build a PRP out of any PRF; it increases the depth by only a constant factor. Second, the following paper shows that no PRF can be computed by an AC0 circuit. Nathan Linial, Yishay Mansour, Noam Nisan. Constant depth circuits, Fourier transform, and learnability. Journal of the ACM, 40(3):607--620, 1993. It follows that no PRP can be computed by an $AC^0$ circuit. 

Optional context: If we had an arithmetic circuit (whose gates are multiplication, addition, and negation) over $\mathbb{R}$, then it would be possible to compute the $n$ directional derivatives ${\partial f \over \partial x_i}(x)$ in $O(m)$ time. Basically, we could use standard methods for computation of the gradient (back-propagation / chain rule), in $O(m)$ time. That works because the corresponding function is continuous and differentiable. I'm wondering whether something similar can be done for boolean circuits. Boolean circuits aren't continuous and differentiable, so you can't do the same trick, but maybe there is some other clever technique one can use? Maybe some kind of Fourier trick, or something? (Variant question: if we have boolean gates with unbounded fan-in and bounded fan-out, can you do do asymptotically better than evaluating $C$ $n$ times?) 

Here is a brief overview and introduction to steganography: Christian Cachin. Digital steganography. In Henk C.A. van Tilborg, editor, Encyclopedia of Cryptography and Security. Springer, 2005 Since you asked for an introduction specifically for information-theoretic approaches, you might try this paper as a starting point for more detail: Christian Cachin. An information-theoretic model for steganography. Information and Computation, 192(1):41-56, July 2004. If you want a coding-theoretic perspective, you might also enjoy the following papers: Yin Wang, Pierre Moulin. Perfectly Secure Steganography: Capacity, Error Exponents, and Code Constructions. IEEE Transactions on Information Theory, 2008. Pierre Moulin, Joseph A. O'Sullivan. Information-Theoretic Analysis of Information Hiding. IEEE Transactions on Information Theory, 2003. I do encourage you to also take a look at computational steganography (e.g., the Hopper et al. paper you cite in your question), as many of the powerful results are in the computational security regime 

Lower bound At least $\Omega(1/\sqrt{\epsilon})$ queries are necessary to distinguish the two cases. Consider the sequence $a_1,\dots,a_n$ given by $\epsilon,2\epsilon,3\epsilon,4\epsilon,\dots$, with $n$ chosen so that $a_1+\dots+a_n = 1$. In particular, we can take $n \approx 1/\sqrt{2\epsilon}$. Now construct a new sequence $a'_1,\dots,a'_n$ by modifying a single element of the above sequence by subtracting $\epsilon$. In other words, $a'_1=a_1$, $a'_2=a_2$, etc., except that $a'_i = a_i - \epsilon$. Notice that $a'_1 + \dots + a'_n = 1-\epsilon$. How many probes does it take to distinguish $a_1,\dots,a_n$ from $a'_1,\dots,a'_n$? Well, they differ in only a single element (the $i$th element), so it takes $\Omega(n)$ probes to achieve a constant probability of distinguishing. Now recalling that $n \approx 1/\sqrt{2\epsilon}$; we find that $\Omega(1/\sqrt{\epsilon})$ probes are needed. Upper bound I think you can distinguish the two cases using $O(\lg(n/\epsilon) [\lg n + 1 / \epsilon^2])$ queries. I don't know if this is optimal. Here's how. Let's partition the range $[0,1]$ as follows: $$[0,1] = [0,0.25\epsilon/n] \cup (0.25\epsilon/n,0.5\epsilon/n] \cup (0.5\epsilon/n,\epsilon/n] \cup (\epsilon/n,2\epsilon/n] \cup (2\epsilon/n,4\epsilon/n] \cup \dots \cup (\ldots,1].$$ This is a partition, so each $a_i$ value must fall into exactly one of the ranges above. We'll partition the $a_i$ values according to which range they are in. Each $a_i$ value falls into exactly one of these ranges, and the ones that fall into a particular range all appear consecutively in your sorted order. Therefore, for any given range $[\ell,u]$, we can find the indices $i,j$ such that $a_i,\dots,a_j \in [\ell,u]$ using binary search. This requires $O(\lg(n/\epsilon))$ binary searches. Assume we've done that. Now, we'll estimate the sum of values in each range. The first range will be handled separately from all the rest: