Probably the most famous instance in which a greedy algorithm is known to solve an LP is for the special case of the transportation problem. Hoffman ("On simple linear programs", in Convexity, vol. 7 of Proceedings of Symposia in Pure Mathematics, pages 317-327, 1963) proved that if the cost matrix for a (maximization) transportation problem satisfies the Monge property ($c_{ij} + c_{kl} \geq c_{il} + c_{kj}$ when $1 \leq i < k \leq n$, $1 \leq j < l \leq n$) then an optimal solution can be found in a greedy manner like the one you describe. Hoffman also has a survey paper ("On greedy algorithms that succeed") from 1985 in which he discusses known cases in which a greedy algorithm gives an optimal solution to an LP. Besides his own work cited above (about which he says, "most of the linear programming problems known [by 1963] to be susceptible to a greedy algorithm were special cases of the Monge idea"), he mentions Edmonds' linear programming interpretation of a generalization of matroids and a discussion of the case when $A$ is nonnegative, among other things. I imagine there are more recent results, but hopefully this at least partially answers your question and gives you some ideas of where else to look. 

Maybe it's worthwhile to talk through where the dual comes from on an example problem. This will take a while, but hopefully the dual won't seem so mysterious when we're done. Suppose with have a primal problem as follows. $$ Primal =\begin{Bmatrix} \max \ \ \ \ 5x_1 - 6x_2 \\ \ \ \ s.t. \ \ \ \ 2x_1 -x_2 = 1\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ x_1 +3x_2 \leq9\\ \ \ \ \ x_1 \geq 0\\ \end{Bmatrix} $$ 

Now, suppose we want to use the primal's constraints as a way to find an upper bound on the optimal value of the primal. If we multiply the first constraint by $9$, the second constraint by $1$, and add them together, we get $9(2x_1 - x_2) + 1(x_1 +3 x_2)$ for the left-hand side and $9(1) + 1(9)$ for the right-hand side. Since the first constraint is an equality and the second is an inequality, this implies $$19x_1 - 6x_2 \leq 18.$$ But since $x_1 \geq 0$, it's also true that $5x_1 \leq 19x_1$, and so $$5x_1 - 6x_2 \leq 19x_1 - 6x_2 \leq 18.$$ Therefore, $18$ is an upper-bound on the optimal value of the primal problem. Surely we can do better than that, though. Instead of just guessing $9$ and $1$ as the multipliers, let's let them be variables. Thus we're looking for multipliers $y_1$ and $y_2$ to force $$5x_1 - 6x_2 \leq y_1(2x_1-x_2) + y_2(x_1 + 3x_2) \leq y_1(1) + y_2(9).$$ Now, in order for this pair of inequalities to hold, what has to be true about $y_1$ and $y_2$? Let's take the two inequalities one at a time. 

Suppose that P = NP is true. Would there then be any practical application to building a quantum computer such as solving certain problems faster, or would any such improvement be irrelevant based on the fact that P = NP is true? How would you characterize the improvement in efficiency that would come about if a quantum computer could be built in a world where P = NP, as opposed to a world in which P != NP? Here's a made-up example of about what I'm looking for: 

I think I can summarize your objection as follows: You are saying, "once we've contradicted the initial assumption that HALT exists, it is impossible to continue with a proof, because we've encountered an absurdity." I'm not sure that it's quite right to say that the halting problem is axiomatically guaranteed to be decidable, but the point is that we've assumed that it is decidable, for no reason other than that we want to show that it isn't. I'm not sure what you think a proof by contradiction (or negation--I admit I use the two interchangeably, even if I shouldn't) is if not this. This may be qualify as decidedly "non-research level" mathematics, but in the interest of giving you a metaphor that may help I'll say this anyway. Suppose I claim that I can build a spaceship that will fly to Alpha Centauri and back within ten seconds. You could say, "Well, let's assume that that's the case. Then you would have built a spaceship that is capable of traveling faster than the speed of light, which is impossible. Thus, no such spaceship can exist." This is (vaguely) similar logic to what is being used in the proof; we claim something can exist, then demonstrate an absurdity, and conclude that this indicates that it cannot exist. If there's anything unclear about this, let me know and I'll try to re-explain. Also please see the link I gave in the comments for alternate proofs of the Halting Problem. 

It's probably worth summarizing the implications of this argument for all possible forms of the primal and dual. The following table is taken from p. 214 of Introduction to Operations Research, 8th edition, by Hillier and Lieberman. They refer to this as the SOB method, where SOB stands for Sensible, Odd, or Bizarre, depending on how likely one would find that particular constraint or variable restriction in a maximization or minimization problem. 

Putting all of these restrictions on $y_1$ and $y_2$ together we find that the problem of using the primal's constraints to find the best upper-bound on the optimal primal objective entails solving the following linear program: $$\begin{align*} \text{Minimize }\:\:\:\:\: y_1 + 9y_2& \\ \text{subject to }\:\:\:\:\: 2y_1 + y_2& \geq 5 \\ -y_1 + 3y_2& = -6\\ y_2 & \geq 0. \end{align*}$$ And that's the dual. 

Here's another perspective that might be helpful. Suppose there are $n$ variables and $m$ constraints. If you write the simplex method in matrix form, the optimality condition for a minimization problem is $${\bf c}^T_N - {\bf c}^T_B B^{-1} N \geq {\bf 0},$$ where ${\bf c}_B$ and ${\bf c}_N$ are the cost vectors for the basic and nonbasic variables, and $B$ and $N$ are matrices consisting of the entries in $A$ that correspond to the basic and nonbasic variables. Since you know ${\bf x}^*$, you can construct a basis for the optimal solution. You need $m$ variables to be in the basis. First, every nonzero $x_i$ goes in the basis. If there aren't $m$ nonzero $x_i$'s (there can't be more than $m$), then you will need to choose enough of the zero-valued $x_i$'s to be basic so that you have $m$ basic variables; it doesn't matter which ones. (If this happens it means the optimal solution is degenerate.) Once you have a basis you can determine $B$ and $N$. Then the set of cost vectors ${\bf c}$ for which ${\bf x}^*$ is optimal is the solution set to the above vector inequality, which is just a set of $n-m$ linear inequalities in which you have $m$ free variables (the values for ${\bf c}_B$). 

Assuming P != NP, the answer is surely yes; based on your guarantee, you've restricted the problem to polynomial time machines, and thus the language you describe should be trivially empty. The challenge is proving that a machine that rejects every input truly decides the language--not just finding this machine. Of course, if P = NP, you have a Rice's theorem problem (and the answer is no). Since the language is now non-trivial, you're describing a non-trivial property of some Turing machines--specifically, the ones bounded by n^c. The following is drawn from David Richerby's comment above. Given a Turing machine M1, let M2 be a machine that simulates M1 and rejects if it hasn't halted after n^c steps. Deciding whether a machine M1 solves SAT with the promise (that the machine halts after n^c steps) is equivalent to deciding whether M2 solves SAT without the promise. Deciding M2 without the promise is undecidable, by Rice's theorem. 

I'm going to try to answer you question by proposing an alternate model for the question. I typically ask more questions than I answer on here, so I hope you'll be forgiving if my answer isn't optimal, although I'm doing my best. I think that the way to phrase the question that would be optimal for allowing game theory to be useful would be to assume a more competitive scenario. I.e., there needs to be competition among a variety of different actors. So, I would assume the following: 

Now, assuming no cooperation on any problem is possible, consider what I'm going to refer to as a "dynamic iterated game." This is a game that is played repeatedly, but that changes slightly each time it is played. Let M be the number of moves, or turns, in the game. The initial manifestation of the game could be represented as a list that contains every actor (researcher) and every problem that they could work on, in addition to all of the values associated with each actor and each problem that I listed above. (I'm assuming, of course, that every researcher knows everything presently known about all of the problems, and about all other researchers, making this a game of perfect information.) During each iteration of the game, a given actor chooses a research question to work on. Each actor is permitted to switch questions at any time, and if a problem is solved, the benefit to career U gets dropped to 0 for all other players. If a player invests sufficient time and fails to solve the problem, then that particular player is prohibited from trying to solve that problem again...although any other player is allowed to continue working on the problem, and another actor may be able to solve it successfully. The game ends after all M turns have been taken. Each turn that a researcher has selected a problem will cause that player to get closer to reaching the "moment of truth," and possibly solving the problem, Nature permitting. A problem, once solved, adds a certain benefit to the researcher's career based on l. Research talent amplifies the probability of success, while free time amplifies the ability to make progress in a given turn. I doubt that there is any polynomial time algorithm for solving this; I see no reason why researchers should be restricted to playing pure-strategy Nash equilibria, so the problem would involve mixed-strategy Nash equilibria and thus be at worst PPAD-complete, if you consider "solving the problem" to mean "finding a Nash equilibrium for the problem." (One could imagine that if you are the most proactive researcher around, you might go ahead and calculate your favorite Nash equilibrium and then signal it to all other players...thus giving you some confidence that no one will change strategies away from the strategy profile you've signaled.) At any rate, this is the most involved answer I've ever posted. I hope it is of at least some value. Please let me know if anyone has any response or to it or recommendations for improving it. 

The second inequality: $y_1(2x_1-x_2) + y_2(x_1 + 3x_2) \leq y_1(1) + y_2(9)$ Here we have to track the $y_1$ and $y_2$ variables separately. The $y_1$ variables come from the first constraint, which is an equality constraint. It doesn't matter if $y_1$ is positive or negative, the equality constraint still holds. Thus $y_1$ is unrestricted in sign. However, the $y_2$ variable comes from the second constraint, which is a less-than-or-equal to constraint. If we were to multiply the second constraint by a negative number that would flip its direction and change it to a greater-than-or-equal constraint. To keep with our goal of upper-bounding the primal objective, we can't let that happen. So the $y_2$ variable can't be negative. Thus we must have $y_2 \geq 0$. Finally, we want to make the right-hand side of the second inequality as small as possible, as we want the tightest upper-bound possible on the primal objective. So we want to minimize $y_1 + 9y_2$. 

The first inequality: $5x_1 - 6x_2 \leq y_1(2x_1-x_2) + y_2(x_1 + 3x_2)$ We have to track the coefficients of the $x_1$ and $x_2$ variables separately. First, we need the total $x_1$ coefficient on the right-hand side to be at least $5$. Getting exactly $5$ would be great, but since $x_1 \geq 0$, anything larger than $5$ would also satisfy the inequality for $x_1$. Mathematically speaking, this means that we need $2y_1 + y_2 \geq 5$. On the other hand, to ensure the inequality for the $x_2$ variable we need the total $x_2$ coefficient on the right-hand side to be exactly $-6$. Since $x_2$ could be positive, we can't go lower than $-6$, and since $x_2$ could be negative, we can't go higher than $-6$ (as the negative value for $x_2$ would flip the direction of the inequality). So for the first inequality to work for the $x_2$ variable, we've got to have $-y_1 + 3y_2 = -6$. 

I've been reading about convex volume estimation, and have found the paper "Simulated Annealing in Convex Bodies and an $O^{*}(n^4)$ Volume Algorithm" by Lovasz and Vempala, which can be read here. The algorithm provided relies the standard multi-phase monte-carlo technique of producing a series of values, the first of which is easy to compute, the remainder estimatable by random variables, and then combining each of these variables in some way to produce an estimate of the volume. This particular technique involves extruding an $n$-dimensional shape into an $n+1$-dimensional pencil, and estimating the volume of that pencil. The pencil is defined as the intersection of a cylinder over the convex body and a cone. Let $K$ be the convex body, and $$ C = \{{\bf x} \in \mathbb{R}^{n+1} | x_0 > 0, \; \sum^n_{i=1}x_i^2 \leqslant x_0^2\} $$ The pencil, $K'$, is defined: $$ K' = ([0,2D] \times K) \cap C $$ By assumption, $K$ contains the unit ball, so the set of points in $K'$ with $x_0<1$ is exactly the $n+1$-dimensional hypercone with height $1$ and whose base is the $n$-ball of radius $1$. This cone is referred to as $C_B$. We also denote by $\pi_n$ the volume of the unit n-ball. The estimates of the volume of the pencil are provided using the following function: $$ Z(a) = \int_{K'} e^{-ax_0} d{\bf x} $$ For a sufficiently small value of $a$, $Z(a)$ can be shown to be a good estimate for the volume of $K'$. For $a \geqslant 2n$, it is claimed that $Z(a)$ is close to the above integral taken over the entire cone $C$. We know that $K' \subseteq C$ from its definition, hence $$ Z(a) = \int_{K'} e^{-ax_0} d{\bf x} \leqslant \int_{C} e^{-ax_0} d{\bf x} = \int_0^\infty e^{-at}t^n \pi_n dt = n! \pi_n a^{-(n+1)} $$ We know also that $C_B \subseteq K'$, hence $$ Z(a) \geqslant \int_{C_B} e^{-ax_0} d{\bf x} = \int_0^1 e^{-at}t^n \pi_n dt $$ At this point, I can no longer follow the paper's reasoning. It is stated that $$ \int_0^1 e^{-at}t^n \pi_n dt > (1-\varepsilon) \int_0^\infty e^{-at}t^n \pi_n dt $$ No bounds are specified on $\varepsilon$. Indeed, our estimation algorithm should be able to take an arbitrarily small value of $\varepsilon$. The integrand, hovever, is non-negative across its domain of integration, so it must be the case that $$ \int_0^1 e^{-at}t^n \pi_n dt \leqslant \int_0^\infty e^{-at}t^n \pi_n dt $$ Surely, then, the inequality in $\varepsilon$ would only hold for a sufficiently large value of $\varepsilon$, rather than for an arbitrarily small one. Further, stating that $a\geqslant 2n$ gives no information about a relationship between $\varepsilon$ and $a$. How is it even possible to reach the conclusion from the premise in this case? Further still, an independent talk on the algorithm found here suggests instead that the inequality only holds for $a>6n$, rather than $a>2n$. The paper states that this is true by standard computation, so I do feel that I must be missing something here, but I cannot tell what. I've emailed both of the authors at the email addresses I can find for them, and haven't received any response from them, so hopefully someone from here can help. 

This was too long to post as a comment (in response to your comment), so I'm posting it as an answer. Turing's proof of the Halting Problem is not mathematically fallacious. Here is the paragraph in which you explain why the proof bothers you: 

This is a "historical question" more than it is a research question, but was the classical reduction to order-finding in Shor's algorithm for factorization initially discovered by Peter Shor, or was it previously known? Is there a paper that describes the reduction that pre-dates Shor, or is it simply a so-called "folk result?" Or was it simply another breakthrough in the same paper? 

It follows from Rice's theorem that you cannot determine whether or not two Turing machines decide the same language. My question is: Does this also apply in descriptive complexity settings, particularly when it comes to testing a pair of SO-Horn queries to see if they describe the same language? I'm not aware of any descriptive complexity version of Rice's theorem, and I could conceivably see that it might not be all that difficult to test two second-order formulas for equivalence. 

(Motivation: I am curious about this, and relatively new to quantum computing; please migrate this question if it is insufficiently advanced.) 

While reading a post on Scott Aaronson's blog about Eigenmorality, I ran across the idea of the iterated prisoner's dilemma tournament. I've studied some TCS on my own, but had never really thought carefully about algorithms for an IPD. I subsequently read that the generally-acknowledged "best strategy" for winning IPD games is the tit-for-tat strategy, but that there are some possible improvements. My question is, given an IPD tournament, is there a sort of "evolutionarily stable" bot that always wins against "mutant bots"--that is, an algorithm that will always defeat every other algorithm at IPD? If so, what is it, and can its dominance over all other bots be proved? If not, can this be proved? (Is there a better question I should ask, such as, "If you assume that you are pitted against 5 million other randomly selected bots, which bot should you choose?") My question may be too vague to be answered precisely, so if you could point me in the direction of relevant references, that would be just as helpful as a full answer.