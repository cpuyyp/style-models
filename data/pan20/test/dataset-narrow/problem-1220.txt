Both of these topics have been done many times over, but it would be interesting to see how well/poorly Mathematica deals with the underlying structures. 

The COSTA tool developed by the COSTA research group does exactly what you want. Given a program (in Java bytecode format), the tool produces a cost equation based on a cost model provided by the programmer. The cost model can involve entities such as runtime, memory usage, or billable events (such as sending text messages). The runtime equation is then solved using a dedicated tool to produce a closed form, worst-case upper bound of the cost model. Naturally, the tool does not work in all cases, either by failing to produce a runtime equation or by failing to find a closed form. This is not surprising. 

When encoding a logic into a proof assistant such as Coq or Isabelle, a choice needs to be made between using a shallow and a deep embedding. In a shallow embedding logical formulas are written directly in the logic of the theorem prover, whereas in a deep embedding logical formulas are represented as a datatype. 

Moved from comment at Kaveh's suggestion First you need to select a proof assistant. Coq is what I use, but there are many others. Coq is based on higher-order logic (the so-called Calculus of Inductive Constructions). Other proof assistants are based on first order logic, so may be more suited to your needs (modulo the comments above). Then you need to commit to learning the proof assistant. The linked document is a tutorial for getting of the ground with Coq. Becoming a Coq expert requires years of dedication and practice, but simple theorems can be proven in an afternoon. The key to learning Coq or any other proof assistant is to do proofs, such as the ones in the linked paper. Just reading the paper will help very little, because the whole experience of interacting with the proof assistant cannot be conveyed well on paper. Within a few days you ought to be able to encode simple theorems, such as the one above, and prove them. Don't expect that we will do this for you. You'll learn nothing that way. When you do succeed in proving these theorems, feel free to post your answers here and maybe leave a few comments about your experiences. Are you up for the challenge? 

In Definition 2.1 of Contextual Petri Nets, Asymmetric Event Structures, and Processes, Baldan, Corradini and Montanari are working in the setting of prime event structures, which consist of a set of events $E$ along with binary relations $\#$ and $\le$ on $E$, known as the conflict and causality relations, respectively. This is the close to what is going on in your your setting. In this context, they state that the relation $\#$ is hereditary with respects to $\le$ whenever for all $e_0,e_1,e_2\in E$, if $e_0\#e_1$ and $e_1\le e_2$, then $e_0\#e_2$. It does not make sense to say that a relation is hereditary on its own; it is always taken with respect to another relation. In general, saying that a relation is the least hereditary relation is not equivalent to saying that it is the least hereditary, symmetry, irreflexive relation, as the definition of hereditary does not imply that the relation is symmetric. In the specific case you mention, it does seem to be the case – after going into the details of the paper a little more – that one need not specify that the relation is symmetric or irreflexive, as these come for free from the context. Well spotted!! 

I've been toying with a combination of R, to do the analysis and some visualisation, and gephi, for visualisation. R seems very powerful, with the right abstractions for doing statistical manipulations. It has three packages (at least) for doing social network analysis, , , and . Gephi has a nice feature for drawing dynamic social networks, but I haven't fully explored its capabilities yet. 

You could easily formulate this as a integer linear programming (ILP) problem. This is a linear programming problem where the solutions are constrained to be integers. The problem is NP-hard as indicated in other answers, but software does exist to address this sort of problem. Let $L$ be the ultimate length you are aiming for, $a$ be the number of Track A used, $b$ be the number of Track B used, and so forth. Finally, let $A$, $B$, $C$, and $D$ be the amount of each kind of track you have. ($a,b,c,d$ are variables; the rest are constants.) Then the integer linear programming problem is formulated as: 

Baars, Löh, and Swierstra implemented Permutation Parsers for Haskell (Journal of Functional Programming / Volume 14 / Issue 06, pp 635 - 646). These can be used to specify the permutation of a collection of parsers. If each of these parsers is an optional parser for a single character (that is, matches the character or nothing), then you'd have the ingredients you are looking for. I believe that their library is available with GHC. 

Asking the following query at the command prompt in a prolog interpreter will give you an answer for X: 

Of course there are many systems for modelling processes. These fall under the category of process algebras. The key examples are $\pi$-calculus, CCS, ACP and CSP. Process calculi have basic mechanisms for specifying process behaviour including: sending and receiving of messages (synchronously or asynchronously), creating parallel processes, nondeterministic choice between behaviours, and the replication of processes. Although the calculi are small in terms of the number of constructs, they are very expressive and a vast amount of research has gone into studying their properties. The $\pi$-calculus differs from the others in that it allows, in essence, processes to be passed as first class values. It actually allows channel names to be passed around as first class values, enabling changes in the dynamic topology. This is probably the calculus you want because it offers the greatest dynamicity. CSP (communicating sequential processes) is a little odd, when seen from a perspective of modelling molecules. It does have plenty of backing theory and tool support. (Invented by C. A. R. Hoare.) CCS and ACP have less dynamicity than the $\pi$-calculus, but they are much easier to analyse and simulate. A solid toolset called $\mu$CRL (and $\mu$CRL2) are available for ACP. Similar tools are sure to exist for CCS. I'd start examining the related work (see below) and then find which of the modelling formalisms suit what you are looking for. There has in fact been quite a lot of work modelling chemical reactions and biological processes using process algebra. Probably the best place to look is Luca Cardelli's publication list. His whole line of research on BioComputing has probably 30 papers on the topic. This talk gives an overview of much of his work. This one is slightly more formal, though reading the papers is really the only way to see the details. One approach that directly models chemical processes is CHAM (the chemical abstract machine). The key ingredient here is a solution of molecules and membranes. There are heating and cooling rules for rearranging the molecules and for removing junk. These rules are reversible. Finally there reaction rules which model reactions. In contrast to process algebras, CHAM models are not so worried about the syntax of processes, so you can invent your own representation of the molecules. Rewrite logic as realised in the toolset Maude offers another more or less direct approach to specifying such reactions. One need only specify the rewrite rules, the handing of the "soup" is automatic. The toolset would enable the simulation and analysis of (smallish) chemical reactions. A probabilistic variant of Maude also exists. 

The coalgebraic tecnology has advanced in the last 10 years, though I don't know whether the connection has been further explored. Edit Jan Komenda (and here) seems to have been following up the connection. Other possible approaches could involve using process algebra, I/O automata, interface automata, and hybrid variants of these things. The interface automata have a very strong game theoretic feeling which corresponds closely to somethings done in control theory, namely, the distinction between controllable and uncontrollable actions can be though of as actions being played by two different players. I'm not sure whether anything has been done in that area. The connection seems quite obvious. A final connection that could be worth exploring is between control theory and epistemic logic. The connection can be seen via the games analogy. What does each party know? How can they use that to achieve a suitable outcome in the system being controlled? 

Another possible connection to explore is the use of coinduction and coalgebraic techniques for reasoning about control theoretic systems. Jan Rutten did some work in this directions some years ago, namely: 

I'm looking for a formalism for expressing such predicates in terms of game-theoretic semantics. A link to existing work would be greatly appreciated. The main thing I'm having trouble getting my head around is the winning condition (as discussed in some of the comments below). Even though the game goes forever, infinite plays do not constitute a draw. 

The key difference is that you have a field tracking the size of the list. This field can be accessed in constant time. The field must be updated for every addition or removal. If you compute the size of the list by counting the elements, then it will be linear.