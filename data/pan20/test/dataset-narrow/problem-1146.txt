The fields of philosophy and CS apparently have different definitions/interpretations of the thesis. In CS, I believe it is standard/accepted to define the Church-Turing thesis as the article's "Misunderstandings" section's "Thesis M" (under the narrow/worldly view). However, the article claims that this is an incorrect definition of Church-Turing. So we simply disagree. (And let's try to avoid starting an argument with them about it ... pointless arguments are their forte, after all.) The approach taken by philosophers is unfortunate, as the average layman is probably interested in the CS Church-Turing thesis, not the philosophy one espoused in the article. So they will cite the article while thinking it refers to our practical/reasonable definition, when it doesn't. So my answers to your specific questions: 

I've tried to comb the literature and seen a lot of references to results that almost but don't quite seem to address this. 

(For example: If we show that $A$ requires time $2^{\sqrt{n}}$, should we say that $A$ requires exponential time?) If we are thinking in the style of EXPTIME, I think we would say "yes": interpret the statement as saying that $A$ requires $2^{n^c}$ time for some $c > 0$. But thinking in the style of E, we would say no. For instance, the Exponential Time Hypothesis states informally that 3SAT requires exponential time, and formally it says that 3SAT requires $2^{cn}$ time for some $c$ (note the crucial difference!). Because of this, I am not sure if there is a commonly accepted interpretation of the above statement. Ideally we would state more precisely what we mean when using the term, unless it's clear from context. Perhaps others can mention if they think one interpretation or the other is more "standard". ... In summary: if $f(n) \leq 2^{\sqrt{n}}$, then $f$ is definitely at most exponential. But if $f(n) \geq 2^{\sqrt{n}}$, it is not clear to me whether we would say $f$ is at least exponential. 

It's important to understand that computer scientists use the term "nondeterministic" differently from how it's typically used in other sciences. A nondeterministic TM is actually deterministic in the physics sense--that is to say, an NTM always produces the same answer on a given input: it either always accepts, or always rejects. A probabilistic TM will accept or reject an input with a certain probability, so on one run it might accept and on another it might reject. In more detail: At each step in the computation performed by an NTM, instead of having a single transition rule, there are multiple rules that can be invoked. To determine if the NTM accepts or rejects, you look at all possible branches of the computation. (So if there are, say, exactly 2 transitions to choose from at each step, and each computation branch has a total of N steps, then there will be $2^N$ total brances to consider.) For a standard NTM, an input is accepted if any of the computation branches accepts. This last part of the definition can be modified to get other, related types of Turing machines. If you are interested in problems that have a unique solution, you can have the TM accept if exactly one branch accepts. If you are interested in majority behavior, you can define the TM to accept if more than half of the branches accept. And if you randomly (according to some probability distribution) choose one of the possible branches, and accept or reject based on what that branch does, then you've got a probabilistic TM. 

Proof. As stated above, $H(X|Y) - H(X|Y,Z) = H(Z|Y) \geq 0$. To construct examples arbitrarily close to $0$, fix $p$. The intuition is $H(p)$ is concave, so we will have sometimes $\Pr[Z|Y] = \epsilon$ and sometimes $\Pr[Z|Y] = 1-\epsilon$, so that $H(Z|Y) = H(\epsilon) \to 0$, yet still $H(Z) = H(p)$. Let $Y = -1$ with probability $1-p$ and $Y = 0$ with probability $p$. If $Y=-1$, then with probability $\epsilon$ we have $X=Y$ and otherwise $X$ is uniform on $\{1,\dots,m\}$. If $Y=0$, then with probability $1-\epsilon$ we have $X=Y$ and otherwise $X$ is uniform on $\{1,\dots,m\}$. Now we can check that $H(Z|Y) = H(\epsilon)$ and $p = \Pr[Z] = (1-p)\cdot \epsilon + p\cdot(1-\epsilon) = p$. Taking $\epsilon \to 0$ gives the example. $\square$ Step 2. 

Yes, as far as I can tell. I'd tell them that the article refers to a highly specialized philosophy-definition of Church-Turing. But regardless of what one calls the "true" Church-Turing thesis, the following thesis is almost universally believed among computer scientists: "Any usable machine of computation that can be built in this universe can be simulated by a Turing Machine". If by hypercomputation you mean physically possible/realizable, then no, it's not taken seriously. But it is interesting to study hypercomputation models even if they cannot appear in the real world, and we do so all the time. For instance, we may consider a Turing Machine that has access to an oracle that solves the halting problem. This object is studied all the time in theory and is uncontroversial, but nobody believes that one can actually be built. 

I would like to second the advice given by Justin. There are a lot of great suggestions in the replies, but many of them are just about effective work habits and don't specifically have anything to do with research. I was thinking about this recently after viewing a talk by Steven Johnson on the subject of his latest book, "Where Good Ideas Come From". (One version of the talk can be found at TED.) His basic premise is that there are certain environments which act as natural incubators for new ideas. He gives the example of the coffee house in Renaissance England, which aside from the effects of the coffee itself, brought people of varied backgrounds together and encouraged the exchange of ideas. It is this cross-pollination of ideas which gives rise to new, novel ideas. Now quite possibly this hasn't been mentioned in most of the other responses because it is just assumed that everyone is already going to be doing this, but I thought it would be worth emphasizing the point. 

(Oh good, this question hasn't been closed yet!) On a practical level, I agree with the comments by Warren and Suresh that this question isn't answerable given the current state of the art in TCS, and it sounds more like an AI question. But I have to wonder, does it necessarily have to be that way? The question of how semantics arises from syntax has to be one of the most foundational questions there is, and I wouldn't want to rule out TCS having a role in answering it. I tend to think of AI of being more like engineering and TCS like theoretical physics--we ought to be supplying the equivalent of Newton's Laws that provides some overarching principles that the AI engineers can use to go and build their intelligent machines. Now, so that this answer isn't totally without actual content, here is a paper that maybe touches a little bit on this question: Universal Semantic Communication I by Juba and Sudan. They make an attempt at modeling "meaningful communication" in terms of computational power. I'm not sure how successful their approach is, but I do think it's important in that they are using techniques from computational complexity to analyze these issues. 

From what I understand (which is very little, so please correct me where I err!), theory of programming languages is often concerned with "intuitionistic" proofs. In my own interpretation, the approach requires us to take seriously the consequences of computation on logic and provability. A proof cannot exist unless there exists an algorithm constructing the consequences from the hypotheses. We might reject as an axiom the principle of the excluded middle, for instance, because it exhibits some object, which is either $X$ or $\lnot X$, nonconstructively. The above philosophy might lead us to prefer intuitionistically valid proofs over ones that are not. However, I have not seen any concern about actually using intuitionistic logic in papers in other areas of theoretical CS. We seem happy to prove our results using classical logic. For example, one might imagine using the principle of the excluded middle to prove that an algorithm is correct. In other words, we care about and take seriously a computationally-limited universe in our results, but not necessarily in our proofs of these results. 1. Are researchers in theoretical CS ever concerned about writing intuitionistically valid proofs? I could easily imagine a subfield of theoretical computer science that seeks to understand when TCS results, especially algorithmic ones, hold in intuitionistic logic (or more interestingly, when they don't). But I have not yet come across any. 2. Is there any philosophical argument that they should? It seems like one could claim that computer science results ought to be proven intuitionistically when possible, and we ought to know which results require e.g. PEM. Has anyone tried to make such an argument? Or perhaps there is a consensus that this question is just not very important? 3. As a side question, I am curious to know examples of cases where this actually matters: Are there important TCS results known to hold in classical logic but not in intuitionistic logic? Or suspected not to hold in intuitionistic logic. Apologies for the softness of the question! It may require rewording or reinterpretation after hearing from the experts. 

A prerequisite for that would be that human intelligence actually has some supra-computational properties, no? Frankly, the more I view human behavior, the more we appear to me to be automatons. But perhaps the recent political climate has left me overly cynical. :) Of course, human intelligence is better than the current algorithmic state of the art for many tasks. For a practical application of that, you could look at, say, Mechanical Turk. On the theoretical side, there are people (including some who happen to be computer scientists) who have advanced philosophical arguments for human supra-computational abilities. You could look at, for example, the work of Selmer Bringsjord. You could also do a search on the term hypercomputation, but I suspect you will find that the parts which fall under the domain of TCS have nothing to do with people, and the parts that attempt to deal with human intelligence have little to do with TCS (or science in general). 

You're probably already aware of this, but on his blog, Scott Aaronson has links to number of his course lectures on quantum computing, as well as links to QC primers by others (just scroll down the right side-bar to find these). If you'd like a book-length introduction, but something that's gentler than a text like Nielsen and Chuang, I would recommend Quantum Computing for Computer Scientists by Yanofsky and Mannucci. They spend a fair amount of time reviewing the mathematical prerequisites before diving into the QC itself. If you have a strong math background this book might seem too basic, but I found it quite useful. 

(Comment --> answer) The inequality unfortunately fails to hold, a counterexample is $$p = (1,0,\dots,0)$$ and $$q = \left(\frac{1}{2},\frac{1}{2n},\dots,\frac{1}{2n}\right),$$ where the support size is $n+1$ and $x=2$. Then $\sum p_i^r = 1$, but $x^{r-1}\sum q_i^r \approx \frac{1}{2}$: \begin{align} \sum q_i^r &= \frac{1}{2^r} + n\left(\frac{1}{2n}\right)^r \\ &= \frac{1}{2^r}\left(1 + \frac{1}{n^{r-1}}\right) \\ &\to \frac{1}{2^r} \end{align} as $n \to \infty$, so $x^{r-1}\sum q_i^r \to \frac{2^{r-1}}{2^r} = \frac{1}{2}$. 

I think this is an example showing no kind of approximation is possible except with exponential$(k)$ value queries. Let $f(S) = 0$ if $|S| \leq k$, otherwise $f(S) = |S| - k$. Now pick a special set $S^*$ uniformly at random from all sets of size $k$, and let $f(S^*) = 0.5$. I'm claiming that this function is supermodular because every element initially has $0$ marginal value, then possibly $0.5$, then $1$ thereafter. So marginal value of any element only increases in a superset. $S^*$ maximizes the function subject to a cardinality constraint of $k$, with an objective value of $0.5$. Meanwhile any algorithm making subexponentially many value queries will be unable (except with tiny probability) to find a feasible set with nonzero value. 

Here's a heuristic argument to say that, if each vertex has an expected number of edges of at least $\Omega(\log d)$, then you can get such a bound, and furthermore it depends mainly on this expected number of edges per vertex (not much on $n$ or $d$). (Edit: As Aaron points out in comments, though, a bound for $H_d$ does not look like it will help us bound $Z_d$!) Let's write $H_d$ as $H_d = \sum_{i_2\neq 1}X_{1i_2} \left(1 + \sum_{i_3 \neq 1} X_{i_2 i_3} \left(1 + \cdots \sum_{i_{d-2}\neq 1}X_{i_{d-2}i_{d-1}} \left(1 + \sum_{i_d \neq 1} X_{i_{d-1}i_d}\right)\cdots \right) \right)$. Now use Chernoff from the inside and move outward. For each index $i_k$, let $\mu_k = \sum_{j \neq 0} p_{i_k j}$. $\Pr\left[\sum_{i_d \neq 0} X_{i_{d-1}i_d} > (1 + \delta)\mu_d\right] \leq \approx e^{-\delta^2 \mu_d}$. So replace this innermost sum with the constant $(1+\delta)\mu_d$ and add $e^{-\delta^2 \mu_d}$ onto the probability we need. Now expand outwards, adding the probabilities (union-bounding). For example, at the next step we say $\Pr\left[\sum_{i_{d-1}\neq 0} X_{i_{d-2}i_{d-1}} \left(1 + (1+\delta)\mu_d\right) \geq (1+\delta)(\mu_{d-1} + (1+\delta)\mu_d)\right] \leq \approx e^{-\delta^2 \mu_{d-1}}$. At the end we get $\Pr\left[H_d > (1+\delta)\mu_1 + (1+\delta)^2 \mu_1 \mu_2 + ~\cdots~ + (1+\delta)^d \mu_1 \cdots \mu_d \right] \leq \approx de^{-\delta^2 \mu_{min}}$ where $\mu_{min}$ is the smallest $\mu_k$. If the number of edges per vertex were independent across vertices (not true, but maybe "close" if $\mu = \omega(1)$), then we could say $\Pr\left[H_d > (1+\delta)^d E[H_d] \right] \leq \approx de^{-\delta^2 \mu_{min}}$. So based on this it seems like a sufficient (and hopefully not too loose) condition should be that for every vertex $k$, the expected number of neighbors is $\mu_k = \Omega(\log d)$. 

Whether or not $P^{A,B}$ equals $NP^{A,B}$ is going on the particular oracle languages A and B that you are using. Iirc, in the BGS paper the language A is TQBF (or any other PSPACE-complete language). The language B, somewhat ironically, is actually defined via a diagonalization construction. Now, if you use the construction from the paper and apply it to TMs with an A oracle, then the resulting language B will be such that $P^{A,B} \ne NP^{A,B}$. On the other hand, if you stick with the original language B from the paper, it will depend on the details of B's construction. If B is in PSPACE (I don't know offhand if it is or not), then a TM with an A oracle could simulate any queries to B, and you should have $P^{A,B} = NP^{A,B}$. The bottom line, though, is that this has nothing to do with consistency--it is just a technical question about the specific oracle languages A and B (and you could get different answers depending on the particular choices of A and B that you use). 

The short answer to your question is, yes. Once you fix a particular encoding scheme for your TM and your inputs and outputs, then there is a smallest TM that will compute your function. There may be multiple "smallest" TMs (of some size, say, k symbols), and if you want your choice to be unique, you could just choose the lexicographically first such TM. Determining what this size k happens to be is undecidable in general, though. This is essentially what Kolmogorov Complexity is all about, except that there the desired algorithm is something specific--what is the smallest TM that will output string x, starting with a blank input tape. I can't comment on the discussion at LTU, except that it seems to be more focused on technical limitations of the lambda-calculus than on ambiguities about TM encodings. I think I'm going to take exception to Andrej's reply to your question. Placing an answer to the halting question, or otherwise supplying the answer to the problem being input on the input itself, is a form of a promise problem. The TM has no way of verifying the input--and if the input is "lying" then the TM is free to output whatever it wants. So this doesn't really address the question at hand. Likewise with the issue of Wolfram's minimalist machines. When deciding whether those are Turing-equivalent or not, the question of how much one is allowed to massage the input becomes key. But I wouldn't consider those machines to be TMs in the first place. All of this falls under the stipulation, "once you fix a particular encoding scheme...", and from the way you phrased your question it seems like you are also making this stipulation. 

For a rough answer, if you are new to TCS you can probably think of it as a sub-area of mathematics: Theoretical computer science consists mainly of proving theorems. If you want a contrast with standard "mathematics", TCS is (I think) primarily algorithmic: Focusing on the design, analysis, and theoretical capabilities/limitations of algorithms. Computer science can also be experimental, i.e. a natural science like physics or biology, but this tends to more often fall outside the realm of "theory". 

A Turing Machine, informally, consists of a set of states along with an infinite tape with a read head, and a set of rules: When you read symbol $x$ and are in state $q$, write symbol $y$, go to state $q'$, and move either left or right. I do not know of any Turing-complete system of computation that is simpler to explain to, say, a layperson, than that. The formal definition of a Turing Machine seems to me at least as brief as the formality required to define, say, the $\lambda$-calculus, the $\mu$-recursive functions, or the Game of Life. 

More formally, suppose I have a game on $n$ players and a sequence of strategy profiles $(s_1^{(1)},\dots,s_n^{(1)}), (s_1^{(2)},\dots,s_n^{(2)}), (s_1^{(3)},\dots,s_n^{(3)}), \dots$. Each $(s_1^{(i)},\dots,s_n^{(i)})$ is a $\epsilon_i$-Nash equilibrium, and the sequence $\epsilon_1,\epsilon_2,\epsilon_3,\dots$ converges to zero. My questions: 

Google Scholar has BibTeX citations for (almost all?) research papers. When you search for a paper (e.g. $URL$ each result has a link called "Cite" which has an option for "Import into BibTeX". (Edit: as per comments, these are of course usually not complete -- you may often need to make edits or additions.) If you are signed into a Google account, then on the scholar homepage, you can go to "Settings" --> "Show links to import citations into BibTeX" to make this a one-click operation from the search results page. Edit: Forgot to mention, the ACM Digital library also has this feature. Look for "Export formats" on the right side of the page. Other sites do too (see the comments) but these are the ones I use most frequently because they are quickest and most likely to have a reference. 

I was thinking about this same question the other day, when I was replaying some of Feynman's Lectures on Physics, and came to lesson 4 on the conservation of energy. In the lecture Feynman uses the example of a simple machine which (through some system of levers or pulleys or whatever) lowers a weight of one unit by some distance x, and uses that to lift a second weight of 3 units. How high can the weight be lifted? Feynman makes the observation that if the machine is reversible, then we don't need to know anything about the mechanism of the machine--we can treat it like a black box--and it will always lift the weight the maximum distance possible (x/3 in this case). Does this have an analogue in computation? The idea of reversible computation brings to mind the work of Landauer and Bennett, but I'm not sure this is the sense of the term in which we are interested. Intuitively, if we have an algorithm for some problem that is optimal, then there isn't any wasted "work" being done churning bits; while a brute-force approach to the same problem would be throwing away CPU cycles left and right. However, I imagine one could construct a physically reversible circuit for either algorithm. I think the first step in approaching a conservation law for computational complexity is to figure out exactly what should be conserved. Space and time are each important metrics, but it's clear from the existence of space/time trade-offs that neither one by itself is going to be adequate as a measure of how much "work" is being done by an algorithm. There are other metrics such as TM head reversals or tape cell crossings that have been used. None of these really seems to be close to our intuition of the amount of "work" required to carry out a computation. The flip side of the problem is figuring out just what that work gets converted into. Once you have the output from a program, what exactly is it that you have gained?