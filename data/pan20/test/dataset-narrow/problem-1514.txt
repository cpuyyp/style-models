I split the sketch vector into b bands of r bits Each band of r bits is a number. I combine that number with the band number and add the movie to a hash bucket under that number. Each movie can be added to more than one bucket. I then look in each bucket. Any movies that are in the same bucket are candidate pairs. 

So now I have a workable solution, and all I need to do is work out whether using 3 steps like this will actually help me get a better result with fewer overall hash bits or better overall performance... 

Use the sparse features first and develop the best model. Calculate the predicted probability from that model. Feed that probability estimate into the second model (as an input feature), which would incorporate the dense features. In other words, use all dense features and the probability estimate for building the second model. The final classification will then be based on the second model. 

You can use Binary Logistic Regression for this analysis. Prior to using Binary Logit, you'd have to spend some time preparing the data for this analysis. You can create several RFM types of features from this data set. Examples: Number of donations, time between donations, time since the most recent donation, time since the first donation, average donation amount, the amount of first donation, the most recent donation amount, etc. (I can provide more examples, if needed.) Since your task is to predict the probability of donation during a four-month timeframe (Mar-Jun 2016), you can create those features (leading indicators) for each donor as of the end of October 2015. All leading indicators would be created based off of timeframes prior to that cut-off point. Your observation window is from Nov-2015 to Feb-2016. This is where your event flag (dependent variable) should come from: 1 if a donor donated (again) during the observation window, and 0 otherwise. In order to make this model generalizable, I'd recommend pulling several such cross-sections of your data (in addition to the October 2015 slice explained above,) 

1st point is the most difficult of the two. Hopefully you have taken enough math and physics by now to wire your mind to think programmatically. If so then yes, you absolutely can learn a language! There are guides out there that teach out the syntax and functions. For example: 

I thought about it this way: the training and test sets are both a sample of the unknown population. We assume that the training set is representative of the population we're studying. That is, whatever transformations we make to the training set are what we would make to the overall population. In addition, whatever subset of the training data we use, we assume that this subset represents the training set, which represents the population. So in response to your first question, it's fine to use that shrinked/resmpled training as long as you feel it's still representative of that population. That's assuming your untouched training set captures the "real characteristics" in the first place :) As for your second question, don't merge the training and testing set. The testing set is there to act as future unknown observations. If you build these into the model then you won't know if the model wrong or not, because you used up the data you were going to test it with. 

Perform clustering on the combined data set, but use only those features that are non-missing across all sources. Perform three different cluster analysis, one for each source. This way, you can ensure that you are using as many features (information) as possible. The determination of "abnormal" behavior can then be determined within each source. This can be an added benefit since it would allow you to be more specific about why a use might be abnormal, as you have more features that can be used to explain this. The results can also be then summarized across all sources to create one consolidated report. 

I'm trying to build a cosine locality sensitive hash so I can find candidate similar pairs of items without having to compare every possible pair. I have it basically working, but most of the pairs in my data seem to have cosine similarity in the -0.2 to +0.2 range so I'm trying to dice it quite finely and pick things with cosine similarity 0.1 and above. I've been reading Mining Massive Datasets chapter 3. This talks about increasing the accuracy of candidate pair selection by Amplifying a Locality-Sensitive Family. I think I just about understand the mathematical explanation, but I'm struggling to see how I implement this practically. What I have so far is as follows 

I would start by looking at the distribution of bounce rates for each individual. That distribution would provide some details around what could be considered a "normal" bounce rate, and where you would draw a line for an extreme bounce rate that implies a 'bad'. Additionally, you can also look at average bounce rates (divide bounce rate by the tenure), and review these averages by tenure. This may reveal that different threshold values might be appropriate for customers with different tenure. For instance, if a customer is new (first month of activity) then even a moderate bounce rate could imply a 'bad'. Additional metrics -- like the ratio of bounce to valid payments, time between bounce -- could be useful to look at as well. Once a set of initial thresholds are identified, I would closely review some of the customers who are identified as 'bad' based on those thresholds to ensure that there are not too many false positives. The business point of view (from those who would be using this information to make decisions) would be great to incorporate into this review. 

the logical mindset to phrase the solution to your problem in procedural code to know the programming language, functions, and libraries needed in this field. 

Data Scientists code every day. However, just because you don't have background doesn't mean you can't pick it up! The level of programming you need to know to start doing Data Science isn't very high, but you will at least need: 

the say you're storing it is fine in general you can further transform/store your data depending on your use case 

First thing that came to mind would be one's personal workout data from running or biking apps. Otherwise there is a dataset around NYC's taxi trip data. Quick Googling brought me this: $URL$ Variables include time and location for both pickups and dropoffs. Another dataset comes from Chicago's bikesharing service. It can be found here: $URL$ 

Comparing this to 3.6.3 of mmds, my AND step is when I look at bands of r bits - a pair of movies pass the AND step if the r bits have the same value. My OR step happens in the buckets: movies are candidate pairs if they are both in any of the buckets. The book suggests I can "amplify" my results by adding more AND and OR steps, but I'm at a loss for how to do this practically as the explanation of the construction process for further layers is in terms of checking pairwise equality rather than coming up with bucket numbers. Can anyone help me understand how to do this? 

Since some features are missing for specific sources, the missing values are not missing-at-random but are systematically missing. In this situation, I'd advise against doing clustering on the combined data set with all available features. If missing values were occurring at random, you could have used some missing value imputation method before performing cluster analysis. However, since the values are systematically missing, imputation would be difficult to tackle. (You could try to predict those missing values, but I am afraid that will add a lot of unnecessary noise in the data.) I'd recommend choosing from one of these two options: 

I have say 1000 movies each with ratings from some selection of 1M users. Each movie is represented by a sparse vector of user scores (row number = user ID, value = user's score) I build N random vectors. The vector length matches the length of the movie vectors (i.e. the number of users). The vector values are +1 or -1. I actually encode these vectors as binary to save space, with +1 mapped to 1 and -1 mapped to 0 I build sketch vectors for each movie by taking the dot product of the movie and each of the N random vectors (or rather, if I create a matrix R by laying the N random vectors horizontally and layering them on top of each other then the sketch for movie m is R*m), then taking the sign of each element in the resulting vector, so I end with a sketch vector for each movie of +1s and -1s, which again I encode as binary. Each vector is length N bits. Next I look for similar sketches by doing the following 

Allowing me to, for example, look at the relationship between the Distance_speed1_Control vs Energy_speed1_Control columns. Basically subset/aggregate your data and then use the dcast to get the rows and columns the computer needs. 

To expand on #2, if I want to study Distance vs Energy across all subjects, then I would format my data like this: 

Personally I would recommend Python first. To me the language places more emphasis on readability and cleanliness, making it a great first language. It's also a general purpose language so it's good to know. I did start with R though and it's also good, but is more function-over-form IMO. Try both out and see which feels best first, since you'll likely have to pick up both if you delve into this field anyways.