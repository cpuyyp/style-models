Does partitioning affect performance? Absolutely! Will things get slower as the drive fills up and gets used? Yes! BUT will this performance be noticeable in real world, perceivable performance? Probably only a small amount to not noticeable at all on a drive like you mentioned given today's circumstances. Partitioning a drive down for the OS and "short stroking" it absolutely affects synthetic performance. The first and biggest speed hindrance is the seek time of a drive. Mostly this matters when accessing and reading small files. When the drive is accessing many small files, it will spend more time looking for the files by physically moving the head to the correct location and then waiting for the data to arrive at the head, than actually transferring data. While today's modern operating systems are suppose to manage files to minimize the seek time, making a dedicated OS partition of minimal size (fully loaded with 25%-33% free space) guarantees your files you use most actively, the OS files and program files, will be located physically close together on the drive minimizing seek time. The other performance peccadillo of platter drives is the data is transferred at different rates depending on the location the data is read from the drive. A hard drive spins at a constant RPM. This means the linear velocity, or speed at which the data moves across the head, is higher or lower depending on the location on the platter. Simply put, the data at the edge moves faster across the read head and therefore transfers faster than data in the middle or at the end. Graphing transfer rates across a drive makes this obvious. Further the first partition is at the edge and the last partition is in the middle (typically) so the first partition data will be read faster than the subsequent partitions. The worst case scenario of past Microsoft OS's was the data was not organized. So as files got updated, they went wherever they could. This lead to small little OS and program files that began close together, but quickly got spread across the drive as updates were done and programs were removed and installed at later dates. This could mean out of 100's or 1000's of files accessed during the loading of a program and the OS, the head would be seeking much more that it would be reading. Short stroking solved this and used to be a large real world performance increase. For real world performance these days, while short stroking does increase performance because of the physical implications of data location and drive mechanics, it does not have quite the real world effect that is used to. This is because OS's manage the locations of the files much better and they utilize RAM more to cache files so the files are not actually read from the drive as often as they used to be. There are definitely more reasons than this. But be it as it may, if we are using a platter drive, we still partition it for the guaranteed performance increase, small as it maybe. It only takes a short time to set up, and then you just leave it. Part of our process is moving all personal and temporary files off the OS partition as well to ensure the OS drive does not outgrow the partition and minimize the the number of active files on the OS partition. Chrome's temporary files is (was?) the hardest part of that because we had to create a junction, but it really was still pretty simple. We also defragment (rarely) by access times with the hope that it will line up files in the order they were used. The logic does not hold perfectly, but it does help. There are other benefits to partitioning the drive unrelated to performance that we like too. 

If you have more than half your drive available, Add a partition to your existing drive then move the non-OS files to the new partition. If you have less than half your drive available, then create a new partition using 90% +/- of the free space, and move as many files as you can to the new partition freeing up space on the original partition. Then enlarged the partition and move more files. Do this until your have moved as many files as you would like. After that, it becomes a simple partition copy, that is copy the OS partition (now smaller) to the new drive in its entirety. 

Why would the evaluate formula give the wrong result with the correct values? In the evaluate formula window of Excel I get the final following steps. 

VLANS are inherently segregated and i would recommend keeping them as segregated as possible, that is minimize/eliminate any "inter VLAN" routing. In the end, this really should leave you mostly solving how to share a single internet connection across all the VLANs. Physically, all your wires can all come into a single patch panel for ease of management. Then, you can run VLANs off a managed switch if you want to simplify your equipment. But you don't have to run VLANs, you can just run separate LANs as well with separate switches per LAN. VLANs would help alot with combing your equipment failure modes so you don't have to manage all the other switches separately but it is not required. From there, it just depends on how you want to manage things. Ultimate, complete segregation is the best. Or you might combine the unsafe internet stuff, (3,5 &9), the safe internet stuff (7,8) and non internet stuff (2,4 &6) in a way that makes sense. Either way you can bring the networks to a very configurable router or linux box to manage the cross network routing. Or, or, or... There are a plethora of ways to configure this all because you have options at each layer of the OSI model. Go through the OSI layering and nail down your desired options at each layer and implement them. What YOU implement will depend on YOUR use case, YOUR needs, YOUR budget, etc. This really is more of a long winded comment, but didn't have enough room in a comment and as such I would expect this question to get closed for being too broad. 

I don't think your partition vs drive question relates to your array being a MD RAID 10. The partition vs drive question is preferential. And when it comes to growing or shrinking arrays, if MDADM can do something for RAID 1, it can probably do it for raid 10. On that note, it is easier if you do not use the whole drive for your array just in case another drive you get in the future ends up being smaller, and whether or not you address such an issue now or later is a matter of preference. I personally use a partition that is slightly smaller for my MD RAID 10 array because it is safer than messing with sizes after the array is in place. And in contrast to what Hennes has said, you DO get a performance boost when you use a software RAID 10 on two drives. My performance literally doubled the moment I rebuilt my array into 10 from 1 using the same drives in the same computer. Seems ridiculous RAID 1 doesn't naturally read from both discs, but it doesn't! I am thinking about adding a third drive for double redundancy since I am using older drives and I think I run a high risk of failure during a rebuild due to using older hardware in general. And the speed boost doesn't hurt. As a note, I even have two different size drives (1TB and 2TB),from 2 different manufacturers. Not the best scenario, but it works great! Writes are slower than a single drive, but reads are fast! Only 2 drives and rebuilds are quick! (no parity) Enjoy your two drive RAID 10 and just enjoy all those people like Hennes who are SURE you need 4 drives. :) 

Software RAID is much simpler to move around from computer to computer during hardware upgrades. It also avoids controller failure possibilities and and controller compatibility issues on upgrade. You said it, you plugged in the drives, re-synced and were up and going. Simple. Hardware RAID does not work like that across different controllers (or motherboards in your case). Software RAID and hardware RAID will have very similar performance on arrays without parity (RAID 1, 0, 10, etc). The hardware solutions start to shine on arrays with parity (5, 6, etc). But many techs are moving way from straight RAID 5 arrays because of inherent problems so even if you had the ability/apparent need for RAID 5, RAID 1 or 10 might still be the way to go. Just make sure you look at a RAID1 array for what it is, a single copy of files with data on two drives that needs backed up. Just because it is two drives, does not mean it is two files. If the file is corrupted, overwritten, accidentally deleted, etc, RAID will propagate such things to both sets of data. Make sure you have a backup first. So to have a RAID, you need 3 drives (2 for array, 1 for backup). If you only have two drives, then simply back up with no RAID. For simpler setups, software RAID is the way to go. Great price, performance, simplicity, functionality, etc. Hardware RAID does have a place, but is more complex to setup, not as flexible, and cost more for your requirements. 

The biggest performance hit would be from the difference in speed of your controllers. If your onboard controller benchmarked 50% faster speed than a card, then you would a see a 50% performance hit when data is accessed on the slower controller. But if your controller benchmarked at the exact same speed, I bet you would not see any real performance hit over the drives being on the same controller. The best thing you might do is benchmark your controllers first separately using the same drive to get a feel for what the performance of your controllers is. Then setup some arrays on the controllers using different combos then test and measure the performance of the arrays and compare to the benchamarks of your controllers independently. Then, make sense of the data and choose you're best configuration. There are so many variables between drive difference, controller differences, MB chipsets handling the data in varying ways, and even how the software handles things that what performance you really see will be heavily dependent on the variables and parameters of your setup. So in short, you shouldn't see any performance hit due to splitting controllers, but the best performance can not be figured until you setup varying configurations, benchmark them, and compare them. Can you run a 2 drive RAID 10 using btrfs like Linux MD Raid? 

Either your headset volume control is defective, or the design of the headset volume control is defective. The volume control options in the OS is software controlled (no wires, knobs, ect). The volume control on our headset should be hardware controlled, that is physical items that adjust the volume (knobs, resistors, potentiometers, etc) which is completely independent of your computer. Normally, turning the volume down on a hardware type device attenuates the signal to 0db, which is 0volts and therein 0 current. If you can still hear sound, then you volume control does not attenuate the signal like it should. I can't imagine a volume control designed to only attenuate from loud to soft, most go to off. Any thing other than this is a defect. Try plugging your headset into the output of another device like a stereo receiver. And see if it exhibits the same behavior; it should. Talk to your supplier; ask if the other headsets of the same make and model do this as well. If they do, buy a different one; if they do not, get a replacement. 

What is the average life of a SATA hardrive? Almost all data I can find gives failure rates for the first 0-5 years, but none seem to actually find the end of the life of the drives. The reports, charts, and studies by google, backblaze, and the likes only tell part of the story as they focus on the first 5 years +/-. Hypothetically to say 50% of drives die in 8 years does not infer the other 50% die in 16 years. Is there a chart that takes 100% of a set of drives to their death and gives the results? Or something that would provide equivalent information? Assuming heavy consumer work load on consumer drives in typical climate controled home/office, what is a real world average of hard drive lives? Again, not failure rates given a (short) set life span. Real world results for us is we've had less than 10% drive failure in 10 years and never failures close together so I am pretty comfortable with using aged drives but like to be informed where possible; Our current set of drives range from 0-8 (running) years averaging probably around 3-4 years, the most recent failure was a 5 year running drive. Further We have a 40gb and 80 gb drive the each are well over 10 years (manufacture date) old and still get used reliably here and there. Enough data to say SATA HDDs last reliably well beyond 5 years, but not enough to show a trend of how long. Backround: We are moving to an OBR10 setup for a small business with aged SATA drives of 4-6 years and I am trying to figure out how prudent it would be to move to a 3 copy MD RAID 10 vs 2 copy. With daily data mirrors and full backups it would not be detrimental to have a full primary array loss and need to rebuild and restore from backup, but I would love to avoid such a scenario. However I cannot seem to find data that looks well beyond the age of our current drives. and their is no indications that they fail in droves at the 5 +/- year mark where the data seems to stop. 

Why don't you try symbolic links? I know it works in Linux. Just have the files on your host drive. Then make a symbolic link in your VM to the host drive. Voila. Files are on host, but VM "thinks" they are local to it. We currently use this setup for sort of a backdoor into our files via our office site so we can have access to our job files in the field. But we do not want our job files "in" the web server, but we need the web server to server them to the field via our website. Symbolic links are working great for us. 

Leave it to Windows to be the problem, NOT Linux. Solution was to change the sharing AND the security settings of the share to include the necessary permissions. Found a video here on you tube that was the final piece of the puzzle. Essentially you give both sharing and security permissions to "EVERYONE" on the drive you want to share (this probably is true for a folder as well, but that is not what I was trying to share). For the first, go to the drive and then go to Properties>Sharing>Advanced Sharing>Permission and on the "EVERYONE" group/user assign all the permissions. If everyone is not listed, then add them to the list. For the second go to Properties>Security>Edit>Add and add "EVERYONE". Then assign all the permission you want to "EVERYONE". As a note I also identified the network as "private" via Control Panel>Administrative Tools>Local Security Policy>Network Manger List Policies> then selected the applicable network and changed the location to "Private".. Not sure if this affected anything in the end. I am sure the default settings are all for a good cause and by giving "everyone" full control permissions opened some gaping hole that will cause my computer and network to explode by some people's interpretation of future event, but hey, it fixed the problem. 

This might seem like a round about way to manage things, but I have found the best (most effective with least amount of resources) way to manage data against "durability and data corruption" is to both have properly managed disk arrays and have a proper backup and versioning scheme in place. Checksums used by ZFS and BTRFS array/filesystem combos can actively find the data corruption yes, but it does not necessarily give the "answer" of what to do about it and you may still need your backups anyways for that particular set of data. AND Scrubs on other arrays can find the data corruption also to ensure your arrays will rebuild. Also, battery backups and Write-Intent-Bitmaps can handle the write hole problem. Overall, modern storage systems are extremely robust, if implemented correctly, to handle the problems you will actually run into. If data corruption were a big problem that checksumming array/filesystem combos were the answer, then ALL large enterprise firms would only use and check summing array/filesystem combos and in reality this is NOT the case. Instead they have a robust infrastructure with redundant SANs and switches, battery backups, generators, power conditioners, time-tested file systems, well managed arrays, and backups, backups, backups! The reality is it is an extremely rare case that what little data corruption makes it through will ACTUALLY causes a real problem; and it happens so rarely, that personally I have found it is better to rely on proper systems administration to mitigate it then actively try to manage data corruption itself. I know over the last 20 years I've had a few files, usually media files, that randomly do not work and I assume it is data corruption. But not once have I had a file that actually was needed not work, and IF I did, I would just go to my backups for that file, and IF that did not work, my life would go on! Further, I cannot think of a single file that would be an end all for my personal life or business; NOT ONE. An upset client? A missing piece of information in a lawsuit where I am honest and just need to prove it? A corrupted personal memory? These all are things I would prefer to avoid, but all of these things are only worth a limited amount of my personal time and money to mitigate given that chance is near 0 they will actually happen from data corruption... EVER. Point being, in my opinion the best thing you can do to "Manage data durability / file corruption" for smaller setups is: