Each callback of each MonoBehaviour will be called exactly once for each update loop, but you haven't any guarantee about the order (unless you force the execution order explicitly). 

Even if blend options (and lot of other stuff such as stencil test, depth test,..) are stored inside unity .shader files, their aren't compiled into shader code. AFAIK are just "meta data", used by the engine to setup blend modes just before the drawcall with the real compiled shader(that under the hood is just or ). I guess the minimum number of draw call correspond to the different blend mode you need to support. When you generate it and set the indices you can specify the submesh index they belong to. At each submesh you can (maybe must), assign a different material (that can have a different shader accordingly to the required blend mode). Assuming you are using a single pass shader, you can have one draw call for each material. If you have multiple static meshes with the same materials, then unity can batch them together. If you have multiple dynamic meshes, than you can consider using Graphics.DrawMesh with MaterialPropertyBlock in order to minimize state changes. (More drawcall, but cheaper due to less GPU state changes) hope this helps 

I suggest you to have a look at this page, that shows license features comparison. You can deploy to , or buying the related licenses without buying pro license. The pro license has more features than the normal one. Buying it it's more a matter of features than target platforms. 

For what concern how handles callbacks it's another question. I don't know the exact underlying implementation but most hooks like aren't virtual|abstract method, nor implement any interface. Why? Not sure, I often asked my self why (honestly I prefer a more explicit approach like implementing an interface like ISerializationCallbackReceiver). On how the binding between function pointer and C# method happens, it's probably creating a delegate instance. The problem is really different between SendMessage and . I guess Unity is forced to use Reflection for SendMessage, because the method name is know only at runtime. Use it carefully because reflection is really expensive. For the others Update, Start, etc.. the problem is simpler because method name, signature and target object are know. So the expensive binding process can be used only once in initialization. Here's a nice article. 

A scene is basically a collection of objects (and serialized properties) that the engine bundles and can load together. As such, it's allowed to cross reference objects that belong to the same scene. DontDestroyOnLoad mark an object in such a way that when a non additive scene loading operation(Apllication.LoadLevel|LoadLevelAsync) is performed, that object won't be destroyed. It controls the runtime lifetime of an object. In editor mode, isn't possible to reference an object across different scenes. 

Yes there are possible collisions for every hashing algorithm when the amount of information of the hash value is less than the input value. That's a general unavoidable problem. Depending on the case but generally hashing algorithm are designed to minimize the probability of a collision. I don't know which algorithm Unity3D uses to implement that method, but the collision is very unlikely to happen. If you have the suspect you have a collision check your hash values (and eventually signal the problem to Unity staff since the algorithm should be robust enough to avoid it, and Mecanim should have internal mechanism to detect that 2 parameters have colliding hashes). 

What's my mistake? What am I missing? In addition to that. I'm using tex to store Rect coordinates returned from atlas generation, in order to be able of retriving the correct uv coordinate of an atlas inside a shader. Is this a right way to go? 

That's how it works, right. If you want to setup references in one of your persistent object you basically have 2 ways: 

This should be enough. I don't see any other error. I suggest you to read some tutorials on how works painting in Swing, for example this. 

The main problem is that, even if I use an Atlas, share the material, and declare the objects as static to use static batching, there are few parameters(very fews, it could be just even a float I guess) that should be different for every draw call. I don't know exactly how to manage this situation using . I'm trying 2 different solutions, none of them completely implemented. Solution 1 

When terminates it must awake immediately its parent () pushing it in front of the queue, so it will be executed first. When it gets executed, will be suspended and it's next child task is scheduled () in front of the queue so, it will be executed before as supposed to be: 

Those parameters set by the inspector are used inside the shader to scale the texture coordinates. A in the shader with the name of the texture and is populated with those values. For example considered the main texture: 

You can use a Dictionary to map KeyCode to specific action, eventually you can change this at runtime: 

I move the comment here, so we can eventually close the post. Remember that both MUST have a attached in order the to be called. As side notes: 

There are several better techniques, both for the vertex shader or as post process effect (es. sobel filter for edge detection). Which technique you can use is highly dependent on the final look desired, and maybe depends on your asset characteristic too. 

So, you can't assign directly a to another (you don't have set accessor defined). You are forced to assign individual fields of each to the other. For example: 

You could use LightProbes. Create a from the editor. When you bake the lightmap it will be populated with info on the various light sources. If your combined model are receving probes (check in the inspector), then the shader can calculate approximated lighting coefficients sampling them from the probes structure at run time. 

and can interact quite easily. (In the practice C++ issues relative to name mangling often force to have an intermediate layer) There are several resources you can have a look such as Mono P/Invoke docs and unity docs on native plugins. Calling a function from code is simply as using a function pointer (a delegate can be marshalled to be used this way). 

1 and 2 are basically the same (am I wrong? otherwise correct me), because its the child object(sword) that will be notified of the collision. So if the child doesn't awake the player, then the player must perform some kind of polling. Personally I hate polling for several reasons: 

The result of the traversing should be independent from which implementation strategy is used. The traversing result must be deterministic. 

I can give you an answer for what concern forward rendering and Cg shaders. In the shader first pass (BasePass) Unity set uniforms value of the 1 important directional light and up to 4 vertex lights. Base Pass (Directional per pixel light) uniforms: 

The sentence above makes no sense. In fact you never extends GameObjects, but eventually MonoBehaviors that are Components (actually they inheritance chains is ->-). 

You can check SystemInfo.operatingSystem. The string should contain both OS name and version. Never tried on Android but should work. 

In this case will be executed after at every update, meanwhile with a non event-driven traversing from the root node, the will always be evaluated before . So I have a couple of question: 

Is there anyway I can force to allow me using a custom shader to render spotlights in a ForwardBase pass (using Forward Rendering Path)? For what I could see since now: 

I don't know which is the specific error in your case, but at least from my experience, I'd use quaternion instead. You can just lerp between your current camera rotation and the desired one. Something like: 

First of all you should use paintComponent instead of paint (paint method is used by Swing it self. Avoid it in application code.) So first of all refactor the method of Pong this way: 

I used quite often. 2 recent use cases I encountered: First Motivation: Properties that must be serialized but user doesn't need to be aware of and shouldn't modify explicitly. Example: I wrote a Behavior Tree plugin these days. You can edit the tree layout graphically through an editor extension (a UI similar to Mecanim). Every node is represented by a that as a defining its bounds marked with attribute. This way the user is allowed to layout the tree nodes only interacting through the gui and not accessing directly the . In addition to prevent possible errors and inconsistency it helps to keep the node inspectors clean from data not relevant to user interaction. Second Motivation: Minimize Custom inspector code Example: suppose you need to write some custom inspector code. In my use case I need to guarantee that certain serialized property are not set directly but through class methods to produce the required side effects (es. when add a child to a node must also update the parent reference in the child class.). I need to write a custom inspector but the target class has a lot of properties, and I need to customize only a few. I can mark the property I need to manage explicitly with , call from the custom inspector code (which draw every other property in the default way) and just write a bunch of line of codes to customize access to the required fields. 

do I have understand correctly how event driven BT work? How can I guarantee the depth first order is respected with such an implementation? is this a common issue or am I missing something? 

I'd stay away from static stuff. In Unity you usually don't use static managers class. Instead create a component, with a AudioSource component attached and expose public methods for playing your sound fx. For example: 

Edit: There are several ways of finding objects (or Component attached to object) in a given scene. The simplest (and slowest ) way is using FindObjectsOfType. Generally it's better to avoid find methods, or eventually use them only in initialization ( or ). Because of your cameras are all already present in the scene (aren't they?), you can eventually serialize an array of references to in your component, and assign them from the inspector. 

EDIT If you attach your script you should be able to see the Bezier curve drawn. (Pay attention that your control and tangent points are coincident, so it will be always a straight line). Unity just let you draw such parametric curves, you have to evaluate it on your own. The article I linked to you gives you the code. For example: 

Now afaik and point are fixed values. In my game I have an object oscillating between 2 points on its local X axis. I'm interpolating its position using easing equations, but I would like to be able to modify the end point dynamically avoiding discontinous or abrupt changes. Is it possible to do this using easing equations or something similar? Any suggestion? 

I guess you refers to this shader. That's not an "anomaly", it's how this shader is supposed to work(not a great result imho). So let's have a quick look to the relevant code: 

The problem is the point 5. Let's say I have to assign a different id to an object before drawing it, how can I do this? 

Actually the order of execution in my question was wrong. When a task terminates, if it immediately awake its parent pushing it in front of the queue the depth first order is respected. Here's how the queue status is supposed to be just after leaf_A task terminates: 

Well, the curve parameter domain is [0,1]. You could for example start recording a gesture when a touch is detected near the start control point. Supposing you gesture length is [0,N], what you can to is to map the gesture interval domain to the curve one. 

Actually using the overloaded construct that take a WWWForm as second parameter, WWW class is automatically considered as an http post request. Your code can be something like: 

Some clarifications. Only memory allocated by scripts is managed and can be garbage collected. So if you destroy a GameObject the attached script can be eventually collected, but the life time of resources allocated from the C++ side of the engine is managed in a different way. You can force a garbage collection explicitely call GC.Collect. If you don't force it, it's up to garbage collector decide when free the memory with zero ref count. For what concern GameObject.DestroyImmediate I cite the doc: 

Inheritance Make your concrete classes extend a common base class derived from . You can serialize the reference to the required assets. (I'd go this way in your case) Interfaces You can implement the ICondition interface from your ScriptableObject derived class, but to show them in the inspector, and allow a proper serialization and display you need to do some work: