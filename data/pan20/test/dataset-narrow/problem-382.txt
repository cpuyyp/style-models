This very basic script should be configured with details particular to your setup such as the location to backup the files to, crosscheck existing backups, etc. 

Now that virtualization is common and accessible there are no factors that I can think of that would force a business to put all their databases on one server. I see cases where there is more than one Oracle database in use on the same server but if you are talking Oracle and MSSQL and MySQl all on one server this constitutes a single point of failure. 

I would certainly write this differently today but it works.... The key problem with AQ for me is that I have never gotten queue to queue messaging going between different databases. This was described as a primary value for it. Yet I still like having the certainty that the initial transaction can complete without regard for the state of the destination database. If the message fails to be inserted when into the destination database an error is logged and an email is sent notifying the developer. 

Another solution is to create a package that does your CRUD operations on CAL1. When a value is changed, inserted or deleted the procedure that is called can create a job to update CAL2, or insert the values on an advanced queue to be propagated to subscriber databases. Arwen asks for more details: There are a number of ways to implement this and the best way must take into account your databases and network. 

The existing application uses Microsoft Workflow to implement the business rules. This has proven to be unwieldy, almost unusable for us. Drools and their commercial equivalents are equally complicated and will not achieve the goal of allowing managers to understand their business logic. 

(available from 9i but improved for 10g and 11) If you have packages that are not valid and will not be valid, for whatever reason, you can use this package from Martin Mares which will recursively compile all invalid objects except the ones you specify. As far as best practice goes I try to limit the dependency chain. It's always a toss up between "Don't repeat yourself" which encourages dependencies and having stand alone packages. My rule of thumb is that if the package/trigger/object interacts with another database try to reduce dependencies so if there is a issue you have less places to look for sources of the problem. 

Not that unusual for a best practice if done well. I see read only service accounts and more powerful accounts for web services to connect with 

In part this is driven by the application. It creates a pooled connection to the database as required. Users log onto the application but connect to the database as a shared user who has all the permissions required. To go into a more detailed example: User X logs in. They have a username and password that matches what is in Active Directory. After authentication they are taken to the main page. During that time the application queries the database for what permissions this user has and caches them in memory. The main page loads each sub work area. Each area asks the cached permissions: - "Do you have authority to see this area?". If so show the work area. - " Do you have permission to edit this area?" If so show an edit/create link. Edit2: The original poster comments that 

We have an Oracle 10.2.0.3.0 on Windows 2003 Server which is scheduled for upgrade within the next six months. One tablespace is about 2TB in size with over 250 data files. Some of these were created with a maximum size of 8GB, some with 16 GB. This database is heavily used by a number of web services that regularly add large amounts of data daily. It was suggested to extend the smaller datafiles so they are all 16GB but we are concerned that this will affect performance as this would cause new data that is regularly accessed, transformed and queried next to old data which does not see a lot of access. Storage is using Netapp on iSCSI virtual disks. Any thoughts on if putting new data in older datafiles will affect the speed of inserts or selects? 

Edit: @Justin Cave Good point. The query starts out using bind variables and to store it in a table for reuse will product a static query. I guess I want the best of both worlds. I have a few users for this application who do a lot of querying so it would be a nice feature to save their input. Maybe I should save the the bind variables. Edit @Justin Cave If you were to suggest that I hold the user parameters in a collection I would accept that as an answer 

In your script to import set these variables NLS_LANG ORACLE_SID= your database name impdp a_user_who_exists/your database name directory=DATA_PUMP_DIR network_link=original database name schemas= user1,user2 LOGFILE=DataPump.log TABLE_EXISTS_ACTION=REPLACE 

This query runs on 11.1.0.7 Enterprise and provides similar results to the the OEM Grid performance page which requires the diagnostics package. There is a certain irony to running this script through SQL Server reporting services which is beyond the scope of this question. 

Up to Oracle 11.1 you could mark a package as only reading from the database by adding PRAGMA RESTRICT_REFERENCES( DEFAULT, WNDS); This has now been deprecated. I would like to enforce read only on some of the packages. 

If the databases are on the same network and are not too many versions out this should work: You have to use the export utility from your older database to get the data out of the newer database. (this might not work when trying to use an oracle 8i exp on an Oracle 11g database) Add the information about the new database to the TNSNAMES.ora file on the older database Run the exp utility from the command line or script on your old database 

This business model is for a case management database. This is closely modeled on the idea of a file folder representing the phase and a sequential checklist representing the stages. A case consists of a phase that can have one or more stages. A phase can only have one stage that is "Current" or open at any one point in time. A case can only start from one type of stage but can progress to any one of a number of stages that are end types. In this business model there are many different types of phases and stages An example: you apply for a license. The process always starts with you submitting a form but can have different endings: the application is approved or rejected or sent back for more information. Edit: @Colin 't Hart asks what a phase is in relation to a case. Here is where trying to simplify a question can omit details. The complete schema structure is: - one case can have one or more phases but only one phase is open or "current" at at time. - each phase can have one or more stages but only one phase is open or "current" at a time. - there are different types of cases/phases/stages and transitions from the current unit to the next unit require adding a close date to the current and inserting a new record with an open date. An example: a production line for widgets 

This question is more complicated than how SGA relates to cpu utilization. Many other factors can cause a high CPU usage even if there is adequate SGA. Start looking at the other Oracle diagnostics for the usage of the shared pool, large pool and buffer. Even more important you need to investigate the application front end, number of sessions, type of data operations your users are up to... What kind of SQL is getting fed to the database, does it use bind variables, does it require full table scans? Oracle tuning is more successful when you look at the whole picture. Tell us more... 

From this blog you can try this on development first... SQL> spool resolve.sql; SQL> select ‘alter java class “‘||object_name||’” resolve;’ 2 from user_objects 3 where object_type like ‘%JAVA%’; SQL> spool off; SQL> @resolve The clean sweep approach would be to remove and reinstall the java JVM The Java VM is created and populated with system classes during CREATE OR REPLACE JAVA SYSTEM command. Run rmjvm.sql to remove the JVM first, then bounce the database then initjvm.sql I repeat, these should be tried on your development stack first! 

There are a number of other ways to do this including renaming the database using DBNEWID but a new install on a new machine is easy and allows you to increase memory and storage at the same time. Note: 11.2.0.3 is already out of support, why not go to 11.2.0.4 which is supported for a little while longer?