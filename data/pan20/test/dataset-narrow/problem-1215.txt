At the end of execution, after we've finished applying the closure rules, the set of clobber sets on all of the edges from $u$ to $v$ gives the set of minimal clobber sets for all paths from $u$ to $v$. This is effectively a worklist/fixpoint algorithm, and it follows pretty directly from the problem statement. If you prefer a more formal treatment of this problem, see below. 

$\Pr[C(x) \ne D(x)]$ is exponentially small, when $x$ is drawn uniformly at random from $\{0,1\}^n$ (in other words, they have almost-identical functionality); and, $C,D$ differ in graph edit distance by a tiny amount (their edit distance is much smaller than the size of the circuit, say, $O(1)$ or some small constant), meaning that almost all of the gates and wires of $C$ match a corresponding gate and wire in $D$, with only a few gates added/deleted/changed. 

Your problem can be solved in polynomial time, by reduction to bipartite matching. In other words, there is a polynomial-time algorithm to find the largest subset $S$ of rows with your desired property. Construct an undirected bipartite graph with $m$ left-vertices (one per row) and $n$ right-vertices (one per columN). Add an edge $(i,j)$ whenever $M_{i,j} \ne z_j$. Now each subset of rows with your property corresponds to a matching in this bipartite graph: take $S$ to be the set of left-vertices covered by the matching. In particular, the maximum matching in this graph corresponds to the largest subset of rows that satisfies your property. You can compute the maximum matching in polynomial time. 

Let $S = \cup_{d \in D} (d+B)\cap A$. This is the multi-set of suggestions. Count how many times each value appears in $S$. Let $A^*$ be the set of values that appear at least 6 times in $S$. (This can be implemented efficiently by building an array $a$ of 251 initially, initially all zero, and each time the number $s$ is suggested, you increment $a[s]$; at the end you sweep through $a$ looking for elements whose value is 6 or larger) 

Finally, it is easy to see that this 1-to-3 of $k$ problem has a solution if and only if $\varphi$ has a satisfying assignment. That completes the proof. Theorem 2. Your problem is at least as hard as the 1-to-3 of $k$ problem. Proof. Suppose we have a 1-to-3 of $k$ problem with $s$ sets over variables $y_1,\dots,y_r$. Define $f(0)=0$, $f(1)=1$, $f(2)=1$, $f(3)=1$, and $f(k)=0$ for all $k>3$. Define the matrix $M$ based upon the sets $S_1,\dots,S_s$. In particular, if $S_i=\{a,b,c,\dots\}$, the $i$th column of $M$ has a 1 in the $a$th, $b$th, $c$th, $\ldots$ rows and is zero elsewhere ($M_{j,i}=1$ if and only if $j \in S_i$). Finally, set $\alpha=s$. Now $|f(v)|\ge \alpha$ holds if and only if $f(v_1)=f(v_2)=\dots=f(v_s)=1$. Thus, this choice of $f$ and setting of $\alpha$ ensures that the only way to satisfy the equation $|f(v)|\ge \alpha$ is if each $v_i$ is 1, 2, or 3. If we focus on $v_i$, corresponding to the set $S_i=\{a,b,c,\dots\}$, then $f(v_i)=1$ if and only if $1 \le y_a+y_b+y_c+\dots \le 3$. Therefore, this instance of your problem has a solution if and only if the original 1-to-3 of $k$ instance does. 

That said, this might not be exactly what you are looking for. It requires methods to count the number of integer points inside the polyhedron, which isn't the same as the volume of the polyhedron. Also, I don't think they need to analyze polyhedra of dimension 1000 or higher (though I'm not sure about that). 

It evaluates a number of different algorithms for DFA minimization, including their application to your situation where we start with a NFA and want to convert it to a DFA and minimize it. What does the strongly connected components (SCC) decomposition of your NFA (considering it as a directed graph) look like? Does it have many components, where none of the components is too large? If so, I wonder if it might be possible to devise a divide-and-conquer algorithm, where you take a single component, convert it from NFA to DFA and then minimize it, and then replace the original with the new determinized version. This should be possible for single-entry components (where all edges into that component lead to a single vertex, the entry vertex). I don't immediately see whether it would be possible to do something like this for arbitrary NFAs, but if you check what the structure of the SCC looks like, then you might be able to determine whether this sort of direction is worth exploring or not. 

The consequence is that, given a quadratic multivariate polynomial $\Psi(x_1,\dots,x_n)$, and given $E(x_1),\dots,E(x_n)$, anyone can compute an encryption of $\Psi(x_1,\dots,x_n)$. This is super-useful for your situation. In particular, in your situation, we can form the polynomial $$\Psi(b_1,b_2,\dots,b_N) = \sum_{i \ne j} [b_i (1-b_j)].$$ Note that this is a quadratic multivariate polynomial, so given all of the $E(b_i)$'s, anyone can compute $E(\Psi(b_1,\dots,b_N))$. Also note that $R=\Psi(b_1,\dots,b_N)$, so we're trying to compute exactly the value of this polynomial. This suggests a natural protocol for your problem, using a threshold version of the encryption scheme in the paper referenced above: 

I don't know whether your result -- if valid -- would be a non-trivial advance, but here is one sort of problem you could test it on: Problem. Fix a function $f:\{0,1\}^n \to \{0,1\}^n$. Given $y \in \{0,1\}^n$, find $x \in \{0,1\}^n$ such that $f(x)=y$. If $f$ can be computed efficiently (say, by a small circuit), your result implies some sort of solution to this problem. In the cryptographic world, the best known algorithm for this problem does a precomputation (depending only upon $f$) that requires $2^n$ time and $2^{2n/3}$ space, and outputs some advice of size $2^{2n/3}$; then, given $x$, it can find $y$ in $2^{2n/3}$ time, using the advice string of size $2^{2n/3}$ from the precomputation. You can adjust the space vs time tradeoff, to use an advice string of size $S$ and take time $T$, as long as $S \sqrt{T} = 2^n$. As far as I know, this complexity is believed to be the best possible, for algorithms that do not take into account any of the internal structure of $f$. In particular, it is likely to be optimal when $f$ is a cryptographically secure hash function. (This technique is known as the Hellman time-space tradeoff.) So, if your technique can do better than the Hellman time-space tradeoff on some cryptographically secure $f$, it would certainly be news. 

The error in the sum of the estimates for all but the first range will be at most $0.25 \epsilon$. The error in the estimate for the first range will be at most $0.25 \epsilon$. Therefore, the total error in the sum of all these estimates will be $\le 0.5 \epsilon$, which is enough to distinguish between a total of $1$ vs a total of $1-\epsilon$. 

Here is another approach. Given the low $k$ digits of $3^n$, you can learn $3^n \bmod 10^k$ and thus $3^n \bmod 5^k$. It looks like $3$ is a generator modulo $5^k$ (i.e., $3$ has order $\varphi(5^k)=5^{k-1} \times 4$). Therefore, by using discrete log and Hensel lifting, I think you should be able to compute $n \bmod \varphi(5^k)$ from the low $k$ digits of $3^n$ very efficiently. In other words, you start by computing $n \bmod 4$ from the low digit of $3^n$, by taking the discrete log of $3^n \bmod 5$ to the base $3$, modulo $5$; this reveals $n \bmod 4$, and can be done in $O(1)$ time. Then, you find the discrete log of $3^n \bmod 25$ to the base $e$, modulo $25$; this reveals $n \bmod 20$, and can be done in $O(1)$ time (taking advantage of the knowledge of $n \bmod 4$, there are only $5$ possibilities you have to try). Iterate. At each step, you use the knowledge of $n \bmod \varphi(5^{k-1})$ to help you efficiently compute the discrete log of $3^n \bmod 5^k$, making use of the fact that there are only $5$ possible values for $n \bmod \varphi(5^k)$. Now just let $k$ be large enough, and this reveals $n$. You'll need to work out whether the running time is $O(n)$, but it looks to me like it might be. I suspect it's enough to let $k=O(n)$, and I suspect you can do each iteration in $O(1)$ time, for a total of $O(n)$ time. 

Any efficient algorithm to solve the subset $k$-product problem (for $k$ large enough) would also give an efficient algorithm to solve the discrete logarithm problem in that field. Therefore, if you are working in a large finite field where the discrete log is hard (e.g., $GF(p)$ where $p$ is a sufficiently large prime), there is no hope for an efficient algorithm to solve the subset $k$-product problem. Conversely, if you are working in a field where the discrete logarithm problem is easy, it is easy to convert any instance of the subset $k$-product problem to the subset $k$-sum problem: just take discrete logs. Therefore, if the discrete log problem is easy, subset $k$-product is no harder than subset $k$-sum. 

Here's the first approach that occurs to me. Build a complete graph on 11,000 vertices, and label all edges with the great-circle distance. Apply some travelling salesman algorithm to choose the best route you can find. Then, for every edge on that route, query the map service for the road distance and replace the label on that edge with the road distance. Then, apply the travelling salesman algorithm again on the modified graph. Repeat this basic process until it converges. I think the great-circle distance should be a lower-bound on the road distance, so I suspect this should be correct (in the sense that it doesn't miss out on a better solution). One possible complication is that this might cause triangle-inequality violations, so if you are relying upon a heuristic that assumes the triangle inequality, this approach might screw things up. I don't know; you could try it out and see. If you wanted to reduce the number of queries for the road distance even further, here's one more idea you could try. I suspect that the smaller the great-circle distance, the closer it is to the road distance; or, in other words, I suspect the gap between the distances might be largest for points that are far away. That's something you'd need to validate yourself. If it is correct, then a variant of the algorithm I suggested above is: only query the map service to obtain the road distance for edges where the great-circle distance is above some threshold. (You could even gradually reduce this threshold in each iteration of the above algorithm.)