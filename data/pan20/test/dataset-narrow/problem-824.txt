The disk space problem is likely best solved with filesystem quotas. Unfortunately I have little experience with them. If you want to get fancier (restricting filetypes and such) you'll probably need to write your own script to validate the commands being passed from the client. The simplest script would be a shell script that performs a test similar to this one: 

Yes, assuming your sysadmin did not explicitly filter them out. They're sync'd as part of the "mysql" database. 

The code installs the packages for whichever version of Python is being used to run it. It sounds like your system default version of Python is 3.1, so something like this should make it work the way you want: 

Simple shell script wrapped around or . For larger installations, a Puppet recipe could reliably deliver the updated config file and bounce Apache afterwards. 

While you haven't explained what the problem is, a good place to start is by manually injecting a message into Postfix via telnet. A decent article on the subject can be found here: $URL$ Tail the mail log, like Aleksander mentioned, while doing this. 

works just fine on Linux NFS, including from PHP. We use it extensively and have tested it thoroughly to verify it's working as desired. Check to see if you're running all of the necessary services on both the client and server. Look for "portmapper" and "rpc.statd". If they're not running, you need to figure out which init script starts them on your distro. On Debian-based distros it's "" and "". From the client, run "" and see if you get a response. On my setup, I get "program 100024 version 1 ready and waiting" as the result. Oh, also bear in mind that in some circumstances NFS and statd can get upset if both the client and the server don't have reliable hostname entries for each other. Double check on both machines. 

Technically you could put anything at all in the place of "ACCEPT" as the result is ignored; just the presence of the match is sufficient: 

XMPP (Jabber) servers work just fine without an internet connection. I use ejabberd and it works quite well, but there are a number of other options for both Linux and Windows servers. XMPP also has excellent client support, including most of the "alternative" IM clients like Adium, Miranda, Pidgin, etc. 

Now accessing "hello.example.com" will load the site located in . Alternatively, as mentioned by Gabor, you can use mod_rewrite. The solution I'm going to outline assumes that you have no other content under example.com. 

Not really; you haven't included nearly enough info to make a reasonable guess. Start by listing all of the IP addresses involved (feel free to anonymize the public IP if that makes you feel better). Describe the settings you changed on your DSL router. Run an ssh connection attempt with the flag and include the output. I'm sure we'll be able to help you troubleshoot this problem, but you have to give us something to go on... 

I just finished a shell script that does monthly full backups and nightly incrementals, all to an offsite server. This is how it works: The offsite server runs a nightly cron job. On the first Sunday of every month, this script runs and pipes the output to a file. It then scans backwards from the end of the file and records the last revision number in a file. On all other nights, the script runs, via ssh, 'svnlook youngest /svnpath' and compares the output to the last recorded (backed up) revision number. If there have been new commits, it runs and dumps the output to a file. Then it runs some cleanup code to delete full dumps older than $N days and any incremental dumps older than the newest full. 

That seemed to work great for the URLs, properly subsituting in the result from the , but URLs returned just the first part of the redirected URL, leaving out the results from the lookup. If I reversed the order of the rules, the failed/working URLs reversed. As I have always understood syntax, if the URL path doesn't match the pattern, the rule is skipped and processing moves on to the next one. In this case, the I finally managed to get it working by breaking it out into two blocks: 

Meru runs some good bundle pricing on their smaller controllers. I think you need to call the reseller in your area to get pricing. I have 5 of their radios in a 10,000 square foot office and they work great. 

I think m4 would be overkill. My first instinct would be bash + sed, but if you have any familiarity with a higher-level language that might be easier. Off the top of my head, I would imagine something like this: Data file: 

Previous PERC controllers could all be managed/monitored by the LSI MegaCLI tools. The latest release of those tools does not appear to recognize my H700 card: 

If your LDAP directory has been correctly enabled, you will see a full list of all the users, formatted like a passwd file. If that doesn't work, I'm afraid I can't help as I'm not familiar with webmin. Ubuntu 10.04 does make it pretty simple to enable LDAP auth from the command line, though, using : $URL$ 

If you access it locally you can specify a new password. On Linux this would be done via . Something like this should work: 

I've used a variety of vendors over the years and I always come back to Dell. Their build quality is hard to beat. For important servers, I love the fact that I can call them up and say "it won't boot" and 90 minutes later a guy shows up with a box full of parts and begins swapping them out until it works. Fried motherboard on a production mail server. Total downtime: 2 hours. (Yes, it should've been redundant. It wasn't.) 

I'm running MySQL on an Ubuntu 10.04 server, with the MySQL $datadir located on an iSCSI volume. The filesystem is tagged with "_netdev" in . On system shutdown, upstart sends MySQL a SIGTERM, then moves on to shutting down the network interfaces, without waiting for MySQL to exit or for the iSCSI disk to be unmounted or disconnected. This, of course, results in XFS not unmounting cleanly and MySQL needing to spend several minutes replaying InnoDB logs on startup. I'm not clear on the right place to begin poking to fix this. From what I've read, upstart should already be smart enough to wait for iSCSI filesystems to unmount and disconnect before it shuts down the network interfaces, although this isn't happening. And I have no idea how to tell it that it also needs to wait for a clean shutdown of MySQL. This is what the console shows during the shutdown process: 

So I unmount the external drive, attach the next one, mount it, enter at the prompt and the backup continues. Good ol' tar. 

Granting SELECT and INSERT on all tables should allow you to run . The link you posted to lists the commands it calls: 

The config you have now is only going to make sure you only have a single simultaneous delivery to each destination. It's not going to actually throttle anything. I think adding this line to may accomplish what you want: 

EDIT: Further research indicates you probably don't want the option an that you first need to run to load the kernel module. 

Explanation: The command counts the number of copies of your script currently in the process table. The construct is a simple way to do basic math comparisons and operations in the shell, in this case checking to see if the output of the command is less than 10. 

In a sane world, the output of the two would be identical. How they differ should tell you a lot about why this is failing. 

Ideally you would have enough memory for your entire MySQL db to be held in memory. On top of that, you need enough memory for . When I was doing this calculation, our tests indicated assuming 25MB / Apache+PHP5 process was a reasonable number. Combine the calculated size of your MySQL DB and estimated Apache memory requirements and add 25% more for some head room. EDIT: I misread your question. I thought it said 1K requests per minute. So a bit more reasonable requirements: If your DB is big enough that speed is a concern, you still want it to fit in memory. Otherwise your gut is correct, a 512M instance will be just fine. EDIT #2: However, bear in mind that if you end up with a slow DOS or just a small traffic burst that pushes you much past 15-20 simultaneous Apache sessions, you'll end up in swap. And a swapping box is a useless, mostly dead box. So be sure to cap your below that. I would suggest 10 - 15 for a 512M box. 

It sounds to me like your firmware is probably behaving properly and you do have a legitimate IP conflict. Do you have a VMWare instance running anywhere on your home network? That's who owns the MAC prefix... 

This won't, however, tell you anything about modifications their postinstall scripts may have made. I would argue that actually trying to diff a filesystem snapshot after install is overkill. There are a number of tools for tracking file access that may be of help. 

The module in Apache will show you which virtualhost / request a particular process is handling, but this will only be useful to you if the requests are long-lived. Adding execution time () to your Apache is also useful. More important is tracking down the bottleneck. For this you need to investigate and (usually installed by default) and a lovely newer tool that combines the two and adds even more stuff, . 

Postfix sets an header to the original address. In this case it would contain . This header is controlled by the variable in your . 

I'm starting to manage more custom-packaged applications for our Ubuntu Hardy systems. Some are of apps not available in Ubuntu but most are much newer versions than are available via the standard "-backports" process. I'm trying to settle on the best way to manage these packages in subversion. One thought is just to commit the generated diff. Has anyone found a better way?