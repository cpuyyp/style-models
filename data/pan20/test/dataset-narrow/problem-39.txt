It's fine to use photometric units as an overall scale for setting light brightnesses. However, there's a technical subtlety you should be aware of. I'll quote from a blog post I wrote on the subject last year: 

First of all, the "frameless rendering" technique is in the context of raytracing, not rasterization. It's not obvious how it could be made to work effectively with rasterization, given that the basic idea of it is to update an image by a combination of temporal reprojection plus firing rays specifically at areas where the algorithm thinks the image is undersampled. So this technique is, prima facie, not compatible with rasterization-based graphics applications. But if you're already doing raytracing for other reasons, this technique would be interesting to look at; it certainly appears to improve quality relative to an image raytraced from scratch each frame, with the same number of rays per second. Raytracing on a GPU is certainly possible; you don't need an FPGA for that. I've only skimmed the second paper, but my reading of it is that the main reason for the FPGA is to get a close coupling between the display scanout and the rendering activity: namely they "race the beam" and evaluate pixels just before they're about to be scanned out, thus obtaining low latency. The GPU equivalent of this is probably to split the image in thin horizontal strips, and kick off a compute dispatch to render each strip just before it starts to be scanned out. Today, this is difficult to accomplish as it requires either millisecond-precise scheduling that desktop OSes are not currently set up for, or it requires the GPU to be able to dispatch based on an interrupt from the scanout unit—a hardware feature that doesn't currently exist AFAIK (or if it does, it isn't exposed in any graphics APIs). Or you might be able to make it work with a long-running asynchronous compute dispatch, if you can find a way to stay in sync with scanout. So, there are obstacles, but they aren't insurmountable, and I think if there was sufficient interest in racing-the-beam-style rendering, then OS and GPU vendors could come up with a way to do it in the future. So I don't think an FPGA is required for this kind of technique to work. On the other hand, the fact that it's based on raytracing is a much bigger obstacle to using it in "real-world" apps and games. 

It's pretty easy to create a single frustum that encloses both of the individual eye frusta, as shown in this diagram created by Cass Everitt at Oculus. You just have to place the vertex of this frustum between the eyes and behind them a little bit. You can also increase the near plane distance to match the original near plane of the eye frusta. 

The terms have to do with the "thickness" of the voxelization. I'll illustrate with the help of a diagram about 2D line rasterization (from this unrelated question). 

From the way the problem only manifests when you start overwriting previously used texture tiles, I suspect you're running afoul of some driver heuristic that's attempting to manage the lifetime of the texture data on the GPU. In general, when you update buffer or texture data from the CPU, the driver has to make sure it doesn't overwrite anything that might still be in use by the GPU (considering that the CPU and GPU run asynchronously to each other). There are various ways the driver can do this, such as internally double-buffering the texture, inserting sync points (for the CPU to wait on the GPU), or initially copying data to a temporary staging buffer and then later re-copying it to its final destination. All this stuff is under-the-hood and the driver makes its own decisions about what strategy to employ in any given situation, using some heuristics based on texture size, usage patterns, etc. I can only guess at the details, but I suspect what's happening is that the driver is getting into a state where it thinks those previously-used texture tiles are still in use on the GPU somehow. Then when you start to overwrite them, it decides that it needs to make a second instance of the whole texture, and laboriously copies everything over before finally doing the updates you wanted—or something like that. That could be the many tiny draws you see in Nsight. It's still a mystery why it's taking tens of milliseconds to do that, though. Maybe the textures are large enough that a second copy of them causes you to run out of VRAM? In that case, you'd get a lot of thrashing as things are spilled to main RAM. That's just a guess, though. As for why the bad behavior only happens for a few tiles and then goes back to normal, it could be that the driver heuristics are deciding to switch to a different strategy at that point. Anyway, as for how to fix it, you might try using whenever you free a texture tile. This lets the driver know that you don't care what was in that texture tile before, which hopefully will convince it to just update the texture in-place rather than doing anything crazy with it. Another thing to try is to make sure you don't try to re-use a texture tile in the very next frame after you free it, but give it a frame or two of "downtime" before re-using it. But it sounds like that's probably the case already, given that you prefer unused tiles before you start re-using them. If that doesn't help, I'd recommend sending an email to devsupport@nvidia.com. The OpenGL driver team there can likely take your Nsight captures and help diagnose the issue, and fix it if it's due to a driver bug. 

Sketchpad was a system that allowed you to draw simple shapes using lines and curves in 2D. It maintained constraints between the shapes. For example, an endpoint of one line could be constrained to lie on another line. Also, as mentioned in the Wikipedia article, the user could set a fixed length for a line, or an angle between two connected lines. These constraints would be automatically applied by the system while editing the diagrams—e.g., when moving one line, other lines would be adjusted to maintain the constraints. 

What they're saying is you don't have to render the game at the same resolution that it's displayed. For example you could be in a display mode of 1920×1080, but have the game rendering at only 1280×720, to run faster. The OS and display hardware can automatically upscale the frames to the display resolution under the hood. Usually, the OS runs a window compositor, a background process that draws the desktop, all app windows, menus, etc and presents them to the display. In windowed mode, any upscaling needed would be done by the compositor. However if the game's window is covering the whole display ('borderless fullscreen') and nothing else is on top, then the OS can switch off the compositor and let the game present to the display directly. But then if the game is at a lower resolution than the display, it still needs to be upscaled. The GPU can do this in hardware (scanout) as long as the scaling factor isn't too big. For instance, there may be a limit of 4x scaling. So, if your rendering resolution is more than 4x smaller than the display, the OS can't use the hardware upscaling and would need to keep using the window compositor instead. All of this stuff happens automatically in Windows 10, without the app needing to know about it or do anything special. Given all this, they point out that there's no real reason for the rendering resolution of the game to be limited to the classic display modes like 800×600, 1024×768, 1280×720, etc. You might as well have a continuous resolution slider instead of a fixed set of modes. There's no problem with choosing a "weird" resolution like, say, 1000×562 or something. The upscaler/compositor will just take care of it. This what they mean by "consider providing users with the ability to choose a rendering resolution for performance scaling, independent from an output resolution". In other words, in your game's options screen, you should give the user a slider to control rendering resolution, so they can tune it for performance vs visual quality. But it need not be tied to the actual display resolution. 

I'm not aware of any rendering technology designed specially for fingernails. Just eyeballing it, I would suggest that a combination of subsurface scattering with a relatively smooth glossy specular surface would get you most of the way there. In other words, you could use the same shader as you do for skin, but with different textures and different specular parameters. I wouldn't guess that a hair shader would be too useful. Nails are not in the shape of fibers, nor do they have the kind of complex occlusion and shadowing that hair does. That being said, if the scattering properties of the nails are different enough from that of skin, it might make sense to use a different shader (for instance, doing some sort of volumetric scattering instead of a dipole approximation). I don't know of any references on skin scattering that have investigated nails specifically, though. 

You have to use the appropriate register type for the resource: registers for constant buffers, for textures, and for UAVs. AFAIK, it is not possible to bind a UAV to a texture slot, or otherwise mismatch registers and resources. However, a resource can have multiple views, so it is possible to have a resource bound as a UAV in one shader and as a regular texture (SRV) in another, which enables all the usual GPU texture sampling features to be used in the latter case. The details of how the DirectX API concepts of slots, registers, and resources map to the actual hardware are vendor-specific. On one GPU they may literally represent registers of some kind, while on another they may represent a small chunk of metadata in memory somewhere, which describes the resource to the hardware. 

Time runs from left to right and the width of each block represents the duration of one frame's work. After the first few frames, the system settles into a state where the frames are turned out at a steady rate, one every two vsync periods (i.e. if vsync was 60 Hz, the game would be running at exactly 30 fps). Note the empty spaces (idle time) in both CPU and GPU rows. That indicates that the game could run faster if vsync were turned off. However, with vsync on, the swapchain puts back-pressure on the GPU—the GPU can't start rendering the next frame until vsync releases a backbuffer for it to render into. The GPU in turn puts back-pressure on the CPU via the command queue, as the driver won't return from / until the GPU starts rendering the next frame, and opens a queue slot for the CPU to continue issuing commands. The result is that everything runs at a steady 30 fps rate, and everyone's happy (except maybe the 60fps purist gamer ). Compare this to what happens when the application is CPU-limited: 

Most of these techniques are not widely used in games today due to problems scaling up to realistic scene sizes, or other limitations. The exception is screen-space reflection, which is very popular (though it's usually used with cubemaps as a fallback, for regions where the screen-space part fails). As you can see, real-time indirect lighting is a huge topic and even this (rather long!) answer can only provide a 10,000-foot overview and context for further reading. Which approach is best for you will depend greatly on the details of your particular application, what constraints you're willing to accept, and how much time you have to put into it. 

The values will only form a cube after performing the perspective divide, which I don't see happening in your code. That is, you take a vector $[x, y, z, 1]$ and transform it by the projection matrix, resulting in a new vector $[x', y', z', w']$. Then divide out the fourth component to get a 3D vector, $[x'/w', y'/w', z'/w']$. This last is the one that should be within a $[-1, 1]$ cube, if the original point was within the view frustum (and if it's an OpenGL-style projection matrix—Direct3D-style ones have $[0,1]$ for the post-projective Z range, instead of $[-1, 1]$). Without the divide by $w'$, if you just look at $[x', y', z']$, indeed you'll just get a scaled and possibly reflected frustum. Matrix multiplication alone can't transform a frustum into a cube, as it's not a linear or affine transformation; instead, it's a projective transformation, and the divide by $w'$ accomplishes that. 

Dithering effectively allows you to represent intermediate values between 8-bit integer levels. For instance, an area that the shader computes as 82.3 (out of 255) would normally be uniformly rounded down to 82, but with dithering it will come out as a random mix with 70% of the pixels set to 82, and 30% set to 83. The eye averages over this very slight, high-frequency noise and it becomes practically invisible. 

It's not 100% clear what the author means here, but I'll choose to interpret "screen coordinates" as "pixel coordinates". These would be related to the projected points by a 2D coordinate transformation. You're correct that projection is done by dividing each component of a point by its $z$ component. (That's not quite true—actually, we usually use homogeneous coordinates, which have four components $xyzw$, and divide by the $w$ component, which has previously been set to eye-space $z$ by the projection matrix. This frees up the $z$ component of the vectors to be used for the z-buffer.) The projected points $q_i$ are then in "normalized device coordinates" or NDC space, which has a range of [−1, 1] on each axis. To calculate the resulting pixel coordinates on the screen, we then need to perform a 2D scale-and-translate operation, to map the [−1, 1] range to the desired range of pixels. For example, if we want a 640×480 viewport, we could calculate $$x_i = 640 \cdot \frac{q^x_i + 1}{2} \\ y_i = 480 \cdot \frac{q^y_i + 1}{2}$$ You might also need to flip the $y$ axis in this transformation, for instance if the NDC space is Y-up but the pixel coordinates are Y-down. 

UV mapping, LOD selection, and filtering work just the same way as for power-of-two texture sizes. Generating good quality mips for a non-power-of-two texture is a little trickier, as you can't simply average a 2x2 box of pixels to downsample in all cases. However, a 2x2 box filter wasn't that great to begin with, so using a better downsampling filter such as Mitchell-Netravali is recommended regardless of the texture size. 

But in practice, you'd be replacing the above with your own interpolation logic. This will for sure be slower than using hardware bilinear interpolation, but it may be an acceptable cost for your app. If you need more than 2×2 block of texels, you can also use multiple calls to to grab more of the surrounding region, 2×2 at a time. 

The work-efficient version requires half as many threads to begin with. In the naive algorithm, they have one thread per array element; but in the work-efficient version, each thread operates on two adjacent elements of the array and so they need only half as many threads as array elements. Fewer threads means fewer warps, and so a larger fraction of the warps can be actively running. Although the work-efficient version requires more steps, this is offset by the fact that the number of active threads decreases faster, and the total number of active threads over all the iterations is considerably smaller. If a warp has no active threads during an iteration, that warp will just skip to the following barrier and get suspended, allowing other warps to run. So, having fewer active warps can often pay off in execution time. (Implicit in this is that GPU code needs to be designed in such a way that active threads are packed together into as few warps as possible—you don't want them to be sparsely scattered, as even one active thread will force the whole warp to stay active.) Consider the number of active threads in the naive algorithm. Looking at Figure 2 in the article, you can see that all the threads are active except for the first 2k on the k​th iteration. So with N threads, the number of active threads goes like N ​− 2k. For example, with N = 1024, the number of active threads per iteration is: 

It's likely due to shadow rays. When the light source is exactly on the plane of the wall, a shadow ray traced to it may or may not be found to intersect the wall before it reaches the light. The result will depend on arithmetic roundoff error, and may appear as random noise, or stripes or some other artifact. To fix it, you can either keep the light offset slightly from the plane, or set up the wall object not to cast shadows (assuming it doesn't need to in the scene). 

Normal distribution functions are defined a bit differently than you might expect. They're not strictly a probability distribution over solid angle; they have to do with the density of microfacets with respect to macro-surface area. The upshot is that they're normalized with an extra cosine factor: $$ \int_\Omega D(m) \cos(\theta_m)\, d\omega_m = 1 $$ This cosine factor accounts for the projected area of microfacets onto the macrosurface. When importance-sampling the NDF this cosine factor must be accounted for as well. This is actually hinted at in the text of the paper, as it says