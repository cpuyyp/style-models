This is normal behaviour for logrotate as you described it. Normally you would want to logrotate your logfiles not via script but via cronjob(s). For gzipping the files via logrotate you could not include the date and rather let logrotate handle the dateext. So for a "solution" to your issue you could use the logrotate options 

The kernel messages suggest (to me at least) an issue with all 4 drives, which lead's me to believe it's the sata controller who might be at fault. 

However after version 2.4. does not contain anymore so you might need another tool (or it is renamed to another function which i did not find) Qoute Manual from gnu.org 

There seems to be a somhow possibly related BUG from Mr Oetiker on github for rrdcache: $URL$ This actually could be my issue (concurrent writes) but it does not explain the cronjob to not fail. In the asumption i actually have 2 concurrent writes would return exit code 1 (per man page ,confirmed in testing) As i do not get an email with the output either and the observation that the cronjob do actually run fine all the other time i am somehow lost. Example output: 

While this is a rather broad question here are some hints: If you really need to have encryption of data you should at least go for a lot of enterprise disks, md5sum(or similiar) checksum tools for every file in storage, and for the container, have multiple full backups on different media. off-site storage with tapes (obviously checksum the written data aswell. make constant recovery tests on every medium, and plan to switch the hardware depending on the "risk" you want to take. For Server grade Disk: example with fictional values: suppose you have 1 defect sector in ~3 years. this would make it roughly 1/1000 chance of having one defect sector per day. (lets ignore the amount of data variable for this example) As the drive ages the risk increases until it fails. So either you swap them out after you reach a certain threshold (like 1 or 2 %) or you risk having one backup/parts of backup fail, and you have to make sure that : 

what we used previously seems similiar to what you need: Openvpn Server which gets connected by a few 100 clients (they connected via GPRS, every client had its own cert, hostname and own IP. our DNS Server updated the IPs of the hostnames (custom-up.sh and custom-down.sh scripts) and you were able to f.e. ping every client on his hostname. we had a DNS zone for the link hostname-"VPN ip" 

Most likely the insert will actually slow down the connection significally. what you could try : renice and ionice the backend pid with the insert statement. Depending how you start the sql you might be able to directly use the "script" with lower IO prio. 

personally i would check for the file existence and "case" it based on that file see here for some of the other distribution files linuxmafia.com 

You could move the 2nd partition sdb2 to the end of the "disk" and then recreate the first, by then you will have the ability to recreate the first one with "full" size. However depending on your filesystem you should not need to delete it but rather "only" unmount it, and then do (at least for ext2/3/4) , after doing the above move. The move itself could be done with . For how to do this: refer to following links: Parted user manual gnu.org and linuxquestions.org Something like this might work (you need to change your numbers: 

will prevent the spam on the console until the next reboot. Basically the ce_errors are short for correctable errors (aka no "defect" outside the ram). see kernel docu about edac and edac wiki for more details. While i may be totally wrong we have a server with this (ECC RAM) and as there are no uncorrectable errors and memdisk did not show any issues either i let it run with the same ram, change the outputting, started monitoring the uncorrectable errors and there was no further issue for us. 

What do i miss or where can i check further? Remember: Productive system so no dev, no stacktrace or similiar available or installable. 

gives you the order of vhost "execution" so your example1.com is the first port :443 defined domain then this one will be used. Instead you could either make a f.e. self signed cert and inform the user that this domain does not have a https connection right now or do a rewrite_rule which redirects the traffic from $URL$ to $URL$ Note that this would need an "invalid" cert and will inform the user. If you dont use SNI, you additionally need at least 1 IP per Cert. 

Start the server Press Enter key at boot loader to get to the boot menu Press Space to pause the default boot loading Boot into single user mode (press 4) Define Shell : enter the full pathname for the shell (or press enter for default path /bin/sh) remount root file system (/) rw (read/write) 

check , read every logfile there for similar timings to the closing of the terminals. If you cannot find anything useful or related in there try another terminal solution for example instead of kconsole xterm As an alternative testing increase the buffer size of the terminal itself. (depending which exactly you use might be a bit tricky, but google should help) I had a similar looking issue a few years back with kconsole not showing anymore data while having tail -f running. For me it turned out to be a buffer error. 

after the config change. This seems to be the official solution to upgrade from 12.10 to 13.04 Source: Ubuntu.com check though which version you get for completion to know which version is LTS and which is not: 

reads like either the USB chip itself (if there is a single one) is damaged somehow or there might be a short cicuit in one of the connectors. Maybe you will be able to see a small crack on the chip and or the lines to your ports. you might want to check if you can see any dirt or similiar items/small objects short circuit one of the port. additionally you could debug the usb "hub" which provides these ports, usually called the root hub. for linux you could install and/or mount the usbfs to have debug info, but depending on the time you want to spend for "fixing" this it might be way cheaper to just buy a pci(X) USB card. 

I have an ancient xen 4.0 host system with LVM and only one Volume Group Inside the VG there are a few logical volumes. The Logical Volume i am talking about (slave) is a snapshot of another logical volume (origin master). I did extend The "Slave" logical volume for a few hundred GB, however the guest is unable to see this increase.. fdisk still shows the same guest partition shema as before the increase. The Guests (slave and therefor master) has 3 partitions on the Logical volume.. How would i increase the guest virtual harddrive when i can confirm the logical volume is larger.. (normally with 1 logical volume = 1 partition you would just resize the fs..) But how do i resize the partition table of the guest HDD when it is just the snapshot which i did increase? I can extend or resize master or slave however i want, but i cannot create a new master or make the slave not beeing a snapshot anymore (due to size limitations) 

either do the both other answers (downgrading, or reducing security by disabling the check) Another option would be to actually fix the issue by having correct permissions for the root chroot folder. Qouting a nice blogpost, which Marek already linked 

Please check the permissions of the logfiles and the logdirectory. possible snort is not able to write into that file/directory 

i would not go with /etc/issue as this file is sometimes changed or might not even mention the Version. instead you should in my opinion search for the distribution specific file. For your part: 

basically you could do it with some sort of proxy like Squid f.e. www.tldp.org/HOWTO/TransparentProxy.html As for auditing the time, that might not be that easy reliable. You do not know if the user is still reading the page or minimized the Browser f.e. You could go with DPI and have the router(s) log http/https traffic, but that might be illegal depending in which country you do this. There are some full-service products from Juniper/Cisco etc but i doubt you want to pay 50k $ upwards for the hardware alone on this solution. Other approach would be group policies updates with a blacklist and some software you install on every pc. Sadly i do not know of one (nor had i the need to search for it yet) which does exactly what you want. 

additionally : what do you mean by the usage of "setup an account which works internally" your mailserver seems to not know the account, which leds me believe it is not in the virtual_mailboxes list, though i do not know if you could just access an actual "real" user with your above setup. On mine this does not work "realuser@domain.tld" does not work, where "virtual_user@domain.tld" works fine. responding to your addition of the mail log: the fix should be this (qoute from howtoforge): 

The NAT functionality is (from my knowledge of kernel building) part of the kernel, so unless you are able to rebuild the kernel modules and/or can switch the kernel you most likely will be out of luck. 

Possibly try to fix your DNS entries. ~20sec is pretty close to the DNS timeout in some Operation Systems. So it could be that the first DNS resolver might not be working correctly and after beeing too long "idle" cache could be emptied. 

First: i do not know if there might be a bug/feature limit on your USB host controller or spec limitation (in case of devices, hubs in row) change in USB2 vs USB3 (Davids Answer seems to show the limit of the intel controller) Technically my answer is USB2.0 info i give but it might help you get some ideas where to look at. Make sure you do not use too long cables between host and hubs, low cost ones did produce high amount of issues for us on even 3m length, while USB2.0 specs allow for 5m Cable until you need a "Hub" Measure the power output of the USB cables. Most of the time you get 7 Port hubs there are actually 2 Hubs inside the case row. So you should check which port on your hub is connected to the "primary" and "secondary" hub. Additionally you cannot put more than 5 hubs in row, (including root hub + possible already another hub on your mainboard before you even reach the USB Plug) on our setup we were limited to 3 additional hubs per Port. However even with active power supply on the hubs and good cable connections we had issues if we added more than 1 7er Hubs (=7 devices) per physical mainboard USB Port. Our solution was attaching the server with 5 PCI 4x USB Cards which allowed us to stack 110 devices per server. 

The latest available version for Centos (1.3.3g), as Spectre already said, seems to not work correctly with TLS 1.1/1.2 See the following link for having at least some notes about a potential fix for those issues beeing in Version 1.3.5