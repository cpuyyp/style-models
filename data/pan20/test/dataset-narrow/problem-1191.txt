Following up on my comments, here is the rest of the full answer: First, take a look at the 2008 paper Euclidean Skeletons of Digital Image and Volume Data in Linear Time by the Integer Medial Axis Transform by Hesselink and Roerdink. From my reading of your question, Section 2 of the paper is answering your question fairly explicitly. In particular, the process they use is essentially a scan across grid points. Additionally, I wouldn't worry about the runtime being excessive. The algorithms involved are all linear time, and in fact experimental results from the paper show that the $IMA$ transform can be computed in a matter of seconds on a modern computer, even on data sets in the tens of millions. An especially nice piece of this: It appears $MB$ is computed along the way (again, Section 2), so if you simply stop the algorithm at that point, you should have an even faster runtime than "a matter of seconds" in practice. 

In addition to the above discussion, my motivation is an interest in exploring algorithmic options for LWE (of which we currently have relatively few to choose from). In particular, the only restriction known to provide good algorithms for the problem is related to the magnitude of the error terms. Here, the magnitude remains the same, but the range of error in each equation is now "monotone" in a certain way. (A final comment: I'm unaware of this formulation of the problem appearing in the literature; it appears to be original.) References: [1] Regev, Oded. "On Lattices, Learning with Errors, Random Linear Codes, and Cryptography," in JACM 2009 (originally at STOC 2005) (PDF) [2] Regev, Oded. "The Learning with Errors Problem," invited survey at CCC 2010 (PDF) [3] Arora, Sanjeev and Ge, Rong. "New Algorithms for Learning in Presence of Errors," at ICALP 2011 (PDF) 

The "protocol interference" attack: From Fig 2 of $URL$ -- In Flow3A (resp. 3B), before the intended message is delivered, the adversary manages to obtain an encryption under $K_A$ (resp. $K_B$) of a different session key $\sigma'$ (through interacting with $S$ in a higher-level protocol) and sends this "forged" message to $A$ and $B$. This is a successful man-in-the-middle style attack. Why the security definition should include TEST in the middle of execution: If the interaction doesn't terminate immediately after TEST, then the forged session breaks security in a very obvious way. Namely, with probability 1/2, the TEST operation hands a key to the adversary; it becomes the "higher-level protocol" we're worried about with constant probability. If the adversary is not forced to immediately halt, then it can use this key to perform the man-in-the-middle attack above (at least in the BR95 scheme). The security definition given in BR95 is, therefore, too weak as this is a plausible attack (Why would an adversary arbitrarily halt, especially after they've just extracted useful information?). That is, a protocol vulnerable to this attack is (wrongly) proven secure under the given definition. 

Some experimentation (picking some choices for $C$ at random and also some popular linear codes) suggests that, when $\epsilon < 1/2$, the probability of $E_1$ occurring is at least the probability that $E_2$ occurs, for any $C$. Does anyone have any solid intuition or a proof for why this might be true? Or know of an example where it is false? 

This doesn't seem to be possible, at least not without finding the right notion of "easy". Consider a graph formed by taking a grid graph with $k$ rows and $l+1$ columns, and identifying the leftmost column of vertices with the rightmost column of vertices. Here, we have $n = kl$ vertices, and we're interested in $k \le l$. The graph is planar, and has treewidth at least $k$, as it has a $k\times k$ grid graph as a minor. Additionally, a cyclic separator achieving constant $f$ in this graph seems to need $\Omega(l)$ vertices (where the constant depends on $f$). Imagine the graph drawn on a cylinder of circumference $l$ and height $k$, where the graph forms an equally spaced grid on the surface of the cylinder. Then if a cyclic separator has length less than $l$, it cannot "go around" the cylinder. Hence a cyclic separator gives a way to find a region in a $k\times l$ rectangle whose perimeter is the length of the separator, and whose area is at least a constant ($1-f$) fraction of the whole rectangle's area. I don't know the exact isoperimetric inequality for this, but it seems like such a region would need perimeter $\Omega(l)$ in order to have enough area and fit into the rectangle. Thus we have a graph with treewidth $k$ and whose separators need to be of size $\Omega(l)$. Choosing parameters like $k = n^\epsilon$ and $l = n^{1-\epsilon}$, we thus see that a cyclic $(s,f)$-separator for constant $f$ needs to have $s = \Omega(n^{1-\epsilon})$, even for graphs with treewidth $n^\epsilon$. 

Firstly I claim that condition (iii) doesn't help very much, even when combined with condition (i). In particular, there is a reduction from the problem with (iii) ignored to the original version of the problem: Suppose there is an instance with $f_i$'s that are non-decreasing but otherwise arbitrary. Then we can make a new instance (whose data is denoted with primes (')) which has 

As a final note, while technically implied by $\textsf{NEXP}=\textsf{MA}$, the collapse $\textsf{NEXP}=\textsf{PSPACE}$ has another interesting implication. It's known that $\textsf{PSPACE}$ has a complete language which is both downward self-reducible as well as random self-reducible. Ordinarily, all such languages sit inside $\textsf{PSPACE}$ and so we shouldn't hope to say (unconditionally) that $\textsf{NEXP}$ has such a complete language as long as we hope that $\textsf{NEXP} \ne \textsf{PSPACE}$. However, if $\textsf{NEXP} = \textsf{PSPACE}$, then $\textsf{NEXP}$ does have such complete languages. A similar statement (replacing $\textsf{NEXP}$ by $\textsf{EXP}$) was used by Impagliazzo and Wigderson to conclude a sort of "derandomization dichotomy" for $\textsf{BPP}$ in relation to $\textsf{EXP}$, so it may be useful in discovering other consequences of $\textsf{NEXP} \subseteq \textsf{P}/\textrm{poly}$. 

Background: In Subhash Khot's original UGC paper (PDF), he proves the UG-hardness of deciding whether a given CSP instance with constraints all of the form Not-all-equal(a, b, c) over a ternary alphabet admits an assignment satisfying 1-$\epsilon$ of the constraints or whether there exist no assignments satisying $\frac{8}{9}+\epsilon$ of the constraints, for arbitrarily small $\epsilon > 0$. I'm curious whether this result has been generalized for any combination of $\ell$-ary constraints for $\ell \ge 3$ and variable domains of size $k \ge 3$ where $\ell \ne k \ne 3$. That is, Question: 

As an interesting aside, an exactly equivalent class $PC$, short for "Polynomial-time Check," shows up in Goldreich's complexity textbook. $FNP$ is still correct for this site as we've elected to follow the naming conventions of the Complexity Zoo (referenced in Suresh's answer, and in the FAQ). The completely informal way to define $FNP$ is simply "the class of search analogues of NP problems." As others have mentioned, the $NP$-version of HAMILTONIAN CYCLE is "Does the given graph have a Hamiltonian cycle? (Yes or No)" whereas the $FNP$-version of HAMILTONIAN CYCLE is "In the given graph, what is the Hamiltonian cycle? (Either output such a cycle, or output None exists)". 

What I've written below is wrong. See Andras's response for a correct answer. I'm leaving this post up in the hopes its errors are instructive for others as they were for me. 

1) $x_0$ is (indeed) a plaintext. (Perhaps to guess at where confusion arose from:) the 'more modern' version of homomorphic encryption is fully homomorphic encryption (FHE) fully supporting both additive and multiplicative homomorphisms, whereas the paper you're reading was published about 2.5 years before Gentry's original FHE scheme came out. (Check out the wikipedia article on homomorphic encryption for more examples..) This paper uses additive-homomorphic encryption, which allows the computation $Enc(pk, x_0) + Enc(pk, x_1) = Enc(pk, x_0 + x_1)$. Note that additive-homomorphic encryption schemes tend to support (must.. support?) an additional "scalar-multiplication homomorphism." That is, the computation $$x_0 \cdot Enc(pk, x_1) = Enc(pk, x_0\cdot x_1),$$ where plaintext $x_0$ is public information. (Note that the above is different than FHE, which supports both the prior additive homomorphism, and simultaneously the "full" multiplicative homomorphism of $Enc(pk, x_0) \cdot Enc(pk, x_1) = Enc(pk, x_0\cdot x_1).$) 

A course I'm currently taking from Bill Gasarch is called: Ramsey Theory and Its 'Applications' -- the quotes are in the official course title. Interpret that as you wish. >_> Here is a link to the course website (with lecture notes): $URL$ Here is a link to Bill's "Apps of Ramsey Theory" page: $URL$ The latter link is very likely the most complete collection of information for the question you're asking. 

This idea can in theory be extended to larger $k$, where you encode every function as some $k$ columns in the matrix. I don't see how best to argue this at the moment, though. 

It does look like their proof for the time bound has a minor error, though that part of the theorem statement itself looks right. To adapt their proof, instead of picking $c$ so that for large $n$, $2^{n + b\log(n)} \cdot 2^{(2n + c\log(n))/2 + 1} \le 2^{n + c\log(n)}$, you want to pick $c$ to have $2^{n + b\log(n)} \cdot 2^{2(n/2 + 1) + c\log(n/2+1)} \le 2^{2n + c\log(n)}$ for large $n$. (The "$/2 + 1$" gets pre-composed with $n + c\log(n)$, not post-composed.) This overall gives a time bound of $2^{2n + c\log(n)}$. Of course, only $c = \Omega(\log(n))$ can achieve this, but a choice of $c = O(\log(n))$ does work. This gives the stated time bound of $2^{2n}\cdot n^{O(\log(n))}$. 

It follows that you can enumerate every collection of $1, 2, \ldots, D+1$ points, compute the appropriate disk, check that its radius is at most $R$, and then count the number of points in it. You can compute the disk in time $O(D)$ in each case, and you can naively count the points inside the sphere in $O(n\cdot D)$ time. This gives an overall time of $O(D\cdot n^{D+2})$. If you know some more information about what your input looks like, there are very likely to be some heuristic optimizations you can put on top of this algorithm. A simple example is ignoring any collection containing two points with distance greater than $2\cdot R$. 

Once these are established, we can observe that the orbits of $\Phi$ have size 2 and partition the admissible orientations of $G$. It follows that the number of admissible orientations is even. To define $\Phi$, let $\vec{G}$ be an admissible orientation, and consider breaking $\vec{G}$ into it's strongly connected components. $\Phi$ then sends $\vec{G}$ to the orientation formed by reversing all the edges within the strongly connected components. The properties are then straightforwardly checked: 

Let $S_0=\{1\}$. For $i = 1,...,n$, set $S_i\gets \{xy\mod b : x\in S_{i-1},y\in L_i\}$. Then just check whether $a \in S_n$. It's obviously correct, since $S_i$ contains the residues which can be expressed as a product of elements, one each from $L_1,\ldots,L_i$. Just using the definition of $S_i$ to compute it, you get a running time of $O(m n b)$, which is significantly better than brute force when $b$ is smaller than $n^m$. By replacing the linear sweep by divide and conquer, you might be able to improve the running time even more, though I didn't check. (It will still have a factor of $b$ though.) 

In addition to ArXiv, you could subscribe to the Electronic Colloquium on Computational Complexity. It gets a lot of traffic from current research, and the email updates you receive are normally a succinct format of paper title/author(s)/abstract, so you won't have to invest much time to see what's just happened. 

At SODA 2006, Martin Grohe and D$\acute{\rm a}$niel Marx's paper "Constraint solving via fractional edge covers" (ACM citation) showed that for the class of hypergraphs $H$ with bounded fractional hypertree width, CSP($H$) $\in PTIME$. Definitions, etc. For a great survey of standard tree decompositions and treewidth, see here (Thanks ahead of time, JeffE!). Let $H$ be a hypergraph. Then for a hypergraph $H$ and a mapping $\gamma : E(H) \rightarrow [0,\infty)$, $B(\gamma) = ${$v \in V(H) : \sum_{e \in V(H), v \in e} \gamma(e) \ge 1$}. Additionally, let weight($\gamma$) = $\sum_{e \in E}\gamma(e)$. Then a fractional hypertree decomposition of $H$ is a triple $(T, (B_t)_{t \in V(T)}, (\gamma_t)_{t \in V(T)})$, where: 

If I were going to illustrate unique games, I'd do something with a label-extended graph (similar to the suggestion you mentioned). But specifically, I'd contrast the original constraint graph with the label-extended one. For instance, label edges in the constraint graph with the corresponding equation out of: $x_1 - x_2 = 0\mod 3$ $x_2 - x_3 = 0\mod 3$ $x_3 - x_1 = 1\mod 3$ Then there will be a (hopefully visually appealing) "twist" between the $x_1$ and $x_3$ vertex clouds in the label-extended graph. 

This 2009 survey by Daskalakis surveys the complexity of computing Nash equilibria. His previous work with Goldberg and Papadimitriou demonstrated that exactly computing such equilibria is PPAD-complete. This is not as strong a statement as if the problem were NP-hard, but still provides evidence that computing Nash equilibria is intractable, throwing into doubt the predictive power of Nash equilibria. One salvation would be to demonstrate a PTAS for $\epsilon$-Nash equilibria for desired accuracy $\epsilon$. But the current best is an non-oblivious approximation algorithm that runs in time quasipolynomial in $1/\epsilon$. 

Factor $p(x)$ as $p(x)=\prod_{i=0}^{1000} (x_i-r_i)$. Store this as a table $T$ of distinct roots $r_j$ and their respective multiplicities $m_j$. 

For the "lower bound via upper bound" question you asked: The STOC 2010 paper "How to Compress Interactive Communication" [BBCR10] arrives at an improved direct sum theorem for randomized communication complexity by demonstrating an improved compression protocol for interactive communication. Specifically, given two parties computing some joint function of their mutual inputs (i.e. an interactive computation scenario), they show that any protocol that communicates $C$ bits and reveals $I$ bits of new information to the parties involved can be simulated by a new protocol using $\tilde O(\sqrt{CI})$ bits -- the improved upper bound. As a consequence of this improved protocol compression, they show that in the worst case: Given any function $f$ that takes $n$ time to compute individually, computing $k$ copies of $f$ requires at least $\sqrt{k} \cdot n$ time -- the improved lower bound.