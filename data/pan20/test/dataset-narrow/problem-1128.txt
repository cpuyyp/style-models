All of these are due to containments of the classes on the left-hand sides in $\mathsf{PSPACE}$ (though we also have $\mathsf{BQP \subseteq PP}$). 

Question. In adiabatic evolution, to ensure that the ground state high overlap with the unique ground state of the system (i.e. to achieve arbitrarily small error) using adiabatic theorems, it is crucial that there is always some measurable eigenvalue gap. 

Context. I am writing on topics such as the Gottesman-Knill theorem, using Pauli stabilizer groups, but in the case of d-dimensional qudits — where d may have more than one prime factor. (I emphasize this because the vast majority of literature on stabilizer formalism in "higher dimensions" involves the cases of d prime or d a prime power, and makes use of finite fields; I'm considering instead the cyclic groups ℤd .) For any dimension, I characterize a (Pauli) stabilizer group as an abelian subgroup of the Pauli group, in which every operator has a +1 eigenspace. 

In all of the above, entangled states will sometimes be involved, and one may think of codes such as the Shor 9-qubit code as exploiting entanglement (correlations in multiple bases of measurement) to represent a quantum analogue of a simple repetition code. But in all this, entanglement is an incidental feature of the more fundamental phenomenon of correlations of multi-qubit observables representing excitations (or lack of them) in a physical process. That is, the role of entanglement here is the same as in many-body systems: it just so happens that the ground states of Hamiltonians may be entangled, but the mere presence or absence of entanglement is not what makes the states ground-states. Competing dissipation processes Above, I talk about spontaneous versus stimulated dissipation. However, this is a distinction of engineering, not of physics: the fact that the 'stimulated' process is under our control is not relevant from a physical perspective. Physically, we merely have two interaction/dissipation processes with the computational system: an uncontrolled 'noise' process with a bath, and a controlled dissipation process which we might think of as happening with a cold reservoir (corresponding to the ancillas which we prepare to aid with the error correction process). The error rate per unit time is the interaction of the bath with the computational system; the error rate per gate is the effective three-way interaction of the bath with the reservoir and computational system. The error correction is the interaction of the computational system with the cold reservoir alone. In this picture, what would it look like if we could make the interaction with the reservoir dominate these interactions of the computational system with the bath, or the bath-system-reservoid interaction? It would look as if any interaction of the bath with the system would be more and more efficiently transmitted into the cold reservoir, with the computational system left essentially undisturbed. This is how we can interpret what the threshold theorem tells us — if the interaction of the system with the warm bath is sufficiently low, we can simulate a bath at arbitrarily low temperature by simulating a coupling of the computational system with a cold reservoir, where the transmission of excitations from the system to the reservoir is arbitrarily swift; and furthermore the resources to do so is $\mathrm{poly} \log(\varepsilon)$, where $\varepsilon > 0$ is the factor of suppression of noise in the computational system. 

Short answer. It turns out that suspending the requirement of unitary transformations, and requiring each operation to be invertible, gives rise to exact gap-definable classes. The specific classes in question are $\mathsf{LWPP}$ and a 'new' subclass $\mathsf{LPWPP}$, both of which sit between $\mathsf{SPP}$ and $\mathsf{C_=P}$. These classes have fairly technical definitions, which are briefly described below; though these definitions can now be substituted, in principle, with ones in terms of non-unitary "quantum-like" algorithms. The counting class $\mathsf{SPP}$ contains GRAPH ISOMORPHISM. It also contains the entire class $\mathsf{UP}$, so we would not expect exact unitary quantum algorithms to be as powerful as the non-unitary classes (as we could otherwise show $\mathsf{NP \subseteq BQP}$). Longer answer. 

The resulting reformualtion. From this, asking "does $C$ arise from the Cartesian bitwise join of $A$ and $B$?" is equivalent to asking: 

Such a relation can be easily computed by reduction to computing the transitive reduction of the partial order obtained, as I described, by collapsing equivalence classes (using only one representative node for each equivalence class, and then copying relations across the class). This can be done in time $O(r^3)$ or better. Given such a representation of pre-orders $P$ by reductions $R$, one may decide $(i,j) \in P$ by testing $(i,j)$-connectivity in $R$, e.g. by breadth-first search. This, of course, takes time $O(n)$. However, one can quite efficiently compute the reduction $R_{i,j}$ of the pre-order $P_{i,j}$ for $i<j$, by 

No, there is no decomposition of the entire family $\{F_{2^n}\}_{n\geqslant1}$ into a single finite gate-set. Here's why. The QFTs involve only coefficients over $\overline{\mathbb Q}$, the complex algebraic closure of the rational numbers. In analogy to [Adleman+Demarrais+Huang–1997], if we involved any gates which included any transcendental numbers, we could choose a minimal set of transcendentals $\{\tau_1, \tau_2, \ldots\}$ and describe the gate-coefficients essentially as rational functions $\overline{\mathbb Q}(\tau_1, \tau_2, \ldots)$. To obtain the QFT as a product of such gates, we must arrange for all of the transcendental components to cancel (a similar thing must occur to ensure each of the gates are unitary); but then we might as well replace all of the transcendentals with $0$, so that all of the coefficients are algebraic. So we restrict ourselves to algebraic gate-sets without loss of generality. The coefficients of a finite gate set over $\overline{\mathbb Q}$ can all be contained in a finite-degree extension of $\mathbb Q$, which one may construct by extending $\mathbb Q$ by those very coefficients. However, the gates $\mathrm{CZ}_{2^n}$ obviously have coefficients belonging to field extensions over $\mathbb Q$ of degree $2^{n-1}$, i.e. of unbounded degree. Thus the family of QFTs of order $2^n$ does not decompose into any finite gate-set. As a corollary, we cannot hope to have any algorithms in $\mathsf{EQP}$ which relies on QFTs over cyclic rings of unbounded size — note that the same problem occurs for any family of circuits which might use QFTs of arbitrary order. 

quantum circuits can be simulated efficiently on a classical computer, proving NP ⊆ BQP ⊆ P, thereby surpassing every theorist's wildest dreams or nightmares; quantum circuits can't be simulated on a classical computer, but scalable quantum computers can be built to solve problems in NP, giving rise to truly explosive interest in quantum computing and ensuring that experimental physicists have career security for the forseeable future; there is another model of computation waiting to be discovered, intermediate between P and BQP in power, which describes (or rather, better approximates) what is efficiently physically computable. 

The only time probabilities come into play is with the accepting and rejecting axis. While this model of computation is obviously inspired by finite automata, it is simple not useful to interpret any of the other coefficients of the vector, at any point in time (even at the end!), as being probabilities. Note that the accept and reject probabilities at each timestep are conditional probabilities, i.e. they depend on the computation not having yet halted; if you want to compute the total probability of acceptance/rejection, you can most easily do this by dispensing with the "renormalization" step in the evolution (the part where we multiply by the inverse-square-root value; but still setting the accept/reject coefficients to zero if the computation continues), and simply sum all of the probabilistic contributions to accepting/rejecting that arise along the way. 

It's complicated, and depends on whether you approach quantum computing as a technology or a model of computation; and whether you are interested in universal quantum computation, or a special subclass of quantum operations. As a technology We're still working on how to implement a large-scale quantum computer. Part of the reason for this is that it is a non-trivial engineering problem to devise quantum memories on which we can reliably act. Many of the proposed architectures involve physical qubits which are in a fixed(-ish) position, relative to the others. Whether these are quantum dots (attached to a substrate), ions in a linear trap (mobile but often penned in between two other ions), or even nuclear spins in liquid NMR (locations fixed mostly by the chemical bonds within each molecule), those qubits don't have much freedom to move relative to the ones you want it to interact with. You can't necessarily shuffle them around like cups and balls. The fixed wires in a classical circuit get around this physical location problem by being physically extended. The wire acts as if tracing out the trajectory of "moving" logical bits, and may weave around each other somewhat between two logic gates. When we draw quantum circuit diagrams, we're dreaming of this — but at least one of the dimensions in the diagram represents not space, but time between the physical operations which we perform. Not many physical architectures for quantum information admit that sort of physical extension, while keeping the quantum bit coherent and available to be acted upon. If locations are more or less fixed, this prevents us from literally swapping around the qubits. In summary, in quantum implementation, we can't be confident that SWAP operations will be easily achievable "in hardware", i.e. by swapping around the physical qubits. We thus have to resort to be achieving SWAP operations "in software" — by performing suitable operations to 'transport' the information from one physical qubit to another. Thus our interest in decomposing SWAP in terms of elementary gates. As a model of computation As a part of theory of computation, you are exactly right: decompositions of SWAP are only a curiousity, much like the logical universality of the NAND gate. It is worth taking note of, in elementary courses; but much beyond that we treat decompositions of SWAP almost exactly as seriously as we do the decomposition of classical algorithms into NAND gates — not very seriously, most of the time. So long as we consider a model of computation in which elementary gates have a constant cost and which generate SWAP, then SWAP can be performed with at most a constant cost — and if the cost due to SWAP gates does not dominate the run-time of the algorithm, then talking about them doesn't really shed any insight onto the complexity of a problem. Of course, because of the technological promise of quantum computation, the technical difficulty in implementing quantum computers, not to mention the promise of any technique (including classical ones) to simulate bits of quantum mechanics, specialized and related models of computation have attracted some theoretical interest. One of these are "matchgate" circuits, which may be described as unitary circuits in which 

On top of these classes of states may be a miasma of nearby states which are allowed by the promise, and are very close to states belonging to one of the two classes above; but asymptotically, the classes of states satisfied by the promise would converge to these two classes as n grows. The circuit involved in the decision procedure would then essentially correspond to a circuit which maps some basis of pure states in $\mathcal L$ — together with a basis for the "ortho­comple­men­tary quantum language" (so to speak) $\mathcal L^\bot$ — to the standard basis, and decides which of those com­pu­ta­tional basis states correspond to states in the quantum language. In short, it would solve a classical decision or promise problem, encoded in quantum states, with error converging to zero. 

There is a difficulty with the premise of your question — "when does randomization stops helping within $\mathrm{PSPACE}$ — because it suggests that the computational classes $\mathrm{X}$ such that $\mathrm{P \subseteq X \subseteq PSPACE}$ form some sort of linear hierarchy when this is not evident. We can illustrate this by comparisons between the polynomial hierarchy and counting classes. As Emil Jeřábek indicates in the comments, $$\begin{align*} \mathrm{BP\cdot \Sigma_i^p \subseteq \Pi_{i+1}^p} &&\text{and}&& \mathrm{BP\cdot \Pi_i^p \subseteq \Sigma_{i+1}^p} \end{align*}$$ by relativisation of $\mathrm{AM \subseteq \Pi_2^p}\,$; and therefore $\mathrm{BP \cdot PH = PH}$. On the other hand, Toda's Theorem shows that $$\mathrm{PH \subseteq BP\cdot\oplus P}.$$ If you suppose that "randomization has stopped adding power by the time you ascend to $\mathrm{PH}$", then you will be tempted to suspect that because $\mathrm{PH \subseteq BP \cdot \oplus P}$, perhaps in fact $\mathrm{BP \cdot \oplus P = \oplus P}$. But I don't know that anyone conjectures this, or even that $\mathrm{PH \subseteq \oplus P}$ (which would be a necessary consequence); I think that any result of this sort would be considered a major breakthrough. Of course, if you only care about the polynomial hierarchy, and more generally (to scale up to $\mathrm{PSPACE}$) quantified boolean formulas, then you can extract some sort of linear answer to your question — in which case Emil's comments are about as complete an answer as you are likely to get. 

The article by Shiekh [cs/0507003] describes "adding quantum interference to quantum computers". This ignores the fact that: 

An example would be the computation of ground state energy of the Ising model with transverse magnetic fields, as described by [Cubitt+Montenaro-2013]. From the abstract: 

Using these results, and given a candidate pair of sets $S,T$, determining whether or not they "subtend" a unique path cover in this way can be determined in time $O(k^2 n)$; but finding whether or not such sets of endpoints exist that is the apparent difficulty, and the extremal result above (which is only a necessary condition) seems to represent the state of the art in efficient criteria to determine whether such sets exist. 

In the abstract to their paper Structure and importance of logspace-MOD classes on the classes ModkL, Buntrock, Damm, Hertrampf, and Meinel claim that they "demonstrate their significance by proving that all standard problems of linear algebra over the finite rings $\mathbb Z/k\mathbb Z$ are complete for these classes". On closer inspection, the story is more complicated. For instance, Buntrock et al. show (by a proof-sketch in an earlier and freely accessible draft found by Kaveh, thanks!) that solving systems of linear equations is instead in the complementary class coModkL, for k prime. This class is not known to be equal to ModkL for k composite, but never mind that — what I'm concerned about is the fact that they don't make any remarks about whether solving systems of linear equations mod k is even contained in coModkL for k composite! 

[Edit.] For the sake of tidiness, I have removed the computed example. I suppose that it can be seen by viewing the edit history of this post. 

If A[s] = , skip to step 6. Set A[s] ← , and set an iterator j ← f(s). While A[j] = , set A[j] ← , and set j ← f(j). If A[j] = , we have closed a new cycle; increment c by 1. (If you want to keep a record of some representative of this cycle, the current value of j will do as an arbitrary choice; of course, this won't necessarily be the minimal element in the cycle under your preferred order <.) Otherwise, we have A[j] = , which means that we have discovered a pre-explored orbit which ends in an already counted cycle; do not increment c. To indicate that the orbit starting at s has now been fully explored, set j ← s. While A[j] = , set A[j] ←  and set j ← f(j). Proceed to the next element s ∈ S. 

copying the immediate successors of $j$ and sharing them with $i$, copying the immediate predecessors of $i$ and sharing them with $j$, removing the relation $(i,j)$ from $R$. 

Problem. I actually do have a concise proof of this result already, essentially using no more than orthogonality relations of Pauli operators. But I suspect that I've seen something like it before, and I would like to refer to the prior art if I can (not to mention see if there are better techniques than the one I used, which while not onerous felt less than perfect). Certainly Knill's papers [quant-ph/9608048] and [quant-ph/9608049] consider similar subjects and use similar techniques; but I couldn't find the result I was looking for there, or in Gottesman's [quant-ph/9802007]. I'm hoping that someone can point me to where such a proof might have been published before. Note — the result I'm considering is not one which relates the cardinality of the group to the dimension of the stabilized space (which is nice, but trivial both to prove and to find references to); I'm concerned specifically with showing that any stabilizer group which cannot be extended stabilizes a unique state, and vice versa. A reference to a proof that any maximal stabilizer group has the same cardinality would be fine; but again, it must not rely on d being prime or ℤd2n being a vector space.