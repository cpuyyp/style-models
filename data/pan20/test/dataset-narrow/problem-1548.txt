I've recently been looking at autoencoders and kernel PCA for unsupervised feature extraction. Lets consider just for a moment linear PCA. Its my understanding that if a autoencoder (with a single hidden layer) has linear activation functions then the sum-of-squares error function (chosen for convenience rather than statistical relevance) will have a unique solution for the weights. This can be easily seen, consider the sum-of-squares error function for the autoencoder input $\boldsymbol{x}$ and the autoencoder output $\hat{\boldsymbol{x}}$, that we wish to solve: $$ \begin{aligned} & \min_{\boldsymbol{W},\boldsymbol{V}} {1\over 2N}\sum^N_{n=1} \|\boldsymbol{x}^{(n)} - \hat{\boldsymbol{x}}^{(n)}\|^2\\ \end{aligned} $$ An autoencoder is a neural network whose input is its output. Defining the following: Let $\boldsymbol{W}$ be the weights matrix between the input layer and the hidden layer and $\boldsymbol{V}$ be the weights matrix between the hidden layer and the output layer. I'm ignoring the bias terms for now. Let $\psi$ be the activation function for each unit on the hidden layer and let $\phi$ be the activation function for each unit on the output layer. Then we have two mappings in an autoencoder: $$ \boldsymbol{z} = \psi\left(\boldsymbol{W}\boldsymbol{x}\right), \:\:\:\:\:\:\: \hat{\boldsymbol{x}} = \phi\left(\boldsymbol{V}\boldsymbol{z}\right) $$ $$ \begin{aligned} & \min_{\boldsymbol{W},\boldsymbol{V}} {1\over 2N}\sum^N_{n=1} \|\boldsymbol{x}^{(n)} - \phi\left(\boldsymbol{V}\psi\left(\boldsymbol{W}\boldsymbol{x}^{(n)}\right)\right)\|^2\\ \end{aligned} \label{eqn: 1} $$ If $\psi$ and $\phi$ are linear, i.e $\phi(\boldsymbol{s})=\boldsymbol{s}$ and $\psi(\boldsymbol{s})=\boldsymbol{s}$, then $\hat{\boldsymbol{x}} = \boldsymbol{V}\boldsymbol{W} \boldsymbol{x} = \boldsymbol{A} \boldsymbol{x}$ (i.e rotation) and $$ \begin{aligned} & \min_{\boldsymbol{W},\boldsymbol{V}} {1\over 2N}\sum^N_{n=1} \|\boldsymbol{x}^{(n)} - \boldsymbol{V}\boldsymbol{W}\boldsymbol{x}^{(n)}\|^2\\ & \min_{\boldsymbol{W},\boldsymbol{V}} {1\over 2N}\sum^N_{n=1} \|\boldsymbol{x}^{(n)} - \boldsymbol{A}\boldsymbol{x}^{(n)}\|^2\\ \end{aligned} $$ This is recognised as principle component analysis. This solution for the weights "forms a basis set which spans the [linear] principle component space" (Bishop, Pattern Recognition and Machine Learning). Bishop then goes on to say that these weight vectors aren't necessarily orthogonal or normalised. I assume that this is because we have not proposed any constraints on $\boldsymbol{A}$ as we would have done in PCA by way of proposing an orthogonal basis? So am I right in thinking that the autoencoder I've described above does not in general give the linear PCA solution, but in some very special cases they may match? Now bishop goes on to say that the only way to handle non-linear inputs is to add more hidden layers to the autoencoder and have some layers use non-linear activation functions. However one would assume that the same problem as above persists? i.e. that one is not guaranteed to get the same solution as PCA. There seem to be additional issues [Bishop]: 

Therefore my question is, why use autoencoders at all for unsupervised feature extraction? Why not just use kernal PCA? Which will reduce to linear PCA when the data permits it. Is there some other advantage that I'm missing? Under what conditions should an autoencoder be chosen over kernel PCA? 

Therefore the first principle component contains all the information and these three features could be characterised by a single latent variable. So why is this relevant? Well in your data set your eigenvalues are all of similar magnitude. This suggests that the features in your original data are not very correlated (like the first example above, look at the off diagonal elements your original correlation matrix they are likely close to zero) and therefore it is expect that many principle components would be required since there is little shared information in the original feature space. This of course assume that we only have a small number of original features. If you have 50 million features then capturing 62.3% in only ten latent variables is pretty good, 

Training requires solving a non-linear optimisation problem. One may end up with a solution that corresponds to a local minima, not a global one. One must specify the number of subspace dimensions as part of the network architecture. 

You don't state how many original features there are? where there 10, 50, 50 million? The above 10 principle components capture 62.3% of the variance and so we can be sure that there are more that ten original features. However we will assume then that there aren't many more (lets say circa twenty), lets consider what is going on. With PCA you are hoping to separate the useful information from the noise. By taking only the the first two components in you example you are discarding all other information as noise. It depends upon the application but I would say that 25% is never enough (I think serious questions would be raised as why you had). So the question becomes then what is going on? Well lets consider the two extreme cases: A set of uncorrelated features and a set of fully correlated features. Consider first three uncorrelated features: 

I have a python script written with Spark Context and I want to run it. I tried to integrate IPython with Spark, but I could not do that. So, I tried to set the spark path [ Installation folder/bin ] as an environment variable and called spark-submit command in the cmd prompt. I believe that it is finding the spark context, but it produces a really big error. Can someone please help me with this issue? Environment variable path: C:/Users/Name/Spark-1.4;C:/Users/Name/Spark-1.4/bin After that, in cmd prompt: spark-submit script.py 

I have an excel file that contains details related to determining the quality of a wine and I want to implement the linear model concept using the function sklearn.linear_model.SGDClassifier(SVM => Hinge loss) and (Logarithmic regression =>log loss) using python. I learned the basics about these function through the scikit learn website and I am not able to implement the model using excel file. I am very new to python and machine learning and I finding it hard to implement the model. I opened the excel file in python and tried to take two columns [randomly] from the file and use that as an input to call the fit function available in the model. But, I got an error stating Unknown label type: array. I tried a couple of other methods too, but, nothing worked. Can someone guide me with the implementation process? 

I am using Ipython notebook to work with pyspark applications. I have a CSV file with lots of categorical columns to determine whether the income falls under or over the 50k range. I would like to perform a classification algorithm taking all the inputs to determine the income range. I need to build a dictionary of variables to mapped variables and use a map function to map the variables to numbers for processing. Essentially, I would my dataset to be in a numerical format so that I can work on implementing the models. In the data set, there are categorical columns like education, marital status, working class etc. Can someone tell me how to convert them into numerical columns in pyspark? 

I am trying to do this in R. I tried the below function, but my R session is not producing any result and it is terminating. 

Could someone help me in achieving this output? I think this can be achieved using dplyr function, but I am struck inbetween. 

Here I need to group by countries and then for each country, I need to calculate loan percentage by gender in new columns, so that new columns will have male percentage of total loan amount for that country and female percentage of total loan amount for that country. I need to do two group_by function, first to group all countries together and after that group genders to calculate loan percent. 

I have a SQLContext data frame derived from pandas data frame consisting of several numerical columns. I want to perform multivariate statistical analysis using the pyspark.mllib.stats package. The statistics function expects a RDD of vectors. I could not convert this data frame into RDD of vectors. Is there a way to convert the data frame? Code: 

I have a dataframe with columns as defined below. I have provided one set of example, similar to this I have many countries with loan amount and gender variables 

Finally, I resolved the issue. I had to set the pyspark location in PATH variable and py4j-0.8.2.1-src.zip location in PYTHONPATH variable. 

This gives me the values the node "a" and random numbers for all the node "b". I don't know where I am going wrong. I thought it has something to do with the way the data has been set up in my file. So I created a file for each column of my main file and wrote the create statements to get the nodes. I still don't get the actual values of the related nodes. 

I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the captions in the properties section. Sample Create statement: 

Since our cross-validation scheme was resistant to over-fitting, the training and test accuracy are close on the practice set, and our practice training accuracy is close to our test training accuracy. Now we can safely assume that accuracy on the unseen test set for the real data is about worse than training accuracy. However, imagine you a single validation set or no validation at all on the practice data. Now your model is more likely to overfit and you might see results like this: 

The simulation starting at (4) can be run once or many times saving only the best result. At the end you should arrive at close to or exactly the best possible seating arrangement. 

Unfortunately none of these three variables can go directly into linear regression. looks like a numerical variable, but it is actually categorical. For example is probably orthogonal to , and should not be interpreted as twice as significant. The correct way to handle this is to create a boolean dummy indicator variable for each possible site code. You can also use this method for some of your other variables which also appear to be categorical. and will not work well in a linear regression because their relationship is highly non-linear. For example two points can have the same latitude/longitude but be very far apart. One typical method is to convert pairs into predefined zones, and treat the zone as a categorical variable. The final type of variable you have is a . You could convert this directly to a categorical variable, but it might be better to take only the month to reduce the number of categories and generalize seasonal effects better. 

Here we have created a model with high variance by overfitting. Training and test accuracy are far apart on the practice data, and training accuracy on the practice data does not match well with training accuracy on the real data. It is not as easy to estimate test accuracy now, because we can't really say if we have overfit by the same amount on both datasets and the second set is harder, or if we have overfit the second set by less. In the first case we might predict test accuracy, and in the second case we might predict . 

The CNN might perform better since your data is inherently spatial. However you have to decide what to do if two or more points overlap. The simplest solution is to pick one randomly, which might be OK depending on your specific task. With a Recurrent Neural Network: 

These are in comparison to a simpler model like a 1D conv net, for example. The first three items are because LSTMs have more parameters. 

You can replace with and use . This will work if you don't already have rows with entries that you want to keep. 

For multiclass classification where you want to assign one class from multiple possibilities you can use : 

Seperate 12-convolutional-block policy and value networks (as in AlphaGo) A single 12-convolutional-block network with dual policy and value outputs Separate 20-residual-block networks A single 20-residual-block network with dual policy and value outputs A single 40-residual-block network with dual policy and value outputs 

Compute mean and variance on entire data set (train + test) and use these to standardize each set. Compute mean and variance on train set and use these to standardize each set. 

Encode each as the number of times it appears in the training set Encode each as the mean of the target value in the training set. See "Target-based encoding" here. If the target is categorical you can use a vector of frequencies for each target category, eg `[0.4, 0.1, 0.3, 0.2]. 

Contrary to what others are suggesting, trying to extract data on square footage and number of rooms from apartment ads is not a problem for machine learning - especially not when you don't have any training data. You will spend 10 times as long trying to build a machine learning system for this task than you would manually extracting the data yourself. If you really want an automated system, consider building some simple regex rules based on ads you have seen. There are only so many ways people describe these features, and you should be able to get most of them when they are present in the ad. You won't get 100% accuracy, but you're not going to do any better another way, in my opinion. You can start with some simple rules, and improve them as you look at more and more ads. In Python, for example: