The Android file system is case sensitive. Check that the filenames and paths match what the data is looking for. It's possible that the Models directory is lower-case on Android and therefore doesn't match the upper-case M in Model for example. 

The documentation on MSDN is pretty terrible around this, but if you read the words around the related method Mouse.SetPosition you'll notice it says "Sets the position of the mouse cursor relative to the upper-left corner of the window." It stands to reason then, that the MouseState.X and Y are also relative to the upper-left corner of the window. 

I suspect your issue lies in the way a sprite batch works in MonoGame. The performance cost is coming from using different textures for each tile. Let's take a peek into the MonoGame source code and see what's going on. If you follow the code down through SpriteBatch.End you eventually end up in the SpriteBatcher.cs class around about here: $URL$ 

The question explicitly asks how to write Kinect games for the 360. My understanding is that XNA on the 360 doesn't support the Kinect. It seems you can possibly use it on Windows, but as I've never tried I can't confirm (but a quick internet search seems to confirm you can). Unless you're an authorised 360 developer (think have a publisher that's an approved XBox 360 developer or some sort of track record making games that would help get your foot in the door talking to MS) you're not going to get access to the full blown XDK that allows C++ based development and access to the Kinect device. There are no other alternatives for 360 development. 

I've not touched DX11, but in the shader you seem to expect sampler 0 to be the diffuse, with sampler 1 to be the normal map. When you set the textures on the C++ side, you seem to have the slots reversed from what I can see. 

The downside is the extra effort required to setup a content project and manage the process of building your optimized XNB files. Here's a quote taken from the link in your question for clarity. 

The idea here is very similar, except that you can create more than one instance of it and run them in a loop. One difference with this implementation though is that it returns false after the action has fired rather than repeating the action over and over. I'm sure you could make the modifications to make it repeat if so desired. 

As mentioned previously, you may need to scale your input to screen coordinates to and back again. Using a projection is technically the correct way to do it, but ultimately what you are really doing is just scaling your screen size by some ratio. Generally for 2D games you will work with a "virtual screen size" and scale everything based on that. For example, lets say you make your virtual screen size 800x480 (typical Android screen). You can create a scale vector like this: 

There are lots of factors to take into account, type of game, speed of connection, number of peers, how much data you want to send, if people have limited bandwidth allowances you don't want to over use, is this over a LAN or internet, etc. Lets say for example, you have a 64kbs (64 kilobits per second or 8 kilobytes per second) connection (which is slow by modern standards for average home internet but not necessarily slow for phones), with 8 players and you want to update as often as possible for a real-time game, you'll need to send to 7 people at most 9kbs each (64/7). Assuming your update packet size was 1kb (1024 bits or 128 bytes) for each update, you'd only want to send an update roughly 8 times per second, or put another way for a game running at 60fps, every 7 or so frames. You'll need to do your own calculations based on what your target bandwidth is, packet size, etc. There's nothing standard that will scale this for you, however it's fairly easy to keep track of when the last update was done and not send another for X frames or X milliseconds, etc. When calculating the size of your packets, you'll also need to take into account packet header sizes which will potentially vary by the platform you use and apply and a specific overhead to sending anything (which for UDP over IPv4 seems to be 20 bytes). UPDATED due to extra info (and switching TCP reference above for what it should have been, UDP): You'll want to be using UDP instead of TCP as the overall overhead of TCP is more than you want in a real-time game. You don't need/want the guaranteed delivery of packets as it'll require each packet to return a successfully delivered reply (plus by the time it's then re-sent the data is out of date anyway). For the internet you'll need to assume at least a certain level of packet loss and how you cope with this (you can even simulate this in your code by randomly not sending or not sending every Xth packet). You'd also using UDP need to take into account that packets can arrive in a different order to that being sent, so you'd also probably want to send some sort of number (be it time, or some incrementing value) to indicate that if you receive packet 200 first, then that's the current good state and then receiving packet 199 it should be ignored. You could also potentially do other network optimisations, such as sending a less frequent "big" update (say position, orientation, velocity, etc) with more information, then send more "smaller" partial updates (such as controller input, or delta's). Smaller packets allow more to be sent for the same bandwidth (again you need to take into account packet header overhead, you want to have a good ratio of actual data sent to header size). This mostly applies to peer-2-peer over the internet. If this is just for a local area network (unclear from the question), then it probably isn't a concern on how often you send the data unless the actual sending or receiving the data is somehow computationally expensive (not usually). You'd most likely want to send it as fast as you can in a LAN environment for responsiveness. 

There are some workarounds to this problem without needing to completely reinvent the wheel. For example, you should be able to load any file from any location using the TitleContainer.OpenStream method. Then, once you have the stream you can convert it into a Texture2D.FromStream method. Of course, you'll need to do your own research to figure out how to load each of the different content types and some of them may not be supported on all platforms. Perhaps something like this (untested): 

It's worth noting that I didn't align the red lines with the exact edges where the black meets the blue. You could do this if you want, but it's not strictly necessary. In many cases the lines can be pretty rough estimates and it'll still work just fine. Obviously, the goal is the be able to make the source image bigger (wider and taller) without stretching the borders. The 9 patch method does a pretty good job, but just keep in mind that the middle (4) and the sides (1,3,5,7) will still stretch. Depending on your texture, the effect might be great, or it could still look stretched. You sometimes have to fiddle with the numbers to get it to look okay. When you render the box, typically the corners (0,2,6,8) won't stretch, the top/bottom (1,7) will stretch horizontally, left/right (3,5) will stretch vertically and the middle (4) will stretch in both directions. There's a couple of ways to think about these 9 patches. One way is to think about them as rectangles, which they are, but another way is to just think about the distance between the lines and the edge of the image. That's what I've done in the following algorithm from MonoGame.Exteneded. 

You'll need to be an "Xbox 360 Registered Developer". It's not as simple as just wanting to do it. There's more information on the Xbox 360 Registered Developer page on Xbox.com. I believe XNA doesn't support Kinect for Xbox 360, you'd need to gain access to a proper devkit and access to the libraries available only to registered developers. If you have an idea you want to pursue, you might find it easier to find an existing approved publisher and get them to help develop your project. 

Clear() is the only way. To understand why, you need to consider that Direct3D is an abstraction layer itself over the underlying hardware to present a way for you to code against them all. If you dig a little deeper behind the scenes on depth in particular, you start to discover that each manufacturer is free to implement (and therefore optimise) depth as they choose, as long as they adhere to the interface presented. The link here describes a little about some of the different techniques that have been used, it seems to date from 2002 so things will have advanced since then, but it gives you an idea of why they hide the underlying implementation from you. The Wikipedia link on HyperZ here also talks a little about some of the optimisation methods used and the efficiencies they get from them. 

I noticed this question has been around for a while unanswered so I thought I'd put some information here for future reference. It's not a complete answer, but it might help the next person. Is seems there are a number of open issues with lots of discussion around this issue and why it's difficult to implement. $URL$ $URL$ $URL$ The primary issue is that OpenGL ES 2.0 does not support retrieving data from textures. However, it looks like there have been plenty of attempts to solve it and it may be solved now for some platforms. Not sure. 

Applying a matrix to your SpriteBatch transforms the entire draw call at once. This means you don't need to use your camera in your DrawTiles method at all. It could become a lot simpler like so: 

I've recently been implemented something like this in the MonoGame.Extended library. Creating a letterbox / pillarbox viewport in MonoGame is actually kinda tricky. I'll do my best to provide a good answer here. There's basically 3 parts to it. The first is creating a scaling matrix to pass into the call. The variables represent your actual resolution vs virtual resolution. For example, you code the game against a virtual resolution of 800x480 but render it on an actual resolution of 1024x768. This code will stretch the image to fit. 

You seem to be trying to implement the Painters Algorithm. I'm guessing you're trying to write a rasteriser from scratch as a learning exercise, as most modern 3D hardware uses what Bart has mentioned (the Z/Depth buffer). For the painters algorithm to work in all cases, you'd need to be prepared to subdivide the surfaces as they're rendered to solve possible scenarios (such as the overlapping polygon problem shown on the Wikipedia page). By rendering from furthest to closest you're also spending time rendering pixels which will possibly later be occluded by other polygons, which when you start putting textures and complex shaders on the polygons wastes precious cycles. This is the reason modern hardware would prefer you to render from front to back, using the depth buffer to determine if the pixel to be rendered is further away than the one on the screen (and can therefore be discarded). Even with most modern acceleration hardware you'll still need to sort and render from back to front any semi-transparent polygons, rendering this only once all the opaque polygons have been rendered. 

As you can see custom content importers can be quite daunting at first, but once you've done a few of them they are not too bad. I hope this clears some things up for you. Finally, to answer your other question. 

That's a good question. Unfortunately, I haven't got an ideal solution, but I do have a similar setup to you with a PCL and an Android project. As you can see, I have the content in the PCL set to "Content", "Copy if newer". In the Android project I have added the same files under the Asset/Content folder as links to the original files and set their properties to "AndroidAsset". It works just fine, but it does mean I have to keep add content to both projects which is slightly annoying. 

Overall, my suggestion is to keep it simple. Get it working the simplest way fist and build on it. Try not to write highly optimized code until you've got the basics working first. You may find that rendering every tile every frame is not so bad. EDIT: For second part of question. While it's true that you want to keep your batch count low, having 2 or 3 shouldn't be a problem at all. So if you have a good reason to create a second sprite batch just do it. That said, there's probably not a good reason to use a second sprite batch in this case. More likely, you want to draw your player in the exact same way you draw tiles in the same sprite batch with the camera transform applied. It's a little hard to tell why your player is getting left behind without looking at some code but it stands to reason that if you draw your player at the exact same position as a tile, he / she will appear at the same position with the same sprite batch. For example, if you want the player to appear on tile 10, 10 you could do this: 

I'm sure there are circumstances where people have done as you have above, but personally I've always favoured engines as a collection of libraries, with bits you can pull in as needed. That would ultimately give you more flexibility of choice to swap different components in/out based on their suitability to a specific type of game (i.e. you could choose a 2D or 3D physics library or renderer depending on the type of game) without having to have all the other alternatives resident in memory, or you could choose between a rendered based on DX9/DX11/etc). 

The problems that used to require hand rolled assembly are getting fewer in number. What you "might" gain in speed you lose in readability and the ability to debug. It should also be done only as one of the very last optimisation steps on sections of code as in most cases speed problems aren't something that can't be made better with assembly. These days CPU's have gotten much faster while memory speeds have not, often it's more important to control how data flows through the CPU than anything else. With modern compilers they also find it hard to optimise around assembly code as they have to deal with whatever registers you've touched and they can't usually re-order instructions in your hand crafted code. To reduce the need for assembly, there is also now intrinsics which help to get access to low level concepts, but in a way that is compiler friendly and allow them to work with you rather than against. With that said, the SPU on the PS3 is one area where people are still having to use assembly to get the most out of the processor, with manual instruction pipelining for example as explained here.