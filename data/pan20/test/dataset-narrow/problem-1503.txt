Unless you have some very specific or exotic requirements, in order to perform logistic (logit and probit) regression analysis in , you can use standard (built-in and loaded by default) package. In particular, you can use function, as shown in the following nice tutorials from UCLA: logit in R tutorial and probit in R tutorial. If you are interested in multinomial logistic regression, this UCLA tutorial might be helpful (you can use or packages, such as or ). For the above-mentioned very specific or exotic requirements, many other R packages are available, for example ($URL$ or ($URL$ I also recommend another nice tutorial on GLMs from Princeton University (by Germán Rodríguez), which discusses some modeling aspects, not addressed in the UCLA materials, in particular updating models and model selection. 

There is an enormous amount of information on approaches, guidelines and procedures for performing EDA. Potential starting points might include EDA page on the NIST's Engineering Statistics Handbook website, EDA pages on the EPA's website, corresponding chapter from the book "Experimental Design for Behavioral and Social Sciences" and a survey research paper on EDA by Begrens (1997), among many others. It is interesting to note that some sources include less traditional methods into EDA toolset, such as dimensionality reduction and clustering (for example, see the description of this research seminar). While some of the EDA approaches and methods are relatively simple, overall EDA is both art and science, as it combines unstructured (creative) and structured approaches. This aspect is especially important to recognize, as big data exponentially increases complexity of data analyses, including EDA. References Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131-160. Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley. NOTES: For those interested in the Tukey's classic, it is available on Amazon. Various MOOCs on EDA are also available, for example this one and this one (both are R-focused). 

As far as I know, currently there not that many projects and products that allow to perform serious machine learning (ML) work from within Excel. However, the situation seems to be changing rapidly due to active Microsoft's efforts in popularizing its ML cloud platform Azure ML (along with ML Studio). The recent acquisition of R-focused company Revolution Analytics by Microsoft (which appears to me as more of acqui-hiring to a large extent) is an example of the company's aggressive data science market strategy. In regard to ML toolkits for Excel, as a confirmation that we should expect most Excel-enabled ML projects and products to be Azure ML-focused, consider the following two projects (the latter is an open source): 

I believe that this is a case for applying time series analysis, in particular time series forecasting ($URL$ Consider the following resources on time series regression: 

Separately, I'd like to mention two open source big data analysis and visualization projects, focused on graph/network data (with some support for streaming data of that type): Cytoscape and Gephi. If you are interested in some other, more specific (maps support, etc.) or commercial (basic free tiers), projects and products, please see this awesome compilation, which I thoroughly curated to come up with the main list above and analyzed: $URL$ Finally, as I promised in the beginning, Zoomdata - a commercial product, which I thought you might want to take a look at: $URL$ The reason I made an exclusion for it from my open source software compilation is due to its built-in support for big data platforms. In particular, Zoomdata provides data connectors for Cloudera Impala, Amazon Redshift, MongoDB, Spark and Hadoop, plus search engines, major database engines and streaming data. Disclaimer: I have no affiliation with Zoomdata whatsoever - I was just impressed by their range of connectivity options (which might cost you dearly, but that's another aspect of this topic's analysis). 

First of all, the fact that you have known some Java, even ten years ago, already means that you don't "know nothing about programming" (I suggest you update the title of your question to reflect that - change "nothing" to "a little"). I'd like to make several points, which I hope will be useful to you. 

While I don't have enough expertise to advise you on selection of the best similarity measure, I've seen a number of them in various papers. The following collection of research papers hopefully will be useful to you in determining the optimal measure for your research. Please note that I intentionally included papers, using both frequentist and Bayesian approaches to hierarchical classification, including class information, for the sake of more comprehensive coverage. Frequentist approach: 

You need to analyze sentence structure and extract corresponding syntactic categories of interest (in this case, I think it would be noun phrase, which is a phrasal category). For details, see corresponding Wikipedia article and "Analyzing Sentence Structure" chapter of NLTK book. In regard to available software tools for implementing the above-mentioned approach and beyond, I would suggest to consider either NLTK (if you prefer Python), or StanfordNLP software (if you prefer Java). For many other NLP frameworks, libraries and programming various languages support, see corresponding (NLP) sections in this excellent curated list. 

If you are interested in a very high-level (enterprise architecture) framework, I suggest you to take a look at the MIKE2.0 Methodology. Being an information management framework, MIKE2.0 has, certainly, much wider coverage than the domain of your interest, but it is a solid, interesting and open (licensed under the Creative Commons Attribution License) framework. A better fit for your focus is the Extract, transform, load (ETL) framework, which is extremely popular in contexts of Business Intelligence and Data Warehousing. On a more practical note, you might want to check my answer on Quora on open source master data management (MDM) solutions. Pay attention to the Talend solutions (disclaimer: I am not affiliated with this or any company), which cover a wide spectrum of MDM, ETL and data integration domains as open source and commercial offerings. 

You can find useful this blog post by Sami Badawi. However, note that the post is not recent, so some information might be outdated. Plus, the post contains an initial review, which might be not very accurate or comprehensive. If you're thinking about data science, while considering staying within Microsoft ecosystem, I suggest you to take a look at Microsoft's own machine learning platform Azure ML. This blog post presents a brief comparison of (early) Azure ML and SSAS. 

I will try to answer your questions, but before I'd like to note that using term "large dataset" is misleading, as "large" is a relative concept. You have to provide more details. If you're dealing with bid data, then this fact will most likely affect selection of preferred tools, approaches and algorithms for your data analysis. I hope that the following thoughts of mine on data analysis address your sub-questions. Please note that the numbering of my points does not match the numbering of your sub-questions. However, I believe that it better reflects general data analysis workflow, at least, how I understand it. 1) Firstly, I think that you need to have at least some kind of conceptual model in mind (or, better, on paper). This model should guide you in your exploratory data analysis (EDA). A presence of a dependent variable (DV) in the model means that in your machine learning (ML) phase later in the analysis you will deal with so called supervised ML, as opposed to unsupervised ML in the absence of an identified DV. 2) Secondly, EDA is a crucial part. IMHO, EDA should include multiple iterations of producing descriptive statistics and data visualization, as you refine your understanding about the data. Not only this phase will give you valuable insights about your datasets, but it will feed your next important phase - data cleaning and transformation. Just throwing your raw data into a statistical software package won't give much - for any valid statistical analysis, data should be clean, correct and consistent. This is often the most time- and effort-consuming, but absolutely necessary part. For more details on this topic, read these nice papers: $URL$ (by Hadley Wickham) and $URL$ (by Edwin de Jonge and Mark van der Loo). 3) Now, as you're hopefully done with EDA as well as data cleaning and transformation, your ready to start some more statistically-involved phases. One of such phases is exploratory factor analysis (EFA), which will allow you to extract the underlying structure of your data. For datasets with large number of variables, the positive side effect of EFA is dimensionality reduction. And, while in that sense EFA is similar to principal components analysis (PCA) and other dimensionality reduction approaches, I think that EFA is more important as it allows to refine your conceptual model of the phenomena that your data "describe", thus making sense of your datasets. Of course, in addition to EFA, you can/should perform regression analysis as well as apply machine learning techniques, based on your findings in previous phases. Finally, a note on software tools. In my opinion, current state of statistical software packages is at such point that practically any major software packages have comparable offerings feature-wise. If you study or work in an organization that have certain policies and preferences in term of software tools, then you are constrained by them. However, if that is not the case, I would heartily recommend open source statistical software, based on your comfort with its specific programming language, learning curve and your career perspectives. My current platform of choice is R Project, which offers mature, powerful, flexible, extensive and open statistical software, along with amazing ecosystem of packages, experts and enthusiasts. Other nice choices include Python, Julia and specific open source software for processing big data, such as Hadoop, Spark, NoSQL databases, WEKA. For more examples of open source software for data mining, which include general and specific statistical and ML software, see this section of a Wikipedia page: $URL$ UPDATE: Forgot to mention Rattle ($URL$ which is also a very popular open source R-oriented GUI software for data mining. 

Any platform, focused on social networking (not necessarily Twitter), at its core uses the most appropriate and natural abstract data type (ADT) for such domain - a graph data structure. If you use Python, you can check nice NetworkX package, used for "the creation, manipulation, and study of the structure, dynamics, and functions of complex networks". Of course, there are many other software tools for various programming languages for building, using and analyzing network structures. You might also find useful the relevant book "Social Network Analysis for Startups: Finding connections on the social web", which provides a nice introduction into the social network analysis (SNA) and uses the above-mentioned NetworkX software for SNA examples. P.S. I have no affiliation whatsoever with NetworkX open source project or the book's authors. 

My limited understanding, based on brief browsing GitHub API documentation, is that currently there is NO single API request that supports all your listed criteria at once. However, I think that you could use the following sequence in order to achieve the goal from your example (at least, I would use this approach): 1) Request information on all public repositories (API returns summary representations only): $URL$ 2) Loop through the list of all public repositories retrieved in step 1, requesting individual resources, and save it as new (detailed) list (this returns detailed representations, in other words, all attributes): $URL$ 3) Loop through the detailed list of all repositories, filtering corresponding fields by your criteria. For your example request, you'd be interested in the following attributes of the parent object: stargazers_count, forks_count. In order to filter the repositories by number of committers, you could use a separate API: $URL$ Updates or comments from people more familiar with GitHub API are welcome!