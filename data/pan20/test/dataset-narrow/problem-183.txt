If you have a column that preserves order, here is how you could do it in Oracle. You will have to translate to SQL Server. 

The wait event “log buffer space” according to the documentation can indicate that the log buffer is too small or the I/O is slow. We have a system seeing this event in which the redo buffer allocation retries is going up in V$SYSSTAT as observed with... 

After creating this I noticed that the index statistics were different for each run even though nothing should be changing in the table and index statistics are re-gathered on each run. My theory now is that something on the index statistics is retained when gathering table statistics with the cascade option even when index statistics are re-gathered. Granularity is set to AUTO and Cascade is set to AUTO_CASCADE. 

I say all this not to say that the concept of an SQL subtype is bad, just that it is not as useful as it may seem. 

What should happen? Should the update fail on the local database but succeed on the replicated database or should it not run on the remote database because it failed on the local database? If you don't want changes by batchjobaccount to replicate, then why give them permission to make the changes on the local database? If they need a local playground, then can you just make a local copy of the tables and only give batchjobaccount access to those? 

Speed - Your own checks will never be as fast as database internal checks. Completeness- There is always the possibility when you roll your own that you will miss something. 

Just so there is some sort of answer to this, here is some information I have found. On MOS there is a document called "High Numbers of 'Asynch Descriptor Resize' Wait Events Seen" (Doc ID 1273748.1) This seems to indicate that the problem is really another wait event slowing the system down and in turn causing these, so they aren't really the source wait event. Oracle-L has a thread on the wait event, but no mention of it on Windows. OakTable has some interesting info on the problem, and says, 

As an example of the lack of distinction, the Oracle 11.2 Concepts Guide includes SELECTS as DML as follows: 

I don't expect the writer of the question meant it this way, but words mean things and the question specifies a "database trigger", therefore there is no correct answer. 

If you have a fully licensed server with a virtualized database running on it, you might want to consider adding a second virtual machine to the same server to run your test system on. It would save you from having to purchase an additional license if the resources can accommodate it. 

If you just want the data in the database so you can display it in entirety much like you would a picture stored in the database, then a clob for each set of tabular data would work fine. If parts of the data will change, if you want to be able to write queries against parts of the data, if you want to join data from different tabular sets or with other data already in the database, and if you want these things to be fast and scalable, then you will need to do some design. Figure out what data needs to be in what tables and how it will relate. Normalize the data for smaller more manageable chunks. This means there could be multiple tables for each tabular set you have. 

What are the benefits? - Faster data access with less disk access. How will the client side cache be kept in sync? From the Oracle Call Interface Programmer's Guide: 

While there is no privilege you can revoke, and I'd wonder why you can't just create another user, the answer to your question is.... Yes If you have the Enterprise Edition of Oracle you can use the Virtual Private Database feature to prevent tables from being selected. Here is an overview from the Oracle Database Security Guide 11g Release 2: 

Datapump can do this without a dump file assuming you have a database link to the system you are importing from. Oracle Enterprise Manager has a GUI for this or you can just use the commands directly. It will look something like this: 

When you say Oracle 10g Lite I assume you mean Express Edition. I recommend you read through the documentation, particularly the section on Connecting to the Database. If you have a more specific question after reading that, we would be glad to help. 

I'm guessing you're getting the unique constraint violation on the deleted_orders table because you are running this multiple times for the same ONO. To prevent subsequent calls using the same ONO from getting a unique constraint violation, you can either modify the select query to not find them like this: 

Your question is too broad for any answer here to fully address, so I'll just address one point. For the major query type of "Give me the latest data generated by a device", you could create a new column in the table that gets the same data as the "data generation time" column and then have a periodic procedure that nulls older values in that column for each device keeping only the last N. A function based index could then be created that returns the device column where data exists in that row and null when that column is null. The query could then use the same function as that index and would be able to lookup a small set of data in the index for the device and retrieve it quickly. 

Here is the solution I am using for now. I don't like it, but until I find something better, it's the best I can do. 

ANSI is a private non-profit organization that creates voluntary standards. As such it doesn’t actually regulate anything. Often it is to a company’s benefit to follow recognized standards, which is why many database companies follow the ANSI standard for SQL. Of course as each company seeks to differentiate their products, they will develop additional functionality beyond the standards. From w3schools: 

They don't want to write or forget in the future to write this, so they come up with the solution of making all NULLS -5000. Magically their original query handles NULLs without any changes. What they don't realize is that now someone who wants to exclude these values has to write this: 

Jeff's advice is all good +1, but I would take it a few steps further. The and the can be combined into one statement and that statement can be moved out of the trigger into the code block (probably should be a seperate procedure) that marks an orderline with 'Y'. Something like this: 

In this situation a malicious individual could alter there sessions date format in such a way as to give them access to data that they would not normally have access to. 

Your design is reasonable and may be preferable depending on how many and how large the columns in SpecialEvent will be. If they are small and few then they might as well be in the Event table itself (as Phil said). Such a design would be simpler and not sufficiently slower to merit concern. The development time you save will likely be better served optimizing something that true is slow. You should not create a separate table for each special event. This would add repetition, overhead, and a general sense that design is an afterthought not a planned activity. 

Read the entire note for the full context, but it seems that Oracle is beginning to make allowances for even RAC on VMWare. Application Support If your applications are all RAC aware and can gracefully handle the loss of an instance, then some of the benefits that VMWare add would not be as worthwhile. On the other hand, if the applications are not RAC aware and will need to re-connect anyway, then the transparency of VMWare VMotion would be more desirable. Complexity/Flexibility The combination of RAC and virtualization allows for a lot of flexibility, but also increases complexity considerably. To restart a server you could VMotion all the instances off of that node to other nodes using VMWare, or you could bring the instances on that box down using RAC. Each option has pros and cons that you would have to consider. Thoughts 

FrusteratedWithFormsDesigner has the right direction(+1). Here is what I think you are looking for specifically. 

From the error message perspective, the salient part to pull out is "ORA-01100: database already mounted". This indicates that a database has already been created and mounted. It may also be open. Here is an example reproducing your issue, opening the database (which may not be necessary if it is already open) and creating a django user. 

The strange thing is that the package on A is valid and can be recompiled successfully, yet recompiling the package on B is still unsuccessful. It is only when we make an inconsequential change to the package on A that the package on B will then compile successfully. This problem has been occurring every few weeks for the past several months. It may have started when we upgraded B from 11.1 to 11.2. The database link on B is a private database link using a specific username/password to connect to A. The remote_dependencies_mode parameter is set to SIGNATURE on both databases. This problem is similar, but not the same as MOS Bug 9719541, and Doc ID 166680.1. Anyone have any idea what could be wrong? 

Here was my solution to the problem which in my opinion isn't as nice as using our custom aggregate function which already exists. 

We had an OEM job that would run a PL/SQL block using a particular database users credentials. The credentials are the same for two different databases, so there was no problem for the block to use a private database link without credentials to pull data from the second database into the first. We migrated the job to DBMS_SCHEDULER, but the job fails with the following error: 

I am on a 64 bit box using the 32 bit 11.2 client to connect to an 64 bit 11.2 database. Logging in without the wallet works fine. The error message indicates that I should turn tracing on. I have done so, but don't see anything obvious. Does anyone have a suggestion before I contact Oracle support? 

D. Move each table and rebuild the indexes and statistics. E. Repeat step A. I just built most of these queries, so you will want to thoroughly test them before use. I suppose you could create a procedure that would use to create the actual statements to run dynamically, but because queries will receive ORA-08103: Object no longer exists while the move is in progress, I think it is best to control that process manually even if it does mean a bit more time/effort.