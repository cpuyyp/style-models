(1) Are there any issues with the underlying logic of my proposal? (2) Obviously nested for loops are inelegant - Could one do this recursively and if so, how? (3) What other approaches should be considered? 

For starters you should look into the work of Li and Liu who have a number of papers exploring the topic of OCC and PU-learning. 

Use the spy technique to generate "reliable" negative instances that can be placed in your test set; If historical campaign data is available you could identify those individuals who were targeted multiple times for campaign membership, but who rejected it every time -- these could be "consistently negative" instances; If your known membership pool consists of members of varying value to the business, you could divide members into good/bad, and use the bad instances as a proxy for NEG instances. 

The quick (and not very satisfying) answer is "it depends" -- specifically it depends upon what your underlying conceptual model of human emotion is and how it manifests in verbal/written behavior. What is your characterization of neutrality in relation to positive and negative valence? Can documents be put on some sort of quantitative scale with neutral sandwiched between positive and negative? This position has linguistic support at least with simple phrases which express single valence states (the solution is {great > good > acceptable > poor > horrible}). As @dmb poster suggested, if this is your conceptual model, then you might reasonablly argue that neutral falls between pos/neg and all you have to do is determine the optimal boundaries/cutoff. Thus, you can assign neutral to a test case even if the classifier wasn't trained on neutral cases (though you do need some way of determining these cutoffs). But what about more complex cases? What happens when you move from the level of the phrasal unit to larger sentential and discourse level units? How would you rate the sentence "I love apples but hate bananas"? Do the positive and negative elements cancel each other out to create a neutral? You can easily see how muddied this gets when we start to talk about real human texts. In my opinion, I don't think it is particularly meaningful to talk about a document-level emotion score. Rather, I believe people express emotion/sentiment that is directed at an individual objects "I hate bananas" -> hate(subj, obj) that get combined into discourse-level constructions. So, yes, I think you could reasonable defend the use of outputting a neutral category if only pos/neg are used to train... BUT you will need to justify your use of particular cutoffs as well as consider how to deal with longer documents that express multiple (conflicting) emotions. My preferred course of action would be to have people read and label tweets as pos, neg, neutral, and mixed (then ensure inter-coder reliability) to create my training data... let the algorithm do the hard work of finding the cutoff values. 

One final area you may want to explore is evaluation in collaborative filtering research. Collaborative filtering on unary responses (e.g., Facebook "like") 

Signs are the visual equivalent of "words", and just as words can be decomposed into smaller parts, so can signs be broken down into smaller meaningful/useful units. For example, in American Sign Language (ASL), signs are decomposed into parameters such as handshape, orientation, movement, location on the body. Each parameter can take on a finite set of values, for example handshapes can be closed fist, open hand, cocked index, etc. I would expect a neural network would need to learn these parameters and their values, while learning to ignore distractor and nonsense configurations. Perhaps initial training includes learning to differentiate between signs and non-signs. And just as there are different spoken language, there are different sign languages as well. Moreover, there exist distinct sign language dialects like Black ASL that will need to be taken into account by your system. 

Sounds like you are set to do collaborative filtering - you have readers (users) and documents (items) creating a unary-response matrix with 1 cells indicating the reader (row) read the given document (col). There are various types of recommender systems (e.g., user-based and item-based collaborative filters) and several of them can account for the meta-data associated with your documents (e.g., content-based systems). You should look into R package {recommenderlab} 

Why don't you empirically derive happiness based upon a sampling of the population of your texts (i.e. facebook comments). You can recruit reviewers (either "naive" participants or perhaps experts in psychology/emotion depending upon your needs). Give reviewers some sort of scale to rate texts on - either classify texts as positive, negative, or neutral OR have them rate texts on a likert type scale, e.g., on a scale of 1 to 10, with 10 being max, how happy do you think this text is?). By having multiple reviewers you can set thresholds for reliability thresholds (e.g., 2 out of 3 reviewers must agree on assignment/score; 2 reviewers come to consensus on disagreement, etc.). This data then becomes your labelled training data that you can use to score your remaining data. 

Perhaps this field has developed some novel approaches to OCC evaluation that you can leverage. Without knowing the details of your particular case, however, its hard to provide a more detailed solution. Please update this thread with your eventual solution -- there is so much more research that needs to be done in this particular problem domain. 

If you assume duck to mean "irrelevant decorative elements" there are a few things that strike me as likely: (1) the "squares" only roughly indicate location of the target area; (2) squares represent volume, which is incongruent when overlaid on 2d geography; (3) shading of mountains/geographic features doesn't add detail. Also problematic: comparison of h20 volume applied to crop types is difficult to discern using grids. Quick, which crop color uses more water?? 

Another popular technique for detecting partial string matches (though typically at the document-level) is shingling. In essence it is a moving-window approach that extracts out a set of n-grams for the target word/doc and compares them to the sets of n-grams for other words/docs via the Jaccard coefficient. Manning and colleagues (2008) discuss near duplicates and shingling in the context of informational retrieval. 

The OP reports that when a series of one-vs-rest classifiers are chained together in an ensemble from most accurate to least, the overall predictive accuracy of the ensemble decreases compared to the unchained version. This makes perfect sense. Imagine a simpler case of 3 classes of data, A, B, & C that are used to build the chain you describe: AvsBC, BvAC, and CvAB. Let's assume the order described is in most-to-least accurate. Now run a single instance x through this chain. Suppose classifier AvsBC assigns x a posterior probability Pr(A) = 0.51. Under this result the ensemble would presumably stop, and never explore the other options, and thus might miss out on higher posterior probability assignments (e.g., under BvAC you might get Pr(B) = 0.60). Only by running your instances through all classifiers can you maximize your accuracy. 

Besides doing initial EDA and plotting dependent vs independent variables, it would seem prudent to try to first identify the "maintenance" points. According to your description of the problem, we can expect to see a decline in output o punctuated by single spike-step indicating improved performance. In other words you should in general see a negative slope interspersed with very high positive slope. We see a similar type of spike behavior in EEG signal, where the "blips" or "spikes" correspond to unwanted artifacts (e.g., eye blinks). These can be identified relatively easily by running a sliding window over the data and looking for big changes in slope. The time segments marked in green below are identified as artifacts by monitoring for changes in slope. You can look into EEGlab and ERPlab -- which are Matlab based -- to find out more about this approach. 

The approach I was considering was to pre-compute impossible string onsets in the target language. For example, in English, the onset sequences ^ck and ^ng are not permissable because they violate the orthographic and phonotactic constraints of the language. Given an off-line list of impossible_sequences at run time you would incrementally build possible "words" from the target. During each iteration the possible word is checked against the impossible_sequences list and if it matches the search down that search branch is terminated. The following pseudo-code in R uses loops to illustrate my idea. 

During NLP and text analytics, several varieties of features can be extracted from a document of words to use for predictive modeling. These include the following. ngrams Take a random sample of words from words.txt. For each word in sample, extract every possible bi-gram of letters. For example, the word strength consists of these bi-grams: {st, tr, re, en, ng, gt, th}. Group by bi-gram and compute the frequency of each bi-gram in your corpus. Now do the same thing for tri-grams, ... all the way up to n-grams. At this point you have a rough idea of the frequency distribution of how Roman letters combine to create English words. ngram + word boundaries To do a proper analysis you should probably create tags to indicate n-grams at the start and end of a word, (dog -> {^d, do, og, g^}) - this would allow you to capture phonological/orthographic constraints that might otherwise be missed (e.g., the sequence ng can never occur at the beginning of a native English word, thus the sequence ^ng is not permissible - one of the reasons why Vietnamese names like Nguyá»…n are hard to pronounce for English speakers). Call this collection of grams the word_set. If you reverse sort by frequency, your most frequent grams will be at the top of the list -- these will reflect the most common sequences across English words. Below I show some (ugly) code using package {ngram} to extract the letter ngrams from words then compute the gram frequencies: 

It depends what you want to do. If you want a model that will predict who to target/not-target for a marketing campaign, then you want to train a model using only the people who were marketed to, which defines your two classes "responders" and "nonresponders". Given an unseen customer, your classifier will then determine whether they are likely to respond if you send them a directed marketing advertisement. Take a look at uplift marketing. 

I am interested in proposed base R/Python solutions (i.e. without relying on external packages/libraries) to the following problem: 

Your program will just take an incoming sequence of characters as input, break it into grams as previously discussed and compare to list of top grams. Obviously you will have to reduce your top n picks to fit the program size requirement. consonants & vowels Another possible feature or approach would be to look at consonant vowel sequences. Basically convert all words in consonant vowel strings (e.g., pancake -> CVCCVCV) and follow the same strategy previously discussed. This program could probably be much smaller but it would suffer from accuracy because it abstracts phones into high-order units. nchar Another useful feature will be string length, as the possibility for legitimate English words decreases as the number of characters increases.