Unfortunately, won't enable you to write outside of the rasterized triangle's screen-space bounds. All it enables is to modify the values written to the depth buffer within the triangle, replacing the triangle's true depth with a shader-calculated value. What you're seeing in the screenshot in the Unity forum thread you linked to is two intersecting parallax-mapped surfaces, where the depth output enables occlusion between the two surfaces to be correctly resolved in the screen-space region they both cover. But if you look at the right side of the screenshot, you can see the straight-line edge of the bottom brick polygon. However! You might still be able to accomplish the goal of getting correct silhouette edges by using in the fragment shader when the parallax ray gets outside the bounds of the original polygon. This would involve modifying the ray-march code to check if the march has crossed a triangle edge, and exit with if so, rather than continuing to march. You'd have to find a way to get the information about triangle edges into the fragment shader, though, as this isn't provided by OpenGL. Maybe make your vertex/index buffers available via SSBOs; then you can use in the fragment shader to index into them and lookup the vertex data. Unfortunately I don't know offhand of any implementation of something like this, although I'm sure someone out there must have tried it before. 

The effect in the photo is very close to a simple scale-bias per pixel. After a bit of tweaking, I found that applying the transformation: $x' = 0.77x + 38$ to the raw pixel values (as bytes) gives something quite close to your output: 

For learning how to implement many basic image processing operations, I've found these Image Processing Operator Worksheets from the University of Edinburgh helpful. 

The initial text needs to be rendered at a much higher resolution than the resulting SDF, so you can get subpixel precision in the distances stored in the SDF. For example, in the original Valve paper, they render the initial mono bitmap at 4096×4096 in order to generate a 64×64 SDF from it. 

In DX12, a descriptor is a small record, basically a pointer, that tells the GPU where to find some data such as a constant buffer. Since each object is going to have its own constant buffer data with its own particular transforms, lighting/material properties, etc., each object also has to end up with a separate set of descriptors to point to its individual data. There are a couple of ways to set up descriptors in DX12. First, you can add commands in a command list that update descriptors directly in the root table. For example, you can use to set up descriptors for an object's constant buffers, then draw the object. Repeat for multiple objects. These descriptors will be saved in the command list, and will get applied in-order as the command list is executed on the GPU. The other way to do it is use descriptor heaps. A descriptor heap is basically a buffer that stores an array of descriptors. With this approach, the command list doesn't save descriptors for you; you have to manage them yourself. If you overwrite any descriptors before the GPU consumes them, then you'll get wrong results. This sounds like what's happening in your program: you're likely overwriting the same descriptor in the heap every time you issue an object's draw commands on the CPU, so by the time the GPU executes the command list, only the last descriptor written is still there, and that descriptor gets applied to all of the objects. To fix this, you have to allocate enough space in the descriptor heap to store all the descriptors you're going to use for the frame. Write all the objects' descriptors to the heap. Then when drawing each object, point the shader at the that object's data using with an offset handle that points to the spot in the heap where that object's descriptors live. As long as you're still just trying to learn and get a simple app running, I would probably stick with the root table approach as much as possible. For a more serious high-performance app, there are probably optimizations you can do with descriptor heaps, such as keeping all the constants and textures for a given material together so you can reuse them across many objects and bind them all in one call. You'd probably also want to avoid having a constant buffer and descriptor heap per object, but rather aggregate many objects' data into a few large buffers; this will be more efficient when you have lots of objects. It's also worth noting that a serious 3D app will want to have multiple frames' command lists in flight at the same time, to obtain CPU/GPU parallelism, and this will require constant buffers and descriptor heaps to be lengthened to store multiple frames' worth of data. Again, this is something you can gloss over for learning/bringup purposes by just building a single command list at a time, submitting it, and waiting for it to finish before going on to the next frame. 

The near and far planes of a viewing frustum aren't needed for simple 3D→2D projection. What the near and far planes actually do, in a typical rasterizer setup, is define the range of values for the depth buffer. Depths in the [near, far] range will be mapped into [0, 1] to be stored in the depth buffer. However, the depths aren't simply linearly rescaled. Instead, we use a hyperbolic mapping for depth: the value stored in the depth buffer is the reciprocal of the actual depth distance, with a scale and offset applied. 

You're getting a cross shape in the output because in your loop, you're counting pixels on the center row and column multiple times. For example the pixel at (0, 0) offset will be counted on every iteration of the inner loop, so 100 times. Similarly, (&pm;i, 0) and (0, &pm;i) will be counted on every iteration of the inner loop, so 10 times for each value of i. A better way to write the loop is like this: 

While this MSDN page does claim that SV_RenderTargetArrayIndex can be written in a pixel shader, I believe this is incorrect. Viewport array index and RT array index values are both intended to be output by the geometry shader stage. They can then be read by the pixel shader (and have a constant value per-primitive, based on the GS output). However, it is not possible to set these values from the pixel shader. The stereo instancing approach detailed in those slides is interesting precisely because it avoids using slow geometry shaders. However, if you're not using GS, you can't use multiple viewports or render target arrays. That's why the stereo instancing approach uses one large viewport and bakes each eye's viewport transform into its projection matrix, and requires clip planes. Clip planes should work, so if you're having problems with them, you could post another question detailing the specific issues (preferably with screenshots) and we can try to figure that out. (For completeness, note that if you're using OpenGL, the GL_AMD_vertex_shader_viewport_index extension is available on recent GPUs from all three IHVs and allows setting the viewport index from the vertex shader.) 

I don't think you can simply use when the matrix you're transforming by involves a projection matrix. To fully project to screen space, you have to divide by W, which doesn't do (it simply multiplies by the matrix without translation). Also, because of the divide by W, transforming vectors (as opposed to points) by a projection matrix is probably not what you actually want. If what you want is the screen-space vector between two points, you should transform and divide-by-W each point, then subtract them to get a screen-space vector. Otherwise the vector will likely be inaccurate. 

When I needed an estimate of mesh curvature for a skin shader, the algorithm I ended up settling on was this: First, I computed a scalar curvature for each edge in the mesh. If the edge has positions $p_1, p_2$ and normals $n_1, n_2$, then I estimated its curvature as: $$\text{curvature} = \frac{(n_2 - n_1) \cdot (p_2 - p_1)}{|p_2 - p_1|^2}$$ This calculates the difference in normals, projected along the edge, as a fraction of the length of the edge. (See below for how I came up with this formula.) Then, for each vertex I looked at the curvatures of all the edges touching it. In my case, I just wanted a scalar estimate of "average curvature", so I ended up taking the geometric mean of the absolute values of all the edge curvatures at each vertex. For your case, you might find the minimum and maximum curvatures, and take those edges to be the principal curvature directions (maybe orthonormalizing them with the vertex normal). That's a bit rough, but it might give you a good enough result for what you want to do. 

There are a couple of special cases where mirror-like reflections can be rendered efficiently using rasterization techniques, and these are commonly used in games, although they don't work for the general case. Planar reflections If the reflecting surface is flat or reasonably close to flat, the reflected image can be rasterized in an separate rendering pass, by applying a reflection transform and clip plane to the scene. The reflection is then composited into the final image using a shader (or, in older games, stencil buffer masking). Also, a small amount of distortion or blurring might be applied to the image if the reflecting surface isn't intended to appear perfectly flat and smooth. This is very commonly used in games for water reflection in lakes or the ocean, as long as any waves aren't too large. This page from the UE4 docs shows a number of examples of planar reflections used for water. The same technique is also often used for mirror reflections in glossy floors, or for actual mirrors. The main strength of planar reflection is that it automatically gives you correct perspective and occlusion in the reflected image. The main limitation, aside from being limited to planar surfaces, is the performance cost of rendering an extra copy of the scene (with full lighting, etc) for each reflection plane. For instance, having multiple water surfaces at different elevations (e.g. in a boat lock) would require another rendering pass for each elevation. For this reason, usually when planar reflections are used, they're set up in such a way that only one reflection plane is visible at a time. Cubemap reflections Another special case is when the reflecting object is reasonably small and, ideally, not too close to other objects around it. In that case, you can approximate reflections by rasterizing a cubemap around the object. Then it can be used as an environment map during shading, by sampling the cubemap along the reflection vector calculated in the shader. Many games use pre-rendered cubemaps to render static environment reflections on characters and dynamic objects; somewhat less commonly, you can also dynamically re-render a cubemap every frame to get dynamic reflections of other objects. This might be used on the cars in a racing game, for instance. The advantage of cubemap reflections is that they can be applied to any shape of object, not just planar ones. However, since the cubemap is rendered from a single point, the reflections have increasingly incorrect perspective and occlusion the farther you get from that point. This can be partly addressed by parallax correction, and by blending between multiple cubemaps based on location, but ultimately cubemaps just don't give you the accuracy of either planar reflections or raytracing. Moreover, re-rendering a dynamic cubemap every frame is quite expensive, as it requires 6 extra rendering passes (one for each face of the cube). Hybrid techniques There are also cases where rasterization is used to build an initial data structure for ray tracing / casting to traverse into. 

The higher-resolution mips (toward the left) are used for highly polished surfaces where you need a detailed reflected image. Toward the right, the lower-res mip levels are increasingly blurred, and are used for reflections from rougher surfaces. In a shader, when sampling this cubemap, you can calculate your requested mip level based on the material roughness, and take advantage of the trilinear filtering hardware. Both of these types of light probes are used in real-time graphics to approximate indirect lighting. While direct lighting can be calculated in real-time (or at least approximated well for area lights), indirect lighting is usually still baked in an offline preprocess due to its complexity and computational overhead. Traditionally, the result of the baking process would be lightmaps, but lightmaps only work for diffuse lighting on static geometry, and they take up a lot of memory besides. Baking a bunch of SH light probes (you can afford a lot of them because they're very compact), plus a sparser sprinkling of cubemap light probes, allows you to get decent diffuse and specular indirect lighting on both static and dynamic objects. They're a popular option in games today. 

RGB8 format and similar are often not supported by GPUs (or only supported for a subset of GPU operations) because they don't have a power-of-two number of bytes per pixel. RGB8 would be 24 bits or 3 bytes per pixel. All the formats in that list have 1, 2, 4, 8, or 16 bytes per pixel. Using a power of two simplifies addressing calculations in the hardware, because it can be implemented using a shift instead of a full multiply. (A shift by a fixed value such as 2 bits can even be implemented in hardware without any actual "calculations" at all, just by routing wires.) It also means that pixels are always nicely aligned with respect to boundaries of memory pages, cache lines, etc., which no doubt simplifies many things in the hardware. 

I don't think this kind of under-the-hood data transfer should happen in low-level APIs like Vulkan. The GPU is capable of reading directly from host memory, via the PCIe interconnect. So when you render from a host memory buffer, it should dynamically "pull" the vertices referenced by the draw call. I'm not sure exactly how host coherence works with this, but my guess is the CPU's memory controller is probably smart enough to snoop PCIe read requests, and do the right thing with regard to the CPU caches. If you wanted to transfer the data into GPU memory first, I think you would have to implement that yourself, by allocating some device-local memory and doing a copy into it. (For a vertex buffer that's pretty much consumed linearly once per frame, there's probably no benefit to doing that, but it could be worthwhile for a buffer that's updated more rarely, or that has a more random access pattern.) 

Real GPUs, like CPUs, have a cache hierarchy. So the rasterizer doesn't directly store to memory, but stores into a cache that eventually gets flushed to memory. Rasterization will usually have some spatial locality (it will generate writes to nearby pixels close together in time), so if those pixels are in the same cache line it will reduce the overall number of memory transactions. This is more effective for filled polygon rasterization than for lines, but even lines have some locality (especially if multiple nearby lines are being rasterized at once). Another trick is to store the framebuffer in a tiled format instead of linearly left-to-right and top-to-bottom. You'd pick a tile size such as 4×4 or 8×8 and store the pixels in each tile in a contiguous memory block, then make the whole framebuffer out of an array of tiles. This gives you locality in both dimensions, not just one, which improves cache performance. And tiling is easy to do in hardware, as it comes down to just swizzling the order of some X and Y address bits. 

Graphviz is a popular tool for making network diagrams. It takes a text format ("DOT language") describing the nodes and edges as input, and can render the resulting diagram to SVG or other image formats. 

My advice would actually be to stick with solution 1 unless and until something more is needed. It's the simplest and easiest to understand, and it should be quite feasible to make it perform well. On the one hand, it may seem wasteful to re-filter the points and re-upload a bunch of data to the VBO every frame. I guess that this is why you say it is a "bad practice". But consider the total amount of data involved. An order-of-magnitude estimate shows that processing this amount of data every frame is not much work for a modern computer. The struct you quote looks like it's about 20 bytes (assuming that is 3 floats and is one float or int). With 50,000 points, the total size of the data is about 1 MB. Even if your actual data struct is a bit larger, still the total size of the data is only a few MB—a small amount by today's standards. The time needed to process this amount of data on the CPU, and transfer it to the GPU, is on the order of ~0.1 ms. (Assuming ~20 GB/sec CPU bandwidth, and ~8 GB/sec PCIe transfer bandwidth.) So, this is an entirely reasonable amount of data to re-process each frame, and in fact you could handle many times more without stressing your system at all. So, just keep things simple and efficient for memory access: 

It's not. The BRDF in itself does not approximate the integral over all the hemisphere. The rendering equation does that: you integrate over all incoming light directions, but each time the BRDF inside the integral is evaluated, it's for one specific choice of incoming and outgoing ray directions. For microfacet BRDFs, the usual simplifying assumption is that individual microfacets are perfect specular reflectors. Then, given the $L$ and $V$ at which to evaluate, the only microfacets that can contribute are those that are aligned along $H = \text{normalize}(L+V)$, because that's the only way they can reflect light from the incoming to the outgoing ray. The normal distribution function and the visibility factor in the BRDF together approximate the density of microfacets oriented along $H$ that are visible from both the $L$ and $V$ directions. The Fresnel factor is evaluated for those microfacets, so the correct angle to use is the one between $L$ and $H$, or equivalently $V$ and $H$. There are a couple cases where this argument gets modified. One is if the microfacet model assumes something other than perfect specularity. For instance, the Oren-Nayar BRDF assumes Lambertian microfacets. In this case the BRDF has to incorporate some kind of integral over all the possible microfacet orientations that can scatter light from $L$ to $V$. Then the BRDF won't have a standard Fresnel factor at all; it'll have some other formula that approximates the result of integrating the Fresnel factor over the normal hemisphere. The other case that comes up in real-time graphics is the reflection from an environment map. To be really correct, we should integrate the environment map multiplied by the BRDF over all incoming light directions, but in practice we often sample a prefiltered environment map using the dominant reflection vector $R = \text{reflect}(V, N)$ and then multiply it by some approximate Fresnel formula that depends on the angle between $R$ and $N$ (equivalently $V$ and $N$), as well as the surface roughness. This is very much an approximation, but often good enough for real-time use.