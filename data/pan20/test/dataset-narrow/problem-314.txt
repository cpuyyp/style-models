A commit is a user initiated action that tells the database (Oracle in this case) that the transaction is completed and that the changes may be committed and any locks/resources released. Normally the changes are committed to in memory data buffers and to the redo log buffer. A checkpoint though is a database initiated action that writes all of the data to the actual physical disk file based on the changes recorded in the redo log buffer. Some good articles can be found here and here. 

The use of Save Transaction provides you with a mechanism for rolling back portions of a transaction. For example SP A starts a transaction which then calls SP B. At the start of SP B, a Save Transaction Start Processing could be created. If an error then occurs in SP B you could just rollback the change in the SP B allowing the changes in SP A to be committed. 

Start at the YQL Console In the top left hand corner under Datatables ensure you tick the Show Community Tables In the search box type Finance. This should display all of the Finance tables (approx 31) In the "Your YQL Statement" window type your YQL statment. In order to get the average daily volume for Yahoo you could use the following YQL: 

You can reduce the log generated by temporarily switching to BULK_LOGGED while your index maintenance runs, but you lose the ability to do point in time recovery for that period of time. Apparently it also does not reduce your log backup. --More info on log usage from DBCC SQLPERF (logspace) and SELECT LogRecordsGenerated = COUNT(*) FROM sys.fn_dblog(NULL, NULL) WHERE AllocUnitName = 'dbo.Logging.IX_LOGGING' to show actual log sizes, note the index is only 648KB This is the log after populating it with data: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 70.89956 0 LogRecordsGenerated 10903 Here is the log after a log backup: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 5.100313 0 LogRecordsGenerated 0 Log after log backup and then Online rebuild: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 22.642 0 LogRecordsGenerated 11160 Log after log backup and then offline rebuild: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 10.79338 0 LogRecordsGenerated 140 

Maybe I am missing something, but you realize that start off with 8 columns, do an insert then alter the table and add a 9th column and then do another insert. Of course the size is going to be different when the first insert was into 8 columns and the second was into 9 columns. 

The Connection timeout is always set by client. The timeout is normally configured when the connection is created. In order to prevent recompilation of client code the timeout interval is often stored in a config file, registry entry etc You cannot configure the timeout on the database itself. 

Once you have encrypted the data using an AES key the data is encrypted (the point of using AES). The only way to change the key would be to decrypt the data with the old AES key and then re-encrypt the data using a new ASE key. Some good MySQL AES documentation. 

When the SQL is run the SP is created and stored in SQL Server. Once the SP is created the Grant execute statement is executed. When the SP is executed only the contents of the SP are run. The Grant execute statement is NOT stored with the SP in SQL Sever. This can be verified in a number of ways 

As you are looking for the Maximum value for Sequence for each Booking_ID you could write some SQL similar to that below: 

The Blocked process report (which you have) can be a little confusing at first. There are a few important sections Blocking-Process This is the process(SP, query etc) that is causing the blocking. The process is using a resource that is required by the Blocked-Process Blocked-Process This is the process(SP, query etc) that is blocked and is waiting on a a resource that is in use by the Blocking-Process. waitresource This details the resource that the process is waiting on. Taking your example of waitresource="OBJECT: 99:774293818:0 " The 99 refers to the Database ID of your data and the 774293818 refers to the table. Using the ID 774293818 and systables you should be able to find the table that is causing the blocking. I would also set up an Extended Events Trace to capture the parameters of the offending blocking process and review the execution plans. A good starting point is $URL$ 

Is it acceptable to have downtime on this database? This was probably either restored from a replicated database or it was possibly a subscriber that was improperly removed, though that is unlikely. You could try doing a backup from express and restoring to a standard or higher edition then setting up replication again and removing it. Then you can backup from standard and restore to express. As long as you don't enable any features on the database while at the higher edition, there shouldn't be a problem downgrading. You can test this out in advance of an actual outage to ensure it will remove the status and script it all out to minimize downtime. If you don't have another server you can use, grab the evaluation copy and install on your local machine, a VM, the original machine if it is acceptable, or anywhere you can find. You have limited options with express as you have observed. 

Do you know what pages are getting the latches? DBCC heavily uses tempdb as @ShawnMelton was alluding. Have you considered that tempdb contention could be the source of your pagelatches? You should be able to investigate that with sp_whoisactive if you aren't already. How many data files do you have for tempdb? It sounds like you've only got one, try bumping it up to 4 and increase in intervals of 4 as needed, though 8 is often enough for most systems and many suggest starting with that number of data files. You'll also want to ensure the files are equally sized. You can also look at this post which has lots of info and links to reduce the impact of dbcc including a post from Paul Randall on running DBCC on a VLDB which might be helpful. 

SQL SERVER SCOPE_IDENTITY/IDENT_CURRENT/@@IDENTITY. While SCOPE_IDENTITY/IDENT_CURRENT/@@IDENTITY perform similar functionality the values returned from each differs depending on the current scope such as a stored procedure, trigger, batch each. The documentation should be read and understood before determining which one to utilise. CREATE TABLE Test( TestID INT IDENTITY(1,1), TestName VARCHAR(20) NOT NULL ) INSERT TEST VALUES('Test1','Test2','Test3'); SELECT @@IDENTITY,SCOPE_IDENTIY(); In this example both @@IDENTITY,SCOPE_IDENTIY() will return 3 as they are in the same scope. Depending on scope though they may return different values. SQL SERVER Output allows you to access all of the rows returned from an INSERT/DELETE/UPDATE or Merge statement. You may need to use a variable to access the identity. The links provide more comprehensive information. 

From reading the YQL Documentation and viewing the data sources it appears that while the yahoo.finance.quote does provide data on all the stock data, running a query on a large data set make take some time and the data may be returned in multiple pages which means that your application will have to handle the pages. It also appears that the YQL data requires a where clause in order to run so you would need to know the symbols for all of the stocks. There is a YQL Console which allows you to access the YQL data and run queries, view the output etc. I would suggest reading the YQL documentation and running some sample queries in the YQL Console to see if the data you require is available. To access the YQL Console you can