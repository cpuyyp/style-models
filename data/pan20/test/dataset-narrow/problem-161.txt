fails on these three requirements and is therefore not a suitable tool. A quick search did not revealed any relevant alternative, so I went the Scapy route (Scapy is a Python library and interactive tool allowing to freely build and manipulate network packets). Here is the code I used to successfully test CAM table overflow in a GNS3 environment: 

On incoming packets, if and only if the source MAC address is not present in the table will the CAM overflow have any effect since the switch will have no free slot to add this new one and will therefore skip this step. If the address is already present in the table, the switch will reset its aging timer as usual. On outgoing packets, if and only if the destination MAC address is not present in the table will the switch indeed send the packet through "all" of its interfaces. If the MAC address is present in the table, the switch has strictly no reason to act weirdly: it will simply proceed as usual and send the packet only through the port associated to the MAC address. 

The switch receives an incoming packet on a some port, The switch then checks if the source MAC address is already stored in the MAC address table. If it isn't and there is a free slot, it records this new MAC address associated to its incoming port (and by the way if the address is already present but associated to another port, it will update the record with the new port). This is also the occasion to reset the aging timer associated to this entry, no matter if it is new or not. The switch then checks if the destination MAC address is already stored in the MAC address table. If it is, then this is all good and the switch outputs the packet on the interface associated to the matching CAM table entry. If it isn't, the switch will output the packet on all interfaces except the incoming one (all interfaces belonging to the same VLAN + trunk ports as long as this VLAN is not pruned). 

If you're transferring files in your local - reliable and deterministic - LAN you will probably require less time with TFTP than FTP, but if something goes wrong the result will be a corrupted file or a connection abort (requiring you to restart everything and lose more time). Even if questionable from a real point prospective, no one will stop you from using UDP and than write an application protocol that implements the features of TCP, like retransmission, windowing, congestion avoidance, etc. Nowadays, btw, I would say that the most promising protocol is WEBDAV (HTTP), that runs on TCP/IP. But going back to your question my answer would be: TCP can be considered a better choice for developing an application protocol for large file transfers, because it has some built-in features that we can use out of the box, like: 

Nope, clients discover DHCP server/s by sending a broadcast packet (DHCPDISCOVER, UDP packet from port 68 - bootpc - to 67 - bootps), consequently your DHCP server must be in the same broadcast domain of the clients to serve. As an kind of exception, a DHCP Relay Agent placed on the broadcast domain may be enough; basically, it forwards the DHCP messages from the clients in the network it connects to a remote DHCP server (command "ip helper x.x.x.x"). So, for instance, if interface Ethernet1/0 of Router 250 owns 30.0.0.1/8 (broadcast domain), either the config "ip dhcp pool DHCP350" on router 450 should be moved to the 250, or the 250 should be configured as DHCP Relay Agent towards the 450. 

also relies on some brute-force strategy by sending its malicious packets as fast as the attacker's device and the network allow. This cause several issues: 

This is a quick-and-dirty, few-lines examples which could be improved in several ways. For instance, would it be used against real gears it may make sense to use two successive sending iterations, the first one being quick in order to rapidly take over CAM tables, and the second one working at a far more slowly pace, taking full advantage of the 5 minutes aging delay to stay below the radar as much as possible (when this default delay is changed, it is generally to be raised and no diminished, and moreover I have some doubts that someone who do not take care of enabling port security on his switches will really bother changing such kind of setting). Correct a bug currently affecting dynamips Sadly, when you are through all this, you will discover that when their CAM table is properly filled, the switches in GNS3 will not start to flood packets through "all" of their ports, but they will drop them instead. This is due to a bug affecting the function in charge of handling received packets and located around line 2170 of the dev_nm_16esw.c file: 

However, in real gear the whole process behind this parameter is implemented in hardware, and this setting is currently simply ignored by Dynamips' implementation of the NM-16ESW module. Dynamips implements its own garbage collection system which deletes old MAC entries after only 30 seconds, making CAM overflow attacks noticeably more tricky to stabilize (but may be a good training against the "backpressure" functionality described by ≈Åukasz). The code in charge of this can be found around line 2516 of the dev_nm_16esw.c file: 

Actually TCP run atop of IP (ie. you can't run TCP exclusively without IP). Long story short: multiple protocols are involved during a file transfer, and the assumption that TCP/IP is always better (than UDP/IP for instance) for large file transmission can be technically not exact. This is because, as said above, we have many protocols involved (have a look at $URL$ and each one of them can implement features in an exclusive, complimentary or even duplicate way with respect to each others. Practical example: 

With sub-interfaces, the sub-interface number doesn't need to match the VLAN ID (even if it's a best practice). However, many other possible answers exist, for instance: 

Actually if your access switches connected to the so called "core switches" have just one uplink, either to the HP or to the Juniper, full redundancy is not an option given your constraints of no additional cabling. If also an additional cabling from the WAN router to the HP switch is not an option, even resiliency to the Juniper switch failure is not achievable as far as I can see. Back to interVLAN routing, any FHRP protocol (VRRP etc.) can effectively work only assuming that a client is able to reach the backup gateway if the active one fails, but in your case if the Juniper switch goes down there is no way for the clients (connected via the switches with a unique uplink to this Juniper) to reach the HP switch. Furthermore, even in the case you have VRRP in the example just above, the clients connected via the HP would only be able to establish interVLAN communications only with clients connected via the HP (no WAN or anything else). Hope it helps. 

This launches the function every 15 seconds. What this function does is to scan the whole CAM table and check a hit flag associated to each MAC address: 

Using the right tool The tool classically recommended for CAM table overflow attacks is (from the dsniff project, unmaintained for years). However, this tool makes me the effect of a primitive barbarian from some fantasy story: brutal, inefficient and unreliable. This tool generates packets using fully random MAC addresses generated on the fly. This is wrong for two reasons: 

First and foremost, I must indicate that I am mostly active on the IT Security StackExchange website. Since this current website may reach a different public I find it wiser to tell that, while I adopt the point of view of a potential attacker, all information in this post is given only for educational purposes, in particular in order to understand the concrete threats that are affecting networks beyond myths and oversimplified discourses. I do not encourage nor do I approve the use of the methods described below on any unauthorized network (and I really mean it: if you want to learn get GNS3, that's what this whole thread is about :) !). Sorry for the length of this answer, but despite my researches I did not manage to find any satisfying resource on the web on this topic. Most "proof-of-concepts" are biased either by stopping at the CAM table being filled step and assuming but never really demonstrating the switch reaction, or by using some artificial tricks like clearing the switch MAC table before flooding. My goal here is to provide concrete steps suitable both for simulated and real environments, focusing on the issues related to GNS3 virtualization and common wrong approaches, and most importantly to provide sufficient background information to understand why things are the way they are. 

Actually you might find useful to approach the spanning tree operations in terms of network segments. A segment is a collision domain or, technically speaking, a portion of a network where connected devices use the same physical layer. So, for instance, the cat5 wire that connects a switchport with a PC is a segment, and if 2 switchports of 2 different switches are connected to an old hub in which a common IP subnet is assigned, these 2 ports are on the same segment, etc. From the spanning tree point of view, in a single segment only 1 switchport is allowed to forward frames to that segment, and it is the designated port, and any other port should be blocked unless there is a switch that choose the segment as the best path to the root bridge. Indeed, every switch choose which of its segments is the best one to reach the root bridge (and in which it defines one and only one root port). So, in a segment that is not selected by anyone as a path to the root bridge (ie. a leaf of our tree), one designated port is elected and is in charge of the forwarding of the frames into that segment. In a segment that is used by one or multiple switches to reach the root bridge (ie. a node of our tree - eg. a typical connection between 2 switch in cascade), the switch with the best BID will have its switchport in designated mode, and all the other switches will have 1 port each in root mode. In other words, a designated port forwards traffic from the root of the tree of STP to its leaves, while the root ports forward the frame from the leaves and the nodes up to the root (or to the proper intermediate note). 

I will now take each of these point individually. Knowing where difference with real gears lies For performance reasons, a lot of switch things are actually not part of the IOS code but are implemented in hardware. This includes the ARL, or Address Resolution Logic, which provides all the methods to add, remove and lookup entries in the MAC address table. Therefore, for the NM-16ESW module to work in GNS3, Dynamips had to reimplement all these normally hardware provided services, or at least push this far enough to allow an unmodified IOS to run on it correctly. The sad thing is indeed that this is unfinished work, as stated in this module's source code header: 

So you're warned: as stated in ≈Åukasz's answer forget about QoS and expect some oddities. Hopefully here we are not dealing with QoS but with CAM overflow, and except the final bug (of which the correction should be included in some future GNS3 release, I hope) there are two main oddities which are of concern to us: one is affecting the MAC address table size and the other the MAC address aging process. First difference: the MAC address table size tops at 8189 entries This was the main topic of your question but is in fact a non-issue. The CAM overflow attack exploits the fact that a switch is not able to add any new entry to its CAM table, and therefore fallbacks into "behaving like a hub" (as it is often described, I'll come back on this later). Most probably due to a minor bug, it seems that the MAC table is considered full at 8189 entries instead of 8192. However, full still means full: the ARL will still fail to store any supplementary entry and the CAM overflow attack will still be successful. Second difference: the setting is not honored By default, MAC entries should remain the MAC address table for at least 5 minutes (=300 seconds), as defined by the setting: 

In Catalyst switches, VLAN ID must match the SVI number. Anyway, my guess is that you're confusing SVI (used by the switches) with sub-interfaces (used in routers). If this is the case, your configuration may look like: 

QoS takes action when there is a congestion. So, yes, your team mates might be right when saying that a link used at 50 to 70 percent doesn't need QoS. First, let's think to a theoretical link of 1 bit per second with a clock rate of 1 second (meaning that there would be 1 wire that transmit either 1 or 0 for 1 second, because destination wouldn't be able to catch the value if the signal is shorter): until the traffic that we need to send is of 1 bit per second, we just put that bit into the wire. No QoS is needed. But if we receive 2 bits per second from a faster link (a LAN for instance), these 2 bits need to be forwarded to the 1bit/s link, and so we need to either queue or drop 1 of the 2 packets we received, while forwarding the other 1 bit. Here QoS should be used to decide what bit must be forwarded first, and what should we do with the other one (basically, drop or queue). Second, in a real world situation, we have links that have a fixed bandwidth and that can transmit only at that bandwidth; for instance, an Ethernet 100M full duplex can send and receive data at 100Mbps only. If we're connected to an ISP with a 100M Ethernet link but we pay for 50Mbps, our link must send frames/packets at 100Mbps. To achieve the 50Mbps we need to do something like transmitting at 100Mbps for an half second, and than wait another half second without transmitting anything, obtaining the average of 50Mbps in the time of 1 second. In this example, a burst may allow to transmit at 100Mbps for 1 full second if we didn't transmit anything in the previous 1 second. With these concepts in mind, we can understand that the link used at 50%, that hasn't any burst above the link capacity, will never be congested and QoS won't be used. On the other side, in a real world, it's rare to spend a lot of money for a WAN link that is never fully utilised (but it might be not in a LAN); also, peaks of traffic happen in a usually unforeseeable manner. Consequently, moments of congestion should be taken in account in a good plan, in order to permit the flow of the critical traffic while sacrificing the non critical one. QoS is quite complex anyway, if you're interested I wrote this column: $URL$