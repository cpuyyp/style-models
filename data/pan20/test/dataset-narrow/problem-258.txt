Of course. A consistent full backup created in MOUNT mode does not require any archivelogs to be successfully restored. An online backup is inconsistent and requires archivelogs for restoring a consistent state. 

Use an up-to-date client and tool. I remember some old clients and tools that do not support all datatypes. For example, if I remember correct, Toad 9.0 for Oracle does not support the (or maybe it was caused by and old Oracle client, I do not use Toad, just seen this once at a client on-site). Below works just fine from 12.1.0.2 SQL*Plus or SQL Developer (4.1.5.21). 

Resource manager is automatically implemented for maintenance windows. When a maintenance window opens, the related resource manager plan is automatically activated by the Scheduler. This is the default behaviour and requires no configuration. Next step is to identify the statements using the CPU and tune them. 

Not a big difference, but it can still affect the optimizer so that it choses another plan. Above is explained in detail by Jonathan Lewis in his book "Cost-Based Oracle Fundamentals". 

The easy way: You do not need to recalculate the SUM of expenses, since you already have the SUM of expenses except the new amount, so all you need to do is adding the new amount: 

You can create tables because the RESOURCE role has the CREATE TABLE privilege granted. To create a view, you need to have granted the CREATE VIEW privilege additionally, because, by default that privilege is not granted, not even to RESOURCE. 

"As far as I know, for the listener to work, I have to have listener.ora file." Starting with 8.1.5, listener.ora is optional for a default listener. Starting the Listener without a LISTENER.ORA (Doc ID 208968.1) 

You can not use when the instance is not started, but you can not start the instance because of an incorrect parameter value. Create a PFILE (if you do not already have one), edit that, then start the instance. 

You can follow the steps by selecting from the CTEs in the last line in the order I listed them (so , then , then , and so on). 

Here is the full, graphical installer (win32_11gR2_client.zip) for 11.2: $URL$ The 11.2.0.4 version can be downloaded from $URL$ with a valid support contract, which is required for downloading updates. 

This populates the above table (and so the view) with the actual data. When installing a PSU, this script must be executed in all databases running from the affected Oracle Home. In most of the cases, skipping the postinstall script does not cause any problem, so mostly this goes on unnoticed. But there are exceptions, for example I have seen a database throwing ORA-07445 errors during the configuration of GoldenGate, because the postinstall script was not executed after the installation of the PSU. Starting with 12c, the postinstall script is executed as: 

If you find errors, or equals to in the output of the above query, the instance reached the limit at some point, and you may need to increase the value of the parameter or investigate the reason for having such number of processes. 

This however may not be feasible for you, as a customer can have multiple different with your original design. This works without the unique constraint (but not the same result): 

You need to terminate the SQL statement with a semicolon () or a put a slash () in the new line instead of hitting Enter. For example: . Otherwise SQLPlus will believe you have not finished your SQL statement, and it starts counting the lines. is actually a line number, it is the 2nd line of your SQL statement. If you type nothing here and just hit , SQL*Plus will terminate your statement and return the prompt, it can not handle empty lines. 

Archiving the Database Audit Trail Scheduling an Automatic Purge Job for the Audit Trail Manually Purging the Audit Trail Basically you copy the audit trail to another table, e.g: 

The screenshot of your data and output clearly shows the problem. The concatenation operator automatically takes care of and you don't need to do anyhting. But you don't have in your tables, you have , a string, in your tables. 

This is where you are wrong. You have 3 instances, you have 3 redo threads, so you have 3 archivelogs containing the change at . Because of this, you need to recover the database until SCN/change, not . 

As you can see in the 3rd picture, when using the clasue, the grantee can be only an user, but not a role. (By the way, that clasue is used for something else.) Using the regular , this is the error you would receive: 

If you can create a snapshot of everything at the same time, you do not even need backup mode. Backup mode (User Managed Backup) is not deprecated, but RMAN is the preferred tool for making backups. 

That is part of the solution. You can catch the error/exception then display the appropriate message to the client ("you are already logged in", "connection limit exceeded", etc.). This is not something we can solve in the database. I have seen a similar extreme case, where transactions were blocked with the event , that is something that can be managed in the database. But I have yet to see a case where is a database problem. 

Based on your description of this behaviour and your other comment, I guess your database runs on Windows. The command you used should suffice and delete the datafiles. Except on Windows. On Windows, the datafiles are not always deleted, it is a known limitation. Before dropping tablespaces or datafiles, take them offline: 

You can find the other methods using PL/SQL or XML functions here: working with long columns Or just use ALL_SOURCE to find the trigger: 

No grant needed. You do not see any results, because output is disabled by default. Enable output before running your code: 

shuts down the current instance. To shut down all RAC instances, issue the above shutdown command in all instances, or better use srvctl, for example: 

More details at (referring this blog post of an Oracle employee, because I could not find all this nicely collected together in the official public documentation): Component Clean Up Series 

At least 14 GB. Maybe more if the dump file is compressed. Maybe less if the tables are compressed. A bit more or lot more if there are indexes. Even Data Pump will miscalculate the required space in case of compressed tables. Index data is not stored in the dump file, only index definitions. You can have a 14 GB dump file with 14 GB table data and 0 index definitions or hundreds of index definitions, and building those indexes may consume a lot more storage than the base tables, or just a fraction of it. So the correct answer in my opinion is, test and measure it. 

You got the error, because you have a container database, and you tried to create a common user in the root container without the common user prefix. Introduction to the Multitenant Architecture 

Not in 11g. Not even in 12c, but 12c makes life a bit easier, you can delay the "rebuild" part until a more appropriate time. In 12c there is a new feature, called Asynchronous Global Index Maintenance. You can drop the partition, and that still leaves the global index usable, and marks the no longer needed index entries orphaned. Then later, you can clean up this index (, or , or the database can do it automatically for you as well (there is a pre-defined job for it called ). 

Physical: "Differences between the primary server(s) and the standby server(s) are always supported as long as the Oracle software installed on all servers is of the same Oracle Platform as defined above, is certified to run on each server, and is the same Oracle Database Release and Patch Set." Logical: "Differences between the primary server(s) and the standby server(s) are always permitted as long as the Oracle software installed on all servers is of the same Oracle Platform as defined above, and is certified to run on each server, and is the same Oracle Database Release and Patch Set. (Note that beginning with Oracle Database 10.1.0.3, SQL Apply supports configurations where the standby database is operating at a later Oracle Database Release or Patch Set, for the purpose of performing a database rolling upgrade.)" So the short answer is: no (even if it is technically possible). 

No errors, everything should be OK, still, I can't connect. So much for . Now let's do some real troubleshooting, and check the server side. The address I am connecting to: 

Enterprise Manager 11.2.0.4 Database Control Backup Settings are not Retained, Scheduled Backups Fail to Run (Doc ID 2185136.1) 

As you can see, i get a instead of it using . If I simply try to use on the column, data will be lost during conversion: 

Much better. Also notice the Cost column in the plans. The cost of the first plan was 226K, but with disabling XML rewrite, the cost of the second plan was 2. I didn't investigate this topic deeper, but it seems this is another kind of query transformation, that is not based on the cost. 

But this requires at least version 12c, and it is an extra cost option. For lower versions, or without the Multitenant option: Simply create 2 traditional databases on the server: and . Import one dump to , the other to . You can specify in your connection string the service (or SID) for the database you want to connect to. For example: 

Depends on what tables you try to export. Built-in tables are not exported. User-created tables are exported. 

Yes, you need to catalog the backups at their new location. You should run crosscheck also on the backups, so the entries pointing to the old location become EXPIRED. 

Tempfiles can be dropped then recreated without any hassle. I would just try dropping the tempfile and then run nid again: 

If this was a one-time error, just try running SQL*Loader again. If this happens repeatedly, you will need the help of the DBA. The above views barely contain anything informative related to this issue. (Also there is no such view as or .) When this error occurs, the database instance creates a trace file on the database server (alert and file location can be found in the alert log), that is what you will need for further investigation. 

Here you can see BMW, SILV row was updated ( in column ), and BUIC, BLAC row was inserted ( in column ). (Note that a may change.) 

The above command deletes all archivelogs older than 1 day and backed up at least once to your backup server. 

clearly shows the cause of the problem, the listener is not registered in the clusterware. Undo all changes in listener.ora (or simply delete it), then: 

Notice in the above output, the service has 2 instances/handlers, with different addresses. Whenever I connect to , my requests will be forwarded to or . My client can't understand those addresses, but if I fix it: 

Not enough information, so I will just leave this here: Bug 14073795 Wrong results on select statement with TRUNC(date) or ROUND(date) Bug is fixed in 11.2.0.4. 

The database in the above will not open. Start the database in mode, and clean up the archivelog area. 

But we need a function. Finally Oracle 12.2 allows long (128 bytes) identifiers, so we can give more meaningful names: 

The step will fail at the standby site, and the service remains in state, but that is normal. With this, the service is automatically relocated (stopped at old primary and started at new primary) when you perform a role transition with Data Guard broker. And use this service for connecting: 

this may be caused by a bug, permission issue with the executables or other environmental errors, such as deleting socket files from the tmp directory. First investigate this. . For this: 

1), 3) and 4) runs successfully on , 2) throws . On , all 4 runs successfully. The one thing to keep in mind: never rely on implicit behaviour you can not control. Use explicit, unique aliases. 

Whenever you connect through a listener, the listener may forward your request to another address that a participant does not understand => . Make sure your client and server can resolve all addresses specified in the connection string, especially the addresses that appear in the output of on the database server. And below is an example. My client: 

This happens when you log in to the root container of a CDB, because tablespaces are local to PDBs. Configure EM Express at PDB level, for example: 

can be set in the ASM parameterfile and in the GPnP profile as well. Grid Infrastructure needs to know where to search for ASM disks before starting ASM and reading the ASM parameterfile, the GPnP profile makes that possible. dsget 

The above creates a file containing the and statements for your tablespaces, which you can edit or use for creating the tablespaces. 

To prevent this from happening in the future, schedule a regular database backup + archivelog backup that deletes archivelogs. If you do not need backups and archivelogs, just disable archivelog mode while in stage: 

The backup operation will run on the database server, and the backup pieces may be created on the database server, attached storage disk, a backup server through a 3rd party library (Networker, TSM, etc.), or any other server through network shares (NFS, samba), depending on your configuration. 

You don't catalog backups at destination site, because you don't mount the database (controlfile), you don't even have the controlfile there. The controlfile will be restored from the backup, and that controlfile will contain the backup entries, that were transferred to the destination host to the same path as on source. 

We can see the cost would be 1242, and the internal view VM_NWVW_1 appeared as a result of a Cost-based Query Transformation. Because of the GROUP BY, this can be only Complex View Merging and Cost-based Query Transformation, where the cost is not ignored, that is why Oracle did not choose this plan. So knowing that DISTINCT can be eliminated more easily, even without Cost-Based Query Transformations, and that we do not need aggregate results in this query, it would be better to use the double DISTINCT version: 

did not provide the result we expected. It's because the way works. If any row of the subquery returns , will return , so no result in this case. Now your queries: 

Starting with 11.2, you do not need to create the trigger, you can define a role based service with Grid Infrastructure. : 

If the specified dumpfile already exists, this line throws the above error. You can add to overwrite the existing file and avoid this error. 

By default, noone is granted the privilege on with grant option. Even the role does not have the grant option. has access to , but is not authorized to grant privileges on to additional users. You need to connect as to grant privileges on to additional users. Connect to the CDB root as and switch to your pluggable database: 

Technically, you could, but it is incomplete, so better repeat the export after performing the necessary actions to avoid ORA-01555 errors (have an appropriately sized undo tablespace, set and ). 

Most likely you will not find in the list, and that is the problem. Ask the TSM administrators to search for in their catalog. If they find it in a different filespace, or registered under a different nodename, you can restore it by collaborating with them and using the correct tdpo configuration. If they can not find it, you are out of luck as that backup simply does not exist on TSM. 

You did not mention it, but this is quite typical when using PL/SQL. Privileges granted through roles are disabled for named PL/SQL blocks that are defined to execute with definer's rights. How Roles Work in PL/SQL Blocks 

A full export/import contains tablespace definitions with datafile paths as well, so tries to create the datafiles with the original path. Just create the tablespaces manually before running impdp, that is usually what I do. will notice that the tablespaces already exist, and continue with the remaining objects. You can extract the tablespace creation scripts as well if you want, using the clause: 

Split above current maximum value, drop the empty MAXVALUE partition, then convert to interval partitioning: 

There is no easy and supported built-in method for string aggregation in 10g. (Do not use , as it is not supported, and it is not available anymore in 12c.) You need to write your own string aggregation function, for example: 

As you can see, the creation of table statement, that was issued with the module set to , was not audited.