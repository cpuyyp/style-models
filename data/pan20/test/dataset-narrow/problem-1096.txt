Tree Automata can be used to model sets of values of a Herbrand Universe, for example, to model possible values in a functional program. Systems of subtype constraints over set expressions have decidable satisfiability, but this is EXPTIME complete, and NEXPTIME complete when projections are allowed in expressions. See here and here. I'm wondering, has the set constraints problem over equality constraints, instead of subset constraints, been studied? Clearly it's not a harder problem than with subsets, but is it easier? Are there simpler solutions than tree Automata for solving such systems? 

Define the outfix of a language $L$ to be $Outf(L) = \{xy \mid \exists z. xzy \in L \}$. Are any known results about whether deterministic context-free languages are closed under this operation, or other similar "erasing" operations? My preliminary search has not turned up anything, but I thought someone who had knowledge of the area might know of a reference. 

It is well known that CFGs and PDAs are equivalent, and there has been extensive research about the relationship between deterministic pushdowns and $LR(1)$ grammars, as $DCFL$ is a subset of $LR(1)$. I'm wondering, what results are known about classes of grammars that can generate the languages accepted by other restricted versions of pushdown automata? In particular, I'm interested in: 

As a counter-example to this, consider the Context-Free Equivalence problem: it's undecidable to determine, given two context free languages, whether they accept the same set. If your problem were decidable, we could use it to determine CFL equivalence, since it's always possible to turn a CFL into an always-halting Turing machine. So even for countably infinite inputs, the problem is undecidable. It's also worth mentioning that, for standard Turing Machines, all input domains are countably infinite, since they're sets of finite strings. 

Monadic First Order Logic, also known as the Monadic Class of the Decision Problem, is where all predicates take one argument. It was shown to be decidable by Ackermann, and is NEXPTIME-complete. However, problems like SAT and SMT have fast algorithms for solving them, despite the theoretical bounds. I'm wondering, is there research analogous to SAT/SMT for monadic first order logic? What is the "state of the art" in this case, and are there algorithms which are efficient in practice, despite hitting the theoretical limits in the worst case? 

$NCM$, the class of non-deterministic reversal-bounded counter machines, has a lot of interesting dependability and closure properties. It's known that, unlike the deterministic version, NCM is not closed under complement. However, I've never seen a proof of this given explicitly. In Ibarra's paper, the non-closure is implied because if NCM were closed under complement, subset would be decidable. However, I've never actually seen an example of a language where $L$ is in $NCM$ but $\overline{L}$ isn't. I'm wondering, can anyone provide such an example, and preferable, a source describing it that I could cite in a paper? 

In exact exponential algorithmics, the subset convolution is a particularly useful algebraic technique for solving covering, packing and partitioning problems. It generally works very well when the objects to be packed are 'hard', like dominating or independent sets. For example, the best known $k$-colouring (i.e. partition into independent sets) algorithm uses subset convolution. See e.g. Exact Exponential Algorithms by Fomin and Kratsch or this paper by Björklund et al. The subset convolution is most often used in the context of exponential algorithms, but there are some useful applications in polynomial side of things. In particular, see this paper for the so-called 'counting in halves' approach to packing problems. My intuition is that since the subset convolution -type methods count the number of solutions instead of finding just one, they usually cannot be used to obtain fixed parameter algorithms. Also, they are rather space-intensive; their space complexity often equals their time complexity. 

Let $[n] = \{ 1, 2, \dotsc, n \}$, and let $\mathcal{F} = \{S_1, S_2, \dotsc, S_m \} \subseteq 2^{[n]}$ be the input set family. Unless I misunderstood your problem formulation, we want to find a minimum-size set $T \subseteq [n]$ such that $T \not\subseteq S_i$ for all $i = 1, 2, \dotsc, m$. To answer your question, note that $T \not\subseteq S_i$ if and only if $T \cap ([n] \setminus S_i) \not= \emptyset$. That is, $T$ has to intersect the complement of each $S_i$. But this means that your problem is, essentially, equivalent to the hitting set problem (consider hitting set with input $\mathcal{G} = \{ [n] \setminus S_i \ \colon \ i = 1, 2, \dotsc, m \}$): 

My question is, does this latter claim actually hold and if it does, is there a write-up of the proof somewhere? As a background, I've been trying to understand the area around the Exponential Time Hypothesis. IPZ define subexponential problems as ones that have $O(2^{\varepsilon n})$ algorithm for each $\varepsilon > 0$, but this apparently is not sufficient in the light of the current knowledge to imply the existence of a subexponential algorithm for the problem. The same gap seems to be present in the SERF reducibility, but I am partially expecting that I am missing something here... 

I have a question concerning the SERF-reducibility of Impagliazzo, Paturi and Zane and subexponential algorithms. The definition of SERF-reducibility gives the following: 

Now it's clear that a pattern $1\langle S_i \rangle 1$ can match the text at an occurrence of $1[T_j]1$, and only when $S_i \cup T_j = [m]$. The total length of patterns and the length of the text are both $O(nm)$, for instance so a near-linear single-pass algorithm for multiple patterns would give substantial improvements over best known CNF-SAT algorithms... (Note that this does not say anything about algorithms that use lots of time preprocessing the patterns, say, quadratic in the total length of the patterns.) 

Context I realize that subtyping often doesn't admit principle types, and that inference in the presence of subtypes is undecidable. I'm working in a context where typechecking should simply fail there is not a single most general solution, and where annotations can be added to aid inference. Suppose that we have a type system where, during typechecking, some types need to be inferred using unification, and may be type variables $\alpha, \beta$ etc. If, at some point, the constraint $\alpha <: T_1 \to T_2$, where $\to$ is the usual function type constructor, then we can decompose this into $\alpha = \alpha_1 \to \alpha_2$, with $T_1 <: \alpha_1$ and $\alpha_2 <: T_2$. The Problem The problem arises if you have constraints $\alpha_2 <: \beta_1 \to \beta_2$, $\beta_2 <: \alpha_1 \to \alpha_2$. In such a case, we get: $\alpha_2 = \alpha_{21} \to \alpha_{22}, \beta_1 <: \alpha_{21}, \alpha_{22} <: \beta_2 $ from the first constraint, and $\beta_2 = \beta_{21} \to \beta_{22}, \alpha_2 <: \beta_{21}, \beta_{22} <: \alpha_2$ from our second problem. If we substitute from our equalities, we then get (among other constraints), $\alpha_{22} <: \beta_{21} \to \beta_{22}$ and $\beta_{22} <: \alpha_{21} \to \alpha_{22}$, which is identical to the form of our original problem. Clearly if we continue solving this way, we will never terminate. When going to write a proof of termination, the problem is that substitution decreases the number of unsolved variables but increases the structural-size of the problems, and solving subtyping decreases the structural size of the problems, but increases the number of unsolved variables, so they don't work in a well-founded ordering. My Solution Attempt With my definition of subtyping, if $\alpha_2 <: \beta_1 \to \beta_2$, $\beta_2 <: \alpha_1 \to \alpha_2$ has no solution. The problem seems to be the "cycle", so if we do a sort of occurs check and fail when cycles are detected (or turn them into equality cycles, which will fail except for $\alpha <: \beta \wedge \beta <: \alpha$). I'm not exactly certain how to formalize a cycle like this, and how such a check would give me something that I can use in a proof of termination. My Question My work is about the specifics of a particular subtyping system, but the problem here seems very general, so I'm wondering if this is a known problem, if there are known solutions to it, and if there is research that either formalizes or disproves my intuition about cycles. What kind of cycles in subtyping constraints do I need to eliminate to avoid this infinite looping? Or is this a deeper problem with no solution? 

The maximum independent set problem is a packing problem (you can think it as packing disjoint stars), and it has a well-known algorithm with running time $2^k \operatorname{poly(n)}$ in graphs with treewidth at most $k$. 

Networks, Crowds, and Markets: Reasoning About a Highly Connected World, by Easley and Kleinberg probably should be mentioned here. It is rather elementary, but gives a wide selection of social sciences topics that have been considered from a CS point of view and provides a lot of references. Someone with more experience in the field can perhaps tell us how close the book is to the current state of the art in the field? As a more particular answer, with the proliferation of various social networking sites, computer science has become quite relevant in analysing the huge social network data sets from such sites. 

This manuscript seems to prove exactly that. (It doesn't; the complexity parameter is $|T|$, not the treewidth.) In general, most NP-hard optimisation problems have polynomial-time algorithms when the input is restricted to bounded-treewidth graphs. These algorithms use the rather well-known tree-decomposition machinery, which is also used in the linked paper. EDIT: The undirected Steiner tree is, on the other hand, known to be fixed-parameter tractable with regards to parameter $w$ the treewidth of the underlying graph. My suggestion would be to try and adapt this algorithm to the directed case, which would in particular give a polynomial time algorithm for series-parallel graphs. 

If you're interested in set families with $n = \omega(2^{d/2})$, then an another solution conceptually very similar to the one outlined in Yuval's answer is to compute zeta transform $$f\zeta(T) = \sum_{S \subseteq T} f(S)\,,$$ where $f \colon 2^{[d]} \to \mathbb{R}$ is the indicator function of the input family $\mathcal{F} = \{ S_1, S_2, \dotsc, S_n \}$. That is, $f(S) = 1$ if $S \in \mathcal{F}$ and $f(S) = 0$ otherwise. Clearly there are sets $S_i \not= S_j$ such that $S_i \subseteq S_j$ if and only if $f\zeta(S) > 1$ for some $S \in \mathcal{F}$. The zeta transform can be computed in time $O(d2^d)$ using Yates's algorithm, see for example Knuth's TAOCP, vol. 2, §4.6.4. The algorithm itself is a fairly straightforward dynamic programming, and it is easy to modify it to give an example of an included set if one exists. 

Hitting set is known to be NP-complete and cannot be, loosely speaking, solved faster than in time $O(2^n)$ unless the Strong Exponential-time Hypothesis fails. 

For the multiple pattern case, it seems that simply scanning for each of the might be the best possible solution, at least unless the strong exponential-time hypothesis fails. Recall that given sets $S_1, S_2, \dotsc, S_n$ and $T_1, T_2, \dotsc, T_n$ over universe $[m]$, if we could decide if there are $S_i$ and $T_j$ such that $S_i \cup T_j = [m]$ in time $O(n^{2-\varepsilon}\operatorname{poly}(m))$, then SETH fails, i.e. we have a CNF-SAT algorithm with running time $O^*\bigl(2^{(1-\varepsilon/2)n}\bigr)$. Given sets $S_1, S_2, \dotsc, S_n$ and $T_1, T_2, \dotsc, T_n$, we encode the above problem as multi-pattern matching with don't cares over binary alphabet as follows: 

Question description: Consider the problem of finding a minimum $n$-color $k$-state one-dimensional cellular automata (minimizing $k$ for some fixed value of $n$ or vice versa), with communication between cells defined by a 3-neighbor von Neumann relation, that recreates some fixed-size spacetime history "patch" ($URL$ of an unknown cellular automata (e.g. some output of Wolfram's pseudorandom Rule 30 CA: $URL$ initiating from a known seed of some specified size and geometry and evolved for $t$ iterations. Informally speaking, provided an array corresponding to an unknown n-color k-state CA evolved for a known number of iterations from a defined seed specifying initial conditions, can you provide me a sufficiently minimal CA to create given pattern (with the full understanding that there may be a diverse and non-unique set of solutions)? Now let me make a strong (and possibly false) statement: "There is no known computational complexity for this problem, and in the general case, exhaustive search must be employed over the set of all possible $n$-color $k$-state cellular automata (with communication between cells defined by a 3-neighbor von Neumann relation) to find a minimal cellular automata capable of recreating a specified spacetime history provided an initial seed of some defined size and geometry. This is true even if we specify that $n = 2$, as for example Wolfram does for all of his elementary CA patterns. This is true regardless of any guarantee about the properties of the initial seed as long as $r$, the number of cells in the seed, is $<<N$, the number of cells in the specified spacetime history." To what extent is the above statement true, and are there any good literature references that make any strong statements about the computational complexity of this problem? I've been able to find a lot of papers in the literature that talk about, say, using evolutionary algorithms to solve this so-called "cellular automata inverse" problem, but almost nothing that touches on the general case computational complexity of the problem. 

Motivation: In Stephen Wolfram's "New Kind of Science", the central argument of the book seems to be that one can "mine the computational universe" for cellular automaton rules that at least emulate everything from patterns appearing on seashells and stripes on a zebras, to local deterministic models of reality (proven aphysical in Scott Aaronson's review of NKS (section 3.2): $URL$ Saying nothing about whether emulating some pattern or process in nature yields insight into the underlying rules for the process, I'd like to be able to say something specific about the "hardness" of trying to actually actually emulate some fixed sized pattern with a sufficiently simple $n$-color $k$-state CA.