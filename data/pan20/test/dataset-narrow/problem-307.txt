If newer messages consistently have larger ID values in the table than the older messages, perhaps you could simplify the filter conditions in your subqueries by removing the checks involving . Those checks may seem to be limiting the number of rows in the result set (thus potentially speeding up the subqueries), but in reality, if a message matches the conversation and its ID is larger than the last read ID and the poster is not but the other party, then it is an unread message for . So, this is how you could count : 

If you specify 18, then you will be skipping 17 characters. The solution using and , suggested by George.Palacios will work as well but the results of these two solutions might not be the same depending on whether can have trailing spaces. The reason for the possible difference is that the function ignores trailing spaces, while does not, which means that for a string with trailing spaces the result will most likely not be what you would expect. Let me demonstrate that using a simple example with a shorter string. Suppose the maximum length is 10, the number of characters to skip is 4, and the specific string is . (The quotation marks here are just string delimiters, for you to be able to see the trailing spaces.) If you do 

Just group your @LatestFifteenMinTickets table by DomainId and take the difference between and . That will give you the number of sold tickets for each domain within the period collected in your table variable: 

An inner join would be better here since orders without associated customers would not match the condition anyway. Talking of that condition, if the CustomerOrderLink table can never have more than one CustomerID/OrderID pair (and it should not), then you can replace it with or even . Note that if is a nullable column and some customers do have a null there, the above may not return what you expect, because COUNT ignores nulls (and so it will return 1 if a group contains a couple of customers with the same surname and the rest without a surname). 

Your solution seems good enough, I am not sure why you would need to ask for anything more. I would only like to note that if you want to skip the first 18 characters, then you should specify 19 as the second argument for , because in SQL the character positions in a string start from 1. So this makes perfect sense and should work well for you: 

Agree with Scott Hodgin, you essentially want to consider MAX(FormIdNb) instead of just FormIdNb, and count the results based on whether the MAX result is greater than, equal to or less than 3. Adding a correlated subquery to the joining condition, as Scott is suggesting, would be one way. Another would be to use a derived table to first get the max FormIdNb per NumCavalier and range: 

This problem can also be solved in a very MySQL-specific way, using a variable. The variable will store a unique CSV list of values found in and . The value of each column will first be tested against the list (if found, a null will be returned, otherwise the value), then added to the list (if absent from it). This is my implementation of the above logic: 

Basically, it seems you need to group by and the name of the correct month, then pivot the aggregated results on the month name column. Here's how you could go about it, provided you are using SQL Server 2005 or later version: 

Other database products support more advanced SQL syntax – specifically, window functions – that allows you to solve this category of problems without using a join, thus offering better performance. MySQL does not support window functions (yet?) but there is a way to work around that limitation using variables. Specifically, you would use variables to rank rows within each group based on the value and then filter the rows on the ranking value: 

The next, and last, step would be to use the above as a derived table and outer-join to it once more – this time to get the job details (and you would also need to outer-join and to get details from those tables as well): 

You have replaced the column reference in the CASE expression with the same subquery that you are using earlier to calculate the value of . While that certainly resolves the issue of referencing a calculated column at the same nesting level, duplication of code is far from an ideal solution. You can avoid the duplication by abandoning the CASE expression completely as unnecessary. All it does is producing a "yes"/"no" result based on the values of the current and previous only to be compared to "yes" at the outer level. Instead of checking on the value of , your outer level can be checking on the values of and directly, like this: 

Since you want product names to be unique in the output, group this table by , and for select e.g. the minimum value per product: 

However, I would also suggest you consider using implicit joins in your queries. And so, I would probably re-write the above like this (minus the formatting, which is a matter of taste and, therefore, up to you): 

Alternatively, you could use GROUP BY instead of DISTINCT and thus return the sorted three-column set without a derived table: 

If Chris's assumption that four joins should be enough for most cases does not work for your particular case (and I don't mean the simple example in your question, of course), you can throw in more self-joins using the same pattern. 

Other answers have pointed out that CASE is an expression, not a statement, and thus cannot itself encompass statements (like or any others). If conditions are not many – particularly when it is just one condition, – the IF statement is the perfect choice for what you are trying to do, as has also been mentioned. Nevertheless, depending on your scenario a CASE expression could still be used, just not exactly the way you have shown. In particular, if there are many conditions to be checked where a match should result in the same set of actions (e.g. raising an exception and terminating the script), you could use a CASE expression in an assignment statement storing the CASE's result, then follow it with an IF checking the stored result and performing the required action(s) if appropriate, like this: 

Another option, also involving dynamic SQL, could be, basically, to replace every with a comma, put in front of the string and execute the resulting query. The idea is that SQL Server allows you to represent column aliases in the form of and the input string already consists of such name/value pairs, only the delimiter needs to be changed. But, of course, in actual fact that would not be enough, because, first of all, the values would additionally need to be enclosed in quotation marks. As for the names, they might happen to match reserved words in SQL – that means the names would need to be quoted as well. The simplest approach to resolving all the issues mentioned assumes that an equal sign in the input string is always a delimiter between a name and a value and an ampersand is always a delimiter between columns, and neither can ever be part of a value. Given that your input string looks very much like an HTML parameter list, I consider those assumptions reasonable enough. So, the solution's logical steps would be these: 

The GetPermissionStatus inline table-valued function's result can be either an empty set or one single-column row. When the result set is empty, that means that there are no non-NULL entries for the specified page/user/permission combination. The corresponding Pages row is automatically filtered out. If the function does return a row, then its only column (IsAllowed) will contain either 1 (meaning true) or 0 (meaning false). The WHERE filter additionally checks that the value must be 1 for the row to be included in the output. What the function does: 

I would also like to suggest, if I may, that you be consistent in how you specify your constant literals. What I mean is sometimes in your query you are providing the matched values for as strings () and other times as numbers (). You should really choose one way and, of course, it should be the one matching the column's actual type. Also, the in your query makes little sense. It looks as though it may be a remnant of an old technique for intermediate materialisation, which, however, may no longer be working in your version of SQL Server. There is also this HAVING filter in your query (), which would work more efficiently if you moved the condition to the WHERE clause. One last note concerns your second CASE expression (the Type 8 one). One of the columns it is referencing is qualified with an alias that is not found in your FROM clause: . I am assuming that is some kind of a copy-paste error posting your script here but I thought I would let you know so that you could correct it. 

Those are the result of the server's attempts to execute SQL statements issued by your application. They cannot possibly be caused by problems with Visual Studio or SQL Server Management Studio. SSMS is essentially just another application that, like yours, can connect to the database server. Your being unable to run it cannot possibly affect how SQL queries sent to the server by your application are executed. 

The main query does not need to group by , because it is aggregating that value. Just remove the column from both the and . 

This is an equivalent substitute because whatever will be passed to the IN subquery is going to match based on the filter in the query that passes the value. 

and, using them as a derived table, join them back to the source to get the rows matching the maximums: 

For reference, here is an example of a complete query that calculates percentage using window aggregation instead of a correlated subquery: 

A more specific, and probably more appropriate for your situation, equivalent is the IIf function. It is intended for use when you need to return one of two values based on a single condition. In other words, it replaces the following CASE pattern: 

And since you have also specified that you want only the final , you probably need this additional condition in the main WHERE: 

For completeness, and to make the lack of elegance in this solution more evident for the wider audience, this is what the final query would look like: 

It only remains to apply your formula to the obtained values, which is essentially . Translating all the above into SQL, we can get a query like this: 

As you can see, the query is calculating both the COUNTs and the GROUP_CONCATs both at the client level and at the whole set level. But each pair of the results is put inside an IF function, so that ultimately only one or the other result is returned in each column. The condition to check is . If happens to be null, that means that the current group represents the entire set and in that case each IF function chooses the COUNT result. When value is not null, that means we are at the client level and each group of rows represents a specific client. In that case the GROUP_CONCAT result is returned, which is according to requirements, because for clients we must show concatenated strings. Both solutions can be found at dbfiddle.uk. Additional remarks In my explanation above I tried to focus on the solutions and how they work. To avoid distractions, I allowed certain anti-patterns in my code that are worth mentioning. 

Using that as a grouping criterion, you can then use the FIRST_VALUE analytic function (assuming you are on SQL Server 2012 or later version) to get the first per group: 

The nesting, however, is superfluous too. If one pattern is not matched, SQL Server will continue testing the next one, until it reaches the ELSE clause. Therefore, your CASE expression could look simply like this: 

Every / column pair in the common table expression (CTE) is represented as a single column, , aliased . The month name for every value is derived, using , from the minimum value in the same group (or partition) of as the current row. (The requirement is to use the credit date. The credit date is supposed to go before the debit one(s), hence looking for the minimum date.) The query uses a window function to get the minimum s. The column is the sum of all results per . It is calculated using a window aggregate function too, which is this time. (The column is re-aliased as in the final SELECT to match your expected output, but it seemed to me to make more sense to call it at this stage.) Eventually, this is what the CTE produces: 

Furthermore, I would also consider moving the concatenation bit to the front end and use the database only as a data source*. My final query, therefore, would look like this: 

The variable would thus be initialised in the UPDATE statement with the value stored into . Alternatively you could also get rid of the statement by rewriting the UPDATE like this: 

The only thing that both stood out for me and was something I knew what to suggest about is your use of ORDER BYs without LIMITs in derived tables. You are using them on two occasions, once in the derived table that is inside inside , and again inside . Those ORDER BYs are unnecessary and they are not optimised away, if I am reading the execution plan correctly. Not sure if removing them will have much of an impact but it is something you could start with. There are other places where I think you might be doing something in a suboptimal way, but I have no idea what to suggest as a replacement/rewrite without knowing much (or indeed anything) about what you are trying to do there. And I would rather not ask you for more details because, in the end, the query is too big for this question to be of much help to other people. Please do not make any mistake: this site is about helping people with their problems, but the idea is that each contribution builds towards a knowledge base that can be used on many occasions without asking. Thus questions should, as much as possible, be useful not just to the original poster and whoever took the exercise of answering them but to the wider audience as well. Therefore, I think, instead of asking people entirely unfamiliar with the schema, data or business logic to help you with a wall of code, you should consider a different approach. Try starting with a minimal subset of your query that is both working and fast. From that point on, gradually add other parts – like, one derived table at a time – until the query is no longer efficient enough for you. That way you would be hitting a specific problem, which would let you come up with a specific question to the community. Not only would such a question be easier to analyse and answer, it would also have more chances to be generic enough for other people to find the answers useful for them as well, which would be fulfilling the goal of this site.