Base Game class already has GameComponentCollection in it and it handles calling updates and draws, so don't create new one and just use theirs. $URL$ By the way: I think the GameComponent is meant for more complex things then just game entities, for stuff like SceneGraphComponent (that would handle drawing of every SceneNode class) or NetworkingComponent (that would send and recieve queued packets) 

explanation: someone could use your Turret class from their assembly, and inherit from it, at that point they could call Shoot method (because its protected, so only inherited types can call it), but they do not have access to Level class, since it was not public (therefore not avaliable in different assembly) 

and Model should be created in game by new Model(Content.Load()) and not cached (or loaded by some alternative from xml). Materials should be collection of parameters that you pass to effects inside Mesh.MeshParts[].Effects (like texture) 

Lookup tutorials on $URL$ They have whole games with source code there, but all you need for games like snake is how to draw a texture on specific place and how to handle input. For a snake 

Unity does this by having 3rd separate component, called Transform3D - which contains position. So then you can have Physics system, that can use physics components (like Collider) and positon component (Transform3D). Physics system then does physics simulation using physics componets and position component and then update position component accordingly. And Rendering system, that will use renderer component (like Model, or in your case Sprite). Rendering system then renders all renderer components where position component tells you. 

In example you gave, you don't need any sort of alpha blending. (Pixel is either visible or not - and i suspect most of studd in minecraft like game will be like this) You can just call discard in shader for pixels you want to be transparent (and they won't write to depth buffer) (In XNA 3 it was possible to set RenderState.AlphaTestEnable which will do the same.) 

If I rembemer correctly, caching assets is done in Load method. So all you need to do is override this metod, and make custom caching (store loaded asset in dictionary with assetName as key). (This will help you later in unloading only certain items). And then make new method LoadNew which will bypass caching (or create clone) 

note: This is how it works with Xna, and i don't think your shader is ready for what sprite.Draw will do 

Is there a way to render something custom inside some html element in Awesomium? For example: I have inventory screen (that can be moved around) that has 3D avatar playing some idle animation weilding all equipped items. The screen should be html but the avatar is rendered by the game (to some texture) and now i need to display it on html page. 

I've been studying opengl es and an example I saw was using a "Pool" class to keep track of touch and keyboard events. Could someone please explain how and why a pool class is needed. From what I was reading it had something to do with garbage collection and limiting the number of input classes that are made. This all seems a little abstract to me, so if someone could please explain what is going on, I'd appreciate it, I'll pasted some code here: 

Is there an easy way to re-render dynamic vertices in OpenGL ES 2.0 so that one could, for example, make a modeling program? I understand how to make a dynamic vertex array, but what I don't understand is how to re-render a dynamic array, I've only seen examples where you have to re-compile a shader each time you make an array. I did find the function: 

I've researched it and it is a CORS error a "Cross-origin resource sharing" error, but it's a local file! I can't figure out what's wrong. I did make the picture using gimp, and I'm not sure the coding was right on the export, but I eliminated a previous error using "gl.OES_TEXTURE_FLOAT_LINEAR". 

So, I guess I need to know the origins, but the triangle strip looks way off. I am doing this as well: 

it displays a picture with variable light color added to the final pixel color, but s_lightMap isn't even linked into the program, what is happening in this case, at first I thought it would just do the baseColor as the FragColor, but the addition of the 0.25 makes a non-negative result to (lightColor + 0.25). I'm confused, one minute I think lightColor would be set to a texture of 1's the next an array of 0's. Or is it just random data? It doesn't appear at all random in the picture, it looks like it's obeying a rule of shading. I'd like to mimic this effect in code that's not broken. Here's the unshadded image: 

Long story short, I know my coordinates are off and I believe my indices might be off. I'm trying to render a simple 2d rectangle with a texture in webgl here's the code I have for the vbo/ibo: 

What is billboarding, and can or should it be used to create special effects for 3D games on weapon blasting effects? 

How can I add graphics to some control, perhaps a picturebox, on Form1 that could potentially show my game as in a level editor? Thanks. 

I'm using this image: On the properties of this image it says it's 32 bit depth, so that should take care of the gl.UNSIGNED_BYTE, and I've tried both gl.RGBA and gl.RGB to see if it's not reading the transparency. It is a 32x32 pixel image, so it's power of 2. And I've tried almost all the combinations of formats and types, but I'm not sure if this is the answer or not. I'm getting these two errors in the chrome console: 

Content pipeline can not be referenced from project using .NET Framework 4 Client Profile. You can change this in (Right click on project -> Properties -> Application -> Target framework). But can't you use normal content pipeline to build your XMLs and then load them as normal content? 

You need to check if you bullet is touching any part of image, if image is rectangular you just check if bullet is inside rectangle. If your image is more complex then rectangle you need to do pixel perfect collision, that means go through all your image and check if your bullet hit some colored spot. 

Unless i am missing something i think that XNA approach is flawed. You shoud have something like this: 

And then have dictionary of opcodes and handlers and invoke them. So no big switch at cost of dictionary (or array) lookup and one additional method call. This is how you register handlers 

Where should I store input layout? It's a connection between vertex buffer and specific pass, i can live with just 1 pass but I still have diffferent techniques so i need at least an array with some connection to effecttechniques, but I would appreciate something not crazy like dictionary. I could also create wrapper for Effect and EffectTechnique, but there must be some normal solution. 

$URL$ I would suggest to create subclass of SpriteBatch that would have DrawLine and DrawRectangle function and in constructor create 1x1 texture. 

So i would suggest you to create a texture (an image) of one body part of a snake and find a way to display it. (Using Content pipeline, ContentnManager.Load and SpriteBatch.Draw) Find out how to handle when someone presses keyboard and when he does move that body part and display it on next location (using Keyboard.GetState()) Make moving of that body part automatical, and with keypress just change direction (using Update method on Game1 class) Draw other things like food Find out how to handle collisions of snake and food (using Rectangle class and Intersects method) 

Effect has Techniques that tells you what vertex shader and pixel shader to use In your .fx file you will have 1 vertex shader and 2 pixel shader functions, and two techniques 

Something similar is done by content pipeline on basic effect when importing Model, and thats where it should happen 

You can collect draws from all your methods and then draw them all at once. (Because in debug draws you don't really care about ordering) This you will call anywhere in your code (like in update method) - you an make DebugRenderer singleton 

Now even if there is infinite loop, the code will not block your code. And users can write logic "sequentially". As you can see this is essentially what unity does in its Behaviors (I wrote this from head so it has probably some errors in it) Or you can write your own CIL interpreter and interpret few instructions each frame :)