I have no problems sending email to GMail over IPv6. However, I have a dedicated sub-domain for my mail server. (In my experience and research, I have found second level domains are most likely spammers.) IPv6 tends to be much easier to configure correctly for email serves (rDNS) etc. You might be flagged as the address you are using looks like it may be based on the MAC address. Try configuring the address so that you can use "::" in it. The MX in your SPF record is redundant as the IP specification already specify the addresses. Also, including Google's SPF record if you aren't using them as an MX may be a flag. I believe their policy will trump your policy. MX priorities are usually non-zero, you may want to try 10 instead. 

There are no logs for your interfaces. If you check soon enough, you can likely find them in the output of . You should find all that output in . If it has rotated you need to look in . Grep out the time range to a separate file that you can examine more easily. A command like should work. Look for references to in the file. You may also see a reference to repeated messages which may be close to the line that indicates the problem. Also look for references to the driver for your interface, or the manufacturer. Running the command should output error counts, and may give you a hint as to the problem in the counts the follow the errors. 

may be your friend. It will sync differences in the file systems. Use the one file system option to limit the copy to the partition being copied. Piping to should also work, but rsync will do the least I/O. For Ghost-like backups I like Mondo which creates bootable recovery disk. 

On your servers use virtualhost definitions in your web server to define the available websites. There is no siginificant penalty for defining unused virtual hosts, so you could set up sites on several servers but only provide DNS records for one. If you want to handle domains like foo.com which require more than one DNS record. You need to use A records. There are a couple of ways to do this. Have an A record with the same address as the host that serves the domain. 

Unless you have a high volume of email you shouldn't need a dedicated email server. It is common to dedicate a domain name like or to email. This can co-exist on the same server as the web service using the domain. Use of dedicated domains in this manner is a sign of professionalism. A few things you should consider: 

When connecting to clients networks, you should firewall their networks. At a minimum you will need to ensure their networks don't interconnect. This applies even if their addresses don't overlap. Routing rules will need to be across your network. IPv6 addressing avoids this problem. Each device which can connect off of the LAN will have a unique Internet address. IPv6 private network addresses have been deprecated. NAT is not required except to connect to IPv4 addresses. Routing is much simpler. 

Using an image server allows you to serve the images from shorter requests. All the domain specific cookies get dropped. It also makes it easier to set caching policies. If the static content is on a separate server, you can set a caching policy for it without caching your dynamic content. Securing the static content is simpler as well. The server can be restricted to answering only GET requests. Referrer restrictions can be added as well, although they are easy to bypass. Log analysis is also easier as you don't need to filter or process log entries for images and other non page content. 

Yes, changing only the A record is appropriate. Depending on how fast you want the switchover to be you need to reduce the TTL on the website's A record prior to the move. This should be done well in advance of the record change so that cached records get updated. After the move you can increase the TTL to its original value. Keeping a short TTL during the move will allow you to make corrections quickly in case of error. You may want to manage the experience during the changeover. This can be done by keeping the old site up until traffic stops, or replacing it with a proxy that uses the new site. As noted, if the DNS services are provided by the existing web server provider you may need to move the DNS to a new provider. This will require updating the NS records both in DNS and with the Domain Registrar. Again, you should decrease the TTL prior to the move, and increase it after the move. I would move the DNS services before moving the site. 

PowerDNS is not designed to provide recursive results. It is intended to act only as an authoritative server for the domains it serves. This implies it will be serving domain data to other hosts. You could add the Recursor module but I would recommend only using it on intranets. It may have appropriate security to prevent it from being used for DNS amplification attacks. The easiest method to resolve your issue is to configure your file to use external servers to resolve DNS. You can use your servers from your ISPs, Google's, OpenDNS, or others. You should check that you can access your PowerDNS server on its published Nameserver address (which may not be your hosts address). Otherwise you will have to rely on your backup nameservers for local access. 

It is possible to force a command to be run on connection, which can limit what can be done with the key. There are a number of other options that can be applied including limiting the source IP for the connection. See your man page for and/or for details. For a use like this you may want to force a command to pull the deployment into the environment. The deployment can be specified in the connection request, or you may be able to pull the latest file. You may want to provide the deployments via NFS, or use scp to push or pull the build. 

The zone for reverse PTR records belong to whoever you got the IP address from. Contact them and have them add a PTR record for you. They might be willing to delegate the PTR record to you. In that case you will need a zone like X-30.254.144.168.in-addr.arpa. 

Any or all of these responses may be cached by the name server you are using. The first lookup is most likely a cached lookup. You may want to change the NS records for your domain on the old nameservers point to the new nameservers. This will add an additional lookup, but may get the correct nameservers responding. 

This is a chain that accept the required ICMP types. It was extracted from a Shorewall6 generated firewall. It is accessed with a rule like: 

I would expect this is for a relatively low volume case. Servers which need high volumes of reliable DNS lookups belong on a static IP address. If so, tuning TTLs shouldn't be that important. It is difficult to give good advice with partial information. The following responses have worked for me. As I only have limited information, the answer may not be appropriate in your case. In choosing a TTL for this case, I would look at the TTL on the source of the IP address. If you are using DHCP with a lease of over a minute or two a TTL of 10 seconds could be extreme. If you are providing an internal DYNDNS service for DHCP clients, a TTL half or a quarter of the Lease time. Your TTL for an externally facing service getting its address dynamically, you may want a shorter TTL. Factors to consider will be where the data will be cached, how frequently address changes will occur, and how important that there be no discontinuity of service. You should also consider where misdirected connections may land. Internal to the organization or dead addresses are likely less of a concern, than an server on the Internet that you have no control of. EDIT:: Whatever TTL you specify some DNS servers will ignore it an cache for their own TTL. I believe this is more likely to occur with shorter TTLs. A few years ago there was a rash of botnets using fast-flux DNS to try to avoid detection. I did see reports that ignoring short TTLs and caching for a longer time was one approach used to deal with these servers. You also need to deal with negative caching. Bind 9 uses the minimum TTL as the negative cache period. (In your case, this appears to be 10h40, which is far longer than your positive TTL.) For a dynamic service you likely want them the same. I would expect your clients to fall into three classes (who may have different needs): 

After the first full backup, file transfers appear to be similar to those required for an incremental backup. More directory information is transfered. Incremental backups only pick up new and modified files since the last full backup. You need period full backups to track file and directory deletions. The transfer overheads are similar after the first full backup. I just setup backuppc myself. Rsync has the nice feature that if both systems already have a file, it doesn't need to be transferred. There is a long period at the start of the backup where the information to determine which files need to be transferred is determined. The first full backup will transfer the whole file system. From my review of the documentation 1% of the files are transferred and compared to ensure pool is OK. Rsync uses modification time to determine if the file has changed. It may use other criteria as well. Backuppc uses the same criteria. Full backups build a full directory tree so file deletions are fully tracked. After the first full backup the transfer volumes are significantly lower. The backup speed seems similar to that of an Incremental backup. Incremental backups don't record the information required to determine if a file has been deleted. They appear to have a relatively sparse directory tree sufficient to hold the new and modified files. EDIT: I triggered a full backup on a system with only an incremental backup of most files. It ran very quickly and linked most files as same, rather than copying them. This is what I expected. Network load was significantly less than for the initial transfer. 

The RFC clearly states the list of codes is extensible. The Value of the first digit is important. Unknown status codes are supposed to be treated as X00 where X is the first digit of the status code. 

TCP is only required, and usually only used when a long response is required. There can be negative impacts. Zone transfers are done over TCP as they are large, and need to be reliable. Not allowing TCP from untrusted servers is one way to ensure that only small answers are given. With the introduction of signed DNS answers, there has been a requirement for loosening of the 512 byte limit to UPD answers. EDNS0 provides the mechanism for longer UDP responses. Failure to allow DNS over TCP is highly likely to break an secure DNS implementation. It is perfectly possible to run a DNS server which only has UDP port 53 open to the Internet. TCP access to DNS peers is required, but this is a small list of hosts. There is a newer RFC596 that now requires TCP for a full DNS implementation. This is aimed at implementors. The documents specifically does not address operators, but warn that not allowing TCP can result in a number of failure scenarios. It details a wide variety of failures that can result if DNS over TCP is not supported. There have been discussions of using TCP to prevent DNS amplification attacks. TCP has its own denial of service risks, but distribution is more difficult. 

I wouldn't recommend Skype on a secure LAN. It requires a mostly open firewall configuration. All ephemeral ports, and few others need to be open. Incoming traffic needs to be allowed to the PC running Skype. If required, I would setup a separate LAN segment with uPNP (PMP) enabled. I did go through the exercise of figuring out what I needed to do when my wife was traveling. See my blog entry on firewalling Google-Chat and Skype. 

You will encounter this problem with any domain whose servers are hit by a DDOS attack. Having the servers for your own domain hit by a DDOS is more likely to be visible and traceable. Setting up one or more caching name servers on your own network and configuring your computers to use those for DNS resolution should resolve the problem unless the DDOS is extremely prolonged. I use dnsmasq on a linux server, and have run it on an OpenWRT router. It will also read values from and/or other files in the same format. It your external DNS host supports it, you could run a local bind server as a hidden slave. This would provide the best protection you could get without hosting DNS locally. Alternatively, you could run bind as your local caching name server.