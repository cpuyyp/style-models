Theoretically speaking, more data leads to better model. However, in practice, more features often leads to the difficulty of model training. Assuming there are 30 "main" features of your dataset. Feature set A contains 20 "main" features, so it might be easy (20 out of 26) for one "main" feature to be "chosen" and "trained", under certain hyperparameters (in your case, 100 trees). When it comes to feature set B, which contains all "main" features, it's hard (30 out of 96) for one "main" feature to be chosen, and it's harder when there's only 100 trees (cause there are 66 "minor" features which should not be trained, relatively). That's what we called "under-fitting". Back to your question, when the model is under-fitting, if we're lucky that model trained on feature set B (namely model B) contains all these 30 "main" features, the model will be good, and we might not able to find out it's under-fitting. But in most cases, we're not that lucky, model can be ruined with all 66 "minor" features, relatively. In my ML practice, I'll try more training iterations when more samples come, and try a more complex model when more features come, with the control of over-fitting/under-fitting. 

Data augmentation is very standard for annotated image datasets for tasks like image labelling. Images are flipped, rotated, pixelated and so on, to add more training data and make the system robust to noise and not overfit on irrelevant features. How are for example speech recognition training datasets pre-processed? Are they augmented with background noise or other mutations like image datasets are? Has there been any work in this direction? 

the instances where "capacity" occurs, then "capacity" can be guessed to be a label of number-like things. Even the nature of these relationships (eg, whether it should be before, after, capitalised) could be learnt from existing known labels. This can be done with Google BigQuery using the public tri-gram sample data. But I assume that's out of scope and impractical here. A practical approach, might aim to classify these strings as , or or even more classes, and then do human review of the latter. (There are some very tricky cases, for example, "capacity" is numerical, but "capacity type" would not be.) As a starting list you can use: wiktionary:Category:en:Units_of_measure wiktionary:Category:Symbols_for_SI_units wiktionary:Category:en:Mathematics Note that they are in the singular. For your domain considering finding other such lists. If a string is a full match with one of those labels, you can consider it a . Your next concern is tolerant, fuzzy matching. You could treat '.' as a wildcard (so "in*" matches "inch"), or find the actual abbreviations of units like "inch". These you can label as and then review. Likewise if the word is simply contained in the string, eg "arch length" contains "length". I think once you have done this you can make some refinements and add some special cases. Without seeing even a sample of your data, it is difficult to say more. If most of your strings are numerical units, then it may be easier to identify categorical units instead. 

The short answer is yes. While your classes are imbalanced, model will be more likely to "learn" class 2 during the training, especially when it comes to mini-batch updates. (even though each mini-batch is unlikely to have only class 2 samples) The common solution is to use weighted loss function or feed weighted samples to mini-batch. In my practice, I always use weighted samples if I have sufficient data, even when my classes are not that skewed. Weights always help, at least it never ruins the model. 

As your network is working without dropout, I think your problem is about how many epoches you run. In your code, it seems that only one epoch will be run. With dropout enabled, each neuron has 50% percent (for example) chance to be activated. Maybe there are some un-trained neurons in your network, which ruin your accuracy. I think it is worth trying more epoches. In my experience, 100 epoches is always a good start. 

If you look at the derivatives of the function (slopes on the graph), the gradient is either 1 or 0. In this case we do not have the vanishing gradient problem or the exploding problem. And since the general trend in neural networks has been deeper and deeper architectures ReLU became the choice of activation. Hope this helps. 

Yes, what you are seeing is a classic case of overfitting. You stated that you use a linear model such as logistic regression. To regularize these types of models, usually L1 and/or L2 regularization is applied. L1 regularization is simply $||W||_1$ and L2 is $||W||_2^2$ usually. Another method is to alter the labels of the model in a specific way, which is a method of regularization I created (shameless plug). Here is the link to the paper: $URL$ Hope this helps. 

Even though the answer in reality is always or , you can make your class attributes not labels but floating point numbers, ie 1.0 and 0.0 (or 100 and 0). That is, you can frame it is a regression problem, not classification problem. Then the predicted output will likewise be numbers on that spectrum, ie probabilities not labels. Your current implementation is essentially equivalent to a regression model with a threshold of 0.5. With such an output, you or your can client can define a threshold that is acceptable (eg 0.3). Of course there will be more false positives then, but for some applications, like detecting cancer, that is optimal. Oversampling (the positives) or undersampling (the negatives) are also ways to fix this, but must be done thoughtfully, can sacrifice accuracy, and still sacrifices the control to move the threshold after creating the data and training. 

The installation documentation shows that the easiest way to run a brat server on Windows is in a virtual machine running a UNIX-like operating system such as Ubuntu. Also, there's an issue (still opening) about running brat server on windows. I think you're on your own right now. 

8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset. This blog shows a common workflow dealing with imbalanced class issue. Class Imbalance Problem in Data Mining: Review. This paper compares several algorithms created for solving the class imbalance problem. 

As you have commented, you are concerning about over-fitting. In fact, cross validation will help to weaken over-fitting, but it can't eliminate over-fitting. Model scoring on TRAIN dataset sometimes exceeds scoring on TEST dataset. Here are some examples I can find: 

It should return a function . (That function returns a number.) Essentially we are pulling out a dimension or two. In theory of course it gives us nothing, it just postpones some calculation. (In practice this small specialised object is very useful in some production applications.) 

Data augmentation techniques for image data and audio data (eg speech recognition) have proven successful and are now common. Are there libraries or techniques for augmenting text data? For example: 

Note: this is more frequently called canonicalisation than harmonisation. URLify and URL-safe are also terms that can help you in your search. whitespace Combining the built-in functions + is a solid way to canonicalise intermediate and trailing whitespace, including tabs, carriage returns and so on. non-ASCII chars ASCIIification is actually somewhat contrary to canonicalisation. That is, you should think hard about whether you really want to be converting 'München' to 'muenchen', rather than 'muenchen' to proper 'München'. If you do want to go in that direction, you should give more details on the use case and the desired conversions. Here are some examples of possible input: 

Currently I need one data mining platform (or solution) meets such requirements: Data Source I can import data from: CSV, MySQL, PostgreSQL, HDFS. Optional: NoSQL, MongoDB. Interface I can assemble models and datas with drag & drops (like Weka). Algorithms I can model with common machine learning algorithms: Logistic Regression, Random Forest, SVM, word2vec, etc. It would be awesome if it had deep learning support. Optional: PMML Customization I can implement my own algorithms (under certain standard). Optional: I can implement my own APIs (under certain standard). Programming language is not limited. Visualization Simple visualization is okay. License Commercial solution is acceptable, open-source solution would be better (I can do some tweaks in order to meet my requirement). There are two software I've investigated: Weka and SPSS Modeler. Weka doesn't support HDFS, while SPSS Modeler doesn't support customized algorithms. 

Is there a machine learning framework that supports partial evaluation? For example: We train on . Today we call 

Converting to ASCII will be lossy and there are some decisions to make, often dependent on the source and target language. If you define the likely input, scope and desired behaviour then others can make recommendations. 

There are many good libraries for identifying number-like values, but identifying corresponding fieldnames is trickier and likely very problem-specific. A purely data driven approach might look for co-occurrences with numbers, for example: 

Most problems have a curve whereby the results improve as data are added but level off at some point. Are there research papers or industry results that discuss the correlation between data set size and prediction accuracy for natural language identification? 

But the main problem you have is that a number of samples are less than the amount of observations you have. I would recommend a different approach to dimensionality reduction. Autoencoders would be my recommendation to you. Autoencoders can be trained in an iterative fashion, circumventing your memory issue, and can learn more complicated projections than PCA (which is a linear transform). In case you want a linear projection you can have an autoencoder with one hidden layer, and the solution found by the neural network will be equal to the solution found by PCA. Here are a couple of links you will find helpful: 

I re-implemented your set-up in python using keras. I used a hidden layer size of 25, and all my activations were sigmoid's. I got to an accuracy of 99.88%. Try running your algorithm for a greater amount of epochs. Use binary cross entropy as the loss function and try decreasing the learning rate of your gradient descent algorithm. This should help increase your accuracy. My only explanation for the poor performance would be that you are getting stuck at a local minimum, if that is the case different initiations of your weights should fix that problem. 

Note that the parameter is optional (Obviously it's not a lib, but they offer a Python client and their free tier is generous enough.) 

If you are seeking a working solution, I know of an API that supports many languages, including Russian: indico.io Text Analysis sentiment() 

Sentiment analysis, sentiment detection and opinion mining all cover a set of problems, and can generally be considered to be one and the same. The term sentiment analysis seems to be more popular in the press and in industry. In practice, as of 2015, it is mostly about giving a score, to text, between 0.0 and 1.0, for negative to positive sentiment. (Strictly speaking that is only a subproblem and one of many possible formulations.) But whenever any of the terms is used, you should define or ask for definition of the exact problem.