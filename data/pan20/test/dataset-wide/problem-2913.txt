You get a point every time you get the answer right. (In the tails case, you can win 2 points; in heads, only 1.) Solution: guess tails. 50% of the time you will get 0 points, 50% of the time you'll get 2, for a net payoff of 1 point. (If you guess heads, your expected payoff is 1/2.) You get a point if you are consistently right; half a point if you're right half the time; no points if you are never right. Solution: it doesn't matter what you guess. Expected payoff is 1/2 regardless of what you say. 

Whether it is forbidden, required, or somewhere in between depends entirely on your cost function, especially how you view the number and quality of future lives impacting the determination of the greater good. With certain choices (e.g. those who are here now are privileged with respect to new lives), the species may end up--if you follow it to its logical conclusion--courting extinction. For any flavor of utilitarianism that does not risk extinction of humanity, parenthood is a good course of action for many. (I am not aware of a systematic review of the long-term evolutionary fitness obtained by following various ethical systems. This might be nice to know, though.) 

Not all logical consequences are immediately apparent upon setting up some premises. Pointing out consequences can be a "reason". You have just done this for the inductive method ("it's as true as anything can be about the physical world, and here's why"). (One might also want to know whether to apply it in some particular context.) 

While Ockham's Razor is a simple statement of a principle that is important in philosophy and science, in that simplified form it is not actually deployed all that widely in the many of the sciences. First, Ockham's Razor doesn't tell you to ignore other hypotheses, just to take one with the fewest assumptions (all else being approximately equal). But you absolutely have to investigate other hypotheses to make sure that they're approximately equal. This leads to questions like: does this improvement in our ability to explain our observations warrant that many additional assumptions? It turns out that this question is also faced when your "assumptions" are how many variables to include in some model that is fit to quantitative data. Here, at least, there are reasonably sensible statistical or comparative tests that will tell you which model is better supported. Secondly, in many cases, especially in biology, one can have prior information that, for instance gene regulatory networks tend to be complex. So if you are interested in, say, how a fly decides to build a wing, and you find a gene that is needed for wings, your model might be . It's the simplest model. But your priors indicate that it probably looks a lot more like 

You need more constraints for this to be a meaningful exercise. Let's just take it abstractly: here is a universe with one point: . Here is a universe with two points: . Do and have anything at all to do with each other in any way? Maybe not. Maybe they're effectively in separate existences but we're aware of both. What does "next to" even mean when you just have a and ? So to get started you need to define some sort of distance metric to make "next to" even make sense, and say something else about the properties of these points to justify why you care what "next to" means. For instance, in the space of mathematical physical laws, if you have only , and you add , it hardly seems meaningful to even consider proximity between these two equations. 

I can smell peach-flavored oolong tea and tell you what it is; a machine cannot (not yet, anyway). This has nothing to do with mind-body duality. We simply haven't built chemosensors as diverse and sensitive as those in our noses. Auras are, logically, just like the smell of peach-flavored oolong tea (except for not having good evidence that they actually exist). 

Actually, it is the ball, earth, air, etc. that is doing the computation! It's doing a really messy computation involving octillions (or more) of elastic collisions and all sorts of changing rotational, vibrational, electronic, and other energy states. Fortunately, these messy stochastic calculations are approximated well by simpler calculations that we can do on a computer. And, what do you mean by "actually works"? If your mathematical model is indistinguishable from what "actually happens", in what sense do you not know how it "actually works"? That's much of the point of searching for natural laws--to find regularities that we can express with such precision that we can treat the actual universe as an instantiation of our model (or with differences small enough to not matter to us). 

At a bare minimum, you only really need predictability according to any function. This need not be the identity function which says that whatever you measured before will be the same later, or any other particular function. There are some things that will sink the attempt: 

We do not yet know whether the brain is "understandable" in the sense that we accept now. If the brain operates mostly on the basis of several dozen key principles that can be expressed mathematically or with some other formalism, then it will be understandable in the conventional sense. On the other hand, if it is in fact tens of thousands of formulas with no apparent organizing principle, then we probably won't say we understand it in the sense we accept now--but we may be increasingly willing to hand over the details to computers. There's every reason to believe that the brain is understandable in the sense that we can predict what it will do given adequate knowledge of its present state and inputs over time. Some aspects may be chaotic or random, but we know how to characterize such things. But what's not clear is whether "mind" will be like "life"; we generally have a sense that we understand what "life" is since we can break it down into a few basic principles--replication, metabolism, etc.--which in turn can be broken down into a few basic principles and so on. Even though maintaining a living organism is incredibly complicated and we are missing very many of the details, we're mostly comfortable saying that we understand what life is and how something is alive vs. not alive. There is no particular reason why the brain must be understandable in the same kind of hierarchical fashion, however. We can even build things that we can't understand. For instance, rather frighteningly, nobody really understands any more how the electrical grid in the United States behaves in the face of a large failure; this is because of exactly the feared brain-problem: thousands of local decisions about how to handle failure and thousands of local structures contribute to the stability or lack thereof of the whole system. But in the longer run--assuming we maintain an advanced civilization--we don't really need to worry about whether we understand correctly or whether we plus computers together understand (in the high-fidelity-of-prediction sense). 

It is hard to say for sure, since we generally don't set goals for what an understanding looks like, but instead say in retrospect whether we've got a good enough one to satisfy us. (This may involve both advancing our knowledge and changing our minds about what is satisfactory.) However, I would say that most people feel we have a satisfactory theory of life--that is, what the essential difference is between the living and non-living, and an extensive though far from complete understanding of how "living" works. Using this as an analogy, but with the difficulty that we have very few examples of different kinds of consciousness, I am rather doubtful that we'll gain this understanding until we can create a synthetic consciousness (assuming that this is possible and that we ever can) and probe the boundaries of it by altering it. When our theories of what happens to synthetic and our own consciousnesses under various conditions seem to produce reasonably reliable results in many scenarios, we'll probably start to say we understand it (as we do in many other fields involving the study of something really complicated). Alternatively, we might find that there are structural parallels between humans and other animals and that what we call "consciousness" seems inseparable from those (presumably neural) structures; and eventually through work on animal models come to understand the boundaries and properties of consciousness (relying heavily on the architectural similarity between their brains and ours, as we do on the aliveness of our cells vs. yeast). Not a very satisfying answer, but I do not see that we could say much more presently. 

The teleological argument is effectively dead. The last gasp at it was by William Dembski and Michael Behe with "irreducible complexity" (the intellectual core of the intelligent design movement), and they simply failed to understand the actual problem and/or came up with handwaving to state that certain things were impossible, when in fact they were not only possible but there were examples of them. That evolution provides the mechanism to produce all the complexity of life seen today is no longer in serious doubt; and that simple physical laws suffice to produce all the complexity of the universe is also no longer in serious doubt. The only area not completely nailed down is fine-tuning of universal constants, and that makes for an incredibly weak teleological argument since all we know about reality with different constants is that our familiar physics doesn't work. We cannot predict whether there'd be some other complex physical reality admitting evolution, so we can't tell if the numbers are actually finely tuned and thus whether we should be surprised by them. I would go so far as to say that at this point there cannot be any effective challenges to these critiques without a radical re-evaluation of our scientific knowledge. That is to say, such challenges will not come soon, nor will they come through philosophy initially. If there are unexpectedly large flaws in the science, then there may be some wiggle room in which to attempt another teleological argument, but finding such flaws is a scientific endeavor. 

The prisoner thinks: "Wait, that's ridiculous, he just told me I'd die today, and I know the date. So of course I am sure of the date. That means I can't be killed, because I'm sure! Yay! I won't die today! Wait, but if I know I can't die today and they kill me anyway...uh-oh...." Pulling the paradox out via induction just makes it harder to notice the paradox (of the standard self-referential variety). 

However, the question seems to be about the colloquial definition of intentionality; if so the following answer applies: Because we cannot (presently, anyway) create consciousnesses with varying compositions, our formulations are necessarily mostly descriptive rather than prescriptive. It is certainly the case that human consciousness is intentional, and indeed we find intention-like behavior (such as the coming together as a slug or eventual transformation into a fruiting body of the slime-mold Dictyostelium) in creatures that we generally do not wish to ascribe consciousness to. So it seems relatively safe to say that human consciousness is intentional, and then we can leave it comfortably vague what we mean by "intentional" and "consciousness"; the truth or falsity of the claim will not hinge on small details of exactly what we mean. The rest of the questions can just be read out from this underlying insight, if one takes a physicalist perspective. Those systems that display intentionality are made of matter, so yes, of course, under some circumstances matter can be intentional. Matter can also induce pressure, can flow with viscosity, can store funny cat GIFs, and do all sorts of other emergent or bulk tasks. The philosophically contentious claim, if any, is that there's nothing special about intention beyond what the properties of matter can cover. But that's what physicalists are committed to: there's nothing special about anything, whether it be qualia or intention or consciousness or anything else. Indeed, that some matter can be conscious is irrelevant to the physicalist's stance on intention. Intention clearly happens, in some sense, and in those senses in which it does, it is necessarily physical. End of story! Consciousness need not make an appearance. With regards to evolution, we need not concern ourselves with consciousness or intention. For some matter we can describe a pressure; for most we can describe a pressure. Does it follow that evolution has a pressure or a single temperature? Of course not! Those properties don't describe evolution, and so they are inappropriate to use. Likewise, there is no need to ascribe intention or consciousness to evolution. It is described plenty well enough on its own terms. Instead, you should proceed the other way around: decide on a sufficiently precise definition of intention and/or consciousness, and then see if evolution has the requisite properties. If you do not like the answer, maybe you got the definition wrong. All of this is greatly complicated by the fact that we don't understand the biological (or computational!) basis of intention or consciousness very well at all. Usually "intent" implies some sort of underlying model of the external world, some sort of goal, and some sort of computation or thought process that acts upon the model to come closer to the goal (followed presumably by actions that make the real world more closely align with that model). Evolution doesn't do this. But then again, neither does Dictyostelium. Evolution does run a kind of optimization algorithm, but unless you say that every optimization algorithm "intends" to perform its optimization, calling it intentional is quite a stretch. And, regardless, one needn't care about consciousness in order to decide whether it's intentional or not. 

This is simple logic. The difficult steps are not the logic (maybe--I managed to mess it up in my first attempt at an answer!), but that English has many different ways to say the same thing, but whose different ways imply different things. 

Must? Of course not. A constitution is not required to even guarantee an avoidance of slaughter of the entire population (should that population consist predominantly of a suicidal cult, for example). But there are certain potentially desirable properties of political institutions, and one can be time equality, in that the constitution does not play favorites for individuals of any particular time. (The U.S. constitution is not very good at this, incidentally.) Allowing free expression (at least to the extent that the authors had) is a special case of the time equality principle. Another is the right to revoke, rewrite, and replace the constitution, provided that the citizens of the country go through at least as much ardor and care as they did for the first constitution. (This right of replacement is very rare in actual constitutions, though there is almost always a much more limited amendment process.) 

The answer to almost all questions of the form, "What if I perform some small favor for someone who needs it--is this morally wrong?" is no. Human societies perform better when people perform small favors for each other; it is what one would wish to be the case as a general rule; it is advocated by most every major religion; and so on. It's very hard to make a compelling case that it's actually wrong. For example, the bus obviously isn't serving that woman so well, and she has decided the risks are worth the advantage. If you think she's made an error, you can talk to her about it, but you can't know in advance. The converse question, are you morally required to help, is also usually answered "no" according to most systems (though it is yes according to some flavors of consequentialism that ignore the consequences of the moral system on the followers given human nature).