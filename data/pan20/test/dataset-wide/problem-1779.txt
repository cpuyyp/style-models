You almost got it right. K* names cause a service to be stopped. S* names cause the service to be started. rc6.d is for controlling subsystems when entering runlevel 6, which means "reboot". You want to have the service started at runlevels 3 (normal multiuser) and 5 (multiuser + X). Make /etc/rc.3d/S90sshd and /etc/rc5.d/S90sshd symlinks to /etc/init.d/sshd This should cause sshd to be started on system boot. 

After replacing two HDDs you again have errors indicating a hardware problem with a disk. This can mean that you are just plain unlucky, or that there's some other problem, which exhibits itself as a disk failure. This could be: 

Definitely large RAM, speed be damned. Access to random data for RAM technology from XX century '90 is below 100 ns. That's using practically ancient chips that won't even physically fit into anything borderline contemporary. Access to random data for cutting edge 15k rpm hard drives is in measured in miliseconds. 100 ns is 10 000 times shorter (nano -> micro -> milli) than 1 ms. Current RAM is faster, and HDD needs several milliseconds to access data. I couldn't care less if my RAM was 50 000 faster or only 30 000 times faster than HDD, if I could get more. 

Connect to HMC or ASMI and see what they say. You will get error description there, which may be comprehensible (e.g. a fan died somewhere) or utterly cryptic ("there's no detailed description for error code XXXXX"). If it's not something that you can resolve (e.g. "No AC detected for PSU 1", resolvable by re-plugging a loose cable), then call support. Anyhow, there is no error code on the LCD display, so it seems that machine is currently happily running. Edit: Decoding of the panel hieroglyphs: -- Function Code 01. Function description: 

If you have pro sysadmins on standby, who will have to get the manure out of the fan, then ask them. The easier job you make for them, the less will they cost you (if they work on a per hour rate), or will be more productive. Beside that, I would recommend any distribution with a long-time support. 

Ruby 1.8.7 is included in RHEL 6. If you run an older version, then you could try to download the corresponding srpm and recompile. Edit: From my workstation: 

If you really insist on having a single system disk, it doesn't change the layout. On (each of) the system disks I'd have a separate partition for /boot filesystem. The rest of the disk(s) I'd made into an LVM physical volume. I like to have OS filesystems that can fill up on separate logical volumes (/tmp, /var, /opt if something writes logs in there). So that would result in /boot (mirrored, no LVM) and /, /tmp, var and possibly /opt filesystems, each on a separate logical volume on a mirrored disk. On each of the other disks I'd create a single partition of the type Linux raid and create appropriate RAID arrays (one RAID 10, one RAID 0). On each of the arrays I'd create a single partition of the type Linux LVM and make two separate volume groups, one for redundant and one for non-redundant data. Then for each file system you plan to make I'd make a logical volume. In each volume group I would recommend to leave some space unused, so that you can do an LVM snapshot and do fsck of a file system without bringing the server down. I would also disable automatic fsck of all filesystems (tune2fs -i 0 -c 0 /device/name). Rationale 1) Mirroring of the OS disks. Failure of the system HDD brings down the whole machine. Your data is protected, but your production stops until you can bring a replacement disk and reinstall / restore the OS. In a production environment it is usually cheaper to have one more disk installed. 2) Partitioning disks for RAID arrays. All the servers I use have partition tables. You may use just whole disks as RAID / LVM volumes, but then you end up with some machines that have partition tables (stuff on /dev/sdX1) and some, which don't (stuff on /dev/sdX). In case of a failure and need for recovery under stress I like to have one variable less in the environment. 3) LVM on the RAID arrays LVM gives two advantages: easy changes of filesystem sizes and ability to fsck filesystems without bringing the whole server down. Silent data corruption is possible and happens. Checking for it may save you a lot of excitement. 4) tune2fs -i 0 -c 0 Having a surprise fsck of a large filesystem after a reboot is a time-consuming and nerve-whacking affair. Disable it and do regular fscks of LVM snapshots of filesystems. A question: /opt/backup is where you plan to keep backups of your production environment? Don't. Have the backups somewhere else from the machine. A malicious program, a mis-spelled command (e.g. ) or some water spilled in / flooding the wrong place will leave you without your system and with no backups. If all else fails, have two external USB disks for backups, still better than a partition inside the same box. 

I don't know if it's still relevant. I didn't find anything similar on Red Hat's site. I think it is safe to make good backup (after all your machine is locking up, which may eventually lead to problems with data) and try. Worst case, the machine will not boot up. Best case, you'll find a combination of kernel parameters that solves the problem. If it affects a recent version of CentOS I'd open a bug on Red Hat's Bugzilla. It may happen that you'll help solve some problem in the Linux kernel :). 

It looks that 1.8.7 made it into the final version, it could be absent in beta. If you have Red Hat subscription you should be able to get it. Otherwise your best bet is to wait for CentOs, I think. 

A knee-jerk reaction: check (with telnet) if your ISP doesn't block outgoing mail ports (25 and 465). Make sure your firewall allows outgoing connections to TCP ports 25 and 465. 

Add a cron job writing a time stamp into the log every n (possibly n == 1) minutes? Will not help figure the reason, but it could help figure out if shutdown time is correlated with the time you leave your machine or it always turns off at the same time. Another idea: move /sbin/shutdown to /sbin/shutdown.bin and create a shell script /sbin/shutdown which tells you what called shutdown, (find parent process id from $PPID, run ps auxwww to find out who's messing with the system, save pstree output and generally call the cavalry). This way you should be able to catch the offender red handed. 

In disk I/O there is a thing called elevator. The disk subsytem tries to avoid thrashing the disk head all over the platters. It will re-order I/O requests (when not prohibitted e.g. by a barrier) so that the head will be moving from the inside of the disk to the outside, and back, performing requested I/Os on the way. Second thing is I/O request merging. If there are many requests within a short time window, which access different portions of the file, the I/O subsystem will try and get all the data in one go, instead of issuing several disjointed requests. As far as tuning goes. If you are the application writer, there's a lot you can do. You can issue large, sequential I/Os, whenever you can and use fsync() et.al. when you need to be sure that the data is on the platters. If you are a sysadmin, and you absolutely know, that the data requests of 2 apps leapfrog, and they try to read files sequentially (e.g. you have 2 DVDs being transcoded in parallel), then yes, increasing readahead should help. Otherwise you'd need to take a look at your I/O patterns and sizes, consider your RAID level (if any) and other factors, before doing any tuning. Look at what your real bottlenecks are, before you start tuning, it may be difficult to guess, what's really limiting your system. 

For the scanner to gain anyting (other than creating a DOS attack) he has to receive the packets. If the scan appears to origin from a locally attached subnet, then he cannot be anywhere on a route between the source and the origin, because there's nothing in-between the CentOS box and 10.51.1.15. You could: 

On a Linux system 'locate my.cnf" will be the fastest solution. If there are several my.cnf files, all looking likely (e.g. in , , , etc.), then you can run to see where MySQL binary tries to find its configuration file, but I do think that's an overkill. 

Pay the fee and have the DNS records updated. Change the provider to one who doesn't charge you for such minor service. Set up your own DNS servers. 

You might revise step 1. You are limiting your customer base. You could limit the set of distributions supported, but I wouldn't go for just one. For application distribution I would provide packages in a format native to the OS of the nodes. Application update would be a simple package update then, which is as easy on sysadmin as it can be. If you want to prepare a whole environment, from the OS level up, you have 2 options. You might prepare a VM file in a chosen format (or formats). Then you would have to prepare the whole environment, from the OS up. Or you might prepare an automated-install system/image (using the distributions native non-interactive installation system, or combine it with e.g. xCat), which would deploy a complete OS, configure it, and then load your application on top.