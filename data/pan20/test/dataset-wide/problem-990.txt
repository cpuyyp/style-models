My recommendation would be to set up a single MongoDB deployment (ideally starting with a replica set or hosted service with high availability & data redundancy): 

The issue is that you are using a monotonically increasing shard key. This will result in all inserts targeting the single shard (aka a "hot shard") that currently has the chunk range representing the highest shard key value. Data will eventually be rebalanced to other shards, but this is not an effective use of sharding for write scaling. You need to choose a more appropriate shard key. If your use case does not require range queries, you could also consider using a hashed shard key on the field. See Hashed vs Ranged Sharding for an illustration of the expected outcomes. 

This will depend on your backup strategy. In the general sense, you would not be taking a full backup everything a single value changes using command-line tools like . However, a typical deployment uses a replica set which provides for data redundancy and failover across multiple servers. Replication is based on an (operations log) which contains a rolling history of all changes to data that gets replicated to all members of the replica set. Administrative tools like the MongoDB Management Service (MMS) can also use the oplog to create continuous backups and do point-in-time restores. 

As per the MongoDB documentation, you should generally leave the WiredTiger cache at the default size (or possibly even decrease this). You can always try to experiment with tuning the cache size for your workload and deployment, but setting this too large may adversely affect performance. 

MongoDB 3.2 and earlier require downtime and a coordinated restart of your deployment and application to enable access control: all clients and members of a deployment must use authentication once enabled. There is a localhost exception that allows you to create the first user on the database after authentication is enabled. With planning this can be a relatively quick process, but to minimize potential downtime I strongly recommend first testing in a representative staging environment. Enabling access control and authentication is an obvious security measure but still leaves you vulnerable to other possible attacks. For example, you should also Configure TLS/SSL to secure your network communication and restrict remote network access via firewall or perhaps a VPN/VPC between your application servers and your MongoDB deployment. For a full list of security measures and links to relevant tutorials, see the Security Checklist in the MongoDB manual. There are several steps that you can test in a staging environment to help ensure the transition goes smoothly, including: 

If you follow the upgrade process it is designed to avoid downtime -- for example using to gracefully step down the replica set primaries rather than abruptly triggering an election by shutting down the server. Of course there is no guarantee if something doesn't go to plan, but you should test your upgrade in a QA/staging environment before upgrading production. 

Re-start using the wiredTiger storage engine Run: You should see messages about restoring users & roles, for example: 

No. In order to achieve this, the existing data would have to be migrated to another shard. The config servers and do not monitor whether a shard is read-only or attempt to work around this case. When write operations are directed to that shard, they will generate exceptions. If you do have an outage for a shard, one of the steps you should take is to disable the balancer. This will prevent attempts to migrate or rebalance data while you work on restoring that shard. 

This isn't answerable without the context of a specific query, example document, and the full explain output. However, you could run some benchmarks in your own environment for the same query with and without projection to compare the outcome. If your projection is adding significant overhead to the overall query execution time (processing and transferring results), this may be a strong hint that your data model could be improved. If it's not clear why a query is slow, it would be best to post a new question with specific details to investigate. 

The role does not grant access to run the command. Your command is returning an authorization error and you are then referencing a non-existent property which results in the "no results to show" message. You can confirm this by running , which should return something similar to the following if the current user is not authorized: 

What's actually happening according to the query is that the index is being used to return the results in sorted order via an index scan (IXSCAN). Your compound index would require an in-memory sort of the result set, which may be larger than the size of the in-memory sort buffer (32MB) as you discovered by trying to hint that index. If you can add a limit to your query so the results are smaller than 32MB, the compound index may be usable but not ideal. For your query, the ideal index to support the desired sort order (and an unlimited set of results) is a compound index including the price: 

Exporting the users Assuming you have already upgraded all of your config servers to WiredTiger, here are some steps to add the user information: 

As you've noted, MongoDB's configuration only determines which NIC IP addresses the server listens to (not the client IPs that are allowed to connect). To limit network exposure you need configure solutions at the operating system and/or network infrastructure level (eg. firewalls or VPNs). The current version of includes a link to some suggested resources depending on where your MongoDB deployment is hosted: How to Configure Allowed Hosts, Firewall, Whitelisting and Blacklisting in MongoDB. For more information, also see Hardening Network Infrastructure and the Security Checklist in the MongoDB documentation. 

The problem is that your query object uses the same key () twice, so the second value is overwriting the first. Most languages do not support duplicate keys in a standard object / hash / dictionary representation. The outcome is more evident if you evaluate your query object in the shell: 

You may note that I've said there is currently no server-side support for populating data. There is actually a new operator being added to the Aggregation Framework in the MongoDB 3.1.x development cycle (which will culminate in the eventual production release series for MongoDB 3.2). For more information see: SERVER-19095 in the MongoDB Jira issue tracker. 

Repair and backup processes will (by design) ignore deleted data. Based on the filenames referenced in your question, you are using the MMAP storage engine. When a record is deleted in MMAP the first several bytes of the record are overwritten and the preallocated storage space is marked as available for re-use on the free list. There is a possibility you could write a custom script to try to recover useful bytes from deleted records, but there is no supported "undo" process for doing so. Hopefully you took a backup of your data files before running . Repairing MMAP data files results in a compete rebuild of your data files and would free up preallocated space used by deleted collections and documents. In any case where you are trying to recover or repair database files from a standalone MongoDB deployment, stopping your MongoDB instance and taking a file copy backup of your data files is a recommended starting point. 

As per the definitions you've quoted in your question description, assertions are logical tests that trigger exceptions on failure. These are used within most automated testing suites as well as at runtime. In test suites assertions confirm that the results of a function call or data update are as expected and are often set up as macros indicating the comparison type (eg. , , ). In the runtime context assertions are safety checks that cause the current operation or process to abort when it is unsafe or impossible to continue. The usage and behaviour of assertions will vary by product. For the specific case of the MongoDB codebase, there are several types of runtime assertions used. Quoting from the MongoDB Server Exception Architecture documentation these are (as at MongoDB 3.4): 

By definition, all data-bearing members of a replica set maintain the same data set so the write traffic is roughly similar for secondaries - but not necessarily identical if compared to a primary. Depending on your workload and server configuration, it's not uncommon to see secondaries with different I/O activity than the primary. Replica sets are designed to support redundancy and high availability as a key feature, with reads directed to the primary by default (aka a read preference of ). Servicing secondary reads is a lower design priority than ensuring successful replication and minimizing replication lag. One interesting caveat is that writes do not have to be applied identically on the primary and the secondaries as long as reads from a secondary always reflect a state that previously existed on the primary. In particular, writes on secondaries are applied in batches using multithreaded replication which improves the write concurrency of secondaries with the expected side effect of adding latency to secondary reads under load. If you read from secondaries you will have to deal with the possibility of eventual consistency (if there is replication lag) or intermittent latency (if the secondary is actively applying a lot of replicated writes). Replication lag can also vary depending on which secondary you are connecting to, so results may not be as your application code expects if subsequent page requests fetch data from secondaries with significantly different lag. I would amend this contra-indication warning to suggest that "secondaries will likely service reads at a similar or lesser rate than the primary". 

The command was introduced in the MongoDB 3.2 production release series, and MongoDB 3.4 replication expects this command to be available. 

If you are running any of these queries regularly against a large collection (or have concerns on performance impact), you should review index usage via explain results. In particular, negated queries and queries have some caveats on efficient index usage. 

Login to and confirm the users are correctly setup (i.e. auth with admin account, use to check). At this stage you should have: 

If you want a quick way to check if a database is actively being used, you can use the administrative command. This command provides usage statistics with collection-level granularity. For example, using the shell to check if a collection is being actively queried: 

The difference in outcome will depend on whether the target database had the unique index previously created. 

By default collection stats only include information on the collection data (which uses block-level compression rather than prefix compression). To see index details you need to provide an additional option, eg: 

You are trying to use an older version of to inspect traffic for a MongoDB server that uses a newer version of the MongoDB wire protocol. Specifically, it looks like you are using a version of older than 3.2 to inspect traffic from MongoDB 3.2 (which added the 2011 opcode for ). To fix this you should use a version of that matches your MongoDB server deployment. 

It's unlikely that you would be able to restore corrupt BSON documents into MongoDB, but there may be some nuances depending on how/where data is corrupted. In most cases invalid data will trigger an exception/assertion when the server attempts to read the BSON, but valid BSON does not necessarily mean that the field contents are as expected. To be entirely certain you would have to compare the document counts or checksums of the restored data against the original data (assuming you still have a copy other than the gzip archive). 

The average is based on the size of the documents in the collection, so it sounds like your trend over time is to insert smaller documents. The normal behaviour depends entirely on your use case ;-) 

I believe your specific chunk map error is likely due to the hashed index and sharded Map/Reduce both assuming they should create the initial chunks for an empty collection. When you create a Shard a Collection Using a Hashed Shard Key: 

The setting controls the frequency where the in-memory view is synced with the on-disk view of data (aka the "background flush interval"). By default, the is 100ms and the (background flush) happens every 60 seconds. If you adjust the too low, you can end up creating a lot of additional I/O (and reduced performance) because writes to dirty pages can no longer be effectively batched together and the same page will be re-written multiple times. If you adjust the higher, you can create I/O spikes (and reduced performance) with a large volume of changes being committed in a single batch. The and parameters are available as tuneable settings because in some cases it may make sense to adjust these (for example, to try to temporarily help an under-provisioned I/O system by reducing write spikes). In a healthy production system, it would be best to leave these settings as-is. For more information, How MongoDB’s Journaling Works is a helpful read. 

You can only use a single storage engine type per instance. There is only a single setting for a deployment, and you cannot mix & match data files from different storage engines within the same data path. However, you can use different storage engine configurations within members of a replica set (for example, mmap primary and WiredTiger secondaries). If you do this, you should be wary of differences in performance and be sure to test thoroughly in a QA/staging environment with representative workload. Another available option is to set storage-engine specific options when creating a collection. For example, you could override the instance-level defaults for WiredTiger and set different options for specific collections (i.e. higher level of compression, or perhaps no compression).