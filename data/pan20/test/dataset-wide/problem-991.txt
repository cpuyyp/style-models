The website (html5,utf8 all over) using the database displays those characters OK I can import the dump without issues with point (1) above still holding If i "fix" the dump - replacing the pairs with the char that should be there, point (1) above unravels - the browser now shows the familiar question marks on inverted background that is the familiar placeholder for "bad" utf8 sequences. The issue depends on the client: I can SELECT from mysql and have the double-encoding as in mysqldump; OTOH if i GET the web page (composed by php with charset UTF8 connection) I see regular (not doubled) UTF8 sequences. Even weirder, I can obtain correct looking results in mysql/mysqldump if I specify --default-character-set=latin1 (!!!). In this case, however what I get does not round trip cleanly as the UTF8 counterpart does. 

A server SERVER_A has some data. A server SERVER_B defines A as a linked server, and defines some views on the data. The queries are like this: 

The table has a trigger FOR DELETE, which I consider irrelevant here, and a trigger AFTER UPDATE, which reads: 

Up to now, no error, and I can see the linked server in SSMS, "Test Connection" is successful but I cannot query it. 

What is the proper way to define a linked server from the Internet, and if possible assigning a nice name to it (rather than referring to it by IP)? Should I be using SQLNCLI as a provider? 

The is for compressing the transfer, which is something you probably want. For further compression, you may use instead: 

The query may still not use the index if the regex comparison is anchored, when it considers that the results would not actually filter much (from collected statistics) and a sequential scan would perform better, e.g. when most of the rows in that table actually start with . Edit - optimising the query Your query is not optimised for large sets. Firstly, most of you filtering is based on a non-indexed comparison (). Secondly, you apply the filtering condition to the results of the join; PostgreSQL is often smart enough to translate the filter to the subquery, but it is best to construct your queries in a way that will ensure that PostgreSQL will the do the right thing. For example, filter by directly from : 

My problem is that, for the life of me, I cannot find a positive statement of support of the feature on MySQL's documentation site - or elsewhere. I am rather wary of deploying in production something unannounced (which might be experimentally present) so I hope somebody can point me to the official status of this. Edit: It turns out that the manual contains, in parts unrelated, this rather oblique statement: “…an InnoDB table with an AUTO_INCREMENT column requires at least one key where the auto-increment column is the only or leftmost column.” which suggests, but does not state, support. Edit 2: Using information from @rolandoMYSQLDBA below, I also located this additional snippet: "An AUTO_INCREMENT column must appear as the first column in an index on an InnoDB table.". I am therefore accepting his answer. And in case someone is wondering, the multiple column index thing is needed for table partitioning - the customer has auto_increment PK on all tables and they want to partition on a date column. 

I have a SQL Server 2008R2, and I want to connect remotely to a SQL Server 2012SP1, for which I am given: 

book_id is int identity PK (clustered), last_read is a datetime. The query is written with the 15 in single quotes, thus requiring a conversion, but I cannot imagine this being a big deal, because the conversion would only be done once per query. There are 6 indexes on the table, but the column last_read is not involved or included in any of them. The PK is on book_id and is nothing special. The table is very central in database, contains 50 columns and 400,000 rows. The estimated execution plan tells me: 

I'll describe right away the problem, and put details at the bottom. I am dumping a database which I am fairly sure contains (almost: a few cells have wrong, latin1 encoded data) only UTF8 data. Also I have done my utmost to ensure every part of the chain defaults/is set to UTF8. Problem: all UTF8 chars appearing in the output are screwed up as in this example: 

ie: 1 UTF8-encoded char is seen as 2 L1-octets and each one gets encoded in UTF8 again, ends up as 2 UTF-8 encoded chars. So the first question is "where is this happening"? But the really funky part is that 

I read that the change in recovery model only has effect after a backup is taken. However, if after the performing the above I do 

Switching the clearing of tables from DELETE to TRUNCATE TABLE made the deletion much faster (as obvious) but also made the subsequent data copying at least 10 times faster. Interesting to note, the recovery model for the database is SIMPLE, so it is not a matter of logs. Can someone explain why this is happening? Or maybe I am missing something and the gain in speed is caused by some other factor? 

I have a database on Microsoft SQL Server 2016, with a table of about one million rows, let's call it books. This query takes more than one minute, which is not acceptable: select * from books where publisher_id in (857413, 857317, 857316) There is a proper FK from publisher_id to the publishers table. I display the Estimated Execution Plan, and it tells me that 100% of the cost is in the "Clustered Index Scan (clustered)" on the primary key of the books table. And what worries me, it does not recommend an index at all. By looking at the query, it seems obvious that an index will help. And in fact, when I create the index the query returns results instantly. Did something go corrupt in my database, maybe statistics? Or do you believe I should nor, in general, trust what I read in the estimated execution plan? 

Edit: The accepted answer is right on the money. I had since found about mojibake through different channel and expanded my woes in a blog post. 

All of the above confirmed by examining outputs with a binary editor (even though i have LANG=en_US.UTF-8 and UTF8 char display is OK) What I would like to see, obviously, is a clean UTF8 mysql environment which displays the same independently of the client. This is really driving me up the wall. Any ideas? DETAILS: Running mysql-server-5.1.73-5.el6_6.x86_64 on a Centos6 machine. my.cnf (relevant) settings: 

This is what happens if - at some point - a UTF8-encoded character is interpreted bytewise as two latin1 characters which are then UTF8-encoded. Edit: What is happening is along the lines of: 

The most important table of my database is about 300,000 records, and growing. More than 20 tables have a FK to it. Its primary key is a number, but for historical reasons is defined as nvarchar(6) which is obviously inefficient (and ugly). Records are sometimes deleted from the record, so the primary key is approaching 999,999 and I must change it. An identity field would be the obvious choice. An int key, or similar, with the increment produced either by a trigger or by the software, would be an alternative. This would be feasible because the records are always inserted one at a time. Would an int key provide better performances with complex queries? 

Why are you using ? Why not just ? See perl's documentation on Finally, you can simply (and elegantly): 

Another advisable strategy would be separating this query in parts, using temporary tables, step by step. This is not necessarily faster, but convenient for finding problems with the selection logic, or using a strategy that best suits our own knowledge of the data: 

Adding some junk to the phrase breaks it, unless you pass it twice via quote_literal, which makes any string safe to use: 

A possibly simple way is replacing all MS-Access tables with links to views in your SQL Server with the exact same structure as the old Access tables. If the views are simple enough (e.g. a select statement from a single table with a primary key and unmodified columns -other than renaming them-) they'll be directly updatable, otherwise you can use updatable views.