A central question of econometrics has been the question of causality: does X cause Y? Does the minimum wage cause employment? Do taxes cause capital flight? Data science has been mostly concerned with the predictive power of its models: how can we sort pictures according to their contents? how can we sort customers according to their creditworthiness? The two questions of causality and prediction are related: if we find a great predictor of customers' creditworthiness, does it mean that that characteristic causes creditworthiness? The answer is tricky, because excellent predictors are actually not always causal determinants of the outcome. If, for instance, high income students tend to cluster in good schools, this does not necessarily mean that household income causes student achievement. A perhaps more controversial example is that ethnicity predicts (i.e. is correlated with) a range of outcomes (education, health) but does not necessarily cause them. Data science has become central to business applications (from Google Photos to face recognition in Facebook) but not so much to public policy analysis. In contrast, econometrics is ubiquitous in policy analysis. There is some convergence though. A better, and more elaborate answer on how econometrics recently borrowed from data science is provided by Susan Athey (Sep 2016 podcast). $URL$ 

The book "Modern Political Economics" is quite critical about "neoclassical economics", the basic claim being, if I understand correctly, that the models which "neoclassical economics" (armed with mathematical insights starting from John F. Nash Jr, Gerard Debreu and Kenneth Arrow) investigate, are not simplifications of some realistic models, but that they (the models) are just wrong (because of what the authors call "Inherent Error"). Is there a serious criticism of the views such as the ones exposed in this book? added after a comment asking for citations: maybe not a successful choice, but nevertheless (the following two citations are short descriptions of chapter 3 and chapter 8 of the book): 

I'm not sure I fully understand what you mean by computing it endogenously? The nominal interest rate would be endogenous in the NK framework because the output gap and inflation are endogenous. There'd be feedback between nominal interest rates and inflation, which both feed into each other for example. So in that sense I'd consider the approximation of interest rate policy using a Taylor rule as entailing that interest rate policy WAS endogenous wrt. the output gap and inflation. As for why we'd use the Taylor rule as opposed to some more complex but accurate approximation for interest rate policy... well I can only suppose that the Taylor rule is a sufficiently good approximation such that the marginal benefit of adding more complexity / accuracy is minimal and perhaps less than the computational cost of doing so. There comes a point where you start to prioritize parsimony more and more. Perhaps this is the motivation. 

The odd couple of the title are value and growth. From the very start, political economics found it difficult to square the two; to create models or accounts of how the exchange value of things was determined in a growing economy. The chapter begins at the beginning, with the French Physiocrats, before moving to Adam Smith and David Ricardo’s attempts to tackle this conundrum. The Inherent Error makes its first fonnal appearance in these works, before it returns again and again in the following chapters. The essence of the Inherent Error is the impossibility of telling a credible story about how values and prices are formed in complex (multi-sector) economies that grow through time." 

I don't think cash could be described as a security... one of the characteristics of securities is that they have imperfect (if very high) liquidity and provide a return (be it fixed or variable). Cash is the definition of liquid and inherently provides no return - you could earn interest on cash by depositing it in a bank but then you are creating a debt obligation in effect - the cash inherently, as in cash in a physical safe, generates zero return nominal by definition. You could think of cash as a debt security where a debt is theoretically placed on the issuer. But: in practice the debt is impossible to pay. You cannot bring a bill into the fed and demand they honour it. So I suppose it would be a debt security in the theoretical sense but it lacks almost all properties of debt securities: cannot be repaid in practice, never intended to be repaid, perfectly liquid etc, and it has a value that cannot be expressed in terms of anything but itself. 

How does US (its economy, banks, industry, "finance industry") profit from dollar being the "standard currency"? I am not seeking "ideological" explanations, but explanations of the mechanisms involved, of the kind of "how things work", better backed by actual numbers. Could one recommend some honest, preferably not ideologically coloured, accessible reference which discusses such mechanisms? 

"The rest, as they say, is history. Debreu and Arrow emerged from Nash’s seminar and in a few short months applied what they had heard to hammer out their own existence proof in the context of a multiple-sector model economy: one that effectively brought back from the dead Walras’ (fully neoclassical) idea of a General Equilibrium complete with determinate prices for everything, including labour input and capital goods (recall Chapter 6). This they accomplished by procuring General Equilibrium’s ultimate mathematical proof; an existence proof that showed under which conditions a set of relative prices exists such that all markets (including that for labour) are in equilibrium. This existence proof was to mark a new neoclassical turn in political economics; a turn that altered the discipline’s course and returned neoclassical obscurantism to the throne from which it had been removed by the combined forces of the fall of 1929, the analysis of John Maynard Keynes, the engineering brilliance of von Neumann and, last but not least, the experiences of economic policy during the New Deal and the Second World War." 

If you want to forecast it is common practice to split your sample up into two parts - the "estimation sample" and the "forecast sample". The estimation sample is used to, you guessed it, estimate your model, which you then use to forecast the data for which you already have true values contained in your forecast sample. You then compare your forecast predictions to known data and to those of other forecasts. You can do this using r2 or adjusted r2 over the forecast sample. We also commonly use measures like means squared error, mean absolute percentage error, Theil's U, etc. etc. 

Based on data/work with consultants/business school discussions with executives: A good example of a set of industries where marginal costs are rising is commodity production (metals, agricultural products, and energy such as oil). In such industries, companies own many plants (Glencore, BHP Billiton) and they tend to switch on production in a plant when the price is above the marginal cost of that plant. So it makes sense to order plants in increasing order of their marginal cost, and assume that the lowest-marginal-cost plant will be the last one to be shut down. Of course if the firm acquires a plant that has a lower marginal cost than the lowest-marginal-cost plant, it will shift that firm's marginal cost curve. Look for Carmin Nappi's slides online on the aluminum industry, plenty of that analysis going on. Some of the slides are in here, see slide 11. $URL$ Now this is a similar framework adopted by Pindyck, for instance in $URL$ 

To answer the specific question, I believe the largest hf in the world is Bridgewater (assets under management of around 120bn USD). Assets of around 500m USD would be considered average and north of 1bn usd large. Bridgewater is an outlier. I do not know if one larger has ever existed but I doubt it. As to what effects they have on the economy well, funds can serve as catalysts for economic events (e.g. Soros' fund and others catalysing the UK exit from the ERM). They serve as vehicles for pooling and allocating capital and so act as intermediaries between those who wish to lend (investors in the hedge fund) and those who wish to borrow (firms issuing bonds etc. which the hf buys). This function only applies when the hedge fund is making a primary investment though - when it is buying something in the secondary market rather than a first issuance funds do not have this role. They largely determine things like commodity prices, exchange rates through the bulk of money they control together, although no hedge fund is large enough to control these markets directly. 

During the Second World War, economic policy was in the hands of the New Dealers, who ran the economy on a trial and error basis and in the light of the accumulated experience of trying, not with great success, to kick-start the ailing US economy during the traumatic 1930s. Meanwhile, a group of scientists (mostly of Central European origin) were manning the agencies, laboratories and divisions of the civilian and military authorities whose job it was to solve practical problems (e.g. logistics, planning of transportation systems, price setting) by means of advanced mathematical methods. However, after the war ended, and the Cold War began to take hold, both the New Dealers and the Scientists lost out in the struggle for the hearts and minds of academic economics. The winners of that ‘game’ were a small group of Formalists, with John F. Nash, Jr, Gerard Debreu and Kenneth Arrow at the helm. The chapter tells the story of that triumph, which gave neoclassical economics a whole new push, by focusing on the person that the book portrays as the era’s most tragic figure: John von Neumann. His ‘fate’, the chapter argues, was an omen for the type of economics that would prove instrumental in the run up to the Crash o f2008." and yet another citation: