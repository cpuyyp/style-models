Log Shipping: For the sake of completeness, you CAN configure log shipping on a failover cluster. As I say above, be sure to configure the backup location as a shared folder that fails over along with the other clustered resources (e.g. SQL Server Service, SQL Agent, etc.). Multi-Subnet Failover Cluster: This is adding onto what it sounds like you already have, but you can include a different site into your failover cluster. Proper care will need to be taken so that automatic failover to your DR site is only done when necessary. Additional constraints are replicating the data to the DR site. Configuration of this approach is not simple and will depend a great deal on your Windows Version(s), SQL Server version, budget, etc. SAN-to-SAN Replication: This is generally a hardware based approach and will depend upon your SAN vendor. Most Enterprise-level SAN solutions support SAN-to-SAN replication and in conjunction with virtualization/etc. you can run across some elegant and expedient DR solutions. 

Why are you updating keys? The proper fix isn't to disable/enable referential integrity, but instead use surrogate keys in your data warehouse. That way if a user ID or some other "key" that has meaning is adjusted, you're not breaking RI to do so. A summary from the master, Ralph Kimball, himself: 

The Principle of least privilege should always dictate your security approach, but again, yes it can be SQL Injected much like anything that ingests Dynamic SQL. EDIT: In response to your follow-up question in the comments, you can further lock this down by wrapping a call to within another stored procedure where the parameter is statically defined or more tightly controlled. This new SP can then take advantage of impersonation (ref1, ref2) and you would then only need to grant rights to explicit users that you want to expose this functionality to. This should lock things down pretty well and depending on how you limit the use of the parameter being passed to , you may be able to completely eliminate the potential for SQL Injection occurring from normal accounts. Again, any elevated account that can directly call will still have the ability to extort the potential for SQL Injection, but this approach will lock it down about as good as you can. Because I typed this up hastily, here's also an example in hopes it shows the approach a bit better: 

I wonder if your operations generated some gaps in the transaction log which may be preventing the virtual log file from looping back around to the front of the physical log file. Conceptually this is what should be happening within your database when the end of the physical file is reached: 

First, you must remember that everything in computers is represented in binary; however not all numbers are easily represented using binary syntax, specifically when it comes to certain fractions. One such fraction, ⅓, is actually impossible to represent in a computer. Even with the largest amount of memory possible, a computer will never be able to accurately represent ⅓ in binary form. However, it can get more precise (e.g. close) to ⅓ with the more bits (e.g. and therefore decimal places) that are thrown at it. Precision can therefore be thought of as how exact a number you want to represent. The more digits the more precise that number becomes. 

This isn't a verified query as I'm not running replication, but after scripting out definitions for both and via and smashing together the underlying logic, I came up with the following that may give you what you're looking for. 

To further supplement @Cunning's answer, connections made within SQL Developer default to a Basic Connection. A Basic Connection doesn't use the file or any locally installed oracle client on the machine. This is likely how you're able to connect to the database via SQL Developer without much issue. You'll need to install an Oracle Client on the machine and likely create a tnsnames.ora entry for the server as well before rerunning your repository creation utility. There are number of walkthroughs on how to do this, but here's one I wrote on the 11g client for Windows if you're looking for something. I doubt the 12c client is drastically different, but this should get you pointed toward the proper direction. 

The reason is because that field is still a datatype. If you want the output to only show the date, you'll need to CONVERT the output to a (or in this case), such as: 

So I did some analysis on the various approaches posted so far, and in my environment, it looks like Daniel's approach wins out consistently on the execution times. Surprisingly (to me) sp_BlitzErik's third CROSS APPLY approach wasn't that far behind. Here're the outputs if anyone's interested, but thanks a TON for all the alternative approaches. I learned more from digging into the answers on this question than I have in quite a while! 

What version of SQL Server is the PI2 server in this situation? When was the last time the underlying statistics were updated on these tables on the Linked Server? While you updated statistics, it sounds like that was only done on the local database server that you migrated to. Updating statistics on these underlying remote tables may help the optimizer identify a more optimal plan and allow for the entire query to execute remotely. Another possibility is permissions (though you say that's no different between the old and new servers), four-part (aka distributed) queries require proper permissions to remote tables in order to get an optimal plan. From the Guidelines to Using Distributed Queries page: 

Even though it's a bit late, I'm going to field a response with hope that it helps or at least spurns some additional ideas/commentary on this issue because I think it's a good question. First, and I don't know if you're doing this or not, but please don't assume that high fragmentation levels on the index are always going to cause poor performance. Stale statistics (e.g. sys.dm_db_stats_properties) and high amounts of white space per page (i.e. avg_page_space_used_in_percent column in sys.dm_db_index_physical_stats dmv) hold more relevance regarding performance issues than fragmentation alone. Yes, highly fragmented indexes will generate more read-aheads and you typically do see stale statistics and higher levels of white space per page coupled with fragmentation, but fragmentation isn't directly tied to query plan optimizations nor how much memory loading the index from disk will actually consume. Query plans are affected by statistics and your memory footprint bloats with more white space. For instance, an index that is 99% fragmented but has less than 5% avg. white space and up-to-date statistics is likely not causing you drastic performance issues as compared to either a bad execution plan as a result of stale statistics or constant paging of an index that's too big to fully fit in memory because there's a significant amount of white space present per page. If fragmentation is truly an issue, you can reduce it, ONLINE, by issuing an statement as identified by Dan Guzman in the comments. This won't create as streamlined an index as a operation will, but it will reduce your fragmentation. The key here is to identify windows of lower usage on your database and run it then. This could be 15 minutes or multiple hours, obviously the longer the better, but the key here is this operation doesn't rollback and retains any progress made even if you kill it mid-execution. If, in a perfect world where your fragmentation was eliminated, would it make more sense to utilize partitioning on this table? Azure SQL Database does allow for table partitioning and Microsoft has a great article outlining some Partitioning strategies for Azure SQL Database. If your data is non-volitile, partitioning it may help reduce maintenance needs, and if coupled with Table Compression, you may even be able to reduce your overall storage footprint as well. Alberto Murillo's earlier answer alludes to utilizing Horizontal Partitioning based on a data region, and this approach may help create some maintenance windows for you as your data would be more regionally specific instead of global. Transitioning to a partitioned table won't be easy with your current absence of maintenance windows, but you may be able to utilize an approach outlined by Maria Zakourdaev which uses Partitioned Views over the top of your current table and a new partitioned table to start partitioning future data. As time goes on (and hopefully your old data is purged), you can eventually transition fully over to the partitioned table. Again, I don't know your data or application, but maybe this approach is something you can employ. 

Sure, you can use PGP, but your encryption and decryption routines will need to be handled at either the Application layer or by custom CLR routines. In either case, a rewrite to something is likely needed. There are also native alternatives as already discussed that I think should be used over PGP, but really your criteria will dictate what will work better. 

To further expand on my comment, the table is stored, by default, in the SYSTEM tablespace. When the SYSTEM tablespace fills, your database halts. There are a few options available to you to manage this, but these will depend on what version of 10g you have installed. As of the 10.2.0.5 patch, a new package, DBMS_AUDIT_MGMT was included that allows you to move the table to a user Tablespace. If you are at this patch-level, move the audit table, plain-and-simple. Filling a user tablespace makes write operations against said user tablespace halt, but it won't bring everything else down like the SYSTEM tablespace filling will. This functionality was backported, but not to all 10.2 versions. You can check Doc ID 731908.1 on support.oracle.com to get all the details. If you don't have the ability to upgrade to 10.2.0.5, you will need to regularly purge the table by some other means so that it doesn't continue to grow and consume space. If it gets too unruly and you find you do need to "shrink" the table, you will likely need to TRUNCATE it to release the white space back to the tablespace. I've tried shrinking this file using the typical trick of enabling row movement and shrinking the table, but because records are often appended to the end of the table and are added regularly, it often doesn't shrink much, if at all, which is why a TRUNCATE operation is often the most effective method here to free up some space. Finally, your last option is to just disable the Audit. If it's not needed, this is probably the easiest option available to you. 

Some notes, DBCC CHECKIDENT requires db_ddladmin privileges or higher to use. Because of this, I couldn't include a DB Fiddle example. Also, and this may be something you want to dig into as well, but I believe that pushing data from a "test" environment qualifies said environment as "production worthy" which may affect your licensing. Licensing is out of scope, but this could bite you depending on how you have your server licensing setup, so just be wary of promoting data up your environment stack. 

Why is it necessary to utilize Cascading RI? I've always found it to be much more troublesome than it's worth. Since you're already customizing a solution, I think the easiest approach would be adding a second PKEY column that you can reference in the and tables within the trigger. So your Parent Table DDL would now look like: 

I'm suggesting the EXECUTE AT approach instead of using other LinkedServer syntax such as OPENQUERY or OPENROWSET as this will allow you to still generate the date parameters from data hosted on your local server (as your estimated query plan indicates) and pass them as parameters to the query you wish to run remotely in what will likely be a more optimal fashion. 

If you are the application guy, it may be easier to configure the application to log activity directly from the app layer. Maybe configure some sort of verbose logging option? The ALL_TAB_MODIFICATIONS table, which is what I think you mean, is not a reliable source for what you're trying to identify, as it's not updated immediately after a change is made. 

As SqlWorldWide stated in his answer, it's not possible (using out-of-the-box log shipping). A supported workaround is to install a third instance on the server, , and put on that instance with a different schedule. Your server will need to be of sufficient size, but this will allow you to have two different copies of the database on two different recovery schedules. If you can't install a third instance, and you're not afraid to code up some custom routines, there is another option available. Basically, log shipping is a process wrapper that will automagically backup a database, copy the backups to a remote location, and then restore those files to a different database. There's really no mystery about what's going on here, but the routines make this process easy to manage. With sufficient effort, you can customize your own restore routines for the database as the formal Log Shipping jobs for will already have the backups located in a common area for you to reference. You'll need to remove as a log shipping candidate and develop routines that restore the outstanding logs when/how you desire. All backup information will be located in the tables on your primary instance, so you may need to query that over a linked server to generate the restore scripts. This isn't going to be an easy approach, but it's an option if you must have the functionality you want and you've got time to customize some restore routines. Obviously the downside is won't show up in your log shipping monitoring reports, but it sounds like you want this location more as report target instead of a failover candidate in the event of a disaster.