Re: Shrinking. I see so many people getting their claws out at the very mention of 'shrink' and 'database' in the same sentence, so time to clear things up a little. Shrinking data is baaaaad. It literally turns your indexes into quivering shells of their former glory. Don't do it. Shrinking log should not be done routinely, but if you have a ridiculously outsized log (i.e. in one case I saw a 40GB log for a 100MB database, due to whoever set it up putting recovery model to full then never dumping the transaction log), you can shrink the log (after ) to reclaim that space without any ill effects (although, obviously, the will chew up I/O while it's running). PS: Speaking of log files, check your log file size and autogrowth settings. Small autogrowth settings can lead to underperforming log I/O (due to a poorly-explained feature in SQL Server called Virtual Log Files), particularly on batch transactions. 

(I'm assuming SQL Server in this answer, please clarify which RDBMS you're using if it's not SQL Server) The index you want is: 

Short version: It depends. Generally spoken Sybase SQL Server is smart enough to do things the fastest way, though. Long version: Sybase's query processor is, at it's core, very similar to the one used in MS SQL Server. It will create worktables (internal temporary tables; not visible to the user) if the result set is sufficiently large to overflow available memory (similar to a table spill in SQL Server). Otherwise, it'll do a pair of index scans (you do have indexes on and in both tables, right?), then a join and output, all in memory. Caveats: 

It's called Entity-Attribute-Value (also sometimes 'name-value pairs') and it's a classic case of "a round peg in a square hole" when people use the EAV pattern in a relational database. Here's a list of why you shouldn't use EAV: 

You need three tables - , and an associative table for the many-to-many relation between them (a can have many s and an will have many s), call it or something. and will have autoincrement fields as a surrogate primary key, will have FKs to both and , like so: 

Note that "3MillValue" in two places would need to be hard-coded with the value that returns that amount, and that OPTIMIZE FOR is the key to this technique. 

Provided you're using bulk delete/update where appropriate, and have indexes to support such operations, I don't think you'll run into trouble due to the PK standard you use. It's possible that if you later have EF generate queries with joins etc, that they won't be as efficient as they would be with a natural key based repository, but I don't know enough about that area to say for sure either way. 

I see you've just done a big update, so I'll perhaps do a separate broader answer. Since I can't yet comment, I'll ask a few things here: - Are you free to add indexing as your query requires? - Is the replacement of CONTAINS([varchar3], 'moreText') with [varchar9] LIKE '%a%' correct (i.e. do you definitely no longer need fuzzy search on varchar3?) - Will OFFSET always be 0? - Can you give some idea of the proportions of data you expect for columns varchar1, date1 and varchar8? 

If running your query multiple times in serial takes 50% overall of a 16 core CPU, that means it's going parallel internally (i.e. SQL Server is splitting up the work over multiple cores), and you can't expect linear gains by running it in parallel "externally" (or whatever the correct terminology would be...). Try your tests again with "OPTION(MAXDOP 1)" added to the query (SQL Server only), which will ensure each run only uses one core, and I think you'll see what I mean. 

A comparison of the values in the inserted and deleted tables is your only option, because UPDATE() and COLUMNS_UPDATED will be true even if the value hasn't actually changed (i.e. has simply been overwritten with the previous value). To (largely) future-proof your trigger you could dump inserted and deleted into temp tables, and generate dynamic SQL for the comparison based on the temp table structures (in conjunction with Update()) which you can obtain with this: 

I believe type = 4 only applies to pre-2008 fulltext files which have been upgraded, because since then there has been no way to create separate fulltext files -- only separate filegroups. (See type = 4 doc for SQL 2012 at $URL$ which confusingly says the same as for 2016 except version number.) Something like $URL$ may technically do what you are looking for, but won't actually be relevant for your report. 

There's no server-level 'read any database' permission and server-level roles can't be granted database-level permissions So yes, you will have to map users to databases individually. If you're using Active Directory, you can create a windows group, then give that group a login to SQL Server, then apply db_datareader in all databases for that group (you'll need to create users in each database though). 

So no it doesn't lock tables, rows or pages. However, any (even with set) will issue a (schema stability) lock, which basically means no schema changes can happen while the is executing (schema changes = add/drop column, change datatype of a column, change nullability of a column and a couple other operations I can't think of off the top of my head). A lock should not interfere with DML statements (///). 

Example below is, for each column in the table that starts with either 'A' or 'B', return all rows in the table that have a non-null value in any of those columns. Note: I don't have a Sybase install handy to test this. Chances of it actually working are very low, but it should hopefully be enough to make you realise what a bad idea this is. 

Note that if you have a front-end built in Access and just want to shift the database away from JET (Access's internal DB engine) onto a "proper" RDBMS, you can do so by migrating the data across and setting up linked tables inside Access to the new data source. 

(* I'm aware this would probably fail any security audit going, but to my mind if we've let an intruder into the server room that knows to look in the third drawer down for the unlabelled 'sa' password post-it, then we're screwed anyway.) 

If you distribute your application with scripts to build the database (not a database backup or something like that), then in theory it should make no difference which version (of 32-bit and 64-bit) you develop against. Back when 64-bit was new and shiny (which was SQL Server 2000), there were some advanced features that 64-bit didn't support, but in 2005/2008 they should be functionally identical. (Not that it should matter if you're using Compact Edition, but there are some configuration differences between 32-bit and 64-bit once you get to the point where your DB performance is an issue.) 

Since both servers show similar reads and similar CPU "seconds", but the new server example shows that the duration was the same as the overall CPU time, the spikes you're seeing are likely due to less-parallel execution plans sometimes being generated--perhaps simply due to the CPUs being unusually busy at those times. Note that the CPU "time" shown is the total CPU milliseconds used over possibly multiple cores--which would have to be the case in your old server example where the CPU time was 6 sec but the actual duration was 1.5 sec. 

Regarding your first question in general: I do not understand how an execution plan can reveal 3,141,000 Rows if it actually returns ZERO rows. How is this possible? The final output rows count isn't known to the optimiser when it generates a plan. So all it can consider are the estimates it can calculate from statistics. (In the case of your "bad" plan, the estimated output rows was actually 4.4 which was based on the first estimate in the plan.) If those estimates are outdated, or insufficiently accurate (sample vs fullscan of unevenly distributed data for example) then a poor plan can be generated even with a simple query. In addition, if a plan is reused with different variables, the estimates the plan was generated from may be wildly inaccurate. (And I think that's what sp_BlitzErik is leaning towards as the cause in your particular case.) 

Since the three operations ended within 3 miliseconds of each other, it looks to me like the first two simply were able to wrap up and report their completion just that little bit faster than the third. (Note the End_Time is the end time of the operation, not the time the operation was logged, which would be slightly later.) Note row E is the only stored proc call. It's possible there's slightly more overhead in wrapping up such a call. 

This may be a file permissions/uac issue. To prove/disprove that, create a new folder and copy (not move) the excel file into it, then give the windows security group "Everyone" Full control on the folder and its files. Update your export query with the new file location and try running it again. If that works (or if you at least get a different error), then bear in mind that wherever you want to have the export file in the end, the file will need read/write permission assigned for the user account your SQL Server service runs under. (Or the "Everyone" group, but that wouldn't be best security practice.) 

You use when a column isn't being filtered on (isn't in the clause) but is being selected. means that the column will only be included at the leaf level of the index (it's a small efficiency saving, essentially). Also as noted, if you have a statement without a where clause, indexes will never be used (faster to just scan the table rather than muck about with an index). Index seek is only used when you're returning a comparatively small number of rows, for your sample query if most of your and rows are marked as Active then the optimiser may well decide to index scan instead of seek. A query that would be more likely to get an index seek instead of scan would be something like: 

First things first: MS Access was not designed for multi-user access. Every version of Access I've used had a disturbing habit of corrupting tables at a vastly increased frequency if there were >1 users using it. If the two users are connected to the Internet all the time, I'd recommend shifting your table storage to SQL Server and having the users connect to that (use a VPN or some other form of security! If they're on a company LAN it's even better, you shouldn't need a VPN then). It's a fairly straightforward process to convert to SQL Server. The users will still use the Access front end, but instead of having the tables stored inside the .accdb file and having to merge them, the Access tables are converted to linked tables to the SQL Server tables. This is possibly a bit more up-front work, but it'll save you hassle down the road (how often do you need to merge? who's going to do the merging?). Also, if the application ever gets more widely used, you can easily build another front end (in C#, Java, ASP.NET, whatever) and connect it to your SQL Server back end. 

How often you need to run index maintenance/rebuild stats depends on your database load, specifically how often your data is modified (i.e. //). If you're modifying data all over the show (i.e. a staging table for a weekly batch process), you probably want to update stats/reorganize indexes nightly. If your data is rather more static you can probably make it a weekly or fortnightly schedule. 

Many SQL Server configuration features are not implemented in the UI. Scripting is your only option for these (aside from possibly some third-party UI that implements them). 

Assuming that: - You have the looping and sleep code in the CLR code - You know how to call said code from SQL agent (a stored procedure wrapper if nothing else) - Your process is ok with the occasional sub-minute downtime - Your process will not lose any data if it fails part way through (i.e. the next run will process the data the failed run was trying to process) then I would propose a SQL agent job that is sceduled for every minute, and the job step itself have say, 10 retries set (advanced page) so that it will usually restart itself immediately if it fails. Why just 10? Because if it starts consistently failing, you will want to get notification quickly (via agent job failure notification settings). That will be an email a minute. Of course, if it fails 10 times sporadically, you'll get a false alarm notification, but you can quickly see from the job history whether it's failing consistently or just sporadically. 

I believe the basic issue is that the query is doing multiple heavy table scans due to no index support. Try adding the following indexes: CREATE NONCLUSTERED INDEX TEST ON STOCKDEBUGTRIGGERED (ChangeDate) CREATE NONCLUSTERED INDEX TEST ON STOCKDEBUG(ProductID, StockOld, StockNew) There may be further tweaks, so please post the stats and execution plan with these indexes added. 

I'd suggest the first thing to determine is whether or not the server workload actually struggles during those spikes. Do you see for example IO being maxed out, or other queries being blocked (due to the splits increasing the time the write transactions take to complete) or slowed? If not, the page splits are not an immediate concern, but still may be worth looking into. The question is, are these "good" or "bad" page splits. This link might help you determine what you're seeing. Logging your index fragmentation levels before and after a spike might also be a simpler way. "Good" page splits are simply inserts to the end of an increasing index (clustered or otherwise)that require a new blank page, so it's really not a page split as generally thought of, even though SQL Server counts it as such--presumably because there is some overhead, but probably not more than the inserts cost in general. "Bad" page splits are updates or inserts to the middle of an index that overflow the page, and are the ones that cause both internal and external fragmentation, with external not much of an an issue with SSDs and/or shared storage, and internal being of more potential impact due to the IO and cache memory they waste. It could be that you've got a mix of good and bad, perhaps good into the clustered index and bad in multiple non-clustered indexes. That's pretty much unavoidable, and you'll just need to consider your index maintenance and possibly a specific fill factor on indexes that are frequently affected. But read $URL$ first. However if you find your clustered index is being bad-splitted, then it may be worth considering whether a clustered index that better supports your inserts would be in order. Or if the splits are caused by updates adding more data during the life of a record, a specific fill factor might be in order, but really only if the updates are evenly distributed throughout all your data, since a fill factor to support only your recent data would waste a lot of space/IO/cache if most of your data is static over time. The ideal clustering config really depends how your table is used overall though, not just on how it's written to.