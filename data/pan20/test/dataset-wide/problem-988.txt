My wife is a nurse educator who had no even management platform. I developed one in a VERY similar fashion that you outlined here. The logic and thought process was the same. You should be safe with this overall schema. 

We are researching the direction of using Snapshot backups for SQL Server, rather than more traditional backup methods. We are using CommVault (and would be for snapshot too..) I am trying to understand, better, how they work compared to traditional backups, or agent backups using CommVault. As I am not really a storage guy, I don't quite understand how the restore processes work from a snapshot backup for a SQL Database. I do understand the absolute speed benefit, but would like to hear more about the pro's and con's of utilizing snapshot backups for SQL Server. 

Whenever we apply a patch (recently with CU5), end users need are prompted for administrator credentials to print. In googling, I see many different suggestions, but nothing seems to work. I have tried adding the SSRS server to the trusted sites list. Also, I have checked the active X permissions and they seem to be set correctly under Security Settings. Does anyone know the necessary configuration to avoid this prompt with future SQL Server updates on our SSRS server? 

What you will see is that less data can be cached and, thus, some queries will have desegregated performance. The default value for RAM just, kind of, means that SQL Server will use 'whatever it can.' Once it goes up, it won't come back down until the service is restarted as it then reserves all that RAM (because it caches data on it.) This is my favorite resource for determining how much RAM a server really needs. My only issue with it is that it doesn't really take in to account smaller VM based machines and just assumed that you CAN throw 128gb+ at a server. $URL$ A general rule of thumb is 10%, or 8GB, dedicated to OS/Auxiliary with the rest being dedicated to SQL Server (through the MAX Ram setting.) (10% or 8GB, whichever is more.) 

The above worked perfectly. Now the error puts an entry in to my event log and I can trigger scheduled task events from it. 

We have a production server that dumps it's Page Life Expectancy many times a day (climbs to 2500s and falls back to near 0s)... After watching some training videos about virtualization for SQL Server, I was pointed towards several Perfmon counters to look at: 

Ballooned, Reserved, and Swapped are all at 0 but Memory Limit was set to nearly 3tb (perhaps a default setting like SQL's Max Memory setting...) I viewed these counters during an event when the PLE dropped to 0 and none of the counters changed. According to Spotlight, Buffer Cache and Procedure Cache are remaining at high % utilization and no stats seemed to really budge except the PLE itself. To me, this points to not enough RAM allocated to the machine to the point that many of the queries are forcing the server to look to disk to get results. What steps do I need to take to prove or refute this? Server Config: - Windows 2008 r2 VM - 4 cores - 12GB RAM - main DB is ~150GB 

I have a MySQL database that is storing business data... In the database, when running a select against a business name that contains an accented e (Ã©), it comes up correctly (showing me that it is stored correctly.) On our website, though, it is showing up as the diamond with the question mark. I checked out database and found the following: 

Where things get lost is on the website itself... The web page is encoded as UTF8... Any ideas of where the conversion may be failing? -Wes 

Restores a DB from a .bak giving the .mdf and .ldf a new name (so we have have several copies of the same DB up (If specified in the SP's parameter) Creates three merge replication publications (What I need help doing) Generating the snapshots for the three publications using sp_startpublication_snapshot 

My workflow is as above as well, with the addition of disabling all the non-clustered indexes prior to dropping the clustered. Since dropping the clustered constraint index requires the table to save as a HEAP, the amount of time this process takes on our 45m row table is tremendous. The drop on the constraint has been going for 1:17:00 and seems to only be at about 31m (based on Logical Reads in Spotlight for the Session). Is there a more efficient way to handle this workflow? Perhaps a way to drop the constraint index and rebuild as the new clustered index, rather than as a HEAP? Thanks, Wes DDL Statements: TABLE STRUCTURE 

I need to create a trigger on a table (after insert) that simply checks for a string in a cell. If the string exists, I need to log an event in the windows event logger. I am not really sure if I am able to use the inserted tables this way, but I thought I would try. This is what I have so far: 

-- NOTE -- The below answer worked perfectly for me. I did have to have a volume added. I created a second Filegroup and a datafile on the new drive. Additionally, another log file as well. 

From researching further, and a bit from the comment on my question, I found that I needed to generate the DDL statements from the previous database. Once doing so, and setting the correct directory, everything worked perfectly. 

If the driver is a solo-driver, then they have an associated Vehicle_ID If the driver is a company-driver, then they have an associated Company_ID - The company_ID would have associated (one-to-many) vehicle_ID's 

It is still just a one to many relationship. You should be able to reference it in the exact way you referenced the other entities. 

We have a production box that has about 60 publications on it. Some of these are used, and some seem to not be... What I need to do is create, or find, a script that will get the count of subscribers/subscriptions per publication so I can easily identify unused publications. We are using SQL Server 2005. :( Thank you, Wes 

Driver_ID (PK) Vehicle_ID (FK to Vehicle... nullable) Company_ID (FK to Company... nullable) solo_driver (boolean value [kind of optional, but could be useful]) other Attributes... 

This is a pretty good question. This topic, many times, gets over-thought. It really all comes down to business needs. If the need to compartmentalize data in to different subject areas (Finance, Sales, Manufacturing, Inventory, etc...) If this is far out of scope for the business cases, then you should probably stick with your standard database schema. From my view point, a single data mart is not terrible useful except for in fringe cases where a single department has needs for a data warehouse like reporting infrastructure... Then a data mart might be useful. 

We have a 2-node clustered SQL Server 2014, Enterprise, environment for our data warehouse. Storage, network, and servers are all high performing (1tb mem, 32 cores CPU per node, all SSD) We are trying to come up with where to put SSIS itself. Based on Microsoft documentation, it isn't recommended that it be clustered and, therefore, shouldn't be on the 2 nodes (though I did install it with the instances, we don't have to use if from there can could disable the services.) On the flip side, we don't really want to spend money on licensing of 8 vCPU's to put it on an available server in the same ecosystem (used for a proprietary ETL from vendor.) What are the best practices and recommendations on how/where to install SSIS for a clustered SQL Server database environment? Thank you, Wes EDIT: Microsoft discusses this here: $URL$ but does not go over the recommended approach as an alternative. 

We have just completed an 11g to 12c migration to a new server. There is a schema that didn't seem to come over that consists fully of external .dat files. I can't seem to find a way to simple 'attach' these pre-existing (and moved from the old server) .dat files. Is the method to get this data back in to re-create the schema, table by table, pointing to the existing .dat files? If so, is there a simple way to generate all the DDL statements for an entire schema of tables? Thanks, Wes 

Per Brent Ozar Unlimited (linked below), the recommendation for SQL Server Standard 2014 is 192GB RAM. SQL Server will only use up to 128GB of it and the rest is to accommodate the overhead of OS and other processes. This being said, 64GB may be far more than necessary to allow for the of SentryOne and the IIS application overhead. How Much Memory Does SQL Server Need? I believe that, for the cores, you must license each core that is allocated to the VM. You can limit the CPU's using the CPU page on the Server Properties. 

I get incorrect syntax at the CASE statement but I don't quite see why. Perhaps the issue is actually how I'm accessing the inserted table. Where am I going wrong here? Thank you. EDITED CODE: