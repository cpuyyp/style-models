Here are a different class of obvious theorems, these are only obvious in the sense of physical intuition. They took a long time to prove: 

Here is a physically obviously true statement, which can be seen from physical intuition, but which is not proven (as far as I know): 

The question is muddled regarding quantum mechanics: The projection operator T takes measures to measures, but it does not define a Markov chain, or if you like, the resulting Markov chain is not an interesting one, because it is deterministic most of the time. The reason is that T is idempotent, a second application of T does nothing. Even if omega varies continuously, doing T on a continuously varying omega is deterministic in the limit of continuous measurement (see below). The specific questions are not the right questions, but here is an answer: 

To me, the only "unintuitive" applications of uncountable choice is when it turns up in physics. The only case I know where this happens is in the maximal-extension theorem of Choquet-Bruhat (QM does not use uncountable choice). This uses local extension properties of solutions to General Relativity to prove, using Zorn's lemma, that there exists a maximal extension. The use of axiom of choice is, I think, essential. I couldn't see how to sidestep it when I read the paper a long time ago (somebody please correct me if I am wrong). What is the axiom of choice doing in physics? I believe that it is entirely due to the issue of double-sided maximally extended black holes. A maximal extension of General Relativity can contain "wormhole" like solutions (for example, a charged black holes with two patches connected by an interior region), and there can be countably many such bridges in any asymptotically flat patch. But each of these branches can connect you to another different asymptotically flat region, which might have its own countably infinite collection of bridges to other flat regions. The resulting spacetime is like a tree with countably many branches at each node, where each node represents an asymptotically flat spacetime, and each edge is a double-sided maximally extended black hole bridging the two nodes. Such a tree can have infinite depth, and you must extend the solution to the whole tree. It seems intuitive that to patch the solutions together you need to extend the local solutions over continuum many nodes, and since GR is hyperbolic, you will get to make some arbitrary choices at each extension step. The dependence on choice then simply shows how unreasonable the maximally extended model of General Relativity is for physics. 

Karl Marx. So you didn't know he was a mathematician? A book of his collected mathematical papers is in our math library, which is more than most mathematicians can claim. (They are mostly attempts to understand the definition of a derivative if I recall correctly.) They were quite popular during the cultural revolution, Chinese mathematicians presumably figuring that the study of dialectical calculus was better then a one-way trip to one of Mao's holiday resorts. 

This seems to be a combination of a comparison theorem saying that for complex manifolds, ordinary topological cohomology is closely related to l-adic cohomology, and a theorem relating etale cohomology of a scheme to its reduction mod p. The standard reference for these sorts of results is the exposes by Artin in SGA IV, volume 3; all you have to do is wade through about 1000 pages of French. 

It is possible to make sense of this if one generalizes the notion of distribution by choosing a smaller space of test functions. The space of test functions should be chosen so that it is closed under Fourier transform and its elements decrease so fast that multiplying them by an exponential function is still integrable. Some possibilities for the space of test functions with these properties are: *Holomorphic functions on the complex plane that decrease faster than any exponential on horizontal strips (if I have remembered the Paley-Wiener theorem correctly...) *A more extreme space of test functions is polynomials times Gaussians. (Using smooth compactly supported test functions as suggested in another answer does not work as this space is not closed under Fourier transforms) 

The more general question has a negative answer for the following reason (which could probably be extended to the special case with more thought). The mapping class group of a finite volume hyperbolic 3-manifold is finite by Mostow rigidity. But finite volume hyperbolic 3-manifolds can have tori as boundary components, whose mapping class group is infinite. So in this case most diffeomorphisms of a boundary component cannot be lifted to the 3-manifold. 

Constructing quantum field theories is a well-known problem. In Euclidean space, you want to define a certain measure on the space of distributions on R^n. The trickiness is that the detailed properties of the distributions that you get is sensitive to the dimension of the theory and the precise form of the action. In classical mathematics, measures are hard to define, because one has to worry about somebody well-ordering your space of distributions, or finding a Hamel basis for it, or some other AC idiocy. I want to sidestep these issues, because they are stupid, they are annoying, and they are irrelevant. Physicists know how to define these measures algorithmically in many cases, so that there is a computer program which will generate a random distribution with the right probability to be a pick from the measure (were it well defined for mathematicians). I find it galling that there is a construction which can be carried out on a computer, which will asymptotically converge to a uniquely defined random object, which then defines a random-picking notion of measure which is good enough to compute any correlation function or any other property of the measure, but which is not sufficient by itself to define a measure within the field of mathematics, only because of infantile Axiom of Choice absurdities. So is the following physics construction mathematically rigorous? Question: Given a randomized algorithm P which with certainty generates a distribution $\rho$, does P define a measure on any space of distributions which includes all possible outputs with certain probability? This is a no-brainer in the Solovay universe, where every subset S of the unit interval [0,1] has a well defined Lebesgue measure. Given a randomized computation in Solovay-land which will produce an element of some arbitrary set U with certainty, there is the associated map from the infinite sequence of random bits, which can be thought of as a random element of [0,1], into U, and one can then define the measure of any subset S of U to be the Lebesgue measure of the inverse image of S under this map. Any randomized algorithm which converges to a unique element of U defines a measure on U. Question: Is it trivial to de-Solovay this construction? Is there is a standard way of converting an arbitrary convergent random computation into a measure, that doesn't involve a detour into logic or forcing? The same procedure should work for any random algorithm, or for any map, random or not. EDIT: (in response to Andreas Blass) The question is how to translate the theorems one can prove when every subset of U gets an induced measure into the same theorems in standard set theory. You get stuck precisely in showing that the set of measurable subsets of U is sufficiently rich (even though we know from Solovay's construction that they might as well be assumed to be everything!) The most boring standard example is the free scalar fields in a periodic box with all side length L. To generate a random field configuration, you pick every Fourier mode $\phi(k_1,...k_n)$ as a Gaussian with inverse variance $k^2/L^d$, then take the Fourier transform to define a distribution on the box. This defines a distribution, since the convolution with any smooth test function gives a sum in Fourier space which is convergent with certain probability. So in Solovay land, we are free to conclude that it defines a measure on the space of all distributions dual to smooth test functions. But the random free field is constructed in recent papers of Sheffield and coworkers by a much more laborious route, using the exact same idea, but with a serious detour into functional analysis to show that the measure exists (see for instance theorem 2.3 in $URL$ This kind of thing drives me up the wall, because in a Solovay universe, there is nothing to do--- the maps defined are automatically measurable. I want to know if there is a meta-theorem which guarantees that Sheffield stuff had to come out right without any work, just by knowing that the Solovay world is consistent. In other words, is the construction: pick a random Gaussian free field by choosing each Fourier component as a random gaussian of appropriate width and fourier transforming considered a rigorous construction of measure without any further rigamarole? EDIT IN RESPONSE TO COMMENTS: I realize that I did not specify what is required from a measure to define a quantum field theory, but this is well known in mathematical physics, and also explicitly spelled out in Sheffield's paper. I realize now that it was never clearly stated in the question I asked (and I apologize to Andreas Blass and others who made thoughtful comments below). For a measure to define a quantum field theory (or a statistical field theory), you have to be able to compute reasonably arbitrary correlation functions over the space of random distributions. These correlation functions are averages of certain real valued functions on a randomly chosen distribution--- not necessarily polynomials, but for the usual examples, they always are. By "reasonably arbitrary" I actually mean "any real valued function except for some specially constructed axiom of choice nonsense counterexample". I don't know what these distribtions look like a-priory, so honestly, I don't know how to say anything at all about them. You only know what distributions you get out after you define the measure, generate some samples, and seeing what properties they have. But in Solovay-land (a universe where every subset S of [0,1] is forced to have Lebesgue measure equal to the probability that a randomly chosen real number happens to be an element of S) you don't have to know anything. The moment you have a randomized algorithm that converges to an element of some set of distributions U, you can immediately define a measure, and the expectation value of any real valued function on U is equal to the integral of this function over U against that measure. This works for any function and any distribution space, without any topology or Borel Sets, without knowing anything at all, because there are no measurability issues--- all the subsets of [0,1] are measurable. Then once you have the measure, you can prove that the distributions are continuous functions, or have this or that singularity structure, or whatever, just by studying different correlation functions. For Sheffield, the goal was to show that the level sets of the distributions are well defined and given by a particular SLE in 2d, but whatever. I am not hung up on 2d, or SLE. If one were to suggest that this is the proper way to do field theory, and by "one" I mean "me", then one would get laughed out of town. So one must make sure that there isn't some simple way to de-Solovay such a construction for a general picking algorithm. This is my question. EDIT (in response to a comment by Qiaochu Yuan): In my view, operator algebras are not a good substitute for measure theory for defining general Euclidean quantum fields. For Euclidean fields, statistical fields really, you are interested any question one can ask about typical picks from a statistical distribution, for example "What is the SLE structure of the level sets in 2d"(Sheffield's problem), "What is the structure of the discontinuity set"? "Which nonlinear functions of a given smeared-by-a-test-function-field are certainly bounded?" etc, etc. The answer to all these questions (probably even just the solution to all the moment problems) contains all the interesting information in the measure, so if you have some non-measure substitute, you should be able to reconstruct the measure from it, and vice-versa. Why hide the measure? The only reason would be to prevent someone from bring up set-theoretic AC constructions. For the quantities which can be computed by a stochastic computation, it is traditional to ignore all issues of measurability. This is completely justified in a Solovay universe where there are no issues of measurability. I think that any reluctance to use the language of measure theory is due solely to the old paradoxes. 

Mumford in Rational equivalence of 0-cycles on surfaces gave an example where an intuitive result of Severi, who claimed the space of rational equivalence classes was finite dimensional, was just completely wrong: it is infinite dimensional for most surfaces. This is a typical example of why the informal non-rigorous style of algebraic geometry was abandoned: too many of the "obvious" but unproved results turned out to be incorrect. 

See On numbers and endgames: Combinatorial game theory in chess endgames by Elkies for some chess positions with non-integer values. 

If one iterates the map z -> z^2 + c there is obviously a simple formula for the sequence one gets if c=0. Less obviously, there is also a simple formula when c = -2 (use the identity 2 cos(2x) = (2cos(x))^2 - 2). Are there any other values of c for which one can solve this recurrence explicitly? (For all initial values of course: there are many trivial explicit solutions for special initial values, such as fixed points.) Related links: $URL$ (the points c where 0 remains bounded under iteration of this map: this strongly suggests that there is no simple exact solution for general c). $URL$ (gives the explicit solutions above, after a change of variable) Motivation: I once used the map with c=-2 in a lecture to show that one could prove limits exist even without a formula for the exact solution. A first year calculus student pointed out the non-obvious exact solution above, and I don't want to be caught out like this again. 

The paper by Atiyah on K-theory and reality Quart. J. Math. Oxford Ser. (2) 17 1966 367--386. MR0206940 discusses the topological analogue of this question, showing how to relate complex vector bundles over a space with vector bundles over the fixed points of an involution. The idea is to define a sort of intermediate K-group KR of vector bundles with involution, and compare it to the two other K-groups. 

Since this was resurrected, here is the statement that at this time seems to me to have the greatest gap between obviousness of truth and obviousness of proof: 

Here is a problem that requires nontrivial integration techniques, because the closed form answer is a sum of an algebraic and trigonometric function. If you have ever pulled down blinds by the edge, the parallel slats slant down and make an envelope for a certain curve. In the limit of infinitely dense slats, what is this curve? In case you can't picture this, the curve is defined by the condition that the length of the tangent line from the curve to the y-axis is constant. This is one of the few cases where a reasonably general integral comes out naturally. As for the IVT/MVT, they are not particularly profound. It is in my opinion better to prove things by bisection (which easily proves both, gives intuitive proofs for all their consequences, and essentially is sequential compactness). Bisection was used in 19th century texts, but fell out of favor when the completeness of the reals became standardly axiomatized as the least upper bound principle. 

The reason this is obvious is because the deSitter space maximizes the horizon area, which is a measure of the entropy. 

The most destructive aspect of uncountable choice is that it conflicts with random choice. With uncountable choice, any object which is constructed using randomness, like a random walk, a random field, or even a randomly picked real number, cannot exist, because there are sets which it cannot consistently be assigned membership to. In order to define what it means to have a random walk, or a random graph, or a random infinite Ising model configuration, or whatever, you need to define what it means to have an infinite sequence of random coin flips. The result can be encoded as a real number, the binary digits of which are the results of the coin flip, and if this real number really exists, as an actual mathematical object, then this object either belongs to any given set S, or it doesn't. It is so intuitive to think of random objects this way, that they are often illustrated with pictures, showing us what they look like (see $URL$ for a picture of a "realization" of a random walk). These pictures do not signify anything when the axiom of choice is present. The reason is that once you have actual random objects, for which you can assign membership to any set S, then you can define the probability of landing in S by choosing random objects again and again, and asking what fraction of the time you land in S. This always converges, because given any long finite sequence of 1's and 0's which represent independent random events, any permutation of the 1's and 0's has the same likelihood. This means that it is probability 0 that the seqeunce will oscillate in any way, and with certainty it will converge to a unique answer. This answer is the measure of the set S, and every set is measurable in this universe. This makes analysis much easier, because everything is integrable, measurable, etc. This is so intuitive, that if you look at any probability paper, they will illustrate with random objects without hesistation, implicitly denying choice. (I realize that this answer overlaps with a previous one, but it corrects a serious central mistake in the former.)