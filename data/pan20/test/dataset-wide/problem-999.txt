This isn't a supported replication model as at MongoDB 3.2; a replica set can only have a single primary which accepts writes. An offline/disconnected secondary would be read-only. 

No. The chunk size determines the approximate size of documents expected to be represented by a chunk range (by default, 64MB). If a chunk range is observed to be approaching (or possibly exceeding) the configured chunk size, MongoDB will attempt to split that single chunk into multiple chunks representing smaller contiguous ranges of the shard key. Chunk splits update the sharded cluster metadata but do not involve copying documents between shards. MongoDB's sharded cluster balancer uses migration thresholds to determine when it is appropriate to migrate chunks and associated documents between available shards. If MongoDB cannot split a chunk that exceeds the configured chunk size (for example, due to a low cardinality shard key), that chunk will be marked as jumbo. Jumbo chunks are problematic as they will lead to an imbalance of data distribution within your sharded cluster: the balancer will no longer attempt to split or migrate jumbo chunks. The default chunk size works well for most deployments, however it is possible to modify the chunk size if there is a more appropriate setting for your deployment or use case. A smaller chunk size will lead to more frequent chunk split and migration activity; a larger chunk size will lead to more I/O for individual chunk operations such as migrations. The documentation for modifying the chunk size notes several considerations which are worth reviewing before changing this setting. 

I suspect you had MongoDB 2.4 installed previously. In MongoDB 2.4 the service was called and used (ref: Install MongoDB 2.4 on Ubuntu). With MongoDB 2.6 there was an attempt to have more standard package names across Linux distributions, so the service was renamed to (to reflect the actual daemon being started) and the config was changed to (ref: Install MongoDB 2.6 on Ubuntu). 

In current ER modelling, you would create a conceptual or logical model, capturing the requirements of users, then create a 3NF from it, to avoid inconsistencies and capture the correct multiplicities of the relationships (1:1, 1:N, N:M). Here you would also split entities, if necessary. For example split off addresses from customers, because you might need billing and shipping addresses. The last step would be to create the physical model, which considers indexes, surrogate keys, and careful denormalisation. This also includes considering advanced optimisations later on, like native types (e.g. double, int64, ...), types of indexes (btree vs bitmap, join indexes, etc.), special table layouts (row or column oriented, index organised tables, vertical or horizontal partitioning, inclusion of dependent rows). And this only mentions a few of the options that are available to the database designer in a current commercial database. The access path usually is determined by the sql engine and its optimiser on the basis what is known about the data (statistics, quantity structure) and the indexes available and can be looked up in the execution plan for each SQL statement. 

The usual way to solve such a problem, is to use a bitfield. So you would create a tags table, and link that via an n:m table to the sales figures or products. Then in the tags table, for each tag you would assign a unique bitvalue as a power of 2, e.g. for and so on. Using you can then condense these values into a single numeric value and store this alongside the product or sales figures. For example tags "sports" and "cricket" on one product become 5. If the bitsize of the numeric type you have available is not enough to store all your tags, use multiple of these fields and store the number or column name of the field and the bitvalue with the tag. Then for querying use clauses of the form: or = 10th flag set You can now do any Boolean expression on the flags. If you designate a single field for all colors you can also do other tricks, like querying for products, which have a Color tag: and so on. As you're in a column oriented database (redshift), is only executed once per unique value in the column. Depending on implementation, the database will further reduce this, by analysing the -clause and use constraints on size through sort order of column values (for free). And if you need that last bit of performance, you can do tricks by collecting statistics on the flags and the queries and grouping them together intelligently. I expect in the use case that you're describing (perform sum ... group by after filtering), performance that you could gain through this, would be negligible compared to the cost of calculation. 

A three member replica set has a fault tolerance that allows any one node to be unavailable while still maintaining a primary. If you have two nodes unavailable, the surviving node will become a secondary. With any read preference aside from you could still service reads in the sole survivor scenario. For more discussion on secondary reads, see: Can I use more replica nodes to scale?. 

If you don't have many user accounts on the config servers, recreate the administrator & user accounts. This isn't ideal, but is probably the fastest approach. Export the users from your mmap database. This is more involved, but saves you recreating the users & roles. I've described steps for this below. Redo the config server migration with the user & role information included. I expect this is the least desirable option. 

Authentication only controls access to clients connecting to a MongoDB deployment. Securing remote access to your server or encrypting your data files are separate security measures. If an attacker is able to gain access to a shell on your server and run privileged commands (like restarting the service), access control is irrelevant. Similarly, even if your data files were encrypted an attacker with privileged access will be a major security problem. As the administrator of a server, it is your responsibility to have appropriate access control, firewalls, auditing, and other security measures in place. Security involves multiple layers of defense, and access control is only a starting point. At a minimum you should also enable network encryption (TLS/SSL) and appropriately limit network exposure. For a list of recommended security measures, see the MongoDB Security Checklist. 

If you want to restore a backup into a replica set, you need to into the current primary. Data will be replicated to other replica set members via the normal means. You can use to drop each collection (if it already exists) before import, but in a full recovery scenario you would normally be rebuilding the replica set starting from an empty deployment. As at MongoDB 3.6, only inserts documents - it will not drop any existing databases or upsert/update/replace any existing documents. 

Install the PosgresQL ODBC driver and then use that to register Postgres tables in Access. Issuing the SQL you have given in Access will then transfer data from Access to PosgresQL. 

I suggest you have a look at the section on 'rating scales' in the complete guide to writing questionnaires: 

The "object"-reference in object-relational refers to object-oriented programming. This is a programming style where objects like a triangle or a square or some other geographical entity can be told to move itself (coordinates translated) or to rotate a certain degree or to scale some percentage, regardless, which class it is (triangle, square, whatever). As programmer, you don't know which class a particular object is and the programming language takes care of carrying out the correct calculations when you tell this object to move (change 3, 4 or more coordinates). An object-relational database now combines features of object-oriented programming and relational databases and takes care of converting between objects with methods (move, rotate and scale) and tables, which usually lack these methods. 

How are you going to use the rating? Do you need to calculate aggregates like mean, min or max? Then use numbers. If this is not meaningful, don't use numbers. What kind of where clauses do you expect? How will the code look like and how does your design contribute to avoiding errors and do your design choices support that the intention of the code is easily and readily conveyed? Last but not least: Design for clarity and less errors, optimize later. Hope this helps 

Please research data warehousing concepts like Operational Data Store (ODS), Star Schema, OLAP and ETL, which will definetely give performance and maintenance benefits compared to a physical copy (which I assume to be in some form of 3NF) 

Although you're using an ORM, which hides the nitty gritty details of SQL for your developers, you seem to be interested in different approaches to insert a record and a multitude of detail records in a child table. I would opt for the code, which conveys the intend most clearly and which can be maintained more easily. In my opinion, this will likely be the second option. Only if (measured) performance is an issue, then I would start optimizing. 

From a reliability standpoint of deadlocks per second the perfmon counter will be more accurate. The system_health monitor is based on ring buffers which can overflow over time or miss events if there is enough activity within a brief period in order to facilitate reliability. There is a file that backs this which you can look at which will give you more history but if you are experiencing as many deadlocks as you are indicating there is a distinct possibility that you may not have full history within these. This would explain your discrepancy between the two measurements, you aren't doing anything wrong from what I can see. The files will be located in your SQL Server install directory under %Program Files%\Microsoft SQL Server\MSSQL11.[Instance Name\MSSQLSERVER]\MSSQL\Log\system_health_*.xel. 

To answer the question of will you get the same results running an analysis such as Brent's sp_Blitz scripts in a non-production environment the answer will lie in what information you are trying to gather and what that information depends on. Take the two examples below as a means of showing the difference in results (or no difference) based on the type information sought. 1) Suppose I am interested in analyzing the indexes in the database to determine if I have duplicate indexes. To do this I would look at the database metadata which would be part of the database structure itself. In this case the results should be the same regardless of environment because the data that is being queried to draw the conclusion is part of the physical database structure. Brent's sp_IndexBlitz does this as one of the steps in its analysis, among others. 2) Suppose I am interested in analyzing an index to find out if the index is utilized. To do this I would examine the DMV (sys.dm_db_index_usage_stats) to determine if the index in question has any scans, seeks, lookups or updates. Using a combination of this data I could then determine if this index is making inserts run slower or is a benefit to select performance in a way that justifies its overhead. In this case though the data will be different in results between production and the non-production environment unless the exact same workload and configuration is running in both environments. Brent's sp_IndexBlitz also performs this same check and will provide the usage details based on the settings specified. To further clarify on the "why would data be different" which seems to be a sticking point here lets dig in to what DMVs are. A DMV is at high level just an abstraction layer that provides a view of the instrumentation that SQL Server has running. With this being said as mentioned by Tony when SQL Server restarts the data that is within these DMVs is not persisted. For this reason when you perform a backup and restore it is functionally equivalent to a server restart for the purposes of the discussion around DMVs. Once the database has been decoupled from the original host of this instrumentation data the data would be lost unless it was persisted elsewhere. This is mentioned in Microsoft's documentation as shown below: 

Updates in MongoDB are atomic at the document level, so you can achieve transaction-like semantics if single-document atomicity is sufficient. For more complicated updates involving multi-document transactions you will need to consider a design pattern like two phase commits. 

In this case the replica set member in question went stale almost two weeks earlier, so the maintenance mode counter has continued to creep up over time. There's a related issue in the MongoDB Jira you can watch/upvote: SERVER 23899: Reset maintenance mode when transitioning from too-stale to valid sync source. 

As per the MongoDB Extended JSON page you linked to, the shell can parse the strict mode Extended JSON (which conforms to the JSON standard) but without recognition of type information: 

Since the only possible update was to the array, indicates that "mango" was added. Value already exists in the array Add "mango" to the tasty array in the document: 

For shell sessions there is a concept of files which include JavaScript to execute when the shell starts. You can use this feature to extend or customise the behaviour of the interactive shell. If you want secondary reads to be allowed by default for all shell sessions you can either: 

There isn't enough information here to know if the "sync target falling more than 30s behind" was the case, but it seems likely if you are pushing through a lot of activity with all of your replica set nodes on the same machine. There should be some log entries confirming the change; if you're still on 2.6 you should be able to grep for: 

As at MongoDB 3.2, there is no feedback on the reason document validation failed: the overall validation expression currently evaluates as either True ("OK") or False ("Document failed validation"). Validation behaviour can be adjusted with (error/warn) and (strict/moderate/off) configuration options, but this does not provide any further context for validation failures. If you want to have more detailed feedback, the recommended approach would be to add validation logic into your application rather than relying solely on server-side checks. Even with server-side validation, many checks are best done in application business logic to minimize round trips to the database server and provide more responsive feedback to the end user. For example, user input for a web app (required fields, field formats, ...) should be validated in the browser before being submitted to your application or attempting to insert/update in the database. However, it does make sense to validate at multiple levels to ensure data quality and some context to diagnose validation failures would be very useful. There is a relevant open feature request you can watch/up-vote in the MongoDB issue tracker: SERVER-20547: Expose the reason an operation fails document validation. For more information you may also be interested in Document Validation - Part 1: Adding Just the Right Amount of Control Over Your Documents. This highlights some of the general pros & cons of document validation as at MongoDB 3.2, and includes a reference table for the outcome based on and configuration options.