I believe the answer is: yes, indeed, but can't say how. I am only assuming "Answer" status for this vague reply, due to Java problems with "Comments" at the moment. I have always understood that, yes, while early logical positivism and Russell's efforts "failed" in terms of philosophical completion, they were very influential in the development of computing. The reductive merging of logic, mathematics, and natural languages beginning with Frege is ancestral to our whole computerized world. However, I can't say how fundamentally genealogical this "ancestry" is. Can one just skip from Boole to Turing and Shannon without the whole Principia interlude? Did that project set a generation of logicians on the digital path? I'm sure others can answer that. Unfortunately, I seem to have got this idea by osmosis and have no particular references to cite. I imagine it wouldn't be hard to google up the genealogy. 

I never down vote or "close" or otherwise go negative, but I agree with @virmaior the only "philosophical" answers to this question will be about the structure of the question itself. Either we are being asked to validate the simple functions of "game theory" or we being asked to validate dumping infinitely many human complexities into this function. Neither course falls, in my view, under the intermediating remit of philosophy. Here, philosophy defers to the dramatists, i.e. Miller and Mamet, in this case. While the question of the moral status of "salesmanship" may be interesting, it must be tied to something beyond a purely utilitarian calculus if it is to be in some sense philosophically disinterested. That is, in my own view, a connection that can only be made through Marxism. Though other approaches might detour though Smith or even Machiavelli. In any case, game theory is very significant in one logical sense, but tic-tac-toe in another. I agree that this is not yet a philosophical question. Too human (dramatists) on one side, too systems-based (machine logic) on the other. The struggle since Kant has been to mediate the two. 

There is at least one ludicrously simple "proof" that you are not a brain in a vat. First, we must give up the idea that such a scenario can be "true" or "false" in any absolutely inarguable sense. In other words, we address the question as "moderns" free from dogmatic or authoritative answers. I am assuming this is okay with you, as a premise... for now. Second, as so-called moderns, without appeal to dogmatism, we must ask: what is our best recourse to knowledge? Our current position as "moderns" seems to be that we can appeal neither to strict rationalism nor to strict empiricism as grounds of certainty. (Without going into detail, I refer here to Kant's critique of Leibniz and Hume, but there are many others.) We appeal to an "in-between" combination of the two positions, theoretical reason and sensory evidence, called very generally the "scientific method." This is a global skepticism limited and constrained by replicable, sensory demonstration--or, put the other way around--falsifiability. All boring. But let us now ask about brain in vat (BIV) proofs. You asked about this possibility relative to "yourself." This self "you" has a "world" and wonders skeptically if it can be globally falsified. Let's start from that position, your "world." And let's skip over the infinite regress in which your world in in your brain and yet brains are in your world. Given your best recourse to knowledge, in your world is there scientific evidence that there could be such a thing as a "brain" alive and thinking in a "vat"? Not only has such a thing never been demonstrated, its "physical" possibility seems exceedingly remote. As far as science can currently confirm the "vat" capable of recreating a world would have to be as complex as a human body and the planetary ecosystem that sustains it. In other words, big, very complex and currently irreducible "vat." There are even bigger problems with this ludicrous idea that your total reality can be reduced to the two external objects "brain" and "vat." First, no matter how you define them, they are objective subsets of your "world." The idea of "brain in vat" simulating the world it is "inside" has never been demonstrated and cannot be falsified. Lastly, the very idea of a (BIV) simulating a world containing you-me as the simulators only occurs to us through the accepted "truth" that this is itself a "false" simulation. The very idea of simulations subsuming any scientific standard of "reality" is always presented to us, quite self-consciously, as a self-evident falsification, a movie, etc. Never, even roughly, as science. So your choice is to opt either for our best current definition of "justified belief" or for the representational regress of "simulations" captured in the liar's paradox. 

You are absolutely right. As Hume's famous "problem of induction" suggests, the difference between correlation and causation may be purely numerical. Attributions of "causation" simply assume that the "past resembles the present" and "the future resembles the present," and that what happened "before" is our best Bayesian prediction of what will happen in the future. That really is our best scientific consensus. Each "guess" gets stronger the longer it survives and thus becomes a surviving "prediction." We predict the sun will rise tomorrow, because it has risen for millions of tomorrows. We have no better application to certainty. Certainty is only a reduction of possibility. However, this is why modern "causation" is, in many people's view, a very suspect concept, and Aristotelean causation may be making a comeback, I really don't know. We now see ideas like "overdetermination" or "strange attractors" or "statistical states" or various other mediums of change introduced into even physical science in the place of old "billiard-ball"causation. It might be better now to say that "causation" is a subset of "correlations." As you suggest. What limits it to this subset would seem to be that it happened "before" the effect. Before and after. Which leads us, unwillingly and miserably, into theories of time, which are the most difficult and unsettled regions of philosophy and science. 

In the first place, the term "postmodern" is pretty flabby and possibly useless today. It implies no single perspective on science, per se, and could include thinkers such as Kuhn and Feyerabend who are certainly committed to science, while attempting to reveal some its linguistic and social conditions. The term is best applied in aesthetics to described movements in art and architecture postdating those styles dubbed "modern." It is also applied in Marxist theory, where it refers to a later stage of decentralized capitalist development and its cultural-institutional ramifications. One symptom of this stage is a world so dominated by commodities, copies, and representations that the idea of something "original" being represented slides into doubt. So, yes, this could include any idea of "foundational truths." But this problem arises long before Messrs. Baudrillard and Derrida. Indeed, we could place it on the doorstep of Newton himself, who described "gravity" mathematically while refusing to say what it "really is." He simply sidestepped the metaphysical questions of "fundamental truth" and all working scientists followed him. The problems of "fundamental truth" continue famously through Descartes and Kant, who clears the field once and for all. Human knowledge is and must be "conditional" at some level, which we might call... well, the "human condition." Again, Kant intended his work partly as a defense of scientific knowledge, Newtonian mechanics in particular. Science can only progress in practice by accepting a probabilistic, conditional construction of knowledge that is freed from any requirement to demonstrate "fundamental truth," much as some scientists may say otherwise. The so-called science wars did indeed reveal a lot of hubristic silliness among the departmental epigones of "postmodern" theory, and the Sokal prank made its little point. And the field is full of "tricksters of skepticism" who revel in debunking unexamined certainties, sliding signifiers, and evolving definitional categories. Even "war" is a construction of "news" which is a construction of...etc. Much of this is simply rooting out from the deepest cultural levels the "Gods-Eye-View" or "View-From-Nowhere" or "Master Narratives," the "objectivity" that dominates societies through overweening assumptions. A kind of therapy by provocation. Practicing scientists could care less, but are deeply offended that their own work may be open to such skeptical attack. This is a valid, Kantian sort of worry at least on the level of social consensus and stability. No truth? Then we all run wild? But if you actually read the scientists who are philosophically inclined, from Mach to Bohr you will see that "truths" have always been a problematic issue for science. And it is equally the case that many notorious philosophers from Heidegger to Derrida, who are deeply concerned with the operations of language, are by no means just spouting jargon and know far more mathematics and logic than you might think. There is a lot of printed nonsense that travels under the name "postmodern," because philosophy and cultural theory do not have the same axiomatic and consensual filtering systems as "hard science." But would we really want them to? And while they may not think of their work this way, the "hard scientists" themselves are always chipping away in the ongoing effort to falsify today's "fundamental truths." 

In addition to the above, I would note that there seem to be a number of meanings attached to the term "Mathematical Platonism," apart from what might be called Plato's mathematics and idealism. Russell and Whitehead's Principia could be described as a Mathematical Platonism reduced to logical forms and including numbers. Godel wrote about his own mathematical Platonism, which was quite "Platonic." Penelope Maddy and a few other philosophers, on the other hand, have attempted to develop a "Mathematical Platonism," including numbers, based on set theory and a "naturalized epistemology" that correlates "sets" with "object identity" in child development and the creation of neural "cell assemblies." So the term is not even restricted to idealism. Personally, I find it fascinating that geometry in Plato's day eschewed numbers and demonstrated proofs using only the compass and straight edge. This meant that the "Forms" were presumably "Performed" through hand motions, not unlike music, sign language, or carpentry. A very different, and perhaps superior, way of developing mathematical intuitions. A shame it is no longer taught this way. 

Not being religious, I'll focus on the argument that religion is inherently inimical to science and an obstruction to technological development. I would say science and religion are not necessarily opposed to one another, but are and must remain simply incommensurable. Historically, your case is doubtful. Religion was not an obvious impediment to the rise of modern science and technology. Scientific method arose within the cultural milieux of Christianity. It was nurtured within the church universities and the preparatory grounds of Aristotelianism. Most of the great figures in the rise of science were quite devout, though certainly not all. Some, such as Faraday, appear to have explicitly benefitted from "religious intuitions," as opposed to mechanical models. Certainly the practice of science requires leaving God at the doorstep. But one could hardly make an argument, on the cultural, historical, or personal level, that atheism is a prerequisite to the advance of science. The many instances of suppression of science by religion, and vice versa, may say more about political power and authority in general than some inherent conflict. Even today, I doubt that a "scientific study" among cultures and practitioners would demonstrate some clear positive correlation between atheism and scientific aptitude. It is the political and epistemological separation of powers that matters. There is one broader case to consider. In Marxist societies we do have an explicit attempt to suppress religion while accelerating science and technology. These great experiments were more successful than many care to admit. Yet even as a Marxist of sorts, I would not say the experiment was entirely successful, even for science per se. The problem with "eradicating religion" for social ends is akin to one of ecological hubris. One eradicates one "pest" only to find it returning in another guise. Slicing into the whole complex of values, customs, and beliefs may give rise to other totalizing beliefs and scientific Lysenkoism. The "God that Failed" was not an inapt description of the Soviet experiment. Again, it is the separation of epistemological powers that matters, not the absolute purity of secularization. Even in the notoriously "religious" United States, the most serious threat to "scientific advance" comes not so much from Bible thumpers as from "research capture" by the purely utilitarian ends of corporations. Science is a method and a means. It is only a meaningful end in itself "for scientists." Nonscientists participate only as passive consumers. Thus "ends-in-themselves" get generated elsewhere. Sometimes as harmless "culture" sometimes as disastrous "nationalism." I would say that since the Enlightenment, there has been a crisis of purpose, fueling new powers to fill the vacuum left by the collapse of the universal church. Nationalism, marxism, capitalism, art... all of these secularizations can also generate their own "obstacles to science," simply because science cannot generate its own "end-in-itself" or meaningful ends for most individuals. Its method works not by subsuming "obsolete" values, but by limiting and separating its methods from other values. It must always survive in some environment of "irrational, groundless values" by epistemologically disentangling itself. 

This is probably not the place to look for "anti-philosophical" arguments.I would only add that Kant, Hegel, and Heidegger are undoubtedly the philosophers most reviled by "scientific" or "analytical" philosophers, beginning with Moore and Russell. The objection is most precisely and concisely expressed by Ayer. These "metaphysical" thinkers talk about things that have no real physical meaning or correspondence to experience, but only appear to have meaning in the context of sentences and paragraphs. Because you can say "grammatically correct" things about "unicorns" or "consciousness" these appear to be "real" things. From this genesis came a long line of anti-philosophical philosophers and "natural philosophers" also known as "scientists." But as Hegel himself said "you can't avoid metaphysics." The certainties of science quickly fall prey to skepticism. The smallest particle enters an infinite regress into smaller particles. Soon enough, physicist are struggling to define the "physical" and find themselves one again sucked into metaphysics. 

If science gave family names to "forces," we might call it polytheistic. You are right to point out that the Abrahamic traditions are unique and problematic, to say the least. Jaspers makes a good case for an "axial" shift towards universalism in religion around 200-0 BC. But "universal" Abrahamic, "Big Dad" interpretations are highly problematic. In my own view, the tragedy of the West was the political attachment of the Old and New Testaments, with a rich hermeneutics but no proper epistemological basis. Polytheism can pass into "science" with little interpretive difficulty. But the Abrahamic tradition of patriarchal conceptual hierarchy was already there ...in the first texts...fostering the hermeneutical tradition, the necessity of ceaseless interpretation. For philosophy, the difference between "polytheistic" and "monotheistic" is simply a version of ancient "one-many" discussions. 

I asked this in a previous form, but since I asked for a "proof" of mind-independent objects I understandably got few takers. It appears that ever since the "modern" subjective turn, from Descartes through Locke and Kant, there has been a lingering agnosticism concerning the "real" existence of external or mind-independent objects. Significantly, this was never even an issue for the ancients, though "appearances" were. Even for modern empiricists and positivists it is not the naive "object" that is directly perceived but our "idea" of the object. Yet science gets on with its business, and even philosophical language seems to fall back on a common-sense ontology, "sense-data" models, or a phenomenology that simply dispenses with the problem. Due to big gaps in my reading, I am unclear how this problem has been typically treated. What are the most influtential contemporary arguments for a common sense ontology or the existence of mind-independent objects? Or does the matter simply continue in a state of "as if" agnosticism? 

At the simplest level the PL is "incoherent" in the manner of an oxymoron. A little like saying it is possible to "communicate without communicating." More to the point, it might be harder than you think to create your truly "private" language. In the first place, it would have be absolutely unique and, in theory, indecipherable. So you can't just "translate" the grammar and words of some language you already know by some term-grinding equation. Easily reverse engineered. You might "randomize" or "scramble" an existing language by some operation. But if even you can find no "meanings" in the result, is it really your "language"? Finally, how would this language distinguish itself from the continuity of your "private" thoughts? It must somehow become discontinuous with your own thoughts while also modeling them... and then reappearing as "recognizable" to them. Hard to do entirely on your own. And should you ever succeed, how would you convince others of your unique epistemological accomplishment? I think Wittgenstein's point was, as usual, to draw attention to something rather obvious that gets lost only in philosophy, where so many projects begin in some sort of isolated inner dialogue. This is the epistemological version of what Marx called "Robinsonism," the propensity of modern thinkers to construct theories around some atomic Robison Crusoe who just appears there, spontaneously generated, with all his ideas and tools.