Disable query_cache: you don't get too much benefit from it anyway and it's a common contention point. Turn on slow query log with analyse your queries and fix your unindexed joins. From your mysqltuner output: pt-query-digest can be a great help in that. Look at the mutexes a.) Check the output of b.) If you have performance_schema and mutex_instances table check . If not I strongly recommend to set it up. Maybe your queries lock up each other. And simply semaphore spins use up the cpu time. Fix any obvious queries showing up as a possible "intruder". You can look for select for updates, insert .. on duplicate key update .. queries which are very common cause of this behaviour. can give you more information where mysql spends most of the CPU time in. If you get to this point you can form a much more specific question about how to overcome that certain problem. 

We use Quest Spotlight for database server monitoring. Depending on when SQL server decides to fail over automatically, we will get an alert stating that a database has not been backed up in over three days (the factory default) if that server has not been primary in over 72 hours. My suspicion is that SQL Server does NOT replicate this data, but I wanted to hear from the community since a quick Google search didn't provide anything. Plus, I am travelling and do not have time to test and I need to provide a fairly immediate response. 

I've had a maintenance plan in place for years that has run without fail and the associate logs backup this claim. Also, the backups are time stamped and in the place expected. I did a restore in a test environment and ran a consistency check without any errors. I'm not sure that I really see this as a true error or cause for concern considering the aforementioned evidence. So, maybe this is just an anomalous event? Either way, I'd like to poke around and a assuage my sensibilities. Has anyone ran into this situation before? If so, what did you do to investigate? 

You didn't attach the table schemas so I just assume this would work. You might need to fully qualify the field names (using tablename.fieldname). Also feeds_processing_data could benefit from a composite index covering the involved fields. 

There's no silver bullet to tell you how many queries per second can you process. You can set up different benchmarks to try to estimate. A 'good enough' approach is to register the queries with tcpdump and replay them from external server(s) from multiple parallel threads. You can also play with the concurrency. Register the query response time and plot it as the function of queries / second. When response time is above acceptable threshold that's your maximum queries / seconds for specific concurrency level. I wrote a script some time ago to do the same thing with HTTP requests (the collection is not part of it) and report maximum requests / second: $URL$ It shouldn't be hard to translate to mysql queries. If you use replication to balance reads you can also start reducing the number of slaves until the query response time is above the threshold and check then the current query rate is your limit. You can only reduce the number queries if you change the application that sending those queries. Introducing caching, rewriting queries, etc. There are plethora of options but that really depends on your case. Maybe it's completely valid from your application to send that amount of queries and you just need to scale up or out your infrastructure. 

I know that there are times where the vendor would rather their builtin solution manage the maintenance. I'm sure I could find that feature in the application and turn it off, but this question is in case that I can not. How would I configure or edit this job to ignore a specific database or a list of databases in the same instance? 

We created a SQL Login account for reporting use only (SSRS), we'll call it 'MyReportAcct'. We've used this account for some time and it is created for every newly installed instance. It has been working as you'd expect, except on a couple of mirrored instances. I've not seen this issue on any standalone instances in my environment. Randomly, reports will stop working and there is nothing being changed based on our internal processes for Change Management, Windows Logs, and SQL Error logs. Here are two of the error messages we receive: 

That's not specific to replication. It happens even if you would want to restore it to an empty db. On the replication side 1) One way to do it is mysqldump but you need to LOCK the tables or make sure you don't get any writes during the process of dump (read_only=1 for example). Otherwise you get your slave out of sync from the start. Also there is certainly an impact on the application which is connected to the master. More on this you can read on the mysql doc: $URL$ To same or higher major version (these cannot be done if master is 5.6 and slave is 5.5): 2) You can also retrieve the data without impacting the master with hot backup solutions like percona Xtrabackup ($URL$ This is probably going to be faster as well. Once you applied the logs it behaves as normal mysql binaries and you can do the change master command. You will have an xtrabackup_binlog_info file which contains the master position you need to point the slave to. 3) You can stop the master after acquired the master position, copy the binary to the slave, start mysql and change master to ... with the position you saved. This could be also done by snapshotting the filesystem (LVM, ZFS, BTRFS, etc.) if you want to quickly restart mysql. Usually this is the fastest way. If your version mismatch you might need to run mysql_upgrade since with the latter two you're copying binary data not logical. A sidenote: Why are you replicating from 5.6 to 5.5? Replicating from higher version to lower is not guaranteed to work. 

Our Process: We perform/make/implement our updates to the SSIS packages using Visual Studio (v 15.7.0) on the local SSIS SQL server (Microsoft Windows Server 2016 Standard) save the package to a file system drive not located on the server. We then open SSMS on the local SSIS SQL server (v 17.6) connect to the Integration Services on the Local (SSIS SQL Server) and import the File System Package into the “File System Stored Packages” using the GUI. i.e. Right clicking the “File System” note and selecting Import: 

In short, put the OSes on slow storage and all the data/log & backup files on fast storage. I know that my environment is different that yours, but I think similar principles apply. We have a SAN and the storage pools range in different speeds, 7.2k, 10k, 15k. We install the OSes on the slower drive pools and have all backup files and data/log files stored on the 15k drives. There was a time when we accidentally put everything on the 15k drives and we didn't notice a performance difference. It was brought to our attention by our SAN admin after an audit that the OS was stored on the faster drives. We monitor SQL Server performance with perfmon and Dell Spotlight. For grins, we went and looked at the historical metrics that mattered, and saw nothing to note. 

This is a more generic query and will give you back better results if the data is somehow not completely correct. 

You can also include which will give you the possibility to order by relevance. To read more about this you can take a look on a benchmark I did with MySQL Full text search alternatives: $URL$ Option #2 You can use some search engines which provides great full text search capabilities out of the box like Solr or Elasticsearch. A very simple solution for what you want is to use the facet and facet filter with parameters like this: 

copy current version of row update as requested append new current version to a chained list of version 

Everything in the result set was correct, save monitor_server, it had the value of OddServer03! As many of you know, when configuring Log Shipping through the GUI, it is damn near impossible to "accidentally" configure this option. We never intended having a monitor server in the mix. We use Quest Spotlight to monitor the aforementioned servers which will yield alerts when jobs fail or when log shipping gets behind. One other perplexing matter, the job that is getting created is instead of . Why is the service account trying to authenticate to a server that isn't part of the configuration? 

I find it hard to believe that there aren't better native tools to filter SQL Server Error Logs. Even Windows Logs have better filtering capabilities. I'd like the ability to remove certain events from the results or a list of events from the results or to be able to search for a very specific error/event. 

MySQL is using a relatively simple (simpler than other RDBMS) cost model for planning queries in which filtering your dataset has quite high priority. In your first query with the merge index it is estimated that scanning ~9000 rows is going to be necessary while the second with the index hint will require 18000. My bet would be that this weighs in the calculation enough to move the scale towards the merge. You can confirm this (or find other reasons) by turning on, run your query and evaluate the results. 

Then having internet and TV would be 110 = 6. But this is going to be hard to search for and requires some magic to do bitwise operations to extract the features. A middle ground would be to use SET and store the features which the real estate has aka. values that are set to yes. The downside is that each time you need a new feature it requires schema change and 

This server was recently installed and configured. I was in the process of creating some maintenance plans to backup the logs and DBs on a daily basis. Regardless of my method to perform a full backup, it would fail immediately citing the generic error message 

Having 10s of thousands of error messages per week makes it pretty difficult to peruse and check other possible issues 

I've recently audited a SharePoint server and found configurations that are against best practice. I'm hoping there is better and quicker way than right-clicking and changing these settings manually on over 100 database. For instance, all the databases are set to autogrow by 10MB and the log files are set to autogrow by 1%. I'd like for these to be a set number and not a percentage. Any resources or recommendations would be greatly appreciated! 

Do you actually need this? You should be able to drop this to a much lower level and like 8MB or 16Mb and monitor the status variable. 

Then you can see all the queries being executed in the file. 2# Another option is to use slow logs with . This you can manage on session level: 

You didn't write which PG version do you have. A better approach is to have LATERAL JOINs (available since 9.3): 

Partial indexes make sense when they match the query pattern. In your case if you would have queries with condition then the index would be used otherwise it's simply not useful. Cannot use the partial index: