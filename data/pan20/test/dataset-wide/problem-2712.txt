Assuming the Judeo-Christian God. 1) The modern Christian belief system asserts a "fallen" existence. Meaning that at one point everything was perfect, but things have become corrupt by the introduction of sin into the world. This is typically referred to as the fall. The high may be a result of this and therefore not part of the perfection that God intended. 2) The "high" itself is not the evil for most peoples perspective, rather the obsession with that high is the problem. This also may be a result of the fall as described above. 3) It is possible, even likely, that a benevolent God created materials which would result in pleasant feelings. Just as sexual intercourse results in pleasant feelings. Presumedly a benevolent God would create the means to have pleasant experiences as part of his creation. 

I would say that we can divide criticism to logic as pre-logic and post-logic. In pre-logic criticisms, people don't quite understand logic and are resistant to it already. Often they have only that stereotypical notion that logic turns a person into a non-sensitive robot. All their contact with logic was superficial and at a distance. In post-logic criticisms, someone knows logic well enough to get close to its boundaries. Then that person can start to discern where logic can or cannot go, and even deal with that kind of statement that "I can imagine such and such". Probably the person you're refering to is in the first category, but using ideas that are dealt with by people in the second one. The approach I try in those cases is usually acknowledge what the person is arguing, but saying that those things can only be really dealt with after you have some good grasp of logic. And to do so, first you have to get within its boundaries, and within its boundaries, necessary truths are... necessary. :) Besides, when a person argues like that, they are usually trying to make a point by using logic - at least in some sense. They are trying to make you conclude, by the premise of their imagination, that there can be such a universe. Logic is a wonderful system that we stumbled upon - maybe by inventing it, maybe by discovering it - that for some reason works amazingly well. And I think that is enough for us to give it a good credit, even if afterwards we question its limits. 

I will assume the current psychological definition of a self. There are many other definitions, if you meant one of those please let us know. An additional assumption I am considering a healthy individual, so we will not be talking about whether multiple personalities constitute different "self" or not. Further, there is an assumption that the self is a single entity in your question so I will not be talking about conversations between say the ego and the id. Given those constraints, I agree with @Alex there is only one person present. As for the follow on question, why do it then? I expect that we do this because the act of speaking changes how we think about a topic. It requires more precision and effort. Helps us to clarify our thoughts. 

There are two things here. First, maybe ad hominem was historically born for using with individuals (I don't know about its history). But since its purpose is to point to a logical error, I don't see why one could not use it to refer to the same error, only applied to institutions. I don't know of a fallacy specific to insitutions, so I don't see a problem in referring to ad hominem. Well, that said, there's the second thing. A fallacy is usually an error in argument. In this case, if one says "A because of B", I'm commiting ad hominem if I argue that it is wrong because the person saying is ugly. That is because the ugliness of a person is irrelevant to the argument's truth and validity. Even if we use "stupid" instead of "ugly", it would still be a fallacy. Even though "stupidity" may hinder someone's capability of argumentation, their argument should be considered on its own merits. But that doesn't mean that anything citing someone's characteristic is an ad hominem. For example, if I say "A because of B", there is the premise B and the conclusion A. You could challenge my conclusion by saying that A logically does not follow B. Or you might challenge my premise B. If you know I'm a pathological liar, and you don't know if B is true, then you have all the right to question my premise B based on my history of lying. So you can't say that my logic is wrong because I'm a regular liar, but you can doubt the truthness of B. Yes, it is "against the person" in a way, but not necessarily irrelevant to the argument. Hence an ad hominem fallacy does not necessarily apply. 

The Problem of Induction does represent a problem for the philosophy that is often referred to as science. That empirical reasoning is not "proven" may or may not be a problem for you personally. Empiricism has lead to knowledge that is closer to true then what we had before. So empiricism is useful as the computer you are using to view this demonstrates. Is our current understanding of empiricism "true"? Almost certainly not. That is part of the philosophy of the sciences. The assumption that our current understanding is flawed in some way. This encourages people to suggest improvements. As a result our understanding improves over time. So no I do not have an issue with the problem of induction, it only represent another area where our understanding could improve. That science currently has flaws in its understanding of things is a strength, rather than a weakness. 

My background is in Computer Science, so when I started reading philosophy seriously, I had a similar reaction. A lot (maybe most) of what I've read (that is not that much, I admit) from the great philosophers usually ends up falling into one or more of those kind of problems, specially the ones regarding logical rigorousness. But some philosophers like Hegel, Wittgenstein, Russel, etc did know a lot about logic. So let me say how I think this can be handled (although it's not completely solved to me): 

I personally fit better in 2b. I'd love to read everything about every philosopher, but I have neither the motivation nor the time. So usually I prefer to read about previous philosophers by contemporary ones, and read from the originals when I think an idea is worth understanding better. And remember, in philosophy even logic is open for debate, so you will find that sometimes what you have with some philosophers is a different set of premises, including the ones that do not value logic that much. 

There is no universal answer to the question "Is it ethically wrong to kill an innocent person (or people) for the purpose of self-preservation?" The answer depends on which ethical framework you are using. As you point out Utilitarianism would ask which provides the greater utility. Whereas a Common Good ethical framework would ask which provides for the greatest common good? I can easily imagine cases where the preferable answer would go either way. 

That depends on how one defines "the universe". Assuming you include yourself as part of the Universe and the Universe also include everything that exists, then any free will you have is "completely governed by the universe". 

The definition of alive is often given as "not dead". Ironically, the definition of dead is often given as "not alive". I got those from a online definition search. Biologist struggle to define those things that are alive. Though most would agree that a squirrel is alive, there is some debate about a virus and a prion really pushes the envelope. The best definition I have found for what is alive is an entity that can acquire "food", metabolize and act. I put food in quotes on food as it may not be what you expect. Plants "eats" sunlight for instance. Given that definition a Human is dead when they can no longer do those things. 

I have some formal training in philosophy, but I have lots of gaps in my knowledge. I mainly read what interests me, so the same gaps tend to remain - usually about things that doesn't draw my attention that much. A even bigger problem is that I don't even know how large those gaps are. I'm trying to find a good resource to help me handle that. It'd be great to have something made with that purpose - tracking what I don't know -, but I think it's probably too specific to exist. So I was thinking about maybe using a philosophy undergrad program and go through its reading lists as well as assignments. I've searched for a full program in a bunch of universities but only got partial ones so far. Where can I find a full undergrad program in philosophy - preferably by a good university -, including all readings and assignments? Also, any other type of resources that could help me acomplish closing those gaps? Notice that I don't want a history of philosophy book or anything similar. Although the history of philosophy is part of what I want, my main purpose here is not historical. 

I notice two things that run through your steps. The first it is brought up again and again that the method (tool) used to implement consciousness does not inherently matter, but I believe it does. The book is not conscience because it can take no action (it is inert). The computer may be conscience because it can take action. I believe the ability to take action is at least one requirement for being conscience. The second is that the steps accept input from an external source. Is a remotely controlled robot conscience? Is there a difference between the "lookup table" and a remote pilot to a robot? I doubt people would see a remotely controlled robot as conscience, rather they would look to the controller. The flaws come in part from a poorly defined term "conscience". First, let me offer that I believe you mean consciousness. Consciousness is the state or quality of awareness, or, of being aware of an external object or something within oneself. We sometimes say being self-aware. Implied in the definition is that this awareness is a self-contained action. It does not come by virtue of a look-up table or any external other input. I am not saying that it is not learned, just that if learned, it does not require input to be maintained. Assuming that you accept that definition then step three is not valid because the system is not self-contained. Finally, all of the above steps assume a computational model for consciousness. That is to say that consciousness can be mapped into specific steps and choices based on inputs. You have not demonstrated that this is true. The requirements for the tool to implement consciousness depend on the answer to this question. So steps 1,2,4 and 5 are ambiguous.