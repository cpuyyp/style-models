Applicable to what? Meaning assembly? For this, λ-calculus is used only in categorical grammars, which are binary-branching by definition. In syntactic frameworks with a context-free backbone, glue semantics (based on linear logic) is used at times, but it has problems, too. Meaning assembly via composition rules is best done using unification. If one uses Davidsonian (or neo-Davidsonian, i.e., Parsonsian) logical forms, every phrase (including preterminals) is associated with an LF fragment and an individual. If you have a ternary (or, in general, a nonbinary) rule, the individual of a subordinated phrase is unified with a variable in the LF fragment of the head. For example, the predicate of "give" is quaternary and the corresponding rule is VP -> V NP NP. Then the individual associated with V (the eventuality) is unified with that of VP. The individual associated with the indirect object is unified with the fourth argument of the predicate of the verb, etc. The subject variable remains open until it's unified later by another rule. Since (neo-)Davidsonian formulae are existentially closed conjunctions of literals, when the parsing is completed we take all literals used during parsing to be conjuncts and add a quantifier for each variable occurring in the LF. The above procedure can be informally described as a "relaxed" lambda calculus, but rather than replace variables we unify them. It can also be used in dependency grammars if we add unifications to ID rules. 

We can derive from (1) a true sentence if we replace the pronoun she by the definite description the witch reported on in the Gotham Star, but this definite description may not be available to any of the participants in the conversation (perhaps none of them reads the newspapers). The point is that the sentence in (1) may be true even if all the sentences derived from it by replacing she with a contextually available definite description are false. 

The question your teacher gave you has already been discussed by Barbara Partee in her introductory paper ‘Lexical Semantics and Compositionality’ (see reference at the end) - only she used almost half empty and almost half full. If we assume that the constituent structure of both expressions is as in (1), and also make the assumption in (2), the question then arises how come the two expressions differ in meaning. 

I might also add to the discussion the question of grammatical function. In theories that include the specification of grammatical function (subject, object, etc.) in the syntactic representations they posit, the question arises what grammatical functions can be realized by non-finite VPs (or clauses). There is a discussion of this question by Rodney Huddleston in Chapter 14 of The Cambridge Grammar of the English Language (pp. 1206-1220). He argues that non-finite clause complements in English usually do not realize the object function (which is mainly realized by NPs), but a different function, which he dubs catenative complement. A discussion of the grammatical functions of finite clause complements across several languages can be found in the paper “The grammatical functions of complement clauses” by Mary Dalrymple and Helge Lødrup (which can be found on the second author’s webpage). In case you might find it useful, here is a tree for your sentence that follows by and large the syntactic framework in The Cambridge Grammar of the English Language. 

You are right that the auxiliary is merely a TAM carrier, it's a function word without any meaning of its own. It's however completely logical to take it to be the structural head of the "verb phrase" (the whole IP). People often confuse categorial and functional heads. The auxiliary is the head of the IP because we can "observe" this constituent in the sentence. However the functional head is the V because it's the (only) content word giving the IP its meaning. (The situation would be a little more complicated in the presence of a light verb.) In sum, it's safe to say that taking the auxiliary to be a specifier would be (as you put it) "a no go in any variant of X' theory", at least as far as English is concerned. 

Clausal arity can't be expressed structurally at the surface level. I'll modify the Catalan example (I'll use llegir "to read") and add two more phrases: 

There is only stylistic difference. Attributes in Latin can precede or follow the head noun so both are well-formed and have the same meaning. 

It's valid insofar as linguistics studies both. The two issues are closely related. To express "I have seen him" one can use four words (as in English) or one word. Morphology, syntax, and lexical relations vary greatly across languages. One can study one language (synchronically or diachronically) or one can learn 10 or 20 languages and focus on comparative linguistics, typology, or formal grammar theories. 

Subject and object are names of grammatical function categories. The grammatical function of a constituent in a sentence is determined by its relation to other constituents in the sentence. The subject, for example, is what the meaning of the predicate is predicated of, and it also agrees with the head of the predicate - the main verb, if there is one - in person, number and gender. Note that verb is not a grammatical function category but a syntactic category. The verb in a sentence realizes the grammatical function of head of the predicate (sometimes called the predicator), as can be seen in the syntactic structure tree below, where the grammatical function categories appears in blue. You can read a little more about this distinction here. 

Adopting (2) means that according to (1), we derive the meaning of almost half crazy by first combining the meanings of half and crazy and then combining the meaning of almost with that of half crazy. Since half crazy has the same meaning as half sane, it’s not clear how almost gives one meaning when combined with the first and another meaning when combined with the second. One way to solve the problem is to deny that the expressions almost half crazy/sane have the structure in (1) and assign them instead the constituent structure in (3), which is also given by the tree in (4). 

The sentence will be a CP in traditional GG: [CP [C have] [IP I seen the man]]. Note that constituent trees represent sentence topology rather than grammatical relations. In the example at hand, "have" and "seen" are coheads but they don't appear contiguously in the sentence. 

Some Native American languages obligatorily mark sentence perspective. Aymara is one of them and the following paper provides many examples: Pragmatic Structures in Aymara 

where e is an eventuality (also called situation, possible event, or state of affairs). Such logical forms can express everything one can encounter in language (such as quantification and logical connectives) so there's no reason not to use them if it helps elsewhere. And help it does a lot in pragmatically interpreting discourse. Meaning assembly that produces this kind of logical forms can be easily implemented (with or without lambda calculus) in both phrase-based and dependency-based grammar formalisms. 

Lambda calculus is a way of turning open expressions (that is, expressions with free variables) into functions. For example, λx.x+1 is a function that takes numbers to numbers. λx.x+y is a function from numbers to expressions with one free variable (if the domain of discourse are numbers). In natural language semantics, lambda calculus can be used to assemble meaning during parsing. The idea is that every word has a meaning (assigned to it in the lexicon) and syntax helps assign meaning to more complex syntactic units. In higher-order logic, the meaning of "Mary obviously loves John" is 

Assume someone utters this sentence to a group of people who are familiar with Hob, Bob, Nob and Cob and their livestock, but who, up until the moment the utterance of (1) is made, have been unaware of Hob’s and Nob’s belief that a witch has been harming farm animals. Assume further that all the participants in the conversation rightly believe that witches don’t exist and that Hob and Nob are wrong. Such a context provides the two definite descriptions in (2) as possible substitutes for the pronoun she in (1): 

Edelberg shows that there are circumstances in which the sentence in (1) is true, but the sentences derived by substituting (2a) and (2b) for she in (1) are false. He gives the example in (3) (repeated with omissions and slight modifications from his EXAMPLE 2, p. 2): 

According to (3) and (4), in deriving the meanings of the expressions almost half crazy/sane, the meaning of almost half combines each time with another meaning, and therefore it is not surprising that the two final expressions differ in meaning. A second way to solve the problem is to accept the constituent structure in (1), but to deny that half crazy and half sane have the same meaning. Maybe the meanings of half crazy and half sane always combine with some approximation/precision value, and when such value is not expressed it is taken to be the absolute precision value (the value expressed by exactly). We can now assume that the meanings of half crazy and half sane are such that they give the same meaning when combining with the meaning of exactly but not when combining with the meanings of other approximation/precision expressions (like almost). A third way to solve the problem is to accept the constituent structure in (1), but to deny the principle expressed by (2). We can now assume that in deriving the meanings of almost half crazy/sane, the meaning of almost is combined with the meaning of half, even though it doesn’t form a constituent with this element. Reference: Invitation to Cognitive Science, 2nd edition. Daniel Osherson, general editor; in Part I: Language, Lila Gleitman and Mark Liberman, eds. MIT Press, Cambridge 1995, pp. 311-360. 

This book is good though quite old: The Meaning of the Sentence in its Semantic and Pragmatic Aspects Geert-Jan Kruijff gave a nice course at ESSLLI: DG, and you can google up his papers on DG. XDG is one of the best dependency formalisms: XDG 

There's a purely logical definition (given in the MIT Encyclopedia of cognitive science): If sentence s is uttered in context c, then p is a presupposition in s if c entails p. This definition is more or less identical with most linguistic definitions and is equal to that of Jerry Hobbs if "entails" is taken to mean "abductively proves". Rephrased less formally, presuppositions (topics) can be inferred from context (that is, they're predictable from previous discourse and/or shared background knowledge). The remainder of the sentence is focal ("preferred content" in the MIT Encyclopedia mentioned above). Informally it's often said that the focus of a sentence is what's being said about its topic. In formal logic, focus is then taken to be an "Aristotelian" predicate. The unmarked sentence John sings (John topical, sings focal) is formalized as sing(John), whereas JOHN sings (John focal, sings topical) would be λP.P(John)(sing). Discourse-configurational languages assign topic/focus structurally. Hungarian is said to be one (Kiss, who coined the term "discourse-configuratonal"), other examples include Russian (King) or Georgian (Meurer). This approach only accounts for nonemotive sentences since intonation can mix things up. In most languages word order is more or less iconic with respect to information structure. Aside from word order and intonation, some languages have morphological discourse markers. There are topic markers in Japanese and Korean, focus markers in Eastern Armenian and both in (many dialects of) Quechua, to name just a few. As for formal representation in frameworks, FGD uses an ordering on nodes in deep syntax trees to express information structure. In LFG, there's a separate i(nformation)-structure for discourse functions. In the abductive framework of Hobbs, there's no implicit formalization but whatever can't be inferred/proven is taken to be focal. 

(1) demonstrates that certain verbs that take finite clauses as semantic complements can (or must) be separated from them by non-referential it; but this is not an option for noun phrase complements of the same verbs. (2) demonstrates that noun phrases can appear between a verb that selects them as argument and a predicate (rude above) that is also an argument of the verb; but this is not an option for finite clauses. However, phrases of different syntactic categories may sometime overlap with respect to the grammatical functions they realize in the sentence. Both noun phrases and finite clauses (with the complementizer that) may be the subject of a sentence, for example. You can read a little bit more about the difference between syntactic category and grammatical function in this post. 

A case where a pronoun cannot be substituted by any definite description that may be recovered from the context is discussed by Walter Edelberg in his paper ‘A New Puzzle about Intentional Identity’ (Journal of Philosophical Logic 15 [1986]). Edelberg discusses the sentence in (1) from Peter Geach’s paper ‘Intentional Identity’ (Journal of Philosophy 74 [1967]): 

The finite clause (with or without the complementizer that) is a different syntactic category than the noun phrase, as can be seen from the fact that finite clauses have a different distribution than noun phrases. Some examples are given in your question and in @curiousdannii’s answer above; two more examples are given in (1) and (2).