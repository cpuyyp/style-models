Preferably on a smarter location than /tmp. Not only does it contain your database layout, also the administration of your most recent and most valuable backup. 

If you really want fast transactions in your database, make sure that you handle the database correctly. For example, when talking about Oracle it is easy to have it handle 30.000 tps. With a few subtle tweaks it can be brought down to only a few thousand transactions per second. For a very nice demo look at OLTP Performance - The Trouble with Parsing It all comes down to prevent extra work, re-use connections as often as possible, prepare statements and bind variables. Do this and your database can perform and scale in an optimal way, assuming that your storage can handle the load. 

Normally this does not solve a problem. There was a problem that made the instance crash .... check the alertlog, it could have some clues in it. 

Sure you can. Explicitly set the sort_area_size for your session and a lot of PQ. Adding a few tempfiles might be easier. 

Yes, there is. transfer the files to a new server, if you have one, using the same file structures. next create a init.ora file with the dbname and control_file parameters. Start the instance using that init.ora in nomount. If you happen to have snapshot controlfiles in $ORACLE_HOME/dbs/ of you database, start rman: 

In order to find a specific session running a certain script, it is easiest if the scripts makes itself identifiable by using dbms_application_info and dbms_session. See Morgans Library for details. Also see track the parts of my application that are in use for sample code. Doing so enables you to select on v$session and filter not only on username and machine but also on module, action, client_info and client_id. Username and machine are not mutable but the other columns are controlled by dbms_application_info and dbms_session. Using those makes your script recognizable. This of course, only works when the other session has select privileges on v_$session. dbms_application_info is also great for finding where your code is running; a way to instrumentate your code to make performance analysis easier. your code in SQLdeveloper would be as simple as 

SSD's can make READING data faster. Writing won't be any faster. Don't even think about placing the redo's on SSD since they are only written to. To speed up writing to the redo: add more drives and stripe them. Redo's are written sequentially so adding more spindles improves the write throughput, until you meet the controller limit. What is that test migration doing? Does it use procedural code or does it use sets? If using procedural code, be sure to implement bulk operations. Sets are allmost always faster. 

Be happy that your users seem to be going to use SQL to do the analysis, instead of pumping to excel. Make sure you have separated tasks and don't give privs to update the source data. Resource manager is your friend to prevent runaway queries. give them resources that they need, make sure other users don't get pushed out. 

You can not drop the datafile. You can move it to a better location and keep it there. If you want to get rid if it, re-create the tablespace and copy the data to the new tablespace. Drop the old tablespace and rename the new tablespace. Don't forget to give quota to your schemas similar as they have on the old tablespace. 

Easiest is to use Oracle Managed Files for this. Oracle will just place -and name- the files in the locations configured by the 

In case of RAC this again introduces instances running on other nodes so again, filter using the current HOSTNAME 

The update scenario is always faster than using a procedure. Since you are updating column X of all rows in table A, make sure you drop the index on that one first. Also make sure there are no things like triggers and constraints active on that column. Updating indexes is a costly business, as is validating constraints and executing row level triggers that do a lookup in other data. 

Reading the packages can help a lot. For the SQL access advisor take a look at SQL Access Advisor in Oracle Database 

Take note of the owner and group, they should be the owner of the Oracle rdbms processes. Next check the NATTCH column, should be 0 processes attached to the segment. If you still want to remove that segment (which is pretty safe when there are no processes attached), use the ID column as input for 

create resource consumer groups create a resource manager plan using the created groups somehow arrange that sessions are mapped to a resource consumer group in the resource directives specify if a request should be estimated first or not make sure users have switch privileges to the required resource consumer groups 

If you are using ASM, you could even do this online by first adding the new disks to the current diskgroups, rebalance the disk groups and drop the old SAN disks from the diskgroups. During this drop, the first thing that happens is - again - a rebalance action in which the data that is on the dropping disks is moved from those disks to the new disks. A very powerful feature, not sure if it can help you. If it all has to be done using FS copies, it still can be done online, if you are running archive log mode. Since 11g RMAN can clone from an active database, also very powerful. FWIW: some applications check the controlfile and complain if it is changed. This is part of a check for licenses. Short answer for you question, you need the online redologfiles. The archives are only needed for recovery operations so they should remain accessible, not perse on the new SAN. A regular backup would be ok. Copy the online redo logfiles and use RMAN for the copy of the database since it handles everything that needs to be done in a good way. 

As always with Oracle, there are many ways to reach your goal. You might want to check out flashback database. This enables you to restore your database in a previously saved state and might be the easiest in your case. Using Flashback Database and Restore Points Drop user cascade could also do the trick, followed by impdb as you described. What was the problem with the drop user action? Try flashback database. 

A simple example is shown here Running ASMCMD commands in a script Change this example slightly to get it to copy all files in a local directory, also using this Using the ASMCMD cp command as input: 

If you have a tuning pack license - really recommended - you can use the dbms_sqltune to get a better plan and create a profile for it. If an sql_id has a profile for it, the database will use that, if it is enabled. It also works perfectly together with SQL Plan Stability; when you work with SQL Plan Baselines, a baseline is auto created and accepted for a profile's plan. On the other hand, it does look like you statistics do have a problem. Most likely there is a problem with tables that have a kind of a temporary content, that is emptied and filled regularly. If the gather routine runs on a 'wrong' moment, it gives - for example - 0 rows for such a table because it was just emptied on the gather moment and filled with any number of rows > 1 to cause havoc on your performance. With 0 or 1 rows in it, a scan or even a Cartesian product might be chosen, that work real bad in a join there appear to be more data in it than what the stats try to tell. Easiest is to just restore the statistics for the participating tables. 

If going for oracle, take a look at dbfs and Secure Files. Secure Files says it all, keep ALL your data safe in the database. It is organized in lobs. Secure Files is a modernized version of lobs, that should be activated. dbfs is a filesystem in the database. You can mount it similar like a network filesystem, on a Linux host. It is real powerful. See blog It also has a lot of options to tune to your specific needs. Being a dba, given a filesystem (based in the database, mounted on Linux), I created an Oracle Database on it without any problems. (a database, stored in a ... database). Not that this would be very useful but it does show the power. More advantages are: availability, backup, recovery, all read consistent with the other relational data. Sometimes size is given as a reason not to store documents in the database. That data probably has to be backed up any way so that's not a good reason not to store in the database. Especially in a situation where old documents are to be considered read only, it is easy to make big parts of the database read only. In that case, those parts of the database no longer have a need for a high frequent backup. A reference in a table to something outside the database is unsafe. It can be manipulated, is hard to check and can easily get lost. How about transactions? The database offers solutions for all these issues. With Oracle DBFS you can give your docs to non database applications and they wouldn't even know they are poking in a database. A last, big surprise, the performance of a dbfs filesystem is often better than a regular filesystem. This is especially true if the files are larger than a few blocks. 

should do the trick. If you work with Oracle, make sure that the environment is setup correctly. Also note that if you have multiple Oracle installations on the same Windows machine, the PATH is modified such that the last installed Oracle installation is selected first from PATH. That gives quite a few side effects. 

namespaces service level agreements - don't make maintentance impossible by combining conflicting slas. performance isolation - use Resource Manager to handle this application isolation - make sure the apps all use their own tns_alias to connect services - give every application an own service name in the database, if possible. 

Your situation looks most like a powerfailure. This kind can hurt the database badly and in this case it looks like the controlfile or the online logfile is damaged. If you have a backup, start with restoring the controlfile from the backup and recover the database using the backup controlfile option. With a little luck you can open the database afterwards. With a little bad luck, the damage is in the online redolog in which case you can not open the database in a consistent state. This is because your undo segments are active and they lack to recovery information from the redolog file to recover from that. Still it is possible to open the database using some tricks but it does require extra work, setting hidden parameters etc. After that type of 'recovery' only export -> create new db -> import is a valid option. 

For Oracle it's quite easy, a snapshot standby database would do the trick for you. Maybe Oracle will integrate this capability in mysql .... With 8GB databases, making a new copy should not give a lot of problems .... 15 minutes for a copy? 

One of the first things Oracle does at restore time is pre-create the data files to the defined size. So you do need a way to store the full size of the files. A compressed filesystem could be used for that and with a little luck it works. It just does not solve the problem. What you do want is to re organize the source database. The quickest way is to prepare a new database with correct sized tablespaces and datafiles. Next use datapump to transport the schemas to the new database. If needed, this can even be done online by using smart tools like dbvisit replicate (or goldengate if money is of no importance). Trash the old database when the two are in sync. 

version, OS, tool are helping things to help you solve this. I guess OS is Windows. version 9i. export full. If this is the case: connect to the server, set ORACLE_HOME=/where/is/oracle/home/ set PATH=%ORACLE_HOME%/bin;%PATH% set ORACLE_SID=ORCL -> or your ORACLE_SID name 

Assuming 11g. There are a few ways to do this. Easiest is to use the Oracle Job Scheduler with the Remote Job Agent. That agent can run using credentials for a qualified OS account. You could run a job that selects data from the database and spools it in the required output directory. You could also copy a file to the Remote Job agent. This is closest to the code flow you described. For docu and examples see the book I wrote about the Oracle Scheduler. There are no ways to switch OS accounts directly in pl/sql because whatever you do in the database, you are using the credentials of the Oracle Database Instance. The Oracle Remote Job Agent can use own credentials and is accessible from within pl/sql by using the dbms_scheduler package. 

The hint you specified was almost correct. In the hint you should specify the correct index_name, or the column list. Given the fact that the index_name is system generated, I would specify the column list. 

YES, your plan can work since your structure is an exact copy. No recovery needed nor possible since your DB is currently running in no archive mode. Don't forget the listener.ora, tnsnames.ora, sqlnet.ora and orapw{$ORACLE_SID} files. Also check your dba_directories view for any references to your local server filesystem from within the database. If this really is a migration, remove the source database when all is up and running. If it is intended as a copy, change the DBID to prevent an accidental mixup of databases in backup/recovery operations. If this is a prod database, change to ARCHIVELOG mode and use RMAN for regular backups. 

More often than not, a firewall on the database vm is the culprit. You can test this with from your mac. This should give a quick response. If it does not give a quick response, like not response at all, a firewall is blocking the request. In that case allow traffic on port 1521, or disable the firewall. 

this depends on how the dependency trees are. If you have lots of dependend objects on the table[s] where you load the data in, best is not to change the object, the table because lots of invalidations will take place. truncate table gets rid of the data very fast but won't work when referential constraints are in place. It also won't work when the app is running because of locking issues. Best for the application will be a normal delete/insert sequence. Not the fastest option but it is best for availabillity and has the least problems with locking. Other options can be partition exchange loading. Constraints can be problematic and take a lot of time to validate. Never ever drop a table just to get rid of the data. A table is to be considered as application infrastructure and should allways be available, because the application will throw lots of errors to your users. 

The 10g documentations shows explicitly: 1 installation, 1 instance on a machine. Compared to 10g in 11g you can use more userdata on disk. 

Which version of Oracle Database do you use? You essentially want a different plan based on the contents of the arguments. Oracle tried to make that possible in 9i, 10g and finally got this working in 11g. Use SQL Plan stability and Adaptive Cursor Sharing. Adaptive cursor sharing will make the database search for a better plan when it detects that a certain argument causes a bad performance, so the next time it will perform better. SQL Plan Stability makes sure the database ends up with a collection of good/accepted plans that give a satisfactory performance.