Yes, you need to catalog the backups at their new location. You should run crosscheck also on the backups, so the entries pointing to the old location become EXPIRED. 

Long running queries may show up in , but it is far from giving a complete overview. Only a few specific built-in operations are logged here, for example full table scan, index fast full scan, hash join, sorts and some other. shows not only the current long operations, but a history as well. The number of entries retained is 500, according to V$Session_longops Have Limited Records Of 500 (Doc ID 783721.1) This is per instance by the way. For example, I have 480 entries, the first one is almost a week old: 

A cold backup is consistent already, requires no further recovery. All you need to do is open the restored database with: 

As you can see in the 3rd picture, when using the clasue, the grantee can be only an user, but not a role. (By the way, that clasue is used for something else.) Using the regular , this is the error you would receive: 

This is quite typical on 11.2 because of a feature called "cardinality feedback". Check the execution plan of the slow runs, and if you see "cardinality feedback" in the Notes section of the execution plan, try setting the below parameter: , and run your tests again. Some bugs related to this: Bug 12557401 - Suboptimal plan on 2nd execution caused by cardinality feedback (Doc ID 12557401.8) Bug 16837274 - Cardinality feedback produces poor subsequent plan (Doc ID 16837274.8) Bug 8608703 - SubOptimal Execution Plan created by Cardinality Feedback (Doc ID 8608703.8) Bug 8521689 - SubOptimal execution plan on second execution of GROUP BY query (Doc ID 8521689.8) 

First you need to convert it to TIMESTAMP datatype (because DATE type does not handle fractions of second). Then you can truncate it to date for example: 

this may be caused by a bug, permission issue with the executables or other environmental errors, such as deleting socket files from the tmp directory. First investigate this. . For this: 

As always: it depends. However in most cases the default values are adequate. The business requirement of handling 50 concurrent inserts (transactions) alone does not imply the need for setting INITRANS 50. Are they really that concurrent? Will the duration of the transactions overlap each other so they really happen at the same time? All the 50? INITRANS specifies the minimum number of transaction slots (ITL slots) in a database block. Of course, the number of ITL slots can go higher if needed, the database manages that. The maximum number depends on the database block size (as the ITL entries are not allowed to occupy 50% of more of a database block, and 1 ITL entry takes 24 bytes), and MAXTRANS (=255 from version 10.2). So, if: 

This is absolutely normal when you try to perform a complete recovery from a hot backup, because that is how hot backups work, you will never have the latest changes in your backups. Your backup includes archivelogs up to sequence 29. The next changes were in log sequence 30, but log sequence 30 became the current redo log at the time your backup was finished, so you do not have any backup of that. This is not a problem, this is how a hot backups work, you will never have backups of the latest log sequence. Obviously, the next hot backup will back up that, but by the end of that backup, another sequence becomes the latest sequence. If you simply try , RMAN tries to recover the database to the most current state, and it does not know where to stop (except when you have not only the archivelogs, but the redo logfiles also). Your last log sequence in the backup is 29, RMAN will complain about the next one, because it does not know where to stop. If you want to avoid the above error, you can explicitly specify RMAN where to stop (perform a point-in-time recovery). For example: 

And run the queries, in the generated trace file, you can view the final form of the query that is executed. 

This is such a common problem when the database was installed with user separation (grid + oracle user) and DBAs tend to overlook this. When you use RAC or even just Oracle Restart (with or without ASM), you need to install Grid Infrastructure. Grid Infrastructure can be installed as a different user (typically grid). When you have Grid Infrastructure, the proper way to handle listeners is through Grid Infrastructure. If Grid Infrastructure was installed with grid user, then the listener runs as grid user. In Oracle architecture, by design, remote connections log in through the listener, and the database server process is forked by the listener. On Linux/UNIX platforms, the binary is owned by oracle user, and it has the setuid bit enabled. grid and oracle users share a common group, and the binary can be executed by the members of this group. Given the above information, remote connections coming through the listener running as grid user can spawn processes whose UID and EUID is the same as the UID of oracle. So far this is what usually everyone knows. The difference is however the RUID and the inherited privileges because of it. On a machine with users as (this is the default configuration taken from an Exadata X5-2 compute node, so this is how Oracle officially deploys its configuration): 

Use RMAN catalog, because without a catalog, backups taken on one site do not appear in the controlfile of the database on the other site. You need a centralized catalog for backups in a Data Guard configuration. Use role-based services, and TNS entries with connect-time failover, using these services. Use these TNS entries for logging in to the database when performing backups. This way you always log in to the primary/standby database, and you do not need to check for the role of the database. 

By default, an Oracle database does not detect dead clients, you need to enable it explicitly by setting in . SQLNET.EXPIRE_TIME Since the database does not detect dead clients, it maintains its session and locks, so the answer is yes, killed application processes can cause blocking. This parameter is often misinterpreted. Setting this parameter to 10 does not mean dead connections will disappear at most in 10 minutes. This means the database server sends a probe packet every 10 minutes. The time required to detect a dead client depends on the operating system as well. For example on Linux, an already established TCP connection is valuable, so even if there is no answer from the other side, the server keeps trying to reach it. When the database server sends the probe packet and receives no answer, it will try again, 15 times in total (because of the kernel parameter), and the total time needed to detect a dead client can be way more than 10 minutes. 

Your table is owned by USERGOD, but you are importing as DIGITALNOISE. Yes, you have the IMPORT FULL DATABASE, but you are not doing a full import. Using FROMUSER/TOUSER is one solution. Performing a full import by adding the FULL=Y option is another. 

A foreign key requires a unique or primary key constraint on the referenced columns in the parent table. A multi-column constraint is not interchangable with single-column constraints. 

Short version: no. Active Data Guard is an optional feature that allows you to open the standby database in read-only mode while applying logs from the primary. If you need the standby database only in case of disaster, then you don't need to buy this option. 

File 1 belongs to the SYSTEM tablespace. Objects with such low id as 39 are dictionary objects. For example in my database it is an index of OBJ$ (I_OBJ4). ORA-08102 also points us towards an index, which seems to be corrupt based on the error you get. Find the index name by: 

You should not put SET and SELECT in the same line, they are seperate commands. For example this works: 

I would not bother implementing it, because others already did it and spent a lot of time with it. SQL Developer can do this for you. Example: Exporting Metadata and Data for a Table In SQL Developer, click Tools, then Database Export. Set the required options: 

But this should list the image copies that would be recovered, the starting and ending SCN, and the logfile used for recovery: 

Not supported. If you want to send e-mails (JavaMail?) from the database, you do not need Java in the database for that, just use the UTL_MAIL package, available in XE as well. 

What you described here is absolutely normal and intended. This feature is called a global database link: $URL$ It is a "side effect" when for example Oracle Internet Directory is in use, I have seen this at a few companies who have a lot of databases. More and better explanation can be found in the support note: What are Global Database Link and How do you Disable them? (Doc ID 1632329.1) 

Ok, so I had some time to waste, and this is just for the "fun" or "interesting" factor. It's nowhere near that I would use in a real scenario, I have played with it in my lab environment on x86-64 Linux platform, with a few 10g, 11g and 12c databases. At least you can do this even if the database is shut down. When you do a controlfile dump with: 

This simply means, you provided an incorrect SID. is the default in SQL Developer. Check the correct from the output of , and use that for connecting. Or even better, let's just finally forget the , and use . Another possibility is that your database instance was not started, or you do not even have a database created yet. 

The PASSWORD_VERIFY_FUNCTION is used, even when a privileged user changes another user's password, see below. 

A full export/import contains tablespace definitions with datafile paths as well, so tries to create the datafiles with the original path. Just create the tablespaces manually before running impdp, that is usually what I do. will notice that the tablespaces already exist, and continue with the remaining objects. You can extract the tablespace creation scripts as well if you want, using the clause: