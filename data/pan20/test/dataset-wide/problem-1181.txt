It's critical that you do not enable RCSI on a database without the application vendor and your own developers certifying that they support it. The reason is that the behaviour of all your read queries will change, but not actually break in such a way that replaying your workload will detect. Read section 3 at $URL$ To answer your specific questions: 

Note that "3MillValue" in two places would need to be hard-coded with the value that returns that amount, and that OPTIMIZE FOR is the key to this technique. 

Regarding your first question in general: I do not understand how an execution plan can reveal 3,141,000 Rows if it actually returns ZERO rows. How is this possible? The final output rows count isn't known to the optimiser when it generates a plan. So all it can consider are the estimates it can calculate from statistics. (In the case of your "bad" plan, the estimated output rows was actually 4.4 which was based on the first estimate in the plan.) If those estimates are outdated, or insufficiently accurate (sample vs fullscan of unevenly distributed data for example) then a poor plan can be generated even with a simple query. In addition, if a plan is reused with different variables, the estimates the plan was generated from may be wildly inaccurate. (And I think that's what sp_BlitzErik is leaning towards as the cause in your particular case.) 

I must put out a warning. We have now moved away from merge replication, and I would have to suggest that the scheme above could be a major performance issue. The exception to that would require you to have a small publication with either small amounts of filtering and/or filters that are not multiple levels deep. For instance 10 - 100 articles might work fine. If you push the above scheme too much you could have performance / locking issues. Every time you insert a record into the top filtering table the merge replication trigger has to process all the child tables. It also adds records to MSMerge_contents and MSMerge_genhistory for all the child tables. The more child tables you have, and if they have a big amount of records in them then the more processing power will be required. We had a problem with sp_MSsetupbelongs being too slow and timing out. In the end we came to the conclusion we were pushing merge replication too much and that this technology would not work for us. This leads me to suggest that if the filtering scheme in merge replication out of the box is not flexible enough for your situation then either don't filter, or don't use merge replication. Test Test Test though of course, every situation is different. 

Scott's answer made me realise you might have enough space for this: Create a datetime2 column with a temporary name, and transfer the original column contents to it in batches (to prevent your log running out of space -- and I'm assuming your database is in Simple recovery model). Then drop the original column, and rename the new column to the old column name. 

I see you've just done a big update, so I'll perhaps do a separate broader answer. Since I can't yet comment, I'll ask a few things here: - Are you free to add indexing as your query requires? - Is the replacement of CONTAINS([varchar3], 'moreText') with [varchar9] LIKE '%a%' correct (i.e. do you definitely no longer need fuzzy search on varchar3?) - Will OFFSET always be 0? - Can you give some idea of the proportions of data you expect for columns varchar1, date1 and varchar8? 

I believe the basic issue is that the query is doing multiple heavy table scans due to no index support. Try adding the following indexes: CREATE NONCLUSTERED INDEX TEST ON STOCKDEBUGTRIGGERED (ChangeDate) CREATE NONCLUSTERED INDEX TEST ON STOCKDEBUG(ProductID, StockOld, StockNew) There may be further tweaks, so please post the stats and execution plan with these indexes added. 

The contactNo field is an INT NOT NULL in both source and destination, so I cannot see why translating the value using case then casting it to INT is not working? My BCP import looks like this (minus the server, user, password etc), 

I am using Merge Replication with SQL 2012. I look in the snapshot directory, but the largest file in there is a prc file which is 646 KB. I know for sure that the biggest of my replicated tables is 25 MB in the database after replicating, so I am not sure I understand why there aren't larger files in the snapshot directory? Also is there a place I can look for the snapshot files as they are downloaded to the subscriber? For instance the merge agent outputs messages such as, 

And that part of my filter is fixed, I cannot edit [dbo].[berm] and replace it with a view. So what I think you are suggesting is that I can change the above to, 

You're right to consider both Avg_fragmentation_in_percent and Avg_page_space_used_in_percent when considering whether to do reorganise, rebuild, or nothing. Even if Avg_fragmentation_in_percent is low, a low Avg_page_space_used_in_percent may gain benefit from reorganisation (due to the extra IO and cache resources used by unfilled pages). And the "greater than 30% = rebuild" advise may be better read as "if Avg_fragmentation_in_percent > 30% and you have been regularly reorganising, then rebuild". If you haven't been reorganising, then try that first, since as your test shows, that may be all that's needed even with very high Avg_fragmentation_in_percent. As for the exact trigger levels to use, that really depends on your data and how it's used, and on various other factors like those jesijesi posted. 

But if you can instead wrap steps 1 and 2 in a stored procedure or ad hoc query that handles the retry logic without duplication, then your solution will be much more understandable and maintenance-safe. 

Read-only users being able to hold locks makes complete sense if you consider SERIALIZABLE and REPEATABLE READ isolation. And for cases like this where there's no logical sense in the lock, the designers probably decided it was better to have less complexity and consistent behaviour than to consider permissions in the locking code. 

Thanks RThomas it looks all good. I found a table which had records added previously using BCP. After BCP had been run the 'sp_addtabletocontents' stored procedure had been used. I joined that table to the MSmerge_current_partition_mappings table, and found that some items in my table didn't have a corresponding MSmerge_current_partition_mappings record. It seemed fishy, but the reason was very simple. The records in the table corresponded to a partition that no longer existed. I added the partition back in and closing the publications properties dialog took a while (it would have been adding the new MSmerge_current_partition_mappings records). After that was done I could see that all records in my table had a corresponding MSmerge_current_partition_mappings record. It all checks out. 

I am running merge replication with SQL 2012. When I first create my publication the MSMerge_contents table is populated with a large number of records with the colv1 being set to 0xFF. When I run the stored procedure like this, 

I am in the process of implementing merge replication in SQL 2012 with web sync. I am wondering two things, 

You can't use a SP in a default, but you can use a function. But presumably your SP code has to increment the value in the NextNumber table, so that won't work for you. Your best bet for doing this within SQL is probably to have an INSERT trigger on MyTable which calls the SP and sets MyColumn. However you'll have to carefully consider the behaviour of your SP code when multiple concurrent inserts are involved. For example, you'll probably need to select the current value from the NextNumber table using an UPDLOCK hint to prevent another user reading and using the same value. (Note that the UPDLOCK hint will only prevent reads in this way if it's within a transaction [which triggers run within in by default], and if other selects on the NextNumber table use UPDLOCK as well.) 

As far as SQL Server config: Since you're not using SQL Server clustering with shared storage, I believe you don't need to do anything but configure all your instances with Tempdb on T drive (or whatever standard path you choose). See Example A at $URL$ As for how to ensure that T drive is available as local storage in all your VMs for when SQL Server starts up, that someone else will have to help with (perhaps stackoverflow with virtualisation tags). And be sure to test your whole failover process, don't take anyone's word for it. 

When they sync with the server won't it repeat the default constraint and set the value of 'NumberOfSides' back to 4? Am I missing something here? I don't want to disable the default constraints being replicated to the client either because there are valid default values that need to be set. The client side user needs to have these values set to use the software. UPDATE: The explanation about the default constraint being applied only once makes sense. So my only remaining question is if I have default contraints which use sequences I am in trouble because sequences cannot be replicated. Are these my only two options? 

What structure is ideal to use to relationally link street lights together. For instance I have street light 1, 2, 3, 4, 5, 6, and I want to say that street light 6 and 2 are related. They might be on the same street for instance. What are the ways I can do this? 

I am running merge replication with SQL 2012. There seems to be a nasty consequence of the delete triggers added for replication in SQL 2012. Inside the delete triggers are this, 

Many SQL Server configuration features are not implemented in the UI. Scripting is your only option for these (aside from possibly some third-party UI that implements them). 

Since both servers show similar reads and similar CPU "seconds", but the new server example shows that the duration was the same as the overall CPU time, the spikes you're seeing are likely due to less-parallel execution plans sometimes being generated--perhaps simply due to the CPUs being unusually busy at those times. Note that the CPU "time" shown is the total CPU milliseconds used over possibly multiple cores--which would have to be the case in your old server example where the CPU time was 6 sec but the actual duration was 1.5 sec. 

This may be a file permissions/uac issue. To prove/disprove that, create a new folder and copy (not move) the excel file into it, then give the windows security group "Everyone" Full control on the folder and its files. Update your export query with the new file location and try running it again. If that works (or if you at least get a different error), then bear in mind that wherever you want to have the export file in the end, the file will need read/write permission assigned for the user account your SQL Server service runs under. (Or the "Everyone" group, but that wouldn't be best security practice.) 

So it uses the indexes, but does an index scan across the whole index, so 50000 records it scans 50000 records in the index. 

I am using merge replication with SQL 2012. I am trying to rename a published table, and rename a published field. These operations are not permitted in merge replication. However I can think of an action plan if these things are needed. For instance renaming a field would include, 

Adding the new field Migrating the data from the old field to the new field Wait for a while, perhaps a month Delete the old field 

What is occurring here is not what it appeared to be. The stored procedure to write into my top level tables were using transactions. It was the transactions that were causing heaps of extra locks. It worked fine once those transactions were removed. 

UserRegion Table, (think of this as a security table, which region is a particular user allowed to see) 

I am using merge replication in SQL 2012. Some of my merge articles use column level tracking. I am trying to understand how to use the 'lineage', and 'colv1' fields in the MSmerge_contents table to work out which columns have changed in a particular row in my database.