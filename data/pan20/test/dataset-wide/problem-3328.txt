ziek- has no German cognate that occurs to me, but a very obvious English one, so I would use that. And schmacklich is not a real German word. Regarding preserving your German skills, learning Dutch will definitely cause some interference, but it is still better than not using German or Dutch at all. What happens in practice? I learnt Spanish very fast thanks to Italian, but then I of course had trouble speaking Italian, even though my understanding actually improved a bit, due to knowing and using even more Latinate vocabulary. It takes more than a few days to completely erase all effects. On the other hand, even learning and speaking Russian interferes with my Italian and Spanish, for example I have said лучшее (meaning better but phonetically a bit similar to luce, light) when I meant megliore (better). Veel succes! 

I believe you seek a syntactic language model (as opposed to a basic lexical (n-gram) language model). For that you need text data that has been annotated with the syntactic trees, or a parser (which was trained on such text data, and will give some good - but imperfect - annotations on your data). You could then generate random (lexically improbable and semantically nonsensical) grammatical text from that. Note though that lexical and even character-level models with no concept of grammar work well enough for many purposes, especially for a language like English without much agreement and even less long-distance agreement. You could read more about parsing at $URL$ Note that good parsers like spaCy do infer the function of a made-up word from context, for example: 

Like with any overloaded preposition it is possible to construct examples which are ambiguous, especially out of context. And it is trivially easy to construct examples which are grammatically ambiguous but semantically or logically are less ambiguous. In fact it is more difficult to avoid constructing these. The thing is, natural human language is fundamentally ambiguous, and we humans continually solve endless Winograd Schema challenges without even being conscious of it. Nothing specific to German or von. 

Yes, a few: $URL$ They were mostly borrowed via Akkadian, and into other major classical languages of the Eastern Mediterranean beside Ancient Greek - Aramaic, Armenian, Persian, Hebrew... English cane would seem to share such an etymology. Another wave of ultimately Sumerian words came to Southeastern Europe later via Ottoman Turkish, eg kireç. They are not found in literary Greek, but they are found in neighbouring South Slavic and Albanian, so surely they have been spoken in some Greek dialects. 

I think "mama" and "papa" are two words which can be understood by every one in the world, because it is almost the same or similiar across all language. 

What we have gotten about the expected per word entropy of random yet grammatical text is just some upper bound of the the expected per word entropy, because we have not found the exact way to compute the probability of words. The models we adopt to compute the probability of words is just approximate models like bi-gram,trigram,etc,since the precise model of language,say,the grammar has not been established. By entropy formula and models we adopt, we can estimate that the exact entropy is smaller than what we have gotten. There are lots of the approximate results about entropy per word, if the algorithm is correct,and assumptions are approximately valid, one can choose the least one from them. That is it. One should not expect perfect answer in the near future, just focus on the correctly approximated least one that appears.It is not difficult, since so many search tools are available in the days. what is important is that we can just get some upper bound of it,and the best solution is to choose the correctly approximated least one.So I think anyone that claims his model is precise, is wrong. Someone does not agree with what I say above without any reason, I am wondering, if anyone can give any evidence to refute what I say above? Also, Tarski's theorem on truth which can be deduced from Godel's incomplete theorem, the related theorems, have led to model theory, which is the fundamental part of semantics.The definition of pragmatics is somehow the third part of linguistics or semiotics, it is somehow what can not be incorporated into grammar and semantics,Shannon's classic article, has definitely claims that information does not considered any meaning of symbols, which excludes semantics and pragmatics. So we can conclude,any statement that claims to obtain precise information per word of a language by classical information applied to "semantics" and "pragmatics", is misunderstanding and misapplication, and serious researchers have to carefully know the difference among them. The question is now, any evidence or valid theory can refute the relation among information theory, semantics, and pragmatics I say above? Thanks in advance for any evidence, or reference that can refute all I say above. If there is controversy over any definition or theorem about information theory, I will post the question on other professional forum relating to information theory. 

I'm developing an application to teach Quran. I need some textual/lingual analysis based on a common and general corpus. I know about King Abdulaziz City for Science and Technology (KACST) Arabic corpus, yet I can't find a place to download it. The original website seems to be down. Can anyone guide me to a general purpose Arabic Corpus that can be downloaded (free or paid)? 

In NLP, we have two directions. One is NLU (natural language understanding) which deals with a given text to analyze it, like: 

Very similar to COCA in English. Of course searching through Google can give many links. But none has them all. And besides, my French is not that good to understand website contents. I truly appreciate any help. 

This scene of Game of Thrones is about Khal Drogo's speech. While watching this video, I feel a very deep connection to the character as a leader, and I'm impressed by him, ready to rise and obey him. Well, that's just me. Yet when I saw more videos of the Dothraki language, I found out that a lot of people are actually impressed by how glorious and awesome this language is. I thought maybe there are some objective rules based on cognitive knowledge of our brain and language, that makes a language impress us more than other language. Can it be true? Are there objective facts and sounds that make people perceive a language as awesome and glorious? 

In Persian, the word for nest is لانه, which is pronounced somehow as /lʌne/. However, in everyday spoken language, people usually pronounce it as /lune/, and they usually write it as لونه. So, considering that transliteration is common in Persian, this simple word is represented in media as: لانه = /lʌne/ = lane But in everyday language, people use: لونه = /lune/ = loone or lune or loune or lone Being a computer programmer who needs to work and extend NLP libraries, I really can't find out anything about this phenomenon. I can describe it as the gap between media/formal language and everyday language. Do we have a scientific explanation for this? Has it been studied by linguists? What languages have bigger gaps?