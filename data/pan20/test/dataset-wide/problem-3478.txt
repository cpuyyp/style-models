Here is a way to approach the problem: An indifference curve is a set of consumption bundles all of which yield the same utility (the consumer being, of course, indifferent to which she consumes). Nonetheless, the numerical level of utility is inconsequential, it is just a placeholder for the more fundamental (and ordinal) preference--any monotone transformation of a utility function represents the preferences. So let $I_U(x_1,x_2)$ denote the indifference curve (of $U$) that contains $(x_1,x_2)$. That is $$I_U(x_1,x_2) = \{(x'_1,x'_2) \in X \mid U(x'_1,x'_2) = U(x_1,x_2)\}$$ Define $I_V$ in an analogous way. So showing the indifference curves are the same amounts to showing $I_U(x_1,x_2)= I_V(x_1,x_2)$ for all $(x_1,x_2) \in X$. 

Edit in response to a comment: Lets call the above notion of the Sure Thing Principle STP1 and say the preference satisfies STP2 if $f_{-E}h \succsim g_{-E}h \iff f_{-E}h' \succsim g_{-E}h'$ for all $f,g,h,h'$. Then if $\succsim$ is a preorder, it satisfies STP1 if and only if it satisfies STP2. First assume STP2 holds and that $f_{-E}h \succsim g_{-E}h$ and $f_{-E^c}h \succsim g_{-E^c}h$. Then by STP2 we have $$f = f_{-E}f \succsim g_{-E}f \qquad \text{ and } \qquad g_{-E}f = f_{-E^c}g \succsim g.$$ Transitivity implies $f \succsim g$; STP1 holds. Next, assume STP1 holds and $f_{-E}h \succsim g_{-E}h$. Define $\hat f = f_{-E}h'$ and $\hat g$ analogously. By definition $$\hat f_{-E}h = f_{-E}h \qquad \text{ and } \qquad \hat g_{-E}h = g_{-E}h,$$ so our assumption is identically that \begin{equation} \tag{3} \hat f_{-E}h \succsim \hat g_{-E}h. \end{equation} Further $\hat f_{-E^c}h = \hat g_{-E^c}h = h'_{-E}h$ so we have, by the reflexivity of preference, that \begin{equation} \tag{4} \hat f_{-E^c}h \succsim \hat g_{-E^c}h. \end{equation} Now we can apply STP1 to (3) and (4) to obtain that $\hat f \succsim \hat g$, which, given their definition, exactly what we need to show for STP2 to hold. 

I am not sure what exactly you are asking because of notational clutter, but I can take a stab. As I see it, there is no reason for the $g$'s and the $h$'s since they are never exchanged. So lets just use $g_1 \ldots g_k$ and $P_1 \ldots P_k$ as our lotteries (over a set $X = \{x,y\ldots\}$) and probabilities. Further, there is no reason make the second index be the odd one out. Now, reduction of compound lotteries says that if we have $n$ lotteries $g_1 \ldots g_n$ and $\sum P_i = 1$ and $P_i \geq 0$ then we can identify $P_1g + \ldots + P_ng'$ with the lottery that assigns $P_1g(x) + \ldots + P_ng_n(x)$ to each $x \in X$. We only care about the total probability assigned to final consumption outcomes and not how we got there--for example, we do not care about the timing of the resolution of uncertainty.$^1$ Let $P_1g_1 + \ldots + P_kg_k$ by a lottery of size $k$. From our reduction principle this is the lottery that assigns \begin{equation} \tag{1} P_1g_1(x) + \ldots + P_kg_k(x) \end{equation} to each $x \in X$. Then consider the lottery $$g^* = \frac{P_2}{1-P_1}g_2 + \ldots + \frac{P_k}{1-P_2}g_k.$$ (Make sure you see why is this a well defined lottery, in other words why do the probabilities necessarily sum to 1?). We can, by the reduction principle treat this as the lottery that assigns \begin{equation} \tag{2} g^*(x) = \frac{P_2}{1-P_1}g_2(x) + \ldots + \frac{P_k}{1-P_2}g_k(x) \end{equation} to each $x \in X$. Now, what if we mix this new reduced lottery with $g_1$? Take a look at $g^{**} = P_1g_1 + (1-P_1)g^*$. Well, this is the lottery that assigns $$g^{**}(x) = P_1g_1(x) + (1-P_1)g^*(x)$$ to each $x \in X$. Look: this in the lottery of interest (if I read your question correctly)! From equation (2) we can re-write this as $$g^{**}(x) = P_1g_1(x) + (1-P_1)\big(\frac{P_2}{1-P_1}g_2(x) + \ldots + \frac{P_k}{1-P_2}g_k(x)\big).$$ Of course this is a garden variety real valued equation, so lets go ahead a cancel the $(1-P_1)$'s leaving us with exactly equation (1). So these two lotteries are indeed the same. [1] If you are asking how to move from a definition of reduction that only uses 2 lotteries at a time to the (equivalent) one above, this is a different question. Hint: the proof is by induction and the base case is the definition. In spirit its whats going on below. 

As a first remark: the Anscombe-Aumann axioms, in particular Independence, are defined over acts taking the state space to a linear space (generally simple lotteries over consumption objects). Even when we consider the restriction of the model to purely subjectively uncertain acts, we still need to employ the full model or we will lose information. That being said: Lets let $S$ be a finite state space, and $X$ a finite set of alternatives. Let $\Delta(X)$ denote all the lotteries over $X$ and $f: S \to \Delta(X)$ is an act. For an event $E \subseteq S$, let $f_{-E}g$ be the act defined by $$f_{-E}g \begin{cases} f(s) \text{ if } x \in E \\ g(s) \text{ if } x \notin E. \end{cases}$$ Now, we can say that our model satisfies the sure thing principle if $f_{-E}h \succsim g_{-E}h$ and $f_{-E^c}h \succsim g_{-E^c}h$ then $f \succsim g.$ This definition is valid for all acts, not just ones without objective risk, but clearly you can consider only the relevant projection. Assume the antecedent of the STP. From $f_{-E}h \succsim g_{-E}h$ and independence we have that $$\frac12 f_{-E}h + \frac12 f_{-E^c}h \succsim \frac12 g_{-E}h + \frac12 f_{-E^c}h.$$ Notice we can rewrite this as $$\frac12 f + \frac12 h \succsim \frac12 g_{-E}f + \frac12h$$ and, applying independence again, we get \begin{equation} \tag{1} f \succsim g_{-E}f. \end{equation} In an analogous fashion, from $f_{-E^c}h \succsim g_{-E^c}h$ and independence we have that $$\frac12 f_{-E^c}h + \frac12 g_{-E}h \succsim \frac12 g_{-E^c}h + \frac12 g_{-E}h.$$ Again, we can rewrite as $$\frac12 g_{-E}f + \frac12 h \succsim \frac12 g + \frac12h$$ and, applying independence again, we get \begin{equation} \tag{2} g_{-E}f \succsim g. \end{equation} Combining (1) and (2) via transitivity yields the desired relations. Going back to the prefatory remark, notice that to apply independence, we need to mix acts, appealing to objective risk. Thus, even when $f$, $g$, and $h$ have no objective risk, we still need risky acts to serve as an intermediary in the proof. In a sense, this is the grand insight to the whole AA framework---using objective risk to get around the necessity of an infinite state space by using the linearity of expectations to force the STP. Notice only independence and transitivity were used. This should indicated that even state-dependent EU (where monotonicity / state-independence fails) or Bewley EU (where completeness is relaxed) will still satisfy the STP. 

As for even higher moments, the jury is out, but I would imagine their importance diminishes rather quickly. The complexity of the distributions needed to generate differential higher moments increases and I would think heuristics would start to kick in. 

You need to be clear about the definitions. Lets take: (1) $u: X \to \mathbb{R}$ represents $\succsim$ if $u(x) \geq u(y) \iff x \succsim y$. (2) A function $h: \mathbb{R} \to \mathbb{R}$ is monotone if $z \geq w \implies h(z) \geq h(w)$. $h$ is strictly monotone if $z > w \implies h(z) > h(w)$. First, note that every strictly monotone function is monotone. Why? Well let $z \geq w$, and there are two cases to check: (i) $z > w$: apply the definition; (ii) $z = w$: then $h(z) = h(w)$ by the definition of equality (this is because $\mathbb{R}$ is a totally ordered set). So immediately, we see that your condition (b) implies your condition (a). However, (b) is false (I do not understand Kanak's line of reasoning, but it is certainly wrong, although perhaps can be rationalized by non-standard definitions). To show that it is wrong, we need a counter example. Lets let $X = \{x,y\}$ and let $x \succ y$. Then $u(x) = 1$ and $u(y) = 0$ represents $\succsim$. Moreover, $h: z \mapsto 0$ is a monotone transformation. But, $h(u(x)) = h(u(y)) = 0$ does not. Indeed, this example shows that it is weakly monotone transformations that destroy information (they need not be invertible), which in terms of the representation, indicates that strict preference gets collapsed into weak inequality. Showing that (a) holds is a straightforward application of the definitions. (Hint: show that strict monotonicity can be defined via a bi-conditional statement). 

According to the logical positivist view of decision theory, utility functions are just descriptions of observable behavior and have no intrinsic meaning absent this vantage. In other words, $u(x) > u(y)$ indicates that $x$ would be chosen in favor of $y$, but the magnitude of the difference carries no addition information. If $u(x) = 1$ and $u(y) = 0$ the decision maker prefers $x$; if $u(x) = 10000$ and $u(y)= 0$ the decision maker prefers $x$. In either case, we can draw the same conclusions. What does this mean with regard to your question? It tells us what we mean by equality of utility functions: we mean that these two different (so no definitionally equal) functions represent the same observable data. Specifically, exactly when $u_1(x) > u_1(y)$ do we also have $u_2(x) > u_2(y)$. As mentioned in the comments, this is precisely when we can compose $u_1$ with a strictly increasing function and pop out $u_2$. (This is because all we care about is the ordering between objects, and strictly increasing functions preserve order). So, what might our strictly increasing function look like? Well, we can take the log, so that we get $\frac23 ln(x_1) + \frac13 ln(x_2)$ then multiplying by 6 and adding 3 does the trick. Each of these was strictly increasing so too is the composition: $$ (x,y) \mapsto 6 (ln(x) + ln(y)) + 3$$ is our desired function. 

I can shed some light on the question, but am not sure I can answer it as I am not sure it is really even well defined. (1) The Weak Axiom of Revealed Preference is a decision theoretic concept regarding the choices of a single agent. So I do not understand how having $N$ agents is relevant to the problem. (2) Generally speaking, if $U: X \to \mathbb{R}$ is a utility function and $\mathscr{C}$ is a choice correspondence over $X$ such that $\mathscr{C}(A) = \{x \in A \mid U(x) \geq U(y), \ \forall y \in A\}$ then $\mathscr{C}$ will satisfy WARP. This is a straightforward exercise in using the definitions, and the dimensionality of the space should play no role (its true over an abstract $X$). (3) If the consumption space is $\mathbb{R}^l$ then preferences over defined $\mathbb{R}^l$. How you project preferences into $\mathbb{R}^2$ matters. (4) If I interpret your question a la denesp, so that you ask: fix the $2 < k \leq l$ dimensions of consumption and only let dimensions 1 and 2 vary (of course, what we fix the other dimensions of still may make a difference), and assume that this restricted choice correspondence satisfied WARP will the choice correspondence in general satisfy WARP. To answer (4): if the choices comes from a utility function then yes, trivially (see point (2)). If the choices are more general, then no. This fails severely. Take as a counter example a preference over $\mathbb{R}^3$ such that fixing the 3rd dimension, the consumer is indifferent over all elements (i.e., the first and second dimensions are null). WARP holds as $\mathscr{C}^{x}(A) = A$ for all $A \in \mathbb{R}^2 \times \{x\}$. This leaves the 3rd dimension wholly unrestricted. Letting the choice function over this last dimension be cyclic (i.e., fail WARP) and we see that the original choice would as well. (5) What if we instead interpret your question as: we see every two dimensional projection of choice. (That is for every $i,j \leq l$, $i \neq j$, we see the projection of choices over the dimensions $i$ and $j$ fixing the other dimensions arbitrarily. Well, if the way we fix the other dimensions matters to the restricted choices (e.g., $\mathscr{C}^{x}$ over $\mathbb{R}^2 \times \{x\}$ is not the same as $\mathscr{C}^{y}$ over $\mathbb{R}^2 \times \{y\}$). Then we are back to the same type of problem as (5)---$\mathscr{C}$ could be cyclic when the dimensions are fixed differently. What if how we fix the dimensions doesn't affect the restricted choice (so we have separable preferences, a la Koopmans). Then WARP will hold over $\mathscr{C}$ if it is rationalized by a preference relation. But, the result is not very interesting, since from the restricted choices know the choice of out of $\{x,y\}$ for all $x,y \in \mathbb{R}^l$. It is well known that only a binary choice is necessary to verify a rationalization. Without this additional constraint, I believe you could still cook up a counter example in the spirit of (4) where cyclically kicks in when certain elements (at least 3, pairwise distinct over 3 different dimensions) are present. 

Indeed, there is no competitive eq. To see this case-by-case analysis is the best way forward. If either price is 0, consumer 1 will demand an infinite amount, markets will not clear. If prices are both positive, consumer 1 will only ever demand 1 of the 2 goods. But, consumer 2 demands an equal amount of each good is such any such case. No allocation is compatible with both these demands. The failure lies in the fact that the demand functions (in particular of consumer 1) are not continuous. This happens because 1 has a convex utility function. 

Perhaps there is some evidence toward your claim, but I would argue that in most situations, people do not use point estimates (although some "smoothing" likely occurs). In particular, there is no doubt that people care about the second movement (variance). The commonly employed, and robustly empirically documented, notion of risk aversion captures exactly this. Given two risky prospects with the same expectation (say different assets) it seems that most people prefer the one with lower variance (i.e., the less risky prospect). Think: which would you chose when given the option between \$50 for sure or a 50/50 toss up between \$0 and \$100? The difference between these two options is entirely in their second moment. The evidence for this is so vast and varied it is hard to even give an encompassing reference; but since you ask about the stock market, it is worth pointing out that one of the key explanations for why stocks have a higher average return than bonds is that they are more risky. I imagine a google scholar search of 'risk aversion in the stock market" will be fruitful. It is worth noting that there is also empirical evidence that people care about the third moment (skewness). This working paper by Yusufcan Masatlioglu, A Yesim Orhun, and Collin Raymond is an experiment in which subjects 

So, from a conceptual vantage, the difference is that indifference is the existence of knowledge, whereas incompleteness is a lack of knowledge. While interesting in a foundational/philosophical sense, the distinction is subtle and likely not the reason economists have spent so much thought on the subject. However, the difference has behavioral implications! and this is important for economics. For example, imagine various choices between "pizza", "hot dog", or "hot dog and $\$0.05$". Perhaps, the decision maker has never had pizza or hot dogs and therefore finds them incomparable. As such, from the choice set $\{p,h\}$ she would choose either (since neither option is known to be inferior[1]). So $C(\{p,h\}) = \{p,h\}$. Now, the decision maker also likes money, so although she does not know her preference for a hot dog, she will always choose more money to less, all else equal: $C(\{h, h+5\}) = \{h+5\}$. But her indecisiveness regarding the two foods might not be resolvable for 5 cents, so, it is reasonable that $C(\{p, h, h+5\}) = \{p, h+5\}$. But this violates the weak axiom of revealed preference. Such a choice function cannot be rationalized by a complete and transitive relation. Notice if the decision maker was actually indifference between $h$ and $p$ then, $C(\{p, h, h+5\}) = \{h+5\}$. In summary: incompleteness can cause "thick" indifference curves, and this can violate the basic tenets of rational choice theory. 

This is a common source of misunderstanding with regards to the concept of equilibrium. The specification of an equilibrium (at least in the standard theory of a one shot games) is agnostic about how the players figured out they are in that equilibrium, and, by definition, assumes players know the ex-ante strategies of their opponents. We do not need the complications of incomplete information games for this confusion to arise. Think of a simultaneous coordination game: both players playing heads is a NE, but how does player 1 know player 2 is going to play heads, since playing tails could also be a NE? In dynamic and incomplete info games this problem might be exacerbated, but the core issue is the same: equilibria assume the players are best responding to the true (ex-ante) strategy of the other players and therefore that they know which equilibrium is occurring. How to make sense of this? Well, for starters we could accept it as the way things work, assuming that equilibria arise from some learning/evolutionary process, and once an equilibrium is known, no player will want to deviate. This might make sense in simple coordination games, but is unsatisfying in the example you give. To remedy this, we could make assumptions on which equilibria will occur (i.e., refine the set of equilibria), so that in the surviving equilibria, beliefs are more clearly specified. There are many different ways to go about this, but, a refinement that was developed to assuage your concern in particular is the Intuitive Criterion, which eliminates eq. where (shamelessly copying from wiki) "some type $\theta$ who could benefit from a deviation that is assured of yielding him a payoff above his equilibrium payoff as long as other players do not assign a positive probability to the deviation having been made by any type $\theta'$ for whom this action is equilibrium dominated." Notice, this eliminates the pooling eq. since the high type alone could profit by deviating to $high$, which would signal to the Receiver that the separating equilibrium is in play. Lastly, the option that is least practical but perhaps most theoretically satisfying is to give up on equilibrium all together and admit that from an epistemic perspective rationalizability is the strictest prediction implied by rationality :)