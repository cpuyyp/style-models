There are tons of diachronical researches on Turkish, Arabic language and Persian language but great amount of those works are owned by literally prescriptivists and written in Turkish. Keywords: Arapça, Farsça 

I think you ask more Chomskian side of computational linguistics, but I did not run across a thing on Chomsky's theory on computational linguistics (of course, that does not mean there is not). Statistical linguistics is way more computational since it uses corpora, parsers, statistics and possibilities. It is best to start with any corpus linguistics and statistical computational linguistics book and learn n-grams, bigrams, trigrams and other stuff. If you say "I want to parse my texts, use corpora and take statistics.", there are tons of libraries for NLP just like Java's LingPipe, OpenNLP, StanfordNLP, Apache UIMA, GATE, MALLET, FrameNET or Python's holy NLTK module and you can use those libraries to use texts or corpora to achieve any goal. (Assume that you work on a research about women and men complaining. You have samples and you use them to take statistics and generalize results. Or assume that you work on a research about a dead language. You can use those libraries to take statistics of that language to define most-common particles in that language.) On the other hand, if you say "I want to write my own parser, tokenizer, my own corpus database model...", you have to know any programming language, regular expression and statistics/probability knowledge. I do not know if Ruby does but Java and Python is best fit for this work. 

I know Esperanto is constructed on the basis of Romance languages; but what are the main differences and similarities between English and Esperanto? Especially from the following aspects: (how are words constructed and orderly connected? what classifications / parts of speech does English / Esperanto have?) vocabulary (from what languages does English / Esperanto borrow terms / derived words?) semantics (words in what languages share similar connotations and implications with English / Esperanto words?) 

I assume by the "Binding Domain" of an α the author meant the Minimal Governing Category for α, which consists of: 

Since matrix-coding-as-object applies to his son but not to the product, now we are more confident that his son is the subject in (3). The above process is what I think is the utilization of syntactic phenomena that target subjects, which are observations that certain types of constructions only involve subjects (or potentially other grammatical relations as well), as tests for subjecthood in a sentence. By transforming a given sentence into constructions of one or more of these "certain types", we see if the resulting constructions are grammatical or not, and determine the subject based on such results. The more tests we apply, the more confident we are about our judgment. Is this process correct, or canonical in linguistics? From tests for subjecthood, can we possibly derive a cross-linguistic definition of subject? I personally don't consider this possible, since we are only "more confident" about a constituent being the subject of an examined sentence with increasing number of tests we apply; we are never 100% confident. 

I'm looking for a verb network or verb dictionary project in Japanese which shows verbs with its own dependencies (like [iku DP DP] [kaeru DP] etc.). Is there any? 

In my syntax classes, I learnt that v-spec is specific for role. The thing is, I came across a structure in the resource below: 

So, I wanted to ask, is there any solution or description to structures as above without violating , because this tree structure seems rather odd to me? A Brief Explanation Why I've Found This Structure Odd I know trees are just a way of description of syntactic structure in the cognition, however I do not know the limits of this approach. Like [created] just chooses DP dominated by VP2 out of blue, or are there boundaries? That's why I wanted to know if there are any analyzing of structures like those without abandoning . 

What does that mean? I learned that an must always be at v-spec and is really and there. I -at first- thought that does not have any accusative argument, so is at V-spec, but that did not make any sense at all. So, from this point, it went through this way below: 

Is it possible due to mythological reasons or some other human error (what can be the reasons of those errors in that case) that a language's number is flawed for e.g. -> Copainala Zoque number system Rayon Zoque number system In both cases (probably because they are neighbours/dialects) they miscount 8 and 9 as 9 and 10 respectively. (Or is this just some negligence on the linguist's side, which doesn't seem to be the case, as the miscounting is specifically indicated) Also does anyone know of more examples like this? 

I think , of all the IPA consonants and vowels, the ones with association to air would be a bit difficult to pronounce in the same way, 'cause changing gravity (in drastic way) can change air thickness. E.g. Implosives, Explosives, Aspirations, Clicks etc. 

This question is not the exact same as this question here,. Here, I want to ask if code mixing (if that's the right term) is affecting languages which are in contact with English. E.g. Hindi is losing the honorific ji and that has (probably) happened after the contact with English. Has this happened with other languages too? 

( Since I am not an expert on Tibetan and German, this answer might be biased to Sanskrit and Hindi but this won't affect the answer asked for ) I have generally seen krodha when there is a negative connotation of anger or when the anger might result in a (mythological) curse and kopa when it's godly/well-reasoned/non-cursing anger. AFAIK cognates are a pair of words which are similar semantically and phonetically but are from 2 different languages, but krodha and kopa are both used simultaneously (at least now) in Sanskrit and Hindi. P.S. Now though, in Hindi, generally, kopa is used to refer it's religious nature and krodha is used in almost every sense. 

Okay, so let's start with . DP hypothesis simply suggests that is a dependency of a in lexicon, which means [the dog] is DP, not an NP. You can test this on such structures having and at the same time. If you keep assuming the is complement of (and an will always be adjunct of ), the structure below would be grammatically valid: 

A few days ago, we had a conference on multidomination and I firstly saw an there. The solution was to simply abandon the conditions of well-formed phrase markers below: 

If you do not mind if it is Turkish or not: This is the library of TDK (Turkish Language Foundation). Unfortunately there is no single electronic documents but you can search keywords in catalogue. There is also National Thesis Center for searching any thesis. If you are abroad, there is no ability of downloading since registering is allowed for university members (including students) in Turkey, but you can take names and mail them to share their works with you. Coming to search, there is a search bar in the site below, you can use keywords to see the thesis on diachronic researches on it. (National Thesis Center also provides English site prefence for foreingners.) This is a catalogue for original manuscript by TDK. Do not forget all those resources are written in Turkish. 

Modern Icelandic maður is from Old Icelandic maðr. To relate the latter to the former, I would say "Old Icelandic maðr is the ___ of Modern Icelandic maður." What linguistic term goes in the blank? Is there another term for a meaning the other way round, i.e., that fits the blank in this sentence: "Modern Icelandic maður is the ___ of maðr." I thought about ancestor for the first blank and descendant for the second, but I am not sure if this is correct linguistic terminology. 

I don't understand the text in bold at all. Does it mean the gen. sg. ending might be either -s or -ar, or does it mean something else? Also, in A Dictionary, I find (-s, -ar) written right next to noun entries, like this: 

My understanding is that the two constraints are close opposites to each other (one asks featureless pronouns to be bound, one asks pronouns to be unbound), and the important point of the author's argument is that different languages allow different occurrences of the null pronoun since they rank these two constraints (and 4 others she mention) differently. However the issue is whether this oppositeness always holds. I think that depends on whether the C-Domain of a null pronoun would always match with its Binding Domain, but I cannot answer my own question. Any thoughts? By the way, the author considers the null pronoun to be a featureless pronoun. Thus null pronouns are subject to both of the two above constraints. 

If you want to look for an English resource, those are so rare but have a possibility of being much descriptive. For instance, this work seems to be a research on phonological structure of Arabic loanwords in Turkish. (Not free.) This is a page from Wikipedia. This work seems much more sociological, but it is worth to be seen. (Not free.) This is about Persian-Turkish relation. (Not free.) 

However, this is a brand new hypothesis in syntax, an merged two sentences having a structure as below (I made it): 

Now it seems okay. Coming to your tree, you must correct the and according to this hypothesis and you are good to go. 

Well, there was no platform on StackExchange for translation or translators, so I thought asking here is okay. As in title, is there a way to create a termbase and adding to it without creating a project in MemoQ 2015? 

However, I'm assuming you are taking a syntax class, which means this is the visible part of iceberg. You have still lack of knowledge of , the roles on and (I mean the usual movement from v-spec to I-spec, but it depends on situation) etc. I mean I gave a tree, but it is not still valid for current generative grammar, I made it because it is based on your givings above, it is valid to your current state. You can get an information on DP hypothesis here for now. However, I recommend Radford's syntax books as a reference, those are great. The Valid Tree of Late Government and Binding Theory (Phases Not Included) 

Edit: Added an image... As a Hindi speaker myself (which uses Devanagari alphabet) I can definitively tell you that the link has all of the soft consonants. Do tell me if you are confused by any other links and attach the hyperlinks (as you mentioned). 

1) The Major Mnemonic system isn't a language. It's just a coding system, but considering that to be a language, the binary codes fed to a computer is also a language and so on. 2) A language's purpose is to be able to convey thoughts. Thus such a language (which has just numbers) would only exist in a society where people just talk about numbers (which is obviously not possible) , not even mathematicians can talk just using numbers, because they have a "human" life to live which requires words/sounds to convey that meaning. 3) (a personal opinion) Number words came into a language , only after nouns or "object" words. One would have counted something only after she knew what is to be counted. Thus showing that other words "needed" to exist before numbers. And that the numbers aren't enough. (P.S. this answer is to an older version of the question) 

I am using A Concise Dictionary of Old Icelandic and An Introduction to Old Norse (by E. V. Gordon) as my resoources. In An Introduction, it is said that: 

I believe his soni _i to have stolen the product. *I believe the productj his son to have stolen _j. 

I'm reading the paper Constraints on Null Pronouns by Speas, in which the author defines two constraints for a cross-linguistic OT analysis of the occurrence of null pronouns across languages: 

TL;DR: How can we use syntactic phenomena that target subject to design tests for subjecthood (or even possibly derive a cross-linguistic definition of subject)? I am reading Van Valin's An Introduction to Syntax. It has many interesting examples of "syntactic phenomena that target subject", for example, reflexivization, relativization, wh-question and cleft formation and so on. To my understanding, such phenomena often always target the subject but not necessarily exclusively so; they can sometimes target other grammatical relations, such as English cleft formation, which can certainly target direct objects as well as subjects, as demonstrated by the following sentences: 

Say we want to know whether his son or the product is the subject of this sentence. Relying solely on the fact that subjects can be clefted in English does not help us determine which of the two NPs is the subject. It is better to test with yet another syntactic construction until only one of the NPs is eligible to a construction, such as matrix-coding-as-object: 

Welcome to Linguistics SE. 1) Do you have anything to support the presumption that says words like "meow" get replaced by unintuitive ones like "cat" or "kat" ? 2) In general, words in a language are not as intuitive as one might expect them to be; is wrong, that does not hold true, as no words are forced into the vocab in a language, the words develop as people want them to (there are exceptions, lots, but not in the words of daily use, not in the realm of words you are talking about). 3) Google says that cat comes from: Old English catt, catte, of Germanic origin; related to Dutch kat and German Katze ; reinforced in Middle English by forms from late Latin cattus. Assuming (1) to be true, the reason for that might be: a) English (like many other languages) doesn't have it's own set of words (i.e. the set is not exclusive), it has borrowed words from many other cultures and languages along the way and will continue to do so. b) Every language has it's own phonotactics, which I -personally- think might have been another reason for dropping "meow" and accepting "cat". (I don't know how universal this is, but, kids/ people still use meow for identifying cats, it's not vanished yet!) :)