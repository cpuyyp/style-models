A PostgreSQL server connection dropping after 10-15 minutes is almost certainly being caused by a state-tracking firewall (possibly using Network Address Translation (NAT)) between the client and the server. Many such firewalls have default timeouts of 15 minutes (900 seconds). The three server-side parameters, , , are designed to help in these situations. See the documentation located here: $URL$ There are also client-side parameters for this: , , , , which you can set on connection. See the documentation located here: $URL$ 

I think it's easier to keep track of the dependencies by using a purely pull-approach. In a way it's similar to the whole concept of queries on tables or queries on views on tables. It's all "pull", not "push". Most DBMSes have better control of transactions etc in the current database vs the remote database when running code in stored procedures. For example, in Oracle, you can't truncate a table on the other side of database-link. So you'd require a helper-package on the other side to perform these actions. Much better to have the stored procedure execute the truncate locally and pull the data to populate the table. I think "pull" is a better fit to the requirements of a reporting data-warehouse / reporting system. These tend to run at scheduled times. If you have an architecture that does incremental updates of the warehouse or reports, then "push" is a better fit. I don't like to mix the flows. Talk to your admins / ops team. If I was the DBA, I would prefer to admin systems that "pull" there data from others. I think it's easier to manage the cascading effects of eg down time in this way. 

The bad news for you is that substituting this type of index in the database will mean you also need to change the queries on this column/table. This may or may not also have implications for the user interface -- it could be that end users are currently able to enter search terms that are understood by the Oracle full text index searching implementation. 

Read the Oracle Concepts manual from (virtual) cover to (virtual) cover. The latest version is at $URL$ If you know and understand all of this you'll be in the top 1% of all DBAs. 

I would just . Even if I had to only delete if there were say exactly 2 records, then I would add this condition to the where clause, like 

Configure your database in archive log mode, without it you will lose data. Not a question of if, but when. Archived log files go to the flash recovery area. Configure rman retention policy according to how far back in time you wish to be able to go. You need this to recover from user errors that are detected some time after the fact. Size your flash recovery area to be able to keep all of this data. rsync your flash recovery area to another machine, preferable off site. 

Oracle's wrap program doesn't wrap triggers. The way to do this is to move the code into a package or a stand-alone procedure, and make the trigger a one line call to invoke this code. See the documentation on the limitations of wrapping: $URL$ In your case, something like the following (untested) should work: 

You have an error in your query. The CTE will return not a single row, but as many rows as in . Then you are putting all of those rows into the row generator CTE , whereas a row generator construct like you're using should get a single row. What I would do is to define to return a single row with the minimum and maximum values, and then subtract these in to get the number of rows you need to generate. Something like (untested): 

My standard backup regime for Oracle databases revolves around rman, flash recovery area and rsync. Basically as follows: 

You'll need to decide whether you want to use OS authentication or use a password file. See $URL$ to help you decide. Will users administer the database by first logging in to the database server using eg ssh? Then you can use OS authentication. Best practice for OS authentication: No-one should be logging in as the OS user. Each DBA should have their own OS account and login as that. Their OS account should be a member of the OS group (remember that only members of the OS group can login using ). If you want to allow remote administration, then you'll either need a secure connection to the database (just like SSH provides that to the server when administering locally) or you'll need to use a password file. 

Like I mentioned in the comment above, an alternative answer would be to put the subqueries in the select clause, as follows: 

Like many things, the answer to this question is "it depends". In this case, what it depends on is how many transactions each product will get. With low to moderate volumes of transactions, it will be very fast to compute the running total on the fly and you don't have to write lots of code to compute, store and maintain the totals. Index your table! 

however Oracle won't allow this as is a view, not a table. You could try to reference the underlying Oracle data-dictionary table, but that probably won't work either -- not least because you'd need two columns at your end -- and -- but also you'd need to be granted the privilege to reference that table. An approach that I would consider -- as recommended above -- is to create your own table listing which tables can be referred to. You could populate this table yourself, or you could create a DDL trigger to maintain it for you! The other thing to consider is what to do when the table is dropped: will you delete the record in your table, or set its contents to using ? 

Assuming you store all revisions of the document in a table, an approach would be to not store the revision number but calculate it based on the number of revisions stored in the table. It is, essentially, a derived value, not something that you need to store. A window function can be used to calculate the revision number, something like 

But as others have mentioned, optimising expressions isn't going to help you; the Postgresql syntax is merely cleaner. You'll need to show us your execution plan(s) and table definitions for us to be able to help you further. 

but there are many other options. See the documentation at $URL$ for full details. NB You must set the parameter to a value other than to actually enable the auditing to take place. 

If the two tables share ID space, surely then they should have a common parent table? I would create a parent table and make the two other tables children of it with foreign keys. The parent has the autoincrement field, and you have to insert there first before inserting the child. Add a "type" column to the parent table so you know in which table to look for the child with the given ID. The biggest advantage to this approach is that you don't need any triggers. No performance or synchronicity problems to worry about. In PostgreSQL you would implement this using inheritance, but I don't think MySQL has this feature. 

large offset: PG will need to scan through the provided offset number of rows and will prefer an index to do this above a table scan. how to index the data: I think a partial index might help you here. Can you try create index xxx_idx on latest_channel_snapshots(view_count DESC NULLS LAST) where network_id is null; See $URL$ for more information on partial indexes. In particular, example 11-2 on unbilled orders seems to apply to your case. 

This doesn't seem to be specifically an SSIS problem. It seems to me that the logic of the original code is such that the if the previous update statement updates 0 rows that it indicates a concurrent update took place. I'm guessing that the update statement includes one or more extra columns in the where clause on which previous values are matched. The update affecting 0 rows indicates that a concurrent update took place. You'll need to show us the rest of the procedure to determine whether this is the case. 

Another approach which should work for you is to install PostgreSQL from the PostgreSQL Global Development Group's APT repository. They provide compiled versions of PostgreSQL for all supported versions of PostgreSQL and all supported versions of Ubuntu (and of course some versions of Debian). $URL$ is the starting point for this. 

No, it can't be queried. These values are stored in sqlnet.ora on either or both of the client and the server. In either case, none of the contents of the network configuration files sqlnet.ora, tnsnames.ora, listener.ora etc (eg protocol.ora, snmp.ora, cman.ora) can be queried. If you really need to read the contents of these files and can't solve this problem in some other way (programatically via some OS level program or even administratively), you could read the file via external tables or via a Java stored procedure. I would prefer to solve this outside of the database.