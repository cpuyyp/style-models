SQL Server 2016 - Database Mail doesn't work without .NET 3.5 How can I see the current Database Mail configuration? 

The options in Management Studio On Management studio Tools->Options->Query Execution->SQL Server there are options that are set and these prevail over the same options that are set on database level. 

this link below has also helped me specially when I need to copy the database mail settings from one server into another: The database mail configuration saved into a temp table I got the following script that helps troubleshooting databasemail basics. Pay attention to the comments and run one step at a time. 

I basically saves the contents of DMVs into tables (tablebackups) so that I can compare the situation as it is now (straight from the DMVs) and the situation as it was (from the tablebackus tables). works fine. Only problem and hence this question: Sometimes the total_worker_time now is less that what it used to be, for a reason I haven't identified, and therefore it gives me negative values. I have been run this script as a stored procedure that is called by a job every 5 min. so in the last 5 min I know how many times each procedure has been executed, that has been accurate, but the CPU usage has not been, for the reason above. what can I do to tackle this? 

Unfortunately, no. You are installing a program and making service and registry changes, etc. You need to run setup as an account that has local administrative rights. No workaround to this that wouldn't involve some back door into administrative rights. Best bet is to ask for someone to temporarily give you the rights or install it for you. Source: The Microsoft SQL Server books online article for how to install SQL Server 2008 R2Express 

You should set it to automatic if you intend to use it. I am not sure why Microsoft decides to leave the default to manual after installation but I always change it because I always schedule maintenance. So you are on the right track to start and set to automatic. 

In FULL SAFETY mode you should be good to do this without data loss if there are no transactions in flight. Are you waiting for the state to change from Synchronizing to Synchronized after that restart? My guess is application users are in there when you are doing the failover, though. If the applications are still connecting and data is still being modified then there are transactions in flight and that is the price you have to pay. Best to get a window and stop access. Shut down the site or apps accessing SQL, etc. 

Memory can help you by caching and thus reducing I/O. However, that won't reduce CPU usage which is your problem. This is an unusual bottleneck, as CPUs are insanely fast for most database work and I/O tends to be the bottleneck. In your case, it is even more inusual, because you have 16-cores and VPSs tend not to have great I/O performance. First of all, make sure that you are CPU bound. should help here.If you have high numbers on the column, you are probably I/O bound; high numbers on the column could indicate that you are really CPU bound. If you are CPU bound, analyze what queries are you executing and why they take long to complete. You are either executing a lot of queries/s or they include complex calculations (i.e. aggregates, functions, etc.). The solution for the former is usually caching on the frontend, which means executing less queries. The latter is solved by simplifying your queries (if possible- you might have queries which are needlessly complex) and calculating stuff once and reusing it (say you have lots of aggregate queries; create a table with the aggregation results and query that instead of running aggregates continuously). The most efficient way to research about this is by logging which queries you are running and analyzing the log- tools exist which do this neatly. If you are I/O bound, then you can tune memory usage, although the OS cache is often working correctly. Take a look at : 

SQL Server gets its time Zone information from the operating system on which it is installed. So if reveals the right information, then SQL Server is most likely not the problem. You can query the registry on your SQL Server, if you have the appropriate permissions, and see what the server's time zone is set to: 

The only times I have ever seen the State 8 error for 18456 (Password Mismatch) was in fact a password mismatch. Try a new password. Type it in notepad, copy and paste it to SQL and then do the same into your app config. Make sure nothing is happening to that password when it is being handed off to SQL - no trimming, extra spaces, etc. But then the second error you got seems to indicate you may have gotten past that bad password error and are now unable to access your database. That can be one of a few things: 1.) Verify the database specified really does exist. 2.) Verify that the login you are using has been granted appropriate permissions into that database (If the permissions are at the SQL Server system administrator level then this shouldn't be a necessary step) That's where I'd start anyway. 

On the maintenance plans I typically let my maintenance run as the agent account and I normally keep them owned by SA so when I quit or became a consultant things still run. When I help a client with maintenance/management I do the same thing. You see to run maintenance you need more than just CRUD permissions. You are doing backups (backup operator), index rebuild and statistics updates (DDL Admin, Alter or DBO), potentially error log recycles (not sure here perhaps setupadmin, definitely SA), etc. For maintenance, it is a process you own, you setup and you trust. Like a monitoring tool, I see no issue with maintenance running as sysadmin. I would also suggest you check out Ola Hallengren's maintenance solution. Maintenance plans are okay and serve a purpose, but he's put a lot of work into his maintenance solution and it works well. On: 

another thing that comes to my mind is to create the login server specifying a username and password that would have the relevant permissions on both servers. Either that or an account.Would that bring security concerns? 

object_definition and sp_helptext both return the source code of a stored procedure - but how can I use and get the source code including the line breaks as the currently does? for example in the code below I create a stored procedure: 

I got it working maybe not the best but it is ok for now. what I did is, I have added the following code to my script: 

this has all been set up by a consultant and I am not sure if it is all right. what do I have to do to make these reports on this particular server available to my users? or at least, is there a default procedure to start troubleshooting? Below is the view of my reporting services configuration manager. the service account 

Another thing I noticed, you can see on the picture below. These are 2 different Management Studio sessions. On the top one, I am logged in as myself, , on the second one I use Management Studio with and I use a domain account that I use for the replication, which is not . The second one has the in this and other servers as well, whilst mine is the normal green one. 

So this code may not be exactly what you do, but the point is - you are requiring a positive step to identify where you think you are and checking to see if you are where you think you are. By using a Script you are also requiring yourself to type out items like the database you wish to restore, you are forcing yourself to look things over, pay attention to the SSMS window and see which server, etc. You are also preparing yourself for emergencies, by having a process, having a script and having an ability to use the same approach each time. Now when you have to restore and your CIO is standing behind you rapping her fingers on your cube wall repeatedly saying "is it done yet?" you don't have the added stress of doing the process a different way. 

So basically says "if a divide by zero error happens or an arithmetic overflow happens abort the query" This is usually desirable behavior and is the default instance wide setting. If this causes issues with your vendor's queries, I would say that they may have been suffering from some coding issues to begin with. I would ask them for more details on why they are concerned here. Of all of the rules of indexed views, I would call this and many of the set options rules the least controversial. This would have to be set in the connections that interact with the view. So you would want to work with the vendor and really try and understand their reasoning and try and get them to commit to what they are thinking on the big disagreement here. That said - Indexed views are a bit of a big deal. They have other rules and they can impact the application and assumptions the vendor's developers had when building and performance testing. You should really have a conversation with them about the business problem you are trying to solve through indexed views and get them involved in the conversation about how to solve the problem. 

, but it seems a bit overkill to me. You could also force the creation of an artist row in any case, but sometimes this doesn't seem appropriate. Are there any better approaches? Cheers, √Ålex 

You really want to read first: $URL$ If you want to keep your history in a separate table (and probably you do), you will probably want option #3; it tends to be the easier to implement and more convenient, #1 are #2 are pretty ugly and "un-relational". 

I've seen a few instances of entities which have a field which can be either a reference to another entity or free text. You typically represent this in the UI by having a dropdown widget which can optionally have text instead of one of the dropdown items selected. Say you have a song entity which was recorded by an artist- the artist can be either a reference to an artist entity or a random string. I'd usually represent this like: 

So we are writing an app whose schema should reference data which lies currently in an external PostgreSQL instance. We are negotiating being able to put our schema within the external database, but we are evaluating different possibilities. One option I'm pondering is basing our app on PostgreSQL and use its facilities for accessing external PostgreSQL instances. What's the status of this? PG's documentation contains $URL$ and $URL$ , which allow you to reference tables in an external server. What's their status? Are queries performant (i.e. sends WHERE to the other side)? Can you reasonably join between local and foreign tables? There's also pgsql_fdw ( $URL$ ), which seems more featureful. Does it offer an improvement on the above? Anyone using it? Thanks, Alex 

I am sure there are other approaches you can take. I'd probably go with the script route, but again good sign to see someone concerned about preventing this. 

My guess is you are really on a goose chase, and the issue is #2, though. UPDATE - On re-reading your question, I'm more convinced you are likely either A.) in a fine situation with no issues and just being proactive/worried or B.) your application is doing a really high frequency of index or table modifications in the middle of the production day. I am more convinced you are fine here. And the answer of copying all of the data out to just avoice a SCH-S lock doesn't really fit well - because you'll likely have more performance issues copying that data out and keeping it up to date and generate SCH-S locks there. The real answer would be to look at what the operations are in the vendor's app that is causing the SCH-M locks which are blocked by your SCH-S locks. Perhaps looking at ONLINE index rebuilds if enterprise, perhaps less frequency, etc. 

As far as I know, there is no hard "limit" in terms of some absolute value of time that the optimizer searches for a plan. Each execution is budgeted some amount of percentage of time to explore the plan options. There are a number of factors that go into this - I trust that those factors are always changing somewhat behind the scenes between versions of SQL Server - there are optimizer improvements made and some new options or approaches to options could come out, the budget calculated could change, etc. SQL Server (thankfully) uses a process where it looks for a "good enough plan" or a reasonable plan. The more complex your query is the more insanely high the number of permutations of choices the optimizer has to execute your query. So we want it to find a reasonable enough plan and move on - so our query can actually start executing. The whole point is "spend enough time to give me a plan that should work alright, don't spend too much time optimizing and exploring options" We really don't have control over that. And we really shouldn't. We should look to the plan the optimizer came up with and focus more on tuning our queries, tuning our designs and tuning our indexes to best serve the business need. Another way - in about 15 years of SQL Server Tuning, I've not really ever had to consider "darn it, if only the optimizer had 4 more ms of optimization" - normally all you need to know to make your queries faster is right there in the query plan you did get, or exploring questions like "what about a serial plan instead of a parallel plan? what happens if I throw a join hint in just to force order and see what decisions the optimizer makes so I can rewrite accordingly? What about row goals? What if I split this logic up, etc"