Place the root administration machine into the room locked with two keys, and give only one for each of the two admins. The admins will always work together, observing each other activities. It should be the only machine containing the private key to login as root. Some activities may need no root rights, so only part of the work would require to go to that room for "pair programming" You may also video record activities in that room mostly for making more sure that nobody works alone (that much is easily visible). Also, make sure the working place is such that screen is easily visible for both people (maybe large TV screen with the big fonts). 

Also check if you have configured the RemoveIpValve on Tomcat properly. For the configuration like yours, conf/server.xml should contain something like 

Your server stack may be buggy or outdated. On my server (3.19.0-41-generic 14.04.2-Ubuntu, xinetd 2.3.15 libwrap loadavg) xinetd does forward the needed header no problem. I would propose to upgrade the server stack. You can get the xinetd version with 

If I put the entry into /etc/hosts.deny, would it block all requests from that IP address to my machine (like iptables do), or would it only block access for some applications like SSH? 

(Of course you should have mod_proxy_http enabled and loaded in your Apache) If your application makes use of cookies, you might also have to properly set the ProxyPassReverseCookieDomain and ProxyPassReverseCookiePath directives. You can read the related documentation here. Also, you forgot to mention which application server you're running, even if I see you're using some java application server. Mind that often application servers won't like at all if you change the application's context, which you should keep even on the proxy to stay on the safe side. Finally, regarding your url rewriting, I'm sorry but if you don't post your exact configuration I guess that no one would be able to help you. Mind that anyway slashes do actually make a difference to web/application servers. Some add them automatically, others won't. 

Look at that last file and you'll find everything needed to bypass the default options, the file is decently commented. About tomcat6 being ok with the JDK8 instead, that's for you to discover :) 

Rsync+rsync or snapshot+rsync won't really make much difference - with rsync maybe being more handy, since you're able to eventuall compress/encrypt data during the transfer without the hassle of having to use extra commands. In both cases you're going to forever try to catch up with what your users might have copied on the share since the last rsync, including partial file still in transit. Honestly what I would recommend to you is to do a first copy with rsync in a period of low usage. Then, warn your users that there will be a small outage due to needed maintenance. Stop the services writing on the disk. Remount the old share in read only mode, do a final rsync and then completely replace the old nfs share with the new one. If you want/can, you can give customers read only access during that period. 100% availability is a pure dream, and it's better to stop your customers for 1 hour than to chase after possible endless complaints of lost/corrupt data and application crashes. 

First of all are you sure the connections are not just hitting the queue? i.e. you have reach maxconns? What does your stats page show? Also just disable conntrack (it sucks): 

I'm not sure that I would call DRBD easy, awesome maybe but not easy. As suggested by Jeroen: Round robin DNS or HAProxy can be used to route the traffic to both nodes, but yes your actual problem is that your database is only in one place. So hitting both servers may not be what you want. DRBD would ensure storage is replicated between the servers (solving the problem). Or the standard architecture is 2*HAProxy in front of N+1 Web Servers with the database on a separate and highly available platform (maybe master/slave maybe shared storage maybe DRBD or maybe memched). 

Karl, Call me crazy bu don't you just need to remove the OR? redirect scheme https code 301 if !host_mydomain or !{ ssl_fc } should be: redirect scheme https code 301 if !host_mydomain AND !{ ssl_fc } i.e. if NOT the domain I don't want AND NOT SSL 

Andy, The trick is to add another backend that you only use for the extra stick table. You can only have one stick table per backend - BUT you can use them in ANY front/back end... So I just add one called Abuse that you can then use as a global 60 minute ban for any backend... You will need to change my example but try something like this: 

Where can't you find servers with good memory amounts? Of course manufacturers propose low ram solutions, to keep initial prices low. They all propose upgrades, at a price. Manufacturers certified memory is expensive though. Even most economic to mainstream servers nowadays will accept over 32GB of ram. Dual socket servers work well with just one cpu too. It's unlikely that with all that memory busy with data you'll want just a few cores to serve a limited number of clients. If you have so little clients, it's unlikely that you need the extra speed of keeping so much inside your memory. Microsoft doesn't only sell per-core license. It also got Server+CALs licenses. Please see this link. You choice depends on your conditions. The future is virtual machines, sometimes even cloud based. A per-core license adapts very well to it, as in virtual machines you allocate cores, not cpus. Nowadays servers tend to have plenty of processing power; put a virtualization hypervisor on your machine and allocate just the resources (disk,cores,ram) you need. Upgrade in minutes if needed. Or use the same machine to host other stuff. 

You forgot the burst capacity. For both EBS magnetic and General purpose SSD, the "base" speed is as you said. Base speed is the minimuum guaranteed speed in any moment. But, while with magnetic disks you have a burst capacity of a "few hundreds", with SSD you can go up to 3000 iops. It then depends on what use you make of your machine: need stable constant i/o performance on a small disk through the whole day? Go magnetic. Have the casual I/O spike (reboots, a few minutes of heavy traffic, your night backup process)? Perfect scenary for the general purpose SSD. 

It is simple enough to use HAProxy to load balance any TCP connection including telnet (most protocols are very similar to telnet anyway). But you should enable persistence by source IP and for long lived connections you should make sure you set long timeouts and keepalive i.e. 

My first question would be why? If mod_security is on the actual server it will be transparent. If mod security is on a gateway then the client will only ever see the gateway server address. Just use x-forwarded-for to see the client IP in the server logs. It may be possible to use TPROXY in the linux kernel in a two subnet configuration where the servers default gateway is translated through the mod sec box (servers would NOT be able to have a public IP address). But I'm not sure if apache/mod_sec even supports TPROXY it needs code specifically to support it (like in HAProxy). 

At Loadbalancer.org we do this is a slightly complicated way :-). Before we restart HAProxy we check all of the current states and check for any servers that are DOWN or MAINT. Then we BLOCK SYN packets (handles the dropped traffic on reload issue) Then we restart HAProxy Then we manually set all the states to DOWN or MAINT (based on what they were 0.01 seconds ago..) Then we re-enable SYN packets... Giving us the required result of seamless restarts with no downtime or lost packets. If you can't be bothered with ALL the above then just set everything to DOWN state after a reload ..do it fast and cross your fingers :-). v1.6 has some new state-full restart code but I haven't researched it yet. 

With Apache you can do that with a reverse proxy. In your case with Apache you could use this kind of approach in the configuration file: 

I don't use debian but if it can help you, RHEL6 (2.6.32 kernel) uses ext4 as the default fs even for the boot partition since 2010, and I never had any issue with it. Not sure you'll get big performances boosts, sure it got some better algorithms and some tweaked functions but it honestly depends on the kind of usage of the disk. 

Sounds like you simply need to transfer some files. If you have a correct username and password in the domain you should be able to connect and upload your files. Solution 2, connect with a remote desktop and upload your files through it (slower). Or, if you got any troubles or want some piece of mind, why can't you install some ftp server like filezilla... you can set it up in 5 minutes and remove it in even less time when you're done. 

You might also want to try to lower the maximum number of threads and spare threads in Apache first, this might make your website react a little slower but if you tweak it a little bit and you don't have sudden usage spikes you should be fine. 

DSR is implemented in the Linux Kernel (IPVS) which only works with LVS (Linux Virtual Server). You will also need a health checking daemon like keepalived or ldirectord. BTW DSR does not work in Amazon AWS or in Azure due the to the network virtualisation security that they use. It won't work in things like Docker either. $URL$ 

Yes, Traffic is configured to hit a Floating Virtual IP, when the master load balancer fails the slave detects this, brings up the Floating IP and sends a load of gratuitous ARPs to the network. You can achieve this with VRRP, Keepalived, CARP etc. The Loadbalancer.org appliances use HA-Linux & HAProxy. 

Luis, Just some quick ideas: Are you sure it is actually up and running? netstat -l | grep 443 and/or ps -ef | grep haproxy Configure a stats listener so you can see if the health checks are working or not... Simplify the health check or even remove to get it working. 

The normal way to handle multi-site fail over is to have a pair of load balancers on site A in a master/slave fail over config. Then put an identical pair on the remote site. If ALL the backend servers on site A fail then use the public VIP on site B as the fallback (high latencey of course across the WAN)... But back this up with a proper DNS based load balancing services such as Amazon route 53 (which has health checking and fail over) , So that fairly quickly no traffic would go to site A anyway. If you have a really fast stretched VLAN across two sites with the same IP scheme then you can do it with just a single pair of load balancers. But it sounds like you should just forget HAProxy and use Amazon Route 53 for the whole lot? (unless you have persistence issues.)