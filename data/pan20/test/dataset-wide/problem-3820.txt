I'll start with a motivating example and only then proceed to the question. Consider a list of total packages of milk that were purchased on 9 consecutive days on a given store, $z_1,\ldots,z_9 = 1,0,0,2,0,1,0,1,3$ Assume I have an algorithm that predicts $\hat{z}_{10}=2$. I'm interested in assessing the confidence associated to this prediction, however, for stock management purposes, I want to measure the confidence that the predicted value is an upper bound on the true value $z\leq\hat{z}$, instead of $z=\hat{z}$. This is because what I'm ultimately after is to be confident in not running out of stock. I guess an estimate of this confidence, based only on given examples, is $1-p_{\hat{z}_{10}}$ where $p_{\hat{z}_{10}}$ is the probability of finding a value more extreme than the prediction $\hat{z}_{10}$ on the empirical distribution of all 10 observations: 

In other words, if you consider each monomial as a variable, then you can write these in matrix form as 

where A is a square matrix of height and width 3*5*7 which contains the variables x, y, and z but none of a, b, and c, and v is a column vector that lists off the 3*5*7 monomials in a, b, and c. Unless the only solution to your equations is a = b = c = 0, that means that the matrix A is not invertible, and therefore 

The image of $\Lambda_2$ in $G/\Lambda_1$ is finite. Take a set of representatives $s_1, s_2, \ldots s_n$ in $\Lambda_2$. Now consider $\cup s_i F_2$. I claim this is a Borel fundamental domain for $F_1$. It is Borel as it is a finite union of Borel sets. Let $g\Lambda_1$ be a coset of $\Lambda_1$. Then $g\Lambda_2$ intersects $F_2$ in a single point $x$. Now, $\Lambda_2 = \cup s_i \Lambda_1$, and so $x$ is in some $s_i$ translate of $F_2$, and hence in $F_1$. It is unique as otherwise we have $s_i \lambda_1 = s_j \lambda_1'$, which would mean our set of representatives was too big in the first place. 

Numerical Recipes, 3rd edition, Chapter 3 describes a vast number of interpolation methods with source code. It seems from what you are telling us that something is confused in the paper: Chebyshev interpolation has to do with picking points at which to minimize error of the ensuing result, but perhaps something else was meant/the usage is slightly different. 

Let $G$ and $H$ be groups over the ring of integers $R$ of a local field $F$ with finite residue characteristic $p$. Suppose we have a surjection $\phi$ from $G(F)$ to $H(F)$, and another surjection $\bar{\phi}$ from $G(k)$ to $H(k)$. Is this enough to conclude the image of a parahoric in $G$ is a parahoric in $H$? 

Suppose that $S$ is smooth and that $U\subset S$ is a dense open subscheme. Let $X$ be a scheme (not necessarily smooth) and let $f:X\to U$ be a finite flat morphism. I would like to know whether this finite flat family can be extended over $S$, and whether such an extension is unique. More precisely: 

Suppose that $f:(X,x) \to (Y,y)$ is etale at $x$, meaning that it induces an isomorphism $C_xX \to C_yY$ on tangent cones. Then $f$ induces an isomorphism from the cohomology of $IC_{X,x}$ (the stalk of the IC sheaf at x) to the cohomology of $IC_{Y,y}$. Now suppose that $f$ and $g$ are two such maps, inducing two different isomorphisms from $H^*(IC_{X,x})$ to $H^*(IC_{Y,y})$. Question: If $f$ and $g$ induce the same isomorphism on tangent cones, does it follow that $f$ and $g$ induce the same isomorphism from $H^*(IC_{X,x})$ to $H^*(IC_{Y,y})$? 

Let $f:Y\to X$ be a proper birational map of normal varieties over an algebraically closed field which is an isomorphism over the regular locus. Q1: Is it the case that the pullback $f^*\operatorname{Pic}(X)\to\operatorname{Pic}(Y)$ is always injective? (Update: this was answered in the affirmative by Jason Starr.) Q2: Suppose that there exists a finite set $\{C_i \mid i\in I\}$ of projective curves in $Y$ such that a line bundle is ample (respectively nef) on $Y$ if and only if its restriction to each $C_i$ has positive (respectively non-negative) degree. Is it the case that a line bundle on $X$ (interpreted via pullback as a line bundle on $Y$) is ample (respectively nef) if and only if its restriction to each $C_i$ that isn't contracted by $f$ has positive (respectively non-negative) degree? For example, suppose that $Y$ is a crepant resolution of a Kleinian singularity, with exceptional divisor $E\subset Y$ consisting of a finite union of projective lines. Suppose that $X$ is a partial resolution, obtained from $Y$ by contracting some (but not all) of the components of $E$. Then the Picard group of $X$ should be isomorphic to the subgroup of the Picard group of $Y$ consisting of line bundles whose restrictions to the contracted curves have degree zero, and its ample cone should be those line bundles whose restrictions to the un-contracted curves have positive degree. 

which would be $1-p_{\hat{z}_{10}}=0.9$. Assuming the above reasoning is correct (enough :), now to the question: I recently read "A Tutorial on Conformal Prediction" by Shafer and Vovk, and am curious on how to frame this problem on the Conformal Prediction framework. It appears to me the paper exclusively focus the case of estimating confidence for $z=\hat{z}$, so the question how to adapt it for the asymmetric case. If we were interested in the $z=\hat{z}$ case, a natural nonconformity measure would be $A(B,\hat{z})=|\bar{z}_B-\hat{z}|$, where $\bar{z}_B$ is the mean of $B$ (section 4.1 in the paper). That would define the following prediction regions (from the algorithm of section 4.2): $\Gamma^{0\leq\varepsilon<0.5}=\{0,1,2,3\}$ $\Gamma^{0.5\leq\varepsilon<1}=\{1,2\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_0=A(\{1,2,3\},0)=\alpha_3=A(\{0,1,2\},3)=2$ $\alpha_1=A(\{0,2,3\},1)=\alpha_2=A(\{0,1,3\},2)=2/3$ $p_0=p_3=0.5$ $p_1=p_2=1$ It feels wrong to transport this reasoning for the asymmetric case, as it considers both directions (e.g. 0) as "more extreme". In particularly would mean that we have a 0.5 confidence in the $z\leq\bar{z}_{10}$ prediction, much lower than 0.9 which was given above from the empirical distribution. It appears to me that for solving this problem we have to choose a nonconformity measure $A(B,\bar{z})$ that is monotonic with regard to $\bar{z}$, for example, $A(B,\bar{z})=\bar{z}$. That would make "more extreme" asymmetric, resulting in the following prediction regions for the above example: $\Gamma^{0\leq\varepsilon<0.1}=\{0,1,2,3\}$ $\Gamma^{0.1\leq\varepsilon<0.3}=\{0,1,2\}$ $\Gamma^{0.3\leq\varepsilon<0.6}=\{0,1\}$ $\Gamma^{0.6\leq\varepsilon<1}=\{0\}$ $\Gamma^{1}=\{\}$ given that, $\alpha_i=A(B,i)=i$ $p_0=1$ $p_1=0.6$ $p_2=0.3$ $p_3=0.1$ This means that we can be at least 0.7 confident in the the $z\leq\bar{z}_{10}$ prediction. Notably it still does not match the 0.9 confidence obtained from the empirical distribution. I wonder if I am on the right track... Thanks! Marco 

Let $L/\mathbb{Q}$ be a Galois closed field with Galois group a subgroup of $S_6$. Is it the case that $L$ is the compositum of Galois closures of linearly disjoint fields of degree at most $6$ over $\mathbb{Q}$? Currently this looks true from a bashing argument: taking each subgroup of $S_6$ up to isomorphism and either noting it is a direct product, or it has a large subgroup which is not normal and whose normalizer is the whole subgroup. Bashing argument isn't complete yet, so this might still be false. I'd like something more elegant though, and if it works for $S_n$ even better. 

Sato-Tate has a much more general form in random matrix theory stating that the Satake parameters are distributed as the eigenvalues of some random matrix in a compact Lie group related to the Zariski closure of the image of the associated Galois representation. The measure is just the Haar measure on that group. This applies to abelian representations, and recovers Hecke's equidistribution result. 

I've got a new criterion for comparing Galois reps which are four dimensional if we know the kernel of the residual representation mod $5$ (or any large odd prime) and the Sato-Tate groups (should be same for each one) are connected. Note that we don't need to have a large residual image, so this applies to a different set of representations than Faltings-Serre. It also doesn't use a large tower, making it stronger and more efficient than existing variants of the Livne method. What I don't have are some good examples: the one I was working on when I discovered this method really requires a criterion that works for $p=2$, which is a lot more complicated (and I don't have a handle on yet). So does anyone have some Galois representations to compare lying around meeting my critera? The kernel of the residual representation may be hard to determine on the automorphic side: I'm working on ways to fix that. 

and replace every occurrence of a^3 by x, of b^5 by y, of c^7 by z. For example, the first few equations would be 

Now if you treat x, y, and z as constants and consider only the variables a, b, and c, then you will end up with 3*5*7 polynomial equations in the 3*5*7 monomials 

My question is about a method described in Dr.Math forum for simplifying equations involving sums of radical functions. (The following is a transcription of the example given by Dr. Vogler): --- begin example --- We want to convert the equation 

But the determinant is a polynomial function in the entries of your matrix, which means that the determinant of A is a polynomial in x, y, and z. --- end example --- I have tested this method with several examples and it seems to work, at least in the sense that the roots of the radical equation are roots of the resulting polynomial (it seems however that the resulting polynomial may have real roots that are not roots of the original radical equation). The only method I knew about in order to get rid of radical expressions in equations was to carefully manipulate the equation and then raise both sides of the equation to the same power. This is to say I have a very basic math background. So finally, my question is: what is the name of this technique and where can I read more about it, or at least what are the keywords I can use to search google. I have many other questions regarding this technique, but I believe I good book on the subject would answer them. Just for completion, some of the questions are: 

The answer is no. Let $R = \mathbb{C}[x,y,z]$, $J = \langle y+z, (x+y)z\rangle$, $V = \mathbb{C}\{x+y+z\}$, and $S = \{1,x\}$. Then $\operatorname{in}(J) = \langle y, xz\rangle$, and $S$ is a $\operatorname{Sym}(V)$-basis for $R/\operatorname{in}(J)$. The algebra $R/J$ is also free over $\operatorname{Sym}(V)$, but $S$ is not a basis because $(x+y+z)\cdot 1 = x$. 

For each positive integer $i$, let $A_i$ be a fixed representation of the symmetric group $S_i$. I won't tell you exactly what $A_i$ is, but let's say that I have a very explicit description of its character. Let $\lambda$ be a partition of a fixed integer $n$, and let $\ell$ be its length. Let $W_\lambda \subset S_n$ be the stabilizer of $\lambda$. This group $W_\lambda$ is a semidirect product of the Young subgroup $S_\lambda = S_{\lambda_1}\times\ldots\times S_{\lambda_\ell}$ and a subgroup $T_\lambda \subset S_\ell$, which shuffles pieces of $\lambda$ of the same size. How can one compute the character of $\operatorname{Ind}_{W_\lambda}^{S_n}(A_{\lambda_1}\otimes\ldots \otimes A_{\lambda_\ell})$? Here $S_{\lambda_i}\subset S_\lambda$ acts on the $i^\text{th}$ factor of the tensor product, and $T_\lambda$ acts by permuting the factors of the tensor product. In theory, I know that I have all of the necessary information to compute this character, but I wonder if there are any useful combinatorial tricks to organize the data. Regarding the motivation for this question, I would like to apply the recursive formula in Remark 3.6 of this paper $URL$ when $W=S_n$ and $\mathcal{A}$ is the braid arrangement. 

Let $A$ and $B$ be Hecke eigenforms of some weight $k$ and level $N$. We know that there are irreducible representations $\rho_a$, $\rho_b$ of the absolute Galois group of $\mathbb{Q}$ whose trace of Frobenius of $p$ is given by $a_p$ or $b_p$, the Fourier coefficients of $A$ or $B$. $AB$ is a modular form of weight $2k$. It isn't clearly a Hecke eigenform, but its Fourier coefficients are still algebraic numbers, and it can be expressed as a sum of Hecke eigenforms and possibly an Eisenstein series of weight $2k$. We also have $\rho_a \otimes \rho_b$ a representation. It isn't irreducible, but can be expressed as a sum of irreducibles. The obvious hope is that this would correspond to the product, but that cannot be true because the characters don't match up. But convolution if appropriately defined would produce a form related to $\rho_a \otimes \rho_b$. So my two questions: what is multiplication of modular forms representation-theoretically, and what is tensoring of representations on the modular forms side? 

Conway and Sloane, as well as Cassels, and also O'Meara, all have their own idiosyncratic way of expressing the following result (for good primes): Every quadratic form with coefficients in $\mathbb{Z}_p$ is equivalent by a change of variables with coefficients $\mathbb{Z}_p$ to one expressible as $\oplus_{i} p^{e_i}h(\epsilon_i, m_i)$, where $h(\epsilon_i, m_i)$ is $y_1^2+y_2^2+\ldots+y_{m_i}^2$ if $\epsilon_i=0$, and $y_1^2+y_2^2+\ldots+ry_{m_i}^2$ with $r$ a quadratic nonresidue if $\epsilon_i=1$. Furthermore there is a quadratic form $f$ over the integers with given local components $f_i$ if and only if the compatibility conditions for rational equivalence are satisfied. In the case of rational equivalence this has been given an effective treatment by Kirschmer and a coauthor whose name escapes me right now: given a discriminant and a list of primes where the form should have negative Hasse-Witt invariant, they compute a quadratic form. What I would like is a function that given a discriminant (as an integer), and a list of local components, computes a quadratic form in the given genus, i.e. computes the form $f$ described above.