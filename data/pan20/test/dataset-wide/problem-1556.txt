Another useful function you might add is get_once. I do something similar for when I store an error message in the session, then redirect to a new page and display the error message. After that the error message is no longer relevant so I remove it from the session. 

The security aspect of things looks fine, but there are some areas for improvement, I have provided inline comments where possible. The whole quiz could be coded as a class, and would be a lot nicer, however I didn't want to go too far from what you started with so you can see what changes I have done. Overall, I have split the code into functions, and tried to make each function do 1 thing only, and renamed variables for readability. This will become more important when you go to maintain the code. The first bit of code is what I used to test all the functions and catches exceptions that occur. You could improve by adding more error checks after each prepare and execute, but I will leave that to you, as they are only really necessary if the database structure doesn't match the sql. 

Using a like with a leading wild card will likely be very slow as no indexes can be used. As such they should be avoided, which should be easy to do when the string you are checking it a concatenation of values from a table. I would suggest you do another join from the blog table to the blog tag relationship table, and from that another join to the blog tag table but specifying that the tag is php . This should mean any blog with a php tag will be brought back (complete with all its tags concatenated together) while any blog without that tag will be ignored. Not tested, but something like the following 

Use explicit joins with the columns to join on in an ON clause. In this case it will make little difference but when queries are more complex it makes things easier to understand. Better to get into the habit now. Don't use SELECT * in real code. It is a useful shortcut when testing but can cause a potential problem in future. Best to use an integer unique key of the customer on the orders table rather than the email address. This should be faster, take less space and will not cause problems should 2 customers have the same email address, or should a customer change their email address. Orderid looks like the primary key of the orders table and as I would suspect that an order can only apply to one customer you should not have any duplicates to worry about removing. If Orderid is unique then you can just do all the processing of the rows in a single foreach, with no need to store copies of the data in an array. 

Going Faster First, creating a temporary table out of would be huge. Technically, MySQL should be smart enough to do this on its own, and being only three short columns should allow it to fit easily in RAM. However, I've found that the simplest of derived tables can often cause it to go off the rails. I will confess that I don't have a lot of MySQL tuning experience. Second, this would be pretty easy in PHP. Load the results of into an array and use two passes to calculate the minimum, filtered, and total completes. The latter two values can be calculated together during the second pass. Since you probably need these values in PHP in the end, the only real cost is slurping in all that data which again isn't very much here. If you got tricky, you could even probably do it with a single pass while streaming the results from MySQL so you wouldn't have to store the entire derived table in RAM at once. For each condition, you only need to know a) the minimum subcondition completes, b) how many subconditions have that minimum value, and c) how many subconditions are over that minimum. The total filtered completes for that condition equals 

I prefer to have a guard clause at the top, so you can easily see the outcome of the if/else without having to scroll to the end of the code 

Looking at your sql's you seem to be repeating the same queries in a loop. I will pick out one simple one for example, it doesn't take long to run on its own, but if you add them all up, the amount of roundtrips to the db grows. 

You have a lot going on in one middleware. Keep in mind a middleware runs for each request so it needs to be reasonably well optimised. I have split the logging bit out into PlatformLogger class, that seemed like a logical separation. I have also removed a lot of the } else {, to me they just add noise and in most cases you return anyway, so the else will never get triggered. PLEASE NOTE: The conditions below where I removed the else's may not all be the equivalent to the sample provided, it is just to show you what it would look like, you would need to check and make those changes yourself if you choose to do it that way. Also check your if conditions, and do the cheapest test first as they might short circuit the if statement and you won't have to perform the other slower test that involve database access for example. 

Looking at your original query I think you might have issues with the precedence of the AND and OR parts of the WHERE clause. However I agree with the above comments on the duplicate subselects, but not sure whether keeping those will still be the most efficient solution (might well depend on your data), as I can't really see a clean way of eliminating them. First suggestion would be to split off the join to one specifying the userid columns, and then doing joins to the 2 subselects. The sub selects are both the same but the columns they join on are different. This might mean the join(s) are more efficient. 

Doing this in a loop will be slow, and doubling the calls by retrieving each column with a different query will make things worse. Why not use a JOIN between your original query, something like this:- SELECT database.job_id, job_sheet.qty, job_sheet.row FROM database INNER JOIN job_sheet ON database.job_id = job_sheet.job_id In your loop this would mean a single query rather than ~20000 queries. If you do need to do a separate query for each time through the loop then a prepared statement would help but peformance will likely still be poor. 

Other Thoughts I'd be tempted to drop the stack of sections () in favor of storing just the active section since that's all you need. To pop a completed section off the stack, a) store a reference to the parent section in each section instance and b) assign the parent to after each section completes. Finally, you need to handle exceptions so other sections can be run. You should be able to do it once in . On to Phase Two! I'm very curious to see this in action. With QUnit I am often forced to perform the same actions on the fixture as a previous test did in order to set up the current test. This causes code duplication whereas the whole point of setting up a fixture is to avoid it. In Java I've been building separate fixture classes that provide methods for setting up common fixture states that can be reused across many tests. While that's helpful, it's not as easy to do in JavaScript.