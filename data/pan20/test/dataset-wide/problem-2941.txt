"A primitive state of existence, untouched and uninfluenced by civilization or social constraints: when people lived in a state of nature." American Heritage 

I share your concern. Physics and mathematics deal with two different subjects. While we all agree that our mathematics is a human creation, physics covers a subject that man is not really in a position to change at will and that has existed long before man. An axiom is a statement that we consider as a foundation of our knowledge. While in mathematics we are largely free to add new definitions or to alter an axiom in order to derive new reasonings, physics is constrained by experiment (this constrains the choice of axioms). As a result, the research procedure is distinct in both fields. In mathematics, the general idea is to pose a problem (e.g. verify a conjecture) and try to find a solution within the confines of that mathematical world we are able to define quite accurately. In the extreme, mathematics does not care about the semantics of the objects described, aside of the mathematical properties which are explicitly laid down. In physics, there is an additional constraint: whatever is expressed must have a (near-)equivalent in experience. Hence a mathematically expressed law must result in an a (near-) accurate prediction in a series of experiment. The rules for validating a law in physics are also very different: it is impossible to demonstrate a physical law as true (in the logical sense), because it would mean exhausting all possibilities of the contrary. What we call "certainty" is relative and depends on many experiments that matched the theory and no credible counter-example. Hence the nearest mathematical equivalent of a physical law is a conjecture. And while in mathematics it is generally possible to turn a conjecture into a theorem, physics has to live with that degree of uncertainty. Hence, could physics be described by axioms? To a relative degree, yes: as the motion laws of Newton were the foundation of classical mechanics (but had to be altered by Einstein), or principles of QED are a foundation of modern sub-atomic physics. Then physicists use the tools of mathematics to try to match reality as best as they can (the theory could be conceived as a copy of reality on trace paper, it is not the original). Hence limitation of whatever axioms we use in physics is the usefulness and accuracy of the results according to experiment. For the conclusion about the enthusiastic claims on "ultimate laws of physics", here is what physicist Richard Feynman said about this (emphasis added): 

Something adverse affected some party B in the past, against its own will (hence there has to be a damage or an objective prohibited action; plus see the maxim volenti non fit iniuria, i.e. if the other party was willing and knowing in the first place, there might be no complaint possible). A causal effect was established between B (victim) and a person A (defendent). The action of A violated the expected behavior in such circumstances, either prescriptive (do) or prohibitive (do not). 

Grammar is merely a social consensus on how to express oneself in order to be easily understood by others and to understand them. It is not to be represented as stemming from "natural law" or representing any political orthodoxy, or else a program(-me) to devaluate or impair other languages. There are, of course, many examples of abuses in recent times particularly with the reinvention of national identities in the nineteenth century -- when nationalist policies in education were aimed both against foreign (typically imperial languages) and other languages and dialects within the nation-state (deemed "inferior"). There was, in some cases, an element of "social darwinism". It is very possible that the mystique of grammar and fear of making mistakes is traceable to educational methods that were punishing children for using dialectal expressions or inflections, instead of the standard "national" canon. It is obvious that this fear was not conducive to better speaking or writing in the long term, but to frustration, revolt and eventually deliberate avoidance of grammar. It has led as well to the transformation of "minority languages" into political weapons to claim autonomy or independence (and, in some cases, to restart the whole cycle of linguistic oppression). On the other hand, let us guard ourself against a fallacy: abusus non tollit usum. There are very functional reasons why a consensus has to emerge somehow about grammar and be relatively stable in time: if anything, so that we can understand each other unambiguously in daily conversation and correspondence, and so that we can understand the regulations of our society, and possibly decypher texts written one or two centuries ago. 

In the first case, it would have little to do with logic per se, but it would be a social/policy question. The proposition would be decid -able all right, but one doesn't want to go through the process of rationally deciding about it here and now, for some reason. In the second case, it is too early, because there are still a few logical steps to do before getting there. The moral, is that if the argument "not the time or place" had value (and if both sides were rational), then both sides would likely be able to agree on why it is so (on SE, it could be that it is off-topic). Of course, in the context you are mentioning (activists, scoffs and scowls, etc.), the hypothesis that all people involved are rational is likely not satisfied. In any case (especially if you were the only person with your head about you), you could perhaps ask: "Could you elaborate why you consider that this is not the time and place?" and take it from there. This would boil down to the classical: "if we want to argue about it, let us first define our terms". Here I might get slightly off-topic (as this might pertain to workplace discussions): if you were the person to raise such an objection, you might want to formulate it in a positive way such as: "in order to decide on this, why do we not get together at such and such place and such and such time, since [...explanation...]"? 

If we consider that Noah Webster's definitions (1828) are representative of traditional American (and beyond, Western European) views: "The objects of philosophy are to ascertain facts or truth, and the causes of things or their phenomena; to enlarge our views of God and his works, and to render our knowledge of both practically useful and subservient to human happiness." From that perspective, the answer to your first question is a resounding yes. In particular, the modern separation between physical sciences (then called natural philosophy) from philosophy is rather fuzzy, considering that current consensus on the subject is that each 'science' (discipline) is based on a number of assumptions which do belong to philosophy. Einstein's fundamental ideas about space and time belonged to philosophy; yet his work had practical applications. Until the end of WWII, Einstein was mostly called "mathematician", then he became known as a "physicist" when that profession came to prominence. So trying to draw a line between what is philosophical or not is not that obvious, even when we get into highly practical activities such as engineering, finance, jurisprudence etc. (how much philosophy in a cell phone's GPS?), and indeed discussions about it might be moot. We might perhaps call it a frontier, a land full of nuances when one thing slowly turns into another. For your second question (terminology), philosophy with application in mind (generally described as applied philosophy) was put in Ancient Greece under the heading of phronesis, the "wisdom relevant to pratical things". As such it was and is definitely part of philosophy (ethics being part of it). 

Fault thus appears in a social context. Hence the question whether "we are responsible for everything that will ever happen to anyone at all" is far too general and we do not need to answer it in order to answer the original question (it belongs to Occam's Razor). The relevant question about liability is establishing an objective (observable, documented, etc.) causality link (there cannot be fault without it), or at least a "faulty" action/inaction. Furthermore, one could directly cause a damage to someone else, without being considered liable: typically a person who falls from a stair in mall, hurting another one, can be held innocent of the injury, in the case the owner failed to apply a prescribed safety measure that would have prevented the damage, in which case the owner is considered at fault). It is correct to also frame the question from a subjective perspective of "cognitive", i.e. what one can be cognizant about, but as a subordinate question. In judicial matters, that is of course very relevant, together with another key aspect: intention. This applies to civil and criminal law, and more generally to adjudications according to the accepted morals of a human group. Note, however, that they come second in the adjudication (on the part of a judge, peers) as extenuating or aggravating circumstances. Hence ignorance or inability of seeing the consequences of one's actions can be invoked, but it is subordinated to the establishment of an objective (observable, demonstrable) causality as well as the violation of rules of behaviour. Hence an adjudication on subjective factors removes neither the objective causality nor the violation of the rules (e.g. if a mentally irresponsible person killed another, that person still killed the victim and that is still considered inacceptable). That is from the perspective of society. From a personal perspective, person B (victim) could clearly chose or not chose to take more responsibility for their own actions (e.g. the person who got hurt in the stairs could have looked around while walking, instead of at their mobile phone; and they might draw a lesson for the future, on the danger of it). But as long as any legal/moral obligation (prescriptive or prohibitive) was not violated, the rest of society society will not interfere. And that reconnects with the political philosophy of the social contract: that the aim of the rule of law is not to interfere with personal behaviours, but to provide the largest individual freedom that is reasonably achievable in an organized society. 

Before being mathematical or logical, the issue might be philosophical. Are we not facing an aporia? If we make a conjecture that absolute free will exists, then the outcome of any test experiment is free and could be anything. It could be random, pseudo-random (as in pi) or deterministic, or have any property that the free agent cares to give to that result. Any test might or might not be defeated, there is no way to tell, especially if the agent is given a feedback. In practice (at least, e.g. in business process modeling, where human intervention is considered), considering free will is actually a useful simplification, providing there are constraints that limit the number of possible choices. This reason why is Occam's razor: by postulating free will, we don't need to get into the details of how a decision is taken; we only need to know it is taken at a specific point. For example, if we consider an application for a loan from a public administration, we can safely assume that the officer receiving the loan could a) grant it, b) be reject it or c) suspend it pending receipt of more information. The detailed decision process is assumed to be devolved to "free will". The approach of merely providing guidelines (rather than complete and consistent decision charts) greatly simplifies the administrative modeling and might be better suited for practical needs. In those cases, whereas we will not be able to predict the individual outcomes, we will be able to take statistical measurements, e.g. a) 35%, b) 49% and c) 16%), to give us an appreciation of whether the process goes as we want. Here we have would have a case of relative free will.