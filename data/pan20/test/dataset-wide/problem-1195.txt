Many NoSQL products have sharding built-in. The DBMS itself looks after storing a particular key range on a certain server and keeping redundant copies for high availability. Client connections are routed within the DBMS rather than in the application. Multi-server scale out becomes easy, at the cost of CAP compromises. 

Your question is not very clear on this point but I'm assuming you want to find overlaps with a proposed, new booking. Let the values for the proposed booking be held in the variables , and . The SQL will be 

Whenever you have to pass the databse to the customer, first restore a recent copy as a staging DB. Run this against that staging DB: 

So you have several tables each of which holds an address. You're wondering if these common address-related columns should be in their own table. The typical answer is "yes" - a separate would be a good idea. There are any number of posts on this subject on this site. If you do make a common address table will you compare user input to existing rows so you can re-use an existing address_id in one of your tables? If the answer is "no", or you don't have a plan to handle small variations in how the user spells or formats the address, then why bother? 

You could create additional statistics for the data. Include a WHERE clause so each new statistics object references only a subset of the table. By bucketing the rows appropriately (big, medium, small?) the filtered statistics will be less bad than the whole-table ones, leading to better plans. The predicate must match that used in queries. Depending on your settings there may be more system activity for stats maintenance. Of course this assumes the table contains (or could change to contain) some combination of columns on which rows can be meaningfully aggregated. 

Since the work table is keyed by date, your desired fourteen day sliding window can be achieved. Since each value is a sum, summing again for each date's value is not a mathematical problem. Yes, there is a calculation at runtime. It is much lighter than the full standard deviation one, however. When new values arrive the work table can updated synchronously (it's a 1-row update), or asynchronously or in batch depending on the application's appetite for stale data. 

You could stop the passive node, drop it out of the cluster, unplug its LAN cable or otherwise disable the passive node before restarting the acitve services. The consequences could be .. um .. "intertesting", depending on how cleanly this process is handled and how skilled the tech team that configured the cluster were. 

This becomes ugly real quick. My suggestion is to implement it one table per entity. Write the queries and test them. They can be written incrementally, checking first for, say, country validity, then country and index, then country, index and counterparty and so on. If this approach surfaces a particular problem, pause and re-evaluate in the light of the new circumstances. 

The Page Fullness is 98.23%. This means there is only 140-ish byte free per page. This is larger than the minimum row size. So any insert in the middle of the key range, and likely any update too, will cause a page split and fragmentation. You'd need to affect about one million rows to get 30% fragmentation since there are about three million page in the index. Do you have processes which write at this magnitude? 

You could declare a table variable, insert to that rather than printing and dump it in toto at the end of your process: 

with values (45, person1), (45, person2) etc. And and so on for the other columns with repeating values. As for your question about whether it is worth it, well, that's quite subjective. You are using a relational database. The assumption is data is normalised. Your current design isn't. It makes sense to give the tool what it expects to receive, unless there are overwhelming reasons to do otherwise. The only one I can think of is that you only ever treat those repeating groups as a whole and never parse them (in SQL). In that case they can be considered strings that just happen to have commas at certain positions. For this your current design is fine. 

Yes. Normalise your design. breaks first normal form because it holds many values. Create a new table which contains only and . And please, please stop prefixing your table names with 'tbl'. 

If there two different columns, each of which by itself could be used as a primary key for the table, we say there are two candidate keys. One candidate key is chosen as the primary key. The other candidate keys which were not chosen can be referred to as alternate keys. Any key can be a single column or a combination of columns. Keys made from more than one column are called composite keys. A column may be part of more than one key. For example candidate keys may be (col_a, col_b), (col_a, col_c), (col_d); col_a is in more than one candidate key. The first two of these are composite candidate keys, the last is not. 

If your attributes are more-or-less fixed, and you don't mind doing a code release when the attribute list changes, then unrolling them with JOINs and a COUNT would do it, too. 

In my experience the logical data model tends to be developed with multi-part, natural keys no matter how long they are. The physical schema will have surrogate keys as the primary key and unique constraints for the natural key. The surrogate keys are for practical reasons, as @David expaned on, not to follow any theory. Reference tables with terse, commonly-used, stable codes I would tend to leave without a surrogte key and propogate the key e.g. SalesRegion = EMEA, AJP, NA, SA etc. It just makes debugging that little bit easier. In your original example if UserID is defined as being globally unique then that is the primary key of the User table and that is what should be propagated to child tables. UserProduct would then have two columns only. The three-column design you show is consequently not normalised; ClientCode is dependent part of the key (UserID) only. If another business rule says a product may be sold to a client only once then either a) Add a constraint to the UserProduct table which joins the incoming row to User and UserProduct to see if a matching row exists for the product/client combination. This would perform appallingly. b) Add a ClientProduct table with ClientCode and ProductID as the only columns; the PK being the two columns. A (before) trigger on UserProduct would insert to ClientProduct. A constraint violation would cause both inserts to roll back. This is a cleaner, more obvious and more scalable design. c) Have the application insert to ClientProduct before inserting to UserProduct. The application is likely to have the ClientCode in memory anyway. I know this breaks DRI. You could put the statement in a stored proc to protect the app from that rule.