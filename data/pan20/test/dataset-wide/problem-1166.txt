But it can be easily hacked if you change the with So instead you should really use the parameters as the sp_executesql is supposed to be used. 

first of all, the replication variables have effect on the slave only when activated on a replicated "slave" server. you need to understand that the filtering rules on master differ from the ones on slave. on master you can choose only to log a whole db or not. on slave you have more options. here is described $URL$ and here: $URL$ I think, you want to skip replicating a set of tables with a given pattern on slave. So, the variables must be configured on the slave. Change the configuration file on the salve and add the db_name instead of % for db part. 

You can prevent creating a procedure with invalid objects before executing the Create Procedure statement like that: You will see a red line under the invalid object which underlines the error when you go with the mouse over the red line. I am not aware of other method, because the SQL will parse the procedure before executing it considering only the validity of the SQL syntax. 

Do you want to perform the update once or on a regular basis? If the update on million rows is done once, then the best solution is to create a temp column "processed" on #testing2 table of bit (int,tinyint) which will serve as your null filter. The index on bit or int columns works a lot more optimal than on varchar. Also, having 2 type of values on index definition (0 for null 1 for not null) will be very fast. Keep your indexes and add the second index and you will have the following Plan if you change the filtering options in the query on "processed" column as it is marked in the picture below with the plan: 

the script I use will show you all objects (SP, Tables, Functions) for a specified Database name and all the users that have rights on them, but you can narrow the search and extract exactly what you need. 

I came to a dead point in a deadlock analyze. According to msdn: RangeX-X are Exclusive range, exclusive resource lock; used when updating a key in a range. RangeI-N are Insert range, null resource lock; used to test ranges before inserting a new key into an index. So I understand that if I have an Index on 2 key columns - and I insert a new key I would have RangeI-N lock but if I update an existing key from the index I would have RangeX-X. But my question is more or less complicated. Say I have the index IX_keys_included on column A, B and included column C. In Serializable isolation mode I insert a new value for the included column C. Will there be RangeI-N or RangeX-X locks for the index IX_keys_included? Actually , will there be any locks given the fact that I insert a new column for an included column in the index? 

In order to alter any column, first you have to drop the index or any other constraints that contain that column, and after create the index/constraint again. But this task must be done by a DBA because any drop on index or constraint will affect on one hand the data being queried - while the index is dropped, and on other hand put blockage on whole table for the time that the index is re-created. You need to make sure the users will be aware of this little or big (depends on the table size) maintenance. Good luck! However the Index create can be done with Online option which is less blocking. 

I don't think you can find this by an easy way but it is possible anyway to get through this. Profiler offers many event class types that can be used in analyzing the performance of a query. Start a new Profiler session and check following events: 

Make sure no one but one sysadmin has access to the restored database. Put the db in single user mode after the restore is completed. Check the code inside all stored procedures and functions and triggers inside this database. Perform a dbcc checkdb to make sure there are no integrity issues. Check the users which used to have access to the database and remove all of them. Start allowing access, very restricted to specific objects checked by you. 

The most promising exploration comes from removing the trash variable and using a no-results query. Initially this showed NOLOCK as slightly faster, but when I showed the demo to my boss, NOLOCK was back to being slower. What is it about NOLOCK that slows down a scan with variable assignment? 

Now let's build some procs that will DROP/CREATE or REBUILD, plus log (approximately) the time taken: 

I'm fighting against NOLOCK in my current environment. One argument I've heard is that the overhead of locking slows down a query. So, I devised a test to see just how much this overhead might be. I discovered that NOLOCK actually slows down my scan. At first I was delighted, but now I 'm just confused. Is my test invalid somehow? Shouldn't NOLOCK actually allow a slightly faster scan? What's happening here? Here's my script: 

I see no difference when using TRUNCATEONLY when compared to regular SHRINKFILE on the log file. Here's the script I built for testing (2016 dev edition, note the DROP IF EXISTS syntax): 

But appeals to authority are boring, so let's test it ourselves! Performance: Set up a table with a non-clustered index, load it with junk, and build a table for logging time. 

Understanding 1 is correct, and spaghettidba and Joe have good explanations. If you're interested in testing for yourself (on a test instance please), you can use the below script: 

To answer the question in the title, whether the B-tree rebalanced during a delete, the answer appears to be no, at least in the following minimal test case. The following demo runs commands that are best left for a test environment. 

Here are my own results: average time in ms for DROP/CREATE: 2547, for REBULD: 1314 It looks like in my contrived example, the winner is REBUILD. Constraints What if the index was created to support a constraint? In this case, a simple DROP and CREATE will fail because you have to drop the constraint first. Let's look at the clustered index from the previous example. 

1) Log flushing: the SIMPLE recovery model does not clear the log after every transaction, but at checkpoints. (link for more info) 2a) REBUILD ALL: yes, REBUILD ALL works as a single transaction. The index rebuilds within have their own transactions, but the overall operation isn't fully committed until the end. So yes, you might limit log file growth by rebuilding individual indexes (and possibly issuing CHECKPOINT commands). 2b) Proof! Here, have a demo script. (Built in 2016 dev) First, set up a test db, with table and indexes: 

Of course, like Erik alluded to, your actual problems probably have nothing to do with fragmentation, but hey, this was fun. 

This is an application issue more than a database issue. Navision treats datetime entries as UTC, and is adding the hour in its display (assuming your local timezone is UTC+1). I have run into this myself since I work with Navision. If you have a client application, you can look at displayed datetime for a record and compare it to what you see by querying the table. 

This demo shows that a delete can produce a very unbalanced b-tree, with practically all data on one side. 

OPTION (RECOMPILE) allows constant folding of @Step and @FindNo in your second query. Since @Step is known to equal 1, the line will be optimized away. Notice that this affects estimated rows. If you want to see a more drastic example of this, try the below code. Notice how constant folding (enabled by option recompile) allows the second query plan to completely avoid the union, since the optimizer knows no rows could be returned. 

Note how the first open transaction (Transaction ID 0000:000002fa for me) isn't committed until the end of the REBUILD ALL, but for the index-by-index rebuilds, they are successively committed. 

Short answer: start SQL Server with the -m flag from a local admin account. Step-by-step instructions here: 

Bad news: the plus approach in your script completely flattens out the XML document, and will need to be reworked if you have any multiple-X-per-Y structures in your document. If the dynamic SQL is a requirement, then please provide an example to test that aspect more easily. Another approach might be to build queries you need manually. If your fundamental problem is including parent info, then you might modify my demo SELECT query below. (N.B. Mikael Eriksson's answer has an improved query. Please refer to that.) Key ideas: is the context node, and gives you the parent is a different kind of XML function that belongs in the FROM section of the query and is usually seen with CROSS APPLY. It returns a pointer per match, and is what allows working with multiple rows. Read more here. is one of several methods for letting SQL Server know the value method only has one piece of data to work with (fixing the "requires a singleton" error) 

I would seriously advise against this design. Its a database anti-pattern called the Entity Attribute Value pattern. Here are some posts as to why: $URL$ $URL$ A better approach would be to start with the columns/attributes you have and add them at a later stage. You can run statements to make the changes later and there are tools to do this in an "online" manner as well. $URL$ Other things you can do is use a document-based database or XML. You can also store XML documents in MySQL in blob/text files, but you lose a lot of benefits this way. 

mysqldump does have an option of outputting to tab delimited files which you can LOAD DATA INFILE later: $URL$ 

I know what you mean. Alter table sometimes has to rebuild the whole table when you use it. Assuming that the Alter table statement is annoying you because it takes too long AND it locks your table, then you can use 2 tools to make it an "online" alter table. One if from facebook $URL$ And one is from openark $URL$ (MySQL 5.1+ only) Both these tools basically create a new table, copy the data over and keep it updated by updating both the old and the new table with new queries until the new table is completed. This means that you need to have the space available to do this operation. Hope it helps. 

You can use symbolic links with MySQL. All you would need to do is make a symbolic link for the .frm, .MYI and .MYD files of your MyISAM tables into the MySQL database schema directory that you want. After that, run and they should show up. It is also important that the MySQL user is able to access those files, so you might have a bit of an issue with user rights. Also, depending on your MySQL version, there maybe a setting in the my.cnf which enables/disables symbolic links, so you would need to keep an eye on that too. 

You would not be able to partition that table as you need to remove the unique key and make it a regular index instead. To answer your question, if you query on column that isn't partitioned then MySQL would query every partition one-by-one until it answers the query. So if you query a partitioned table on a column that it is not partitioned on, you may get slower results then if the table was not partitioned. That is why you need to choose the column you want to partition very carefully. I would select a column that fits 80% of the most important use cases. 

You need to find out first what makes the load get too high so that you are able to plan ahead. If its IO bound then you need faster disks. If its memory bound then you need to add more memory or find ways to reduce its usage. The best way to find out what the problem is, is to track your system during a peak time. Get a graphing application and look for: 

RBR basically updates the slave with the entire row as it is after it has been inserted/updated and updates using the primary key of the table. The advantages to this over statement is if the master server runs a statement which requires some sort of calculation or if it has a non-deterministic function or sub query in it. With the former, you do not need to perform any calculation on the slave as you are only updating the row after the calculation has been applied to it. With the latter, it may resolve race-conditions if the non-deterministic function or sub-query ran at a time when the result would be different then the time when it is run on the slave. So you can use this to keep your data consistent with these types of queries. Also, it is important to note that you can use RBR in a session (if you are using MIXED binlog_format on your server) for part of your application that you feel has these types of statements (add to that if you create your own temp table and need the slave to do the same). You can do in your application code for that specific (and potentially problematic) part of your code. The disadvantages are if on the master, you run one statement that updates 100,000 rows. To update the slave, you just pass that one statement very quickly. In my opinion, in transactions-based applications (where everything is just 1-2 inserted rows), then RBR can be advantageous. But in any case, you should test it out for yourself.