Stars aren't really orbiting the center of a galaxy, as much as they're orbiting the gravitational potential of the galaxy. The galaxy doesn't need to have a black hole in the center for the stars to orbit, and even if it does (which in fact it does most of the time), the black hole does not dominate the gravitational potential, except in the most central part (see sphere of influence of a black hole). All matter in the galaxy contributes to the gravitational potential, but since dark matter comprises 5/6 of the mass, this is the most important contributor. Only in the hypothetical case of a perfectly symmetrical distribution of masses, would the black hole and the center of rotation coincide. And since the paths of the stars are ellipses and not perfect circles, the center of the potential will not lie in the center of their orbits, but in one of the focal points. Furthermore, galaxies are dynamical and the potential continually changes, in turn changing the exact orbit of a star. Galaxies form due to the collapse of a dark matter halo, and the gas that follows along. Such a halo will in general have a non-zero angular momentum, which is partially conserved during the collapse (they do lose a significant amount, for instance through minor and major merging; see e.g. D'Onghia et al. 2006). As in the case of a forming star, a forming planet, an ice skater, or you on an office chair, reducing the radius means increasing orbital speed. This is the origin of the rotation. 

Since Andromeda is already visible to the naked eye, to a civilization located at half the distance from the Milky Way, Andromeda would be still be visible. Its total brightness would be four times higher, but since its area would grow by the same factor, its surface brightness would stay constant. The Milky is less bright by a factor of ~2.5, but also smaller, so the surface brightness and hence visibility would be similar. Individual stars would not be visible except if 1) one exploded as a supernova or 2) the civilization has evolved into beings with extremely large pupils, so that more light is being recorded by their retinas. In fact, we can calculate how large: According to this list, the brightest star known in the Milky Way is called WR 25, and has an absolute magnitude of $M = -12.25$. Half of the distance from the MW to Andromeda is $d \sim 390,000$ pc, so the distance modulus is $$\mu = 5\log d - 5 = 22.95,$$ and the apparent magnitude of WR 25 would be $$m = M + \mu = 10.7.$$ Under the darkest conditions, humans are able to see objects down to an apparent magnitude of ~6, i.e. a factor of $$f = 2.5^{10.7 - 6} = 75$$ smaller. Thus, to be able to see WR 25, the diameter of the aliens' pupils would have to be a factor of $\sqrt{75} \sim 9$ larger. Since human pupils are roughly 6 mm wide, their pupils would be 5 cm wide, and the eyeballs somewhat larger, incidentally roughly the size of a teacup. Thus, it seems that my compatriot H. C. Andersen predicted the existence of these beings in his fairytale The Tinderbox. 

Even if you're only referring the "ordinary" matter (such as stars, gas, and bicycles) and dark matter, the mass of the observable Universe does increase, not because mass is being created, but because the size of the observable Universe increases. In a billion years from now, we can see stuff that today is too far away for the light to have reached us, so its radius has increased. Since the mass $M$ equals density $\rho_\mathrm{M}$ times volume $V$, $M$ increases. As called2voyage mentions, we have several ways of measuring the density, and we know it's close to $3\times10^{-30}\,\mathrm{g}\,\mathrm{cm}^{-3}$. The radius is $R = 4.6\times10^{28}\,\mathrm{cm}$, so the mass is $$M = \rho_\mathrm{M} \frac{4\pi}{3}R^3 \simeq 10^{57}\,\mathrm{g},$$ or $5\times10^{23}M_\odot$ (Solar masses). However, another factor contributes to the mass increase, namely the so-called dark energy, which is a form of energy attributed to empty space. Since the Universe expands, dark energy is being created all the time. 

Since the clouds lie at a range of distances, they exhibit different time lags, effectively broadening the line: 

The term "color" is a label that humans have assigned to denote the ratio between the intensity at various wavelengths in the three different wavelength bands, or regions, that the human eye is able to perceive. These bands are centered roughly at 430, 545, and 570 nm, but are quite broad and even overlap: 

The term "optically thin" means that the optical depth is small. The optical depth is a measure of the opacity of a medium, in this case dust, experienced by light traveling through that medium, and is defined as $$ \tau \equiv n \, r \, \sigma, $$ where $n$ is the density of the particles in question, $r$ is the distance traveled through the medium, and $\sigma = \sigma(\lambda)$ is the cross section of the particles, which is dependent on the wavelength $\lambda$ of the photons. If $\tau\ll1$, the medium is said to be optically thin, while if $\tau\gg1$, it is said to be optically thick. The fraction of the light that is extinguished by the journey through the medium is $e^{-\tau}$, so the two cases indicate when the light is mostly transferred and mostly extinguished, respectively. Cosmic dust is composed of particles spanning a large range of sizes. Photons interacting with the dust are either scattered in another direction, or absorbed, depending on the albedo of the dust. But in both cases, a photon has a larger probability of interacting with a dust grain which is comparable to its wavelength. Because of the size distribution, an ensemble of dust grains hence has a characteristic extinction curve. In the figure below (modified from Laursen et al. 2009), I plotted the (functional fits to the observed) extinction curves of dust in the Small (dashed line) and Large (solid line) Magellanic Clouds$^\dagger$: 

Collapse of gasous halos Now what makes the intracluster medium (ICM) different from the interstellar medium (ISM)? In the expanding Universe, overdensities try to contract. These overdensities can reach hydrostatic equilibrium only if radiative cooling is small. If the cooling timescale $t_\mathrm{cool}$ is much smaller than the free-fall timescale $t_\mathrm{ff}$, it can collapse and form stars. From the virial theorem, which gives the relation between potential and kinetic energy of the system, you can calculate the relation between the two timescales. Its a bit too much math for this post, but not difficult. I recommend reading chapter 8.4 in Mo, Bosch & White (2010). Their Fig. 8.6 shows a cooling diagram with the locus of $t_\mathrm{cool} = t_\mathrm{ff}$ in the density-temperature plane: 

What's outside the observable Universe, we can't say anything about, but averaged over large enough scales ($\gtrsim$ a billion lightyears), it does indeed seem to be expanding uniformly. However, the presence of mass, or more generally energy, retards the expansion. This means that on the scale of clusters of galaxies, the Universe expands more slowly, and on the scale of galaxy groups, the galaxies' mutual gravitational attraction will prevent them from receding from each other. This is also why our galaxy, Solar system, planet, and bicycles will never get torn apart (unless the cosmological constant is not a constant). Conversely, in mass underdensities, i.e. the huge voids between clusters and filaments of gas and galaxies, expansion is increased (relative to denser regions). In fact, it has been hypothesized that that the observed accelerated expansion of the Universe is not due to dark energy, but could be an "illusion" from accidentally living in the center of a huge underdensity (e.g. Zibin et al. 2008). More recent observations seem to rule out this possibility, though 

Reionization In the early days of the (post-recombination) Universe, hydrogen was mainly neutral, but as star formation emerged, these stars started to reionize the IGM. This process started already at $z\simeq15$â€“$20$, when the Universe was ~200 million years old , but until $z\simeq6$, when the Universe was ~a billion years old, there was still so much HI left that Ly$\alpha$ photons scatter all the time in the IGM. Blue wavelengths are eventually redshifted to Lyman $\alpha$ Now when light leaves a galaxy, it gets redshifted due to the expansion of the Universe. That means that after a while, light blueward of the Ly$\alpha$ line is redshifted to become Ly$\alpha$, and may hence scatter. As the spectrum travels through the IGM, it redshifts continuously, so that every wavelength of the "original" Ly$\alpha$ line eventually becomes Ly$\alpha$. If the HI density in the IGM is high enough, that means that everything blueward of the Ly$\alpha$ line is wiped out. This is the Gunn-Peterson trough. If the density is low, but with some neutral clouds dispersed, then an absorption line occurs exactly at the wavelength that, at the location of the cloud, happens to have been redshifted to Ly$\alpha$. When there are many such clouds around, you'll gets lots of absorption lines, forming the so-called Lyman $\alpha$ forest. The fewer the amount of such clouds, the more sparse the forest will be. The Gunn-Peterson trough doesn't have to start at Ly$\alpha$, but since this is the strongest transition, it is most distinct here. But you will often see a corresponding absorption at Ly$\beta$ (and even Ly$\delta$ and Ly$\gamma$, as well as the stronger metal lines like MgII and CIV). This is visible in the figure below (from Becker et al. 2015) 

With respect to "eternally collapsing", they are probably referrring to the fact that in the reference frame of an external observer, gravitational time dilation prohibits matter from ever reaching the event horizon (the "surface") of the black hole. Denoting the radius of the event horizon $r_\mathrm{S}$ (for "Schwarzschild radius"), time runs slower by a factor of $(1 - r_\mathrm{S}/r)^{-1/2}$ for an observer at a distance of $r$. As $r$ approaches $r_\mathrm{S}$, this factor goes to infinity, i.e. the observer will never reach $r_\mathrm{S}$ However, this is only for the external observer; the falling observer would cross the horizon and reach the center in a finite time. With respect to "not being stable", you're probably right that they are referring to Hawking radiation which presumably makes the black hole slowly "evaporate". Near $r_\mathrm{S}$, pairs of virtual particles are being created and can be turned into real particles by the gravitational field. If they avoid falling into the black hole, energy is taken away from the hole, reducing its mass. 

(cyan=occultation at moonrise/moonset; red dotted=daytime occultation; blue=twilight occultation; white=nighttime occultation) 

The quantity you want is basically the extinction law, and is usually called $k(\lambda)$. An extinction law is a fit to several measurements of the extinction $A_\lambda$ in some direction (or an average of several directions). Cardelli et al. (1989) provides different functional forms for the mean extinction law, parametrized in their Eq. 1 as $$ \frac{A_\lambda}{A_V} = a(x) + \frac{b(x)}{R_V}, $$ where $x$ is the inverse wavelength in $\mu\mathrm{m}^{-1}$, and the coefficients are given separately for IR, optical, UV, and FUV in Eqs. 2, 3, 4, and 5, respectively. The total-to-selective extinction $R_V\equiv A_V/E(B-V)$ takes different values for different lines of sight, but usually lies in the range 2.5 to 6, with 3.1 being a typical value in the Milky Way. To get the quantity you're interested in, simply convert your favorite wavelength to $x$, stick into Eq. 1, and multiply by $R_V$: $$ k(\lambda) \equiv \frac{A_\lambda}{E(B-V)} = \frac{A_\lambda}{A_V} R_V. $$ 

I think the outer edge of an accretion disk is not well-defined, and observationally the radius will depend on which wavelength you consider, since the farther you get from the BH, the softer the radiation will be. But if you look in the UV, then Morgan et al. (2010) find the following relation between $R_{2500}$ (the radius when observed at $\lambda = 2500$ Ã…) and the mass $M_\mathrm{BH}$ of the black hole: $$ \log\left( \frac{R_{2500}}{\mathrm{cm}}\right) = 15.78 + 0.80 \log\left( \frac{M_\mathrm{BH}}{10^9M_\odot}\right), $$ (modulo some uncertainties that you can look up in the paper). That is, if your BH has a mass of $10^8 M_\odot$, its radius will be $R_{2500}\sim64\,\mathrm{AU}$, or roughly 1/3 lightdays. For comparison, its Schwarzschild radius is $\sim2\,\mathrm{AU}$, so your estimate was actually pretty good. This result is consistent with Edelson et al. (2015), who find 0.35 lightdays, also in the UV. However, in you look in longer wavelengths, the disk is much, much larger. If you're interested beyond your homework assignment, take a look at the accretion disk theory review by Armijo (2013), who shows that in the radio regime, the disk is thousands of AU, and even up to ~100 pc. 

Your feeling is right: You shouldn't convolve the spectrum and the filter, you should only multiply so that flux outside the bandpass is suppressed. Subsequently you integrate the resulting function over wavelength, so that flux density (in energy/time/area/wavelength) becomes flux (in energy/time/area). Simply setting the flux to 0 outside $\lambda_1$ and $\lambda_2$ (or, equivalently, just integrating from $\lambda_1$ to $\lambda_2$) corresponds to a "top-hat filter". Most realistic filters are more smooth. So, something like (untested): 

You mention the asteroid 2008 TC3, which had a diameter of 4.1 m. According to the fit, such an event happens roughly $1/(37[4.1\,\mathrm{m}]^{-2.7})\sim$ once per year. You can also see that the largest meteor hitting Earth in a century would be $(37\times100\,\mathrm{years})^{1/2.7}\sim20\,\mathrm{m}$, equivalent to $\sim500\,\mathrm{kiloton\,TNT}$. Similarly, hundred-meter-sized objects ($\sim1\,\mathrm{megaton\,TNT}$) should hit us roughly once per 10,000 years. By extrapolation of their fit beyond observational data, objects of $D>1\,\mathrm{km}$ â€” or 100 gigaton TNT â€” should hit Earth on average once every ~3.4 million years. 

Only very few meteors actually make it anywhere near the surface of Earth; most burn up 75â€“100 km above the surface. From your point of view, however, the curvature of Earth's surface may make it look as if they get much closer, and even fall below the horison. But depending on where you live, the horison often has quite a lot more background light (e.g. from cities far away). That means that when they get near the horison, which you may interpret as "several feet above the ground", the seemingly disappear. 

$^\dagger$As the authors note, "1) SR could be manipulated to give an evolving Hubbleâ€™s constant, and 2) SR could be manipulated to give a non-trivial relationship between luminosity distance, $d_L$, and proper distance, $d$. However, it is not clear how one would justify these ad hoc corrections". 

Until the Universe was 380,000 years old, it was filled with a gas of protons an electrons. There was also radiation, in thermal equilibrium with the matter, and because it was so hot, the protons and electrons couldn't form neutral hydrogen, since every time it "tried", an energetic photon would knock off the electron. This gas was everywhere. And photons traveled and scattered in all directions: 

If the observer has always been moving at a high speed, then yes, s/he will measure a different value than you. But if s/he accelerated to a high speed just now, you would obtain the same result. The reason is that the radius of the observable Universe is defined as the distance light has had the time to travel since the Big Bang, and that does not depend on your instantaneous velocity. However, if an observer started moving at, say, 0.866 times the speed of light when the Universe was born, then in the reference frame of that observer, the Universe would only be $\sqrt{1 - 0.866^2} = $ 1/2 the age measured by you, i.e. 6.9 Gyr. At this age, the radius of the observable Universe was only ~39 Gly, i.e. ~0.8 times the present value. Note that in order for this result to be exact, the observer would have to slowly accelerate. The reason is that in an expanding Universe, a given "peculiar" velocity wrt. the surrounding matter will slowly decrease, because the comoving coordinate system expands with the Universe. For instance, an observer starting out with $v = 0.866c$ at the time the cosmic microwave background was released 380,000 yr after the Big Bang, would only be moving at 80 km/s today if s/he didn't keep accelerating. If the observer were clever, s/he would probably realize that s/he did't live in an asymmetric universe where all matter seemed to be whirling by at close to the speed of light, but rather that the Universe were quite isotropic, but s/he had somehow acquired a high speed with repect to everything else. It would then be possible to deduce the "true" age of the Universe.