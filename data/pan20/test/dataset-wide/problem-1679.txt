I'm sure CIFS is working because if I connect with a local machine, it works well, but not via Internet. Thanks for your enlightenment! :) 

But then, I can't even access facebook.com nor www.facebook.com via http/https. So my question is quite simple : how can I redirect all https access to facebook.com (and eventually all sub facebooks : facebook.fr, www.facebook.fr, etc) to www.facebook.com (redirecting to www domain) in HTTPS ? Thanks for your help ! :) 

I'm looking to set up a webmail server that will be used by a lots of users that will receive and send emails. They will also have the possibility to forward emails they receive. I'd like to know which steps are recommanded/required to indicate to others Mail services (GMail, Outlook, etc) that my server is not used as a spam sender (disclaimer : IT's NOT ! :p) but a legitimate one. I know I have to define a SPF TXT records for example, but what others steps would you recommend me to do ? For example, is there a formula like having a proportional number of servers based on the amount of email sent (for having a different IP address) ? (something like sending a maximum of 1M emails / per IP / per day ?) Something else I'm missing ? I tried to search online, but I mostly find how to avoid emails sent with scripts (like PHP) being put in the SPAM folder. I'm looking for a server/dns configuration side. Thanks a lot for your help/tips, I appreciate ! 

I'm not quite used to IPTables and I'm trying to run an iptables script to allow only ssh connection from all and connection to mysql server only from specified IPs. I made a bash script for this, which is lister under, but when I run this, my master-master replication stops working. For information, here's my network structure : 

And, as you might expect, I have a fleet of servers that contains the actual code and runs uWSGI that subscribe to this fastrouter, like this: 

(with other params). Here's my problem: All the server are publicly available, not a part of a private network (The servers I took comes from different offers and doesn't offer this possibility, unfortunately). For the frontend, that means YOU can plug your uWSGI instance to it ... not great. I've found out that I can secure the subscription system, so this should be ok (can you confirm or infirm it?). But now, the parameter on the sub_servers is public, which is discouraged by uWSGI (first point). So my question: How can I take advantage of / from uWSGI on servers that are facing public access while keeping things secure? Thank you for your help! 

What is wrong with this configuration ? Why a HEAD returns 404 instead of 200 ? Thanks for the help :) 

When I'm trying to access an url like image%20xyz.jpg, I got a 404 error. When I look into the logs, Apache converts the %20 into \xa9 (and every url encoded characters are transformed to their \xXY values instead of the correct chars (space in that case). I tried AllowEncodedSlashes but this doesn't work. Which Apache directive should I put in my VirtualHost or .htaccess to make something like 

I really have a server called db1.mydomain.com but for a reason I can't find, it's relayed to localhost.mydomain.com. Of course, I configured aliases : 

Is it possible for a VirtualHost to listen to the public IP, without having to set it in the file (but by using the environment configuration) : 

I actually own two linux web servers and I was wondering how I could make them work together in case one goes down. From what I found, cluster seems to be the closest, because it would replicate the data (for mysql as example) from srv1 to srv2, and vice-versa. I found a howto on HowToForge, but they talk about 4 servers. After reading, it seems legit, because the cluster need a server to synchronise the data (and a second server in case the first goes down!). So it's not what I'm looking for, or eventually not in that way. What I'd like would be that srv1 & srv2 are identical (mysql and apache files) in case srv1 goes down. The aim at the end will be to build a FailOver (and not a LoadBalancing) system. For the IP, I will switch it manually from my registrar. Is it possible to do it with only two servers? Thanks for your help ! (If I was not clear, I'm sorry, It's not really clear in my mind too). 

I've build a simple Mail server for my personal use, but Google reject my emails whatever I do. I worked hard finding the initials problems (wrong SPF entries in the DNS), but now, everything seems fine, and I still got this kind of messages from Gmail : 

My question is pretty straightforward. Using OpenOffice or Microsoft Office, is it possible to automatically lock a file for edition on the Alfresco file system via Samba? For example, if a user goes to the samba directory, click on a file for editing it, it would be locked. When he finish to edit it (close the document), this document would be reopen to everyone. Thanks for your help :) 

All my users in /home/{user}/ have a specific error_log file in it that may grow overtime. So I was thinking about using logrotate to implement some kind of file reducing on it : when the file reaches 500kb, we remove the first lines to reduce it to lower than 500kb. It's not important to keep what is removed, so keeping the old lines is not necessary. I took a look at logrotate, and I came to this configuration file, but since I'm new with LogRotate, I was wondering if it would work. 

I setup an Apache2 server with the mod_cband to limit download speed at 512kb/s, and I was wondering what happens if I reach the limit. For example, if my server connection is 100Mb/s, I would theorically be able to serve 195 users. What happen if I have 200 simultaneous users ? Does the last 5 will have an error, or just the download speed that will be lower ? Thanks for your help ! 

The folder is correctly removed, but if I go to the /etc/passwd, I can still see the user in the file. I tried the command manually and there was no errors. What am I missing? (I can show any log if you need them, I just didn't find any odd things) Note: If I do this command () manually for the users already deleted, they are correctly removed from the file. This seems to indicate that when it's being called by the script, it fails for some reasons. 

I'm having an hard time trying to find out how to reduce this "repeated lines" for my NGinx configuration, just to change one property : 

From my understanding, when successfully update the certificate, it returns a success state (exit(0)), so the is followed, and so nginx is reloaded. Yes but it doesn't work. I recently had my server showing again an expired certificate, so I certainly misunderstood something, and/or my cron task is not good. Could you show me the path please? :) 

Is there an alternative to this module that would call a script with any GET request? The main thing I want to do is log data, so it's not a problem if the content of the renderer request is not given with it. Thanks for your help :)