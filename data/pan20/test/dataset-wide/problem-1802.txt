You can see that it mentions System.StackOverflowException, which is quite rare to occur, so that is a good starting point for investigation. The second information you have is that it started recently. If you do have a version tracking system, try to locate all application updates within the relevant time range and look for any kind of recursion (direct, indirect, ...). StackOverflow is usually caused by recursion in your code that went out of control. If the code is not obvious (i.e. it is not easy to find any such recursion, or there are too many recursions in the code and you can not identify the one that is causing the problem) you should try to put logs there. Place log entries into various places of your code so that you can recognize when certain functions were entered and leaved in the execution flow. Since you have a crash every 15 minutes, you should be able to debug this using logs very quickly, because you will clearly see the problem in the logs once you put logs in the right (buggy) function. 

For local (without entry in global DNS) web, use HOSTS file entry to translate domain1.com and domain2.com to 192.168.1.9. 

With many hosting companies, it is common that they do not manage their IPs reputation. This means that you can buy a new server and get a dedicated IP address which has a bad reputation because the previous customer of the same hosting was sending spam or had an infected machine. Delisting procedure is specific to each blacklist. A good summary is provided in this blog post How to Get Removed from Blacklists. In short, it really depends on the blacklist owner if and when you are going to be removed after you (or your ISP/hosting) request removal. Some blacklists are not even free to remove instantly. With every new IP we obtain with a new server, we always go with a check immediately and setup blacklist monitoring. If there are problems, we try to ask for removals and if it is not possible, we ask for different IP address. This prevents later surprises. Moreover, some email providers (Microsoft's services like Hotmail, Outlook; Yahoo, AOL) are known to be very strict and actually require a perfect reputation history of the sender in order to deliver all the emails. It happens often with these services, that you try to send email to an account there and it is discarded silently. 

More likely than a server vulnerability exploitation, this looks like spoofing source address. One of the methods to deal with this (but not entirely mitigate), is to use SPF records. There are currently no SPF records for proactech.com domain. This means that the target mail servers can not verify whether an incoming message comes from your mail server (legitimate) or some other (not legitimate). If you install SPF records, the target systems (that are sending you bounce messages) that check validity of SPF records (and there are many of them today) will reject any incoming messages from servers that are not allowed by these SPF records and they will not try to deliver such messages. This means no bounces to you. You can also consider installing DKIM, which is another feature that can help you mitigate a part of the problem. I do think SPF is checked more widely than SPF, so the first thing to do is SPF, but if it is possible, also install DKIM, just to make sure you have done the best you could. 

After trying several different machines I have realized that my hosting is OVH and they block outgoing connections to ports 139 and 445 If this is also your case, use IPv6, they do not block it and it works. If you are having problems with new Cloud 2016 VPS - they do not have IPv6 at all, use $URL$ to get IPv6 connectivity. 

We have Ubuntu 14.04 server on KVM with MySQL database which is just close to fill its disk space, which is 80 GB. We have upgraded the server to 160 GB disk, but the problem is that the new disk space is there just unallocated. I have been recommended to run GParted, so I did (please see the attached screenshot) and found out couple of tutorials on how to resize the disk, but when I click on Resize, it does not allow resizing of /dev/vda5, probably because it is outside /dev/vda2 extended partition. 

Have 53 TCP and UDP ports open in your firewall. Have DNS server listening on the right IP address (either on ANY interface or in your case 192.168.1.70) 

From what you wrote, everything seems OK. nslookup is a "low level" program in a sense that it does not use the libraries that are used by other programs in the system in order to do DNS resolution. nslookup creates and sends raw DNS packets on its own and it also receives and analyzes raw DNS replies. If you want to check your settings, use ping command on the hostname. It does not matter if you blocked ICMP or not, ping command does DNS resolution using standard libraries (that use HOSTS file), so you will see in its output, if the resolution was done as you wanted or not. DNS resolution and using HOSTS file is not related to ICMP firewall settings, so do not worry about your inability to ping that server. 

So this is why I guess the real server would also fail here. Note that I have used "u" command twice, because it was already in "sectors" mode. 

If you add DNS server role to your Windows 2012 server (Server Manager -> Local Server -> vpravo menu Manage -> Add roles and features -> Server roles -> DNS Server) and do not configure it at all, it will behave as cache DNS server - i.e. will get all unknown DNS info from the default machine's DNS server (e.g. 8.8.8.8). The only two things you need to care about are: 

I am not sure where I make mistake. I have two almost identical Windows Servers 2012 R2 installations - servers A and B. One server A, I have setup a shared folder. I have verified that I can connect to this share folder from my Windows 10 using UNC path \\IP\Name. I can see files a folders, read them ... However, if I want to do the same from server B, it always fails. When I try "net use \\IP" or "net use \\IP\Name" on server B, where IP is address of A, I get: 

If two services are executed under the same account (whatever its name is) they have the same access to files on the disk. If you want to separate that, you are going to need to create new accounts. This is very common. For example, when I install FileZilla FTP Server on our Windows servers, I create new user for it. Similarly, when I install Apache web server, I create new user for it. Some software support it on its own, but with most third party software, you need to install it normally, then to stop its service and set it to run under the new user account you have created and start it again. This will provide you the separation you are looking for. It might seem that there is a lot of work to be done, but if you take your time and create your groups and their users smartly, it is not that bad. 

I am considering switching to a new hosting provider and I would like to know if it is possible to achieve database (MySQL 5.6) migration without a huge downtime (= time to copy tens of GB of DB files from one hosting to other hosting). So, is it possible to configure the current live MySQL DB as master and the new machine as slave in some mode that the master would not wait with new data inserts or updates for slave to confirm, and the slave would slowly (i.e. would not consume too many resources of master DB machine) try to sync itself, i.e. it will take up to a whole day, or maybe a couple of days (full speed file copy would take cca 4 hours) to fully sync. Meanwhile I would setup the web server and other services on the new machine and then just switch DNS and switch slave to single master mode and disconnect the old machine. I expect (and I am OK with that) to lose some data during the actual DNS switch (some clients with old DNS record would access the old server and these changes would not replicate to the new machine), but for most visitors, this window would be 15 minutes or so. So, is something like this possible and somehow easy to do? Alternative is cca 4 hours of downtime, copy all files to new server and just start it, but I am not very happy with such a long downtime. I do not mind restarting DB service/daemon couple of times during this process in order to switch it to new configuration. I do not want to do this migration using dumps, when I have to resync tables manually. 

You are probably using Ephemeral IP: An ephemeral external IP address is assigned to an instance for as long as the instance is still running. This means that it can change, which is probably what you do not want. For more, see $URL$ 

You can safely disable anything that you do not need. For example, does your server get its IP address and gateway settings from DHCP? If Yes -> You can not disable DHCP. If No -> You can disable DHCP. Do you need your server to response to pings? If no, feel free to disable ICMP rules. Do you use IPv4 and not IPv6 at all? Feel free to disable all IPv6 rules. Similarly proceed with all rules in the list. Core Networking just means it is related to "lower level" protocols such as ICMP, IGMP, DHCP. I also do recommend moving RDP to non-default port. You can see it in the event log of a server with static IP address and RDP on default port, that there are many attempts to hack into the machine using e.g. dictionary attacks. Move it to different port and you will eliminate these. 

I don't understand how to setup a failover for my quite simple scenario. I am building a service gateway for API. What I want to have is two servers hosted in different datacenters. And I simply want the user to be able to access the service even if one of the servers is down. There is no issue with DB sync, I only care about availability of the service. How do I do that while preventing the user to implement any kind of failover logic on their end? I want the user to be given a single domain or a single IP address and be able to access the service all the time using this single end point. What I do not understand is how this can be achieved. I know I can setup a network node that will forward the requests to the first or second server, depending on which of those two is currently online. However, I fail to see how this setup solved HA problem as we just introduced a single point of failure to the system - the forwarding node. So, if this node goes down, the service is unavailable. Could you please explain how to implement this in the real world? Is it possible to achieve this with reasonable cost (i.e. not more than a cost of hosting of the servers themselves). Edit: It has been suggested that different datacenters requirement is costly. So, feel free to provide suggestions for 2 servers within 1 datacenter. Edit 2: Feel free to mention what is a reasonable cost for the that setup. 

UDP does not have connections, they are just packets that on their own has no relation between each other. However, what you can do to simulate network failure is to configure a firewall rule that will enable / disable the traffic over UDP protocol. You can configure which ports will be affected as well as which hosts will be affected, so this is exactly what you want to do. If you are not sure how to add a firewall rule: Run "firewall.cpl" and click Advanced Settings and add your rule. Or visit e.g. this page with step by step instructions on how to add a rule: $URL$