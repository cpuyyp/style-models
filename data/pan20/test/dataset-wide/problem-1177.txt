Using: SQL Server 2008 R2 I am currently stepping through a query execution plan, and have come across an instance of a clustered index update on a table. The issue here is that the columns that are being updated are NOT part of the clustered index. Table: 

Using SQL Server 2008 R2: I'm currently attempting to track down the cause of a deadlock issue we have. Not sure where to turn. Here's the setup. 

Currently running on SQL Server 2008 R2 I am attempting to increase performance of an UPDATE statement. I notice an Eager Spool operation in the showplan popping up. My understanding of spooling operations is pretty basic - they create temporary storage for the table during the update. I also know that, while they are preventing much worse execution times, eager spools are often indicative of underlying problems with table structure and/or query statements. My question is pretty simple: When you see an Eager Spool in your query plan, what problems do you first look to address? I will be analyzing every part of our system to increase performance - I'm just looking for guidance as to where I should start. 

The execution plan shows an Eager Spool at 36% cost, a Clustered Index Update at 55% cost, and an Index Seek on the name index at 9%, among Compute Scalar and Top items at 0% cost. Why is the plan showing a clustered index update? What could I do to prevent this, and prevent the eager spool? 

A multi-threaded application processes messages, then calls one of two stored procedures for every message. If the application has not processed the user, it will call a stored procedure that inserts a row into the table. If the application has processed the user, it will update any or all of , , , based on and does not change. There will be only one row in the table per . The thread that the application processes messages is chosen based on the player_id, so two threads will NOT process messages for the same player at the same time. The INSERT and UPDATE statements are using the directive. Here's the issue: The process is causing deadlocks with thread 1 locking the primary clustered index and thread 2 locking the non-clustered index. I do not understand why the locks are occurring - the UPDATE and INSERT statements are using row locks, and it's been tested and proven that threads are not accessing the same rows. What could I do to fix this issue? Caveats: There must exist two stored procedures, one updating and one inserting. As much as I would like to switch to a single MERGE, that's not gonna happen. : / 

I use DBeaver 3.7.3 x64 portable version with Windows 7 SP1 x64 Ultimate. The error log window does not log the error, so I don't have any more information there. 

One integrity constraint stemming from common sense / knowledge is that the patient more than one year old at the time of hospital admission (i.e. ), shouldn't say that the patient is a neonate. Another integrity constraint from common sense / knowledge is that a patient cannot be admitted in the hospital if he already is in the hospital (i.e. some checks would have to be done around and ). 

The Wikipedia page on data integrity lists 4 types of integrity constraints: entity integrity, referential integrity, domain integrity, and user-defined integrity. Is there a name for integrity constraints that stem from common sense / knowledge? 

I downloaded a SQLite 3 dump of Northwind, and opened it in SQLiteStudio. Accented characters aren't displayed nicely. How to solve the issue? I suspect some encoding issue. 

I am sure the SSH server is working fine. I manage to connect to the same PostgreSQL database server through the same SSH server with pgAdmin 3 and RazorSQL. I have checked the IP, username and password, they are correct. I disabled all firewalls. What could be the issue? My settings: 

I tried DBweaver on Kubuntu and had the same issue. It looks like the issue is around the fact that the authenticity of the SSH server hadn't been verified beforehand, and DBweaver and unable to cascade the correct error message (e.g., ) and instead misleadingly blames some authentication error. I fixed the issue on Kubuntu as follows: 

In MySQL Workbench, one can use the keyboard shortcut Ctrl + ENTER to execute the statement at cursor (delineated with semi colons). Is there a similar shortcut in pgAdmin? 

I wonder whether I can clear the output of a query in pgAdmin III (aside from running another query). 

This made me think that the SQL area on Oracle DB grew too much to the point of maxing out the shared pool's memory. Running the follow query would confirm it, but ironically and unsurprisingly I get the ORA-04031 error on it as well: 

Since there seems to be no way to define such a keyboard shortcut, I filed a feature request: $URL$ , which got rejected: 

In Oracle SQL Developer, is there any way to include a SQL file within a SQL query? Like the command in LaTeX. Example: 

I wonder whether I can edit the content of table in the data output of a query in pgAdmin III? The query is a . MySQL Workbench allows users to edit the content of table in the data output of a query, but I fail to see the same option in pgAdmin III. 

Stop MySQL server; Check the error log see verify if your shutdown was successful. (No errors) Move your log files: mv /path/to/datadir/ib_logfile* /tmp/ Start MySQL server. This will create new logs; When you’re absolutely sure that MySQL server has started successfully, you can remove the old log files: rm /tmp/ib_logfile* 

The setup can differ slightly in size and be consistent at the same time. Could be a range of reasons for the size difference. E.g. Different file system, fragmentation, method of determining size, etc... If you want to test the consistency of your replicated tables, run a pt-table-checksum. 

@Decebal, Please consider the following, as the current answer is incomplete, flawed and built on an assumption. First I will attempt to explain my previous statement. 1.) Incomplete (cardinality, prefixing and data types): Composite indexes and indexes in general need to remain lean to fit in memory, actually benefit performance and not over complicate nor confuse things. Therefor the only complete answer would be to test cardinality on your table in order to utilize an efficient index prefix length. The next step of completeness is to slim down your data types. It matters for sorting and other explicit/implicit MEMORY engine usage. 2.) Flawed (null-able UNIQUE composite indexes): The `contact_info_ndx` UNIQUE composite index in Rolando's answer contains null-able fields. This will effectively enforce a partial UNIQUE constraint. Any composite containing a NULL in one of the three fields will allow for duplicates, a NULL effectively breaks the constraint. MySQL doesn't know what a NULL is and therefor cannot judge it. 3.) Assumption (Necessity of UNIQUE constraints): This is quite an impacting assumption as a UNIQUE constraint should be avoided unless absolutely necessary. It add's overhead, circumvents an important performance enhancing mechanism (i.e. change buffer) and increases the probability of dead-locks through gap locks on the UNIQUE key when inserting in batches. With that said, here are the steps that I would advise: Step 1: Data type(s). (Not sure if this step is necessary as you mentioned that the table in your question is "similar" to that of your actual User table.) 

You can limit the relationship from many to one by adding a unique constraint. EDIT: I am guessing I received the -1 for my brevity. Sorry for that, I'll elaborate to clarify. Below you will find the one-to-one relationship DDL. I used a UNIQUE constraint on table B to assure it's one-to-one relationship to it's parent table A's primary key. 

The microsecond fractional part of your DDL (I.e. The (6)) is only supported from MySQL 5.6 onward. Most likely a compatibility issue I'm afraid. 

Step 2: Cardinality and left most prefix. On each field you will be installing an index, I would advise you to test how much of that field needs to be in the index for it to be effective and slim at the same time. I wrote a small statement to do just that. Replace the with your table name and the with your field name. i.e. for the field on the table :