Apt usually uses good old port 80, but you should check apt.conf to see what it is set as, since it can use any of several. Also check /etc/apt/sources.list 

I'm pretty sure this isn't the problem. Nothing has changed on the proxy, and this configuration has worked for years. 

MySQL replication is A->B not A<->B, so make sure that you're updating the master database(A), which is the one that replicates down to the slave database(B). If you're not, then that's probably your problem. 

I'm pretty sure you can't actually change the version announcement. The best ways to secure sshd are: 

Well, the 4948 is thin, so I'd be comfortable racking it with only the ears, but I don't think I'd be willing to transport it, in the rack, only secured by the ears. They can fail, and that's a lot of money down the tubes. 

I tend to add the gateway to the /etc/sysconfig/network-scripts/ifcfg-ethX config file for the appropriate interface, eg: 

I agree with the above definitions, though I'd add that NAT and Proxies have a very different purpose. NAT is simply routing: there is no caching, and there is no real "oversight" for lack of a better word. Proxies are put in place for caching, monitoring, traffic shaping. They are very much about control. So NAT has very little overhead, but doesn't offer many options, while proxies have a much higher overhead (sometimes massively higher), but allow a lot of control. 

Even compatible hardware may not help if you don't have a backup of how the RAID was configured. RAID configuration is tricky stuff; nothing auto-detects, and every manufacturer has their own optimizations of the process which means "3 disks in a RAID5 configuration" isn't as descriptive as you might think. My experience says, "Go to backup." Ancient card, unknown configuration...If it's critical data, there are people who recover busted RAIDs for a living. If it's not totally critical, there is RAID recovery software out there that might work, but I've never tried it so YMMV. 

Well, yes and no. The Dyndns address is an external address: that's the whole point. It points to your external interface, the one out there on teh interwebs, not your internal one on your nice safe lan. So if something from your internal network is pointed at that address, it goes out, turns around, and comes back in. That's the only way it knows to get there, the only valid route attached to the url. The quick and dirty fix is to put an entry into your HOSTS file pointing the dyndns url to the local ip address of that machine. You'll have to do it for every local machine, and it'll absolutely break everything every time you leave the apartment, so you'll have to keep changing it. The more complex fix is to set up your own local DNS, and override the record locally. Then, if you connect via DHCP, and let the DNS be automatically assigned, you'll always have the correct address. 

I wouldn't be scared of building from source: it's fun and rewarding. The only big problem you'll have is the same problem you're already having: dependencies. To get around the dependency problem, you'll need a package manager. Hmmmm. You can install Yum, which is what'd you'd get on Fedora instead of up2date...It's pretty good at handling dependencies, and googling Yum and RHEL and Repository gives a good number of hits, so there are repositories out there which will have RPMs built for your system. If it were me, I'd probably go ahead and upgrade KDE and Gnome, if it was that important to install the newer version of samba (that's a lie actually. I love the command line, so I'd just go ahead and break kde and gnome and not look back). Resolving this sort of dependency problem is what it's all about. 

Well, you can check to see if it's being "niced", but it sounds more like the process is bogging down due to memory or IO issues. Can you post the memory usage? 

You might try a third party application like Windirstat which will index your harddrive and give you a nice graphical representation of where all the space is allocated. 

December is a nightmare for Dell, because of all the xmas desktop business. Perfectly normal. I'd still call and scream at them, but that's just because I like calling and screaming. 

How much RAM are you using? SOLR has a known issue dealing with FieldCache which can cause it to eat huge amounts of memory while doing sorted searches. Edit: Okay, try using pmap -d on your big three processes (httpd/php, MySQL, Jetty) and see if some sub-library is eating up all your memory. 

Eh. If your server is IP locked, and your user is restricted to SELECT on a set of tables where you don't care about the information, it's not a huge deal. On the other hand, I set my MySQL passwords by whacking on the keyboard for a minute, and copy pasting the resulting gibberish into a protected file, which I reference in my code whenever I need to log in. This is how it should work. Why make it easy? If the password is attached to a limited local account (as they all should be), then why are you typing it in? If it's not, it should have a password whose strength is relative to the value of the data you're protecting. 

Sure. You're talking about clustering. You're going to be a bit limited with Windows XP, unfortunately. It's not really made for that sort of thing (whenever I cluster with XP, I serve XP as a terminal session to a linux desktop, it allows a lot more flexibility.) Still, there is software out there. Check out BOINC, it's got a lot of potential for what you're talking about, and it runs on everything. (Did I mention it's free? No? It's free.) 

Add a group (), and then add yourself and the apache user to that group (), then chown those files and directories to the group (). Make sure the permissions allow group to rwx (e.g. , instead of ) Now they're your files, and yet it won't break your web services. 

What I thought it was (and wicked tired so YMMV) was that Apache's virtual host directive was choking on *:80 with a ServerName declared; I had trouble with that once. Try putting www.something.com:80 in your VirtualHost line, and see if that works. 

I agree with Chris S. You're too exploited. You need to wipe and restore from backup. And this time, before you go live, you need to be extremely careful with your write and execute permissions. Once an attacker has obtained system-level access, you can't trust your code anymore. Directory permissions are HUGE. This cannot be stressed enough. They uploaded code to your site via an exploit, but that can only be done if your code can write to the local directories. If it can't, or if the local directories that it CAN write to can't be interpreted or used to host executable code, then the damage that can be done is extremely limited. I recommend removing write permissions everywhere you can, ALL OF THEM, even the ones belonging to root. The only things that should be writable anyway are upload directories and whatever directory stores your session files. If you don't allow uploads, then only the session file directory, and that one should be as locked down as you can make it. You should also run regular file integrity scans. Unfortunately, that's not as easy if you don't have complete access to the server. Still you can download the site and compare it to your backup on a regular basis. Ideally, you should be able to overwrite the entire site from backup at any time, and have no one notice the difference. 

It's probably nothing to do with the sizes of the files. If you're running wordpress your backlogs are going to be database and processor. Database for recovering all the information about what the page is supposed to look like, the content, etc. And processor for compiling all that stuff into an actual document and sending it out. Might want to look at tweaking Apache's cache settings. If certain pages are getting requested often, there is no point in building them over and over. 

The easiest way to test speed is to simply copy a file from one machine to another on the lan. What are you using the windows server for? If you're all using it as a file server, I'd say no problem. If it's running some intensive terminal services stuff, might be an issue, but more likely a resource issue on the server, than on your lan. 10 people on a lan shouldn't saturate anything, even at a mere 100Mb. You'd have to be doing something really intense. 

If the virtual host directive isn't unique (or a wildcard), then the config just defaults to the first match, which is why it's not going past the first one, since the IP is overriding the ServerName. 

Even if you have some sort of absurd application that needs to do trillions of writes, you'd probably be better off just having a number of unconnected servers available through some sort of round-robin interface, and just combining the data later in a more controlled environment...There is virtually no way to be sure of replication when you're getting hammered like that. In the end though, most database applications don't perform many writes in comparison to the reads. 

The problem is always going to be the extra IP address/Mac Address pair. That's what they're going to be looking for: even if they don't have DHCP completely locked down, they're bound to be scanning for "new" MAC addresses. You can't secure your physical network if you don't watch out for unfamiliar MACs. To get around it, either you have to route the extra traffic through your machine, using some kind of proxy (which is only possible if you can install software), or you're going to have to slip a switch in between your machine and the network and then NAT all the traffic straight through to your desktop (chances are they have RPD or similar set up so they can admin your machine remotely), and you're going to have to configure the switch to have your machines IP address, and your MAC address, and you may not be able to determine these depending on how locked down your machine is. Now, both of these solutions are going to be completely obvious to anyone who bothers to leave the coffee pot and walk around. If you're lucky, the guy is going to think the rule prohibiting you from connecting your virus-laden tablet PC to the company network is outdated and turn a blind eye. If you're unlucky, they'll disconnect it with a hammer (I have actually done this to an open WAP that someone plugged into my network...Watching them try to complain about it was delicious). Your best policy is to go to your boss and sell them on your peripheral. Then have them go to the guy who is over both your department and IT, and sell them, and then have them dump it Wrath-of-God style on the IT people, who will then cite chapter and verse on why allowing your device onto the network will end civilization as we know it. Depending on which boss sells it better, it will/won't get done, and, either way, eventually, the admins may take you off of their shit list. What I would do, wise as I am in the ways of networks, is ask for an external WAP. If you pitch one that is outside the internal, protected network, you can probably sell it because of all the execs who have Crackberries and iPhones, who will appreciate the wifi. And it's the sort of thing that doesn't cost very much, and soothes a lot of employees, that IT bosses like to do.