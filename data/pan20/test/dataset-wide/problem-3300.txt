the "where" moves out of the way between "to" and "be" because the question word has to be at the beginning of the embedded question, 

I don't have a firm grasp of the distinction you're asking about, but I can give you an outline of my understanding of McCawley's account in The Syntactic Phenomena of English. It is based on a relation between the structure of NPs and the structure of sentences, as we can observe it in the process of nominalization. N' (N-bar) in a NP corresponds to V' (V-bar) in nominalization. An N' is either built from another N', or consists of a N with possible complements, as a V' is either built from another V', or consists of a V with possible complements. One common V complement that we're familiar with is the direct object. The corresponding argument in a NP is the "of"-complement of a N. Occasionally, we can find a direct correspondence between sentential and nominal: 

If you're just talking about "tendencies", how can one ever tell whether you're right or wrong? It doesn't seem to me that this can lead to specific predictions about the stress of English words. I'm not familiar with your reference Vassiliev. I am familiar with the theory in The Sound Pattern of English (SPE), which proposes a "retentive" theory, that is, a theory in which the stress of complex derivative words is inherited from the stress of the simpler forms that they are derived from. I'll give a couple of references to contrary proposals about English word stress, according to which the stress of complex forms is not based on the stress of the basic forms. John Ross gave one such non-cyclic theory in his article "A Reanalysis of Stress within the English Word". It's a long article, so I'll just give an anecdote. In SPE, Chomsky and Halle give what I guess they think is a killer argument for their cyclic treatment. And it is a good argument, at first sight. Compare the stress patterns of "compensation" and "condensation". The first is c2ompens1ation, with no stress on the second syllable, but the second does have stress on its second syllable: c2ond3ens1ation. Yet the canonical C/V forms of these two words are very similar. What could explain the difference? Have you figured it out? The difference is that the second word is derived from a verb cond1ense with stress on the second syllable, while there is no such verb as *"compense". The SPE proposal is that the derivative form "condensation" retains stress on the second syllable from the basic verb form "condense", but that "compensation" has no stress on its second syllable, because there is nothing to retain. But Ross notes that not all English speakers agree to the facts, here, and points out some other pairs of forms that work the other way around -- stress turns up in the derived form just when there was no stress originally in the basic form. There are "anti-retentive" forms, that is. I also wrote a paper criticizing this part of the SPE theory and making a counter proposal. That's English Word and Phrase Stress in Goyvaertz, ed., Essays on the Sound Pattern of English. For one thing, I argued that the SPE cyclic theory leads to missing generalizations about what sort of "weak" syllables can remain without stress, whether they occur after the last stressed syllable of a word or between stressed syllables. 

The problematic example is difficult to interpret because it assumes with no obvious grounds that no one owns more than one donkey. Taking into account that some may, actually, own several donkeys, it should have been: 

I think the answer is yes, certain phonological rules are context free, in the sense that they can be described for any context free language. I'll just consider a hypothetical case in which the phoneme n changes to the phoneme m before the phoneme p. The general idea of this answer is to adapt techniques used in GPSG (Generalized Phrase Structure Grammar), in which theory syntactic transformations are avoided by, essentially, applying them to a CFG (Context Free Phrase Structure Grammar) rather than to language expressions that are generated by a CFG. Here is a very small scale example to illustrate. I use lower case letters standing for phonemes and terminal symbols of a CFG, but to make the examples easier to read, I use ordinary English spelling. Please imagine that the spellings are actually phonemic forms. 

I take that to mean that morphology, grammar inside the word, and syntax, grammar outside the word, share the same concerns, methods, and assumptions, with the only difference being that the first deals with how morphemes go together to make words, and the second deals with how words go together to make phrases. I don't think there is any truth to this at all. Language morphology has a primitive system and concerns mostly memorized facts, while syntax has a sophisticated system and concerns mostly innovation. The person who you quote probably knows very little about syntax. 

Trubetzkoy's archiphoneme theory had no reference to underspecified features, since it was proposed in a day before feature theory existed. The interpretation of archiphonemes as being underspecified segments is ahistorical. Most of Trubetzkoy's case studies of archiphonemes can reasonably be interpreted as involving underspecified segments, but there is no theoretical guarantee that they all can. Maybe FG people want to make clear that their theory is more constrained than Trubetzkoy's (which I think would be correct). Or maybe they are just being fastidious and do not want to attribute to Trubetzkoy something he didn't actually propose. 

For one thing, modern grammar has been strongly influenced by the study of logic and the foundations of mathematics. In the early 20th century, paradoxes were discovered, for which the most popular resolution was to prevent the paradoxical propositions from being expressed in a rigorous, logical language. To resolve the paradoxes in this way requires an explicit and restrictive grammar to be devised which prohibits the expression of the troublesome paradoxes. In logic and mathematics, linguistic interest is focused on theorems and other statements corresponding most closely in natural language, probably, to simple declarative statements. For this type of language expression, linguists generally use "S". It corresponds to "well formed formula", or wff, in some presentations of logic. There is not a great deal of uniformity among logicians, but sometimes this part of logic is called "morphology", rather than the linguists' term "grammar". So that is one place "S" comes from. A related usage is in applications of CFG, context free phrase structure grammar, where by general convention, when a grammar has a single initial non-terminal symbol, one calls it "S". It's just a shorthand to avoid tedious explanation. If the intent of a CFG or other form of generative grammar is to generate strings for language expressions other than simple declaratives, such as questions, imperatives, or other expression types recognized in traditional grammar, then the sense of "S" is usually extended to those other types. Then "S" means, in effect, complete expression generated by a grammar. In CFG, the names of the non-terminal symbols, including "S", are of no intrinsic significance. This probably extends to transformational grammar and other theories, as well. If for stylistic reasons, you prefer other names for initial symbols, you should feel free to use them. What's in a name? 

I think it's very clear that a single intervocalic consonant before an unstressed vowel goes in the preceding syllable. The earliest reference I know of is a paper in Language by James Hoard in 1971. The title was "Aspiration, Tenseness, and Syllabication in English". That they go in the preceding syllable is David Stampe's view in his paper "Divinity Fudge" (CLS) and my and Irwin Howard's view in "Another Mouthful of Divinity Fudge" (CLS). A sort of half-way version of this idea became popular in the 70s on the East Coast, which was that such consonants are ambisyllabic. Why place them in the second syllable as well as the first? I was never able to discern any reasoning behind this at all. I guess, something like "Well, everyone thinks they go in the second syllable, and now I find evidence they go in the first, so, duh, they must be in both syllables." The reason those consonants so often get put in the second syllable in dictionaries and elsewhere is apparently the way people say things when they sound them out, syllable by syllable. "City" is "ci-" plus "ty". But when you say the syllables one by one, you stress all the vowels, and of course that changes the syllabification, so that now, those consonants go with the following syllable. The evidence that such consonants go in the previous syllable has to do with the preference of various lenitory processes for applying to syllable offset consonants. Flapping is the most obvious case, where you find intervocalic t/d/n flapping at the end of a word regardless of whether the following vowel is stressed, but within a word, flapping only before a stressless vowel. But lots of stuff works that way. Syllable offset r in English loses its roundness in "very", e.g. Syllable offset "y" is palatal, but syllable onset "y" is palato-alveolar. In casual speech, intervocalic syllable offset k/g can lenite to fricatives, while syllable onset k/g are immune. I know you said you're not interested in spelling, but it's amusing that Donald Knuth discussing hyphenation (in the TeXBook, I think) remarks that the verb "re-cord" hyphenates differently from the noun "rec-ord". 

Syntacticians generally develop an idea about the tree structure of an expression by using tests. For your examples, one thing to investigate is which sequences of words could be replaced by definite pronouns (he/she/it/they/...) in a context which would supply interpretations for the pronouns. Those are NPs. For your example i), I get "They walked across it", which gives a first approximation of a tree: 

If "feminist" and "feminism" are different forms of the same word, one might expect the following to be interpretable and acceptable English: 

No. But this sort of mentalism is more empty and useless than it is wrong. Suppose I try to explain the meaning of your question by saying it consists of the concept of "when", combined with the concept of "we", combined with the concept of "perceive", and so on, no one will be the wiser about what you've said. Such an account is useless. As to whether it's correct, well, who knows? Who cares? If you give such an account supplemented with an explicit version of "combine with", so that you can tell exactly when concepts can combine to give a sensible result, you get the sort of "semantic" theory called Montague Grammar, or Montague Semantics (after Richard Montague who pioneered such theories), and then opinions will differ about whether you're accomplishing anything beyond giving a syntax. I doubt it. 

A derivation tree is a branching diagram built up by joining subtrees each of whose mother nodes is a cfpsr derived by the replacement rule and whose two daughter nodes are the two cfpsrs used to do the replacement. (4) It's pithy. Although I used -bar names above, since there are no non-bar primitive syntactic types to distinguish them from, it's not actually necessary to use the bars. (5) There are no primitive transformations (other than the replacement rule), but derived transformations can be introduced as relations on the cfpsrs, following the method used in GPSG.