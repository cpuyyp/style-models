I am planning to host a website in Amazon Ec2 instant. I got some basics from the docs. But I have few confusions. If am using cloudfront for CDN, do i really need to server files from s3? The site am hosting is build on Drupal. Does it have any performance difference? 

As of now EBS is the most reliable option you have on AWS. It's not only better, its also easy to take snapshots and re-attach to another instance. We have very large Ec2 instances which are EBS based that are used by media site and have not had any issue like this. Apart from that Amazon guarantees 99.95% uptime for this service and will give a refund incase it goes down. To answer your questions. 

Its quite unlikely, unless your instance health is bad for some reason. But similar issues, with bad IOPS has been reported in past. See Check the "read write IO" vs "pending IO in the queue". So if you have 0 IO when there is pending IO in queue you have a problem. See SLA. Check health of an instance + the health of EBS IOPs. You can use cloudwatch for this to some extend. Check this link. 

I have just installed WS 7 on our stage server. The test server was done some months ago. I have deployed the same applications to stage as on test. One application is causing me problems. I call the login servlet but the result is an 'Internal Server Error' message. From my logs I can see that the servlet processed the log in successfully and redirects to a JSP. That is the last message. I can find no further info in any of the other standard out/err logs. I have tried creating a simple JSP containing only HTML - same error. If I rename the file to test.html, it displays correctly. The plugin configuration appears to be identical with the one on the test server (where everything works) Ideas anyone? 

I am currently investigating an instability problem in my customer's web site. While looking through the access log I noticed a sudden burst of activity from one particular IP. It started off requesting normal URLs but at a high rate - 8 hits/sec. For most of the time the same URL (actually a directory) was requested but interspersed with these were URLs which started off as valid but always ended with a random 11-character value like this: 

What is the best way to install a lampp setup in ubuntu? I know to install using xampp. Is it ok for using as a central development server? Or if am installing individually what are the packages i have to install? Is the below packages enough? 

I don't exactly get what this is, but fairly understand that the user have to hit apache for this. Can there be a better solution? I might be thinking really foolish here, but want to explore the possibilities. 

You can easily map a port in you docker container to host machines port. For example say your host machines ip is , you can make your api in docker machine available in that ip by mapping the api's port to port of the host machine. Now if you are already have something hosted in the host machine, then you can map it to some other port, say . By mapping the docker images port to port on the host, your api will be accessible at , but if you map it to a different port like , your api will be accessible at This can be done using the simple command below: 

If the CPU does not slow down the data input (because deflating takes more time than reading) then extracting is faster than copying. If you instead copy from a SSD to another device and your CPU is from stone age then copying will be faster. 

You have to put both the respective users and the php call into sudoers. It would be risky to allow php calls globally. Instead just the call to this specific script (which really should not be writable by users, the same for all parent directories). 

The first disk probably needs a partition table but shouldn't need . Even the MBR code should be capable of accessing the second disk. You should erase the content of on the first disk, mount on the second disk and run again. If that doesn't work, post the output of . Maybe you mixed up the two ? Also check the content (the disk numbers) of . 

We're deploying a simple newsletter webapp on a stand-alone LAMP platform in the company DMZ. There is some discussion as to whether the MySQL server should be removed from the DMZ and put in the internal network. The server is behind a firewall with only port 80 open and MySql will be attached to a non-standard port. The database contains customer email addresses. Is this a secure setup (or secure enough)? How much more secure would it be by placing the data behind a second firewall? (I'm more of a developer so I'm not really aware of all the security aspects here - can someone enlighten me!) Update Just for clarification and to attact more comment here is our current setup: internet - firewall1 - http server - firewall2 - appserver - firewall3 - enterprise resources This new application was supposed to go completely within the DMZ between firewalls 1 and 2. We're currently discussing pulling the MySQL server in behind the 2nd firewall. 

Add two lines to . The numbers don't matter; this is just for using names instead. Let's call the tables isp_100 and isp_200. Define the routing tables: 

iptables is the right tool. You can create rules without a target. They just count packets then. And you can reset the counter of this rule (or at least of a chain). You just have to decide whether you want to count new connections or really all packets and place the rule accordingly (usually an ACCEPT rule for packets with status ESTABLISHED is at the beginning of the rule set). You need the switch to see the packet and byte counters: 

So check on B for forwarding and on C. Edit 1 The firewall on B can be configured to let those packets through by e.g. 

BTW: There is no need to give the destination port for if it is not changed. Edit 1: You can do that if you do without allowing the connections explicitly, if you are fine with allowing everything that has been DNATted. In that case you would stick to your 

Our IT department has around 90 people. We manage systems and create applications. The systems are of varying size from SAP to Lotus Notes to Tivoli Access Manager (TAM). Increasingly, the applications we create connect to these systems via web services, LDAP queries etc. As time goes by it is easy to forget what connects where so, that when changes are made, service connections can become disrupted. Example: The TAM system contains an LDAP with user information including which applications they are authorised to view. An unrelated application fetches this user information to build a dynamic web-based menu containing personalised links. The TAM crew are told to tighten their security and so close all the ports. They have long forgotten about the mini web app that builds its menu every night. The next morning the production boss is in a rage because his people cannot start their applications because the menu is half empty. Question What kind of systems could be introduced to help remind all concerned which services are in use?