The approach we standardized on was to put up a username/password logon web page (in front of the application) which accepts the credentials. When the credentials are submitted, the application would in turn validate those credentials against the directory and then respond accordingly (in a .NET world you could use Forms Authentication $URL$ to force access to the application via this login page). Since the application is doing the credential validation, you get rich information as to the nature of the login failure. In addition, even if the login succeeds but there's relevant information to display to the user, e.g. their password will expire shortly, etc., this provides an opportunity to do so. UPDATE: I forgot to mention that if you adopt this approach, you'll need to allow anonymous access to the IIS application root. This will allow access to the login web page without first attempting the automatic NTLM authentication. It's up to you whether you leave NTLM authentication enabled; perhaps you do want some clients to still automatically log in. 

I have a Cisco RV110W small office router (this configuration process is common to many Linksys/Cisco routers) and I am trying to define QuickVPN clients. When add a client of type "QuickVPN" the router gives me the following warning: (You can find a larger version of the screenshot here 

Too broad question. I won't speculate on minimal specs, but if you are going to serve large files to two dozen customers, then SATA+RAID5/6 seems like the way to go performance-wise. NAS vs custom built Linux box. Two factors here: 1) Availability of admins experienced with Linux whom you trust enough to believe if they say "Yes, we can do this". 2) HW support of the parts you order. Still, if you decide to build your own system, then you may be able to have 3 boxes for the price of 1 NAS system (according to your own calculations), so you could have 2 in production and one idle, ready to be cannibalized in case of any failure. NAS boxes give you: External support -- somebody else to blame if things go pear-shaped ;). Usually more user-friendly interface (click here to setup your box) and, usually, ability to replace failed disk by a trained monkey. They also have a limited feature set, which you cannot realistically expect to be changed on your request. Custom-made Linux system will be much more flexible, give more bang per buck, no artificial limits of feature lists and ease of expansion (throw more HDDs into it). OTOH they will require somebody, who knows what he is doing, to handle them. Also, if your main board/disk controller dies terribly, you can just move the HDDs into any Linux box and have your data available. This may not be the case with a NAS box if it uses a HW RAID solution. As far as OS-X support goes I think you should be happy with NFS, but last time I was configuring a file server for Macs it was XXth century outside and the boxes talked AppleTalk. 

Making backups is really a game of probability. Assuming the data is successfully written to any media (as confirmed by the backup program's "verify backup" function), the weak link becomes the shelf life/survivability of the media. Backup tapes can break and be demagnetized. Hard disks can crash. Optical media (like DVDs and Bluray disks) degrade over time. I view the question of "Is it safe to back up to media X?" less of a yes or no question and more one of your goals and retention requirements. If you're looking at a one-time/one-off/adhoc backup that you plan to use in the short term for recovery, then it's less an issue of reliability and more a question of convenience. Assuming that you're looking at a corporate server backup solution (e.g. ongoing backups, some media rotation schedule and some retention period requirement for each backup), it's still less of an issue of reliability (since you'll assumingly have at least a daily backup) and more one of convenience. So assuming your backup process is rigorous (done according to a schedule and verified for errors) and frequent, I see no issue taking advantage of the larger capacity of Blu-ray disks. Under no circumstances though would I rely on any optical media for long term storage. For long term, tape will be most reliable. To really reduce risk of long term backup storage and avoid restore failures I think it's important to have multiple backups stored in different locations. 

I would be very surprised if your switch can support this. You might have more luck giving both network interfaces the same MAC address. Saying that, I definitely agree with Holocryptic, here be dragons. 

However that won't survive a network restart or a reboot. I could put it in the rc.local but that doesn't survive an ifdown && ifup. The way I've found which seems to work is: Add an entry to /etc/ethers along the lines of: 

By disabling encryption and watching the client in wireshark, I check actually what queries are being run and it is searching for auto.opt.$OSNAME.$OSREL.$ARCH without expanding the variables. I've tried running automount -fd -D OSNAME=Linux and this had no effect. I've also tried putting all of this in flat files (not LDAP or nisplus but this didn't help). What do work are mounts along the lines of: 

Do you want it only to happen at system start up? If not you could maybe do something with automount, along the lines of an executable map which checks to see if the USB device is available and if so makes the dependent locations available for mounting. Something like: auto.magic: 

I'm configuring a Red Hat Linux server which will be sending UDP packets out but never receiving ARP responses. So a static ARP entry is required. The obvious way to do this is. 

While a virtual machine guest theoretically doesn't "know" anything about it's host (it generally doesn't even know it's even a VM), you can get information about the host by simply treating the host as just another machine on the network, from the point of view of the guest. This assumes that the guest can see the host on the network. Once you have network access (and proper credentials) to the host you have a number of options to gather information: 

You can create a text file containing the computer names you want to scan (1 name per line) and then run the MBSA command line program as follows: 

where computernames.txt is your text file and and "..." are the other command line options you wish to use. There's a great reference at $URL$ 

We faced a very similar problem. We eventually concluded that while integrated NTLM logon support in Internet Explorer and Firefox is convenient, there are so many exception cases which result in failure that we changed our approach. The problem with integrated authentication is that it works only when the currently logged on username and password are still correct and properly authorized to access the resource. There are more circumstances where it doesn't work however: 

Apache is running multiple processes to have them ready when a client request comes in. Spawning a server process is slow, so it's best to have one waiting for a client. For memory usage, you should take into the account RES size (as displayed by top), which is the amount of physical memory used by the task. Why do you think you have too many apache2 processes using too much memory? What do you expect? Why do you expect so? 

The backup solution will depend on RPO and RTO goals for the organisation, available bandwidth, hardware and backup windows. One large VMware environment I know relies on scripted VMware backup solution for OS recovery (full OS disk image snapshoted every week) plus in-OS agents for backing up data. Having dedicated boot disks in the machines helps with this approach. I wouldn't recommend backing up ESX servers themselves, it's much easier to prepare a scripted install CD and recover the server by non-interactive re-install. The interesting part are virtual machines. Because you have so wildly heterogeneous environment I see two approaches: 1) Continue doing whatever you do, treating the machines as if they were separate physical boxes. This may be an administrative pain in the nether regions because of n separate backup and restore procedures, but if it worked so far, it worked, right? 2) Standardise on one product, that can handle all the environments. I'm hearing a lot good about TSM, but I may be not fully objective here, because my employer sells and supports TSM. 

I then have UNDERSCORETODOT="1" in /etc/sysconfig/autofs and automount will happily pull everything out of NISPLUS. LDAP/RHEL5 (almost works) I tried to do the same in LDAP, the home and workgrp shares work fine but the /opt, not so much: 

(in LDAP or flat files) This would be fine except for when the same library or app works on multiple platforms, for example we don't particularly need a 64bit version of vim and I don't what to have a copy of vim-7.3 for every kernel as the RHEL4 one works on everything from 4u2 to 6u0! man 5 autofs suggests that substitution should work in both the key and locations part of an automount map so I'm not imagining this ( $URL$ ). Anyone got any ideas? Is autofs just broken on RHEL5? 

lookup(program) means automount thinks your map is an executable, does it have the executable bit set? lookup(file) is what you should be seeing See $URL$ 

I'm migrating from NISPLUS to LDAP (openldap) and from RHEL4 to RHEL5. The LDAP server is running on RHEL5u4 as are the clients. On RHEL5 variable expansion in auto.master never works, whether in files or LDAP. NISPLUS/RHEL4 (works) I used to have a whole load of tables in nisplus along the lines of auto_opt_Linux_2-6-18_x86_64 containing shares of programs for that particular arch/os combo. EG. in in NISPLUS: 

NAT definition rules; routing rules (if some packets leave via eth0, and some via eth1 things may get strange); 

If your servers all the same hardware, then the reason must lie within software. Older version of CentOS means older kernel and older LAMP stack (unless you compile your own). CentOS 4.8 runs kernel 2.6.9. CentOS 5.5 is 2.6.18. 2nd factor would be tuning. If you fine-tuned 5.5s, but run an out-of-the-box 4.8, this will factor in as well. Developers strive to improve speed both of kernel and applications, so yes, OS version may play important role in the speed delta. 30% improvement on identical hardware? Not impossible. OTOH if your servers don't run on identical or very similar hardware, or if the 4.8 one is the one connected to the rest of the net by that ancient over-heating half-duplex 10 Mb hub locked in the broom shed, then hardware may be the dominating factor explaining the speed difference. CentOS 5.5 on quad Nehalem with 32 GB RAM should be significantly faster than CentOS 4.8 on a P IV. 

Expanding on Raphnik's comment, you can save yourself a pipe through grep by puting into the regex part of locate or find commands: -- it will match files with .avi and .flv extensions.