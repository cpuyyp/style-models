Your friend is to check/script out permisssions. Below is the script that I am using when doing a refresh of PROD on DEV or UAT server. I will script out before hand all the permissions and after the restore I will just run the script. 

KB 3034297 - Cumulative Update 6 for SQL Server 2014 describes the issue that you are encountering and is fixed in SQL Server 2014 CU6 

The above will indicate that your maintenance especially, backups will always happen on PRIMARY replica. Since we are talking about maintenance, its worth mentioning the Recommendations for Index Maintenance with AlwaysOn Availability Groups 

When you do Export data from the database (using the wizard - Import\Export) select -- write a query to specify data transfer. 

This normally occurs when the log-shipping is not removed cleanly. To remove the orphan entries you have to delete them from msdb logshipping tables. 

Above all are few areas to look into, but it will be difficult for you to corelate and find a pattern. Best is to scan your web-server IIS error logs using tools like LogParser. Additional References : 

Subscriber can have different set of objects than publisher. Its perfectly fine to have a stored procedure on subscriber ONLY. @Tara is correct, it wont drop the proc at the subscriber when you reinitialize the publication. The only time that I think you would have to remember to recreate the SP is when you have to Initialize a Transactional Subscription from a Backup of publisher . 

Do not run profiler while grabbing execution plans (very expensive operation !), instead run a server side trace (modify as per your need) or sp_whoisactive and grab the tsql you are interested and then run the tsql separately using Plan explorer to get actual execution plan. The best way to get all the relevant data is to run the query from sql sentry's plan explorer (its free - but worth spending a little money on Pro version). The free version is sufficient as it gets you the Table IO (same like stats IO ON) along with Plan diagram, statements tree, plan tree and top operations, etc 

The algorithm for a REBUILD vs REORG is different. A REORG will NOT allocate new extents as opposed to a REBUILD. A REORG will work with currently allocated pages (allocates one 8Kb random page so that it can move the pages around) and moves them around and then deallocate the pages if needed. From my SQLSkills internals (formerly IE0) notes .... For REBUILD : 

Instead of writing your own solution, I would highly recommend to use Ola Hallengren's SQL Server Backup Solution. Refer to : Bad habits to kick : avoiding the schema prefix by Aaron Bertrand. Below should work for you (I have not tested it): 

If you only want to allow UserA to execute SP's which are owned by SchemaB then below statement will do the job: 

Recently, we upgrade from SQL server 2012 to SQL server 2014 and got hit by the new Cardinality estimator short coming - queries were timing out, cpu pegging close to 100%. After much troubleshooting, updating stats, rebuilding indexes, doing query plan analysis, we figured out that changing compatibility level to sql 2012 works well. Paul White explains - Cardinality Estimation for Multiple Predicates 

Which is most robust? - Test it out what suits your requirement. Do not drop the entire db or table - what is the point to drop and recreate .. when you are reusing it anyway. Recently I had a requirement to import 3+ GB csv files into sql server. Below is what I did to achieve 100K rows/sec (YMMV due to hardware and environment difference) on a physical machine (was a little less on a VM ) using SSDs and 16 cores and 256GB RAM. 

Navigate to Start -> All Programs -> Microsoft SQL Server 2012 -> Configuration Tools -> Reporting Services Configuration Manager. In the RS Configuration Connection dialog, make sure that your local report server instance (for example SQLExpess) is selected and click Connect. 

Just to add that you don't have to drop and re-create the replication just to change the "not for replication" bit. You can do it using T-SQL without generating a snapshot or breaking your replication -- sys.sp_identitycolumnforreplication 1 = not for replication 0 = for replication and this causes the problems with Identity colums on subscriber side To change it for all the tables : 

To balance CPU and I/O throughput to achieve good performance and maximize hardware utilization, SQL Server includes 2 asynchronous I/O mechanisms - sequential read ahead and random prefetching When SQL Server has to scan a large table, the storage engine will initiate the read ahead mechanism to ensure that pages are in memory and they are ready to scan before they are needed by the Query Optimizer. If you are using Enterprise edition, then there is a mechanism called advanced scanning or merry-go-round scanning that will allow multiple tasks to share full table scans. This advance scanning avoids users to compete for buffer space and taxing other resources on the server. 

When you use synchronous mode, it will allow you for automatic failover. In my testing, asynchronous mode (without automatic failover - manual failover) has proven more benefits in terms of performance. SO depending on the data loss allowed by your company and the downtime, you can choose between them. AlwaysON allows you to Configure Flexible Failover Policy to Control Conditions for Automatic Failover 

Auditing has been more improved in SQL Server 2008 and up. There are many ways to trace a SELECT .... (make sure you are not doing it against an HR database :-)) Option 1: Through SQL trace, where in depending on what trace events have been selected, you can get logged/audited. Option 2: Under database --> Security --> Database Audit Specifications . Here and here are 2 articles that have explained Option 2 in detail. 

Since Master Data Services is available on the 64-bit editions of Business Intelligence and Enterprise only, you can leverage partitioning for data management. You can partition the data by month or a quarter or year (depending on your requirements). This way you have a solid archival strategy and you can load new data into new partitions. You can have different filegroups based on your archiving strategy as well. Check this Partitioning & Archiving tables in SQL Server to get you started. Also refer to Best Practices + Table Partitioning: Merging Boundary Points and SQL Server Partitioned Tables with Multiple Filegroups for High Availability. To fully automate the process of partitioning - switching in and out, you can use SQL Server Partition Management tool from CODEPLEX. Note that if you absolutely dont need the data, you can even purge the partition and get rid of old data. 

Make sure you have error logs set to manageable size. Check MSDN : Configure SQL Server Error Logs and kb 2199578 for managing SQL Server error log. EDIT: Below is what we do in our PROD environments: 

From database side, if you are running with a witness in high-safety mode (high-safety mode with automatic failover), then ONLY you will have flexibility of automatic failover, rest are manual failover. Even though, mirroring allows you to do automatic failover in high-safety mode, make sure your keep monitoring the REDO queue as this can impact the recovery during a failover scenario. FROM BOL : 

No it wont invalidate foreign keys. Rebuilding an index drops and re-creates the index thereby removing fragmentation and it reclaims disk space by compacting the pages based on the specified or existing fill factor setting, and reorders the index rows in contiguous pages. FOREIGN KEY constraints do not have to be dropped in advance. Now if you disable Index then : The Query optimizer wont be able to use the index and any of your queries that uses index hint referencing the disabled index will FAIL. SQL Server retains the metadata about the index and the usage stats in . Also, Foreign Key constraints are DISABLED. Disabling Non-Clustered (NC) Index: This will straight away delete the index pages and thereby freeing up the space in the database. Disabling a CLUSTERED Index: All related NC indexes and views become unavailable and FK's are also disabled. Any queries that reference the table will fail, as a CLUSTERED index is itself the data. Note that the data will still remain, but will be inaccessible. Only way to again access data is to REBUILD the index. 

you can also create a generic SP for all your agent jobs to check first in master to check few status e.g. dbo.usp_CheckDBOnline 

ProcDump has a parameter for CPU threshold at which to create a dump of the process. Excellent References : 

You can use server side trace (different from using Profiler GUI that incurs more resources) during your testing or your business cycle and capture only stuff related to SP's. Then you can load that in a table or excel for further analysis. Second approach, is to use DMV sys.dm_exec_procedure_stats (with limitation that if sql server is restarted, then the data is flushed). You can even schedule a job to capture DMV data to a table to keep it persisted. 

if you want a GUI for DDL Triggers or Event Notification - TSQL DDL Code History Tool is a good choice. David Wiseman (author of the tool) has written a good article on SSC - A DDL Auditing Solution 

The most important thing when doing a large update or delete is to avoid the transaction log growing out of control. To allow log reuse & avoid bloating to Tlog (Transaction log truncation) 

this is your logshipping to mirroring conversion. Make sure you take regular log backups on the principal else your log file will grow. 

Above holds true even today for sql server 2012 or 2014. Note that T-Rep on sql 2005 and up has improved a lot, but the argument your DBA has made is baseless. We even has some legacy applications running on sql 2000 and we use replication heavily and is stable in sql 2000 - no issues at all. 

Yes, TF 1800 is required to be turned ON even if you are on SP1. This is because you have mis-aligned disk sector size. 

A SID is a mechanism that assigns privileges to the service itself, rather than to the account under which the service runs. For Clustered installations: It enables you to use a domain account with minimal privileges on the box, which also improves your security because people end up knowing the domain service account credentials. For non-clustered installations: Per-Service SID, it allows you to use Network Service as the service account, improving security by getting automatic password management, but without the traditional downside of having Network Service accumulate excessive privileges from multiple services. Refer to : 

For 64 bit OS, you dont need AWE and even enabling AWE using has no meaning. Refer to Great SQL Server Debates: Lock Pages in Memory by Jonathan Kehayias : 

If you are creating package from scratch using BIDS, then in the execute sql task editor, in the sql statement 

Here is an excellent article from SQL CAT team describing how it will reduce I/O : Fast ordered delete 

Another compliment to @Tara's answer is to use Microsoft's Tigertoolbox (has a lot of scripts which are actively maintained by the CSS engineers) - 

Not true. SQL server does not lock BACKUP File. You have to use and on the test server since the database is already on test server and you are overwriting it with a fresh copy of PROD backup. You can automate the whole process with SQLCMD / PowerShell and use SQL Agent job for regular scheduling to refresh your TEST server. 

There are many ways of how you can do database capacity planning. msdb backup history if gets regular trimmed, you wont be having much data left for analysis As Mark pointed out, it can be done using the method described by Erin - trending database growth from backup. You can even use PIVOT to find out the database growth over a period of 12 months from the backup history as below : 

Dont just blindly go for rebuilding all the indexes. There is a much intelligent way of doing it. (hint: SQL Server Index and Statistics Maintenance by Ola hallengren) This is an official guideline from Microsoft (and its a good starting point - instead of blindly rebuilding all the indexes): click here for enlarge Also, if you are using sql server enterprise edition then Rebuild is an ONLINE operation (REBUILD ONLINE = ON) Also, read : Online Index Rebuild - Can Cause Increased Fragmentation - when it is allowed to run with and directives. As a side note: Paul Randal has a nice checklist of VLDB Maintenance best practices from his MS days :-) 

No. As I mentioned in the comments section. THe database wont be able to maintain its ACID properties. Either the query fails and returns an error or the lock is held by SQL Server until the transaction is completed and then released. 

As Jason Pointed out, that to send email using SSIS, you have to use Script Task. Below links with scripts will help you out. 

A login is for authenticating to the server. A login maps to a database user to access the database. will list logins for Azure. 

This is a very common question not only with DEV, it applies to every team in IT and business. What changed ? ==> can be answered by facts and figures. Facts refer to for example