Jackendoff (2002) Foundations of language: brain, meaning, grammar, evolution. Hackl (2013) The syntax-semantics interface. Conference by Barbara Partee (2009) video 

I have heard the term "tritransitive" for verbs that are said to take four arguments. One example in English is this . As you can see, this is different from your example with causatives, because the fourth argument taken by the main verb is the whole that-clause. If I recall correctly, causative constructions have been analyzed as having only two or exactly three arguments. A problem for the analysis of any construction with regard to argument structure is to determine whether two or more arguments are taken by the same predicate, and thus are coarguments. You need syntactic and semantic evidence to prove that a verb can have four arguments, and as far as I know, causatives are not part of that kind of verbs. I'm sorry I can't tell you anything else about argument structure outside Germanic and Romance linguistics. 

I don't think that the expletive insertion could work for but not for . Your test sentence indeed proves that is a raising predicate. Look at your wrong result: . Of course it's unacceptable because it doesn't have the (CP) that-clause that you used in the correct expletive test. If I recall correctly, this kind of clauses prevent the subject to be raised, but the expletive comes to the rescue. The verb has to agree with the expletive, not with a noun phrase that is blocked for movement. That's why you get and not . 

It is not easy to describe the relation between syntax and semantics, but it is probably easy to say why that is not easy: there are different perspectives about syntax and semantics, so the relation depends on what you understand by form and meaning, structure and content. If you look at the history of Chomskyan linguistics, you will find the chapter in which a group of people were working on deep structures so much that they actually were doing semantics and not syntax. Moreover, anaphors and quantifiers became really problematic for the framework, so it became insufficient to explain the linguistc phenomena under discussion. But Chomsky and others were not happy about the division, and today there are many syntacticians who keep themselves away from the "dangerous" interface with semantics. Now, to be more specific about your question, but still general about the definitions, I think that you could see syntax as independent from semantics but not the other way around. Let's say the goal of syntax is to develop theories about the similarities and differences between linguistic structures within and across languages. Let's also assume that we can study elements that are necessary for those structures to be well-formed, and that their meaning is not essential for the interpretation of the whole structure. Then it is possible to say that syntax does not need semantics, or that it is structure what determines meaning. Whether that is interesting or helpful is up to the syntacticians who work under such view. As for semantics, it simply cannot be studied without reference to syntax, for any meaningful phrase or sentence is always a that, a phrase or a sentence, so it must have a certain structure. If we want to study language in a more comprehensive way, I think the relation bewteen syntax and semantics must be one of interdependence, and thus it is more fruitful to study the way structures are built up and also the meaning that arises from such building operation. Just as we have structure building from a syntactic perspective, we have function application from a semantic perspective. This is one of the several general descriptions of the relation between syntax and semantics, but again, the specific views depend on the theories of syntax and semantics which you are working with. Even if the view is that syntax and semantics are related in some way, there are approaches in which syntactic and semantic structures are generated independently, for instance 1 below. There's also an interesting and recent article about the syntax-semantic interface that you might find useful, and that's 2. Another interesting presentation of the mutual influence of syntax and semantics can be found in 3 (link to video). 

I would say that there is no "typical linguistic response" here. The paradox is interesting mostly for philosophical semantics, but much less for many of the linguistic semantics perspectives. For instance, one view is that the sentence in question turns out to be a paradox if we assume a referential theory of meaning, and if we assume that the indexical "this" refers to the sentence itself. When we analyze this sentence taking into account a given context of utterance, then we might have a sentence that perhaps does not refer to itself. The point here is that any sentence from natural language that matters for linguistics is a sentence that has content, its relevant in a given context, and its related to the world in some way or another. If we see the liar's paradox from the perspective of truth-conditional semantics, I would say that what we want to understand is the truth conditions of the proposition expressed by the sentence, and we need to consider the state of affairs or particular model of reality it describes in a given context. Here it is important to know the difference between truth conditions and truth value, because we can know the meaning of a sentence without knowing if it is true or false. So, for the proposition expressed by "This sentence is false", what we know is that its meaning is whatever makes the sentence true or false depending on the model. If the sentence is uttered in a model or a possible world where the context states that "this" refers to any sentence, we just have to check what sentence are we talking about to evaluate its truth value. If the model states that the sentence refers to itself, then we are moving from the domain of linguistics to the domain of philosophy, and we have different options. One is that this is a problem of confusing hierarchy levels: language and metalanguage (Russellian view); another one is that we are just uttering a pointless sentence, where nothing true or false is really being said (Strawsonian view). There are other alternatives, but again, they are philosophical approaches to the logic of language, and are far from the scope of most linguistic theories of meaning in natural language. 

A late answer but perhaps it will help someone. Here's some code to generate a rule very similar to that of the example. 

Yes, there is at least one to my knowledge. I have read (and heard) about this realisation of the phoneme. The variety that is commonly mentioned is Andean Spanish, and for this allophone, the main locations where it has been registered are some regions of Venezuela, Colombia, and Ecuador. 

I would like to make a more general contribution, but still regarding your specific question. There are many definitions of synonymy. Let's agree in a simple one: it is a semantic relation between the meanings of words or sentences. We now have the problem of defining "meaning". In one way or another, the concept of synonymy is uderstood without requiring absolute identity between words, if there is such thing. Without going in the details, a logical account for absolute synonymy would say that two linguistic forms are synonyms if they are interchangeable salva veritate, that is, keeping the truth value of the expression they are part of. In his attack on the concept of analyticity, Quine (1951) discussed the idea of whether this kind of interchangeability was a condition strong enough for synonymy. Saying that is a synonym of was saying that the proposition all and only bachelors were unmarried man was analytical. But that is circular, and Quine wanted to discuss whether the sufficient condition for cognitive synonymy was interchangeability and not analyticity. Hundreds of articles have been written about this and I cannot review all the arguments, but the point was to illustrate one of the basic ideas about logical synonymy. If you think about the example of the correspondence between and , you will soon realise that there are not really many words like those in natural language. So, if we talk about words, we think that synonyms are those that can be substituted for each other in sentential contexts, and if we talk about sentences, then they are said to be synonyms if the substitution preserves truth values. But still we have to be careful with the concept of synonymy. For words, we can have different semantic values that may not have a one-to-one correspondence to each other. Think about the words . They are not interchangeable in all contexts: vs. . So it's not only about the meaning of the synonyms, which is unclear, but also about their relations with other words on their contexts of appearance (the basic idea behing compositionality in semantics). We can also use euphemisms as synonyms for the words we don't want to use. Here we are opepning the door to a whole new set of factors that can influence synonymy, as some previous comments/answers point out. We must then consider that synonymy (either absolute or partial) depends on what are the meanings we are trying to compare and the compositional nature of the relations between words in the context of sentences. This is a matter of debate for any kind of approach to meaning, either in semantics, pragmatics and other subfield. Let us admit that there can be at least two kinds of synonymy: there might be a full synonymy for words that are logically (salva veritate) interchangeable, but that is not very common. And there is also this everyday use of synonymy that treats sameness of meaning in a more or less unrestricted way. One could see meaning as some sort of continuum and words as mapping certain parts of that continuum, with overlaps between them that allows us to call them synonyms. This can be analysed theoretically and empirically in linguistics. As was mentioned before, definitions depend on the framework used to explain meaning. If the meanings of words are their referents, then it is easy to find synonyms. A second option is to understand that meanings are senses (more or less in he Fregean conception), but then you will find many debates as well. A third option is a psychological perspective were meanings are representations or concepts in the mind, and there you will find even more disagreement. The definition of synonymy is then closely related to the semantic perspective adopted and the definition of meaning endorsed. You can compare words in terms of denotations, semantic features, semantic maps, representations, etc. You can also study synonymy in terms of sociolinguistics, comparing dialects and registers, or even from a cross-linguistic point of view. The use of computational methods and corpus research is also frequent. But at the end, you can only say that there is absolute synonymy if you analyse all the possible meanings/contexts, and that is quite difficult, if not implausible. In sum, definitions of synonymy are relative to the theoretical framework adopted, and particularly dependent on the definition of meaning. What you call "absolute synonymy" can also be what has been defined as logical or full synonymy, but not everyone will agree. Words like and can be said to be full synonyms, but then again, these examples are extremely rare and there might be differences we don't know yet. That is also the case of and , mentioned in a previous answer. Perhaps the reason is that these words have a very narrow range of contexts of use. It might be the case that words with more meanings and more frequently used tend to be less close to the end of the continuum where full synonymy (if it exists) is found. Besides, two or more words that mean exactly the same seem to be not economical for language, and thus it would tend to be avoided. Are there absolute synonyms? There can be, but then again, the answer itself is not absolute. I am sorry I can't provide references, but it seems that's the situation, at least in linguistic semantics. Partial synonymy is assumed for methodological purposes without much discussion, while absolute synonymy is considered rare because it is inefficient and uneconomical.