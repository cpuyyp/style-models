Looking then at their closures: [AB]⁺={A,B,C,D,E,G} [BC]⁺={B,C,A,D,E,G} The teacher then did something about closures I don't perfectly understand: F¹⁺=∅ F²⁺=∅ F³⁺={AD → E, B → D, AB → D,AB → E} 

How much does the following query costs in terms of operations when the attribute is a key or an attribute ? 

C → D shouldn't be lost but isn't in F¹ nor in F². Why is it important to say that? (from F¹) and (from F²) therefore, The result which I don't understand at all is that it is without loss of dependencies... 

Npag(R) "because indexing is on the key" and "according to the screenshot" (which is provided above, but for me he is wrong on the cell as far as it isn't a memory operation but a equality search) Npag(R) Yeah, it depends ... 

I found a similar question on SO which states that the error indicates that zookeeper quorum is not running - most probable cause can be some inconsistency with your zookeeper.quorum setting in but it didn't helped my file looks like : 

I would say that those which aren't equality searches are those that may have less benefices from the multi-attribute index, but I'm not sure why... Here is an an example of a multi-attribute index (which remains as a link for I don't know which reason ... : 

How much does the following query costs in terms of operations when the attribute is a key or an attribute ? 

I have to justify the answer and give an estimate of query execution cost in all three cases. To my mind it was 

I know this wrong, not enough accurate. Can you help me learning how to create an accurate and right trigger? The related database scheme in UML is (in French, unfortunately): 

Is an equality search. As far as there can be several records with this attribute, with an attribute search it costs Npage(R). Yet with the Key search I've been told to follow the given array (without understanding why...)... But I don't know what does induces Tips from my teacher 1. Attribute search It should be the same costs for the attribute search as far as RC = Csearch + Cwrite + Csort + Cread Yet I merely understand this equation... 2. key search "we need to estimate the number of bytes to store Erec" said he. where Erec = Nrec(R)/Lr= 100000/100 = 100. (I think there is a mistake here as far as first 100000/100 = 1000 and second it shouldn't have been Lr as the denominator but the number of files that actually have the attribute we are looking for => 100000/1000 = 100) Furthermore, the size of Erec = 100 x 4 = 400 <1024. We would only need one page to order Erec. (Yet, I don't understand why is it multiplied by 4.) References This question comes more or less from a book written by Dario Colazzo and some others from Pisa University Relational DBMS Internals available in the link. 

A → B x²²= b B → C x¹³=x²³=c Therfore we should see that it is without loss of information (we should have a straight line of determinated data). However, the answer says that it clearly lacks informations and in order to show that it is with loss of information, one should find an instance r such that r ≠ r¹⋈ r²⋈ r³ I know that I have then to do some arrays and some junctions but I don't even know how to start the arrays... 

Is it possible to view the current I/O benchmarks used by the optimizer? Is it possible to trigger an update of those benchmarks? Does the optimizer account for the current I/O cost of an operation? For example, if there is heavy concurrent load on the storage device causing a reduction in performance, does it consider that the relative cost of I/O will temporarily be higher? Does the optimizer consider latency/random-access performance versus throughput/sequential performance? If you have data files on multiple storage devices with different performance characteristics, does the optimizer have and apply device-specific costs? 

I am unable to find an explanation for why Oracle is scanning a table multiple times during the process of adding a primary key constraint. I have a large range-partitioned table. I created a local unique index "MY_IND" on this table (with the partition column leading) and built it on every partition. Every partition of this index is listed as USABLE in USER_IND_PARTITIONS. I gathered statistics on the index, so the engine should not be grossly underestimating the number of rows in the table. I then attempted to add a primary key constraint on the same columns as the unique index, including the "USING INDEX MY_IND" clause to indicate that it should use the existing unique index. At this point, I expected the database to be able to create the constraint without doing any reads, since it should already know that the column list is unique. Instead, according to V$SESSION_LONGOPS, the session adding the primary key did a table scan, one partition at a time. Then it did it again. Now it is on (at least) the third pass through the table. Why would Oracle need to scan the table in this instance? In particular, even if it does need to scan the table, why would it do so multiple times? 

I am unable to find an explanation for why Oracle is scanning a table multiple times during the process of adding a primary key constraint. I have a large range-partitioned table. I created a local unique index "MY_IND" on this table (with the partition column leading) and built it on every partition. Every partition of this index was listed as USABLE in USER_IND_PARTITIONS. I gathered statistics on the index, so the engine should not be grossly underestimating the number of rows in the table. I then attempted to add a primary key constraint on the same columns as the unique index, including the "USING INDEX MY_IND" clause to indicate that it should use the existing unique index. At this point, I expected the database to be able to create the constraint without doing any reads, since it should already know that the column list is unique. Instead, according to V$SESSION_LONGOPS, the session adding the primary key did a table scan, one partition at a time. Then it did it again at least two more times. Why would Oracle need to scan the table in this instance? In particular, even if it does need to scan the table, why would it do so multiple times? 

What is the number of disk cylinders ? Isn't it one whole cylinder like the following screenshot seems to show or is cylinder a synonym of platter ? 

R is stored with heap organization with data unsorted with both respect to K and A, in pages of size Dpag=1024 bytes each. I have an array giving me the following costs for heap organizations (but actually for deletion it is Cs+1). 

I have to create a trigger that calculate the the quantity in stock and this is my first one on PostgreSQL. When entering the quantity bought at a Provider (Fournisseur) or a delivery at a client, the quantity has to be updated. I tried: 

According to my teacher the number of deleted records for an attribute operation is Nrec(R)/Lr = 100000/100 = 100. But I don't understand why. Therfore the cost of the operation would be : Npage+100. 

I am trying to import a csv file into Cassandra which is very long. These are food products: ingredients, nutrition, labels... It comes from Open Food Data. List information on food products: ingredients, nutritional information, labels, etc. Most of the data comes from crowdsourcing information. The file this envelope of the open platform of French public data.gouv.fr The command I tried I try the following command with all the columns that I was able to collect with a python script: 

I have several csv files on university courses that all seem linked by an ID, that you can find here, and I wondered how to put them on Elasticsearch. I know, thanks to this video and Logstash, how to insert one sole file csv file to Elasticsearch. But do you know how to insert several such as those in the provided link ? At the moment I started with a first file for a first csv file : . But it would be painful to write them all... The file is : 

Yet I did the advised settings Apache gave in order to make the data persitent I tried to run a and it gave me back : 

I don't understand why is he saying the first and where does he take the second. My teacher uses this book he co-writted. 

I think that the answer in relational algebra is : ΠConstructor(Product ⋈ Laptop ⋈ Printer) Yet I don't know how to write it in SQL, would it be : 

Update May, 3rd Without knowing how to use a file to implement csv files to Elasticsearch, I fell back to Elastic blog and tried to do a shell script for a first file before trying to generalize the approach : importCSVFiles : 

I'm lost in the sea with the following example of decomposition without loss of dependencies... Let be the dependencies R is decomposed in & I want to know if such a decomposition is without loss of dependencies and without loss of information. I understand that it is without loss of information but why is it without loss of dependencies? It is said so because 

I am learning dynamic tree-structure organizations and how to design databases. Consider a DBMS with the following characteristics : 

I have attached a screenshot showing the Date of Birth attribute. The Date of Death attribute is identical other than looking at the Date of Death Key field. 

I have a two-node SQL cluster (2008 R2). Some of the databases within that SQL instance are mirrored to another server on a remote site, using the High safety with automatic failover. The mirroring connection timeout value for those databases is set to 90 seconds. When I move SQL from one node in the cluster to another node, using the Failover Cluster Manager application's option of "Move this service or application to another node" the databases that are mirrored are instantly failing over to the mirror. This is undesirable behaviour. My reason for setting the mirroring connection timeout value is that I only want to fail over to the database mirror if the cluster fails completely and there are no functioning nodes. Is there any way to achieve this? It feels as though it should be possible, otherwise the concept of mixing clustering and automatic failover database mirroring would be unworkable as every node failover within the cluster would trigger a mirror failover. Thanks. 

leave them with the data warehouse database put them on the server that will have Reporting Services installed on it create another server specifically to host those two databases. 

returned "The backup set on file 1 is valid." Which is ironic given that I was only using the MAXTRANSFERSIZE option to try to speed up the restore, but instead it cost me half a day! The error only occurs on backups taken from last night onwards. Earlier backups verify fine whether I specify MAXTRANSFERSIZE or not, which is strange, but I guess I can live with not knowing the reason why that is unless somebody has an insight into that behaviour? 

I'm currently doing some work on monitoring SQL Server performance. I've found the following script[1] which calculates 'Page Lookups Percentage' - the suggestion being that a "good" value is something less than 100. 

I'm in the process of building a new data warehouse and I am starting out by building a few dimensions. I have created a fairly typical "Date" dimension and that seems ok. Now I am trying to create a "Person" dimension which contains "Date of Birth Key" and "Date of Death Key" attributes, both of which are related to the key attribute of the Date dimension. When I process the Person dimension I am receiving an error about duplicate keys. Looking into it, I found that the code that is generated for processing the Date of Birth attribute is incorrectly joining to the Date dimension using the Date of Death column: 

Using Kenneth's code, and a few other bits and pieces that I pulled together, I came up with the following code which I think should do the job. If anybody sees any potential problem with this I'd be interested in knowing! 

Perhaps the easiest way is to just do a insert...select, re-referencing at the same time by using set identity_insert on and adding a fixed value to the existing identity to ensure no conflict between the data sets ? Something like this: