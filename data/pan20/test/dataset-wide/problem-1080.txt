But I'd rather use the same thing everyone else uses. Question: What's the right way to do ? Answer: From SpBlitzErik - use Ola hallengren's scripts, you dummy. Followup Questions: 

You can also do it using a date table and ordering it by newid(). I've used this technique to scramble lots and lots of data in the past. One advantage is that you can scramble any field by joining the table to itself on Note: if your person table is bigger than your date table, in this example, loop the date table insert a few times until it is bigger. 

3. Set up a .dtsx package: You can do it directly in SSIS, but I find it easier to use the SQL Server Import and Export Wizard by right click on the database, hitting Tasks and Export Data. Set the Data source to by SQL Server Native Client, set up your server and database, then hit Next. Set the Destination to be Microsoft Excel, and the excel file path to your shared folder (not your blank template): 

I'm trying to break all the rules of databasing using the stuff function. I want to smush every applicable row into just one, for science, you know? Problem is, I have to write a new function every time. They look like this: 

Here's the James May method. Looking for feedback & ways to do it smarter. 1. Write your query in SSMS. To get dynamic date ranges that always fall to the first of the month, I do it this way (there is almost certainly an easier way): 

How often do I need to run it? It needs to run at the same interval that I keep my backups, right? If I keep the backups for 1 day, I need to run it every day? Can I restore the databases to my test environment and checkDB there? If I've got lots of sql servers, but they all use the same SAN, am I going to want to stagger my CheckDB's? 

I've got a query that checks for new patients in the ER, and if they have been there before in the last 72 hours, it notifies the killbo... uhhh... care managers. I could set this up as a trigger, but I don't think this is a good idea. What I want to do instead is set it up as a job that runs every 5 minutes, with this in the where clause: 

Step 2 will be the .dtsx package. Change the step type to be SQL Server Integration Services Package, and at the bottom, specify the path for the package, which we saved at 

I've got a cursor sending out pager messages, and occasionally it sends out duplicates. The syntax looks like this: 

It turns a list like this: Albuterol Sulfate Amlodipine Besylate Aspirin Benztropine Mesylate Bisacodyl Ciprofloxacin Collagenase Divalproex Sodium ... into a string like this: Divalproex Sodium, Collagenase, Ciprofloxacin, Bisacodyl, Benztropine Mesylate, Aspirin, Amlodipine Besylate, Albuterol Sulfate I can write it as a dynamic stored procedure, but you can't call sp_executesql from a function (or at least, I don't know how). Question How would you write this function in a way that it could be used on any table? 

We are building a new server setup following this guide: $URL$ And it reccomends 7x 960GB SSD's in Raid 5. Question: If the databases are on the SAN, what is all this extra space for? 

You're nearly there. the trick is writing your group by. I don't want to bitch about your question being too long to read, but I definitely didn't read the whole thing. one other suggestion, if you're only putting dates into a dateTime column, consider altering it to a date column. 

But the query itself takes 03:31. Question: is current_timestamp the timestamp of the beginning of the query? here's the entire big, bad query: 

You'll get that error when one of the columns in the select won't fit into the destination table. Tracking it down can take some time, but it's just like kindergaten - try and fit the shapes in the holes. If you compare #Variable_info to #stored_proc_info, you will probably find that one of the columns in #variable_info is larger than its corresponding column in #stored_proc_info. I can't find #variable_info in my version of the sp_blitzcache, so I can't tell which columns don't match. Often, I track it down by running a query like this: 

Here's how you can join a table to itself randomly and update a column. This method will also maintain the distribution of your data: 

One thing to experiment with is changing your clustered indexes - in a lot of ways they aren't really indexes at all, they are the order in which the data is stored. make copies of your current clustered indexes first, then try adding the columns in the joins & where clause to the clustered index. Clustered indexes are often the same as primary keys, but they don't have to be. your primary key and clustered index can be completely different. Another thing - the - do you need every column from that table? If you only take the columns you need, it might help. Then, there must be a way to stop a query if a website user bails... Other things you can do - archive some data. I'll bet some of your tables are pretty big. Do you need all the data there? With Enterprise edition, you can also partition tables. Check for locking - if you have two users putting this query to your DB at the same time, are they blocking each other? I'm going to catch some shit for suggesting this, but you could join with (NOLOCK) and see if that helps. Oh, and when did you last update your statistics? The query tuning videos on Brent's site are really good, if you need more detail. 

That's it. Records affected is verified as 1. UPDATE: I don't think it can be dirty reads. The read that shows the previous value happens minutes after the write. There are things about this situation that I have a hard time believing, so it's totally possible there is more to the story I'm not being told. 

Update 2: I was hoping to not do this, but maybe the exception handling is part of the problem. Here is the create procedure for this, and also another stored procedure being called in the exception handler. Sorry for the length: 

I'm not sure if there is a named pattern for this, or if there isn't because it's a terrible idea. But I need my service to operate in an active/active load balanced environment. This is the applicaiton server only. The database will be on a separate server. I have a service that will need to run through a process for each record in a table. This process can take a minute or two, and will repeat every n minutes (configurable, usually 15 minutes). With a table of 1000 records that needs this processing, and two services running against this same data set, I would like to have each service "check out" a record to process. I need to make sure that only one service/thread is processing each record at a time. I have colleagues that have used a "lock table" in the past. Where a record is written to this table to logically lock the record in the other table (that other table is pretty static btw, and with a very occassional new record added), and then deleted to release the lock. I'm wondering if it wouldn't be better for the new table have a column that indicates when it was locked, and that it is currently locked, instead of inserting an deleting constantly. Does anyone have an tips for this kind of thing? Is there an established pattern for long(ish) term logical locking? Any tips for how to ensure only one service grabs the lock at a time? (My colleague uses TABLOCKX to lock the entire table.) 

These images show the two elements with the highest cost. They have identical properties between both the query plans. The difference in the final row totals is the result of joins with other tables. But those joins all have a low relative cost. 

P.S. I did edit this a little, changing names of the procedures, the name of the table only, and removed comments only. 

This may be a terrible question, because I'm not sure how information I can include to help. We have data segregated by customer. One customer apparently has higher volume of data. The same query ran for a small customer returns in 2 seconds, and the result is 11 rows. The larger customer takes 47 seconds, and the result is 6600 rows. This is a complicated query with 11 joins. This is just for a report, but the report is timing out and the operator is complaining. It's possible the difference is just the volume of data, but I want to investigate. When I look at the query plan, the percentages for the query cost are exactly the same between the two. There are no suggestions for indexes. In both query plans, I can see the highest cost is from a join to a table that has 3.8 million rows. However, both sets of data are joining with that same table. In both cases the resulting "actual number of rows" is 3.8 million. In both cases this join has a clustered index scan that is 39% of the query "cost". So my question is this: Do the percentages and query cost even mean anything? are they referring to the estimated data from the query plan, and do not reflect the real cost? Otherwise how could the same join make up 39% of the "cost" for both, but one takes 2 seconds and one takes 47 seconds? Is SQL server lying to me and the actual difference in cost is the "nested loop" for the inner join that produces the final row count even though it lists that as 0% of the cost? 

I have a query, that as far as I know has failed exactly one time. It's a simple select count(*) from one table, no joins. But at least this once, executing that query resulted in no data read from SqlDataReader. Not even null, just nothing. First call to Read returns false. No exception was raised. Has any one ever heard of that before? Any scenarios you can think of that would cause it? I'm not even sure what to ask for to look at beyond SQL server logs. It's not something we can duplicate. I am assuming I'll have to chalk it up to a fluke and move on if/until it becomes a chronic problem. Here's a similar query: 

Is there a class of error that will thrown an exception out of the query, but not raise it to the application when executing a command or reading from the resulting SqlDataReader? UPDATE: Here's the code executing this. 

I can be reasonably sure there are not conflicting updates to this record (record is owned by a single user). There would be a lot more wrong than this if that weren't the case. Newer versions use a stored procedure here, but it would not be possible for me to change that in this version.