According to your question and its comments, your buffer pool may still be too small. Since the Buffer Pool is < 1G (1024M), innodb_buffer_pool_instances drops to 1. This, in turn, would force innodb_page_cleaners to drop to 1. You can verify than with 

Since it is a backup, you are right to be concerned with whether it can be restored or not. To be on the safe side, you must run 

Naturally, running CHANGE MASTER TO generates a master.info file, which includes master_ssl. No harm, no foul. Relying on my.cnf for older versions of mysql results in option being ignored when no master.info exists. They are no longer valid. 

Additionally, if you do not need every column from the hits tables, then only include the column you know you will access. The same goes for players, stadiums, and games. In other words, as an example, if you only need the playerName from the player table, then you do not need player.* in the SELECT. You will need just player.playerName. VIEWPOINT #4 : You may need to index the leagueLevel column You will need to do the following to make the needed index: 

The message itself looks like an authentication issue. Here is what you should do: If you can connect to the hosting company's mysql database from the mysql client, run this command: 

If you really want to hunt down columns in all table rows, there is something you can do. SCENARIO You have a table that looks like this 

because the LIMIT is being applied at a different stage. The first query returns nothing if tablename has less 1000 rows The second query returns nothing if the subquery has less 1000 rows CONCLUSION You will have to sculpt the query to make sure you are sorting data at the right stage 

In other words, SELECTs are not blocked on a MyISAM table as long as newly INSERTs rows are entering a table with no gaps. IF any row being INSERTed has to fill any gaps, then the conditions for Concurrent INSERTs are no longer applicable. SELECTs go back to being handled the way they are normally handled. If you are performing bulk loading of a MyISAM table, you will need to rev up certain things. For instance, you will need to add this option and restart mysql: 

If the DOS prompt appears after some error messages, then mysqld failed. If error messages come up and the DOS prompt does not appear, mysqld is up. If the settings you configured in my.ini on PC 2 is not in then mysqld cannot see the my.ini. To reiterate, mysqld expects my.ini to be in the parent directory. 

CAVEAT : I'll leave the rest to your imagination to run the mysqldump and execute on the target DB server. Give it a Try !!! UPDATE 2013-12-06 18:51 EST Here is a crazy idea requiring some elbow grease STEP #1 Remove the first line of do it looks like this: 

UPDATE 2011-12-09 19:57 EDT If you are concerned about diskspace taken by indexes, you have a decision to make OPTION 1 : Diskspace is no object This can only be beneficial if your queries effectively use covering indexes and you are willing to live with (and provide for) large diskspace. OPTION 2 : Diskspace is a precious commodity It's time to do some forecasting. Both and are BIGINT. You can cut space down some as follows: 

CAVEAT With regard to caching, caching takes a dive quickly because of doing a full table scan. For MyISAM index pages flow in and out of the MyISAM Key Cache. For InnoDB, data and index pages flow in and out of the InnoDB Buffer Pool. 

You have to think in terms of the design. After triggers on the table cannot help populate the table since and have no immediate relationship. Although you should not need this, you could have before triggers on to check for validity of and . 

If you mysqldump from MySQL 5.1, please exclude the schema. How do you port over the MySQL Grants? Like this: 

MySQL is a big pain in the neck in Windows because of the way open files behave. In Linux, whenever I see a log file filling up a disk (i.e., /var/log/mysqld.log), I attempt to truncate the file 

After running , every time you run all the other lines, the list rotates. Note that I ran those lines 7 times with 6 rows. Here is the output: 

If you can send the output to a text file, look for user_id 12 and compare its to the total number of rows in the table. My guess is that user_id 12 is probably more than 5% of the table. The Query Optimizer would give up using any index on user_id and just do a full table scan. To make sure that is the case, please run 

OPTION #2 : Have mysqld execute script before allowing connections You would have to add this option 

This will perform a full index scan rather than a full table scan because all three columns are in the index. That's a lot less baggage to walk around with for temp table generation because you are scanning just 3 columns in a smaller resource (the index) as opposed to 6 columns as from a bigger resource (the table) as well as not sticking url and bodytext (which are probably VARCHAR(300) and TEXT fields) into the mix just for the sake of gathering keys. Next, make the SELECT sid query into an inline table and connect them back to the news table using only the fetched keys. EXAMPLE : Suppose your LIMIT variables are 200,10. This means you want to move to the 201st row of the news table and get 10 keys from that point. This means that no matter which page you are on, you want to collect 10 keys at a time, and only 10 keys at a time. Here is the new and improved query: 

When you run this, the table on the Slave is compared to that of its Master using the Primary Key (or Unique Key is there is no Primary Key) of the Slave's . The output will be a series of SQL commands, usually and . The script is to be executed on the Slave. When done, the data in should be identical to its counterpart on the Master. To test it then, run these commands: 

In the strict sense of the word, no you cannot set foreign keys on views. Here is why: InnoDB is the only built-in storage engine for MySQL that features foreign keys. Any InnoDB table will be registered in information_schema.tables with engine = 'InnoDB'. Views, while registered in information_schema.tables, has a NULL storage engine. There are no mechanisms in MySQL to have foreign keys on any table that has an undefined storage engine. 

That's 4.56GB. That space is used for the Insert Buffer Section of the InnoDB Buffer Pool (a.k.a. Change Buffer). This is used to mitigate changes to nonunique indexes into the System Tablespace File (which all have come to know as ). The InnoDB Storage Engine is managing the Buffer Pool's internals. Therefore, InnoDB will never surpass 62.5% of RAM. What is more, the RAM for the Buffer Pool is never given back. WHERE IS THE 70.2% OF RAM COMING FROM ??? Look back at the output of at these lines 

ASPECT #1 This sounds like you are suffering from a classic case of bulk insert buffering. (Forgive me if I sound like a doctor). LOAD DATA INFILE takes advantage of a tree-structured bulk insert buffer. The size is set by the option bulk_insert_buffer_size. Please note what that part of the Documentation says: 

You would load first, then SUGGESTION #3 : Read the next auto_increment of every table into a script You can use a query like this to make the SQL script: 

Since mysqldumps were in alphabetical order by default, there might be some cases where the parent table of a foreign key relationship would appear after the child table rather than before. As a consequence, the mock table was created to have a parent to adopt the aclGroup table. Since a child cannot have more than one foreign key reference to the same parent on the same columns, then NDB is ignoring it and still trying to enforce Under the hood, this is what the NDB storage engine should be doing after 

This timestamp is from the OS. You can't go wrong on this one. UPDATE 2011-12-21 22:04 EDT [mysqld] innodb_max_dirty_pages_pct=0; Add this to my.cnf, restart mysql, and all InnoDB tables will experience fast flushes from the buffer pool. To avoid restarting, just run 

You have to create two different datadir locations. If they reside on the same disk, you will have two ibdata1 files being updated on the same disk, which will suffer increased disk I/O and will slow down both MySQL instances. Suggestion #1: You could accommodate this having the datadir of each MySQL Instance on a different disk using its own disk controller (Make sure your disk controllers have up-to-date kernels). This is the same concept you would apply if 

and BINGO a MySQL Connection ID of 3 instead of 2 There is a record of it in the Global Status variables 

If you are content with adding and deleting a view_id per user_id, here is what I see Adding View 12 To User 10 I see three queries 

Your Question The indexes of your table may not be loaded into memory if the MySQL Query Optimizer decides not to use. If your WHERE clause dictates the a significant amount of row have to be read from the indexes, MySQL Query Optimizer will see that when constructing the EXPLAIN plan and decide to use a full table scan instead. Parallel I/O operations on a MyISAM table is unattainable because it is unconfigurable. InnoDB can be tuned for increase performance like that. 

is definitely the book for you (Third Edition is out now) !!! I would still have MySQL 5.0 Certification Study Guide 

When you attempted to update too much data in a single query and you ran into a error, you did not run out of space on your HD. What ran out of space is the undo log inside ibdata1. I have discussed this back on (MySQL Index creation failing on table is full) where I personally worked with a 2TB ibdata1/ibdata2 and someone's query failed with that error even with 106GB of unused space in the ibdata files. That unused space is used for undo logs (128 of them). I discussed this situation even further back: 

STEP 08 : On the Slave DB Server, run You should never encounter this issue any more. Give it a Try !!! UPDATE 2013-04-22 12:45 EDT Please run this query 

You will see all the users that have shutdown privilege. SUPER privilege does not have the shutdown privilege. There is a separate privilege called SHUTDOWN. You can quickly revoke that privilege. For example, note the user . It has remote shutdown privileges. You can yank it with 

This is good to remember if you do MySQL Replication where the Master if MySQL 5.1 and the slave is MySQL 5.0. This could present a really big headache. Replication from Master using 5.0 and Slave using 5.1 works fine, not the other way around.(According to MySQL Documentation, it is generally not supported for 3 reasons: 1) Binary Log Format, 2) Row-based Replication, 3) SQL Incompatibility). Anyway, do a mysqlbinlog on the offending binary log on the master. If the resulting dump produces gibberish in the middle of the dump (which I have seen a couple of times in my DBA career) you may have to skip to position 98 (MySQL 5.0) or 106 (MySQL 5.1) or 107 (MySQL 5.5) of the master's next binary log and start replicating from there (SOB :( you may need to use MAATKIT tools mk-table-checksum and mk-table-sync to reload master changes not on the slave [if you want to be a hero]; even worse, mysqldump the master and reload the slave and start replication totally over [if you don't want to be a hero]) If the mysqlbinlog of the master is completely readable after the top gibberish you saw, it is possible the master's binary log is fine but the relay log on the slave is corrupt (due to transmission/CRC errors). If that's the case, just reload the relay logs by issuing the CHANGE MASTER TO command as follows: 

and that's it. Even if you ran on an InnoDB table, the statistics can get rather stale. By default, is already disabled in MySQL 5.6. If you have MySQL 5.5 and prior, you can disable the update of the statistics by doing this: 

That's it. That makes all the data as of that dump available on Server2 If you wanted to establish MySQL Replication so that everything written on the Master immediately gets done on the Slave, the steps are a little different. For this example, you will need the Private IP address of Server1. You can get that with . Let's suppose the Private IP address of Server1 is 10.1.2.20. Step 01) You need to add this line to my.cnf on Server1 

If nothing appears, then you do not have the RELOAD privilege. Consequently, you have no choice but to contact the system administrators to run for you. ALTERNATIVE SUGGESTION Ask the system administrators to configure max_connect_errors in 

The WHERE should not have a function on both sides of the equal sign. Having date on the left side of the equals sign makes it easier for the Query Optimizer to use an index against it. SUGGESTION #2 : Supporting Index I would also suggest a different index 

From this, the last GTID executed is This means that the binary log that has or had no longer exists. I can see this happening if you stopped replication on the Slave, left replication off long enough for the Master to rotate its binary logs (via expire_logs_days) the slave still needed to see, then turned on replication. In your particular case, try doing a mysqlbinlog dump of the binary log . If nothing comes out of it, you will have to reload the Slave and setup replication from scratch. 

If ARCHIVE does not expose the operation, but allows to secretly do so, this could impact performance greatly. SUGGESTION #2 and can peacefully coexist within an ARCHIVE table. Of course, the one and only exception would be if you insert a new row and SELECT that same row concurrently. SUGGESTION #3 Get a bigger data disk and a lot more RAM. This should give zlib a lot more head room for data compression in RAM before resorting to using the . 

That's it !!! Every time mysql is restarted, the repair is executed. This should be fast since there are only 16 rows. Give it a Try !!!! 

STEP 03 : Locate all users in mysql you want to change EXAMPLE: Suppose the user in question is and you want to changed to Run this 

This solution can be done with mysqldump but with a bit of a risk For the sake of this example, suppose you have the following: 

When it comes to MySQL, you need to search the INFORMATION_SCHEMA database. The following tables will allow you to hunt down a column through all your indexes. 

If id is a primary key, this should go fast. Yet, you do not need to say get all columns in a subquery, and then read on the name from that subquery. You could just craft the SQL as 

This is an interesting question because different databases have unique approaches for providing auto_increment. MySQL : Only one auto_increment key is generated to uniquely identify a row in a table. There is not a lot of explanation behind why, but just implementation. Depending on datatype, auto_increment values are fixed by the length of datatype in bytes: 

Yes, it is by default. Although binlog-do-db exists, I would not recommend it since you are doing Master-Master. If you have additional slaves, it is best to setup filtering on the slave. That way, you have all binlog events to point-in-time recovery and other incidentals. Such incidentals might include Star Topologies (which feature Distribution Masters, a Master that does not house any data, only binlogs Such binlogs could be filter with binlog_do_db to reduce binlog traffic to slaves). 

What you are looking for is the inverse of a GROUP BY aggregate query using the GROUP_CONCAT. If you are willing to store the results in a temp table, I got just the thing. First, here is the code to use you sample data in a table called and a temp table called to hold the results you are looking for.