This is counterproductive, both ecologically and economically, and is also well beyond the capabilities of current robots. Ecologically counterproductive Forests have evolved to live with, and in many cases, to depend on forest fires. Aggressive fire prevention by the US Forest Service and other agencies throughout much of the 20th century resulted in unintended consequences. Many native species need fire to reproduce. For example, the pinecones of lodgepole pines remain sealed shut for years and only open when exposed to the high heat of a forest fire. Decades of active fire suppression led to the replacement of some of those native species by invasive ones. Another unintended consequence was making large fires even larger. Some of the most intense fires happened in areas where small fires had been successfully suppressed for a number of years. The resultant buildup of dead wood, brush, and smaller trees gave fires a way to spread to the canopy level, something that didn't happen so often when small fires regularly burned away that lower level of fuel. Economically counterproductive One of the biggest fire threats to people is those people who insist on living in forested areas and then let the trees grow right next to their houses. The simplest solution: Don't do that then! The Forest Service and others offer education to homeowners on how to make their homes more fire resistant, and in some cases, polices areas to ensure homeowners are following some minimum standard. Another solution, one not yet adopted by the Forest Service, is an approach taken by the Federal Emergency Management Agency with regard those who live in the most flood-prone areas. After flooding one too many times, FEMA essentially forces homeowners to sell their houses at market value to FEMA. FEMA then promptly tears them down and marks the area as uninsurable. This might appear to be a waste of money, but in the long run it saves FEMA money, and potentially saves lives. Texas and some other states take even more drastic measures with coastal properties. A house is condemned once the coast line has eroded to the point that it is closer than some minimal distance of the house. We can't build such robots, yet The terrain where forests grow is not exactly suitable to robots. Much of the forested land in the contiguous US is mountainous. What isn't mountainous is dotted with bogs, swamps, and lakes. The weather is often rather nasty. An autonomous robot that could navigate and survive through such terrain is far in the future. Adding the ability to clean up terrain is even further in the future. Aggressive fire suppression requires clearing out shrubs such as chaparral, which is rather tough stuff, dead and down trees that can be several meters in diameter, and small to medium sized non-canopy trees whose trunk can be up to a meter or so in diameter. This is not the job for little goat-sized robots. This is a job for an industrial-sized robot, and ED-209 won't cut it. 

The two articles you found are spot-on. An isothermal atmosphere is indeed the condition that maximizes entropy for a given amount of energy. Yet a positive lapse rate is almost always observed. Temperature typically decreases with increasing altitude. The key to resolving this apparent paradox is how heat flows into and through the Earth's atmosphere. The Earth's atmosphere is a system that is very far from equilibrium. If the atmosphere isn't in that maximum entropy state (and it never is, at least not from very bottom to very top), heat transfer will set up to put move it toward that state. Very key: That entropy-driven heat transfer rate is rather low. If other heat transfer processes are in play (and they always are), it is very easy to overwhelm that low entropy-driven heat transfer that would nominally equalize the temperature. The Earth's troposphere is heated from below and cooled from above. This alone overwhelms the entropy-driven heat transfer, and by multiple orders of magnitude. The other factor is how heat flows through the atmosphere. The presence of greenhouse gases in the atmosphere set up the conditions that make a positive lapse rate possible. By way of analogy, imagine a perfect blackbody heat source that generates heat at a rate $Q$ and has a surface area A. If that heat source radiates to the dark sky, the equilibrium temperature will be given by the Stefan-Boltzmann law, $T_\text{top} = (Q/(\sigma A))^{1/4}$. (Note: I'm assuming that $Q$ is much, much greater than the tiny amount of heat coming in from the cosmic microwave background.) If you put a perfect blackbody blanket atop that heat source, the blanket will radiate at a rate $Q$ outward and inward. The heat source will receive heat from above and below, making the equilibrium temperature of the heat source with one blanket equal to $(2Q/(\sigma A))^{1/4}$, or $2^{1/4} T_\text{top}$. Yet another blanket makes the thermal equilibrium equal to $3^{1/4} T_\text{top}$, and so on. The atmosphere with its greenhouse gases that are optically thick in the thermal infrared acts as a bunch of blankets. Obviously not perfect blackbody blankets, but blankets nonetheless. This is what favors the development of a positive lapse rate. This favoring isn't universal. There are times when weather creates conditions where the lapse rate is negative, that is, temperature increases with altitude. This turns out to be a very stable condition (the name for this is a "very stable atmosphere"). This negative lapse rate precludes rising and falling parcels of air. With calm air as well, the only thing that can transfer heat is diffusion, which is a very slow process in the atmosphere. 

One way to approach this is to treat the Earth as an oblate ellipsoid. This would mean the errors arise from the uncertainties in the Earth's equatorial radius and the flattening. From Groten, "Fundamental Parameters and Current (2004) Best Estimates of the Parameters of Common Relevance to Astronomy, Geodesy, and Geodynamics," Journal of Geodesy 77:10-11, 724-797 (2004), the Earth's equatorial radius in the mean tide system is 6378136.72±0.10 meters, and the inverse flattening is 1/f = 298.25231±0.00001. The uncertainty in volume due to the uncertainty in equatorial radius is about $3 V \frac{\Delta a}{a}$, where $V=\frac 4 3 \pi (1-f) a^3$, or about 51,000 cubic kilometers. The uncertainty in volume due to the uncertainty in inverse flattening is tiny in comparison, about $V \frac {\Delta 1/f} {1/f (1/f - 1)}$, or about 120 cubic kilometers. That 51,000 cubic kilometer figure sounds like a lot, but it's not. It means the volume of the Earth can be expressed to seven places of precision. 

Not at all. The Arctic (both the Arctic Ocean and far northern land masses) has and will continue to experience greater temperature changes than other parts of the Earth. Other than the Arctic, land masses have and will continue to experience greater temperature changes than the oceans. The warming seen to date is a portrayed below. This image portrays annual temperature anomalies averaged over the last ten years, with anomalies relative to a 1951 to 1980 baseline. 

The backscattered red light doesn't come from the high atmosphere. It comes from the troposphere. The high atmosphere (the stratosphere and above) contains very few particulates. The upper atmosphere is the only part of the atmosphere that remains sunlit above the eastern horizon after sunset. The sunlight hitting the upper atmosphere is Rayleigh scattered, so you see that very faint blue scattered light. You don't see the backscattered reddish light from the particulate-containing lower atmosphere at the eastern horizon because that part of the atmosphere is fully in the Earth's shadow above the eastern horizon. 

That extreme sensitivity to initial conditions means that in theory, a scenario in which a butterfly in Brazil does or does not flag its wings just so are enough to create slightly different conditions that eventually result in tornado hitting Texas. Even if this is the case (and Lorenz did leave that as an open question in hist talk), it's not quite fair to say that the butterfly caused the tornado. The Lyapunov time for the disturbances created by a flap of a butterfly wing is very short. Saying that some event caused some later event else when the two events are separated by hundreds of Lyapunov times just doesn't make sense. What this means is that forty plus years after Lorenz's talk, weather forecasters still can't make an accurate two week forecast, and they may well not ever be able to do so. They can make now a fairly accurate five or seven day forecast, and that was something that was beyond the skills of meteorologists forty years ago. Caveat: Don't believe the forecast by your local TV station. They are notoriously inaccurate. If the US National Weather Service says there's a 100% chance of rain tomorrow, it's best to cancel your barbecue. If your local TV station weatherman says the same, there's a good chance tomorrow will be sunny. 

Source: $URL$ accessed 2 November, 2017. Laplace's dynamic theory of the tides says that the oceans' response to each of the tidal constituents comprises a set of amphidromic systems. Each such system is centered about an amphidromic point, a place with a null response to the component in question. A wave whose amplitude increases with distance from the the amphidromic point rotates about the amphidromic point at the constituent frequency. The $M2$ tidal response is depicted below. The $M2$ amphidromic points are at the centers of the dark blue areas. The white lines emanating from the amphidromic points are cotidal lines, curves along which high tide occurs at the same time. The different colors indicate height, which again increase with distance from the governing amphidromic point. 

Just because something is published in a scientific journal does not mean it is fact. Publication is where science starts rather than ends. Sometimes, pure garbage manages to slip through peer review and get published, even in reputable journals. This is one of those times. Moreover, the publisher of the underlying journal, Taylor & Francis, has had issues with shoddy peer review. The Earth's energy imbalance is 0.6±0.17 W/m2. The Earth's internal energy budget, the amount of energy that escapes from the interior of the Earth, is 0.087 W/m2, about half the uncertainty in the Earth's energy imbalance. (That largish uncertainty is because the imbalance is a difficult quantity to measure.) Even if all of that 0.087 W/m2 is due to humans removing the Earth's insulating layer of hydrocarbons (it isn't), it does not come close to accounting for the 0.6±0.17 W/m2 imbalance. The numbers don't add up. Or as John Russell put it in his response to the referenced article, "This is arrant nonsense!"