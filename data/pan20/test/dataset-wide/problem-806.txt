I'll partly answer your question with the following plot from the ATNF pulsar database (which is easy to play around with). The plot shows the Galactic z coordinate of the pulsars (distance from the Galactic plane in kpc) versus their estimated age (from their spin down rates, in years). The basic picture is that young pulsars are all near the Galactic plane, because they are born from high mass stars that are born, live and die near the plane and even if the neutron star got a kick, there has been insufficient time to move far from the plane. A typical kick might be 500-1000 km/s, roughly 0.5-1 kpc in a million years. Once pulsars reach a million years old, then indeed some reach a kpc above/below the Galactic plane and then a few even older examples reach 10 kpc. Most normal pulsars only show pulsar activity for 10-100 million years (the older ones on the plot are probably a different population of pulsars in binaries), so they aren't observed at much greater distances (though there will be old neutron stars out there). 

The luminosity of Trappist-1 is estimated to be $5.25\times 10^{-4}\ L_{\odot}$, but it has not always been like this. The luminosity of a brown dwarf decreases with time and it this measured luminosity (along with the spectral type) that allows an estimate of the mass and a lower limit to the age using stellar evolutionary models. If I look at the Baraffe et al. (2015) low-mass evolutionary models and look at the locus of luminosity versus time for a $0.08\ M_{\odot}$ star like Trappist-1, you can see that the current luminosity implies an age of $\sim 500$ million years. But if you go back in time, the star was more luminous and for that reason, planets that are currently in the habitable zone (said to be planets e,f,g) were not so in the past. The details of a habitable zone (HZ) calculation can be complex, but basically the radius of the habitable zone scales as the square root of the luminosity. If planets d and h are not currently in the HZ then we can use these as a conservative definition of the HZ boundary. From this (and using the published orbital radii of the planets), I can see that if the luminosity is increased by a factor of 9, then none of the planets b-g are in the HZ, it is larger than all of their orbits. Trappist-1 had a luminosity that was 9 times larger when it was younger than 27 million years old. On the other hand, if I want to move the HZ just outside the orbit of planet e (and simultaneously include planet h inside the HZ), then this would occur when Trappist-1 was at an age of 206 million years. As a final thought, you can see from this particular model that Trappist-1 may fade by a further factor of two as it gets older. This decreases the HZ radius by a factor of 1.41 and would mean that g (and possibly f) would fall outside the HZ, whilst d (and possibly c) would be brought into the HZ. It should be noted however that: different models give slightly different results, these loci are mass dependent and the mass is not known, it is inferred from the same models using a temperature estimate (which is also uncertain). So whilst my qualitative conclusions about the past location of the HZ are likely to be correct (though the detailed age numbers are model-dependent), the future behaviour of the HZ is more uncertain because Trappist-1 may be slightly more massive than assumed and already reached its minimum luminosity. 

A supernova is caused by the sudden (less than 1 second) collapse of the core of a massive (initially more than 8 solar masses in total) star. Less massive stars do not undergo core collapse and do not produce core-collapse supernovae. The thing that determines whether a supernova remnant is a neutron star or black hole is largely the mass of the core at the onset of core collapse, but may also depend on how effectively the rest of the star is blown into space by the supernova explosion and how much material falls back onto the collapsed core. If the collapsed core has a mass of less than about 2 solar masses (and perhaps up to 3, though there is no observational evidence for this), then the collapsed core, consisting mainly of neutrons, can be supported against its gravitational weight and stabilised as a "neutron star". The support is provided by neutron degeneracy pressure and the extreme repulsion between neutrons when they are squeezed together, which is caused by the strong nuclear force. If the collapsed core is more massive, or if more material from the exploding star falls back onto the collapsed core, then even this strong nuclear repulsion will be unable to support its weight. In fact, above about 3 solar masses, General Relativity dictates that no force can possibly support the weight of the compact remnant and it will collapse further to become a black hole. No light (or anything else) can emerge from inside a black hole; so-called Hawking radiation will be negligible; so the only light received from such objects (after any supernova has faded), will be due to material being accreted into the black hole that is heated as it spirals towards it. A third possibility exists, which is direct collapse to a black hole. It is possible that for very massive stars (at least 25-30 solar masses), that the collapsing core is never stabilised even as a proto neutron star, that no bright supernova is produced and that much of the mass of the star directly collapses into a black hole. This is a very active research topic since the discovery of 20-30 solar mass black holes by the LIGO gravitational wave experiment. To summarise, I can do no better than quote from an excellent review paper by Fryer (1999): "Thus for core collapse models we can define three regimes of compact object formation: (1) low-mass, core-collapse stars drive strong explosions with little fallback and produce neutron stars; (2) moderate-mass stars produce explosions, but the fallback is sufficient to form black holes; and (3) high-mass stars are unable to launch shocks and collapse directly to black holes. The question for core-collapse theorists, then, is to determine the limits for these regimes.", where the separation between these regimes is (1) $\geq 8$ up to perhaps 20 solar masses; (2) 20-40 solar masses and (3) more than about 30-40 solar masses, but these boundaries depend on metallicity, rotation and a correct understanding of supernova and neutrino physics. 

OK, I see what you want. What I did was take the revised Hipparcos catalogue and select all stars closer than 50pc. At this distance there should be a complete sample of the types of stars you were interested in. I can plot an absolute magnitude versus colour diagram for these 7096 stars (see below) and use this to select giants and OB main sequence stars. I do this by using data from chapter 2 of Zombeck (1992), I define O and B main sequence stars to have $B-V<0$ and $M_V<5$ (to exclude hot white dwarfs). Evolved giants are selected (a bit arbitrarily) to lie significantly above the main sequence. 

The telescopes in your picture are optical telescopes. They are used to detect cosmic rays using the Cherenkov radiation caused by charged particles moving faster than the speed of light (in air) in the atmosphere. These are normally secondary particles that have been produced in collisions between cosmic rays and nuclei in the upper atmosphere. Radiation produced by particles moving at close to light speed is beamed in the direction of motion. An array of optical telescopes picks up the beam, and the orientation of the image is a projection of the original track of the particle. By combining images taken with telescopes covering a wide area, and taken at almost the same time, one can reconstruct the paths of the secondary particles to work out which direction the original cosmic ray came from. The intensity of the images tells you something about the original energy of the cosmic ray. Possibly this is the answer to your question - a single optical telescope might detect the Cherenkov radiation from a single secondary particle, but would be unable to reconstruct the path or energy of the original cosmic ray on its own - at least a pair of telescopes, or better still, a network is required. The properties of these "cosmic ray" telescopes are also quite different from "ordinary" astronomical telescopes. Cosmic ray telescopes need as much collecting area as possible, because the Cherenkov radiation is faint - but they can do this at the expense of image quality. They use very wide angle cameras (5 degrees or more) and use huge faceted/sgemented mirrors that give massive collecting area, but image quality (maybe 3 arminutes full width half maximum for a point source) that would be completely unacceptable for conventional optical light astronomy. Or (thanks Conrad Turner) you are asking why we cannot use the mirrors of an optical telescope to focus the cosmic rays onto a detector and image their source? The reason for that is simply that cosmic rays are not reflected from glass/silvered surfaces in the same way that light is. They are extremely energetic particles that either pass straight through the mirrors or are absorbed within them. i.e. They do not reflect or refract in the same way as light. In other circumstances, cosmic rays are a nuisance when conducting observations with optical telescopes. They are a source of background noise in CCD detetor images. Cosmic rays (or the secondary particles) are capable of liberating electrons in the silicon and therefore simulating small intense light sources in the sky. Often these are seen as small bright pixels, groups of pixels or trails on the CCD image, confusing what you were originally trying to look at. However, these cosmic rays are not focused by the telescope, they are essentially being picked up by the detector itself and would have been so if the detector was not even attached to a telescope at all. They are nearly impossible to shield against because they have very high energies and your detector needs to have a hole to let the optical light in! 

EDIT: An update. A new analysis by Karim & Mamajek (2016) yields a distance above the plane of $17.1 \pm 5$ pc and they provide a meta-analysis of previous analyses that gives a median distance of $17.4 \pm 1.9$ pc. They also refer to a paper by Schonrich et al. (2010) that gives the velocity away from the plane as $7.25 \pm 0.37$ km/s (though with an additional systematic uncertainty of 0.5 km/s). 

If the mass, radius and total angular momentum of the planet is the same as Earth, then I can put forward an answer. (Of course if the angular momentum is allowed to be different you could have any rotation rate...) First, surface gravity is unchanged since this only depends on the mass and radius of a planet. Second, if the mass is constant then the average density must be the same as Earth. But water is less dense than the average density of the Earth, so the interior of the planet would need to be denser than that of the Earth in order for the average to be similar. If the density is higher than Earth on the inside, but lower on the outside, then the moment of inertia, which depends on how far mass is from the axis of rotation squared, will be lower. Then, because angular momentum (assumed constant) is the product of moment of inertia and rotation rate, we deduce that the rotation rate would need to be faster. NB: Simply covering a larger area with Earthlike ocean would make hardly any difference whatsoever. You need a lot of surface water to make a difference. 

Ashley's comments are all fine. I'll add that homogenisation just means that where you have results from different instruments at the same frequency, they have been averaged in some way. The reference code refers to something like this 1995MNRAS.273..559J That can be used to look up the paper referred to at somewhere like the NASA ADS catalogue. Can you add a sample of the output you are looking at: uncertainty would usually mean it indicates an "error bar" on the flux or magnitude in question - i.e. what is the estimated uncertainty in the measurement. The units are probably just referring to whether this is quoted in magnitudes or Janskys right? 

It depends what parallax uncertainty you are prepared to tolerate. Very Long Baseline Interferometry (VLBI) at long wavelengths currently provides the most precise parallaxes. Parallaxes to bright radio sources measured in this way can have precisions of around 10 microarcseconds (see for example Reid et al. (2014). According to the review by Reid & Honma (2014), the most distant source with a VLBI-based trigonometric parallax is the star forming region W49N. The source has a parallax of $90\pm 6$ microarcseconds and a corresponding distance of $11.1 \pm 0.8$ kpc (Zhang et al. 2013). The precision of these parallaxes is similar to what is likely to be possible with Gaia for the brightest stars (e.g 5-16 microarcseconds according to $URL$ ). Only the most luminous giants will be this bright at distances of $>10$ kpc.