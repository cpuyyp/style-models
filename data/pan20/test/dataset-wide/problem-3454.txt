In the restaurant business, there is always the possibility and a non-zero probability, that a customer gets poisoned by the food. Businesses implement various measures to guard against such an incident, from simple cleaning routines and raw food inspection, to sophisticated mquality assurance procedures. These measures absorb productive resources and so are reflected as costs. These costs are eventually included in the price-list of the restaurant. On top of that, restaurants purchase insurance policies that will cover the indemnity costs when such a risk is realized. These insurances policies are production costs also, and they are eventually reflected in the price list of the restaurant. In the nuclear power plant business, there is always the possibility and a non-zero probability, that some customers, alongside some employees, will get poisoned by radioactive materials, or even be blown to pieces by the initial explosion. Businesses of the sector implement various measures to guard against such an incident... etc. Since the OP is asking about contingent costs associated with uncertain events, the concept of externalities would not be accurate here, since under these concept we classify realized detrimental consequences of production processes that are not, or are difficult to, be priced (natural resource depletion and air pollution being the archetypal examples here). 

Assume a money demand function of the form $$M^d_t = P_tY_te^{-\theta i_t}$$ where $P_t$ is the price level, $Y_t$ is output, $i_t$ is the nominal interest rate. Equilibirum in the money market imposes $$M^d_t = P_tY_te^{-\theta i_t} = M^s_t $$ Forward once, take logs and then differences, to obtain $$\ln P_{t+1} - \ln P_{t} + \ln Y_{t+1} - \ln Y_{t} = \ln M^s_{t+1} - \ln M^s_{t} +\theta(i_{t+1}-i_t)$$ The difference of logs approximates the growth rate. So the left-hand side is inflation, $\pi_{t+1}$ plus the output growth rate, $g_{t+1}$ and denote $m_{t+1}$ the growth rate of the money supply. Then we get $$ \pi_{t+1} + g_{t+1} = m_{t+1}+\theta(i_{t+1}-i_{t})$$ The growth rate of the M2 measure for Money in the USA for the period (end-of) 2013-2014 was around $5\%$ (World Bank data). Meanwhile, inflation in the same period fell from $1.5\%$ to $0\%$. GDP growth rate was at $2.2\%$ Then, according to the above relation, we must have had $$\theta(i_{2014}-i_{2013}) = -2.8\% = -0.028$$ which, given also that the estimates for $\theta$ are below unity, did not happen (it would require a drop in nominal interest rates more than $3$ percentage points, while they were virtually unchanged). So the equation appears too crude... or, it does open the way to break the money supply into two components, one of which does not affect the price level in an economy. This won't be a mechanical decomposition: it will require economic arguments in order to single out which of the channels of money supply increase are to be considered "neutral" with respect to the goods price level, and then measure them separately and test empirically this break-up. For example, the M2 measure that I used above is defined as (World Bank website quote) 

What changes is the macroeconomic identity of the model: from $$F(K,L) = S + C$$ it becomes $$F(K,L) + V = S+ C = (1+v)F(K,L) = S+C$$ where $V$ denotes the value of reparations per period, and the last expression reflects an assumption made in the question, that reparations are a constant proportion of domestic output. "Using the Solow model" means I guess, using the standard Solow model, and so it implies that you don't change its basic assumptions. So there is no distributional issues related to this transfer payment from abroad, while also the savings rate remains fixed, and it is applied to the whole available income (irrespective of how it becomes available). I guess you can take it from here. PS: It appears interesting to also consider a fixed level of $V$, which would reflect a situation where as time passes and the domestic economy grows, reparations become less important in relative size (which is a "reasonable" scenario, something like "we will help more when you are weak, and as you get economically stronger, helped also by the reparations, our contribution will diminish in relative importance") 

We see that it is still a slowing depreciation, which reflects the assumption that capital is more productive in its first years of usage (since depreciation is the mirror image of productive contribution), an assumption good enough for capital equipment with moving parts (and for human capital), and perhaps also for roads, but not necessarily for buildings and things like furniture (here perhaps the Straight-Line method, widely used in Accounting, would be more appropriate -but it creates the need to keep track of capital vintages, since depreciation in that case is a fixed percentage of initial acquisition cost). So it appears to be a quantitative/empirical matter. With $\theta <1$ capital depreciates slower, but the qualitative behavior is the same, and for smaller values of $\theta$ it can even mimic the Straight Line method, which, as said, is realistic for some forms of capital. So yes, we can have $\theta<1$, it does not appear to contradict any fundamental assumption. 

I will provide a simple game-theoretic modelling of the situation. A new year starts and a company wants to make a wage-increase offer to an existing employee. Let $e$ be the employee's current efficiency and the corresponding wage $h(e)$ (which, represents an increase over previous wage). Let $v$ be the premium observed in the market for new hires (so if the employee goes to another employer he will earn $h(e) + v$). Let $c$ be churn costs (recruitment plus loss of efficiency etc) to the current employer, if the employee leaves and needs to be replaced. This is a sequential game so we have to use the extensive form. A) Firm offers a wage First, the case where the firm ($F$) offers a wage and the employee ($E$) decides what to do: 

The "trick" you are referring to is a property of the trace of a product of matrices, namely $${\rm tr}(ABC) = {\rm tr}(BCA)$$ assuming the dimensions are conformable of course. Now, note that the dimension of $(x_n-\mu)^T\Sigma^{-1} (x_n-\mu)$, for each observation, is $1 \times 1$. So trivially, $${\rm tr}\Big[(x_n-\mu)^T\Sigma^{-1} (x_n-\mu)\Big] = (x_n-\mu)^T\Sigma^{-1} (x_n-\mu) $$ But since also, from the mentioned property of the trace, $${\rm tr}\Big[(x_n-\mu)^T\Sigma^{-1} (x_n-\mu)\Big] = {\rm tr}\Big[\Sigma^{-1} (x_n-\mu)(x_n-\mu)^T\Big]$$ combining one gets the alternative expression for the mutlivariate normal log-likelihood. 

The argument relayed in the question is trivially true and also, irrelevant as regards the debate over piracy. At the micro-economic level, piracy has to do with violation of property rights, and so with distribution of output, not with its level (to which the argument in the question alludes). It has to do with the principle 

In theory, the elasticity quantifies in proportional terms the "reaction" of the dependent variable to a change of the independent variable, where the two are related through a functional relationship. So the concept is universal and mathematical -it applies to any univariate function, and not only in the field of Economics (when we have a multivariate function, then partial elasticities can be defined and used, but they face some constraints). So there is nothing "intrinsically aggregate" about the concept. In its theoretical conceptualization. But in order to estimate/calculate the elasticity of any function, we must have the function in the first place. So the question boils down to "Can we obtain an individual demand function?" It would appear that the matter is only one of data availability. If we somehow could have data on many transactions of the same individual related to the same good, then we could use these data points (price, quantity) =$\{(p_1,q_1),...,(p_n,q_n)\}$, in order to approximate its demand function... and we would probably be mistaken. And this is because these points do not necessarily represent movements along the individual demand schedule. They are equilibrium points, points reflecting not the preferences of the individual alone, but their "crossing" with the supplier's behavior. Only if we could be reasonably certain that for a given set of transaction data, the demand schedule have not shifted (due to say, income effects), we could then use these data to estimate the demand function (by the way at market level, this is the classic endogeneity problem: using market outcomes to estimate a relation we estimate neither market demand, nor market supply, but rather the locus of market equilibrium points). So while realized transactions possess the strong element of "truth" (the individual did actually bought such quantity at such price), they do not necessarily reveal to us his demand function. In your case, if you can have other data on the buyer, especially in order to assess any possible "income effects" (for the case you are examining, that would be turnover and/or profit data for a company), you could combine them with its purchases from you to arrive at an elasticity estimation (there are also possible issues with substitution effects, but for them you will have to have data on what your customer purchases from your competitors). Is it a listed company? If it is, I guess you can find quarterly Financial Statements over the web (meaning that you will have to transform the data on purchases to quarterly too). This could take the form of a multiple regression model $$E(\ln q_i) = \beta_0 + \beta_1\ln s_i + \beta_2\ln p_i, i=1,...,n$$ where $q_i=$ quantity purchased in period $i$ (month or quarter), $s_i$ would be the turnover of the customer (assumption = if the customer sells more to its customers, he purchases more from me), and $p_i=$ average price of $q_i$. One could also include Customer's profit data as an additional regressor, in order to control for their possible effect on purchases from us. And of course, any other regressor that you think can have an effect on the purchasing decisions of the customer. The use of natural logarithms, means that $$\hat \beta_2 = \frac {\partial E(\ln Q_i)}{\partial \ln P_i} \approx \eta_{q,p}$$ i.e. it is the estimate of the price elasticity of demand. The above model assumes a demand function that is characterized by a constant price elasticity. You can make it more complex or even non-linear, if you suspect that the elasticity of demand in your case may vary. 

The terms "one-sided" and "two-sided" lag-polynomials, are used when the text considers the option to specify an equation with both "lags and leads", i.e a relation where both past but also future values co-vary with current value. When the lag-operator is "one-sided" it contains only lags in the one direction. When it is two-sided, it contains "lags" in both directions. In some cases, scholars have used the term "forward" operator to verbally contrast it with the "lag" operator, but my impression is that it is not so widely used. One may wonder "so if I read "one-sided lag-polynomial", does it mean that I will consider only past values, or only future values?". The answer is that in almost all cases the invertibility condition holds, so in principle and in theory, it makes no difference. In other words, in principle the term "one-sided lag polynomial" may mean either $$\phi(L) = 1+\phi_1L+ \phi_2L^2+...$$ or $$\phi(L) = 1+\phi_1L^{-1}+ \phi_2L^{-2}+...$$ where $L^{-1}x_t = x_{t+1}$. A specification with a two-sided lag polynomial may appear to violate a basic causality postulate (the arrow of time), but we should remember that estimated relationships are relationships of association, not causation. So when the concern and goal is purely forecasting, "anything goes" -or at least this is what many Time Series Theorists and Practioners essentially say. 

In Kreps' (1988) book "Notes on the Theory of Choice", the issue is dealt with in chapter 9 "Savage's Theory of Choice Under Uncertainty", after discussing subjective probability in chapter 8. As usual, Kreps' style helps: he has the ability to seamlessly inject his -always formal- approach with very down-to-earth comments and examples that are strong in intuition (and he does it better than Savage, I might add). But also, here "formal" does not translate into "complete exposition": he explicitly refrains from formally proving parts of the whole apparatus, mentioning that "this is a two-page proof", and "this is another two-page proof", and "if you want to prove this, good luck". For these parts he falls back on Fishburn's (1970) "Utility Theory for Decision Making" book, chapter 14 "Savage's Expected Utility Theory". And Fishburn is formal alright (more symbols than words in a page). My impression is that combining these two sources can be beneficial. 

Before discussion Shannon's entropy, there is another point that should be discussed: it appears that you have in mind cardinal utility rather than ordinal. "Normalized" utility functions can be derived of course in both cases. But the concept of "relative preference" can be defined and measured only in the context of cardinal utility. And the issue does not arise at the two extremes you describe, but in all the possible intermediate cases. A simple example: assume that there are three "outcomes", $A, B, C$ (say, consumption levels, or three different goods each at some quantity). Your utility function assigned to them the values $$V(A) = 1, \;\;V(B) = 9,\;\; V(C) = 90$$ Under ordinal utility, this just tells us that $$A <_{pr} B <_{pr} C$$ Certainly we can normalize these by dividing by $100$ to obtain $$U_V(A)=0.01, \;\; U_V(B) = 0.09,\;\; U_V(C) =0.9$$ and the ranking of the three outcomes is preserved But under ordinal utility, we could very well use another utility function that would assign $$W(A) = 31, \;\;W(B) = 32,\;\; W(C) = 37$$ and obtain $$U_W(A)=0.31, \;\; U_W(B) = 0.32,\;\; U_W(C) =0.37$$ The ranking is the same so the two utility functions $V$ and $W$ are equivalent under ordinal utility. But in what you are describing, the $W$ utility function represents different relative preferences than $V$ and so it is not the same utility function. But this is meaningful only under cardinal utility, where quantitative comparisons between utility numbers are assumed to have meaning. Are you familiar with the issues surrounding cardinal utility? 

Both books are very good, and they are not competitive but rather complementary. I would say that compared to $CT$, $W$ is a more "traditional" econometrics book, while $CT$ includes whole chapters in broader underlying statistical issues. At the same time, $CT$ are relatively more concerned with the dirty reality of doing applied econometrics with real-world data sets, and how this feeds back to the theoretical models. Regarding your choice, it matters what each instructor does with each book. Ask them. Learn which chapters are covered in each class and compare, to see if they cover the same subjects or not... there must be a reason why there exist two available classes. 

Equations $11$) and $12$) are not derived, they are assumed. Specifically, with eq. $9$) Lucas defines/assumes that the value-expected for the $t+1$ price level is the conditional expectation based on $x_t, x_{t+1},\eta_t$ (plus $\eta_t$ on its own). This is imposing the Rational Expectations Hypothesis. He then says "we will seek linear solutions..." which is translated "assume that the functional form of $E(P_{t+1}| x_{t}, x_{t-1}, \eta_{t})$ is linear in its arguments, and then determine (or estimate) the coefficients". Certainly, this is in all likelihood an approximation to the true functional form of this conditional expectation. So assuming that $$E(P_{t+1}| x_{t}, x_{t-1}, \eta_{t}) = \pi_{4} x_{t} +\pi_{5} x_{t-1} + \beta\eta_{t}$$ and inserting this into $9$) we get $12$) with the mapping $\pi_{6} = 1+\beta$. Inserting $9$) in $10$) we get $$(1+a) P_{t} - a [\pi_{4} x_{t} + \pi_{5} x_{t-1} + \pi_{6} \eta_{t}] = x_{t}$$ $$\implies P_{t} = \frac{1+a\pi_{4}}{1+a}x_t +\frac{a\pi_{5}}{1+a}x_{t-1}+\frac{a\pi_{6}}{1+a}\eta_{t}$$ which is $11$), after compacting and mapping the above coefficients to the $\pi_1, \pi_2, \pi_3$ ones.