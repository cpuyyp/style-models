So they do restore the way they're supposed to but I'm here wondering, how come they vary in size, I can totally understand the change after the compression but after (?). Why would the backup behave in this way? Any known reasons? Additionally: Nothing really going on in the logs. 

I'm not very clear on what you mean here, if what you mean is to have an Access frontend where you collect data and you want it to 'send' this data also to an SQL Server database (which sounds very eclectic, btw) you can use VBA for that. Heres a discussion on the matter that would do just that: VBA to Append Access data to SQL Server Table If this is not the case then you'll need to update your question with more info. 

A way of knowing why is this error happening would be to check the SQL Server log and look for details into why is this happening: Try to login again with SQL Server Authentication, after getting your Error 18456 change to Windows Authentication and in your Object Explorer go to: Management -> SQL Server Logs -> Current - XX/XX/XXXX XX:XX:XX Now search for your newly logged error and identify the error State With this State number assest the nature of the error using the following table: 

SSIS does comes with SQL Server evaluation. Additionally if you're a small company or a start up you could take advantage of the Bizspark program and test the full version throughout and/or develop with it for 3 years. And to spice it up a bit this article can give you an additional input on the subject before you jump in: The Hidden Costs of SSIS 

I had to work around a similar issue where the two locations where 13,000 kilometers away and the internet connection on one of the locations was 1995 level of poor: 

This other one when down a bit and then further down in size and finally got bigger than the first time it backed up with compression. 

I tried for a bit to replicate your scenario and finally I managed to effectively send an email to myself using SMTP, but I had to create an SQL CLR stored procedure as follows: 

I have a table holding 'People' data (about 70K records) that coexists with an 'Address' table so there's one per every person in the 'People' table. The idea was to centralize the data in this single 'Population' database, problem started once duplicated records started to appear due to a poorly executed Import process (Data coming from different sources), this left the table with as much as 7 records for a single Person. Additionally the Address table started collecting 'all sorts', so let say I live in "24 Wickam Heights" you can found this address in the following ways: 

I just came across a somewhat old database (and frontend) and it has funny way of dealing with the aspect of unique ids. I've got one table with a single column and row storing an Integer (currently 31448). This number is used on an table as 'InvoiceNo', the table also has a unique auto-inc id (currently 2847) It looks a bit like this: 

Completely forget about the "user-centric" model and think to a lower level, Individuals. Individuals can have any number of Accounts and Accounts can have any number of Roles. Individuals can be represented on any number of abstractions around the database or even across databases. What do you think? 

No actual relationship between the two tables other than the InvoiceNo which is not actually an enforced relationship. The frontend then, when the user adds a new invoice performs the following: 

I don't think you should make tables to represent Roles (User, Manager, Seller, etc) I think you could implement something more scalable like this: 

Which is nice and easy to use for generating a chart, for example. But as I mentioned earlier I started to look into this sort of query and I believe they can turn into something more compact as in a cycle. I'm aware of the old time If its not broken don't fix it so any input in the matter would be appretiated. So far I'm stepping into something around these lines: 

This will output: , and again, the final number would be 1 instead of -1 unlike described in the question. Hopefully the OP will pop along to clarify a bit more. 

This is not appealing to me since I tend not to trust the user input, I want more control thus being able to trigger an error and not importing any part of the file would be better. I'm not sure if this effectively wipes the possibility of using for this process? I also tried reading line by line and inserting using a cycle, this is effective but unfortunately its takes to long even after optimizing my code I just went from 15 minutes to 4, and that's only for 850 records. I'm a bit mad that the process worked so smoothly when running from the Access frontend. That was because of the VBA method. This allowed for fast XLS import into a temp table and from that one the execution carried on updating tables. I would like to know what other options have guys tried when importing data from flat file sources into SQL Server. Admittedly I'm a bit confused at this point. Any help would be appreciated. 

You could deal with such scenario by using Descriptive Metadata tables, have a look at the Wordpress database diagram. As you can see it makes use of three metadata tables that can store any number of parameter per Post, Comment and User. So in your case a bare-bones implementation could work like: 

In some cases as bad as over 20 different versions of the same street... The most beautiful part is that the data from this database is being referenced from at least 5 other databases in the same server, making every change a very risky process. So I'm thinking, what steps can be taken to get rid of the duplicates?, What alternatives there is for avoiding the address table accumulating such an amount of data inconsistency?. Maybe even to ask, is there any salvation for such a big mess? It really is a nightmare. 

I feel that you could add an additional Entity in there, one that represents people as a part of this (school-like?) institution, call it "People" (yeah...) So , , , , etc are all People. That table will hold Attributes that the sub classes/entities will share (because they're all people, right?): Firstname, Lastname, DoB, etc. You then refer to this table on your other entities. Eg the by using the FK (or whatever you call it =) A student is then able to become a tutor and even a Teacher or a Head of Department at some point without being added to simultaneous tables within the database. To, then, define a relationship between your , your and your you'll use a junction table in between that could look like: 

What's wrong its trying to copy such amount of data (straight from a table, mind you) to the Clipboard. But since this is Access we're talking it wouldn't shock me that's suggested over Exporting the data safely into a format like .CSV or Even better .SQL Is funny that you mentioned the system architecture because on first sight I would check that as possible cause. I would highly suggest changing the approach into something more around this lines: $URL$ 

So I went for a cycle that loops through the rows adding up correlatives by two and storing the result in a variable, then, and depending on the result of the result variable (Positive, Negative) it would either progress by using the last number (positive) or retain the current one (negative) for the next operation.