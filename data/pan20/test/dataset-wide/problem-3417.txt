In basic intertemporal models the general assumption is that the objective utility function does not change. mathematically it makes the model simple to compute, however it does not reflect the changes in preferences over time. However, if we do allow the individuals utility function to change over time in our model, does this not violate the requirements for a rational consumer ? How do we model an individual with changing preferences over time and is it economically sound to do so? 

I'm currently reading up on matching models in terms of their ability to describe frictional unemployment, they generally follow a cobb-douglas form. $$M(u,v)=m=\mu u^\alpha v^{1-\alpha}$$ Where $M(u,v)$ or $m$ is the number of employed, $u$ is the number of unemployed, $v$ is the number of job vacancies, $\mu$ is a constant and $0<\alpha<1$. moving around the formula we obtain vacancy rate. $$v=\left(\frac{m}{\mu u^\alpha}\right)^{\frac{1}{1-\alpha}}$$ Economterically estimating job vacancy would seem to be much more difficult for countries which do not have a history of report it.This is mainly due to omitted variable bais, which would skew our estimates for $\alpha$. How does one estimate job vacancy in countries that only report employment and unemployment? 

There's a difference between "free software" and "free software for students studying economics". For example, when I was studying economics at University, the University provided (free) access to software such as Maple, MATLAB, Datastream, and Stata, to mention just a few. These software packages are not free, but to economics students at many Universities, there is free access to all of this software making it free, effectively (to the economics student). As a result, I'd recommend that economics students consult with their instructors to find out what software is being made available to them. The University doesn't pay the licence fees just for kicks. 

In the case of a firm which has a production function with one input and one output where $p=w$, such that $p$ is the price the output is sold and $w$ is the price of the input across all production possibilities sets. Can such a firm be called profit maximising at any level of output? 

The second regression I ran was a regression of the change in revenue on change in price. Giving the formula: $$\Delta y_{Revenue}=\alpha_0+\alpha_1 \Delta Price+\alpha_2 \Delta Price^2$$ 

Throughout the course load of an economics student's undergrad we are exposed to the concept of cardinal utility via utility functions which give us a specific quantifiable number of "utilis" as a result of our inputs which represent consumer goods. Practically speaking though, what type of data is used here and what methods are employed to accurately measure utility in a cardinal sense? 

The point is concluded pointing out that too much detail (i.e. advanced mathematics) can distract from what's really important in the wider context: 

The above quote marks the problems that arise when mathematics is not used in economic modelling. Walras makes a further statement with regard to mathematical language, offering a critique to his predecessors. He says: 

However, Marshall provides a more muted opinion about mathematics being a necessary output of economic modelling: 

The question about using mathematics to model economic phenomena is a rather old one dating back to at least the marginal revolution (19th Century). A couple of the key figures of this era were William Stanley Jevons and Leon Walras. I don't aim to provide a complete, or terribly accurate, history, but let's just focus on Walras. In the preface to his Elements of Pure Economics, Walras paints a picture that suggests mathematics is a necessary input (basic ingredient) of economic modelling. He says: 

It seems to be still profitable for the future to come this is explained in The Economics of the Audiovisual Industry: Financing TV, Film and Web (its also a free ebook on Kobo). This is because there are a number of venues which audiovisual products can be enjoyed. Meaning, its a completely different experience when watching a movie in a movie theatre than on a laptop, this is referred to as Product Differentiation. Although the industry is hurt by illegal downloads and piracy, it has not made the whole industry profitless as there is still demand for the "movie theatre experience". As long as that is there, profit can still be made from making movies. 

If we now fast-forward to the present day, we see that mathematics is everywhere in economics. Thinking of doing a PhD in Economics? Well, mathematics (and statistics) is going to be served up first - no question about that. So, what can go wrong when using mathematics in economic modelling? The main problem, I believe, is not being able to see the woods for the trees. This point is illustrated in Klemperer (2003) Using and Abusing Economic Theory. In this paper, Klemperer gives the following example: 

Lastly, the OPs question could also be addressed from the viewpoint of econometric methodology, but I'll omit that aspect because there are numerous controversies in econometric modelling. Names to look up would be: Clive Granger, David Hendry, Edward Leamer, Lawrence Klein, Christopher Sims, to just scratch the surface! 

How would one go about modelling "supply induced demand" (SID) using the regular supply and demand model and how is SID different from price discrimination? 

The classic formula for where a monopolist produces is where marginal revenue equals marginal cost. Is there a similar formula for a monsopsonist? 

As for the second point $Q_d$ is $Q$ because we can only have a single quantity supplied and demanded in equilibrium. 

I definately reccomend Introductory Econometrics for Finance by Chris brooks. a little more finance focused in its case studies, however it is an excellent primer to forecasting in economics. Builds up from univariate modelling methods to VARs and VECMs, it has lots of case studies. I found it very useful. 

I think the main alternative to the Johansen (statistical) approach is the methodology propounded by Pesaran and Shin in their so-called long-run structural modelling (economic) approach. The main formal reference is Pesaran and Shin (2002). The methodology is also presented to a wider audience in Garratt et al. (2012). Although referred to as the long-run structural modelling approach, you will also read about the type of models associated with the methodology, which are called VARX* models. The distinguishing feature is that the cointegrated models are estimated using reduced-rank regression and (over-)identifying restrictions are derived from economic a priori then tested. The sequencing is slightly different to Johansen since economic theory is given priority. In the same school of thought is the cointegrated VAR approach associated with Juselius (2006). Again, Juselius propounds an economic approach to cointegration as opposed to the statistical approach by Johansen (and others like Phillips). Pesaran's approach has been programmed and is available in the Microfit software. Juselius' approach has also been programmed and is available in the RATS (CATS) software. To my understanding, you can find MATLAB code in the GVAR toolbox, and this should give an indication of what's required for the Pesaran approach. In terms of programming, I haven't seen much difference when compared to Johansen (although I just quickly investigated this). Note that the GVAR approach can be thought of as an extension to the VARX approach (the difference lies in stacking individual country models, but estimation of the individual models is the same). References Pesaran, M and Shin, Yongcheol, (2002), LONG-RUN STRUCTURAL MODELLING, Econometric Reviews, 21, issue 1, p. 49-87. Garratt, Anthony, Lee, Kevin, Pesaran, M and Shin, Yongcheol, (2012), Global and National Macroeconometric Modelling: A Long-Run Structural Approach, Oxford University Press. Juselius, Katarina (2006) The Cointegrated VAR Model: Methodology and Applications (Advanced Texts in Econometrics). GVAR Toolbox 

Theres a book called Models In Political Economy: A Guide To the Arguements by Michael Barratt Brown which provides a basic overview of the many economic schools of thought. There is a whole chapter which discusses this whole concept of feminist economics, however there is one section which helped me understand the point of view. 

Well the problem can be dodged by the use of some simple solution of just adding a constant to all values which circumvent issues of taking logarithms of negative numbers. Usually the mean is used as a centering point. If the mean is negative though some other number must be chosen. Interpretation of such a transformed variable would be different from its regular logged counter part. These are just some hunches on what could have been done. Its odd that methods aren't mentioned for this case. 

Let's begin by fleshing out what the EViews code is saying. The EViews code says that the variable CLOSE depends on its first difference and there is also an ARMA(3,3) error term included in the model. Supposedly, the ARMA error process is to whiten the residuals, which can be, and has been, argued against as a modelling strategy. It's also worth noting that there is no constant term in the equation. The reference to BACKCAST indicates how the MA process is initialized - the details of which can be found in the EViews documentation. ESTSMPL typically refers to the estimation sample. Having understood the EViews code, the question is now: how does one estimate a linear regression model with ARMA errors using MS-Excel? The easiest solution would be to find an existing add-in that provides this feature. An alternative method would be to write the code yourself using the MS-Excel programming language (VBA). Beware though, what makes this application tricky is the MA term; AR processes are easier to program and more common in certain fields, e.g. VAR models appear more frequently in economics than VARMA models. Another approach, if you are willing to delve into some code, would be to look at some R source code (either base code or from some time-series package) to get a sense of the task ahead if you do decide to program from scratch. Programming this from scratch seems like overkill, though, as this sort of estimation has been around for decades. Why bother if all you're interested in are the estimated parameters and not the underlying routines? It doesn't answer your question, but my advice would be to use R if EViews is not available to you. Or, seek out an MS-Excel add-in that will do the job for you. A useful thread from the EViews forum is here: $URL$ Note: "Estimation, and forecasting of MA processes is complicated, especially when backcasting is used to obtain starting observations for the error terms." Good luck! 

The reason why the profit maximising condition is like that is because the monopoly is Price Discriminating between those who are willing to pay at $P_1$ and $P_2$ for $Q_1$ and $Q_2$ respectively. This increases the area of producer surplus to an area greater than what it would be by charging $P_1$ and $P_2$ alone. 

The paper computes the value of your coefficient $\gamma$ on its own with comparison to $\alpha +\gamma$1. The interpretation in this context, would be that there is a limit of the returns to RightShare given the presence of observations existing contemporaneously with RightPM and InRange.