I have MySQL 5.5 running in RHEL. I have checked for my.cnf file in my filesystem and its available in two destinations. One in the user's home directory and the other is in /data/etc_ORIG directory. It was not seen in /etc or /etc/mysql folder. Is there any issue with the config file present in a different path? And how do I check which config is being used by MySQL? 

I have enabled default auditing in one of my database. Oracle 11g. Audit_trail is set to DB, other than that for a particular schema 'test' I have set: 

I have a OS user 'test' who doesn't belong to dba group or oinstall group. I need to access the database using sqlplus but I'm unable to access the sqlplus even after setting oracle home ans path and tnsnames. Any solution? 

Telnet is working from the local machine to the target (scan is in dns) nslookup from local machine is also working. It is resolving the IPs. Checked with it is listening and port is open. Tried connecting using Basic connection and as well as TNSNAMES.ora using instant client in my Windows machine. I get this error. While trying sqlplus from my local machine, it gives me 

If I enable --log-slave-updates in my active master also, consider the case of a disaster where in my primary master has a issue, then I will make my passive master the active one and point my application to my new master. Now my new master will be generating binary logs which will be waiting to replicate to my actual master (which is down now). SO what happens when my actual master comes up? Binary of my secondary master gets copied to my primary without any issues? 

Oracle 12c database comes with default Unified auditing enabled even if we set it to FALSE which has ORA_SECURECONFIG policy enabled by default. Apparently, the audit options for that particular policy captures many default options such as Alter and create statements. My question is, by default for any new installation, mixed mode is enabled which means that the traditional auditing is enabled and also Unified auditing is enabled as well both capturing the same audit_options. Why is it that way? What is the point of Mixed mode? When by default tradionally Oracle audits certain user actions, why should ORA_SECURECONFIG should also audit the same default user actions and write to AUDSYS schema? And if I need to keep only Unified auditing, then how to disable traditional auditing? Should I change audit_trail from DB to NONE? 

In what way I can make my slave to be in sync with Master in such conditions? Why does the slave lag here? 

I'm planning to go for a 3-node active-active MySQL cluster which will not have more 100 databases. Many experts here have already told there are issues with MySQL 5.6 and 5.7 where circular replication happens which is very vulnerable due to increase in failures, in what way Percona XtraDB provides edge to MySQL cluster? Percona doesn't do the circular way? Just trying to understand. The application which i'm going to use is not very much resource intenseful. What is the best option for 3 node active-active cluster? 

While trying to install Mariadb galera cluster through yum in my RHEL server I get the following error: 

The reason for the above mentioned movement of backup pieces is necessary for my OS team for some activity at their end. My question is that, in this scenario, will my control file lose the information of older backups? Do I need to do catalog start with the /orcl_bkup/old_bkup and catalog the backup location to delete the obsolete backups? 

AC does not require any changes on my application code. This is one of the reasons i'm going with AC because i'm using thin drivers where I cannot use TAF and I do not want FCF as that requires application code changes making it RAC aware. I believe all that is required is to only create a service for AC on my RAC server. 1.(a) Does enabling FCF add advantage in addition to AC? 

I have many databases for which I have configured data guard setup. When it comes to monitoring the standby's, I use OEM to check the lag and I have enabled mail alerts as well as and when gap is more than 50 archives. But the thing what I recently observed is that in one of my standby databases my recovery got stopped for one or more reasons and unfortunately lag also did not go above 50 so there was no alerts. In such a situation how I can ensure when my standby's recovery MRP itself is not active and I should be alerted in such a situation.. How is it possible? Any soultions? 

Table 2 has 200 million rows. Though the number of insert records was only 5000 but the operation lasted for 30 mins. I observed replication lag during the insert operation. I have load infile disabled due to security concerns. Hence I can't insert using that as well. I went this article from Percona which says that this can be resolved if txn isolation is used as ROW and versions above 5.1 that this is resolved. 

I have a SQL server A and B which is production databases. The DR of production 'A' is production db 'B' and DR of production db 'B' is production db 'A'. DR feature that is used is log shipping. All in the same location. Now, what are the very critical issues that I could face in case of disaster? 

I have Percona Server 5.6 running in linux platform.. recently I faced an issue where mysql instance was continuously restarting for no reason.. mysql logs had this error: 

I'm running an two node RAC database with ASM having two DGs one for data and one for redo. I have only one copy of redo and I have the need to have another copy of redo logs. 1. How to go ahead with adding mirror copies of redo? 2. Will it demand downtime? 3. Can I add the copies which I'm going to create in the data DG? 4. I also need to create a copy for the control file. How can I do that as well? Thanks, Tesla. 

Why common schema is not available in MySQL 5.6? What is the significance of it? So If there is a need for common schema for an application, what is the workaround to have common schema in 5.6 or should I go back to 5.5? Also, Can I download the common_schema from code google and install it in 5.6? Are there any implications as such? 

I have daily full backups going on for my Oracle 12c databases on linux. Daily retention on disk is 6 days. This is what happens: Full backup happens at location 

The above log entries is getting updated as and when instance is restarted.. the instance continues to restart every 4-5 seconds. Help required here.! 

Is it possible to separate out data files and log files of MySQL from one drive to another? For eg: Data (actual database data and system tablespace ibdata) in one partition, say /data and log files (ibdata0 and ibdata1 only) in one partition, say /redo? Is it possible? I know the binary logs can be moved to a different partition but I have the doubt for above scenario. I'm using Percona Server 5.6 on linux. Edit: Just checked out, If I set innodb_log_group_home_dir to a different location, would it work? 

I have an Oracle 12c database running in OEL 6.. yesterday due to some OS related issues, my Oracle home software mount-point got erased completely. The only thing that is left is the pfile and data files. I have got that. I do not have backups as well. What are the options I have got? If I just install the New Oracle home and start with the pfile and with the data files, will I be able to start the database as it is? 

I have a requirement from one of my teams which is database copy (cloning) from production to my test server on daily basis at a particular time. I know this is a very simple process which every DBA would perform. Should I use RMAN duplicate for cloning? Should I use Goldengate? Is there any way I can schedule the same without manual intervention? Server - RHEL Database - Oracle 12c/11g 

I have two SQL Servers 2014 in AlwaysOn AG setup (OS - WSFC2012R2) with secondary acting as a synchronous replica. I'm facing an issue in the role switchover on daily basis. Every day morning around 7 or 8AM, due to some reason Primary fails over to the secondary. Ideally, this happens due to network glitch. But i'm unable to find the root cause behind this failover. This happens on daily basis. Is there any way to narrow down the issue which causes the failover? 

So I have an Oracle database (standalone) running in RHEL and I have scheduled RMAN for backups on daily and weekly basis. Sometimes my file system (archive mount-point) becomes full because of high archive generation. Now what is the right way to delete the archives from the file system? My RMAN script has this 

What is the difference between instant client and the original Oracle client? The Oracle client is almost 800MB and the instant client is not so much. I need to install oracle client, ODBC drivers and JDBC drivers. What is the best way to go and which one should I choose? The original 800 megs client or the instant client? 

I have a file system for my archives which gets full in a short span of time. I want RMAN to backup all the archives automatically and delete the archives after backing it up when the archives mount-point gets full. Is that possible? 

I have few doubts regarding MySQL replication (Master-Slave). Is it mandatory for a table to have primary query for replication to function properly? Referring this Percona link for the above question as it mentions that If there is no primary key or unique key defined then it’s even worse because INSERT may be re-executed and you will get multiple rows with the same data – which again means you’ve got inconsistent data with the master But with InnoDB as the storage engine, if a table does not have primary or a unique query defined, the engine itself creates hidden clustered index on a synthetic column containing row ID values as per Jeremy Cole's blog So, even in the case primary or unique not present, the replication should not have any impact it in itself creates a clustered index which should ensure the replication is smooth, correct? I'm not sure on this part. Would be great if someone can throw some light on the need of Primary key in Master-Slave Replication setup. 

I have 5.5.44-log MySQL Community Server (GPL) running in one of my RHEL 6.4 server. I need to upgrade the server to Percona Server-5.6.29-76.2. How to go about the upgrade? Should I completely remove the MySQL 5.5 binaries and then proceed with fresh installation of Percona? If not, how to proceed for the same? I have some 5-6 databases running on the instance with sizes less than 1GB. 

One of my developers is trying to perform insert into a table by selecting from a table. Below is the query that was run. The query takes extremely long time for execution (4hrs odd). Number of records on the table where it is getting imported is 1.2 million. Database - Oracle 12c on Linux with archivelog mode on. 

I have a requirement to fetch all the schema available in 100+ Oracle instances running in RHEL environment. All the schema information such as username, roles assigned and is to be dumped in one place from where it will be used for other purposes. Is it feasible? I can provide more info if this question is sounding vague. Are there any utilities to perform this? 

Please note I have not given scan exclude and scan order in both the nodes inspite of this being multipathed disks, yet the same is getting mounted in node 1 but not in node 2. What could be the issue? 

I have a table 'class' owned by the schema 'test' and I need to audit all operations on the particular table. -- will this be sufficient to do the trick? Should I enable any other kind of Fine grained auditing to capture all the details. Oracle12c/RHEL - I have set audit trail to DB. And sys operations to true. Inputs needed here 

While I connect to the database using the service which I created through sql developer and I run the below query it says it is connected by service which I created but failover_type,failover_method,failed_over is NONE Why is it that way? 

I have 6, two node RAC databases running in the two physical servers. While I try to create new diskgroup, I get the following error: 

But no-luck! Does sql-developer needs any tweaks for connecting to RAC scan-ip's? I have tried all the possible the answers in SE and googled. Thing is i'm trying to connect to the database through one PC but not every PC. What could be the possible issues? How to go about debugging? 

After each day's full backup gets over, I make sure that the above mentioned path only contains that day's backup pieces in that path and the older backups will be moved to another folder 

Currently I have set log_bin_trust_function_creators=1, and it doesn't take in effect after restart. 

With the above setting, I know I can get to know what action has been performed (alter,create,update,etc) but will I be able to fetch the SQL statments (all DML operations) with the exact SQL statement fired from that schema from the audit records? Let me know how? If the above is not possible, what is required to fetch the exact SQL statement that is run by the schema. 

I would like to store the database size for a particular database on daily basis on to a table which would help in understand the growth of the database over a period of time. 

I'm setting up a master-master replication where in the second master is only read only there by reducing the issues of conflicts. My doubt is that to reduce the downtime when my master goes down I would be pointing my application to the second master (which is in read-only) and continue the work. Is it possible to have a DNS CNAME record pointing to one of your master. In case of failover, just update the DNS CNAME to other working master while you fix the broken one... once you fix and bring up the broken master, it will catch up with the transactions. If yes, how should I do it? 

I have two databases - one is 11g and another 12c. 12c db has the exact data of 11g (expdp/impdp). Now I'm facing an error in one of the tables with unique constraint error. I have checked the structure of the table and constraints enabled for the table in both the databases, that is same as well. Now how can I sort this out? I'm facing this error on the application which is running on it, when I click one of the tabs. is the error i'm getting 

I have set sql.expire_time parameter to 10 which recommends me to keep in my two node RAC database. At times, my application gets the error ORA-3135, therefore I suspect that this parameter could have some role to play here.. What exactly does this parameter do? Does it have anything to do with the error im facing? I do not have any firewalls between my database and app server. 

I'm running my MySQL backups through cron scheduled every night, but the thing is in cron the username and password is given for it run, is there any other way I can securely run the backups? where I can credentials remain secure only to the DBAs and not to anyone else? Btw, I use percona server. But I use mysqldump for backups, just that it has to be secure. Thanks! 

Requirement is that I need to clean up my database in which my application has extensively tested on, and now I will have to clean up the data and have a database which will be ready to go live. By clean up, I mean the metadata of the tables, users will stay but their data will have to cleaned. What is the fastest and the best method to go about with? Truncate? Any help would be appreciated. 

But since this creates a read-lock in my 2nd master (Slave 1). The changes that I make in Master 1 is not propagated to Master 2 (Read-only). What should I do such that - changes in Master 1 are getting updated to Master 2 but Master 2 should not be in write mode, Master-master(Active/Passive). Is it possible to do this? Regds 

Is there a way to backup a Oracle 12c database which has a missing datafile? I dont have backups. Database is running in archive log mode. One of the data files is physically missing from the disk. But querying dba_data_files, it shows up as a datafile and when I try to backup using RMAN or expdp, it throws an error and fails. What are the options I have got here? Oracle12c/RHEL 6 

Why is it not possible to rebuild corrupt LOB indexes? So if my LOB index is corrupted, how to recover it? 

In the link mentioned here --> $URL$ it says there is increased virtual memory footprint when running my applications in conjunction with Instant Client.! Why is that so? If that is the case I would rather install the native Oracle client which takes time to install but still end of the day performance matters right? Why should I prefer instant client when there are memory leaks with it?