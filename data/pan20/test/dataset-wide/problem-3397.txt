The marginal cost is the same for any firm in a perfectly competitive market at equilibrium. Now, let's prove it. Suppose there is at least one firm (firm 1) that has a higher marginal cost, $MC_1$, than the remaining firms, at $MC_0$. The price, $P$, is the same for every firm because goods are homogenous. Rational consumers with perfect information would never buy the same good at a higher price. As firms maximize their profits and the technology does not have increasing returns to scale, their margional costs must be equal to the equilibrium price, $P=MC_0=MC_1$. This contradicts our initial assumption $MC_1>MC_0$. We have just shown our result. An interesting case to get a complementary intuition is the Bertrand competition. Suppose the technology has constant returns to scale. In other words, producing a quantity $Q$ for firm 1 costs $MC_1 \times Q$, the marginal cost is constant. Imagine many firms able to produce the same homogenous good but with different marginal costs. As consumers would go for the cheapest price. Firms with the lowest marginal cost (the most efficient firms) can prevent the other firms for making a positive profit by setting a price low enough. The less efficient firms would leave the market and only the firms with the lowest marginal cost would exist at equilibrium. The equilibrium price would be equal to this lowest marginal cost. The result holds because of the same assumptions: the ability of consumers to choose the cheapest good, homogeneous goods, enough market competition. 

Second, identifiability does not exactly define whether an equation is structural or not. I would rather say that identifiability is the reason why we are forced to work with nonstructural equations. Parameters in structural equations cannot be identified most of the time. What can be estimated instead are reduced-form (or nonstructural) parameters. For instance, take the following simulatenous equation model \begin{align*} x_i =\alpha_0 y_i +\alpha_1 z_i +\epsilon_i\\ y_i = \beta_0 x_i+ \beta_1 z_i +\mu_i \end{align*} with two endogenous variables $x$ and $y$, an exogenous variable $z$, and $\epsilon$ and $\mu$ as error terms (suppose also $\alpha_0\beta_0\neq 1$). These two equations are structural. They may correspond to demand and supply equations of a given good, with $x$ the quantity and $y$ the price for instance. Without more assumptions, the coefficients are not identified and cannot be estimated. We can however estimate the parameters $\zeta$ and $\gamma$ of the reduced-form equation: \begin{align*} x_i = \zeta z_i + e_i\\ y_i = \gamma z_i +u_i \end{align*} with $\zeta=\frac{\alpha_1+\alpha_0\beta_1}{1-\alpha_0\beta_0}$ and $\gamma=\frac{\beta_1+\alpha_1\beta_0}{1-\alpha_0\beta_0}$. In the second paragraph of the extract, the point is that identifiability (of the structural equations) is not necessary "as long as the structure remains unaltered". For instance, you can well predict the values of $x$ given the knowledge of $z$ and a good estimate of the nonstructural parameters $\zeta$. The last sentence illustrates the fact that, in our case, i) we do not need to know the causal structure to make a good prediction, ii) it does not help to distinguish between structural and nonstructural equations, and iii) we do not care about identifiability of the structural model. 

First, you made a sign error in the computations. After correcting for your error, a crucial hypothesis you miss is that $X$, the choice set, does not depend on the variable $t$ in the theorem (with the notations of the theorem). To apply the theorem properly, the interval $[0,L]$ should not depend on $L$. A) The sign error $$\frac{\partial H}{\partial L_K}=-\alpha (L-L_K+1)^{-1}H+(1-\alpha)(L_K+K)^{-1}H=0$$ We define $L_K^0= (1-\alpha)(L+1)-\alpha K$. B) Why we could think the envelop theorem's result may fail Presuming that $0<\alpha<1$, there are four possible cases. (1) $L_K^0\in [0,L]$. One can check the objective function is concave, thus $L_K^*=L_K^0$. (2.i) $L_K^0\notin [0,L]$ and $H(L,0,K)< H(L,L,K)$. Then $L_K^*=0$. (2.ii) $L_K^0\notin [0,L]$ and $H(L,0,K)> H(L,L,K)$. Then $L_K^*=L$. (2.iii) (just to be exhaustive) $L_K^0\notin [0,L]$ and $H(L,0,K)= H(L,L,K)$. Then there are two solutions, $0$ and $L$. In case (1), $$\frac{\partial F}{\partial L}(L,K)=\frac{\partial H}{\partial L}(L,L^*_K,K)+\frac{\partial L^*_K}{\partial L}.\frac{\partial H}{\partial L_K}(L,L^*_K,K).$$ The second term of the right-hand side is equal to zero thanks to the first-order condition. This is compatible with the envelop theorem's result for an interior solution. In case (2.i), $F(L,K)=H(L,0,K)$ and so $$\frac{\partial F}{\partial L}(L,K)=\frac{\partial H}{\partial L}(L,0,K).$$ This is compatible with the envelop theorem's result for a corner solution here. In case (2.ii), $F(L,K)=H(L,L,K)$ and so $$\frac{\partial F}{\partial L}(L,K)=\frac{\partial H}{\partial L}(L,L_K=L,K)+\frac{\partial H}{\partial L_K}(L,L_K=L,K).$$ We have to be cautious about the notations here, $\frac{\partial H}{\partial L}$ means the partial derivative corresponding to the first argument, and $\frac{\partial H}{\partial L_K}$ to the second one. The second term of the right-hand side is nonzero, which does not fit with the envelop theorem's result. C) Why it actually does not fail Write the problem as $F(L,K)=\max_{x\in [0,1]}H(x,L,K)$, with $$H(x,L,K)=(L-x L+1)^\alpha(x L+K)^{1-\alpha}.$$ This problem is equivalent to the initial one. The key difference is that the interval $[0,1]$ does not depend on $L$ or $K$. This is the reason why we can apply the envelop theorem, whereas it was wrong to apply it before. We can check that the case (2.ii) is compatible with the envelop theorem, we have $F(L,K)=H(x=1,L,K)$ and so $$\frac{\partial F}{\partial L}(L,K)=\frac{\partial H}{\partial L}(x=1,L,K).$$ 

The hint is to use the recursive Euler equation to express all future consumptions $c_{t+j}$ as a function of current consumption $c_t$, and to play with the indexes in the sum of the left. 

Economics in undergraduate and graduate programs already contain a non negliglible amount of maths. Mathematics are at the core of modern orthodox economics. I would propose you to have a look at the general textbooks for undergraduates/graduates in economics. You will find an extensive use of maths in some of them. If you are interested in theory Microeconomic Theory (by Mas-Collel, Whinston and Green) is a masterpiece. Introductory Econometrics (by Woolridge) may be relevant if you are interested in applied econometrics. This is not an exhaustive list of course. 

Let's solve the differential equation (12). As a first step, we look for a "simpler" differential equation, namely (A.1). (A.1) can be written $$\frac{\dot{z}}{z}=-(1-\alpha)\left(\delta+\frac{\bar{x}e^{-\bar{x}t}}{e^{-\bar{x}t}+\bar{x}A}\right)$$ On the left-hand side, you have the derivative of $\ln(z)$. You can integrate to obtain the form of any solution $z_c$: $$\ln(z_c)=-(1-\alpha)\left(\delta t-\ln(e^{-\bar{x}t}+\bar{x}A)\right)+ constant$$ You obtain $z_c=\kappa e^{-(1-\alpha)\delta t}\left(e^{-\bar{x}t}+\bar{x}A\right)^{1-\alpha}$, where $\kappa$ is the exponential of the constant in the previous equation. This equation is equivalent to (A.2) for a particular constant, such that $\kappa=\bar{x}^{\alpha-1}$. This choice of the constant comes from some boundary conditions on $z$ (which should be stated in the paper). As a second step, we use the method of variation of parameters, meaning we are looking for a solution of (12), $z_p$, that has a particular form, $z_p=z_c.\Psi$. If we find an expression for $\Psi$, then we have found a solution of (12). We substitute $z_p$ in (12): $$\dot{z_c}.\Psi+z_c.\dot{\Psi}=(1-\alpha)\left[1-\left(\delta+\frac{\bar{x}}{1+\bar{x}Ae^{\bar{x}t}}\right)z_c.\Psi\right]$$ This expression simplifies since $z_c$ satisfies (A.1): $$z_c.\dot{\Psi}=(1-\alpha)$$ This is (A.3). Then, I guess you we have to find a solution $\Psi$ of this equation, and we are done. 

I suggest this graphic illustration. You can first plot your utility U as a function of the quantity x of pizza eaten. Utility is expressed in \$. Plot a positive concave function starting form the origins (0 pizza leads to 0 utility), going up to a maximum (at $x=4$ let's say) and then slightly decreasing after $x=4$. The decreasing part means that you are loosing utility by eating too much pizza. Now assume a unit of pizza costs \$1. You can now plot a second graph illustrating the first order condition. Plot first a decreasing curve corresponding the marginal utility (a straight line if you have chosen a quadratic form to the utility function) starting from $(x,y)=(0,5)$ (or any positive $y$), going down and crossing the X-axis at $x=4$ and being negative after $x=4$. Then, plot the horizontal line $y=1$. The intersection of these two curves gives the optimal quantity of pizza to eat, $x^*$, given the cost. For $x<x^*$, you should eat more pizza because the marginal unit of pizza eaten worth more than it costs. For $x>x^*$, you have eaten too much pizza comparing to what it costs you. You can insist on two other points. First, imagine you do not pay as you eat pizza but you are invited at a party and food is free. In this case, the actual $MC$ is 0. You will then eat until the point you get sick, meaning $x=4$. In other words, you eat as long as the marginal utility of eating is positive. Second, back to the case in which a pizza costs \$1, you can show that $x^*<4$. It means that you stop eating before the point you get sick. You stop eating before because you have no interest in eating a marginal share of pizza that yields you \$0.5 of utility for instance but costs you \$1. Edit In the example I give, the marginal cost of eating pizza is monetary, meaning money that you pay. The marginal benefit is the marginal utility received from eating pizza (possibly negative), it encompasses both the "good feeling" of alleviating your hunger but also the "bad feeling" of eating fat and damaging your body. If you want to define $MC$ as the sum of "bad feeling" and monetary cost, and $MB$ as the positive feeling, you need to specify how both feelings increase/decrease when you eat pizza. This is more demanding in terms of hypotheses than using the utility-based approach, and maybe too ad-hoc. 

$q(\theta)$ is defined as the job-filling rate. Note that market tightness $\theta$ is not necessarily constant over time (Pissarides makes a dynamic analysis at some point). It may help to denote it $\theta_t$. As an approximation, $q(\theta_t)\delta t$ is a probability for a firm to meet a worker between $t$ and $t+\delta t$ for $\delta t$ small enough. 1) If you choose a large $\delta t$, this approximation is not valid anymore. In other words, assuming a large $\delta t$ prevent you from interpreting $q(\theta_t)\delta t$ as a probability. To define an equilibrium, Pissarides will anyway take the limit $\delta t\to 0$. 2) $q(\theta_t)$ is a rate (that is not constrained to be lower than 1), whereas $q(\theta_t)\delta t$ is a probability. Thus, the probability of a firm not meeting a worker between time $t$ and $t+\delta t$ is $[1-q(\theta_t)]\delta_t$. $1-q(\theta_t)$ has no clear interpretation (it can even be negative). I guess (but I would like confirmation) that the rate of a firm not finding a worker cannot be defined in this case (or it would be $+\infty$). 

First, structural equations assume causality. The following quotation is from Cameron and Trivedi's textbook Microeconometrics: Methods and Applications: 

I think Barro means in the footnote that Giovanni and Weil find the same equation, $U_t=\Phi C^{1-\gamma}$, but using the optimal path of $C_t$. In Barro's paper, the approach is different given that the dynamics of $C_t$ is exogenous: $C_t=Y_t$ by assumption. Barro uses the limit case when the length of a period gets close to 0. Maybe what may bother the reader is that the model is defined as discrete. Rewrite the model First, we can rewrite the model with a length of period $\delta$ and then use $\delta\to 0$. The GDP dynamics write $$\log(Y_{t+\delta})=\log(Y_t)+g\delta+u_{t+\delta}+v_ {t+\delta}$$ with $u_{t+\delta}\sim \mathcal{N}(0,\delta\sigma^2)$, and $v_{t+\delta}=0$ with probability $1-p\delta$ and $\log(1-b)$ with probability $p\delta$. The utility satisfies $$ U_t=\frac{1}{1-\gamma}\left\lbrace C_t^{1-\theta}+\frac{1}{1+\rho\delta}\left[(1-\gamma)E_tU_{t+\delta}\right]^\frac{1-\theta}{1-\gamma}\right\rbrace^\frac{1-\gamma}{1-\theta}. $$ 1) Find $\Phi$ as a function of $E_t\left[\left(\frac{C_{t+\delta}}{C_t}\right)^{1-\gamma}\right]$ From now suppose there is a $\Phi$ such that $U_t=\Phi C^{1-\gamma}$ (note that $\Phi$ depends on $\delta$ a priori). Define $H(U)=[(1-\gamma)U]^\frac{1-\theta}{1-\gamma}$, the utility satisfies \begin{align} H(U_t)= C_t^{1-\theta}+\frac{1}{1+\rho\delta}H(E_tU_{t+\delta}). \end{align} We substitute $U_t$: \begin{align} H(\Phi)C_t^{1-\theta}= C_t^{1-\theta}+\frac{1}{1+\rho\delta}H(\Phi)\left(E_t\left[C_{t+\delta}^{1-\gamma}\right]\right)^\frac{1-\theta}{1-\gamma}. \end{align} Hence, we obtain for $C_t\neq 0$, \begin{align} \frac{1}{H(\Phi)}= 1-\frac{1}{1+\rho\delta}\left(E_t\left[\left(\frac{C_{t+\delta}}{C_t}\right)^{1-\gamma}\right]\right)^\frac{1-\theta}{1-\gamma}. \end{align} 2) Find $E_t\left[\left(\frac{C_{t+\delta}}{C_t}\right)^{1-\gamma}\right]$ fromp the GDP dynamics The trick is to find the expectation in the right-hand side from the GDP dynamics. \begin{align} \left(\frac{Y_{t+\delta}}{Y_t}\right)^{1-\gamma}= \exp\left((1-\gamma)g\delta\right).\exp\left((1-\gamma)u_{t+\delta}\right).\exp\left((1-\gamma)v_ {t+\delta}\right). \end{align} Taking the expectation and using the independence between $u_{t+1}$ and $v_{t+1}$, it follows \begin{align} E_t\left(\frac{Y_{t+\delta}}{Y_t}\right)^{1-\gamma}= \exp\left((1-\gamma)g\delta\right).E_t\exp\left((1-\gamma)u_{t+\delta}\right).E_t\exp\left((1-\gamma)v_ {t+\delta}\right). \end{align} The expectation of $\exp(X)$ where $X$ follows $\mathcal{N}(0,\sigma^2)$ is $\exp(\sigma^2/2)$. $\exp\left((1-\gamma)v_ {t+\delta}\right)$ is a random variable equal to $1$ with probability $1-p\delta$ and $(1-b)^{1-\gamma}$ with probability $p\delta$. We substitute the expectation operator: \begin{align} E_t\left(\frac{Y_{t+\delta}}{Y_t}\right)^{1-\gamma}= \exp\left((1-\gamma)g\delta\right).\exp\left(\frac{(1-\gamma)^2\sigma^2\delta}{2}\right).\left(1-p\delta+pE[(1-b)^{1-\gamma}]\delta\right). \end{align} Finally, we use $C_t=Y_t$ to compute an equation for $\Phi$: \begin{align} \frac{1}{H(\Phi)}&= 1-\frac{1}{1+\rho\delta}\left\lbrace\exp\left((1-\theta)g\delta\right).\exp\left(\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\right)\right.\\ &\left. .\left(1-p\delta+pE[(1-b)^{1-\gamma}]\delta\right)^\frac{1-\theta}{1-\gamma}\right\rbrace. \end{align} 3) Take the approximation $\delta\to 0$ The last step consists in taking a first-order approximation (I abusively keep the equal symbol): \begin{align} \frac{1}{H(\Phi)}&= 1-(1-\rho\delta). \left(1+(1-\theta)g\delta\right).\left(1+\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\right)\\ & .\left(1-\frac{1-\theta}{1-\gamma}p\delta+\frac{1-\theta}{1-\gamma}pE[(1-b)^{1-\gamma}]\delta\right). \end{align} Pursuing the first-order apprixmation (all the $\delta^i$ with $i>1$ can be neglected), we have \begin{align} \frac{1}{H(\Phi)}&= \rho\delta -(1-\theta)g\delta-\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\\ & +\frac{1-\theta}{1-\gamma}p\delta-\frac{1-\theta}{1-\gamma}pE[(1-b)^{1-\gamma}]\delta. \end{align} Substitute $g$ using $g^*=g+\frac{\sigma^2}{2}-pEb$, \begin{align} \frac{1}{H(\Phi)}&= \rho\delta -(1-\theta)g^*\delta+(1-\theta)\frac{\sigma^2}{2}\delta -(1-\theta)pEb\delta -\frac{(1-\gamma)(1-\theta)\sigma^2\delta}{2}\\ & +\frac{1-\theta}{1-\gamma}p\delta-\frac{1-\theta}{1-\gamma}pE[(1-b)^{1-\gamma}]\delta. \end{align} We take $\delta=1$ and invert function $H$ to find the solution in the footnote 7 of the paper. The right-hand side of this equation "simplifies" to the within braces in the formula. 

Part of your question is to find the threshold price of a crafted item that covers exactly its cost of production, especially when production is random. Once you know this threshold, the price you will ask to a buyer has to be greater than this threshold. Otherwise, you make losses. Consider the craft of a precise item. First, you need to determine a value for each item that enters as inputs. The sum will be denoted $V_I$. If you do not have any idea about the values, you can set them as average market prices, so that $V_I$ is the average market price for all the inputs. In other words, $V_I$ is what it costs you to produce the item if you have to buy the inputs on the market. If there were no randomness in production, you would ask at least a price $p\geq V_I$ to any buyer who does not provide you with any input. Conversely, if the buyer comes with all the inputs, you have zero cost and your constraint is simply $p\geq 0$. If the buyer comes with some inputs, the price should be higher than the value of the remaining inputs. It becomes more difficult with randomness in production. You need to determine two elements: i) the probability $q$ to fail in producing the item, ii) the value of non-desirable outcomes $V_N$. You can estimate $q$ by dividing the number of times you failed by the number of times you tried. If the non-desirable outcomes are not always the same, $V_N$ can be an average. Again, if you do not know which value to associate, you can take average market prices. In that case, $V_N$ would be on average the money you get from selling the non-desirable outcomes on the market. There are two possibilities. Either you make the buyer bears the risk, or you cover the risk. In the first case, imagine for instance that you manage to produce the desirable item after 3 trials. The price you will ask to the buyer should be at least $3V_I-2V_N$ because you needed 3 times the inputs but you will keep twice non-desirable outputs. Whether a buyer is lucky or unlucky, it is not your problem. However, you do not have an invariant price of the crafted item. In the second case, you can ask a fixed price to the buyers (higher for non-members), irrespective of their luck. If you make enough transactions, the lucky buyers will pay for the unlucky ones, and you can make positive profits at the same time. We need some maths to find the expected (or average) cost of producing the item. The average cost is the sum of the values of inputs minus the values of non-desirable outcomes, weighted by the probability of each event. $$AvCost = \underset{1\ trial}{\underbrace{(1-q)V_I}} + \underset{2\ trials}{\underbrace{q(1-q)(2V_I-V_N)}}+\underset{3\ trials}{\underbrace{q^2(1-q)(3V_I-2V_N)}}+...$$ This writes as $$AvCost = (1-q)\sum_{k=0}^\infty q^k[(k+1)V_I-kV_N].$$ Using $\sum_{k=0}^\infty q^k(k+1)=\frac{1}{(1-q)^2}$, we obtain $$AvCost=\frac{V_I-qV_N}{1-q}.$$ The condition to make positive profits on average is to fix a price $p\geq \frac{V_I-qV_N}{1-q}$. In other words, if you want to be the most generous with members, they have to pay at least $\frac{V_I-qV_N}{1-q}$ so that you make no losses on average. If a buyer comes with some inputs, you reduce this cost by the value of her inputs. Members vs non-members You can also decide to make losses on average for your members, $p_{member}< \frac{V_I-qV_N}{1-q}$. These losses would be compensated by gains from non-members, $p_{non-member}> \frac{V_I-qV_N}{1-q}$. You have to make sure that the gains compensate the losses. About being competitive on the market We have established the conditions to make positive profits, but there is no guarantee that you will be competitive on the market. It is possible that the minimum price you ask to a buyer ($\frac{V_I-qV_N}{1-q}$) will still be higher than the price of your competitors. They can produce the inputs more easily for instance, or they can value the fact that the skill to produce will level up.