Now we just miss one thing to pass all your test: The bulk loading method where you map N keys to M values. This is a "cartesian" product, which is a fancy word all N's maps to all M's. In Python this is a walk in the park as we can iterate over both and reuse our method: 

So far so good. A slightly annoying thing is that we can't really read what is in the database even though we can get things in and delete them. This calls for the SELECT STATEMENT - or python's method. But(!) we need to be careful, as our database stores data internally as a s that are accessible from keys. So we need to unpack them onto something useful. As I like working with lists, I have chosen to provide lists, unless its a single value, whereby I only return the value itself: 

I wrote the following implementation of the k-nearest neighbor algorithm (for a binary classification task). I am not familiar with OCaml's built in functions, I have the feeling that some of them somehow reinvent the wheel : 

In order to have a "unified" (I realize how ambitious this actually is) syntax when working on dataframes, I wrote the following functions that is a general purpose dataframe set of tools in R. 

Another option would be to have a or value in your class. Multiple accesses inside the same loop Here you have to access every time in the second loop. This harms performance and makes this second loop unclear. 

Next we need figure out how to get data into the database. This is similar to the SQL INSERT STATEMENT, where we maintain both our keys and values in single transactions: 

(1) a new key to a known value only means an update to the values. Next we worry about if it's a new relationship with a new value: 

We have now implemented the INSERT STATEMENT in our database. Now it's time to worry about how to delete records and relationships. For this we hack the function to both take a key and a value. Why? Because otherwise we won't know whether the user wants to delete a single relationship only, OR all entries associated with the key I thereby choose that: 

I don't really like the methods which forces the user to call it every time before calling . Besides, the algorithm is very basic... Are there libraries doing this already ? Is there another algorithm, simple to implement (performance is not really an issue, this was working with roughly 1000000 elements) My tests are really poor as well, are there better guidelines to write them ? 

I want to plot (on the complex plane, with ) the power series whose general term is the number of partitions of an integer. The more points I want (and the larger the degree is), the longer the plot takes. I first wrote a naive version, added memoization of the sequence, changed the naive evaluation of the polynomial with the Horner algorithm. All these modifications led to speed ups, and I want to know if there are more optimizations I could do. 

Next we can test our bulk-load method: together with the "many keys - shared value" which is focal in your problem: 

Testing Now I need to test this. First we load a single key:value pair, where the key is a hashable structure. I chose a tuple with 3 integers: 

Update2 Above: Runtime test without powerfunctions and with Justin Peels recommendation to use return list(a.values()) 

This is a simple repeatable question regarding the usage of Python3's comprehensions: Could the Python3 syntax be used in another way to speed up the process, further so the gap between Python3 and numpy would be reduced? (see chart below). The results & code: 

Is there a smart way to optimize these bottlenecks ? I feel like there is something redundant in evaluating hashes and storing values in hashtables... 

Function naming and are kind of misguiding, I would expect that , but this is not the case as each function treats the diagonal in a different manner. In order to improve readability, I would write a method and a 

I am running a simple OCaml program which opens a CSV file with a pseudo dict-reader and hashes "key" + "value" (where key and values are strings). Then some counts are evaluated on the hashes (but it is not really relevant for what follows). After a quick look at the default OCaml profiler (), I noticed that my program was mostly spending time in hashing elements (I don't know what does though). 

Now we can add and delete. But how about bulk updates? They constitute a special case because can't see that you want to propagate your update onto multiple values. So we need to tell it WHAT to update. Here is a proposal that uses the relationship between the key, the old value and the new value that should be accessible to all keys that other would have a relationship with the old value: 

This also works as our assertion in the last line doesn't complain: The value of d[1]...d[3] are the same: The text string I add two more tests, which should look very familiar to you, though with minor exceptions: 

This is a simple implementation of a generic binary tree that holds elements of type T. I was wondering if there was something that could be done better (especially in the EnumerateNodes methods). 

Note : I am using OCaml 4.02, but I could upgrade it to 4.03, especially for the Array.map2 methods. 

My knowledge in graph theory is very limited. I have to look for elements in an (undirected) graph who are in the same connected component. The idea is simple. First, build the graph. Then, allocate a "color" to a point and spread it to its neighbours recursively. Once there are no neighbours to color any more, pick a point without any color and repeat the same process. This ends when all the points have a color. I wrote the following class : 

This is a very basic relational database. So the first piece of feedback is that new programmers often start coding with poorly defined test-cases or non-normalised test-cases whereby they get lost developing their test portfolio. If you would have started with clarifying your case, I'm sure you would have cracked this problem. Let's start defining our operations from examples Sometimes we will have the database maintaining links: 

In test03 I need to pack the value into a list, because iterload needs two iterables. I could made my code much harder to read by accounting for many different cases, but I think the programmer should build a function for one thing and make it clear for future recall of what the function is supposed to do. In test04 - below - I have added the usage of the update function. 

Comments I don't really see the point in on top of your methods. You can just type which automatically propose a neat documentation, per example, in the MatrixCreate function : 

Hardcoded values They are usually a bad idea (what happens if, per example, you use this value somewhere else, and decide to change it someday?): 

I wrote a simple online logistic regression, calibrated using gradient descent to compare the speed of an OCaml implementation vs the same Python script, executed with Pypy. It turned out that the OCaml implementation was slightly faster than the one run with Pypy (by about 10%). Now I would like to optimize my code even further. The assumption about the data is that the values of each rows are sparse (can be considered as factors), they are encoded as integers (collisions are allowed) and stored in a large array. maths.ml