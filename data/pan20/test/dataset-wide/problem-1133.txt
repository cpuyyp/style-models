Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: 

When setting MAX_IOPS_PER_VOLUME in a Resource Pool, what exactly does "volume" mean? Specifically, how many "volumes" would be the following cases: 

I assume the answers to #3 and #4 are "1 volume" and #5 is "2 volumes" but it's #1 and #2 that I'm most curious about. The specific reason I'm asking is wondering if it's possible to increase the Resource Governor's IOPS limit for locally-attached SSD tempdb while having a lower limit for our SAN data storage. So I'm wondering if splitting a single physical disk into multiple partitions might be a way to do this, by putting separate tempdb files on each partition so the total tempdb If #1 above makes SQL Server treat one physical disks as multiple volumes for throttling purposes, this may be an option. I'm assuming that this won't work-- that SQL Server is smart enough to know that 2 partitions is one "volume". But was worth asking. 

I have a group of about 30 tables, and I want to know the physical size on disk of all of these tables (plus indexes). Is there an easier way of doing this than through the GUI in SQL server 2008 R2? 

This is in addition to other IT concerns you'd want to keep in mind like maintainability, support contracts, and your space/power requirements. 

I have the need to restart Microsoft SQL Server's group of services on a given computer, remotely, through some sort of programmatic method I can insert into a workflow. I'm looking for the best way to do this, I've seen a few different ways and I'm wondering if there's anything I'm missing: 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten? 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it. 

The way to gauge hardware performance is to be take a look at your current workload. So I would begin by quantifying that: 

I'm not sure of the change that caused this (the logs don't show anything specific) but ever since Monday afternoon I've seen the following error repeated: 

However I would like to have that integer replaced with a variable that is the RunID of the row updated (which is causing the trigger to fire). How can I do that? 

Currently this takes something like 3 minutes to run. Is there a better way to get the information I'm looking for? 

Does this same fill strategy apply to tempdb? And does this answer depend on the type of query, e.g. parallel vs. non-parallel? Or is the answer different based on the kind of tempdb I/O, e.g. for temp ables I create vs. tempdb usage by the database engine for worktables or spilling? 

When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

I have 2 instances of SQL Server installed on one server: SQL002. One instance is a default instance, SQL2008 R2, and the SQL service is generally not running except for specific tests. The other instance is a named instance, SQL 2014 (so SQL002/SQL2014). This is generally running all the time. Today I just had a developer tell me that when they connect to SQL002 in SQL Server Management Studio it will connect successfully. I replicated this from my machine and also see a successful connection, however when I look at the properties of the connection it is reporting that it connected to the named instance. What is going on here? Is there a redirect I'm not aware of? 

Locally attached disk that's split into two partitions E: and F: Software RAID 1 set E: composed of 2 locally attached disks (yes I know software RAID is bad-- adding this case to help me understand SQL Server's definition of "volume", not to design a production setup!) Hardware RAID 1 set E: composed of 2 locally-attached disks SAN disk E: on who knows/who cares how many disks. 1 SQL Server filegroup spread across two locally attached disks E: and F: 

So we're wondering if there's a lower-cost solution that would store as smallint but expose the colunms as ints to readers. Like this: