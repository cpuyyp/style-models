This would allow another Slave to connect to a Slave with binary logging enabled as a Master REASON #2 If a Slave is not a Master, but you want to failover to the Slave to become a new Master 

Give it a Try !!! UPDATE 2011-12-16 21:08 EDT I just caught on to your problem: Your program is not running because you did not supply the password. It is stuck in the background waiting for a password. That's your actual problem. Kill the job and run it again with the password on the command line: 

Since you do not set anything, MySQL has set to 2, which is very bad for InnoDB. This at least explains why InnoDB won't start up. You will have set to 1 and try again. Give it a Try !!! 

This will acquire the lock, insert or overwrite the previous lock owner, and cleanup the lockbox by locating all entries whose connection had terminated. Since GET_LOCK() is unsafe for statement-based replication, you can locate the warnings written in the error log and should see the names of the locks. 

This will filter rows with duplicate fields. Look over the new table. Once you are satisfied with the contents of the table, do this: 

Performing a mysqldump creates a consistent and logical dump of the data which can be point-in-time recoverable. There are two ways you can stop replication partway METHOD #1 : mysqldump option --dump-slave When you perform a mysqldump with --dump-slave, it will stop only the SQL thread. The I/O thread continues collecting binary log events from the Master. Note how this is explained in the MySQL Documentation: 

You are overlooking something far more basic MySQL Documentation clear says the following on GET_LOCK() 

A prompt will not return to you. You can now open another Command Line session and login using the mysql client. UPDATE 2014-07-28 17:51 EDT You need to run the following: 

The column list did not include the numeric primary key. When the 1062 error came back, I would use the same query it failed on, run the query manually. It did not hit the 1062 error. Then, I ran the usual skip slave commands: 

If you want to setup a DB Cluster using MySQL that commits DB Changes on a Master and a Slave at the same time, your best setup would have to involve PXC (Percona XtraDB Cluster). PXC uses the WriteSet Replication package from Galera (made by CoderShip) It works on the premise that a transaction committed on a Master is then committed on the Slave. If the Galera code detects that the Slave could not commit and rolls back, then the Master rolls back. This process repeated if there are more than two DB Servers using PXC. Upon installation, Percona Server has Galera integrated. Don't worry about interoperability. Percona's server binaries are a complete drop-in replacement for MySQL's server binaries. Read documentation because there are some limitations.Such limitations include 

Once you have mysqldumped all data, remove the Then, perform a complete cleanup of InnoDB as follows 

You may find this intriguing but you can double or triple the speed of InnoDB writes. How ? Disable the double write buffer (using innodb-doublewrite) First, add this to ( for Windows) 

If you want to find all the active connections that have this difference, perhaps you could have the application dump the output of that query to a text file along with the connection ID (using the CONNECTION_ID() function) and parse it as you wish. 

This will display all storage engines available in in the MySQL instance. Here is a sample from a server running MySQL 5.5.9: 

Installs of MySQL for Windows is like drudgery but possible I usually use the no-install zip file. After installing it in an appropriate location, I go to the bin folder and run 

It's a global variable in MySQL 5.6, but it is not dynamic. Since you are using MySQL 5.5, it is only a startup option. Therefore, you must bite the bullet and restart mysqld. 

Look over as many examples of this setting as you can find. Take note of the queries attached to and find out what those individual queries do. Then, the purpose of will become more clear to you. More example available here : $URL$ If there are any users of pgBouncer out there in the DBA.SE, please chime. I am a only nominal PostgreSQL DBA myself. 

This is only a guess but here it goes... The only way I know that a PRIMARY KEY for a MyISAM table would be OK and all secondary indexes be out-of-date would be under this unique circumstance: Running this 

I suggest this order of columns because the entries would all be contiguous in the index. Then, the query simply collects values without skipping gaps in . SUGGESTION #3 : Bigger Key Buffer (Optional) MyISAM only uses index caching. Since the query should not touch the file, you should use a slightly bigger MyISAM Key Buffer. To set it to 256M 

but that might take a very long time. You may need a temp table to accomplish this: Step 01) Create a table that will hold the one million col2 values 

Give it a Try !!! UPDATE 2013-01-22 13:44 EDT Here is untested BlackOps approach: STEP01) Make a copy of the mysql schema into a database called mysql_orig 

In your case, you can create the /tmp/ListOfTables.txt file manually. In order to inject , just create the dump file with the as the first line. Then, append the output of mysqldump to it. Keep in mind that each table is described as dbname.tablename: 

You have max_questions, max_updates, max_connections, and max_user_connections. In MySQL 5.0, you can limit use of the following server resources for individual accounts: 

Setting this option to 0 disables the Double Write Buffer. Here is Pictorial Representation of ibdata1: 

Since you only have rights to the database, please make sure you are specifying that database when you connect. If you do not set the default database at authentication, it will not let you connect at all. Here is what I mean: Let say you trying connecting from the command line 

As shown from the end of the error log, it performed a clean shutdown at 2:39 PM. MySQL came back up when Windows was started. 

See the 5.4M per thread? That is multipled by max_connections. In this example, that would be a maximum of about 10.8G of RAM. Therefore, each time you bump up max_connections, you should run mysqltuner.pl and check if you are pressing the OS for too much memory. In any case, limiting who has SUPER privileges give such users opportunity to mitigate flooding mysqld with DB Connections. 

Give it a Try !!! Your query did not work because the RIGHT 2 of 100 is 00, which would be numerically first. 

You need not worry about infinite circular replication unless you are dealing with more that two masters. There have been rare times when someone with, let's say four Masters, removes one of the four servers from circular rep cluster. Let's suppose the the server_id is 13. It is remotely, but still, possible for binary log entries whose server_id belongs to the server that removed to be inside the relay logs on other servers. Only in such a scenario would you worry about infinite circular replication. To circumvent such situations, MySQL 5.5 has a new option for the CHANGE MASTER TO command called . You would do the following to repair things on all the remaining servers: 

It looks like it should work because the general_log_file is dynamic (according to the MySQL Documentation). Some six(6) years ago, Joel Hanger answered How do I output MySQL logs to syslog?. In his answer, he added it to the my.cnf. Evidently, a mysqld restart would be needed. In your particular case, you may have to enable the general log first using 

This seems to be an on-going issue Views are messy to handle with Dynamic SQL Earliest Bug was Cannot create VIEWs in prepared statements from 11 years ago. There was a patch put in to address it. Another bug report, Prepared-Statement fails when MySQL-Server under load, states that error 1615 is not a bug when the underlying tables are busy. (Really ?) While there is some merit to increasing the table cache size (See MySql error when working with a mysql view), it does not always work (See General error: 1615 Prepared statement needs to be re-prepared (selecting mysql view)) ALTERNATIVES Over a year ago, someone mentioned this in the MySQL Forum (MySql “view”, “prepared statement” and “Prepared statement needs to be re-prepared”). Someone came up with the simple idea of not using the view in the prepared statement but using the SQL of view in a subquery instead. Another idea would be to create the SQL used by the view and execute it in your client code. These would seems to be better workarounds that just bumping up the table cache size. 

Changing Columns with older the DATETIME format will not work. You would have to use other means to make the new table. 

That's it. The mysqld process will autodetect the presence of the new database. CAVEAT Please keep in mind that all tenant databases share usage of three commodities: 

IMHO the best option would be to Convert First to InnoDB then perform the mysql upgrade. I prefer this because performing a mysql upgrade mostly involves alter the grant tables. (This especially includes the mysql.user table since MySQL 5.0's mysql.user tables has 37 columns while MySQL 5.5's mysql.user tables has 42 columns) I would not want to mess with connectivity or SQL Grants issues first. Given the following facts: 

This will initialize the value you need to retrieve before starting a unit test. If you wish to initialize the now as of 3:30 AM, 5 days ago, do this 

the complete parsing of the SQL, preparing for execution, and the parameters already being passed in through the SQL statement are all done on the server side with one call. Please read that Percona blog for more details. 

For more clarification on this, please see MySQL Documentation. BTW once you are done with the maintenance window for throttling user connections, simply set the values you changed back to zero(0) to remove the limits as follows: 

This should give you a good starting point. You could probably create it as a table with something like this 

STEP 04) Once you startup mysqld, the slow log entries are recorded in the MyISAM table mysql.slow_log; To rotate out the entries before midnight, you could something like this: 

DiskSpace Metrics nrgram_rec has 17 bytes per row 8 bytes for ngram_id (max unsigned value 18446744073709551615 [2^64 - 1]) 8 bytes for 4 smallints (2 bytes each) 1 byte MyISAM internal delete flag Index Entry for ngram_rec = 10 bytes (8 (ngram_id) + 2 (yr)) 47 million rows X 17 bytes per row = 0799 million bytes = 761.98577 MB 47 million rows X 12 bytes per row = 0564 million bytes = 537.85231 MB 47 million rows X 29 bytes per row = 1363 million bytes = 1.269393 GB 5 billion rows X 17 bytes per row = 085 billion bytes = 079.1624 GB 5 billion rows X 12 bytes per row = 060 billion bytes = 055.8793 GB 5 billion rows X 29 bytes per row = 145 billion bytes = 135.0417 GB 

At least you will have a localhost user named with enough privileges to login. You should then run this query: 

It's there for me. I installed this a couple of month ago using the no-install zip file. SUGGESTION Download the latest no-install zipfile for MySQL 5.6.15 and look for the folder UPDATE 2013-12-16 13:00 EST I just downloaded the no-install zip file. I looked in it and it is in there. Please download it and extract. Of course, make sure you have Perl installed on the Windows machine. 

Your original query joins everything together then traverses 50000 rows into the joined data before presenting the next 100 rows. Try this 

The quick and dirty argument that should come up against such policy is the implementation of business logic. Mundane auditing of data traffic is only one good use of triggers. Maintenance of those triggers for business logic makes the use of triggers take on a life of its own, as if it is part of the software lifecycle. For some databases like Oracle, PostgreSQL and SQL Server, such use of triggers in an ACID compliant arena should never be suppressed or relegated if business logic is best implemented in the database. On the other hand, one DBMS that has issues with ACID compliance in triggers is MySQL. The stored procedure language in MySQL has a rocky history although a measure of success of implementation can be achieved. Getting away from any specific database and to keep things in perspective: If referential integrity is to be maintained, let the database's storage engine do it. Why should an application reinvent the wheel of database integrity as part of business logic ? What if a transaction (series of SQL statements) needs to be rolled back and the triggers have already fired off in the middle of the transaction ? Can the audit trail info that was recorded be rolled back ? Does the storage engine allow rollback for trigger level data ? Will you have to design the rollback features independent of the paradigms of standard SQL ? Due diligence of research on these topics will clarify whether triggers are a healthy alternative. If there are some arithmetic operations you can perform in a trigger that will not slow down the processing of each row in a table, then triggers are OK. If there is some massaging of data you need to do and you'd rather have a trigger do it instead of at the browser code (PHP, Python, Ruby, whatever) that will not slow down the processing of each row in a table, then triggers are OK. CONCLUSION IMHO you should make sure audit trails are handled in transactions rather than in triggers because each recording of a row causes intermittency of database operation, especially when doing bulk processing or in a high-trafficked website. Triggers can accomplish business logic in many (but not all) situations.