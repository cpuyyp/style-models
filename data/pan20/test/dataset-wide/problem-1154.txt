Now here is where it gets murky. And I need help with the schema here. The FlowDirection records whether it was a purchase or a sale. The PurchaseId records the invoice number of the purchase IF it was a purchase. The SalesId records the invoice number of the sale IF it was a sale. There is a constraint on the table where the nullity of PurchaseId and SalesId attributes are XOR'ed, i.e. One and only one of them will be null for a given record. I am thinking of getting rid of the FlowDirection as it is obviously redundant. Secondly, I also will get rid of the Invoice attribute, as it is already recorded in PurchaseId/SalesId. Or, I can point Invoice column to a union of Num attribute of the INVOICE table and the Invoice attribute of the Purchase table (I have got to rename all these tables and attributes too). Which one should be a better approach? 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgresQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM approach. Here is the problem with the design. This is the simplified description of my schema. I have a Clients table, an ItemDetails table and a RateChart table, among others. The Clients table records details for each client, including their address. The ItemDetails table records details for each item we sell to our clients, including a short description for the items. We do update the description from time to time. The RateChart table has the different rates for items for each client. Now, I also have to save the details of each and every sale. So, I have an Invoice table, and InvoiceItems table. The Invoice table saves the invoice number (primary key), the date, the total amount of the order and the client name(foreign key, refers to client name in client table). The InvoiceItems table, on the other hand, records the items sold, the quantity of each item sold and the rate at which it is sold. Now, here's the problem. The items sold in InvoiceItems list is a reference to the ItemDetails table where details of all the items are saved. Now, the details of items are changed from time to time. For example, the description of items are changed. So, whenever I change those item details, those changes will cascade down to this InvoiceItems table, and will make the older invoice records erroneous, because that was not the description when the product was sold. Same problem lies with the Client field in sale table. Whenever I change an attribute any record in the client table, the changes will cascade down to the sale table. Here is the schema for illustration. 

So, in PURCHASES table, we record the Material we have bought and the Quantity of it. With each purchase, we need to make three modifications: 1. Insert the purchase details in PURCHASE 2. Modify CurrentStock in MATERIAL(increase it by the amount we have purchased) 3. Insert the amount of increase in Materials in STOCKHISTORY table. Please note that StockHistory does not record the cumulative stock, but just the amount of increase/decrease associated with each transaction. Similarly, for each sale, we need to make three modifications. 1. Insert the sales details in INVOICE. 2. Modify CurrentStock in MATERIAL(decrease it by the amount we have purchased) 3. Insert the amount of decrease in Materials in STOCKHISTORY table. Please note that INVOICE table here represents the invoices generated by us, i.e. the sales made from our end. Here is the schema: 

It turns a list like this: Albuterol Sulfate Amlodipine Besylate Aspirin Benztropine Mesylate Bisacodyl Ciprofloxacin Collagenase Divalproex Sodium ... into a string like this: Divalproex Sodium, Collagenase, Ciprofloxacin, Bisacodyl, Benztropine Mesylate, Aspirin, Amlodipine Besylate, Albuterol Sulfate I can write it as a dynamic stored procedure, but you can't call sp_executesql from a function (or at least, I don't know how). Question How would you write this function in a way that it could be used on any table? 

I've got a query that checks for new patients in the ER, and if they have been there before in the last 72 hours, it notifies the killbo... uhhh... care managers. I could set this up as a trigger, but I don't think this is a good idea. What I want to do instead is set it up as a job that runs every 5 minutes, with this in the where clause: 

You can also do it using a date table and ordering it by newid(). I've used this technique to scramble lots and lots of data in the past. One advantage is that you can scramble any field by joining the table to itself on Note: if your person table is bigger than your date table, in this example, loop the date table insert a few times until it is bigger. 

How often do I need to run it? It needs to run at the same interval that I keep my backups, right? If I keep the backups for 1 day, I need to run it every day? Can I restore the databases to my test environment and checkDB there? If I've got lots of sql servers, but they all use the same SAN, am I going to want to stagger my CheckDB's? 

But the query itself takes 03:31. Question: is current_timestamp the timestamp of the beginning of the query? here's the entire big, bad query: 

I'm trying to break all the rules of databasing using the stuff function. I want to smush every applicable row into just one, for science, you know? Problem is, I have to write a new function every time. They look like this: 

Here's the James May method. Looking for feedback & ways to do it smarter. 1. Write your query in SSMS. To get dynamic date ranges that always fall to the first of the month, I do it this way (there is almost certainly an easier way): 

You're nearly there. the trick is writing your group by. I don't want to bitch about your question being too long to read, but I definitely didn't read the whole thing. one other suggestion, if you're only putting dates into a dateTime column, consider altering it to a date column. 

But I'd rather use the same thing everyone else uses. Question: What's the right way to do ? Answer: From SpBlitzErik - use Ola hallengren's scripts, you dummy. Followup Questions: 

We are building a new server setup following this guide: $URL$ And it reccomends 7x 960GB SSD's in Raid 5. Question: If the databases are on the SAN, what is all this extra space for? 

Now, the solution I have thought of is highly inelegant. I have considered saving the client details for every sale in the Invoice table, and the item details in InvoiceItems. Of course, that would be a great deal of duplicate data. Another solution might be to keep the defunct details of items and clients, which would mean polluting my ItemsDetails and Client table with records no longer valid. Of course, I can add a boolean IsValid column, but then again, the solution does not seem to be succinct. Any help would be heartily appreciated. 

Pens x1 (nickname) --- Pen (MaterialType) ----- 1 (NumberOfPieces) Pens x2 (nickname) --- Pen (MaterialType) ------5 (NumberOfPieces) 

Material records the material whose stock has changed. Amount records the amount by which the stock has changed. Invoice records the invoice number of the transaction. 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgreSQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM and Repository approach. The simplified proforma is like this: Items: Details of items we deal with. Purchases: Records the details of items we purchased. Sales: Records the details of items we sold. Stock: Quantity of each Item in hand. Now, I also have a stock table where current stock of each item is maintained. The dilemma I am facing is this: When any Invoice is generated, it indicates that a sale has occurred, and the stock of each item in the Invoice is updated accordingly. Similarly, when a purchase from our side occurs, we record it in the purchases table, and the respective items in Stock table has their quantity updated. I would have to also consider the scenario of correcting the stock table if an existing invoice is modified or deleted. These are the only three scenarios when the stock table would change. Now, I can achieve this update of stock table using Triggers. I can also use stored procedures in order to encapsulate the entry in Purchases or Sales table along with updating the stock table. I can also do this in application logic using LINQ. Which approach would be the most pragmatic one? THE DETAILED STORY Please allow me to discuss the entities that matter in this current context. We are a reseller. We buy materials in bulk, we process the materials and repackage them and then sell them. In this table, the items we sell are marked as the entity ITEM. There is are two attributes in the ITEM table worth noting. One is MaterialType and the other NumberOfPieces. The entity MATERIAL is the raw material we buy. Now suppose, we buy 100 pieces of pens, and we resell them in units of 1 or 5. So, in this case, the records in the item table would be 

Now, if we need to delete or modify any record of PURCHASES or INVOICE, we also need to modify the CurrentStock in MATERIAL as well as the corresponding records in STOCKHISTORY table. Here is a rundown of STOCKHISTORY table: 

I do it more or less the same way as you, and I don't get duplicate messages. How are you actually sending the messages out? If you're doing them in batches and getting duplicates, then my guess would be that one batch is starting before the previous batch has been written to the SentMessages table. To get around this, I'd start with smaller batches - or slow down the frequency of the job, so that the last batch has been written as sent before the next batch starts. If that won't work, I'd put it in a cursor that looks something like: 

One thing to experiment with is changing your clustered indexes - in a lot of ways they aren't really indexes at all, they are the order in which the data is stored. make copies of your current clustered indexes first, then try adding the columns in the joins & where clause to the clustered index. Clustered indexes are often the same as primary keys, but they don't have to be. your primary key and clustered index can be completely different. Another thing - the - do you need every column from that table? If you only take the columns you need, it might help. Then, there must be a way to stop a query if a website user bails... Other things you can do - archive some data. I'll bet some of your tables are pretty big. Do you need all the data there? With Enterprise edition, you can also partition tables. Check for locking - if you have two users putting this query to your DB at the same time, are they blocking each other? I'm going to catch some shit for suggesting this, but you could join with (NOLOCK) and see if that helps. Oh, and when did you last update your statistics? The query tuning videos on Brent's site are really good, if you need more detail. 

I've got a cursor sending out pager messages, and occasionally it sends out duplicates. The syntax looks like this: 

Here's how you can join a table to itself randomly and update a column. This method will also maintain the distribution of your data: 

You'll get that error when one of the columns in the select won't fit into the destination table. Tracking it down can take some time, but it's just like kindergaten - try and fit the shapes in the holes. If you compare #Variable_info to #stored_proc_info, you will probably find that one of the columns in #variable_info is larger than its corresponding column in #stored_proc_info. I can't find #variable_info in my version of the sp_blitzcache, so I can't tell which columns don't match. Often, I track it down by running a query like this: 

3. Set up a .dtsx package: You can do it directly in SSIS, but I find it easier to use the SQL Server Import and Export Wizard by right click on the database, hitting Tasks and Export Data. Set the Data source to by SQL Server Native Client, set up your server and database, then hit Next. Set the Destination to be Microsoft Excel, and the excel file path to your shared folder (not your blank template):