is not some type of regular expression defined to just match digits. Any range in a pattern matches characters between the start and end character according to collation sort order. 

Showing that the plan for the qualified query was successfully shared between the users with different default schemas but the non schema qualified query needed a new plan compiled. If you don't see this sharing happening ensure that all logins you are testing have the same ,, , as these are among the cache keys listed at the beginning and any differences in those will prevent the plans being reused between sessions. 

then the predicate is pushed into the scan on and performance is much better. It is guaranteed by the join condition on that the two are equal so this does not change the semantics. With the example data I added to your question and an hint the original version had 

Probably Parameter Sniffing. To troubleshoot this without running Profiler 1) Add a GUID into the text of the query as a comment. e.g. 

This would make the window of opportunity wider but, whilst a transaction could potentially read the "old" value from an index yet to be updated, it would not be possible for a read committed transaction to read the "new" version of the value until the transaction was committed. It is certainly possible for a read committed statement to read two different committed versions of the same value though. In one connection run 

There are quite a few ways to achieve your desired results. Undeterministic methods (in the event that many rows in table 2 match one in table 1) 

The is in both cases but is following the transaction commit whereas all of them were removed after the rollback. 

I was looking at the article here Temporary Tables vs. Table Variables and Their Effect on SQL Server Performance and on SQL Server 2008 was able to reproduce similar results to those shown there for 2005. When executing the stored procedures (definitions below) with only 10 rows the table variable version out performs the temporary table version by more than two times. I cleared the procedure cache and ran both stored procedures 10,000 times then repeated the process for another 4 runs. Results below (time in ms per batch) 

There are no technical issues with this. It isn't going to make any difference to SQL Server. From a usability point of view identifier names that begin with a number must always be quoted so 

I'd use for this unless and are both indexed and a relatively low proportion contain the value . (Borrowing JNK's table variable) 

And then hopefully you will see disordered results without having to change the isolation level. For the second query you want a hash aggregate rather than a stream aggregate. You can force this with a query hint or less obviously with a plan guide but this is more likely to be chosen organically if there are relatively few distinct groups compared to the size of the table. 

Not something I've used much, if at all, myself but always worth having additional possible techniques to consider. The specific use case in your question isn't something I would consider a table of constants for though. TBH the hardcoded same "hometown" for everyone just seems a nonsensical requirement. 

The string returned from the view is 10 times longer than the maximum final string required to make it extremely unlikely that removing these characters will leave insufficient characters. Then select from that 

Doesn't benefit from column statistics. It can just use that directly and even if statistics are disabled will be accurate. A query with would use the statistics and return the whole table. As it happens the statistics don't get updated in your case anyway when such a query is executed as you haven't hit the threshold for modifications for an auto update to occur. But the estimates are still accurate. The statistics records that there were 1,000 rows at sampling time. 

will just use the standard guess for an inequality predicate that 30% of the rows will be returned. The query (in your case) presumably estimates much fewer rows will match the predicate. Note that the leading wildcard still prevents an index seek. An entire index is still scanned but it uses a different one that is narrower than the clustered index. The narrower index doesn't cover all columns used by the query so the second plan requires a key lookup to retrieve the missing columns. This plan is extremely unlikely to be chosen with the 30% estimate. SQL Server will consider it is cheaper to scan the entire clustered index and avoid that many lookups. See this article on the tipping point for additional examples. 

I'm not sure if it is straight forward to get the same plan on 2008 or maybe it would need an on to simulate it. 

It appears to do it in sequence in your plan. Not in parallel. The insert happens into the heap. The inserted rows are inserted into the eager spool which is a blocking operator. When all rows are inserted into the base table the rows are inserted from the spool to the view. As for the percent issue it appears that the spool confuses SSMS. Despite showing up twice in the plan (with the same ) its cost should only be counted once but it seems as though its cost is completely disregarded when calculating the overall estimated subtree cost for the plan and then it displays the operator percentages as a proportion of this incorrect total. 

(From previous experimentation I'd found the highlighted bytes were the only ones that changed between runs so these are definitely the right ones). One surprising aspect is that = . This is exactly 3600 (one hour) less than reported by . I believe this to be some attempt to disfavour pages being kept in cache by calling itself. For a "normal" page select this one hour adjustment does not occur. After running 

"User Databases". The same "System" vs "User" distinction is also made for other objects such as tables. For example 

A CLR aggregate will almost certainly be the fastest way of doing this. But perhaps you don't want to use one for whatever reason... You say that the source for this is an expensive query. I would materialise this into a table first to ensure it is only evaluated once. 

No is not sargable. Your own test demonstrates this well. An exception would be if you created a computed column with the expression and indexed that. 

This is far more generally useful as the most common reason for having an column is to allow SQL Server to manage the values. I suppose it could imply that if there was an extra column in the insert source on an table that it should generate an execution plan for the explicit case but not much benefit to this IMO. 

If you use SQL Server Data Tools to manage your database schema instead of making schema changes directly in SSMS then this does have a rename refactoring. It isn't 100% comprehensive as you would still need to review for any possible usages in dynamic SQL or references to the table in external scripts but it would fix the issue above. 

/ have 1 byte each for precision and scale. have 2 bytes for max length and 4 bytes for collation id. have 2 bytes for max length. 

Is there any way to write a query that just seeks into that composite index key and then follows it along to retrieve the next chunk of 1000 rows? 

And loaded EmployeeBenefitData with integers from 1 to 4,000,000 (6,456 pages) And FilteredEmployee with integers from 2,000,000 AND 2,010,000 (19 pages) And then ran 6 queries of the following form 

Which has zero impact on existing permissions, and low administrative effort compared with other possible alternatives such as the solution you propose of dropping and recreating all indexes. 

If you select the Clustered Index Insert Operator and View the properties window you can see the same information as shown in the XML. 

An application is processing the rows from this table in clustered index order in 1,000 row chunks. The first 1,000 rows are retrieved from the following query. 

The execution plan for this has one scan of . The plan is in fact the same as for the 2005 compatible rewrite that uses 

is exactly 3. is 2.99999999999999955591079014994. If you are looking for an efficient expression maybe a expression with the 10 different cases would actually work out less CPU intensive than calculating logarithms (or possibly you could have nested case expressions to do a trinary search) 

I prefer this as it doesn't rely on hardcoding a string that it is assumed no legitimate data will sort after. I'd rather not have to consider that on some case sensitive collations z sorts before Z and on others the reverse applies, will that be an issue? Similarly will "Å½eljko Ivanek" sort before "ZZZZZZZ" reliably on all collations? Are there other characters from other languages that might exist in a name and sort after "Z"? A potentially more efficient solution if there is an index on (as it hopefully will avoid a sort) 

The one affected by using two part names is If you try the following under the credentials of a user with default schema 

You dont need here. In fact this will give you incorrect semantics as is a wild card for matching any single character. So your query would incorrectly exclude certain other strings too (though this is hypothetical as there is no wait type that it can clash with in practice). One other possible approach to give your desired semantics is described here 

Shows no reads against at all. The seek of is under a pass through predicate and the operator is never executed. But 

Now add 3,000 additional batches to T1 (with batch numbers 2 to 3001). These each clone the existing thousand rows for batch number 1 

As stated in the other answers it depends on your browser. If your browser does not have native support for Web SQL Database then it ends up submitting back to the server. 

SQL Collations are provided only for backward compatibility. This has been the case for over 10 years. Use a more modern collation such as . This does not produce that behaviour. 

This isn't generally possible but is in the specific case in your question. Windowed functions are allowed only in steps 5.1 and 6 in the Logical Query Processing flow chart here (The and ). cannot be used to reduce the returned rows. In SQL Server 2008 can only do so in conjunction with (For 2012 can filter on the basis of ). As you are only interested in ones where the expression evaluates to in this case you can use 

I assume that it is trying to contact the domain controller to establish that I have some permissions or other. I don't get this with other CLR assemblies and so suspect that this may be something to do with TSQLT not being a assembly (permission set is ). Anyone know what's going on here and how I can fix this and work disconnected from my company's network without encountering this? 

This isn't possible with the proposed table structure declaratively. You would need triggers to enforce this. A unique index on both columns, together with a pair of check constraints with scalar UDFs, gets quite close however. 

columns are not updatable. Greater control over cache size with sequence The sequence value can be acquired before the insert if needed but if using sequences the various related functions obviously won't work. Inserts of large amounts of rows can be faster with due to fewer log buffer flushes. 

For the first query you can see different ordering if SQL Server uses an allocation order scan. So make sure the table is at least 64 pages in size and that the allocation order isn't the same as key order then run the query at read uncommitted isolation level. 

So the first version ends up wasting a little bit of effort doing some updates to various system tables as follows 

Then a request for either or will be satisfied from reading the persisted version of the data rather than doing the calculation at runtime so the matching appears to happen (at least in this case) in column order. 

An insert is always within a transaction. If you don't have an explicit or then the statement runs as a self contained auto commit transaction. The trigger is always part of the transaction for the action that fires the trigger. If an error occurs in the trigger that causes transaction rollback then the firing action will be rolled back too. Triggers implicitly have on. An error with this setting on will automatically lead to transaction rollback (except for errors raised in the code with the statement). 

I don't see that it makes any difference. If I try the following and compare the lock output for both isolation levels in winmerge they are exactly the same (and even putting it up to doesn't change the output). 

The actual and estimated rows are spot on and all redundant operators have been removed from the plan (so it won't spend time creating build input for the unneeded hash join in this case). 

I presume that on the source machine these were on the drive and you just copied them over lock stock and barrel to the drive on the destination? One other option now you are in this situation is add startup parameter to tell SQL Server not to start any database except . You can then use to fix up the paths for and to the new location.