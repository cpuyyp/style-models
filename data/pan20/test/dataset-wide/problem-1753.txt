The 'transport_maps' option in the postfix main.cf is the one that should be used. The transport_maps points to /etc/postfix/transport.db file that contains the entries. You can add the transport entries in /etc/postfix/transport file and run the 'postmap' command which will generate the transport.db file. 

You can telnet to localhost on port 25 and try sending the message from there to your local user account and then check the mails if the local delivery is happening. Then you can check of remote mail delivery to gmail is happening. It is always advisable to have a static I.P Address with proper reverse DNS entry set when trying to relay mails directly to the internet, the other option is to use a relay server, either the relay server of your ISP or a third party relay server. For incoming mails, the mails come in via the MX records, so if the MX records for your domain point to your server/machine then your mails will reach your machine. If the MX records for your domain does not point to your server then you can configure an application like fetchmail to fetch mails from your Mail Hosting Provider at regular intervals. There is yet another option where mails can be relayed to your server (say if you also have a hosted incoming email/spam filtering service) from a third party server and your server should accept/allow the mails from those I.P Addresses/ I.P Address range without any authentication. The actual ways of implementing the above is actually left to you. ps: I use postfix and Cyrus. Hope this information helps. 

NB: is a separate active NIC (Ethernet cross cable) but that should not matter IMHO. The problem The setup looks good to me, however, PING does not work (this was run on ): 

One of the decisions left to take is whether to use software RAID1 or hardware RAID1 + BBU. Software RAID is the solution I'm very familiar with (I'm managing a number of servers since 15 years and I know how the tools work). I never had a serious problem with it (mainly only the HDD fail). These are the reasons why I prefer software RAID. What I dislike about hardware RAID is the incompatibility between controller vendors and the lack of experience I'm having with them: different configuration options, different monitoring method, different utility programs - not a good feeling for creating a cluster system. I know that, when using a BBU, hardware RAID can both be fast and reliable (write through cache). However, since all data will be stored in a highly redundant manner in the cluster, my idea is to use software RAID1 and disable barriers in the file system to increase write performance. I expect that this will lead to similar performance like hardware RAID1. Of course, I risk data loss due to the volatile write cache, however IMHO that should be handled by the clustering mechanisms anyway (the whole machine should be able to restore data from the other nodes after failure). I'm not having concerns about the CPU resources needed by a software RAID implementation. Is my assumption correct or am I missing some important detail that would help me making the right choice? 

If the user is roaming, check if telnet to port 25,110,143 is occurring. Since you are not facing an issue with sending mails from the web interface a host of factors including connectivity, your hosts software firewall/antivirus settings or as a few other suggested before me, a corruption of your Mail Client profile could cause this to happen. If you have been provided by a web interface to sent out mails, try using that for a while till you diagnose the root cause of your current problem. 

BIND stands for Berkeley Internet Name Domain, is the most commonly used Domain Name System (DNS) server on the Internet. Named is the daemon used by BIND. 

First check if your motherboard supports visualization, then go for Xen, Qemu or KVM whichever suits your need. 

Can you paste the output of the /var/log/messages that that you have, just before the server got re-booted ? It is not possible to find out the reason for the lockup without checking the log files. Also is the lockup recurring or was it a one off event ? 

I use zabbix to monitor backupexec jobs. What I did was to enable SNMP in backupexec. So, when it executes a job, it sends a trap that is listened by zabbix, that records the traps on it's database. So, from zabbix console I can monitor all my environment. 

And finally, you should uncomment line into . Start your c-icap server like (adjust the log level (-d) as you wish) and test it using . As a response, you will receive: 

Since you wrote that the clients are win xp, I'm assuning that you have an active directory. So, you could build a script using WMI to do it to you. Take a look at this script, it will retrieve information about printers installed on a local machine. And you could join it with this script, that will retrieve all the computers registerd in your domain. By joining both scripts you will end up with a script that will walk over your network retriving printing information. At this site you will find lots of good resources. 

The retention of log files depends on the criticality of the data being logged and the actual size of the storage medium on which the logs are stored as well as the various compliance procedures in the geographical location where the Server is hosted. There is no hard and fast rule to the number of days the logs files should be retained though logs for at least a month ( space willing) would not be a bad idea. Storing old logs as tar.gz files is also a nice idea if space is a constraint. 

named.conf is the BIND configuration file, you will have .hosts and .rev files for your forward and reverse look up settings respectively in your If your registrar is also your DNS service provider, then you do not need DNS entries in both places. 

The PTR record is a must have for all Mail Servers that directly relay mails to the Internet failing which many DNS Blacklist will outright reject mails originating from the I.P Address and it could get your I.P Addresses blacklisted globally and many of the DNS Blacklists are globally replicated depending on the Blacklist. 

We are facing a virus problem on our network, but I'm unable to identify it, so we can't properly deal with it. The symptoms are that the virus duplicates a word document (.doc) generating a new archive with the same name, but with an exe extension, and, after that, the virus hides the original file. So, when the user clicks over the file, it propagates itself. Symantec AV seems to be able to block it: every time that the virus tries to generate the exe, symantec blocks it, but at this point, the original file was already converted to hidden, so the user thinks that the file has been deleted. Symantec identifies it as a simple trojan horse. I already started a full scan, but it didn't found nothing. I'm trying to know the virus name in order to fight it. Does anyone has any kind of information? TIA, Bob 

I have Linux devices with a single ethernet interface and two IP adresses. The first () is statically configured to . The second () is configured via DHCP and it may happen that it gets a similar IP like , meaning that the subnets overlap. The routing table looks like this: 

Bonding A Linux Bonding interface is now set up. Again, since this is just a first proof of concept test, I'll only add a single slave to the bond. Later on there will also be a real separate Gbit NIC with a dedicated switch that will act as the primary slave (with the VPN being just a backup), but for now the bonding interface will use the VPN only. 

I have a relatively simple web application that consists of two Docker containers working together. One container hosts the web server and the other one hosts a image rendering software. On my development machine it works flawlessly using . For production this application will make use of Amazon SES and some database like Amazon DynamoDB. In that combination the containers itself don't need any persistent storage. Because of those services, my first thought is obviously to use Amazon EC2 to host the application. My primary goals now are: 

My suggestion: go with something that already has proved monitoring capabilities. Do not develop the SERVER SIDE monitoring tool. You can use open source solutions such as zabbix or nagios and even paid ones. Zabbix has something that is called UserParameter. You can use it to extend zabbix "standard" capabilities. For instance, you could create an application (or a VBS script, powershel, etc) that verifies how many logged users the kiosk has (or anything that you need). By using a local zabbix service, this application/script will be called and executed, and the gathered value will be sent to zabbix server. Using this technique, you could setup zabbix to send alerts/thresholds to inform you when the kiosk has too many logged users. By using a "professional" monitoring tool, you'll also be able to monitor other kiosk's aspects, such as cpu, memory, network, etc. The company that I work for has some kiosks, and we use zabbix to monitor it. --EDIT -- Regarding your statement "so that the customer can view particular details remotely", this is another requisite that, in my opinion, drives the solution to an already established one: security. Zabbix for instance can be "attached" to an LDAP server and it has a very good control over the users (what a user can see and what a user can do). 

At the beginning of the website launch I expect that a single EC2 instance will handle the load easily, with perhaps peaks of two or three EC2 instances during holidays. Amazon's own Docker hosting solution (with load balancer and auto scaling) looked very promising, but being an Amazon newbie I haven't been able to get this thing working, also because the configuration looks very complex and you have to get the pieces together yourself. I'm pretty familiar with other types of clusters (mostly Proxmox-based) but still the Amazon Web Services looks complex (at least the configuration console). Is there any alternative solution that makes this easy? I spent a lot of time looking into Docker Cloud/Tutum (seems expensive), Docker Swarm, Rancher, and others (most of them "beta"), but still am confused which way to go. How would you deploy such an application? 

I'm trying to get Linux bonding working over a VPN (GRE-TAP). The funny thing is, that it only works when I have running on both hosts, but more on that later... There are two machines, called and . They are connected together using a simple switch via eth1. 

I recommend you to spent some time reading COBIT, that is Control OBjectives for IT. In fact it's used by many auditing companies to audit IT area. I also recommend you to use tools such as nessus (that will check your network/servers for vulnerabilities) or mbsa (microsoft baseline security analyser), but it will only check windows hardware. Since you asked for a starting point, I think this could help you. 

Putting third party scripts into an could be an alternative. Common sense is that an iframe has a high cost (in terms of load), but if it's cost is less then the cost of loading third party elements, you could give it a try. You probably you did most of things that are discribed here, but it's a good resource anyway. 

And what about , and ? With SNMPD I'm sure that's safe to turnoff. You'll only need it if you intend to monitor the server using snmp. I'm not really sure about NTP and POSTFIX. As far as I know, NTP is network time protocol, so if you don't intend to sync it with other devices, should be safe. And POSTFIX is an email solution, isn't? So, I think that's safe also. Regards, Bob