I'd say if you are trying to limit yourself to noun phrases, you are probably better off with "because of NP". I'd suggest you use the following in the search bar, and limit yourself based on the resulting table: 

In this case, few and a couple overlap and allow for a choice in everyone's personal dictionary. So this has nothing to do with stems, it is based purely in semantics. Also exists in, say Hungarian, where the word has the same exact connotation meaning a couple (male and female of the same species) and also a few, more than two. 

Otavio, I am afraid, you will not be spared the expense and the honour to come up with the full chart yourself. However, the following might be helpful: Diachronic Studies in Romance Linguistics has a rudimentary outline for some changes. From Latin to Modern French: on diachronic changes and synchronic Variations meets some of your other criteria. 

Though there is no clear measure for linguistic independence, I'd be tempted to say no to your question: "are words more independent from syntax in non-analytical languages?". Analytical languages rely heavily on word order to convey a particular meaning whereas say agglutinating languages are a lot more generous so word or constituent orders. But there are still subtle differences in focus: For reference: 

This is a lexical gap, also borderline case of an accidental semantic gap ($URL$ Simply put, there is a concept that is missing a word from the lexicon. Therefore the next best thing that is available, its immediate neighbour pours into that space and integrates its meaning. In this case, on an imaginary scale, this would be along the lines of: 

It is ambiguous. Spoken language is a lot looser than well formed written sentences thus there is no way for us to even tentatively say yay or nay. Intonation, stress, non-verbal sings would point in a direction, but even then, there is no way to ascertain the speakers intentions. Consider the following scenarios. Jack: My mother passed away yesterday. Jill: I'm sorry ... Jill: That you guys are idiots ... is an understatement. vs. Jill: I'm sorry ... Jill: That you guys are idiots ... but it is something I'll need to learn to live with. Final thought on this: Interpreting isolated utterances does not make much sense from a linguistic point of view. Interpreting their written transcription makes even less sense. For a successful attempt, you need context, and very often more than just the transcription of the words. 

The tagger, as pointed out above seems glitchy as it does not interpret the punctuation tag properly. This because of [n*][y*] fails badly, and I could not squeeze out a reasonable output with any variation at all. PUNC would be the punctuation mark, but it does not seems to parse correctly: 

At my previous company we used the Stanford sentiment algos - it was a mixed blessing, took a fair amount of tweaking and calibration: $URL$ For Twitter, it was surprisingly reliable as the short discourse limits ambiguity. Other than that, several machine learning algos are there for you to train. Burak Kanber has a pretty good blog around it - though he used JS for a naive Bayes classifier: $URL$ Limitations of machine based solutions: sarcasm, ambiguity, cynicism, metaphors and similar mythical beast are still beyond its capabilities. 

So clearly the syntax plays a major role in differentiating in terms meaning even though the same morphological constituents are at play. 

Though your exact research purposes are not quite clear, based on your specs, I believe the University College London English Usage Resources ($URL$ would be a good place to start with. (Though it is diacronic 1950's to 1990's, methinks.) Similarly, the Boston University Noun Phrase corpus also has a pretty decent search facility: $URL$ But, probably in terms of breadth, Brigham Young University takes the lead with its incredible list of corpora: $URL$ 

The answer is yes. They are called loanwords, more specifically popular borrowing - where the word used by the other culture is adopted without translation. Unlike learned borrowings that are often influenced by schooling or science, popular borrowings, popular in the sense of adopted by the masses, have always been the means to name something that does not exist in the original culture. The Wikipedia entry has a few gems ($URL$ for example. Though I am not aware of any studies that specifically set out to follow up on trade routes, the fact that these new words appear in a language clearly indicate that they were in need of a new term for a new thing. 

In just about every respect r behaves like the two glides: w and j. So Why exactly is it not classed as a glide? It is particularly its intrusive behaviour that makes me wonder why. 

Then you have semantic trees / databases - such as WordNet ($URL$ that help you extend your corpus based on data already mined for you. Third, manual review. Lots of it. Your Bag of words will need to be trimmed seriously if it is to be truly useful. The right and wrong branches of semantic trees will also need to be pruned. In any case, this is labour intensive and may require serious expertise in certain fields. Having said that - you may be better of purchasing some of these corpora. 

I don't thinks this is an interference in the production process. There are rules at play here, as these forms are entirely predictable. Consider the following pairs for now: 

Consider two phenomena: Dialectal differences emerge in isolation. The more isolated a community is, from a linguistic perspective, the more likely it is that its dialect will differ from those of its neighbours. Children do not learn their dialect from their parents, but from the reference groups in their childhood, most likely peers, nannies, preschool teachers. Taking these two into account, if a part of the linguistic peer pressure comes from say television that uses a different dialect from that of the kid's parents, and local community, as a result, the child will use a slightly mixed dialect. So in a physical sense, telecommunications does break down some of the boundaries conducive to dialectal differentiation. But... as communities will still need to strengthen their cohesion, they will isolate along different, non-physical boundaries. To name a few: age (various layers of teenage groups will use markedly different vocabularies for instance), sports(tennis players mingling with say beach-ballers), class (you are free to use the previous example to point out working class vs leisurely classes), hobbies (gamers vs surfers)... There is an interesting article on contemporary changes influenced by television: $URL$ It takes a slightly different view, arguing that television may actually play a role if the adoption of dialectal variants. I am not sure if this counts as "slow[ing] down dialect differentiation" as you put it. 

You are looking into implementing probably three strategic pillars for your corpus. Machine learning has given computational linguists great tools, and one that you are probably needing is a plain-vanilla bag-of-word algorithm ($URL$ Python is usually a popular choice. Take a look at: 

You might have a hard time collecting a sufficiently large number of responses to validate your analysis. However, you could turn this around and look at how various languages express certain primitive / instinctive emotions. This wiki entry: $URL$ has a fair amount of reliable data both for emotions and for things that have characteristic sounds - these could serve as a reference point for both theories as well as a pretty solid ground for your research hypothesis. This is by no means supposed to imply not to conduct your experiment, but it might be a lot more tricky to conduct than you think. A fascinating read that would also be appropriate for the second half of your questions would be: $URL$ Even if you do not have the time to read it cover to cover, its overview of language origin theories would be a solid background. The experiment design sections may also be beneficial for your own poll setup. 

What actually is going on that the various rules are competing during the process of production, and if your lexicon fails you, your morphology helps you out. And the same goes on at the speech community level, hence some speakers may prefer to use a set lexical form: , others may consistently opt for morphological suffixation from the verb: ; while others feel that the most popular -[t/s/x]ion suffix is so universal that they no longer need to remember odd-ball forms, hence . Finally, there are those who say, in English if it's good for a verb, it's good for a noun with an initial word-stress, thus we end up with . Add a few hundred years, and a few generations of language learners and you get a beautiful, healthy, vibrant grammatical faculty that enables both speech creation and comprehension for most of these word forms. Stekauer: English Word-formation: A History of Research, 1960-1995 has a nice detailed analysis of there here: $URL$ Examples like and the like are also highly similar, but cause too much havoc among descriptive and prescriptive linguists. 

You are treading on a slopy terrain. What is evolution? How do you measure its speed? What do you compare it against? 

One approach that linguists use is to train themselves in reverse chronological order. I'd suggest you do the same: Read a few hundred pages in each century, going backwards. Choose similar genres and topics if possible. This allows you to adjust to changes in vocabulary, grammar or simply fashion in phrasing. Three things you may want to look out for: #1 You may go back to the 1800's for your first read, but from there on, do not move on until you feel at ease reading. #2 Plan out your journey beforehand. Make sure you know the final piece you want to arrive at, and choose the intermediate steps so they lead into it. #3 Never just read a single author, and allow yourself some margin for error. Some books from the 1600's may simply just not cut it today. 

@Probably, by the looks of it, you are in for a lot of primary research, though, I'd suggest you consider flipping it upside down, for the following reasons: 

I myself had a fair bit of struggle with English dialects both regional and in terms of time. What I found most useful as a speaker and as a linguist is actually watching different interpretations of the same plays from Shakespeare. I would argue the following are the most influential reasons: - Most actors receive extensive coaching on the pronunciation nuances, so it is just as good and as authentic as it can get. - You have time to read it beforehand, as indeed you should, to prepare. - The language is shown in context which makes the interpretation much easier. - Watching, listening, processing it several times deepens the experience and opens up those nifty details that often escape us the first time. If you have the opportunity, Shakespeare's Globe, live in London, is a marvelous experience. (Just sit on the side otherwise you might end up baking in the sun for 3 hours.) If you cannot be there in person, they often televise live performances to cinemas and theatres globally to audiences abroad. It is also a moving experience, I can personally attest to it. ($URL$ The popular Hollywood interpretations are also surprisingly good - definitely worth watching as one of several viewings (Take the 1993 version of Much Ado for one, for instance - $URL$ And the BBC has an incredible collection of Shakespeare ($URL$ available on Amazon as well. Alternatively, Jane Austen, a hundred years later would also be an excellent choice for such an endeavour.