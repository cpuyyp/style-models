The first thing that I noticed is that the query plan compilation time was over 3 seconds for each query. Wow, this is a really complex query! Because the solution space of potential execution plans is so large (it grows exponentially with the number of the number of objects involved in the query), SQL Server is only going to be able to explore a tiny fraction of the potential query plans when coming up with a plan for these queries. Remember that SQL Server's job isn't to create the best query plan possible, but instead to create a query plan that is good enough and to do so as quickly as possible. I have often found that small changes in the way a query is formulated, even if they don't impact the logic of the query, can have a significant impact on the query plan. Anecdotally, this grows more and more likely as the query grows more and more complex. One possible reason that this could happen is that a tweak to the query might cause SQL Server to begin cost-based optimization with a different initial plan. As cost-based optimization proceeds, this different starting point could yield a different exploration of the space of potential query plans--kind of like a different random seed impacts random number generation. Note that the query plans you provided are significantly different (compare images of the plan shape below!) and SQL Server actually does estimate that the @table variable plan is slightly cheaper. In terms of why the table variable vs. temp table would have such an impact on cost-based optimization, I'll hazard an only-partially-educated guess: inserting into a table variable forces a serial plan (see the of that appears in the table variable plan, but not the temp table plan), and this may impact the code path that the query optimizer takes either generating an initial plan or in some phase of plan optimization. If possible, the next step that I would try is to simplify the query so that fewer tables are used and/or the query is split into multiple queries (with intermediate #temp tables) so that each query is simpler and has better statistics available. If that's not possible, you could also try more hacky options such as using query hints (e.g., force MAXDOP 1 on the temp table query, and see if the plan comes out more like the table variable query). Query plan with #temp table: 

To store rows in a b-tree and perform a seek, all that is needed is an ordering in which the rows should be sorted. Just like you can sort on , you can also sort on the tuple . In the latter case, rows are first sorted by and then any ties are broken by sorting by . The resulting index will use the same b-tree structure, but the value for each entry in the b-tree will be a tuple. 

And now on to the details: Create rowstore data set Nothing too exciting here; we create 40MM rows of pseudo-random data. 

I had a script that does a rudimentary version of foreign key traversal. I adapted it quickly (see below), and you might be able to use it as a starting point. Given a target table, the script attempts to print the join string for the shortest path (or one of them in the case of ties) for all possible source tables such that single-column foreign keys can be traversed to reach the target table. The script seems to be working well on the database with a couple thousand tables and many FK connections that I tried it on. As others mention in the comments, you'd need to make this more complex if you need to handle multi-column foreign keys. Also, please be aware that this is not by any means production-ready, fully-tested code. Hope it's a helpful starting point if you do decide to build out this functionality! 

These queries yield results like the following, and in this case rows 1, 6, and 9 would be marked as being too fragmented for optimal performance because the full-text index is over 1MB and at least 10% fragmented. 

Thanks for adding the query plan; it is very informative. I have a number of recommendations based on the query plan, but first a caveat: don't just take what I say and assume it's correct, try it out (ideally in your testing environment) first and make sure you understand why the changes do or don't improve your query! The query plan: an overview From this query plan (as well as the corresponding XML), we can immediately see a few useful pieces of information: 

is not going to see the not-yet-inserted row, which is why it returns true on this second INSERT in your example: 

It's not clear why monitoring pg_database_size(your_database_name) doesn't give you the information you are after. Plot the size of all your databases daily, fit a curve to the plot, and you should be able to come up with an estimate of when you need to buy more disk space. What else do you need to know, and why? For this part of your question: 

Each of those directories with an integer as the directory name represents a database inside my PostgreSQL cluster. The integers (OIDs) in the directory name match the you would see from a query like: 

What happens when a hits is that dirty buffers held in are guaranteed to be written (i.e. fsync'ed) to disk. The WAL files must have already been fsync'ed to disk at COMMIT time, assuming synchronous_commit=on and fsync=on, etc. Your question of: 

H/T to Erwin for this tip. Remember that these XID values wrap around at 2^32. For what it's worth, I think that blog post you linked to is excessively complicating this topic. To watch out for XID wraparound, you really just need to check: 

Postgres can actually (in the following contrived case) use an index to satisfy queries without adding range scan kludges like the suggested . See the comments on Craig's questions for why Postgres is choosing this index in this particular case, and the note about using partial indexes. 

In your example, the you have set of 256MB should be seen by the subsequent command, because you have changed this GUC inside your session. In fact, the docs suggest bumping up (as you showed) for just such a purpose. 

The comments so far are roughly correct, but to give an authoritative answer from looking at src/backend/tablecmds.c: If you're only performing , then will be invoked to handle the , and it uses to perform a WAL-logged block-by-block copy of the table. However, if you were to specify additional actions to the command which require table rewriting, then the new copy of the table should be built (and compacted) in the new tablespace via . 

If you'd like a snapshot of your primary database refreshed nightly, you could do this with a cron job restoring RDS snapshots every night. I don't think RDS has a button to do this automatically for you, but it shouldn't be too hard to script up a nightly create-db-snapshot + restore-db-instance-from-db-snapshot using the AWS CLI, or boto, or whatever interface to AWS you like. You could even maintain a Route53 entry which would always point to the most-recent instance, and leave the old instances lingering for a day or so before being killed off, so that sessions running against existing instances overnight wouldn't be interrupted. 

has some logic by which it determines whether it is safe to keep and reuse the password you entered initially, see the logic about in command.c. Presumably when you are able to use the meta-command to reconnect from within an existing session, is preserving your initial password and reusing it -- either that, or your pg_hba.conf rules allow a connection without a password with the given user/database, e.g. a trust or ident rule. 

Sounds like your terminal's pager is taking over the output: you can tell psql not to send its output to the pager when you're using the invocation via: 

Supposedly it is possible to hook up Bucardo to RDS now that RDS Postgres supports the session replication role, but if you want a nightly snapshot I think you'll be much better off using RDS instance snapshots. 

and raise an alarm if that max. age is over 1 Billion or so. And if it gets close to 2 Billion you are in imminent danger! You can use a tool like check_postgres.pl which will handle this check for you with some configurable thresholds. 

I'd be interested to hear what privileges your existing superuser role had which were not automatically granted to whatever new superuser role you created by default. Edit: and if you're interested in copying over the per-role configuration parameters (i.e. those documented under configuration parameters), then you could use a function like this (demo only, you may need extra error handling, security considerations, etc. for production use): 

You may also put the line in the file , however it seems ~/.psqlrc directives are honored only in interactive mode, not for commands passed in via . More great tips for non-interactive psql usage from Peter Eisentraut.