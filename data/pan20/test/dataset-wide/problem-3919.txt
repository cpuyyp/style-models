However, I find that 6" devices are more portable, unlike the (bulkier?) 8" or larger screens. But having a smaller screen means less screen estate and the PDF format cannot re-flow correctly because the page layout is designed for a specified size and is constrained by graphics and positioning of other elements. This means that what you see on the screen is a smaller version of the page, sometimes not easy on the eyes or downright illegible. The solution is to read in landscape mode and, if possible, to cut the page margins in the PDF. For PDF and Dejaview formats take a look at Pocketbook 301+. In my opinion, this is the best PDF reader in the 6" class. There is a newer model (Pocketbook 302) but it has a glossy touch screen, so I would avoid that. I use it mostly for technical documents, heavy mathematics with graphics and it's great because: 

I heard good things about Combinatorics and Graph Theory by Harris, Hirst and Mossinghoff (Springer Undergraduate Texts in Mathematics). It's easier to read than Diestel or Bollobas, but not dumbed down. But of course, it's neither thorough nor exhaustive. 

The PB301+ is a EB600EM clone with 8 gray scale levels (revised in 2009). Do not mistake it with other EB clones (there is a gazillion of them): the firmware is what makes Pocketbook a great PDF viewer and you cannot install it on other clones. Concerning larger device: there is a number of them arriving soon on the market, including the Pocketbook 901 (9.7" screen size) expected in September 2010. Beware of the touch screens: they reflect a lot (screen glare). I owned a Sony PRS600 but I had to send it back. I really wanted this to be a non-issue, but the glare and the reflections ended by getting in my way when reading: I needed a constant conscious effort to not see the reflection of my face in the device, all this combined with a sort of "fuzzy display" due probably to the "through the glass" effect induced by the screen. On the web, the mother-lode site of mobile readers is $URL$ (the technical descriptions are here: $URL$ The forums are a good source of personal opinions. Now, surprisingly, reading mathematics on a 6" screen turned out to be a much better experience than I have thought. This is subjective, but again, I expected worse. Here are some of my impressions (for a PB301+, 6" screen): 

I am not sure what exactly you are looking for. But, looking at the data, a linear one dimensional model does not fit so well. To see this, try CCA (Canonical Correlation Analysis): the linear version computes two linear transforms of the input spaces, such that the transformed data are maximally correlated. I suspect from your data that the maximum correlation you will get is no more that 0.7-0.8. If you are looking to "explain the data", you need a generative model, perhaps a Gaussian Mixture for good results. For the practical aspects of data analysis, you can try Handbook of Statistical Analysis and Data Mining Applications by Nisbet, Elder, Miner. If you look to understand the theory, The Elements of Statistical Learning by Hastie, Tibshirani and Friedman is the standard graduate text (though it might be too difficult, if you are not at ease with probability, measure theory and some functional analysis.) 

I don't think it's possible to find a "nice" (say, smooth) function $f \in L_2(\Gamma \backslash \mathbb{H})$ such that $(1) \int_0^{1} f(x+iy) dx = 0$ for all $y > 0$ and $\lim_{y\rightarrow \infty} f(x+iy) \neq 0$. This may be total overkill, but consider the spectral decomposition of such an $f$, namely $$(2) \qquad f(z) = \sum_{j} \langle f, u_j \rangle u_j(z) + \frac{1}{4\pi } \int_{\mathbb{R}} \langle E(\cdot, 1/2 + it), f\rangle E(z, 1/2 + it) dt.$$ By unfolding, the inner product of $f$ with the Eisenstein series $E(z,s)$ is zero by the assumption (1); initially this is easy for the real part of $s$ large but then follows by analytic continuation. By inserting (2) into (1) we see that $\langle f, u_0 \rangle = 0$, that is $f$ is orthogonal to the constant eigenfunction. Now in (2) take $z= x+iy$ with $y$ large. Each term in the sum is very small since all the Maass forms vanish at the cusp, and the projections of $f$ onto the constant eigenfunction and the Eisenstein series are zero. 

If $x$ is large enough, then Rankin-Selberg theory will show that $S(x) \gg x^{1-\varepsilon}$. However, if $x$ is not large enough, then it is unknown how to obtain a lower bound for $S(x)$. In particular, it is unknown how to show that $S(x) \neq 0$. A good starting point for this is Chapter 13 of Iwaniec's book, Topics in Classical Automorphic Forms. 

In some of my previous work on mean values of Dirichlet L-functions, I came upon the following identity for the Gamma function: \begin{equation} \frac{\Gamma(a) \Gamma(1-a-b)}{\Gamma(1-b)} + \frac{\Gamma(b) \Gamma(1-a-b)}{ \Gamma(1-a)} + \frac{\Gamma(a) \Gamma(b)}{ \Gamma(a+b)} = \pi^{\frac12} \frac{\Gamma\left(\frac{ 1-a-b}{2}\right) }{\Gamma\left(\frac{a+b}{2}\right)} \frac{\Gamma\left(\frac{a}{2}\right)}{\Gamma\left(\frac{1-a}{2}\right)} \frac{\Gamma\left(\frac{b}{2}\right)}{\Gamma\left(\frac{1-b}{2}\right)}. \end{equation} As is often the case, once one knows such a formula should be true then it is easy to prove it. I give my proof below. My questions are 1) Has this formula been observed before? I have no idea how to search the literature for such a thing. 2) Is there a better proof? (Of course this is totally subjective, but one thing that would please me would be to avoid trigonometric functions since they do not appear in the formula.) Proof. Using \begin{equation} \frac{\Gamma(\frac{s}{2})}{\Gamma(\frac{1-s}{2})} = \pi^{-\frac12} 2^{1-s} \cos({\textstyle \frac{\pi s}{2}}) \Gamma(s), \end{equation} the right hand side is \begin{equation} 2 \frac{\cos(\frac{\pi a}{2}) \cos(\frac{\pi b}{2}) \Gamma(a) \Gamma(b)}{\cos(\frac{\pi (a + b)}{2}) \Gamma(a+b)}. \end{equation} On the other hand, the left hand side is \begin{equation} \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)} \left( \frac{\Gamma(a+b) \Gamma(1-a-b)}{\Gamma(b) \Gamma(1-b)} + \frac{\Gamma(a+b) \Gamma(1-a-b)}{\Gamma(a) \Gamma(1-a)} + 1 \right), \end{equation} which becomes after using $\Gamma(s) \Gamma(1-s) = \frac{\pi}{\sin(\pi s)}$, \begin{equation} \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)} \left(\frac{\sin(\pi a) + \sin( \pi b) + \sin(\pi(a + b))}{\sin(\pi(a+b))} \right). \end{equation} Using trig formulas, we get that this is \begin{equation} 2 \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)} \frac{\sin(\frac{\pi}{2}(a+b)) \cos(\frac{\pi}{2}(a-b)) + \sin(\frac{\pi}{2}(a+b)) \cos(\frac{\pi}{2}(a+b)) }{\sin(\pi(a+b))} \end{equation} I think I've run out of space? The rest is easy trig. 

To make notation a little bit lighter I am going to use $N=(N(t)_1,N(t)_2)$, $X=(X(t)_1,X(t)_2)$, $X'=(X'(t)_1,X'(t)_2)$. $x$ is any of the nine possible answers. The only case when $Pr(X'=x) \neq 0$ is when $N=0$. If not $X'$ is continuous, because $Y$ is continuous, and $Pr(X'=x)=0$. $ Pr(X'=x) = Pr(X'=x|N=0)Pr(N=0) + Pr(X'=x|N\neq 0)Pr(N\neq 0) = Pr(X'=x|N=0)Pr(N=0) + 0 = Pr(X=x|N=0)Pr(N=0) = Pr(X=x)Pr(N=0)$ Step 1: $Pr(A) = Pr(A \cap B) + Pr(A\cap B^{\complement})$ Step 2: we know that when $N\neq 0$ $Pr(X'=x|N \neq 0)=0$, as explained above. Step 3: if $N=0$ then $X'=X$ Step 4: $N$ and $X$ are independent. $Pr(N=0)$ can be calculated using the link you provided. 

I think altering the notation a little bit can be useful here. Let's start with the hand that player 2 has been given. This is a binary variable ($H$) with probability $Pr(H=K)=p$ of event K. In code, $H \sim Bern(p)$. Second, the actions that can be taken by player 2. Let $n$ be the number of possible actions he can choose. I am going to assume that whatever hand he has been dealt, he can choose from the same set of actions. Some of those actions might not be smart to choose, but he has that option. Third, the probability of player 2 taking action $i$ for $i \in \{1, ..., n\}$. Let us call this probability $X_i$ and let $X=(X_1, ...X_n)$ the vector with probabilities of all actions. Whatever hand player 2 has been dealt. So get rid of $Y_i$. You assume that $X$ has the Dirichlet distribution. That can be correct, but it should be noted that $X$ can only have this distribution conditional on $H$. Thus, $X|\{H=h\} \sim Dir(n,\alpha)$ ($h$ can be a number representing $K$ or $Q$). Only then the condition that their sum equals 1 is met. So the probability of player 2 taking action $i$ depends in some way on the cards dealt, but I did not specify how. I think the most reasonable choice is to let this happen via $\alpha$. Thus, your model would look something like $X|\{H=h\} \sim Dir(n,\alpha(h))$. The expectation of $X_i$ can then be calculated using $E(X_i) = E(E(X_i|H)) = E(\frac{\alpha_i(H)}{\sum_i \alpha_{i}(H)})$. The exact expression and whether it can be obtained in close form depends on how you let $\alpha$ depend on $h$. I hope this helps. 

Dear all, I have recently been breaking my head over this question. The idea is that a certain variable $Y$ is normally distributed with a parameter $X$ in both mean and variance. $Y|X \sim N(\mu X,X^2)$ This parameter $X$ is assumed to be normally distributed as well with parameters $\alpha$ and $\beta$. $X\sim N(\alpha, \beta)$ Now I am interested in the distribution of $Y$ (with $X$ marginalized out). My current progress: Simulation shows me that the distribution of Y seems approximately normal as well. This does not proof anything off course. The integral $\int_{all x}f_{Y|X}(s;x)f_{X}(s)dx$ seems unsolveable. Kind regards 

There's a nice paper by Kowalski, Robert, and Wu that discusses this problem. It's on the arxiv here. 

The problem with using Euler-Maclaurin is that $e^{2\pi i \tau x}$ is oscillatory. The remainder term in the Euler-Maclaurin formula will involve the integral of the absolute value of the derivative of the summand. The oscillation of $e^{2 \pi i \tau x}$ means that this derivative will be roughly the same size as the summand itself so the remainder term is not helpful (unless $x$ is going to zero in some sense). This is in contrast to the case with $x=0$ where each differentiation gives an extra saving factor of $(\tau +a)^{-1}$. 

Here's a variation on the arguments using the explicit formula. If there were no zeros in the critical strip then the explicit formula would say $$\psi_0(x) = x - \frac{\zeta'(0)}{\zeta(0)} - \frac12 \log(1-x^{-2}),$$ where $\psi_0(x)$ is the same as the usual $\psi(x)$ except when $x$ takes integer values $\psi_0(x)$ is the average of the left and right limits of $\psi(x)$. The right hand side is continuous for $x >0$ while the left hand side is not. The argument generalizes to show that there cannot be finitely many zeros in the critical strip, or even an infinitude that do not grow "too quickly." Precisely, if $$\sum_{\rho} \frac{x^{\rho}}{\rho}$$ converged fast enough to form a continuous function of $x$ then the same argument would carry over. 

See Montgomery's book, Ten Lectures on the Interface Between Analytic Number Theory and Harmonic Analysis, specifically the examples on p.55. He considers variants of your sum, namely $\sum_{n \leq N} e((n/3)^{3/2})$, and $\sum_{n \leq N} e((2n/3)^{3/2})$, and shows that they have very different asymptotic behavior. The former is asymptotically $c N^{3/4}$ while the latter is $O(N^{1/4})$. These examples illustrate why the hypothesis (3.3.3) in [GK] is assumed. 

These types of questions are pretty speculative. One should be aware firstly that there is no reason for there to be roughly equal numbers of rank 2 curves and rank 3 curves. Mark Watkins has a paper where he comes up with a conjecture, using random matrix theory, that the number of rank 2 elliptic curves with conductor up to $X$ is asymptotically $c X^{19/24} (\log X)^{3/8}$. The paper is: Mark Watkins, Some heuristics about elliptic curves. Experiment. Math. 17 (2008), no. 1, 105â€“125. At the end of section 4 of the paper, he remarks that possibly there are around $X^{\frac{21-r}{24}}$ elliptic curves of rank $r$, for each $r \geq 2$, compared to around $X^{5/6}$ total elliptic curves.