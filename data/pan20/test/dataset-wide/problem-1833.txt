You want your Hyper-V Host to be as clean and as simply configured as possible. It is highly recommended to not install other applications or roles onto your Hyper-V host, especially one as complex as ConfigMgr. 

Let's say your provider has given you 203.0.113.0/28 for address space. You decide to chop it up into two separate subnets: 203.0.113.0/29 for DMZ machines and 203.0.113.8/29 for internal machines. Your firewall sits between your entire setup and the internet and has three interfaces: one for your provider's upstream connection, one for 203.0.113.0/29 and one for 203.0.113.8/29. Any communication between any of these networks will thus pass through the firewall where you can do the following important things: A) selectively pass only the traffic you need to between hosts, B) monitor and log that traffic. The real goal is that there should be no direct communication between any of these networks. This is the ideal you should strive for. 

Is this 100 simultaneous clients total or 100 simultaneous clients per access point? Either way accomplishing this on your budget with any semblance of reliability will be incredibly difficult if not impossible, although the former is within the realm of possibility, just not your budget unfortunately. The reason you don't find any data on how many simultaneous clients consumer-grade access points can support is that consumer-grade access point barely support a dozen clients. They are just not designed for this type of use case. Cisco's recommendation is 24 clients per access point and that is with "enterprise-grade" Aironet equipment (reference). There is no product/s in the market that I'm aware of that can accomplish your goals within your budget. Anyone who tells you otherwise is probably lying. Robust industry grade 802.11 networking is very difficult to do right and thus correspondingly expensive. I would avoid DD-WRT (especially DD-wRT! UGH!) and consumer grade access points if at all possible. You'll be better served by putting your existing budget towards hiring a specialty contractor to temporarily setup and rent you a wireless infrastructure or investing in proper equipement. 

That is really an application design problem. You could split the download requests out to a different IP i.e. downloads.myapp.com and set maxconn=1 i.e. 1 request at a time? Or you could balance on URL so the same download request goes to the same application server: But it sounds like you just need in memory caching of content or more servers or faster I/O. 

At Loadbalancer.org we do this is a slightly complicated way :-). Before we restart HAProxy we check all of the current states and check for any servers that are DOWN or MAINT. Then we BLOCK SYN packets (handles the dropped traffic on reload issue) Then we restart HAProxy Then we manually set all the states to DOWN or MAINT (based on what they were 0.01 seconds ago..) Then we re-enable SYN packets... Giving us the required result of seamless restarts with no downtime or lost packets. If you can't be bothered with ALL the above then just set everything to DOWN state after a reload ..do it fast and cross your fingers :-). v1.6 has some new state-full restart code but I haven't researched it yet. 

I'm glad that you seem to have solved the immediate problem with fail2ban, and it does make sense to block at the iptables level, but you can do the exact same thing in your HAProxy config: You can use acl's with src_http_req_rate() or even src_http_err_rate(Abuse): I've used them in some examples of haproxy configs for DDOS mitigation here. Longer term: It sounds like you might want to implement a double login for that page, if the users will accept it then you could just put an extra htaccess password in from of the login page, so that they need to separate logins (pretty hard to crack with a script). Or you could use mod_security to do the same thing or some kind of honey trap. 

You can configure ssh to run a command of your choice when you log in using public key authentication. To do this, generate a pair of keys: 

The browsers may be caching the DNS results. It might be worth trying the query directly to the configured DNS servers to see if they are the problem -- perhaps one of them is particularly slow or down. Take the IP address from the line in and run 

The obvious solution here is to run x11vnc on the same display as Firefox so that you can at least see what Firefox is complaining about. 

I've always believed the administration advantage of SSH (I use push_check) outweighs any additional load. Modern CPUs are so fast that the cost of encrypting a handful of bytes is pretty minimal, so it comes down to running two processes (SSH and the check script) vs one (check script fired off by NRPE). For check scripts written in an interpreted language, I would expect the overhead of firing up the interpreter (Perl, Python, Bash) to exceed the CPU cost of starting an SSH session. Given modern CPUs, your machines are more likely to be disk or memory limited rather than CPU limited. Provided your Nagios machine is coping -- it has to set up 20 SSH connections every second -- I would err on the side of convenience. Not really an answer to your question, more of an argument that life is too short to worry about it :) 

Of course, anyone who can elevate their account to root on computer B can defeat this protection, so you need to lock down NFS client computers sufficiently. 

The parameter determines which domains Postfix relays to -- that is, the domains that your email server will accept mail for in addition to its mailname -- so this line isn't going to help you here (I assume this is what you are trying to achieve with it, anyway). You are restricting the MAIL FROM address to those listed in -- make sure that includes . You have fairly early in your line, and according to postconf(5), that rejects unless either 

Either set up 2 * HAProxy instances as a master/slave pair. Or why not just use Amazon Route 53? Its cheap, fast, transparent and has health checks. Load balancers are most effective when the application is clustered on multiple servers. 

Thousands should be fine, the total number of health checks on backends may become an issue but ports are not. For a definitive answer you might want to try the HAProxy mailing list. 

First of all are you sure the connections are not just hitting the queue? i.e. you have reach maxconns? What does your stats page show? Also just disable conntrack (it sucks): 

You can use socket commands to clear the table i.e. echo "clear table Abuse key 192.168.64.12" | socat unix-connect:/var/run/haproxy.stat stdio The documentation is here: $URL$ 

DDOS is never an easy problem to solve but it sounds like the type of low level attack you are experiencing would definitely be helped by the queuing facility in HAProxy i.e. don't flood the Apache server with too many requests (maxconns). You could also try and prioritise your existing customers using a cookie facility, like some busy sites do black Friday protection. 

I know this is a fairly old post but I was searching for load balancing remote desktop on AWS and came across it. Personally I prefer NOT using RD Gateway and just sticking to plain old session broker with a load balancer in front of it for high-availability). You could always use a VPN for extra security. 

Luis, Just some quick ideas: Are you sure it is actually up and running? netstat -l | grep 443 and/or ps -ef | grep haproxy Configure a stats listener so you can see if the health checks are working or not... Simplify the health check or even remove to get it working. 

Try removing parts of the system to find the bottle neck. 15 test servers sounds an awful lot! you should be able to get 1000's of TPS out of a single test unit. You are not asking HAProxy to wait for a response are you? i.e. utilising the maxconns functionality and queueing functionality? Like I said try simplifying, but if you do think it is HAProxy then please post the configuration. 

Please don't take this the wrong way but if you are doing anything serious with this process consider talking with your system administration team or hiring someone either on-premise or a contractor that has the relevant experience. A small book could be written on each of these topics. It might be fine for you to hack through some PowerShell to automate some development or testing VMs but this doesn't sound like a good road to start heading down for serious production stuff. Just a friendly word of caution... make sure your DevOps isn't all Dev and no Ops. Hopefully this gets you started. If you are doing this with any regularity and volume in a Production environment you should give System Center 2012 R2 Virtual Machine Manager and System Center 2012 R2 Orchestrator a serious look. 

Darkstat is a very simple but powerful network traffic monitoring program that displays information about network traffic moving over an interface via HTTP. It will do 90% of what you need with none of the hassle of using a more feature rich but complicated solution like Naigos. 

The offending process is SVChost.exe which is wrapping the DHCP Client (dhcpcsvc.dll), EventLog (wevtsvc.dll) and LMHOSTS (lmhsvc.dll) services. I'm certainly not a Windows internals expert but I could not seem to find anything especially amiss when viewing the process with Process Explorer other than it appears the EventLog is triggering a ton of RpcBindingUnbind calls. 

What? There's a package in the main repository: nginx (1.2.1-2.2) . If you're having issues installing stuff nginx's repositories it's because they are not maintaining them properly or haven't updated them yet. If you go to their dists folder you'll notice they have not yet provided a package tree for wheezy. Use the version of nginx included in the main repository or build your own package from the latest stable release's source until nginx updates their repository. 

Andy, The trick is to add another backend that you only use for the extra stick table. You can only have one stick table per backend - BUT you can use them in ANY front/back end... So I just add one called Abuse that you can then use as a global 60 minute ban for any backend... You will need to change my example but try something like this: 

If you have MySQL listening to BOTH ports then simply leave the backend server port out of the HAProxy config i.e. let the client decide the end port to connect to. OR if your MySQL is only listening on 3306 then force all the connections to go to that port by specifying it on the backend config. If still stuck try posting your config. 

Karl, Call me crazy bu don't you just need to remove the OR? redirect scheme https code 301 if !host_mydomain or !{ ssl_fc } should be: redirect scheme https code 301 if !host_mydomain AND !{ ssl_fc } i.e. if NOT the domain I don't want AND NOT SSL 

This should be fairly simple: a) You must configure the passive mode ip as x.135 (which you have done) b) You must restrict the port range on the ftp server (which you have done) c) You must bind haproxy to ALL relevant ports (your config doesn't show that) Be careful to remove the port from each real server when you do this i.e. 10.11.130.140:21 becomes 10.11.130.140 You almost certainly need to enable persistence/sticky in the haproxy config... and longer timeouts - 15mins? But more importantly when testing - Turn off the firewall stuff completely test if your haproxy config works from the local network - once you are happy that is all working. Then start configuring your firewall. Do you really need a local stateful inspection on each FTP server? Just configure your external firewall correctly. Hope you get it working.