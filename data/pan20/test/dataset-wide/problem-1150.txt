Whereas you can have multiple unique keys in a table and the referenced columns may allow null. By declaring a primary key, you are saying "this is the candidate/surrogate key that should be referenced by foreign keys". This is well understood by database developers who are unlikely to apply a foreign key to a unique constraint when a primary key is available. Unique constraints can also be used to enforce rules such as "a customer can only have one default contact number". A primary key can't be used for this purpose, as customers may have multiple non-default numbers, so the constraint can't uniquely identify all rows in the table. 

Your view has to apply some form of function to start and end dates to figure out if they're the same year or not, so I believe you're out of luck with this approach. Our solution to a similar problem was to create materialized views over the base table, specifying different partition keys on the materialized views. We've tailored ours to match common base queries so that we get query rewrite benefits as well. You may need to get users to use the MVs directly to ensure you get the partition pruning working as you need, rather than relying on query rewrite. (Updated to remove incorrect example and add info regarding applying functions to partition columns) 

One alternative/workaround if you can live without SSMS management, is to utilize certificates for access to the underlying sp_update_job* procs, and then create wrapper procedures (which is why you cannot use SSMS still). For a full example, see Uwe Ricken db Berater GmbH response in this MSDN post. 

Review How Merge Replication Detects and Resolves Conflicts A lineage column is used which contains the generation and the node that made the change. The actual winner decision during the conflict depends upon the conflict resolver used for the article. Using the default prority-based resolver, a priority value per node should be assigned per subscriber to determine the winner for all conflicts. With regards to referential integrity and the concept of conflict resolution in mind, the processing order with which tables (articles) are processed is your only means of guiding the conflict resolution before your child tables. You can modify the order of the article processing in the merge publication via the @processing_order parameter for sp_addmergearticle 

The difference is purely based the cardinality of the data at the point when the stats were updated, and the modifications made prior to running your query. 

Review the backupset and related tables in msdb as the source for if a successful backup was taken. If you see gaps in when a scheduled backup was to have occurred, your job either failed or never ran. Every backup command generates an internal checkpoint, which in turn, moves the log sequence number (lsn). Review the first_lsn and last_lsn pattern with every log backup in the backupset table. Likewise, a file will always be generated if your job backs up to a time-stamped filename (default behavior of maintenance plans). You can verify this behavior by manually/interactively running a log backup command multiple times on a database without an transactions between runs. 

As Justin's said (and the links in his post prove), the cardinality rule is a myth. This aside, there's a good reason to use bitmap indexes on fact tables: separate bitmap indexes on can easily be combined by the optimizer to reduce the numbers of rows to access. This is very useful with fact tables with a large number of dimensions. While any single dimension may return a large percentage of the data, when combined with others this may fall dramatically. For example, you may have thousands of orders per day and thousands of customers (with hundreds of orders each), but a given customer is likely to only have 1-2 orders on any given day. This saves you having to create multi-column b-tree indexes. As (ignoring some skip-scan conditions), the leading column in an index must be referenced in the where clause to be used. So with three dimensions you need to create six multi-column b-tree indexes to ensure an index is available for every query your users may throw at you ( ind1: col1, col2, col3; ind2: col1, col3, col2; ind3: col2, col1, col3 etc.) With bitmaps, you just need three single column indexes and can leave the optimizer to decide whether it's beneficial to combine them or not. This example shows how the two single column bitmap indexes are combined, but the b-tree indexes aren't (note Oracle can convert b-tree indexes to bitmaps, but this is rare): 

I have a database table with 900,000+ rows, 20 columns, mostly string data. Using MySQL Workbench, I want to page through the entire contents of the table to see what exactly has been inserted into the table (I am reviewing it because I believe my code somehow accidentally only partially loaded the contents of a CSV file and I want to understand why and where the process ceased loading the entire CSV file). The problem is that, because of the somewhat large size of the DB table, running a "SELECT * FROM..." query with "Don't Limit" causes my computer to take far too long (over 10 minutes) to produce a result in MySQL Workbench's result grid and basically appears to freeze up. Is there a way to make the MySQL Workbench result grid load much much faster? Or should I be using a different tool? I'm a bit of a noob, so it's possible I'm doing something pretty stupid here. Thanks in advance! 

I am creating a database table for storing numeric time-series data sets regarding cities that will be accessed via a web app. Primarily annual, quarterly and monthly data will be stored in it. There will be hundreds, potentially a couple thousand data sets, for each city. Some data sets may have data on a monthly, quarterly AND annual basis while other data sets may only be available on a monthly, quarterly OR annual basis. I am trying to decide between 2 different designs for the table's columns: Option #1 Option #2 ... With Option #1, a hypothetical row look like: 

As Phil says, you can't really to this retrospectively. You can create DDL triggers to capture this in the future however. These will fire before/after a DDL event, allowing you to capture the dependencies to a table: 

The fundamentals of indexing etc. all work in exactly the same way, so strictly speaking the only difference is the cost of getting this wrong! That said, here's a (not necessarily complete) list of things worth bearing in mind: 

The explain plan is just a prediction of the join methods that will be used when executing a query. This can make it hard to infer which step is the most time consuming, as a different plan may be followed when the statement is executed. To get actual stats about how long each step takes you'll need to run an sql trace of the statement and review the trace file - manually or using a tool such as tkprof. This will show you how many rows each step processed and how long it took. That said, looking at the listed at the end of each line will give an indication of how many rows are to be processed. Steps processing more rows are likely to take longer to execute as there's more work to do. So in your example line which is expected to process 102,068 rows is likely to be the most expensive as the other steps are predicting one row. This is only true if these cardinality estimates are accurate however; you'll need to verify that these cardinalities match the actual rows returned. The easiest way to do this is via an sql trace as stated above. 

Just place a DENY ALL on the table for that user. DENYs are always evaluated first, and will override the GRANT off SELECT to the overall schema: 

Yes. Create (database) MASTER KEY first, then BACKUP, DROP, and CREATE the certificate (importing from earlier BACKUP), but this time WITHOUT a password. This will encrypt the certificate's private key for the certificate using the database master key, which will automatically open the private key as needed. Realize by doing this, you are exposing your data (via the private key for the cert) to anyone that has access to the database and sysadmin to the server. In reality, all you have done is kept the data-at-rest encrypted on disk. Anyone with access to your backups and the service master key can decrypt your data. Make sure you follow best practices for keys (and certificates) in SQL and backup the database master key. You can view the private key encryption type by looking at the pvt_key_encryption_type_desc column in sys.certificates database dmv. 

You are talking about removing data piecemeal from a database. That is 3rd party only if you are trying to use backups only, but that really isn't an ideal scenario for that use-case. If you are that short on space (so much so that you cannot have a third copy of the full database), create another database (name it "DevDB" for example) on the stand-by server, setup an ETL process to script off the objects of your database, and then import only the data you want into the "DevDB" database. 

You can use the package to "re-organise" a table like this. It provides functionality for you to create a temporary table with modified contents of an existing table as you describe. It also allows you to copy over the grants, triggers, indexes etc to the new table, then switch the tables over so the temporary table becomes the live table. See this article for a worked demo. 

This also has the benefit that you can add further fields to give more information about why a resource was allocated to a project on a given day, should this need arise in the future (e.g. general notes, unbudgeted resources, project overrun, etc.) I'm not clear what your requirements are regarding allocating resources to single or multiple projects. If some resources can only be assigned to one project on a given day and others to multiple projects on a given day, then you could have two allocation tables with the structure above. The different would be in the primary key definitions: 

However, to get these benefits you need tables where you (nearly) always include the leading column(s) of the primary key in queries and you're likely to be fetching several rows at once. Some common examples of such tables are: 

If I do #1, I will have hundreds (or maybe a couple thousand?) of columns and many rows will have a lot of 'blank' spaces because a lot of data sets will not have data for each row (e.g. some data that is only available quarterly will always be 'blank' in columns for rows describing monthly data). Also, I think there will need to be a corresponding table for describing the sources and units (e.g. U.S. Dollars, Square Miles, etc.) for each dataset column. I am concerned that the large number of columns and blank spaces may be problematic. Alternatively, if I do #2, I will have very few columns but millions (potentially billions?) of rows and will have many queries with a clause searching to see if the column equals a certain string (e.g. ). Also, since all types of numeric values are stored in the column, the data type here will have to be pretty flexible (Maybe something like ?). I'm primarily concerned this may become slow with a lot of data. I am using MySQL. My Question: Is possible to say which of these 2 general table designs is most likely the better design choice if my primary concern is query speed? Or is it fairly clear that I should be approaching this in a completely different manner? 

As you've identified, storing the price on the order makes the technical implementation easier. There are a number of business reasons why this may be beneficial though. In addition to web transactions, many businesses support sales through other channels, e.g.: 

Oracle database licenses are quoted per processor, not per machine. They don't care whether you have two machines with two cores each or four machines with one core each. Both equate to 4 licenses you need to purchase. For clustered installations, you need to purchase the appropriate database licenses and the RAC licenses. 

Mostly you'll learn by writing queries, looking at the (expected) explain plans and comparing these to the actual execution plans (either via tracing the query or using the SQL monitor). Then re-write the query, add/remove indexes, etc. and see how it affects the plans and execution times 

I would strongly consider explicitly storing all the days a particular schedule actually runs on. This would give a structure looking something like this: 

There will be cases where the compound index saves you 1-2 IOs. But is it worth having two indexes for this saving? And there's another problem with the composite index. Compare the clustering factor for the three indexes including LOTS_VALS: 

To get the total, you need to put the and within a and exclude these from the . This will result in you having just one row for each , pair. Also, you don't need a clause when you have a - the grouping will remove the duplicates for you. If you're getting duplicate values you weren't expecting, it's because your group by columns are wrong. This gives you a query looking like this: 

Execute the above right after you have updated the stats being used, and note the modification_counter value. It should be zero. Now run the original query, then the above again. The difference between the estimated rows minus the last modification_counter value (assuming you are not deleting rows), then you should have your answer. The stats aren't the same because your data has changed. 

A TSQL step in the agent job does not support cmd mode type options (like switching servers). You can run the script (when saved to an external file) by using an operating system step, directly calling sqlcmd.exe The best option for periodic data movement/consolidation cross SQL instances is to use a linked server or an SSIS package. If you need near-realtime, then using transactional replication would be next. 

Unfortunately, the SQLAgentOperatorRole msdb role is the most privileged role for "managing" only jobs on a SQL server, and it only gives the users the ability to disable/enable jobs/schedules not owned by the user. The only supported way for giving full access to all jobs AND manage them using SQL Server Management Studio is to add the users to the sysadmin server role. The reason for this is the way the sp_update_job* procedures are written, they are actually checking role membership for access. Excerpt from sp_update_job is below. Note the comments and the explicit check for @x_owner_sid <> SUSER_SID() along with NOT being in sysadmin.