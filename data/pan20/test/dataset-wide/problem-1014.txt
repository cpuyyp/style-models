We'll take page 296 for this example. It's a data page (Pagetype=1). Let's look at which 2 records are on this page. (Each record is 4015 Bytes so 2 records per page.) 

So the NETWORK_IO you were seeing in the extended events log, wasn't related to your insert loop. (If you wouldn't turn nocount on, you would have massive async network IO waits, +1 Martin) However I don't know why the NETWORK_IO show up in the extended event trace. Sure the writing out to a async file target of the events accumulates ASYNC_NETWORK_IO, but surely this is all done on a differenent SPID then the one we are filtering on. I might ask this as a new question myself) 

3 important pieces of information are missing to pinpoint exactly what went wrong in your particular scenario: 

There are other causes for page splits but these are the biggest ones. All of the above will cause fragmentation. Now that's bad, but in a strange way good. Because that is about the only thing that can properly help you decide on the proper fill factor. I'll explain why. What you do is: See if any of the above apply. For the indexes that do, do the following. 

Transactional Replication is based on the transaction log and breaking the LSN Chain breaks replication. If you look at your publication and subscription you'll see that they are both gone. You have to recreate them from scratch. NOTE: Deleting the log file is quite big step. I don't have any tools to only corrupt a single log record in the transaction log file. I'm not sure if the publication and subscription would be dropped too in that scenario. Metadata corruption When you configure replication, there are several system tables created that hold data that is needed for replication to work. So called metadata. There are tables created in master, MSDB, distribution db, publishing db and subcribing db. Personally I don't have experience with this kind of corruption but supposedly you would have to re-setup replication if any of these system tables in your publishing db gets corrupted. I just tested it strangely I was able to corrupt syspublications, sysarticles, sysarticleupdates and replication kept on going. ?? User data corruption Now this is where it gets interesting. Because you could have inconsistencies between publisher and subscriber without you knowing. The reason is, that if you "fix" stuff with DBCC REPAIR_ALLOW_DATA_LOSS it's fixed on a physical level. Transactional replication works with a logreader that reads all transactions in the publishing database log file. If the transaction is marked for replication, it is read and translation in a logically replication statement that is executed on the subcriber. It can deal with Statements like INSERT, DELETE, etc. But it can't deal with repair statements where for example datapages are deallocated. It's not that the repair is not logged. it is! It's just that the log reader doesn't know how to handle them. Here is an example: Let's find out which pages are used for the ReUserTable that we created in the RepSource db with the previous script: 

Your provides 2 sets of the data: - DB Size + Unallocated space - These numbers include BOTH: Data and log file; - Total statistics of RESERVED space for all objects within the database; I bet your 36 GB of free space are in the Log file. For real numbers use following query: 

However, there are plenty of objects in the database, which were created with credentials of that account. I've tried to run one of these procedures and it was executed successfully. When I've tried to recreate that scenario in my test database it returned me an error: 

Get the same error message. In addition to SQL 2014 installed SQL 2016. With SS2016 behavior is little different: It produces an error in ANY case, even if I set SQL port to 1433. I guess because it is "Named" instance. So far see only that extra evidence within the file that SQL Server loads during the request (I assume it is normal): 

So, the question is: How it is possible that procedure created with an option can be successfully executed while SQL Server login does not exist for that user? 

Finally found an answer: You can create database user, which won't have a SQL Login and won't be able to authenticate, but will have permissions and can be specified in clause for stored procedures and functions. The simple way to create such a user is just to use clause during user creation: 

I'm not a replication guru, but record may be marked as deleted only in case when an update is generating Page Split. For the case of replication you can test it by looking at DBCC PAGE and look through transaction log. 

By copying the complete DATA directory, you most likely also copied the system databases, specifically your master database from your old server and overwritten the system databases of your second SQL Server. When copying a USER database in general, the easiest method is to make a backup of that user db on the source server and restore that backup on the destination server. there is no need to ever copy the master, model, msdb or tempdb files from one server to another just to migrate a user database. To fix your server again: 

General setup I'm using the following script to setup a publisher, distributor, subscriber on the same server, create a source and destination database and a table with 10k rows and a publication and subscription for this table. I'll use this setup for the next tests: 

You can create a only on a column (or combination of columns) that contains no duplicate data. The fact that you create the index with the keyword means that you are now constraining users from adding non unique data. (in the key column(s) of the clustered index) A can be created on a column (or combination of columns) that contains duplicate data. So in this case, the clustered index is not constraining you from adding non unique key data. However, since a clustered index key is the row identifier, it needs to be able to uniquely identify a row, even if it's created on a column containing duplicate entries. Because of this, if you create a for every row that is a duplicate, SQL Server adds a 4 byte uniquifier to the row, so that the index in fact becomes unique. This is all done under the hood. 

Try both of them for both of your data sets to see the difference in query cost and amount of I/O using . For instance, when you force for the second data set I/O for table jumps 24 to 215 reads. So, be VERY CAREFUL using any kind of these hints. 

There are 2 options: 1. Create clustered index by account_id. Delete all other indexes. 2. Add new column and create clustered index on that column. Create another non-clustered index on ONLY one column . The second option is more preferable because it is much faster. You join only by 4 bytes. While your current queries join by 40 bytes. That means your current operation is 10 times more expensive. Also, ask yourself couple of questions: - Do you really need have Account_ID as Unicode? - Can you convert Account_ID to INT or BIGINT? Hopefully, you've got my point. ADDITION: 

In your sample case SQL Server made very good choice. Here is what it was actually doing: - In the first set of data it extracts 10 rows from table based on the column. Then it extracts 10 corresponding IDs from table using kind of operation for each ID. - In the second scenario, when there are 100 matching rows in table SQL decided not to do operation, but use instead, which is much cheaper from the I/O perspective. Yes, you can use hints for your queries ($URL$ 

The problem occurs ONLY when I switch default port to another value. When return its value back to 1433 everything goes back to normal. During the execution could capture following waits: 

(Make sure that #Import is actually populated, since depending on when you stopped the previous bulk import loop, it could be empty!) Again, run sp_lock in a second window until you see the ENCRYPTION_SCAN Resource popping up: 

It seems as if the Windows OS Scheduler tries to divide the load across both cores by alternating the task between cores. Set the processor affinity explicitly to use all cores. From a SQL cpu perpective everything is still the same. SQL Server still has goth both vCPUs are both usable. 

You can see that 68 seconds of NETWORK_IO is registered. But since the insert loop is a single threaded action that took 36 seconds, this can't be. (Yes, multiple threads are used, but the operations are serial, never in parallel, so you can't acummulate more wait time than the total duration of the query) If I don't use extended events but just the wait stats DMVs on a quiet instance (with just me running the insert) I get this: 

Ever since Windows 2008 I've noticed the following behavior: I'm using a windows 2008R2 VM with two vCPUs but this also works on a physical server. I'm using SQL server 2008 R2 for this example. Make sure you have processor affinity set to AUTO (the default): 

However, there are tricky things with indexrs combined with where clause that influence locking that could influence results and definately concurrency so it might be good to provide your table definitions including indexes. So people could assist you. 

as long as you use tiny int (0 to 255) using a char(15) or 15 tinyint is the same (size wise). So then from a performance perspective, go for the 15 tinyints since you save on the extraction and string handling. UPDATE if the marks are double digits, you'll need CHAR(30) and that is twice the size of 15 times a tinyint. 

Can see it only on my SQL2014 on Win8.1 VM. Error persists even when run query locally as local admin 

You did not get the point of SQL programming. You can have 1000 copies on multiple machines of a script to create/alter an object, but until one of those scripts is executed against a server, that object on that server will be unchanged. Yes, you can have multiple tabs/windows with the same procedure, but for SQL Server all of those are different/competing sessions and whoever is last - wins.On another hand: SSMS. You can have multiple copies of a script of the same object in different tabs, which is very handy - you can try different scenarios just by switching tabs. It is not job of SSMS to track your changes and keep records which script is a "Master Copy". That is YOUR responsibility. However, if you are in a project development, you and your boss can't completely rely only on your discipline. Project manager have to choose which way to do a source control. Here is a not full list of things, which can be done: - Implement Change Data Capture on the server; - Restrict permissions on Dev/Test and allow "playing" only on Dev or personal machine; - Use change tracking/deployment/source control tools, which will keep records of any changes. And do not be surprised with SQL. All developers are experiencing that issue with source control. See it here: $URL$ 

Yes you can. Use SQL $URL$ By doing that you'll have a full control behind the curtains, but please fully document it and be consistent. It is a nightmare to troubleshoot databases with synonyms. As an alternative you can use dynamic SQL, it might be more transparent. 

only applies to non leave level pages. By default those are filled full. So specifying together with is only useful when you specify a fillfactor smaller then 100%. In your case, specifying is not necessary, since by default your leave pages are how you want them. Full. 

You could initial load it in simple model. (as long as the load by itself isn't a multi step process where you want to be able to do a point in time restore). Now after the load, it could be that your logfile has grown quite a bit if you had a long running transaction regardless of being in simple model. You can then shrink the log file. Then you need to size the log file. A good starting point would be to check your largest table. If your in full recovery model, you'll need at least as much transaction log file space as your largest table. Since when you do regular index mantainance and you do a rebuild, the transaction log generated by the rebuild is about as much as your table. So sizing for your bigest table is a good start. However, you'll see over time if your transaction record rate compared to your log backup frequency is making hte log file grow or not. You can either ajust the size of your log file or the frequency of your log backups. Sizing up front is important. Having the file auto grow is a huge performance penalty, since the log file needs to be zero initialised. Remember, it always autogrows at the moment you are doing transactions, the transaction log writing is synchronous, so waiting for the autogrowth to finish is a direct wait on all pending transactions. Then the auto grow option is also bad for the internal layout of your log file. see: $URL$ and: $URL$ so properly sizing it up front will be best.