The catalogue table sys.columns shows the collation for each character-based column. When the column is created the collation is copied from the database default, unless explicitly stated. It is not inherited in a run-time sense. So, if the DB collation changes, every columns' collation remains as is until explicitly altered. 

Reading this table will be efficient with an index on business_name and one on group_identifier. Trying to maintain a minimal set of groupings may be tricky if the bussiness often change groupings. 

To stress a system you can either increase load or decrease resources. There are many commercial load-creating packages out there, and probably some freeware ones, too. It sounds like you've isolated it to just one SP, though, so a custom script to fire off 500 examples of it in parallel should do the trick. Running the script on the DB server will induce even more stress on SQL Server. I have some Powershell scripts that can max out a four-core server for minutes at a time! Multiplying the data with some (with appropriate primary key adjustments) will make the SPs work harder, too. On the resource side, reducing max server memory will cause more bufferpool paging and, maybe, more SP recompiles. Setting the instance processor affinint so it has fewer cores available will cause each to work harder. Of course these may cause subtle, secondary changes which affect what you're trying to measure. New, different execution plans for example. On the monitoring side I've found these DMVs helpful to understand locking. HTH 

The returned and can be used to determine which values to choose from the normalised tables. These can be joined to each other in the normal fashion. Since there are four layers in the hierarchy there can be only four possible queries (service, service/ city, service/ city/ state, service/ city/ state/ country). It would be simple to call the appropriate one like this: 

I would keep them separate. Although the details differ, your circumstance smells a lot like the One True Lookup Table antipattern. By combining you will have problems defining foreign keys (which parent to point at?), amongst other things. Indexing strategy will essentially be about spliting the combined table into its constituent parts. And so on... The justification for combining would be if Foo and Bar are sub-types of some more abstract entity type and all comments could hang off this super-type. 

If you are executing your second query from the database then the value from is being coerced into 's collation, not the other way around as you suppose. 

Add three attributes to the project and asset documents - , and . Populate these in the "real" data. When a record changes copy it but with IsActive flag flipped. Retain the original values for and . The original record is updated with the amending user's id and the time stamp. I do it this way around so any other record holding this record's magic id retains that pointer. When reading only look for records with the right value of IsActive. IsActive can be omitted if the archive records are written to a separate collection / namespace / database / whatever your technology supports. 

An alternative method for serializing access to a section of code is application locks. The downside is it requires marginally more development work. The upside is multiple statements can be inside the applock scope, preventing deadlocks. 

It may or may not be faster, depending on your data distribution and the queries you execute. What it will be is better. Normalisation isn't a goal in itself. It's a series of processes one applies to remove potential errors while changing data. The output is a schema which is resilient to changing data. Without normalisation you leave yourself open to inconsistent data. It really doesn't matter how fast your site runs; if it's wrong it's wrong and it's no less wrong by being wrong quickly. Separated values could potentially be faster if your current queries retrieve too much information because it is all bundled up together in CSV columns. It will definitely be faster if, down the line, you discover you have to join to another table on one value currently embedded in a CSV. At that point your performance will fall off a cliff. 

The CASE statement will work. It returns a value depending on a condition, so you can assign each invoice's value to exactly one age bucket. To be able to roll up the values at whatever granularity required use a sub-query to put values into appropriate buckets, then in the super-query perform the required grouping. 

If your user community sees these as two separate things then model them as such. If a row for one type populates one set of columns but always leaves other columns empty whereas a row for the second type populates the columns the first type left empty, that is an indication that separating them may be beneficial. There are many other things to consider, however, such as use patterns, update frequency, the number of columns involved and the characteristics of your particular DBMS software in each permutation. You may be willing to accept a compromised design if it runs quickly. 

As for the second part -- "Is it possible to enter the records with keys in a different order to have a tree of less height?" With 5 keys and two keys per node we need at least 3 leaf nodes to hold all values and a height of 3 to form the tree. So my arrangement is optimal for the given data, sequence and algorithm. The book uses a very different pointer arrangement to what I use, and a different page split arrangement. This will be significant, leading to part-full pages. That there is a section on page 42 called "Data Loading" that shows how fuller pages can be achieved by loading out of key sequence supports my hunch. However, I hope I've given you sufficient pointers and you'll be able to use the book's pointer structure to work that out for yourself. 

.. with appropriate error handling and return code checking, of course. This can be packaged as a single batch, a job with one step per triggered job or one step per statement. I prefer the last as restart is simpler. 

Startup stored procedures run from master each time the instance starts. Replication with this database as the subscriber. Not code per se, but it will change data. 

What you're looking for is generally known as a "greatest of group" solution. There are several patterns for this. A search of dba.SE or SO will show you some. The posted question and the SQL Fiddle have the table names swapped. This code is from the Fiddle: 

Ask yourself if the thing under consideration continues to have an existence if all the other things in your model go away. If so it's an entity, if not it's an attribute. Does the thing you are considering have a way to distinguish it from all other items in its class i.e. a primary key? Then it's an entity. The answer you come to may depend on the context in which you are working. For one computer application a particular value may be treated as an attribute. In another it may be an entity. This is not a contradiction; it is a natural outcome of simplifying the real world into a model and in doing so selecting the important parts and excluding the others. If you intend to implement your model you should recall that first normal form requires that each attribute hold only one value.