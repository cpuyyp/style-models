Run the following query in the context of the database in question in order to get the Change Tracking table names used on the database. (Query via Kendra Little on BrentOzar.com) 

the job executes successfully (or at least gets far enough to only encounter logic or other syntax issues). This approach won't work for a final solution, because there is a requirement to keep the PowerShell script internal to SQL. So I have a different CmdExec step that embeds the PowerShell script like so: 

Create the Foreach Container Here, you will create a Foreach based on the Catalogs schema rowset. This will get us the for each database in the instance and the will be put into its corresponding variable. 

I want to dynamically back up all the databases on a given SSAS instance using a SQL Agent job (which would most likely involve executing an SSIS package). It is imperative that this is a dynamic process - if users add databases or cubes, I want to set up a job one time that can automatically detect all existing SSAS metadata. Unfortunately, I don't see anything out there that tells me how I can automatically and dynamically back up all of the databases on an SSAS instance in a clean way. By "clean", I mean: 

On a schedule, checking all the definitions on the database server with dimensions and updating any changes. And assigning transactions to “Unknown” is the dimension hasn’t arrived yet. Putting triggers on all the definition table and updating the data warehouse as the changes happen Updating the stored procedures to update the data warehouse every time a change is made Using something like Microsoft Sync Framework, and incorporating the transform in there. 

On our SQL Server, we have a database for each of our web apps. For reports, we use Reporting Services and all report data (including report parameters) come from stored procedures. The stored procedures are in the same database as the data in the report. So, for example, the procs that serve the Stock reports are in the Stock database. Some reports show information from more than one database and then the proc will be in one of those source databases. The report parameters get their data from procs in an Enterprise database that has data like stores, employees etc. This means that all reports have at least a connection to the Enterprise database and another connection to another database -- and sometimes more than that. My question is: is there a benefit of moving the reporting procs into a separate "Reports" database. I know the benefits of moving reports onto another server and I'm not talking about that -- this would be on the same server. Things that might affect this are: 

Create a new OLEDB connection manager that uses the Microsoft OLE DB Provider for Analysis Service Using the Property Expressions Editor, set the ConnectionString property as follows: 

Sometimes certain SQL Server maintenance plan tasks can be problematic, like Shrink Database, Rebuild Index, Update Statistics, etc. I'd like to search for the use of one or more of these tasks on an instance of SQL server, for further review of whether they are necessary. Is there a way to query for maintenance plans that perform one or more specific tasks? (Not limited to the tasks that I mentioned.) 

Connection Managers Use a real connection in design time so that the metadata plays nice. Creating the connection managers now isn't required, but it makes it easier for later. For each task in the process, you will have the appropriate connection manager available in the drop-down without the need to create any on-the-fly. ADO.NET 

Create a new ADO.NET connection manager that uses the Microsoft OLE DB Provider for Analysis Service Using the Property Expressions Editor, set the ConnectionString property as follows: 

We have a suite of restaurant web applications that stores data in SQL Server 2012 on database server. We have now started to move data for reporting onto a separate data warehouse server. The data needs to get into the data warehouse quite soon after being updated/created. Knowing when to move transactions to the data warehouse (and into fact tables) is easy -- we do that periodically or when certain events occur (like the manager finalising something). But how do I know when to move the definitions to the data warehouse (and into dimension tables)? We are storing definitions as SCD Type 1. We’re considering... 

We plan to backup our SQL Server 2012 databases every night, FTP the backup files to our test servers and restore them. Then deploy the latest schemas to those databases. The reason for this is so we have the latest data with the latest schemas to test on. I'm using Ola Hallengren's SQL Server Maintenance Solution to back up the databases and Core FTP to FTP the backup files. The problem is that Ola Hallengren's SQL Server Maintenance Solution puts each backup file in a different folder and the Core FTP has been setup to FTP all the backup files from a single folder. As I see it, I can: 

Output is from a 11.2.0.4.6 Enterprise Edition database on Oracle Linux 7.1 x86-64 platform. Lets start with question 2 and an easy example. DISTINCT and GROUP BY are handled differently: the optimizer is able to completely eliminate a DISTINCT under certain circumstances, but it can not do the same with GROUP BY. Here is an example: 

A NOMOUNT instance stays in BLOCKED status with dynamic registration. You need to use static registration to connect a NOMOUNT instance remotely. Add the below to and restart the listener: 

The above creates a file containing the and statements for your tablespaces, which you can edit or use for creating the tablespaces. 

But what you tried works only one way, you can not parse text like that (with suffixes) into a date type. Datetime Format Element Suffixes 

If you really want to have that specific space and line break, you should not rely on enviroment settings, but explicitly specify it: 

This is not guaranteed to work, the database may remain open or crash immediately/randomly depending on the scale of inconsistency of the datafiles. 

change the location the databases are backed up to change the way Core FTP is setup, or add some SQL script to the main store procedure to copy the backup files to a single folder find new FTP software that can FTP files from many subfolders with a single folder (any suggestions?) 

The database and data warehouse servers are on separate servers – which we are planning to move to SQL Azure. So, how do I know when to run the ETL for dimensions? And, any suggestions on technology would be helpful (we are currently using SQL Server linked servers). 

Hopefully, there will be a reliable way to copy all the backup files to a single folder -- even when we add a new database. EDIT: changed title of the question - used to be "Move SQL Server backup files to a single folder" EDIT: added option #4 ("find new FTP software...") 

For SQL Data Collector: Do you need SSIS (SQL Server Integration Services) installed on the server that you are collecting data from. Or can the packages be run from another server -- like the one that the MDW database is on.