When using JDBC I like the new to establish the connection. Further I like the isolated set-Methods for the properties like , , , , , , and so on. The question is (esp. for the use with Oracle), if there is a way to set up LoadBalancing with two or more servers without specifying the URL directly with 

There are 20 or more of those points. As dealing with time series, the first idea might be the usage of a time series db. On the other hand, for me it seems as if the main purpose of time series db is storage of scalar values. Of course, my measurements are scalar values. But I'm not sure if it would be a good idea to store every scalar value as a (time/value/measpos_id)-triple - leading to an enormous number of entries. I think single of those entries would never be evaluated. Another idea could be the storage of the measurement vector (all values from that second) together with the starting time and the measpos_id. But howto do that? Taking all values as a blob? Not every system is capable of dealing with vectors - and maybe they differ in length. Are there concepts in timeseries-db for such problems, which I'm not aware of? Further for evaluation (extraction) I think maybe the exctraction of the complete vector would be the most used case. Please feel free to ask, if my description is incomplete or some more details could help in finding a good solution. What are your recommendations? NoSQL or relational SQL? Further ideas? Every hint is welcome. Thanks in advance. additions: 

Execute a statement or a command. Use to remove all use of indexes for the table. Insert data into the table with . This does not update any indexes and therefore is very fast. Re-create the indexes with . This creates the index tree in memory before writing it to disk, which is much faster that updating the index during LOAD DATA INFILE because it avoids lots of disk seeks. The resulting index tree is also perfectly balanced. Execute a statement or a command. 

In this configuration you have all the advantages in case of a master crash(promoting the standard replication) and all the advantages for checking your "past" :) and/or a point in time recovery 

You can take a look into MySQL Fabric (Official Doc) but it requires more db server I have tried this tool only in R&D env for testing a basic HA It supports some sharding scenarios Here some high level pros and cons Pros: 

I can understand each scalar value of acceleration as a single value with a given timestamp (could be calculated from start of measurement, index of value and sampling frequency). I think that could easily be done with a TS-DB. The timestamps would have two different sampling properties - one is f_s and one derived from P. I'm not sure, if it would be efficient extracting the measurement arrays/vectors, as always it would neccesary to find the beginning of those arrays/vectors. Furter I'm not sure, how to store the Fourier spectral information, as they belong not to a single measurement or timestamp and it is a complete array/vector and not only a scalar value. Storing the whole acceleration measurement vector with the timestamp together with the Fourier vector (understanding each array/vector as a related to the timestamp) might be a clearer view. But how to store best? 

You might be able to get a performance improvement by using OPENQUERY from the SQL Server side to the Oracle database set up as a linked server. Then you could use the SQL Server MERGE function to merge in any new rows from Oracle to SQL Server, that is, any rows that do not match, column for column, with what you already have in SQL Server. Something like this: 

I understand why of course, and I never do, mainly because I never need to. Now though, I have a database with an .mdf file that is 800GB. It was a system designed to collect data over a certain amount of time, with no retention period. A retention period of 90 days was recently placed on this data, and as such the developer has cleared about 3 years worth of data from the tables. Data and Indexes now total roughly 70GB, so now I have an .mdf file which is grossly over-sized. I want to shrink it to reclaim some of that valuable disk space. I'm planning to perform a shrink prior to rebuilding indexes and updating statistics (i.e the weekly maint. plan) I'm not breaking any DBA laws here am I? I assume this is an acceptable scenario in which to perform a SHRINK as it is a true one off? Thanks 

In short, no, your CALL is not replicated You can take a look into the FAQ starting from B.4.22 and the Binary Logging of Stored Programs B.4.23: Are stored procedures and functions created on a master server replicated to a slave? 

Antivirus or "Non-Conventional" Backup software has also often caused corruption. Your first step is to try to find out what is changing the MySQL files. Countinuous table repairing can only be a temporary workaround. 

For MySQL and PostgreSQL you can use the Snapshot Backups in order to obtain full or incremental backups: Some file system implementations enable “snapshots” to be taken. These provide logical copies of the file system at a given point in time, without requiring a physical copy of the entire file system. (For example, the implementation may use copy-on-write techniques so that only parts of the file system modified after the snapshot time need be copied.) It is available through third-party solutions such as Veritas, LVM, or ZFS. 

Hoping to address my question within the right Forum. If not, please assist me, where to ask. Having an use case in which I get one-dimensional acceleration measurents delivered with a given sampling frequency (for example f_s=10 kHz) for a given range (for example T=1 s). That measurement would be repeated periodic (say for example every P=60 s). In that example, it will give me a vector of acceleration measurements of length 10'000 float values each minute. For evaluation purposes I would like to store the attentand Fourier Transformation of that vector also. Now my question: In which database (timeseries-db or simple relational) and how to store that data? In my understanding TS-DB are highly optimized for storing timestamped scalar values. But what, if I have (not so short/small) timestamped vectors? I'll present my thaughts on that question for deeper insight: 

Each array/vector (time domain data and spectral data) as a blob? Compresses? Each array/vector as a linked CSV-file? Compressed? other ideas 

However before proceed check the documentation of your MySQL 5.1 version and make a backup copy of your ,, and file in order to avoid loss of data 

Another test you can try... "probably" in your table the cardinality of 'domain_id_resource_type' index is lower than 'domain_id', you can try to skip the optimizer choice and declare the usage of 'domain_id_resource_type' instead of 'domain_id' 

In my previous response I have not taken into account behavior, so I have tried your test, first using the mysql native client and all works fine, column are automatically converted, and then using a jdbc connector and actually I got the same error... but after some test and some research I have found the solution. You have to add in your jdbc connection url the parameter and set it to 

Putting your query into the function, cleaning your general query log for this kind of process require a bit of work Here a simple output of a "personal" file, like the above, with 9 query, 7 insert and 2 select, +2 BEGIN/COMMIT: 

I recently inherited a SQL Cluster (2008R2) which for the most part behaves itself impeccably. The windows cluster is made up of two nodes running Active/Passive, Node1 and Node2 are dedicated blades in two different data centers. There are 3 SQL instances all running on Node1. Quorum is established by a File Share Witness and we have a heartbeat between the two nodes. The other day someone switched off the file share witness by mistake, and the windows server failed over from Node1 to Node2. Or should I say, in Failover Cluster Manager, Node2 was now specified as the active node by Windows. However, the SQL Cluster didn't do anything. All the instances stayed up and hosted on Node1. I would have expected them to move Nodes, but no. There was no adverse affect on the databases at all. Once power was resumed to the File Share Witness I brought it online again and the Windows Cluster failed back to Node1. Our Windows Technicians are looking into why the cluster failed over, and I'm left scratching my head with the SQL bit. All I can think of is that the heartbeat kept the SQL instances on Node1 and losing the witness wasn't important. I'm still learning the small details of Windows Clustering, being much more used to Log Shippping and Mirroring when it comes to HA solutions, so any insight into why the SQL Instances didn't failover would be appreciated. 

performs the preceding optimization automatically if the MyISAM table into which you insert data is empty. The main difference between automatic optimization and using the procedure explicitly is that you can let myisamchk allocate much more temporary memory for the index creation than you might want the server to allocate for index re-creation when it executes the LOAD DATA INFILE statement. In order to obtain better performance from the myisamchk you have to tune some params like : 

Try increasing your (probably a default value of 60secs is too small in your scenario of blob and binary values) Ref: net_write_timeout and in general: net_read_timeout 

UPDATE FOR FREE BLOCK EXPLAIN When you delete a row a free block can occur, if you have free blocks in the "middle" of your insert, for example if you replace deleted rows(index and/or primary key) with your load data, concurrent insert do not work, when free blocks are filled in future insert become concurrent again. 

This article gives all the filter arguments and available columns including the code. I use it often as a reference to create server-side traces. 

As Remus Rusanu says you do not need rights to run a trace, you need permissions. I don't know anything about your company, but as a DBA in a large public organisation I have much experience of users asking for rights because they want to run a trace to 'figure out what a query is doing...' When asked that question, I don't flatly say no, I explain why it isn't a good idea to run client side traces and to put SQL Profiler in the hands of the users. Sure one of the reasons is long traces can have a performance impact on production systems, which is of course a worry, but there's also the setting up of the profile and the interpreting of the output - you don't want any help with that? The fact that you may have never used Profiler before or understand its complexities and consequences would fill me with worry. I always engage in a bit of dialogue about why server side traces or extended events is potentially better. I ask why they're investigating what the query is doing - maybe I, or one of my team, can help without a trace. It works both ways though, I am wondering if you have fully explained what you want to do to your DBA's or IT Management Team. I think sometimes when people go guns-blazing asking for SysAdmin rights without effectively engaging in a bit of dialogue you end up with closed doors and brick walls (bureaucracy as you call it) rather than collaboration, co-operation and learning experiences. You may have done this of course, and your IT team may just be stubborn - but this is just my two cents. Plus, if I found out that third-party tools were being used in isolation without the authority to do so I would put that user in breach of our acceptable use policy and report it - so please be careful if you're going down that road. Doesn't matter what company you work for - one team, remember? 

Yes, using the MySQL User-Defined Function (see MySQL 5.5 FAQ: Triggers) and installing the lib_mysqludf_sys Then, for example, you can write your own trigger calling the sys_exec like this: 

Alternatively you can manage the delay of a replica using the MySQL Delayed Replication, note that this feature is available only on 5.6.x + MySQL version. I think that a good idea where to start (but there are many configuration for your scenario) is to have 2 node attached in replica, one standard replica and one delayed replica: 

Source: 5.1 Driver/Datasource Class Names, URL Syntax and Configuration Properties for Connector/J Running my java code using the above connection url the table are correctly converted (I have tried only with an empty table): 

The contains the exit code of the external program There are other useful functions in this library: 

This is only a trace not a complete solution, "your" solution is based on your fantasy (and some work) ;) I have done this kind of test capturing and cleaning query generated by the general query log and using sysbench 0.5 after writing my own file, you can find some samples in the official source code, on a mirror of my database(hw and schema/data -a snapshot so I can restore the original status of data immediately- ). Then you can run a command like this: