No, it is not. It is not guaranteed (well, it is, with , but that is in the subquery, and that is not good enough for the optimizer) that your query returns at most 1 row for each key value from the base table. You need a unique constraint on to make this work: 

No, there is not. You have already defined manually the filters for each table, why not use that? Just dynamically generate the parameterfile before running : 

You should not edit the spfile directly as it contains binary data, but you can create a pure, text-based parameterfile from it: 

This will make all the ORA-01775 errors logged in the alert log plus a trace file, and that trace fail should contain the statement, that caused the error. 

Notice how it still failed, because I did not define the function, I did not tell the database how it can compare two user-defined objects. Now try again, this time with defining (and here I assume objects have unique identifiers and two objects equal if their identifiers equal): 

A database can exist without an instance, but to use that database, you need to start an instance, then mount and open the database in it. You can "create" a database by simply copying the files of an existing database. You do not need an instance for that. But to use that database, you need an instance. If you want to create a database from scratch, you need a running instance first. 

One possible reason: Bug 18162779 : ORA-01428: ARGUMENT '000000000000000000' IS OUT OF RANGE Try running the query after setting the below: 

The above documentations refers to OUI (Oracle Universal Installer), not DBCA. Saving response files in DBCA is available starting with version 12.2. See the below example (Save Response File in bottom right corner): 

When it comes to Windows, something is always different. works on Linux/UNIX, but not on Windows. The Windows version of is . SQL*Plus Environment Variables 

Another method to confirm that your session is would be querying ASH. V$ACTIVE_SESSION_HISTORY In ASH, the column has 2 different possible values: 

And that is why we use role-based services instead of the default service derived from and . If your primary database is called (), there will be service with that name at the primary site, and that service will be available even after a failover or switchover. The listener receives your request, sees that you are requesting to connect using the service , and forwards your request to the instance(s) that provide that service. The instance is in stage, and you receive . Create the role based service at both sites (replace -db and -service values with your current ones): 

In order to perform open resetlogs, and incomplete media recovery should be performed (even if it can not do anyhting): 

Note that this will not force parallel execution even if the syntax suggests that. It's like adding parallel 8 hint to your statements in your session. 4) If column is indexed, than the statement need to maintain the index(es) as well. Sometimes it is better to make the index(es) unusable, run the DML, then rebuild the index(es). 5) If the content of column does not fit its original place ( becomes ), and your table blocks are full, you may experience row migration, that makes performance worse not only during the , but for later queries as well. You would be surprised how much better CTAS can be. 6) If column is part of an enabled foreign key constraint, you can not simply update it to any value. The database needs to check the parent values to make sure you are updating to a valid value. Again, disable, run the DML, enable validate, may improve performance. But this is just guesswork without the facts. An SQL Monitoring output, or execution plan with runtime statistics could help. If that is not enough, monitoring the wait events + parameters in v$session, or a raw SQL trace could help even more. 

There you should see the original 184320 , and 92160 for the disks you have just shrinked. Next use lvreduce to shrink the LV: 

I would not touch that directory, I have not found any references of other users using the SQL Developer software that comes with database binaries. Additionally, SQL Developer shipped with the database binaries is kind of outdated. Even 12.1.0.2 comes with SQL Developer 3.2 I would just perform a seperate install. Also, I find it unusual that someone, who is not the DBA, wants to run SQL Developer on the database server itself. Why not just install it on the client side? (To be honest, I would not even include SQL Developer in the database binaries package and install process, but it can not be customized.) 

Starting with 12c, we have DBMS_UTILITY.EXPAND_SQL_TEXT. You basically pass the query text through the first, input parameter, and receive the rewritten query back through the second, output parameter. But since you are on 11.2, you can not use this yet. You can however collect the optimizer trace. If you already ran your query, and have its sql_id: 

If you omit the responsefile, the installer will try to start the interactive graphical installer, as there is no terminal based interactive installer (I wish there was...). 

If you want to keep the PDB in the source CDB, you do not need to unplug the PDB, you can just simply clone the PDB to another CDB: 

Once you have enabled monitoring for an index, later you can check if the index was used in the view (when logged in as the owner of the index). Starting with 12c, you need not to log in as the owner to query this, the new view can be used for this. When you do not want the index to be monitored anymore, disable monitoring by: 

This generates as many rows as the largest range requires, with data , starting with , so no correction needed, when we add these values to . Now simply join this to your original table and limit the number of rows by , finally add the row number to :