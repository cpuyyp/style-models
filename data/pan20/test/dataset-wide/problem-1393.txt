Use spaces instead of tabs. Avoid to name variables as built-in functions/objects. I mean . If you need this function later, it will take time to figure out why it doesn't work. keyword closes file automatically if exception happens. it's preferable for opening files. 

If you need to import your module or part of it, your code will execute each time. To prevent this, use condition 

You also may use csv module to iterate over your file. Your code would be faster because of C implementation and cleaner without 

so we expect the total number of nodes to be at that point, right? Nope, you immediately change your mind and write 

This swaps an \$\mathcal{O}(n^2)\$ operation for an \$\mathcal{O}(n)\$ one. You only check for is-empty, so this can be swapped with: 

This does each direction separately, adding them into an integer board. Alternatively, one could define the board in Numpy and use a convolution, which would be much faster but less "pure". At this point, it takes about 8.5 seconds for CPython 3 to run 1000 iterations on my computer. PyPy takes just 1 second, or less. It's still possible to go faster, though! A lot of time is spent rerunning when only small updates have been made. Perhaps you can use that to optimize out calls to ? There are even faster strategies, such as Hashlife. These, though, are outside of the scope of this review. Here's the updated code: 

Now the instances just builds from an empty function with a prototype fallback (always the same function) so we're not redefining a function for every instance (note: in non-JIT interpreters we might actually redefine the function for every firing of the factory, but I would expect JITs, which I'm certain Node uses, to cache everything defined locally in this case) One of the things I personally dislike about CoffeeScript is reduced flexibility in the manner in which I go about building my objects and defining my functions. 

Your code is messy and difficult to understand. I believe that in a few month you will waste time to understand what you have written earlier. So it is can be ok if you are not going to support/extend/re-use your project. Scrapy offer quite good architecture to organize your code properly: Item Loaders and Input and Output processors, Item Pipeline. It will move processing logic and make your main parse method cleaner. 

You should almost never ; instead raise appropriate specific exceptions. In this case I would use . Don't line break onto the indentation; add a hanging indent. This code is overcomplicated: 

Quick manual optimization can be made by avoiding large intermediates. This is possible by generating sequential cubes with two consecutive additions: 

You should never do on strings in a loop unless you know enough to know it doesn't apply. In this case it definitely doesn't. Here's an alternative: 

I think I may have been wrong about that. Objects are passed via reference, so they shouldn't be taking up more memory than the bytes that go into linking to another namespace/scope. By passing in as local parameters, you've probably eliminated overhead for the call object compared to if they were referenced from outside of your function constructor but you could probably build dozens of similar objects without seeing a major impact on memory. (although I don't know what the last three lines do - they might create new object instances with properties that are passed by value which would hit memory harder). The advantage of function constructors is that you can build factories where vars that would be passed as values are instead accessed via closure. In JIT compilers at least, I wouldn't expect those to take up more space in memory either. Example: 

CPython improves a lot with this, although PyPy doesn't really improve. This makes sense: you do a lot fewer operations but each operation is more complicated. Since CPython has a really fast implementation under the hood but a really slow interpreter, fewer operations is important. PyPy, however, is bound by the size of the operations themselves. Of course, for PyPy you'd want to modify the "original" code: 

Note that this allows input like or which parse as and respectively; a full check is more complicated. You should probably separate the prime-counting code from the input-output code, although this requires a bit of hassle to return multiple values: 

4. You don't check exceptions that are very likely when you send HTTP requests. In this case your script stop and you have to attack server from the beginning. I also advise you to close connections. With-statement does it automatically. 

You have 2 models (Ringtone extends Contenido). As I understand you store same , , in both models and every update/delete/insert operation on the first model must be synchronized with another one. You can avoid this, make foreign key to base model: 

Right off the bat, I see you're using arbitrary-width types to represent fixed-width values; though this isn't wrong per-se, it is certainly needlessly confusing. If you use something like , comment it with why you did so. As far as I can tell from looking at the assembly, this never produces different output than a straightforward, standard , so I'm strongly tempted to use the later. Being performance conscious doesn't even mean you get to drop operator spacing, so do more of that! We see that your is late; doing an early test as 

Well I can only guess without seeing the code where the DOM is actually accessed but one thing that looks big to me is that you appear to be manipulating each individual element in seperate frames. If you did the calcs in one function for all elements that would be (I believe) one set of reflow calcs set off in the browser rather than per elements-moved. Basically any time there is a change to the DOM a series of reflow calculations is set off in the browser. I think when say three DOM changes are made in one function call, the browser is aware of all three before it starts doing anything. Whereas with seperate funcs it does that same work multiple times. How much reflow calculation gets set off for an action on the DOM in any given browser isn't something I'm a real wizard on but IIRC tends to be a fairly crude operation that doesn't necessarily take "out of flow" elements like absolutely positioned containers into consideration. That said, I still tend to find the DOM seems to have an easier time of it when you try to optimize by taking movable elements out of flow first. If you don't need to support older browsers, look into CSS3 animation approaches. These can drastically improve animation performance since browsers can optimize for actions before they happen. Barring that, avoid touching the DOM as much as possible in loops. If you must access properties on a DOM element multiple times, don't keep reacquiring it. Assign it to a var, effectively caching that operation (note: might not matter in newer browsers). But actually making changes to the DOM is where you need to optimize the most. Sidestep with pre-set CSS3 animations or follow some of the older-school stuff I've outlined here and we can examine in more depth if needed. 

Since you seem to by using this for caseless comparisons, note that might be more appropriate - it depends on whether you're using Python 3 (or Python 2 with ) and what alphabets you expect. 

The file should be closed as soon as you are finished with it, and the comment should be reformatted. 

This is far simpler that it originally was. I'd be tempted to remove and split into two parallel lists and flip the order they are searched, allowing several more simplifications: