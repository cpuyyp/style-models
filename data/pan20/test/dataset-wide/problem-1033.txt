The most promising exploration comes from removing the trash variable and using a no-results query. Initially this showed NOLOCK as slightly faster, but when I showed the demo to my boss, NOLOCK was back to being slower. What is it about NOLOCK that slows down a scan with variable assignment? 

I'm fighting against NOLOCK in my current environment. One argument I've heard is that the overhead of locking slows down a query. So, I devised a test to see just how much this overhead might be. I discovered that NOLOCK actually slows down my scan. At first I was delighted, but now I 'm just confused. Is my test invalid somehow? Shouldn't NOLOCK actually allow a slightly faster scan? What's happening here? Here's my script: 

(The directives allow the parser to distinguish between the end of a compound-statement trigger or procedure body and the ends of the individual simple statements it contains, and are only required by some MySQL clients.) 

I believe MySQL requires the statement to restate the column definition (datatype, default value, column constraints &c.) even if it's only the column name being changed. 

To distill, not changing the value of will almost certainly eventually cause you problems if you use Unicode characters outside the base character set. 

With your in place, you're attempting to declare variables after you've begun "doing stuff", which isn't allowed. Move the after the last of your s, and things should work much better. 

According to this link, turning the Parameter Sniffing option off in 2016 is equivalent to setting trace flag 4136. You should read up on that trace flag. Especially important information from that trace flag link: 

Here are my own results: average time in ms for DROP/CREATE: 2547, for REBULD: 1314 It looks like in my contrived example, the winner is REBUILD. Constraints What if the index was created to support a constraint? In this case, a simple DROP and CREATE will fail because you have to drop the constraint first. Let's look at the clustered index from the previous example. 

1) Log flushing: the SIMPLE recovery model does not clear the log after every transaction, but at checkpoints. (link for more info) 2a) REBUILD ALL: yes, REBUILD ALL works as a single transaction. The index rebuilds within have their own transactions, but the overall operation isn't fully committed until the end. So yes, you might limit log file growth by rebuilding individual indexes (and possibly issuing CHECKPOINT commands). 2b) Proof! Here, have a demo script. (Built in 2016 dev) First, set up a test db, with table and indexes: 

Jack up the security on the table so normal users can't directly access it at all, and remove the protecting trigger. Create an updatable view on the table which is accessible by normal users, and add an trigger to protect the column from updates (assuming you want it still visible; if not, simply omit it completely from the view definition.) Create a function with definer's rights which bypasses the view and directly updates the column in the base table. 

Note that I haven't tried to compile this, so I make no promises regarding syntax errors, but the semantics are sound. Also, I've used a -clause join instead of a -clause join, as I'm not sure you can use the latter to join on . 

Of course, like Erik alluded to, your actual problems probably have nothing to do with fragmentation, but hey, this was fun. 

This demo shows that a delete can produce a very unbalanced b-tree, with practically all data on one side. 

This is an application issue more than a database issue. Navision treats datetime entries as UTC, and is adding the hour in its display (assuming your local timezone is UTC+1). I have run into this myself since I work with Navision. If you have a client application, you can look at displayed datetime for a record and compare it to what you see by querying the table. 

Now let's build some procs that will DROP/CREATE or REBUILD, plus log (approximately) the time taken: 

You use in compound-statement syntax to declare a local variable, cursor, condition or handler. However, your variable is a session global variable due to the presence of the prefix, and it is an error to attempt to redeclare a global variable. Change the name of your variable from to , and then read up on MySQL variables. 

Triggers are procedural, so my inclination would be to perform the join and update as separate steps; this IMHO would make for simpler and more readable DML.This code should do what you want: 

However, in this case your entire trigger body is a single statement, so you can omit both the statements and the directives: 

According to Paul Randal in his Pluralsight course "SQL Server: Logging, Recovery, and the Transaction Log" the LSN is composed of three parts: 

To answer the question in the title, whether the B-tree rebalanced during a delete, the answer appears to be no, at least in the following minimal test case. The following demo runs commands that are best left for a test environment. 

Short answer: start SQL Server with the -m flag from a local admin account. Step-by-step instructions here: 

Understanding 1 is correct, and spaghettidba and Joe have good explanations. If you're interested in testing for yourself (on a test instance please), you can use the below script: 

is not part of the which precedes it, so the identifier does not refer to the column by that name but to a separate (undeclared) variable. The way to test whether the did anything or not is to test the implicit cursor attribute , as in 

MySQL is particular about the order in which you do things in a statement. I'll be burned if I can find it in the docs right now, beyond a cursory mention under Syntax, but the order MUST BE 

You've stumbled onto one of the imponderables of MySQL's default configuration. To quote from the documentation on Character Set Configuration: 

You need to have directives both before and after the statement, you need to end individual simple statements with semicolons, and you need to end the complete with the delimiter specified in the first directive: 

Note how the first open transaction (Transaction ID 0000:000002fa for me) isn't committed until the end of the REBUILD ALL, but for the index-by-index rebuilds, they are successively committed. 

But appeals to authority are boring, so let's test it ourselves! Performance: Set up a table with a non-clustered index, load it with junk, and build a table for logging time. 

Bad news: the plus approach in your script completely flattens out the XML document, and will need to be reworked if you have any multiple-X-per-Y structures in your document. If the dynamic SQL is a requirement, then please provide an example to test that aspect more easily. Another approach might be to build queries you need manually. If your fundamental problem is including parent info, then you might modify my demo SELECT query below. (N.B. Mikael Eriksson's answer has an improved query. Please refer to that.) Key ideas: is the context node, and gives you the parent is a different kind of XML function that belongs in the FROM section of the query and is usually seen with CROSS APPLY. It returns a pointer per match, and is what allows working with multiple rows. Read more here. is one of several methods for letting SQL Server know the value method only has one piece of data to work with (fixing the "requires a singleton" error)