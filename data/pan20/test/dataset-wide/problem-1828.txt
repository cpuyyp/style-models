But I do not want to spam the app people if network is down or the server people if the application is down. So I need a way to express dependency on alarms: ping < login < app How do I express these dependencies? 

It should preferably survive if any of the machines change its internal IP address (e.g. as part of a DHCP change). TeamViewer delivers 1, 2, and 3, but fails 4 because of its license. Google Chrome Remote Desktop delivers 1, 2, and 3, but fails 4 because of its license. The different VNC tools deliver 3 and 4, but fails for either 1 or 2. 

I have stop-started the on server l, and mounting l:/disk/l on another dir still hangs on server e. After booting server l I can mount on server e and everything is fine. But I would like to avoid the reboot as that disturbs users. How can I kick the nfs-kernel-server so hard that it actually restarts completely and not just half-way restarts as seen above? Edit: I have also restarted portmap. When I restart nfs-kernel-server it spends a lot of time after printing 'nfsd'. It seems it is hanging at: /usr/bin/rpcinfo -u localhost nfs 3 

I would like to monitor a service that depends on the server and a login service being up. So I would like an alarm to go off: 

I'm working with postfix on fedora 9 and I'm attempting to make some changes to a system setup by my predecessor. Currently the postfix server on [mail.ourdomain.com] is setup to forward mail sent to two addresses to another server for processing. The other server [www01.ourdomain.com] receives the email and sends it to a PHP script to be processed. Then that PHP script generates and sends a response to the user who sent the original email. We're adding more web servers to the system and as a result we've decided to move these processing scripts to our admin [admin.ourdomain.com] server to make them easier to keep track of. I've already setup and tested the processing scripts on [admin.ourdomain.com], and on the mail server doing the forwarding [mail.ourdomain.com] I added [admin.ourdomain.com] to /etc/hosts and also added another, aside from the one for [www01.ourdomain.com], entry to /etc/postfix/transport for [admin.ourdomain.com]. I also restarted postfix as well. I've tested the communication from [mail.ourdomain.com] to [admin.ourdomain.com] using telnet and the [admin.ourdomain.com] domain and everything runs correctly. But as soon as I change the forward address and attempt to send an email to the mail server I get a bounce message stating "Host or domain name not found. Name service error for name=admin.ourdomain.com type=A: Host not found". If I change the forward settings back to [www01.ourdomain.com] then everything works fine. Is there some setting I'm missing in Postfix? The server itself and telnet work fine it just seems to be postfix that's not able to discover the location of [admin.ourdomain.com]. 

Grrr.... is not needed. is needed. A restart of is only needed if was changed from no to yes. Disabling my own was needed very much! I had * statements in my which re-used the ssh channel and thus I would not discover the change. Thanks to Samed Beyribey and quanta, whose help gave me the idea to run which gives very different output when you have * statements. 

At this point do not use the directory: In the test scenario I have at least once had xfs complain and crash. So instead: 

The first 3 are the 2 physical disks and the hardware RAID on top of those. The 4th is the external diskbox connected via SAS. As you can see there is no corresponding device in /dev for the external VessRAID. The VessRAID is an external SAS-RAID box that presents itself as a SAS device. Linux clearly sees the device, but somehow it does not make it into a device in /dev. There are two logical drives on the VessRAID. I have the feeling I need to tell Linux that this is a SAS disk device that Linux should use and scan for logical drives and make them show up in /dev. But I have been unable to find the the magical command that does this. Digging around gave this: 

Ian Howson gives a good answer on why it is slow. If you delete files in parallel you may see an increase in speed due to the deletion may use the same blocks and thus can save rewriting the same block many times. So try: 

I want to migrate from my current mail server (old_server) for my domain mydomain.com. old_server setup is Postfix+LDAP+Cyrus. Now I want to migrate my domain mail to Zimbra server (zimbra), but I am considering option to leave current mail server working in the first phase, and then to only have subset of email addresses to be forwarded to zimbra server. It seems that zimbra refers this in their documentation as 'edge MTA'. Current config 

I have mail server setup to use postfix and cyrus imapd + ldap for mail lookups. Now I would like to specify that for particular address (e.g. someuser@domain.com) instead of delivering mail to mailbox it gets passed to script (ruby script to be more precise). I need this functionality since I would like to have mails passed to particular address to end up on our redmine server. How to do this? Currently I have in main.cf: 

old_server to receive mail for my domain as before, but for some of the email addresses I want them to be delivered to zimbra server. I should be able to determine which email addresses will be forwarded. I would like to avoid possible false spam detections for mails from mydomain.com due to this setup. 

/etc/postfix/ldapvirtual.cf is LDAP configuration file to search for mail addresses. What do I need to configure on postfix? 

I would like to configure pam module to use flat files for authentication. Basically I need same thing like just with possibility to use different files (other than and ). Is there existing pam module that provides this functionality? 

The magic seems to be and then only giving the devices that are known good + the last failing device. For the test scenario that would be: 

I have setup Zabbix to autodiscover hosts by pinging all IP-address in my range. It then adds the discovered hosts to 'Discovered hosts'. I have an action: Conditions: 

maxes out at around 100 MB/s. The new experimental option --pipepart delivers > 2 GB/s, but requires in.txt to be a real (seekable) file: 

I know I can set the values using and , but I would like to test that the is doing the right thing without rebooting. How can I make sure that the values are being set when people login using ssh without rebooting? 

NeilBrown (neilb (o) suse.de) answered this by email. The problem is the loopback-devices are too small. 1 MB is too little. If size is changed to 30MB it works: 

GNU Parallel instead spawns a new process when one finishes - keeping the CPUs active and thus saving time: 

I have added it in /usr/lib/zabbix/externalscripts where I have a different script, that works. I have configured an item: 

Again this confirms that Linux sees the device, which is already known. /dev/disk/* sees the RAID'ed internal disks as one device with 2 partition. It does not see the external disk: 

/dev/sdam is clearly (hd13). The rest of the drives are a software RAID60. Can I force GRUB to install on /dev/sdam1 (hd13) without probing? 

Found the answer if anyone is interested, here it is. Turns out the instructions I received from my predecessor were missing a key command which needs to be run after the transport file is updated. 

For ubuntu the apache and PHP user is www-data. Run and see if that fixes your issue. Also don't use 777 for the file permissions as Ignacio Vazquez-Abrams stated "0444 or 0664 for files, and 0555 or 0775 for directories" 

On two of our servers we have an Ubuntu LAMP setup with PHP code designed to allow large image and audio uploads. One of our clients is having an issue where they are unable to upload any files larger than ~4MB from any computer in their office. They get a "The connection was reset" this error in FireFox and in chrome they get "Error 101 (net::ERR_CONNECTION_RESET): Unknown error." In chrome I can watch the upload percentage and see the upload fail around the time ~4MB is reached (53%) on an 7.79MB file. It's not a speed issue as I've successfully uploaded files from slower networks. Apache is returning no errors in the logs, and is recording the start of the post in the access log. The PHP.ini is set to allow files up to 500MB and we have other clients doing this with no problems. I've upped the script timeouts of PHP as well. I've tested uploading from a number of other locations to the servers with no errors; and I've also tested uploading from the clients location to other services, also with no errors. I'm really at a loss i can't tell if it's a server error a client and I'm hoping someone might know of something I can use to test or perhapses possibly a setting I might have missed. 

I can, however, not find a way reload these values with out rebooting. I have read that the values are reloaded when logging in; it works when I do but it does not work through . I have the pam_limits.so in /etc/pam.d: 

So that could be related to the problem. Edit 2: I tried removing and re-installing portmap and nfs-kernel-server. No luck. So I mucked around with and did so the output is now: 

So you are running around 600 jobs per second. The overhead for a single GNU Parallel job is in the order of 2-5 ms, so when you are getting more than 200 jobs per second, GNU Parallel will not perform better without tweaking. The tweak is to have more s spawining jobs in parallel. From $URL$ 

Also I can no longer mount the local nfs on the local machine. I imagine that I some how need to register nfs in rpcinfo. Restarting nfs-kernel-server does not do that (or if it does, it does not work). still works. 

The problem was not in the Linux end but in the storage end. What was needed was assigning Linux as an initiator on the storage device and disable LUN Masking. After doing that I simply to force a rescan. Then the disks showed up in : 

I have a server with 2 internal disks with Adaptec hardware RAID and an external disk box connected via SAS. finds all the devices: