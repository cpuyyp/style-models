The larger shops I've seen have often gone in the direction of setting up a single common polling infrastructure. A set of dedicated pollers pulls the data and then makes it available for various consumer apps. This scales a lot better and ends up leading to a lot less traffic and reduction in control plane usage on monitored devices. This sounds like what you're after. There's actually an SNMP proxy MIB out there that's built into Net-SNMP. You may be able to set up a couple of machines in such a way, although it will likely require some customization. There are also commercial packages like this that may provide additional value in terms of protocol translation, support, etc. 

The switches don't necessarily have to be physically connected to one another but the ports connecting the bond members need to be in the same broadcast domain (usually synonymous with VLAN). Keep in mind that the IP for the interface can potentially show up on either switch. If the connected ports are discontiguous how would the rest of the network know where to send packets? So - say, for example, you had a common pair of aggregation switches where the L3 gateway resides and runs HSRP/VRRP. Two access switches independently connect to these aggregation switches and are passed the same VLAN. This would be fine. In contrast, if you hook up to two random switches that each are configured with the same subnet but are otherwise disconnected then it's not only not going to work during failover but would likely be broken under normal circumstances as the rest of the network has no way of knowing which network is currently active vs passive (unless you start dealing with custom tuning metrics and dynamically signaling state somehow - which is definitely gilding the lily for simple NIC failover). 

The mathematical formula you are referring to is actually the way to determine the most efficient transmit window size settings for TCP, not the actual bandwidth available. TCP uses a mechanism called sliding windows that allows for adjustment of transmit speeds based on network conditions. The idea is that a TCP transmitter will send more and more data without requiring an acknowledgement from the receiver. If there's a loss of data then the amount of data sent between acknowledgements decreases, thus also decreasing the effective bandwidth. The formula in question actually determines the ideal sizing of that TCP transmit window based on the latency and round-trip latency between a given pair of hosts. The idea is to have a window sized such that the amount of data 'in flight' corresponds to what's known as the bandwidth-delay product. For example, if you have 50 megabits per second (6.25 megaBYTES) and an average round-trip latency of 100ms then you'd have 6.25 * 0.1 = 625 kilobytes of data. This would be the value that TCP would negotiate (if configured correctly). As the latency and bandwidth characteristics of your links varies then so too does the window size. What you need is a bandwidth management tool like iperf (free) running on both the source and your various destinations. This should give you an idea of the actual amount of throughput possible (independent of other apps) while also providing some insight into latency. Running an extended ping between hosts will also provide a general idea of latency characteristics. When you have this data you'll have a better idea of what you should be seeing as far as throughput goes. BTW - The use of any kind of LAN optimizer will often incorporate data compression, TCP optimization, caching, etc.. While handy, it can obscure the nature of the underlying links. Once you have an idea of the raw bandwidth / delay (and packet loss, potentially) you can take a closer look to make sure your various hosts are set up to take proper advantage of available bandwidth. 

The Intel 82599 is a 2 port 10GE controller. QSFP is a 40GE connection mechanism. QSFP is a 40G connection mechanism but can operate in a mode where 4 10G lanes are broken out. Absent any other specs I assume that they're just using two out of the four. Honestly it seems like an expensive and inefficient way to produce what probably should just be a couple of SFP+ ports. As far as what can be connected? You can find QSFP adapters that break out to 4 twinax or AOC cables (fixed length .5 -> 30 meters with SFP+ connectors). You can also get a few different fiber options which will be MPO/MTP 12-strand connectors. Depending on what the vendor supports this will likely include multimode (10GBase-SR) which can support up to 300M on OM3 and possibly some single mode options. I know of 40G optics that break to 10G LR (10KM). 10GBase-ER is rated for 40KM but I've never heard of a QSFP breakout module that would support these. So - you need to get specs from the vendor for what PHY they'll specifically support. If we assume it's SR (not unreasonable) then you can procure standard breakouts to OM3 LC connectors. You'll have a live TX and RX for each of the two ports. To connect to another server (or a server and a workstation) cross the respective TX and RX links. It's just a back-to-back 10G Ethernet connection, after all. 

You can separate clients within a VLANM if your switch supports PVLAN (private VLAN) which can be configured to allow any host to talk to the firewall while being unable to communicate with any other device. You can additionally configure your PVLAN to also allow communication amongst limited groups of servers. What sort of switch are you using? 

The GD designation is, unfortunately, mostly a memory - and even when GD code was available it tended to be a year or two behind in (usually critical) features. Even ED vs LD isn't 100% indicative of ideal code versions in all cases. Bugs can exist in any version. The bugs that involve your particular set of features are the ones you care about. Check out the bug navigator to list the open/fixed issues in the version of code that best fits your requirement. Your Cisco SE / support team should also be in a position to run through a quick bug scrub to see what the prevailing wisdom is as far as best of the latest-and-greatest but ultimately the bug toolkit is the best tool you've got. As to upgrading from 12.2 - what features would you gain that would be particularly helpful? Some shops I've seen have made the move to be able to support SSH certs or new knobs on routing protocols while others just want to be on a reasonably modern rev just as a maintenance practice. Toward this end you might also want to run through the feature navigator to determine exactly what's changed from version to version. 

Issue the shutdown first, then issue the "no keepalive" command, then bring the interface back up. It should show up/up at this point and hold that state indefinitely (for a GE, anyhow). Turning keepalive tracking off while the interface has already marked itself down isn't going to bring it up. 

The kinds of factors that drive the need for a dedicated server don't necessarily correlate to volume of traffic or typically observed threats. A site with massive back end I/O requirements might hand out small amounts of tabular data to a handful of users. The decision to pursue a dedicated firewall should be approached in the same way. The other point, of course, is that adding a dedicated firewall later isn't (or shouldn't be) very invasive. 

Both Ethernet ports should be running auto/auto, as should the 4900's ports. This isn't the source of an ignored packet, though - media mismatches will show up as CRC errors, input errors, etc.. How many ignored packets are you seeing and over what period of time? A few hundred over a period of months isn't that big of a deal - especially if you're not noticing a performance hits. Are you measuring CPU usage at all? Even tracking on 'sh proc cpu history' might give some clues as to whether your counter increases correlate with high cpu usage (..which can contribute in its own right to these kinds of issues). Also confirm that cef is enabled on the box globally as well as on the individual interfaces. Run 'show int st' to get a sense of the ratio of process switched to fast switched packets - the number should be predominantly fast switched. If not, a particular feature or configuration is likely hanging things up. Are you doing NBAR, fancy firewall/IDS or lots of un-accelerated crypto? If you're at all memory constrained I would go slowly with the buffer changes - going too far can cause more harm than good. 

Use multicast. Each client would need to join the group, but this is a very low overhead task. You'd have the additional benefit of being able to have clients on other machines be able to efficiently receive the message. Failing this, look into one of the various message bus packages (i.e. mqueue, rabbitmq, etc) that will allow for reliable delivery of programmatic information to various processes without a need to reinvent the wheel. 

1.) An access port by definition has no tags applied to packets. Trunk ports have tags. If your laptop is connected to a trunk port then it will be able to see packets with VLAN tags applied. 2.) Under normal circumstances the switch does the tagging. 3.) Again, definition of access port = no VLAN tagging, definition of trunk port = VLAN tagging present. The idea is that the switch will apply a VLAN tag on ingress based on its configuration (i.e. port x/y is on VLAN 5). The tagged frame is then switched according to that tag - potentially across trunks. When it arrives at its destination the egress port strips the tag off. The hosts on the VLAN simply see a common broadcast domain. It is reasonably common that certain servers (especially virtualization servers) can be configured to apply and strip VLAN tags, effectively giving them the ability to run a larger number of virtual interfaces on top of a single physical NIC. 

Your diagram is slightly unclear - I'm assuming that the two Ubuntu hosts are each connecting to the "router" device, which provides L2 and L3 connectivity. Anyhow - if the various VM's are on the same subnet (likely) then multicast will work between the hosts. Apart from making sure that multicast support is enabled on the guest operating systems there isn't a whole lot to do. If these VM's were, instead, on different L3 subnets then you'd need to make sure that the router supported multicast routing. 

While dd can be a useful tool it's also important to remember the possibility that you're testing a file that has already been cached. In NFS environments we've gone so far as to unmount and remount partitions between test iterations to make sure we were definitively hitting the server rather than relying on something local.