The key to understanding how to manage moving a TDE encrypted database from one server to another is understanding that the private key of the certificate is the only key needed to decrypt the Database Encryption Key in the TDE enabled database. The encryption heirarchy below the certificate can contain different values for the keys, as long as it is fully functional. When the TDE Certificate and private key is restored, the server will encrypted it using the DMK in the master database. If a DMK does not exist and you create a new one, then it will protect the certificate private key with the new DMK. When you execute a command to create a DMK, the server encrypts it using the password you provide and also encrypts it using the SMK, so you don't need to create it with the same password because the server can decrypt it when needed using the SMK. As long as each keys in the heirarchy can decrypt the subsequent key, the TDE will work. The encryption heirarchy in SQL Server is implemented based on the ansi x.917 standard. While backing up and restoring a DMK may not apply to TDE, it works well when restoring a database with column level encryption. If there were no command for backing up the DMK and SMK, then restoring a database with column encryption would require restoring the master database and implementing the same service account on the destination server to regain control of encrypted column data, which is practically rebuilding the server to restore one database. 

I would recommend the 3 column solution with area code optional. It will be able to handle cases of countries where mobile numbers do have area code (like the U. S.) and will be much easier to maintain and service. 

It sounds like you might have network issues, although withojt further details specifying the "sometimes" when your connection does work, it's hard to tell. It could be something like your server is not configured to listen on that specific network interface (check and make sure, like the manual entry in this link explains) or you might have some other piece of software interfering, such as a firewall. Maybe the times when the connection does work somehow trigger a different rule, which allows them to connect. As other answers state, a reboot might be a good starting point, but if you haven't done anything weird regarding network config I doubt that will fix it. 

However, I have to leave this running for a long time, and I would like to not bog down the usual DB activity while the update happens. Is there a better method to do this than running a process that does everything in one go? Thanks very much! 

Most data models are lacking in a good DATE Dimension and thus force developers and report developers to rely on date arithmetic to find date boundaries that are relevant to the business model. (Fiscal Year, Fiscal Quarter, Fiscal Period, Calendar Quarter, etc.) A good CALENDAR table would go a long way to making your life easier. A simple EventDate BETWEEN SYSDATE - 458 and SYSDATE risks truncating dates out of your oldest quarter. Take TODAY as an example: SYSDATE - 458 yields 2010-09-28. If my math is correct the 3rd Quarter of 2010 started on July 1, 2010. You need roughly 548 days to make sure you are covering the entire range of current quarter plus the previous four full quarters. Trouble is that when you this will cause some overlap as your current quarter is partially complete. So you are faced with some additional logic to truncate out the fifth oldest quarter that you don't wish to include. My PL/SQL isn't the sharpest right now to write that logic but I hope the explanation helps shed some light on the approach you will need to take. 

Trying to figure out how I should expect my database to perform. Basically, I need to determine when the server I have set up is reaching its performance limits - which would help me plan server scaling better. This question aims more towards ways that I can calculate or estimate (really, any idea of this would be good) expected performance. This should ideally help me come up with a formula I can run based on several factors (like record size, number of rows, etc), instead of a subjective assumption based on a particular server / DB. So, what is a good way to determine expected performance on a PostgreSQL server? Thanks very much! 

We created an image of one of our database servers in Rackspace. Then, we created a new server using that image, expecting things to work. However, the index performance we have seen seems to be bad, and even the query plans, when comparing the outputs from the first server to the second one, have been different. Basically, the second server - the one that was restored from a saved image - does not seem to rely on indexes as it should. Is there something related to image restore that would cause this behavior? If so, what may need to be done to fix the issue? Should I look somewhere else? Thanks very much! I can provide more information if needed, so let me know if that would be useful. 

Also, when you create a symmetric key, you can specify the argument key_source, which forms the basis of creating the actual key, but if you don't the database engine will create a random key for you. The symmetric key is protected by the certificate, not a derivative of it. It would be very dangerous if the symmetric key were able to be derived from the certificate or it's private key. The Open Master Key command is redundant since it is already been opened so that the private key from the certificate can be used. I would also highly advise against using the master database for column level encryption for your user data. I hope that the above description was clear because I wanted you to understand why you are having a problem before providing the resolution. The problem is that the Service Master Key on your local SQL server instance can't decrypt the Database Master Key. You can fix this in one of three ways. Back up the SMK from production and restore it on your local SQL Server or backup the DMK for the production database and restore it on the database on your local SQL Server or move the command to open the database master key by password before the open symmetric key command. Backing up the DMK would be the better and less impactful choice because restoring an SMK could be resource intensive. I would advise one of the first two resolutions since you don't want to put passwords in your code for security reasons. 

Need to know how PostgreSQL orders records on disk. In this case, I would like to take advantage of index combination as stated in the docs, which as I understand uses bitmaps to get matching rows and returns them according to their physical location. The table in question has been clustered by its primary key. As I understand it, PostgreSQL does not automatically continue doing clustering after a clustering is finished (although it does remember that it clustered according to a certain index). Now, since this is the primary key, I wonder if the physical storage order would be according to that (which if true I would like to use to our advantage for a specific query). In summary, how does PostgreSQL order its new records, especially after clustering? Thanks very much! 

Run the query. Use tool on the query. Analyze the results of using a certain tool. Check indexes.... 

Trying to tune our application and would like to know if the stats data that pgBouncer shows as , according to the PgBouncer docs on usage: 

Looking for a good way to update the value on a column for all the rows in a database (not huge, but big enough - about 10M records), without locking the whole table, so operations can continue while the update is working. The update operation is pretty simple, because the value of the new column is basically computed from another column, kind of like this: 

To configure server side SSL for SQL Server, you will need to place the public CA certificate and private key in the certificate store of the service account running the database engine and place the certificate without the private key in the certificate store of the java VM initiating the connection. Then use SQL Server Configuration Manager to open the connection properties and set force encryption to yes and select the certificate to use, which would be the public CA certificate, for encrypting the connection. When an SSL connection is initiated, the SQL Server will present the certificate to the java VM and the java VM will find it in its store of trusted certificates and validate it directly against the CA without the need for walking the chain of trust for intermediate certificates. Since the server is on the Internet, it wouldn't hurt to configure ipsec so all traffic is encrypted. Also, It's good that you have a certificate from a public CA because if no certificate is configured, then SQL Server will used a self signed certificate and that configuration will leave you vulnerable to a man in the middle attack. 

First of all, kudos on your effort at designing a DB for this problem! It's great to find people from different backgrounds using software to solve their problems. Now, for the meat. For starters, generally for database (or any kind really) design problems, there can be many possible solutions. Depending on the requirements and constraints for a particular problem, those solutions may become acceptable or not. In this case, while it may work, I would suggest improving your design by reducing the number of tables and relationships between them, and therefore more cleanly and succintly representing your real world situation. Something like this is what I would set up: 

Notice HealthReports is the principal table here, around which the data is structured. This seems to me a more natural way to structure it based on the workflow you have described. I hope this helps! 

Some queries on my database server seem to take a long time to respond, and I believe the CPU usage is high. When running , I see ~250 "idle" connections (which I believe to be too many). I haven't started doing a full diagnosis, but I wanted to know if this is a good place to start looking. I am also using PgBouncer with transaction-level pooling. I am suspecting that I can easily reduce the number of connections by tuning the pool size. However, I don't want to start doing too many changes unless there is a good reason to do it. Can many connections in PostgreSQL 9.2 affect performance? Thanks very much! 

Sounds like you are describing dimensional data modeling where you have a fact table supported by multiple dimension tables. Your Line Item table is your "fact" table and then you have a product and price "dimension" tables. Your price table could very well be a fact table or a slowly changing dimension table as well because there is a time element associated with a product's price changing over time. 

There are a multitude of factors that go into determining which Teradata platform and the configuration of the platform that will suite your needs. Teradata has spent untold amounts of money on intellectual property and decades of experience working with potential customers to help them properly size a configuration that not only meets the immediate needs of a customer but provides them capacity for which the environment can adequately grow and evolve. I would strongly suggest you reach out to Teradata directly and engage them in a pre-sales capacity if your company is considering their technology to meet the needs of your data warehouse environment. For a sandbox environment, you could may be able to get away with using the one terabyte version of Teradata Express on an adequately sized server or consider using Amazon EC2 to stand up a instance of Teradata to complete a proof of concept. It should be noted that either of these options should not be used to gauge the performance of a production environment for service level agreements but whether or not the technology will accomplish what you are trying to do. 

I saw this example in a Quora question about sharding MySQL tables, where it is specifically mentioned as a good idea for sharding if and are to be queried together. How is the modulus calculated so that rows with the same are on the same shard? 

In MySQL Cluster, and generally in sharded databases, how is a compound primary key's modulus calculated? Say, for example, there are two tables that have primary keys like so: 

A HealthReport record is created whenever something wrong is found. I would suggest having a cage ID instead of a room ID, if possible (this, of course, depends on what data is available). Observations are made for each HealthReport. These are basically just descriptions of issues found with mice in the report, but you may extend it if you want to. Depending on how specific you want (or can) make your data, some possible extensions would be: mouse gender, weight, age, a type of observation/injury, number of individuals... the list can go on and on. Again, this depends on what data is available for you and what you wish to include. Also, keep in mind a possible extension mechanism would be to add a ExtendedObservations table that references Observations, in which you may include additional data. That would be more complicated and IMHO, not necessary for this particular application... but it's a possibility. A Case is filed for each of the HealthReports. Here, again, you may choose to represent the "no case was filed for a report" event by not adding a record to the Case table (which would make the IsReal column redundant and therefore warrant its deletion), or, as I have, by creating a Case for each HealthReport and have it specify whether a vet determined it was valid or not. Note that the DateClosed allows for s in case you want to add the record of an open case first and close it at some later point in time. If you do not want to allow this, I would suggest not allowing values to be inserted into the table to maintain better data integrity. I would strongly suggestnot to allow s if you can help it, so your data is as dense as possible (then you won't have to worry if a is there because a Case is still open or because someone forgot to close it). 

By default, Teradata SQL Assistant will attempt to query the views , , to populate the Database Explorer. It is possible in your environment that those objects are not accessible to developers or end users via the user. Instead, you may need to modify your connection settings ODBC or .Net Provider to use the X-Views in DBC. These are a collection of views which restrict the rows returned based on the privileges your user account has been granted to access or which you have created. The ODBC DSN, .Net Provider, and JDBC drivers for Teradata have a means to use the X-Views by default to enable database tools such as SQL Assistant or Teradata Studio/Studio Express to populate the database explorer controls "transparently". Try this and see if it works. 

The space consumption for statistics on Teradata is not significant enough to qualify as a disadvantage. For example, the statistics for a single column is retained in a 16KB VARBYTE column on DBC.TVFields The rule of thumb has always been 10% change in the data which you have statistics collected or if they are stale. Unfortunately, stale has never really been clearly defined. Teradata 14.10 will introduce a more automated mechanism for maintaining statistics to help reduce the cost (CPU and IO) associated with the collecting stats using a homegrown maintenance schedule. This enhancement will be supported through Viewpoint. Teradata 14 also introduced some changes with statistics that have to be taken into consideration from previous releases. Carrie Ballinger has done a good job of capturing these changes in her articles on the Teradata Developer Exchange found here and here. Your stats maintenance schedule will be driven by the size of your environment, your ETL schedule, and manner in which your ETL maintains the target tables. We have multiple streams that maintain the same set of large target tables. As such we have moved the stats maintenance for these target tables to an external process instead of within each ETL stream.