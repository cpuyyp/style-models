Let's try this with computers. "When you smash your CPU, maybe it continues to exist as a computational entity." Say what? We don't know very much about the implementation of consciousness, but in all cases we know of (which is a lot of cases), it requires brain activity. Of course you can always hypothesize that there are various inaccessible states that exist and are affected by what we do but can't be measured (e.g. heaven), but in general parsimony (and sanity) requires one to reject random flights of fancy about nominally possible yet unmeasurable things. So, anyway, we already know the answer to a high degree of certainty. Our certainty comes not from obviousness but rather enormous amounts of experience with various medical conditions. (I should point out that it's not so hard to argue that we have some duty to verify that a person is actually brain-dead; given that a functioning motor cortext is not required for the brain to be alive, "does not respond to stimuli" is already known to be inadequate. But equating non-responsiveness with brain death is a different issue--medical practice taking shortcuts (or relying on historical norms) instead of following evidence.) 

Daniel Dennett (among others) advocates a type of moral responsibility with only as much free will as you can get with determinism or determinism + unwilled randomness (more or less compatible with Sam Harris' assumptions). There's a pretty good summary post here. In brief, the argument ends up redefining terms somewhat while claiming that this is what we actually mean anyway (or is good enough): you, as a complex information-processing agent, are said to be responsible for those actions that you have computed to be the ones you want to take (with provisos for being subject to decisions made by others). 

Mobius strips aren't that interesting physically because they're all just rings. Everything is three-dimensional. If you took a noodle and squashed it so it was 30% wider than it was tall, and instead of doing it flat you did a half-revolution so that one edge of the bulge went from left to up to right, you'd have a "Mobius strip" of a sort. But it's a really unimpressive one because the curvature is still nearly constant as you travel around (rather than along) the noodle. Doing it with paper just makes the aspect ratio more and more extreme. You can squash your noodle into a Y shape had have it rotate by 120 degrees also to get an interesting shape (if you forbid yourself from going over the high-curvature "edge"). I don't think any of this proves anything about the reality of objects, but it demonstrates that practical geometry is fun. 

Your neurons are constantly turning over their component molecules, changing synapses in response to input patterns, and so on. You are, at a neuronal level, not exactly yourself after a matter of minutes, much less decades. Given that you postulate an exact algorithmic copy implemented in a different way, and therefore that computer-you is more like real-you-now than real-you-now is like real-you-in-a-year, of course one must say it's still you. Or one can invent unobserved phenomena that prevent you from retaining identity (souls, dualism, etc.), but this would be an evidence-free leap. So at least we can say that there's no reliable evidence to indicate that you wouldn't be you throughout the entire transformation. Of course, this is hypothesizing the existence of technology that might not even be physically possible. So it's likely to remain a thought experiment for a very long time (perhaps always). 

The cube exists, is known (shockingly recently, though!), and is one of an infinite series of such constructs. As you say, it's not obviously of much use; just a mathematical curiosity (and a pretty obvious one at that IMO, but maybe the intersection of philosophers and algebraists/topologists is not well inhabited). 

You can convert everything into Do. "I have X" is equivalent to "I am a haver-of-X". (I have a car. I am a car-owner.) "I am Y" is equivalent to "I am engaging in having the Y-property". (I am hungry. I hunger. (I am owning a car.)) English grammar doesn't always make it easy to make these conversions, but there's no conceptual problem. (It would be weird if we could not, as physics is just state unfolding over time, which is a "do" even if sometimes the bulk properties are quite stable.) On the other side, if you split "be" and "have", why not reflexive and transitive verbs? What about "give", which involves three entities, or "trade" which involves four? Bottom line is that there's nothing fundamental here beyond common patterns in language which reflect common relationships in daily life. There's no logical necessity, just convenience. 

The two situations are essentially the same, as far as you've described them. The difference, undescribed, lies elsewhere. The theory of gravitation has immense predictive power--you can calculate times of eclipses, send spacecraft to Saturn, etc. etc.. This doesn't mean that every attractive force is gravity, though (e.g. magnets!). But a lot are, and the model fits observation extremely well. (So much so that people infer the presence of "dark matter"--something that has a gravitational effect but isn't visible--to explain things like galaxy shape. Dark matter is not as certain as gravitation, but gravitation works so well that dark matter needs to be very seriously entertained as a hypothesis.) On the other hand, noting that humans make complex objects is kind of like noting that gravity pulls things together. It's a pretty good bet that certain kinds of artifacts have been made (archeology would be really difficult otherwise!), but we now know of a process (evolution) that enables living objects to become more elaborate and complex in appropriate circumstances. And we know that appropriate circumstances are pretty common. If you don't know about electromagnetism (or strong and weak nuclear forces), it's not entirely unreasonable to suggest that attractive forces are all gravitational, even if your hypothesis doesn't seem to account for magnets very well. Likewise, if you have no idea that a complex object can arise in any fashion other than creation by a human, postulating that everything was created by humans or human-like entities is not that unreasonable, even if that hypothesis also doesn't account for many things very well. Once you find a superior hypothesis, holding onto the old not-very-satisfying best effort is not the scientific approach. And when your best effort doesn't fit the data all that well, doesn't provide all that much explanatory power, you should be skeptical of it. When it gets down to it, any process that helps you explain observations or theories and has some robust path for you to discover if you're wrong can count as scientific reasoning. It's easy to get caught up in the sociology of how the scientific method is typically practiced now (e.g. peer review in journals), but that's not logically necessary, just a heuristic that works sufficiently well for us at the moment. 

Your confusion I think stems from the difference between Free will and free will. (I forget whether one/both of the Churchlands or Dennett (or all three) make this sort of distinction.) That is, most people are capable of differentially reacting to stimuli and to some extent optimizing their conditions. That there are occasional exceptions with brain tumors affecting regions of the brain responsible for declarative control etc. does not change this basic fact. All you need to justify our current system of laws is to note that people are free in this sense, not Free in some metaphysical sense, and that since we do not want people to do X, we say that if they do then Y will happen, engaging their volitional abilities to prevent them from doing X. All you need to have a moral sense is an awareness of the constraints of operating in a social environment, and we have those innately as one would expect (see works by Marc Hauser, Robert Wright, and Jonathan Haidt among others). You don't get into trouble until you run into situations where you know that P couldn't avoid doing X--so your reason for inflicting Y is invalid--but also know that Q can only stop themselves from doing X if they are sure they will be punished, and they will only be sure if you actually punish P. Now you have to ask awkward questions like: is it ethical to punish P for its beneficial-to-the-rest-of-us effect on Q? A few moments' reflection should convince you that this is always the case: if P did X, they in practice were unable to avoid doing X, so the strategy of threatening Y failed. I'm unaware of a really solid argument showing where to go next, though I will note that the universe is completely happy to smash your nose if you can't help running into walls, so imposing punishments anyway is not completely without precedent. (Also, punishments that also serve as prevention (e.g. imprisonment of someone violent) or restitution (e.g. fines used to fix whatever problem the person caused) are justifiable (to some extent anyway) independent of whether the agent in any sense could have avoided doing what they did.) Anyway, bottom line is: (1) moral sense is built in, as you'd expect from game theory; don't worry about that; and (2) people have thought about punishment in situations where there is only freedom not Freedom and concluded that it is non-problematic. I encourage you to read them, or maybe watch: $URL$ (Disclaimer: haven't watched it myself, but I know what it ought to contain and should be relevant....) 

I haven't seen anything written on this, but I think this is strongly related to lexical semantics. The reason is that you have picked only one of a profusion of problems with "if the match is struck, it will light". The match will not light if it is wet, at absolute zero, struck too gently, struck in an atmosphere without oxygen, struck at relativistic speeds, struck the way you strike a drum, struck on a smooth surface, etc. etc.. That you are supposed to import all the relevant context (standard temperature and pressure, appropriate strength, motion, and contact surface for a match, and so on) is a typical part of natural language--and indeed, if you're not pretty savvy in this regard anyway, you will misunderstand the word "light" to be the same thing as what a lightbulb does. But just as lexical semantics opens up all sorts of wiggle-room for comprehension, so does context-aware analysis of truth statements. If one wants to know why formal logic is not always as useful as one might hope for everyday situations, this is a big part of it.