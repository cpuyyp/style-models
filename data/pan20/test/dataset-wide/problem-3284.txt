If there are crossing lines in the syntactic tree, then one or more discontinuous dependencies is present. A projectivity violation is a discontinuous dependency. There are a lot of terms used to denote such dependencies, many of which are listed at the start of that Wikipedia article (discontinuous dependency, long distance dependency, displaced constituent, discontinuity, projectivity violation, etc.). These terms are denoting the same thing for the most part, although there are preferences acoss various theories. Looking at some of the trees in that article, the discontinous dependencies are the ones where the solid dependency edge (line) crosses one of the dotted projection lines. Note that each of the trees that contains crossing lines contains just one such discontinuous dependency. The interesting question concerns how theories of syntax address the crossing lines. Chomskyan syntax has traditionally assumed syntactic movement: $URL$ Other grammars (HPSG, LFG, some DGs) assume feature passing instead. In any case, most theories examine the tree structure and consider the path through the tree that one can trace from the displaced unit to its governor/head. 

Yes, there are. Head-marking languages generally allow for free word order in case the language is caseless. Macedonian pops to mind, a language without cases on nouns but with free word order. Grammatical relations are indicated by clitics attached to the verb. Likewise, Northwest Caucasian languages have free word order and little morphology. In Circassian, only specific NPs are marked for case; Abkhaz lacks cases altogether. 

The book you mention (Partee et al.) is the best one. If it's not an option for you, I'd recommend this one: Gamut 

The sentence will be a CP in traditional GG: [CP [C have] [IP I seen the man]]. Note that constituent trees represent sentence topology rather than grammatical relations. In the example at hand, "have" and "seen" are coheads but they don't appear contiguously in the sentence. 

You'll need a parser to identify objects automatically. There are a few online interfaces to natural language parsers. For example, you could use the Stanford parser (link) and look for "dobj" to find direct objects. 

It actually came form Late Latin (e.g., probatum habeo). It's a natural process, a similar construction with "to have" has developed, for example, in Northwest Russian which is very interesting because Russian has no corresponding verb so it had to resort to its "у+NP" construction (e.g., у него корова подоено "he has milked a/the cow", lit. "at him cow milked") which clearly shows the origin of this tense is semantic. 

Part of the/a explanation to the question is provided in librik's comment and part in Jlawler's comment. The disappearance of morphological case from English and French has led to the divergence of the two groups of pronouns. Languages with rich case systems do not have the division, e.g. German. The insight that I think can be added is that the marked forms (I, he, she, they) appear obligatorily when they constitute the entire subject of an overt finite verb. Adjacency in a strict sense is not necessary, e.g. 

If ambiguity is what the question is most concerned with, I think the short answer to the question is no: distinct structural analyses of an utterance almost always point to distinct meanings. There are, however, numerous sources of ambiguity, syntax being one and lexical meaning being another. If the grammar does not have a way of acknowledging the cues that disambiguate the utterance at hand, an incorrect structural analysis is easily possible. Consider the following sentence in this regard: 

For most dependency grammarians, the terms phrase structure grammar and constituency grammar are synonymous. For those constituency grammarians who do not pay attention to dependency grammar, the two terms phrase structure grammar and constituency grammar are not synonymous, however. The term phrase structure grammar denotes a non-transformational approach to syntax along the lines of GPSG or HPSG. These frameworks do not acknowledge movement in the sense of traditional Chomskyan syntax. They intentionally use the term phrase structure grammar to mean 'non-transformational grammar'. For an example, see Borsley (1991: 8). Borsley, Robert. 1991: Syntactic Theory: A Unified Approach. London: Edward Arnold. The GPSG/HPSG crowd is likely to use the term constituency grammar as an umbrella term to denote all constituency-based approaches to syntax, be these approaches transformational or non-transformational. In this regard, constituency grammar and dependency grammar are opposites in the relevant sense, whereas the term phrase structure grammar denotes a subtype of constituency grammar, a non-transformational one. 

There are no fundamental properties. Some/most (?) natural languages are mildly context-sensitive to allow for features such as cross-serial dependencies. Pure context-free grammars are too cumbersome to be used in linguistics, one needs to add a constraint system (in the form of a formal logic, typical an equational logic) which makes the whole system Turing-complete even if the backbone is a context-free grammar. 

-asъ changed to -axъ in this context, it’s a pretty known fact in historical Slavonic linguistics. Note that -s- can still be found in modern Lithuanian in plural locatives. 

One can't account for information structure at the level of surface syntax. Consider the sentence илсьірбоит (I'm showing it to her; Abkhaz). It has three topical arguments (only topics can be pro-dropped) and since a sentence without focus wouldn't be felicitous, the predicate is focal. But the syntax tree consists of only one node. It's useless. The LFG people suggest to ignore the categorial structure and introduce discourse functions into the functional structure. But it was Tracy Holloway King I think who pointed out that this approach doesn't work for subconstituent focus. It's easy to see that any dependency grammar would have this problem. The solution within LFG was to add a new layer of representation for discourse functions to the theory. In FGD Sgall et al. reorder their tectogrammatical trees with respect to information structure. But at the same time they require that the trees be projective. I don't think that their solution is particularly elegant but if they weren't able to come up with a nice solution, then there's probably none within the framework. One possible "solution" is to say that information structure isn't part of syntax sensu stricto and capture it at the level of pragmatics. This is what Jerry Hobbs did in his framework and it makes sense, but it also moves us from linguistics to computational logic and/or automated reasoning. 

All headed constituency-based structures (i.e. endocentric structures) can be easily translated to the corresponding dependency-based structures. One need merely collapse all the projections (minimal, intermediate, and maximal) of a word down to one node. A non-headed constituency-based structure (i.e. it has exocentric structures), however, cannot be translated to a dependency-based structure, because dependency by its very nature views all syntactic structures as headed (i.e. endocentric). See the distinction between endocentric and exocentric structures here: $URL$ All dependency-based structures can also easily be automatically translated into corresponding constituency-based structures. But here's the difference: the constituency-based structures that result from the translation are necessarily quite flat, much flatter than most constituency-based grammars want to assume. Above all, the resulting flat structures do not acknowledge a finite VP constituent. Without the finite VP constituent, there is no way to do much of standard Chomskyan syntax. For instance, all explanations of syntactic phenomena that build on c-command are not possible without the finite VP constituent. The distinction between dependency and constituency is quite profound. Dependency is a strict one-to-one ratio (words to nodes), whereas constituency is a one-to-one-or-more ratio (words to nodes). If one operates with a dependency-based model, the amount of syntactic structure that one can posit is very limited. Whereas if one is operating with a constituency-based model, one has the ability to assume much more layered (i.e. taller) syntactic structures. One has the ability to acknowledge many more groupings of words (i.e. constituents) in the constituency-based model. As an experiment, count the number of nodes and edges in the two trees in the question. The dependency-based tree contains 7 words and 7 nodes with 6 edges. The constituency tree, in contrast, has 13 nodes and 12 edges. These numbers bear witness to drastically different ways to approach the syntax of natural language. Dependency-based models are minimal, whereas constituency-based models are maximal. The pertinent question is whether the extra structure that constituency enables is warranted. My personal view is that it is not, but each grammarian has to make up his or her own mind about that. 

The existence of a VP constituent is language specific. There are detailed constituency tests. One of the definitions of a language being configurational is the existence of a VP. The main evidence comes from sentence topology. In German, for example, no finite VP can be identified because the sentence splits into Vorfeld, Mittelfeld, etc., that is, it's organized along a completely different scheme. 

Ditransitives of the English type are very rare. Most languages use either case marking on nouns to signify grammatical relations or polypersonal head-marking. By "English type" I mean that both objects can be passivized. Your example can be paraphrased as "A book was given to Mary by John" or "Mary was given a book by John". 

One must be careful in making generalising statements, for example, languages without (morphological) case needn't be SVO or OVS, just look at Abkhaz. However there are typically some tendencies or predominant patterns. In the case of SOV, the languages tend to be agglutinative (which mostly implies rich morphology and therefore pro-drop) and non-configurational whereby SOV only applies to non-emotive (information-structurally unmarked) utterances. They also often exhibit head-marking predicates. If a SVO language has adpositions, they tend to be postpositions. BTW German is neither SOV not SVO but V2 in the standard typology. 

demonstrative determiner: Ginny likes that cake. demonstrative pronoun: Ginny likes that. demonstrative adverb: Ginny is that hungry. subordinator (i.e. subordinate conjunction): Ginny said that she likes the cake. relative pronoun (controversial): The cake that Ginny likes is chocolate. 

In these cases, the preposition of appears optionally. With what, the preposition of cannot appear, e.g. *what of a beautiful child. Furthermore, the indefinite article is obligatory in such cases. With what, the indefinite article need not appear, e.g. 

The coordinated strings are distinct in syntactic category in these cases (Adj+NP, Adj+VP, NP+VP), hence an account of such cases in terms of phrase structure rules is quite at a loss; there are hardly any conceivable rules that could capture such coordinate structures. The notion of syntactic function is a more promising way to approach such data, i.e. the coordinated strings must be alike in syntactic function, as pointed out by Thomas Gross. The greater point is that one is not going to get far with phrase structure rules as they are commonly understood. They are quite incapable of shedding light on the nature of coordination and many other areas of syntax. When it comes to coordination, I have worked on them extensively and would be happy to point to relevant literature that provides another means of analysis (and one that I think is much more promising than phrase structure rules), if anyone is interested. 

In affixal polysynthetic languages such as Inuktitut, Yupik and Greenlandic the criterion is pretty simple, a word is composed of exactly one lexical stem and a number of bound morphemes. Cross-linguistically, words have one (primary) stress which helps reveal boundaries between them - with the exception of clitics. In compositional polysynthetic languages an otherwise freely occurring stem is considered incorporated if it can't be moved out of its position within a compound word. Typically, nonspecific objects tend to be incorporated whilst specific objects tend to be adjuncts, though on this point languages differ quite a lot. 

The IL used in symbolic NLP is called first-order logic. There are various more or less differing notations but it all boils down to plain old FOL which can easily capture the meaning (literal or context-specific) of any well-formed sentence. 

I suppose you mean a rule-based parser since nobody would think of developing his own statistical parser (there are so many good open-source libraries). Building a parser is quite complicated. The best way is to have a context-free grammar (CF parsing is trivial) and build up the dependency structures via constraint rules. This is how LFG works, whose f-structures are just plain old dependency trees (in general they are DAGs but can be thought of as trees with coreferences). If you don't want a context-free backbone in the parser (which doesn't make much sense for most Indo-European languages), you should devise syntax rules based on feature constraints. The dependency tree of a phrase or sentence is a rooted spanning tree over a graph whose nodes are the words of the phrase with edges representing possible dependencies that conform to the constraint rules. In Latin, for example, one would say that an adjective depends on a noun if they agree in case, gender, and number, such as puella pulchra (nominative), puellam pulchram (accusative), etc. Likewise, verb phrases would be constructed via constraints. In a sentence like tu pecuniam debes, the constraining rules must state that the subject of a verb is in nominative and its direct object in accusative. But note that in most languages you'd need a ton of word order rules (that's why it's better to use a context-free backbone). Moreover a good parser needs a lexicon with valency frames to resolve ambiguous structures. A simple parser can be quickly developed in Prolog. 

In a dependency grammar, complete subtrees are constituents. In both (1a) and (1b), the string much to learn is a constituent, i.e. a complete subtree. In (1b) it can be labeled the "topic". As a constituent, it has a concrete status in the dependency hierarchy. The following trees (taken from the article on topicalization in Wikipedia) illustrate this state of affairs in both a constituency and a dependency grammar: 

The simple answer to the question is NO, there is no established term such as determiner group used to denote the word combinations in the question, and there is a reason why no term has been established to denote such word combinations. This reason is that the relevant words (e.g. my three, both the, all the many) hardly qualify as a unit of syntax. At most, one can acknowledge that they form a string. Such word combinations qualify as a constituent in no theory of syntax (that I am aware of), not in phrase structure grammars and not in dependency grammars. Furthermore, such word combinations do not qaulify as chains, that is, they do not qualify as catenae: $URL$ In contrast, verb predicates (e.g. will have fixed, has been fixed, may have been being fixed) do qualify as chains of words, as catenae. The fact that determiner strings qualify neither as constituents nor as catenae means that it is more difficult to denote such combinations with a particular syntactic term. Theoretical syntax tends to focus on word combinations that are recognizable units. But there is nothing to stop one from establishing a new term, e.g. determiner string or determiner group. In fact, since there is no pre-established term for such combinations, one has more freedom to call them whatever one wants. 

Any context-free parser can be used. However "pure" CF grammars aren't practical for real applications. I'd recommend to use LFG or a similar tool that generates more useful underlying representations. 

Are you familiar with Sgall's Functional Generative Description (the theoretical framework used in the Prague Dependency Treebank)? He doesn't have synsemantic words in dependency trees and I think this is the right approach. He has the simple rule that in a phrase with an auxiliary or a preposition the autosemantic word is the head and whatever the auxiliary or preposition contributes to the syntactic structure is an attribute of the corresponding node in the tree. In other words, "I saw him" and "I have seen him" have the same dep. tree. Likewise, "Mary loves John" and "María ama a Juan" have the same dep. tree as far as structure is concerned. To sum up, only content words appear in syntax trees. If for some reason one would like to have function words in dependency trees they should be children of the nodes that represent content words (since they modify them, not vice versa). 

The question is struggling with what school grammar (the type of grammar that many of us were taught in middle school) considers to be a clause as opposed to what theories of syntax consider to be a clause. According to school grammar, a complex sentence consists of two or more clauses, where a clause is generally understood to contain at least a subject and a predicate, e.g. 

Depending on the situation, this sentence can be true or false. But regardless of whether it is true or false, it presupposes that Tom has a sister. Presupposition is identified using negation. If a sentence is negated, the presupposition nevertheless remains intact, e.g. 

This is simply a trait of the plural demonstrative pronouns (these and those) -- there is no good explanation why plural demonstrative pronouns behave differently than definite personal pronouns; they simply do. Note that the plural demonstrative pronouns also behave differently than the singular demonstrative pronouns in this regard, e.g. 

Proform substitution is a test for constituent structure that is widely employed in beginning textbooks for syntax. If a definite proform can be substituted in for a string of words, then that string is likely a constituent. In this case then, his can be substituted in for Ali's, indicating that Ali's is a constituent. Similarly, his can be substituted in for Ali's father's, indicating that Ali's father's is also a constituent. Consider these facts with respect to the first DP tree in the question. That tree shows Ali's as a non-constituent, hence my conclusion is that the first tree in the question is also incorrect. The traditional NP-analysis, which would show Ali's as a constituent, is more defensible. Concerning the second tree in the question, I think the constituent structure shown would be accurate for the NP Ali's father's mother, because the combination Ali's father's would receive the status of a constituent. But the labeling in that tree is all off.