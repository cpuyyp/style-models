One of the decisions left to take is whether to use software RAID1 or hardware RAID1 + BBU. Software RAID is the solution I'm very familiar with (I'm managing a number of servers since 15 years and I know how the tools work). I never had a serious problem with it (mainly only the HDD fail). These are the reasons why I prefer software RAID. What I dislike about hardware RAID is the incompatibility between controller vendors and the lack of experience I'm having with them: different configuration options, different monitoring method, different utility programs - not a good feeling for creating a cluster system. I know that, when using a BBU, hardware RAID can both be fast and reliable (write through cache). However, since all data will be stored in a highly redundant manner in the cluster, my idea is to use software RAID1 and disable barriers in the file system to increase write performance. I expect that this will lead to similar performance like hardware RAID1. Of course, I risk data loss due to the volatile write cache, however IMHO that should be handled by the clustering mechanisms anyway (the whole machine should be able to restore data from the other nodes after failure). I'm not having concerns about the CPU resources needed by a software RAID implementation. Is my assumption correct or am I missing some important detail that would help me making the right choice? 

I've setup an NFSv4 server (Debian) and client (Ubuntu/Mythbuntu). It seems that uid username mapping is working nicely out-of-the-box (I get correct usernames on ls -l if the usernames match between the two boxes, even if the uids don't), but ACLs are not working. I've installed nfs4-acl-tools and I can read the ACLs correctly on the client, but they don't get applied. What needs to be done for ACLs to work? To clarify; username mapping works for regular permissions. ACLs are applied using uid/gid (I can even set ACLs by uid and they work). 

You might want to use bootchart to see what are the boot time hotspots. There's also readahead: $URL$ , which I haven't tried. 

Normally, you set up an LDAP server such as OpenLDAP and then tell boxes to authenticate against it. Alternatives are Kerberos and Samba. 

The other host is set up as 172.16.1.1/24 with HWaddr 02:00:0a:01:01:c7. This results in a theoretically working bonding interface: 

At the beginning of the website launch I expect that a single EC2 instance will handle the load easily, with perhaps peaks of two or three EC2 instances during holidays. Amazon's own Docker hosting solution (with load balancer and auto scaling) looked very promising, but being an Amazon newbie I haven't been able to get this thing working, also because the configuration looks very complex and you have to get the pieces together yourself. I'm pretty familiar with other types of clusters (mostly Proxmox-based) but still the Amazon Web Services looks complex (at least the configuration console). Is there any alternative solution that makes this easy? I spent a lot of time looking into Docker Cloud/Tutum (seems expensive), Docker Swarm, Rancher, and others (most of them "beta"), but still am confused which way to go. How would you deploy such an application? 

I'm pretty sure they don't. I use Apache httpd with mod_proxy and mod_cache and it works pretty well. If you take the time to send proper HTTP response headers, it's better... 

No problem at all proxying Tomcat via an existing Apache httpd instance. You might want to look into mod_proxy- it's easier than mod_jk. You might also want to look at this article: $URL$ 

Well, basically, you have two paths; use Windows or use Linux. I'm biased, but I'd really go and use Linux. Running Windows is most certainly going to be more expensive (Windows licenses have a cost), and, in my opinion, although PHP runs on Windows, it's more at home at Linux. If you use Linux, things are much simpler. Choose a distro and use the distro's packages for PHP, Apache, MySQL, etc. They will work out of the box, receive security upgrades easily, etc. If you want to use Windows, things are a bit more difficult. For starters, you might want to consider using IIS, which is Windows' official web server. I don't really know if IIS+PHP is really "better" than Apache+PHP on Windows, but I suspect so- and Apache's pretty UNIXy, so probably IIS "fits better". On Windows, there's no "official" packaging of PHP, MySQL et al. as there's on Linux distros, so there's choices to be made, basically using the software from the original source or use a distribution such as XAMPP. You have to balance effort (probably distributions require less work), timely updates (which are vital for Internet publicly facing services- probably original packages have advantages here) and quality (can't really tell). 

When two of those devices are in the same network it is obvious that the address causes a collision and should not be used. The DHCP address should be used. The 10.1.1.146 is intended for 1-to-1 connections. Unfortunately, when doing simple things like or when trying to reach the Internet, then the Kernel chooses to use the ...146 IP address as source in such a situation. AFAIK that is because it prefers the /24 network since it's smaller. Question: Can I somehow give the DHCP subnet precedence (perhaps via some command usage), even if there is another subnet that qualifies? 

I guess creating a queue for parent won't work as packets won't be marked on that level (right?), since all packets are VLAN-tagged. I'd like to have the same QoS as I had with FireQOS but what's really important is the VoIP part. So, how should I configure the Queue Tree? 

Have them fix it. Any solution other than that will cause problems down the road. Tell them its bad for SEO. If you cannot, you might want to look at something like $URL$ 

Caching. If you have "public" pages that are heavily used and hit the database, frontend caching is very simple to setup and will probably do wonders (think httpd + mod_cache) Optimize your application. I'm going to assume that you already know your bottleneck is Postgres. Run pgFouine on a day's worth of logs, find the top queries, index them or try to avoid them. Improve hardware on the Postgres server. If you are CPU bound (rare case), maybe a better CPU or more cores will help. You can always add more RAM so that more stuff is cached (and make sure you use it! Postgres default memory settings are very conservative- this might be your problem, check that you are using all your RAM). Also, a big enough RAID10 array will do wonders... 

I'm trying to get Linux bonding working over a VPN (GRE-TAP). The funny thing is, that it only works when I have running on both hosts, but more on that later... There are two machines, called and . They are connected together using a simple switch via eth1. 

I'm about to set up a Linux cluster of 5 physical server nodes (more nodes to be added later, probably). 

Update I get a working setup when I set the bridge interface into promiscous mode (). I'm not quite sure if that is normally needed. OTOH I don't think it has any downsides... BTW, there a similar RedHat bug report exists, but setting down/up doesn't help in my case.. 

I have a relatively simple web application that consists of two Docker containers working together. One container hosts the web server and the other one hosts a image rendering software. On my development machine it works flawlessly using . For production this application will make use of Amazon SES and some database like Amazon DynamoDB. In that combination the containers itself don't need any persistent storage. Because of those services, my first thought is obviously to use Amazon EC2 to host the application. My primary goals now are: 

Keeping your system patched should be easy. I suggest you use "stable" distros with long term support (meaning you get no updates to software beyond security patches and major bug fixes). These mean that their package manager 'update all' operation will probably be smooth and easy. You should also subscribe to the distros security mailing list and evaluate all messages concerning software you have installed. You should also audit all means of entrance to the box, make sure that there are no unnecessary net-accessible apps running and that necessary net-accessible apps are properly secured (i.e. use encryption as necessary and have strong authentication). Watching log files is somewhat overrated, but you might find packages that simplify this. E.g., Redhat Enterprise (and CentOS) install by default logwatch which sends you a daily report by email of your log files. Also, for systems that need to provide services 24/7, you should set up monitoring and, if needed, fail-over measures. Also, backups!