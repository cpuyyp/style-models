1=1 - remove it all, it a DUMMY condition and do nothing in Your query If You change STRAIGHT_JOIN to the normal JOIN, MySQL could or couldn't change reading order. If it decide change it - Your expected index start work. Choose or not - depends from many unclean parameters, 

not sure about count(distinct Rate) > 1, think distinct is not necessary (but this depends from Your logic and not change query) 

Visual Studio + SSDT + SSIS = full power ETL Tool, with only one real drawback - it only work under windows It need Windows + SQL Server for run packages, but work with mostly all sources. For transfer/migrate data - a lot of products on the market. Commercial, Open Sources, Community/Express and etc For migrate code - all not so good. Even if software promise "convert triggers, procedures and functions without problems", in fact - only simple, most code migration - manual. 

it is fasted way for restore, it could take same time as mysqldump for backup, but would be dramatically faster for restore. You can combine methods. Disadvantages of percona backup or file level cold backup - You are restore all databases at once and will need operate all time with 120+Gb of data. If You will make periodical dumps by database - You can restore them separately on any server. 

in first - You still can use mysqldump, and it will be fasted method without additional steps and/or packages if size of single dump is big - make dump by single database, table 

have a lot of integration projects not always source and target system same. Most resent in my case - MySQL to Redshift, DB2 It also allow realise "event-based" integration - fire Sub-Job when specific data changed Have more than 1 subscriber with the same producer 

It return You information about which indexes from present MySQL will use when run query Than You will check cardinality of the indexes 

General answer - No! First of all - mysqldump it is not a simple sql command, this is complicated logic behind it, such as: - show definition of all objects - tables, routings, foreign keys - manage export import in proper order, like disable constraints and etc Second (in addition to first) - mysql console (and SQL as language) do not support database level commands, like select all - You would need do this for each tables, objects, routines. Federated engine (FederatedX) also not resolve "problem" 

with time difference will need adjust server settings but if we check size on disk (tables include only 1 column, PK) 

not really big choice - it will be normal B-Tree index with autocomplete You can not use FullText because it work only with full words at the same time queries like: 

for ERROR 1146 (42S02): Table 'xx-xxx-xxx-xxx' doesn't exist - not all sql dump include create table instructions, best choice check the text of script, again if file huge - use 

If computer Similar - You can install MySQL from installation package and then copy only data folder (name and location depend from OS) If computer different (like Windows -> Linux) - You can install MySQL on target computer, on Source computer dump all data using mysqldump, on Target computer make restore from dump using command line client 

I do not know, what part of table You have in MyISAM, but it always better - finish all works with database as fast as possible, than continue other steps GZIP is may be exactly slowest part of Your script and it really not optimised. It use single core not depending how many of them You have. 

You can use any other filter for warrant single result from subquery Also - wrong loop, in this case (for each UserID) define cursor as 

First of all - think Your server not crash, but just stoped (rebooted) by VPS provider - they normally have script which restart server in case of loading. in Your case You can check - all of Yours tables InnoDB (if not - convert) and add --single-transaction option 

You never can be sure - what information fetched? And what if it official (government required) report? and You fetch wrong, not current address? Or You send package from Your internet store to wrong (outdate) address? 

About Variant B - I personally look for tools similar pt-table-sync as for excellent but occasional tools in Your toolbox, and not for permanent solution. I can not explain drawbacks, it just personal vision Variant C: Tools like - SymmetricDS Variant D: I personally actively work now with CDC variants, when You parse bin-logs and ship them to subscriber over message queue. I have tested under loading at least 2 working solution (really it more for now): 

I just top-up prevision answer. The biggest difference described on Flyway web-site on central place: 

indexes - must be, all comparable columns must be same type, such as events. bigint(20) == events_tags. bigint(20) 

in You case last part work with full table, which with 166 column and 400k rows could be really big and in Mb Plus - other processes if present can take table resources and make possible table locks. This construction could be used for some cumulative reports, but still better exclude all not necessary columns 

not enough to change datadir in my.ini file, need also point windows to run service with proper my.ini file 

Size of bin-log will not affect to performance if You will use filter to replicate only selected tables, and even if You will replicate all - it also normally not a problem. For compare we replicate 20Gb (and more) between datacenter with real-time loading (telecom) Keep version of Source and Target Server same - it good practice even if You choose Variant B - it avoid many other troubles 

put the computer name\service name, like TOOLS-PC\SQLEXPRESS2012 if instance on computer only one - can put only computer name or just - (local) name of server instance You can check in service list or by run SQL Configuration Manager 

$URL$ if strict mode not enabled, You can use "any" path (any enabled by windows) for output file, but not use user folders or root folder for avoid permissions collisions. create something like c:/exchange 

it not use INDEX because no any WHERE condition in SELECT statement, so from SQL server point of view - it always make FULL-SCAN of table, so no reason for use index. possible make changes 

Query return aprx 60 columns - did You really need all of them for statistics? Reduce number of column for strictly and absolutely necessary. Even if You will need drill down for some information - You always can do this by second request by ID (primary key). From Your structure not seen no one indexes other than Primary Key - each column from JOIN part must have index This query time grow and will be grow unless You not implement incremental strategy - any conditions in WHERE part would reduce total number of returned rows (not forget about indexes again). As variant - calculate and store daily or monthly statistics and store result, so new iteration could work with only fresh information.