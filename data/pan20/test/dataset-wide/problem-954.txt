Yes. Earthquakes are Dangerous No. Because big earthquakes happen so infrequently (~300 years in Seattle's case) that one shouldn't really worry about it. Engineers design buildings to withstand large earthquakes. Of course there is the possibility of failure, but generally the US is very good about their infrastructure. Seattle is not like Oklahoma, where they are completely unprepared for earthquakes. I lived in Los Angeles in 18 years, and while my time there was during the Northridge 1994 earthquake, most of the damage was to the buildings (which were insured) and not to the people. Still there is some risk, but I think the risk of driving in Seattle is probably riskier than living in Seattle for 50 years of earthquakes. 

The electromagnetic field is modeled as a dipole, but in actuality has ~10% component that is off the dipole. The wandering of this dipole (and by consequence, off dipole) magnetic field causes a shear in opposite directions at the inner and outer core. This inner and outer core shear scales similarly at the edges (at the inner core outer boundary and at the core mantle boundary) but in opposite directions (so they are of the same magnitude). This torque, causes the flow of the inner core and the outer core to spin in opposite directions, as shown by the last figure. 

Clive D. Rodgers, Inverse methods for atmospheric sounding, Theory and Practice. ©2000 World Scientific Publishing Co., London, UK. 

To be sure about why it appears to be centered around -0.2°C, you'd have to go to the original source of the graph. However, as an educated guess, I would imagine that they are showing the anomaly compared to some recent period, such as 1961-1990 or 1971-2000, as is commonly done. From your Wikimedia Commons link, this figure appears to be original work by a Wikipedia author. The link does not appear to state this, so this is rather sloppy and perhaps illustrates why it's preferable to obtain information from peer reviewed sources. If the temperature increase stays at +0.4°C, it is not catastrophic. However, considering the greenhouse gases (GHG) we have already pumped into the atmosphere, temperature increase will not stay at +0.4°C. With currently projected increases in GHG emissions, the global average temperature increase may range from anything between +1.5°C and +6.5°C, with some regions exhibiting far larger temperature increases than that. At such scales, the entire climate system might enter a different state, which means many regions will have climates very unlike before. And then I'm not even mentioning the problems associated with sea level rise yet. 

You ask what we should change to better predict earthquakes, and I'd think almost everything. As geophysicists we might know the what of earthquake detonation, but are relatively blind at the how. There is a strong group of scientists who feel that single event prediction is NOT a realistic goal, and may perhaps be impossible. Some good guesses as to why this is still a problem: 

To add to a already complete answer by kwinkunks, perhaps one of the most surprising things about plate tectonics to geophysicists was the fact that in some cases extension in the lithosphere (rifting) is a direct cause of subduction zones. When the idea of subduction zones was finally formalized (~1969) most geophysicists expected all subduction zones to be accompanied by an area of compression, but what is later found is that rifting centers opened up nearby. What studies further found is that the motion of the upper plate and the viscous coupling of the subducting plate caused the trench to roll backwards in the opposite direction of subudciton! This trench rollback opens ups an oceanic ridge, what we call back-arc spreading centers! So the whole point of this small digression is that plate tectonics is a very complex issue, and not all the forces are well understood. Models show that the mantle is convecting near plate margins at orders of magnitude greater than plate motion. Inside the mantle wedge of subduction zones we see rapid and complex mantle flows that react to plate geometry in unpredictable ways. Melting (lava/magama) certainly plays a apart in our plate motions, but we really have no idea how much it does. To be honest, if we did, a lot of geophysicists would be looking to do something else. 

Does that mean we can say nothing? Well, it's not quite so pessimistic. There are many attempts to answer this question. The figure below shows estimates for the regional distribution of the shortwave and longwave cloud radiative effect, again from AR5 WG1: 

A controlling factor is a factor that acts as a cause for others. A reactive factor is rather the consequence: that which reacts to the controlling factor. For example, the Sun is certainly a controlling factor. Nothing we do influences what happens on the Sun, but what happens on the Sun certainly influences us very strongly. Fortunately, its total power output is quite stable so as far as climate is concerned, we don't need to worry about it too much as far as the next couple of million years are concerned (although eventually, in hundreds of millions to billions of years, increasing power from the Sun will render the Earth uninhabitable). CO₂ is both a controlling and a reactive factor. We add CO₂ to the atmosphere, so the CO₂ concentration reacts to our emissions (reactive factor). But CO₂ levels are also a cause. If you were to make a simple climate model, you would put in the CO₂ concentration as a fixed value or perhaps an increasing value as a function of time: in such a model, it would be a boundary condition. If you instead made a model of a single factory or car and modelled how changes in the fuel cycle would affect CO₂ emissions, then those would instead be a reactive factor. It all depends on the context. For H₂O, however, it makes no sense to put in a fixed version as a boundary condition. Its concentration varies rapidly on short timescales: today may be dry and tomorrow a wet airmass may get in. Of course, physically, H₂O causes even more of the greenhouse effect than CO₂, but the increase of H₂O is not what is causing anthropogenic climate change; if anything, an increase in H₂O is one of the effects. So, think of it as a system: you add CO₂ (cause), many things happen, including an increase in H₂O in some places (effect). The cause is the controlling factor. The effect is the reactive factor, even though physically, both CO₂ and H₂O are strongly contributing greenhouse gases. 

from $URL$ You see, these changes in the internal portions in the Earth's magnetic field cause a shear. The Earth's magnetic poles are not stationary, they wander. There is clear evidence of this from the paleomagnetic record. This is caused by the viscous convection of the iron nickel outer core. Their numerical models showed that the electromagnetic torque : from P.W. Livermore et al. 2013 On the outer core has a westward direction, while the inner core's torque scales accordingly in the opposite direction. 

will give you a scatter plot with Lat being the x, Lon being the y, and rain being the z. Also note that because your vectors are different legnths, you cannot plot them. You must have vectors of the same lenght to plot (each point must correspond to another point on the axis). Other types of plots will require you to interpolate between the data points. If you want to plot it like in the comment below: pcolor is your best choice. Say your Lat/Lon vectors have n points. Then to make an nxn grid, you can use the command 

In atmospheric remote sensing of methane, a dominant source of error is the uncertainty in spectroscopic parameters. For example, in the retrieval algorithms described by Batchelor et al. (2009), the CH₄ error due to line intensities is estimated at 16.5% (dominating the total error of 16.8%), versus 10.5% for O₃, 3.8% for N₂O, and only 1.6% for HCl. The article documenting the 2012 HITRAN spectroscopic database update (Rothman et al., 2013) states: 

Most likely, climatology means there was no retrieval at all. Bayesian retrievals combine information from an a priori with information from measurements. When the retrieval fails for whatever reason, or the measurement contains insufficient information, instead of reporting no measurement at all, they copy over information from the a priori and use the flag to indicate that the reported "measurement" is in fact simply the climatology. For details, see optimal estimation, in particular the book by Rodgers: Clive D. Rodgers (2000). Inverse Methods for Atmospheric Sounding: Theory and Practice. World Scientific. 

This is a good question, and the basic answer is earthquake seismology. To answer this question, lets accept a fact: Waves propagate through the least time pathway, and not the least distance path. This property of physics is known as Fermat's principle. When an Earthquake occurs, energy propagates similar to a ripple of water, and spreads. Each point on this ripple can be modeled as a ray, each following its least time path through the Earth. As these rays propagate, they reflect and refract off heterogeneities inside the earth. These reflections are important: we can detect these reflections using surface seismometers. Using multiple seismometers, and the location of the source (earthquake), we can calculate the "lag" between when the closest seismometer detects the earthquake and the others(further from the source). This lag time can be used as a proxy for the depth of the reflected wave's heterogeneity. Using how deep this reflection originates, we can then figure out the least time path of the wave in terms of actual distance. Velocity is just distance over time, even for seismic waves. So since we have the distance traveled by the wave, and the time the wave took to get to the receiver, we can complete the equation (distance/time). In actual practice, this process is far more complex, as you have to look for many different waveforms: The picture above illustrates an earthquake sending waveforms through the earth. The SKS wave is a shear wave that refracts into the liquid outer core, turns into a compressional wave (since shear waves cannot propagate through liquid), and then is converted again to a shear wave as it enters the mantle and goes to the surface. A trained seismologist looks at the readings from seismometers to tease out specific waves, which we know the properties of, to determine properties of the planet's interior. The speeds of these waves are highly dependent on the assumed densities and other rock properties of the crust, mantle, and core. 

Provide a monthly average in every grid cell, and describe how many measurements were used for each cell. There is no fixed rule for the minimum number of days for reporting a monthly average. A reasonable threshold will depend on the geophysical quantity of interest y, in particular on how much y varies on short and long timescales. If y varies from month to month, but does not vary a lot from day to day (example: sea surface temperature), you can probably get away with reporting a monthly average even when only one or two days have cloud-free measurements. On the other hand, if you're measuring precipitation, your signal will be relatively noisy even if you choose a threshold of 20 days. And any threshold is going to introduce a bias, because a lack of observation is typically due to clouds. Although you will get a bias in any case. In other words, if you only have one day per month you see a gap and in the clouds to observe whatever you want to know, it is a pity to throw it away. Therefore, personally I choose a minimum of 1 observation. Critically important though: when releasing the product, add a field that tells the user how many days worth of measurements make up the monthly average in any given cell! Like that, the user can decide which ones to use and which ones not to use. More broadly speaking: producers should document everything they do. For your specific examples: search for the ATBD and/or scientific papers describing the product. Hopefully, there is a field in the level-3 product that describes the number of measurements per cell. If the answer is not in there, write to the producers and tell them to redo the product... You will need this information to accurately calculate a yearly average! 

This is a very good question. There a few main heat sources: Heat(and work) left over from the formation of the earth, work (potential energy) generated by dense iron sinking into the center of the earth and forming a core, and radioactive energy. Since the Thermal diffusivity of the materials that make up the earth is low, heat transport is very slow and thus the planet retains a significant amount of its heat. Another reason why the earth is so hot, is that mantle convection is a very inefficient way to transport said heat, and thus the earth loses energy very slowly. We know how hot the interior of the earth might be because of laboratory experiments, and also volcanic eruptions. How much energy(temperature) does it take to melt gabbro (mantle rock)? Interestingly enough, the earth is cooling. And the inner core / outer core boundary is a freezing front (meaning turning from a liquid outer core to a solid inner core). Though, it is a very hot freezing front. 

The Swedish meteorological agency SMHI on its website (NB: freezes Firefox on Linux for me, likely due to plugins, works in Chrome) indicates how certain the forecast for a particular day is. Looking at today (Wednesdays) forecast for the coming week for Kvikkjokk, they estimate that the forecast for Friday is ganska säker (somewhat certain), for Saturday is osäker (uncertain), and for Sunday is säker (certain); they estimate that the forecast for Sunday is more certain than the forecast for either Friday or Saturday. I suppose that what they do is look at the forecast ensemble and estimate certainty based on agreement between different runs from the ensemble. But as the state of the atmosphere on Sunday should depend on the state on Friday and Saturday, is there any way how it can make sense that the forecast for Sunday is more certain than the one for Friday or Saturday?