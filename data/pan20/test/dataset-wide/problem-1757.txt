Howerver, it is likely to log both to syslog and a logfile. If so, you can keep on playing with configuration file to achieve your goal... Hope this helps 

"OpenSSH 7.0 and greater similarly disable the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use." Source : $URL$ 

Depending on your monitoring system, you should be able to write a script that parses the output of ifconfig and displays an alert if the number of errors of dropped packets is too high. 

If i understand your question correctly, you want to receive all mail for test@example.com, but none of the mails sent to joe@example.com 

We have to find a string, let's say "foobar" among several webapps. However, some webapps contain zipped files, eg log4j.jar. Therefore, grep -IR "foobar" /pathto/tomcatroot/ won't work, because of compressed files. unzip -c /pathto/tomcatroot/libdir/log4j.jar |grep foobar can solve the problem, but only for one file Is there a way to achieve this for a whole directory? 

Background: I have a Windows 7 workstation and use PuTTY for SSH connectivity to Linux servers with session logging enabled. I previously used the option but that has the benefit of no escape characters but the drawback of making commands I've typed unsearchable if I used to autocomplete or because I corrected a typo (or 3) as I was typing the command. NOTE: I have installed for additional command-line tool support (i.e. , , etc.). Recently, I had to go back and find some commands to set the record straight with a coworker about something that happened on a server and the inability to see the final commands I issued is problematic and makes it more difficult to search the logs as well as being much more difficult to readily demonstrate what actually happened for my coworker. Example #1: This is an actual PuTTY log file of the 'pwd' command initially misspelled as 'pdw' and then corrected to 'pwd' with enabled when viewed with or in . NOTE: There is no difference between and in this case because there are no codes and only printable output was captured. 

It is not possible (even for the AWS support afaik) to use Elastic IPs which are not for VPCs with VPC instances. So you're stuck here - the only possible way to do things like this is not to rely on a fixed IP address (you will get the same problem if you try to use ELBs or more than one instance). Your customers should NOT point to an IP address but they should use CNAME records with the given subdomain you provide for them. With that architecture you're able to migrate the whole domain with all subdomains to a new IP address if you need to and with the CNAME records nothing changes on the customer side (as the subdomain they're pointing to has the new IP address). The only solution for you now would be to send out an email to all customers which use the IP in their DNS records to change it to a CNAME and after migrating all customers to CNAME you can switch to the new Elastic IP and change your own DNS records. UPDATE: As pointed out below it is now possible to move an Elastic IP from "classic" to "VPC" - you will find the details here: $URL$ 

Your issue was solved by rebooting the box. Therefore, we won't be able to determine the root cause of the problem. 

You can try to type mount -a just after system has booted. If this workaround works, you can set up a script whose content is just something like "sleep 60 && mount -a" and make it executed at boot time (via cron, systemctl or any other mean). It's really dirty, the good solution would be to investigate why some fs does not mount correctly. 

It looks like your dhcp server is broken. According to your question, the dhcp server does not provides the correct gateway to your client. Try setting a static IP adress by ediiting /etc/network/interfaces (you'll find correct syntax on the internet). 

I have no idea how it works with storcli64, but I just had to replace a raid1 failed drive (slot 0) with megacli64 : 

Debian Lenny is very old and I assume that's the reason why Oregon doesn't provide the kernels you need to run your image. The only thing I can think of is to update your PV-Grub and create new AMIs. Maybe this helps: $URL$ If you don't find matching AKI in the other region there is no way afaik. 

From the connection point of view "something" needs to answer your requests (GET, POST, PUT, everything). First of all you have a TCP connection and "something" needs to make sure it is understanding layer 7 and making sense out of the bytes the client is sending. Only at this point it is possible to handle GET requests differently than POST requests or one URL than another URL. So in the end you need a service which is capable of understanding and routing HTTP. The following services are capable of doing this: CloudFront ELB/ALB API Gateway (limitation comes later) API Gateway uses CloudFront internally (without giving you the chance to actually configure anything on the CloudFront level) - that means there is no way to run CloudFront and API Gateway side-by-side as in the end this would mean you run CloudFront with CloudFront side-by-side. CloudFront gives you the chance to select different origins based on patterns - but you can only select S3 or ELB/ALBs as an origin - not Lambda functions (besides the Lambda@Edge functionality). ALB/ELB can only use EC2 instances as a backend - no Lambda or S3 here. The only ways I can think of which might do what you want to do are these: 

As you can see, if you were to search for you would get no matching results. I have used iTerm on Mac and know that it can automatically replay logs and it just seems like there should be a way to see the end result of what I eventually used. Enter the PuTTY logging option. Ok, so here is the deal, with option enabled, the log file gets inundated with codes for terminal color and non-printable characters like . Example #2: This is an actual PuTTY log file of the 'pwd' command initially misspelled as 'pdw' and then corrected to 'pwd' with enabled when viewed in . 

Ok, so I'm almost to the actual problem. If I use with option enabled, it looks perfect. It is exactly what I want to see and work with. Example #3: This is an actual PuTTY log file of the 'pwd' command initially misspelled as 'pdw' and then corrected to 'pwd' with enabled when viewed using . NOTE: This is the exact same log file as above. This is also the exact visible output and format that I want to be able to search. 

Which user is running mysqld? It is likely that the user who runs mysqld, usually mysql, does not have write access on /home/site/public_html/mysql-logs/ Therefore, it cannot write log files. Try creating a group for both /home/site/public_html owner and mysql, then chmod 0775 directories and chown directories 

I'm also just shooting blindly here: Maybe ovh has enabled some kind of "network protection" to prevent servers being compromised by brute force attacks. To figure out, try to run run sshd on a different port eg 6022 and see if the problem persists. 

It looks like a dns issue. To figure out, try in cmd.exe : ping myhostname It should not work. Then you have two option : 

You have to recompile ioncube or reinstall it correctly. To have a hint about whet went wrong or what dependency is missing, just type ldd /usr/lib64/php/modules/ioncube_loader_lin_5.6.so 

There is another solution to your problem. Instead of making wget convert those links to the new domain name, you can make your webserver rewrite links on the fly. with apache, you can use mod_sed to rewrite links. eg : 

So my call would be the last option - but that means you need to point the clients/browsers to a separate subdomain for all static assets (or for all POST requests). It sounds like you want to have a look at technologies like AngularJS or React to build a truly API-driven application in the browser. With this approach you're running a real API which is handling all the "dynamic" requests with an API Gateway and delivering the application itself from S3 as an static asset. Maybe looking at those might help you to find your way - even if you don't use them, the architectural pattern on how to build things like this is what you're asking for imho. 

I assume we're talking about dynamic IP addresses so you can't limit their IPs inside of the security group as they're always changing. The only way I can think of right now is blocking every external IP address in the security group of the Redshift cluster and setting up a "SSH jump host" inside of the same VPC. If you now enable this host to connect to the Redshift cluster users can access the Redshift via a SSH tunnel. As you can roll out different SSH private keys to each machine you can limit and allow the people independent from each to other to access the Redshift cluster. Several SQL tools are available for DB Connection through SSH tunnels so maybe that is something your users already use and you're able to limit the access to certain users. 

apt-get install libssl-dev You must install openssl development files in order to compile php $URL$ Also, once you compile php successfully, don't forget to change apache module to the right directory for php.so There must be a line looking line LoadModule php7_module modules/libphp7.so in apache configuration 

It seems that everything is okay but your telnet command tries to open port 23 and not 10000. ( "connecting to ...could not open connection to the host, on port 23: connect failed") try 

You can use tcptraceroute to guess where port 443 is blocked. tcptraceroute 52.56.123.246 443 should give you the ip adress of the firewall which is blocking requests to port 443 

027 is better from security perspective. Even better is to use 077 for root. With 027 mask, when root creates a file, it can be read and executed by users who belongs to the group of the file. With 077 mask, only root can read write and execute files. Why is 077 better? 

Is there a reason why you don't use EBS snapshots? You can use those to save the whole EBS device (incremental) with a simple API call and the snapshots are saved within S3. If you need an old version back just create a volume from this snapshot and connect it to your instance instead of the broken EBS. 

Security groups in AWS are way more comfortable AND even better from the security point of view (as the traffic you want to block doesn't reach your instance at all). So turn off the iptables and configure the security groups as limited as you can. 

The check is a "<=" (I assume you overlooked the -infinity instead of +infinity) - so the value you put in is the upper bound in that case. ("Scale in as soon as CPUUtilization is between your value and -infinity") Just to mention that here: you could also use the "target tracking policy" - with this you can say that auto-scaling should scale in a way that your target value is matched - that includes scale-out AND scale-in out-of-the-box. Hope this helps! 

"what happens when server #1 is under such heavy load that he cannot handle the requests properly" Usually clients get "connection timed out" error message. Don't worry about the load balancer : with modern hardware, they can handle thousands of requests, especially if you use cache for static content. Most of the time, the bottlenecks are : 

sender_canonical_maps (default: empty) Optional address mapping lookup tables for envelope and header sender addresses. The table format and lookups are documented in canonical(5). Example: you want to rewrite the SENDER address "user@ugly.domain" to "user@pretty.domain", while still being able to send mail to the RECIPIENT address "user@ugly.domain". Note: $sender_canonical_maps is processed before $canonical_maps. Example: sender_canonical_maps = hash:/etc/postfix/sender_canonical Source : $URL$ 

According to your firewall logs, you've forgotten to allow dns queries. Allow port 53 udp and your setup will work better ! 

split-brain occurs when two nodes of a cluster are disconnected. Each node thinks the other one is not working.