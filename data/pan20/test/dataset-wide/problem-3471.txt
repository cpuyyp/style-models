To add another book, Shimer: Labor Markets and Business Cycles deals with the extent of which the RBC model and the Mortensen-Pissarides model can deal with the Shimer puzzle. He goes through many extensions and concludes that rigid wages are an exciting future research avenue. 

How is the minimum wage determined? Yes and no. It is a political choice, but with economic reasoning. In the end, that's the case for every economic rule, maybe with the exception of monetary rules, as we try to preserve independence for central bankers (but it isn't completely true for the US FEDs). When setting the new minimum wage, you compare minimum wages with other states, and with your own (earlier) state, to get a feeling what's possible and what is not. In the end, the minimum wage is a trade-off between raising earnings for some parts of the population, and decreasing employment chance for others. How does the minimum wage affect employment? The degree of trade-off (if you will, the transition rate between the two) is unclear. In the data, we tend to see no employment effect of minimum wages, but an increase in earnings (Dube et al (2010): (Ungated version). In fact, we see a large number of estimates around 0: 

Let's say we want to approximate a second order differential: $$ \partial_{aa} V(a)$$ Now, say we have a grid of values for $A$, where the step size is $\Delta$, $i$ is the index on this A grid, and we define the short hand notation $$V_i \equiv V(A_i)$$ Achdou et al (page 13, equation 35) then compute the approximation to the second order differential on the grid at a point $i$ as $$\partial_{AA} v_i = \frac{v_{i+1} - 2v_i + v_{i-1}}{\Delta^2}$$ They don't explain why, and I can't follow here. Could someone provide me with the rough ansatz for why this is the case? 

I'll try to ask this question in the most simplistic environment possible, so lets think about a household that can consume and save. After some cumbersome work, we have solved for the optimal savings policy, and know that the change of assets, given his assets $a$, is given by $s(a) = \dot a(a)$. Denoting with $g(a, t)$ the density of households with assets at level $a$ given time $t$, I can compute the Kolmogorov Forward Equation for the distribution of assets: $$ \partial_t g(a, t) = - \partial_a [s(a,t)g(a,t)]$$ Where I followed the methodology of Achdou et al, Appendix A3: First compute the CDF $G(a, t)$, and then use that $g(a,t) = \partial_t G(a,t)$. However, now assume that we have birth and death of households. Most importantly, I don't want there to be a constant death rate, but let's make the death rate conditional on the asset level: $d(a,t)$ - and let's denote birth by $b(a,t)$. Intuitively, I'd think that the new KFE would be given by $$ \partial_t g(a, t) = - \partial_a [s(a,t)g(a,t)] - d(a,t)g(a,t) + b(a,t)$$ Intuition aside, how can I derive the correct KFE that allows for death rates and birth from the scratch? 

As Tobias said, wages in general only adjust in neighboring regions if people move between the regions until expected utility (of working in either region) equalizes. If there is a (fixed) cost of moving, people will not move until move until expected utility equalizes, but until the difference in expected utilities from moving in either region is equal to the utility cost of moving. In the extreme, if some people just cannot move due to regulations, cost of moving would be infinite, and you could sustain any difference in expected utility. Differences between wages and productivity Why do I keep saying expected utility instead of wages? That is because wages in general reflect more than just productivity. They also contain compensating differentials. That is, jobs have different non-monetary benefits (happiness you gain besides wages). To get a worker to do a job that is really not fun to do, you'd have to pay him more than for a different job. This is a general concept. You have to compensate workers for working longer hours, under more strict bosses, having a more frictional labor market or higher unemployment risk, lower non-salary compensations, higher costs of living. Some of these factors (and potentially many more) may speak towards German companies having to compensate their workers more - especially the cost of living. This does not deny potential productivity differences However, at the same time, you could really think that in a structured well-disciplined and efficient country such as Germany, companies get more out of their workers than elsewhere - meaning that their workers are more productive. Since it's not easy to measure productivity and it's causes at the firm level, I'm not sure whether this question has been addressed empirically. 

Kaplan and Menzio's shopping time model is a search&matching unemployment model where we, for a steady state equilibrium, need to determine to variables: 

tl;dr: No matter how you look at it, ticket scalpers are freeloading off the producers and reaping producer surplus. Side comment: Just because something is arbitrage, doesnt mean it is good for the economy. Hence, declaring something as arbitrage is no ground what-so-ever for legalizing it 

This is easier when one has it formulated as $Q(TC)$. Here, we have $TC(Q)$. Intuitively, constant returns to scale (CRS) require that costs change one-for-one with quantity. However, this does not translate easily into an expression using $TC(Q)$ only. As @NickJ hinted at, it's easier to work with average costs. Using average costs In other words, CRS implies that average costs do not change. One way to look for that is to say that we have increasing (decreasing) returns to scale whenever $\frac{d \frac{TC(Q)}{Q}}{dQ}$ is negative (positive). Using total costs Let's start with the previous expression and see whether we can get a direct formula using $TC(Q)$. $$\frac{d \frac{TC(Q)}{Q}}{dQ} = (TC'(Q)Q-TC(Q))/Q^2 = 0 \\ TC'(Q)Q - TC(Q) = 0 \\ \frac{TC(Q)}{Q} = TC'(Q)$$ It's not as simple as I initially thought, but we get a meaningful expression: We have CRS whenever average costs are equal to the marginal change in total costs. 

For example, if the variable is income, this gives me average income of unemployed. Now, I would like to bootstrap the standard errors. I'm in Python. I 

In the context of the Diamonds-Mortensen-Pissarides models, there was a reasoning why a separation shock cannot fit business cycles. I think it was brought forward by Shimer, but I could be wrong. Could someone summarize why separation shocks don't fit the data (besides that it is a decrease of vacations and not an increase of separations that that we observe in crises), and perhaps even reference the original paper? 

Whenever you need to make someone indifferent between $x$ and $y$, it means that $$U(x) = U(y)$$ a: Denote amount he would pay by $z$. Paying $z$ to avoid the lottery gives him "certain utility" $u(100.000 - z)$. With Von Neumann-Morgenstern utility as given, we can denote the lottery utilities as $$prob*U(10.000 + x) + (1-Prob)*U(10.000-x)$$ (Understand why we can do that!) b Without uncertainty, he will have $u(w)$. Then, you need to compute the utility of the lottery as the function of $p, x,y$. That is, you need to solve $$U(100000) = prob*U(w+y) + (1-prob)*U(w-x)$$ Now, I'm not sure whether you were meant to say loses x or wins y, I suppose it is loses x or wins y. Anyhow, the value of the lottery is on the right hand side. Can it ever dominate the left hand side, given the additional information for $p$ that you were told in that subquestion? 

Well, the obvious mistake, to summarize what other answers have hinted at, is that you mistake correlation with causation. All you have is evidence for the former, but you claim the latter. As you're too far to rethink what you're doing, the following won't help you much right now, but it's alternative ways to go in the future. Natural Experiments The best way to go would be to have data on a (quasi-) natural experiment. The holy grail. Policy reforms (despite not really random either) are often taken as the second best. A diff-in-diff based on different education reforms in different local areas (with long-term data to evaluate the results) would be infinitely better than what you have (basically, because what you have is 0). Structural Regression Another way to go would be to build a theoretical (micro-) model about the connection between growth and education, based on (testable) assumptions. Test these assumptions, use the structure to run the regression. This is how these sort of things are often done in macroeconomics, where the lack of natural experiments has driven people to evaluate theories and connections differently (is this what @EnergyNumbers was hinting at in his comment?) 

Is there any paper that looks at the distribution of wages? I'd like to see how many workers are in each percentile of labor income. I know that you could compute it using CPS or US tax data, but I suppose somebody must have done that before.. 

Take some state variable $X(t)$, which follows the law of motion $$ \dot X(t) = f(t)X(t) $$ where $f(t)$ is a policy function, and determines the growth rate of $X(t)$. As a second shock, we have $\psi$, which is iid. The agent defaults whenever $$ g(X(t), \psi) \leq 0$$ Allow the agent to borrow some money that he will have to repay continuously. Let's compute the risk premium. The probability of default at $t+\epsilon$ is $$Prob(g(X(t+\epsilon), \psi) \leq 0) $$ As the lending has to be repaid continuously, the interest rate, given some risk-free interest rate $r^*$ and risk-neutral lenders, is given by $$ r^* = r \cdot \lim_{\epsilon\to 0} \left(1 - Prob(g(X(t+\epsilon), \psi) \leq 0)\right)$$ However, as the law of motion for $X(t)$ is continuous, in the limit, this becomes $$ r^* = r \cdot \left(1-Prob(g(X(t), \psi) \leq 0) \right)$$ This would mean that the agent's risk premium is independent of what he is doing: his policy $f(t)$ does not appear anymore. But since $f(t)$ affects the state $X(t)$ and the latter the default probability, I feel it should. What's my mistake here? References are fine. Most of continuous time finance references I know are much too deep for this rather simple question. 

The probability of observing a binary outcome is a function of two variables, . Both and are continuous, and I have no prior on the functional form of . I have shallow knowledge of kernel density estimation, but have only seen that done in the case of a univariate variable. How should I proceed here / is there some basic literature on this? 

I'm not going to answer the questions for you, but here is some food for thought, you should be able to conclude yourself. Ad 1 We know that in the long run equilibrium, there are zero profits: If they'd be positive, firms would enter, if they'd be negative, firms would leave. Can we conclude something about a firm's profits if it has average total costs smaller (larger) than the price? Ad 2 What induces change in the total amount of companies in such a branch? Is having zero-profits one of these reasons? 

It is designed to give reason for policy intervention, just as the old keynesian theory. However, the intuition and the mechanism are completely different than from the Old Keynesian Story, Cochrane has nice papers and even blog posts on the exact mechanism. It's very debated, not just because its policy implications, but also because price stickiness is again implemented as a "behavioral rule", breaking with the DSGE paradigm. If you want to know more, you should read up on the papers or ask a more precise question, otherwise this will head nowhere. 

One simple way would to allow the measure of goods (items) grow constantly over time. This would imply a new supply of a specific item $i$ of $S_i$. Let time be denoted by $t$. Compute, at any time, the actual demand for goods by players given the current prices $D_i(p_{i,t})$. Given initial $p_{i,t}$, 

Where do you see that this expansionary policy is itself causing interest rates to rise? If capital flow restrictions are damping inflationary pressures then the increased stock of money minus the decrease in buying power might cause an increase in imports. This increase in imports increases demand for foreign currency and decreases demand for domestic currency. This method of exchange subverts the capital flow restriction and the depreciation follows. 

Complex numbers and complex analysis do show up in Economic research. For example, many models imply some difference-equation in state variables such as capital, and solving these for stationary states can require complex analysis. However, as others already emphasized, complex analysis is mostly a byproduct of solving equations. I'm not familiar with any paper where complex analysis is at the heart of the model. 

I assume that this is about Macroeconomics in particular (you might want to add that tag). Often we don't need profits The interaction of firm profits is often irrelevant. It is well summarized under capital rents. Having firm profits does not add any insights to, say, Solow's growth model or the standard Neoclassical model. Operative profits vs Life time profits In some models, however, we do need profitability of firms. That typically comes from additional costs of running the firm. For example 

Now, when I compute the confidence bands, they're not centered around my actual estimator. Even worse, for one or two data points, the estimator is outside of the bands. I suppose a alternative way to do the bootstrapping would be to i. Draw from the annual table, each row with probability relative to . ii. Stop drawing once is equal to in original data? iii. Then do all the computations without using weights. I thought my way was better because (a) I use the exactly same estimator both times without changing weights, and (b) it is clearer: How would I do (ii) exactly? When I have room for left, do I only draw from rows with ? However, given that my confidence bands dont line up with my original estimation, I'm befuddled. What should I do? 

Kydland and Prescott's seminal paper on RBC theory uses a log-log specification on consumption-leisure preferences, arguing it is the only one that matches long-run constant share of working hours (one of the Kaldor facts). This is false. In fact, there's a whole class of additively-separable utility functions (King-Rebelo-Plosser, which were discovered (published) in the same decade) where income and substitution effects from labor income cancel and do not affect the working hours decision. Why it is relevant? Well, because their log-log specification gives them a huge Frisch elasticity, which is the only reason why they match labor. The capital stock (as opposed to investment) doesn't vary that much over the business cycle. It is then circular logic to get TFP shocks as Solow residuals from the data, feed them into a model where capital does not move that much, and observe that you get back $AF(K,L)$ (where $K$ is almost constant and $A$ comes from the data). Don't get me wrong, it is still a very important paper, as it is the basis of most modern macro. But people at that time were astonished how good RBC fits the data. Well, if you don't fit labor data, the rest isn't that surprising. 

Without the teacher, everyone knows that there is at least a red hat, but nobody knows that everyone knows - the fact is not common knowledge. With the introduction of the teacher, 

Mueller (2015, working paper) says that an HP filter with smoothing parameter 900,000 (for monthly data) corresponds to a smoothing parameter of 100,000 for quarterly data. How does one do this exact calculation? If I were to calculate the corresponding annual parameter, how would I proceed? 

I have an intermediate sector that operates using labor only. There is 1-1 pairs between firms and workers, with profits $$\pi = (Ap - w)\cdot 1$$ $p$ is the price, $A$ productivity, $w$ wage. Entrance to this market comes through a Mortensen Pissarides matching market, where vacancies and unemployed meet. Free entry and search cost $c$ implies $$ \frac{c}{q(\theta)} = \pi $$ where $q(\theta)$ is the matching rate given market tightness. There is matching on the final goods market happening as well, which is why there is a free-entry given search cost $k$ to becoming a final goods producer. The details are not important here, but the final goods producer operate without using labor. Let $e$ denote the measure of intermediate firms (since it's also the employment rate), and $s$ the measure of final firms. I'm wondering how prices are determined here. Wages are given by Nash bargaining, such that $$ w = \arg\max_{\tilde w} \beta \log (\tilde w - U) + (1 - \beta) \log(\pi(\tilde w))$$ for some outside-option value $U$. However, I wonder how the price of the intermediate good $p$ is determined, given that firms of both sides ($e$, $s$) have some market power. I'm happy to do additional assumptions if necessary, as long as the approach is in line with what has been done in the literature. To be frank, I don't care about details on this margin at all, I just need closure. 

We live in continuous time. Let there be some discount rate $D(t)$, which consists of a discount rate, and some death probability. $V(t)$ contains the flow value of, say, being alive. If you are alive, you get some flow value $A(t)$ every period. I have started at the following discrete-time equation $$ V(t) = \Delta_t A(t) + \beta(\Delta_t)V(t+\Delta_t)$$ where $\beta(\Delta_t)$ is related to the discrete-time analog of $D(t)$. I have derived, through some reordering of terms, and letting $\Delta_t \to 0$, the following equation: $$ V(t) = \frac{A(t)}{D(t)} + \frac{V'(t)}{D(t)}$$ Unfortunately, I cannot trust in the flow values $A(t)$ being continuously differentiable, and hence I don't think that $V(t)$ is continuously differentiable. I hence want to rephrase this equation in a way that does not require the derivative $V'(t)$. I was expecting the resulting $V(t)$ to have the shape of $$ V(t) = \int_{t}^\infty D(s) A(s) ds $$ That is, the discounted sum of future fundamental values. Basically, I'm looking for the continous-time equivalent of $$ V(t) = \Delta_t A(t) + \beta(t)A(t+\Delta_t) + \beta(t)\beta(t+\Delta_t) A(t+2\Delta_t) + \cdots$$ I tried to get there by integration: $$ \int_t^{\bar t} V'(s) ds = \int_t^{\bar t} D(s)V(s) - A(s) ds \\ V(\bar t) - V(t) = \int_t^{\bar t} D(s)V(s) - A(s) ds $$ However, this doesn't seem to help into getting it as a present discounted sum. What would be a correct approach here?