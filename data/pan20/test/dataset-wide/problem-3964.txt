$l^1$ has the Schur property (every weakly convergent sequence is norm convergent) and $L^1[0,1]$ does not, so the two spaces cannot be isomorphic. Is this folklore, or is it credited to someone? (Also I wonder whether the original proof of non-isomorphism was this one.) Edit: to clear up any confusion, I am asking about the nonexistence of an isomorphism, not the nonexistence of an isometric isomorphism (which is basic, as pointed out in the comments). Edit 2: the comment section seems to have degenerated into attempts to find alternate proofs of this fact (and debate over common misconceptions, such as whether a Banach space which is isomorphic to a dual space must itself be a dual space). That's great, although I still think the Schur property argument is the easiest one. However, this was not my question. Who first proved this? 

I'm not sure what you mean by "the canonical embedding is not multiplicative" if you're only taking $A^{**}$ as a Banach space. A natural way to give $A^{**}$ multiplicative structure is via the Arens product, which does make the canonical embedding multiplicative. So this shows that your prescription of importing a product into $A^{**}$ using $i$ and extending by continuity does work. The analogous statements for the adjoint operation are also easy. Since you're asking about the C*-algebra structure of $A^{**}$, are you aware that it is naturally $*$-isomorphic to the weak* completion of $A$ in its universal representation? This construction is called the universal enveloping von Neumann algebra and the statement of isometry is the Sherman-Takeda theorem. 

Previous answer: It follows from Robert Bryant's brilliant answer to this question that the answer is yes in an important special case, when $A = I_4$ (as pointed out in the comments, there is no essential loss of generality in assuming this) and $B$ is a rank $2$ projection. Namely, work in an orthonormal basis that diagonalizes $B$ and write $C = \left[\matrix{C_1&C_2\cr C_2^*&C_3}\right]$ where $C_1$, $C_2$, and $C_3$ are $2\times 2$ matrices and $C_1$ and $C_3$ are Hermitian. We seek a rank $2$ projection $P$ with the property that $PBP$ and $PCP$ are both scalar multiples of $P$; if so, then any orthonormal vectors $v$ and $w$ in the range of $P$ will be good for $A$, $B$, and $C$. If $C_3$ is a scalar multiple of $I_2$ then $P = I_4 - B$ is the desired projection. Otherwise, according to the answer cited above we can find $a \geq 0$ and a $2\times 2$ unitary $U$ such that $$C_1 + a(C_2U + U^*C_2^*) + a^2U^*C_3U$$ is a scalar multiple of $I_2$. A short computation then shows that $P = \frac{1}{1 + a^2}\left[\matrix{I_2& aU^*\cr aU&a^2I_2}\right]$ has the desired properties. 

A cardinal $\kappa$ is real-valued measurable if there is a probability measure on the $\sigma$-algebra of all subsets of $\kappa$ which is zero on singletons and additive on disjoint families of fewer than $\kappa$ subsets. What if I weaken this to: $\kappa$ is uncountable and there is a finitely additive probability measure which is zero on singletons and such that the union of any family of fewer than $\kappa$ null sets is null. Has this notion been studied? What is the consistency strength of the existence of a cardinal with this property? Edit: A paper has been written on the project which provoked this question. It has been posted on the arXiv. Ashutosh's nice result is discussed in Section 7. (I am adding an "operator algebras" tag because the paper is about large cardinal aspects of von Neumann algebras.) 

You might look at Chapter 2 of my book Lipschitz Algebras. The Banach space ${\rm Lip_0}(X)$ is already the dual of the space of finitely supported measures on $X$ satisfying $\mu(X) = 0$, equipped with Wasserstein distance (though I suppose it should then be called Arens-Eells distance). Going to Radon measures enlarges this space but you remain within the completion of the finitely supported measures, so its dual space doesn't change. 

For what I think you are asking, it would suffice to produce a single yes/no question whose answer cannot be "decided". Then we could ask this question on all inputs and have a "problem" for which no specific instance is decidable. And I also think that by "decided" you really mean "proved". Well, if we're talking about provability within a given formal system that contains first order arithmetic and is known to be consistent, then by Godel we know that there do exist statements whose truth value cannot be decided within the system, for instance (a standard arithmetization of) the statement that the system itself is consistent. But this is not such a good example because we just stipulated that we know the system is consistent, so evidently we do in fact know the truth value of this statement even if we can't prove it within the system in question. The issue becomes more subtle if we're talking about the general semantic notion of provability rather than provability within some particular formal system. I can't see how there could be any meaningful sense in which we could definitively establish that the truth value of some statement could never be known. Maybe the best candidates are statements of the form "ZFC plus large cardinal axiom X is consistent" which, if they are true, cannot be proven to be true in ZFC. I guess we cannot exclude the possibility that there is some totally new principle whose truth is intuitively evident and which does decide questions like this, the best we can say is that it doesn't seem very likely. 

Tait (Finitism, Journal of Philosophy 78 (1981), 524-546) has argued that finitistic reasoning coincides with PRA, on the grounds that this is as far as you get with "finitist types". What makes a type "finitist" is supposed to be the idea that you can understand what an arbitrary object of that type is without having to quantify over an infinite set. If you accept this then that will explain why induction up to $\omega^\omega$ is not finitistic, since that is the proof-theoretic ordinal of PRA. Tait's analysis is not universally accepted; for instance, see Ignjatovic (Hilbert's program and the $\omega$-rule, J. Symb. Logic. 59 (1994), 322-343). 

Dixmier's book is a fine, if dated, introduction to C*-algebra, but really not the right place to learn about noncommutative topology. The place you want to go is Alain Connes' Noncommutative Geometry. Edit: in response to Yemon's comment I might refer to my own book Mathematical Quantization. Most of the correspondences mentioned are covered in Section 5.1, and many others can be found throughout the book. 

No, $f$ does not have to be locally constant. Let $a_n$ be a sequence of irrationals that decreases to zero, define $f(x) = 0$ for $x \leq 0$, and let $f(x)$ be a (single) rational number in $(e^{-1/{a_{n+1}}}, e^{-1/{a_n}})$ for $a_n < x < a_{n-1}$. Voila! 

Yes, the Heisenberg uncertainty principle literally is the Fourier uncertainty relation. For simplicity consider the one-dimensional case. The Hilbert space of a spinless one-dimensional particle is $L^2(\mathbb{R})$ and position is represented by $Q = M_x$, the operator of multiplication by $x$. Whereas momentum is represented by $P = -i\hbar \frac{d}{dx}$, which is just to say that $P = \hbar F^{-1}M_xF$ where $F: L^2(\mathbb{R}) \to L^2(\mathbb{R})$ is the $L^2$-normalized Fourier transform. Heisenberg's uncertainty principle is then a fact about the trade-off between the variance of a wave function $\psi \in L^2(\mathbb{R})$ and the variance of its Fourier transform, because the position variance of $\psi$ is $$\|M_x\psi\|_2^2 - \langle M_x\psi,\psi\rangle^2 = \int x^2|\psi(x)|^2 - \left(\int x|\psi(x)|^2\right)^2$$ and its momentum variance is $$\left\|-i\hbar\frac{d}{dx} \psi\right\|_2^2 - \left\langle -i\hbar\frac{d}{dx}\psi,\psi\right\rangle = \hbar^2\int x^2|\hat{\psi}(x)|^2 - \hbar^2\left(\int x|\hat{\psi}(x)|^2\right)^2.$$ You can also see from this that, yes, a particle cannot simultaneously have a bounded range of possible position values and a bounded range of possible momentum values --- that is just saying that $\psi$ and $\hat{\psi}$ cannot both be supported on bounded intervals. 

I'd like to try to give a more comprehensive answer. In the elementary formulation of quantum mechanics, pure states are represented by unit vectors in a complex Hilbert space $H$ and observables are represented by unbounded self-adjoint operators on $H$. The expected value of a measurement of the observable $A$ in the state $v$ is $\langle Av,v\rangle$. We could also say that the state is represented by the linear functional $A \mapsto \langle Av,v\rangle$, and this interpretation generalizes to say that a mixed state is represented by a linear functional $A \mapsto {\rm Tr}(AB)$ where $B$ is a positive trace-class operator satisfying ${\rm Tr}(B) = 1$. The fact that $A$ can be unbounded is forced on us by basic physical examples like position and momentum. Mathematically, it is easier to work with bounded observables, which can be obtained from unbounded observables via functional calculus: if $f: \mathbb{R} \to \mathbb{R}$ is bounded and measurable then we can give meaning to $f(A)$ as a bounded self-adjoint operator. In the C*-algebra formulation, bounded observables are self-adjoint elements of a C*-algebra $\mathcal{A}$ and mixed states are positive linear functionals on $\mathcal{A}$ of norm one. The pure states are the extreme points of the set of mixed states. We can always pass from the C*-algebra formulation to the elementary formulation using the GNS construction: given a state $\phi$ on a C*-algebra $\mathcal{A}$, we can find a Hilbert space $H$, a $*$-representation $\pi: \mathcal{A} \to B(H)$, and a unit vector $v \in H$ such that $\phi(x) = \langle \pi(x)v,v\rangle$ for all $x \in \mathcal{A}$. Why bother with the C*-algebra formulation, then? Well, maybe you don't need to. But sometimes there are good reasons for looking at things this way. 

Take $[0,1]$ with Lebesgue measure, let $X$ be any subset of $[0,1]$, and for each $x \in X$ let $E_x =[0,1] - \{x\}$. Then each $E_x$ has full measure, but their intersection is the complement of $X$, which could be anything. However ... would this help? Say that $A$ almost contains $B$ if $B - A$ is null. Theorem. Let $\{E_i: i \in I\}$ be any (possibly uncountable) family of measurable subsets of a probability space. Then there is a measurable subset $E$ of the space such that (1) $E$ is almost contained in each $E_i$ and (2) $E$ almost contains any measurable set $E'$ which is almost contained in each $E_i$. The set $E$ is unique up to modification on a null set. In other words, the family of measurable sets modulo null sets is a complete lattice. Actually, this is true for any $\sigma$-finite measure space. To prove it in the finite measure case, let $\alpha$ be the supremum of $\mu(E)$ with $E$ ranging over those measurable subsets which are almost contained in every $E_i$. Then choose a sequence of such sets $E_n$ with $\mu(E_n) \to \alpha$, and take their union. 

No. First of all, this condition only makes sense if $A$ is a factor: if $A$ has a nontrivial center, then everything in its center commutes with both $A$ and $B$. But the condition fails even for factors: you just need to find a proper von Neumann subalgebra $B$ of $A'$ such that $B' \cap A' = \mathbb{C}I$. (Anything that commutes with $A$ and $B$ lies in both $A'$ and $B'$.) There are lots of examples of this; Google "trivial relative commutant". I guess the general technique for proving that $B$ generates the commutant of $A$ is to check that every operator that commutes with $B \cup B^*$ belongs to $A$. This would imply that the commutant of the von Neumann algebra generated by $B$ equals $A$, and hence this von Neumann algebra equals $A'$. (Maybe a disappointing answer.) 

Yudkowsky and Herreshoff have a (messy but) great paper which relates the second incompleteness theorem to issues in theoretical artificial intelligence. (This paper of mine might be a more accessible introduction to the subject.) In principle, one way an intelligent agent $M$ might achieve a goal is by building an auxiliary agent $M'$ and tasking it with the goal. But presumably $M$ cannot satisfy its criterion for action unless it can prove that $M'$ reasons consistently --- otherwise it could be building an agent who might fail because it reasons incorrectly. But by the second incompleteness theorem, $M$ cannot prove that the system within which it itself reasons is sound, which means that $M'$ would have to reason within a weaker system. It's especially a problem for the idea of self-modifying AI. A sufficiently advanced AI ought to be better at designing AI's than we are. So we might want to design an AI which is capable of improving itself by modifying its own source code. But the incompleteness obstacle seems to imply that it could only do this at the cost of weakening the formal system in which it reasons. Since proof-theoretic strength is gauged by ordinals, after finitely many iterations it would reach imbecility. At first sight it seems like there should be a trivial resolution, but the more you think about it, the more serious you realize the problem is. 

No, take $m=n=2$ and let $X$ be the $x$-axis. For every line $Y$ through the origin there is an isometry from $X$ to $Y$, and if $p\neq 2$ they obviously don't extend to isometries from $\ell^n_p$ to itself. 

It's easy if $d = 1$, right? So the first case of interest is $d = 2$. Here is an example which shows why I think no nontrivial bound is possible. (Of course you can always let $\hat{f}$ be a constant function which takes a value halfway between the max and min values of $f$; then $\|f - \hat{f}\|_\infty \leq \frac{\sqrt{d}}{2}$ since the max and min values of $f$ differ by at most the diameter of $[0,1]^d$, namely $\sqrt{d}$.) Example: define $$f(x,y) = \begin{cases}x-y&\mbox{ if }x \geq y\cr y-x&\mbox{ if }y\geq x\end{cases}$$ on $[0,1]^2$. This has Lipschitz number $\sqrt{2}$, so feel free to scale it down by that factor if you like. Now let $\hat{f}$ be any function on $[0,1]^2$ which is continuous and affine on each of the $k^2$ sub-squares. The key point is that $\hat{f}$ is determined by its values on the $x$ and $y$ axes. That is because an affine function on a square is determined by its values at the SW, SE, and NW corners of the square. So once we know $\hat{f}$ on the left and bottom boundaries of $[0,1]^2$, an easy double induction shows that we know it everywhere. Indeed, it is not hard to see that we must have $$\hat{f}(x,y) = \hat{f}(x,0) + \hat{f}(0,y) - \hat{f}(0,0)$$ at every point. Now $f(1,0) = f(0,1) = 1$ and $f(0,0) = f(1,1) = 0$, but if $a = \hat{f}(0,0)$, $b = \hat{f}(1,0)$, and $c = \hat{f}(0,1)$ then $\hat{f}(1,1) = b + c - a$. So if $b$ and $c$ both exceed $\frac{1}{2}$ then one of $\hat{f}(0,0)$ and $\hat{f}(1,1)$ must exceed $\frac{1}{2}$, and this shows that $\|f - \hat{f}\|_\infty \geq \frac{1}{2}$ regardless of mesh.