As commented, best tool around is sp_whoIsActive by Adam Machanic. It can be used in several ways, to see what is running at the moment you launch the script or you can run it in loops to monitor some specific action, as slow queries for example. To run in a loop take a look here: How to Log Activity Using sp_whoisactive in a Loop To detect slow queries: How to Use sp_WhoIsActive to Find Slow SQL Server Queries You can directly use DMV's to get your slowest queries and act from there. Check Glenn Berry's diagnostic queries. And finally you can use this query to find most time consuming queries. You can play around with the dm_exec_query_stats to add more data or join with other ones to get more information. Be aware that dmv's gets flushed away and refreshed each time the server is restarted. 

I have a couple of SSIS packages that copy the content of several tables to another db. One of the columns updated in the process is a datatype. From what I read this datatype can't be updated, it gets updated automatically with each or . But digging on the net I've found this and this about issues on SSIS when updating this type of column and using and types. If I got it right, using SSIS packages those columns can be modified. My issue is that now I want to replace those SSIS packages with stored procedures that will be called by the SQL Server Agent. I'm reusing the already existing code on the packages but of course I'm getting the errors "Cannot insert an explicit value into a timestamp column." or "Cannot update a timestamp column.". Is there any way I can replicate the timestamp values from one database to the other? Without changing the table structure or column type as the whole process is already configured and running on several databases. 

There are several ways to use a Dark Theme on SSMS main coding window, either importing a vsettings file, applying Dark Theme that is disabled on a config file or doing it manually. But all those options do not affect Grid Results, Object Explorer and other windows. Those 2 are the main ones I use other than the coding one. I tried the usual Tools>Options>Environment>Fonts and Colors then selected on the combo box Show settings for: the option Grid Results and using White for Item Foreground and Black for Item Background. Saved, restarted SSMS but just the text is white, background is still white. Any idea on what is happening or how to do it? I can't find how to do it for the Object Explorer. 

I'm doing some cleanup on one of our database for development purposes on a test server. There are a couple of tables with one column. Both tables take roughly 50Gb of space, being the image column the culprit as the the other files are just int, datetime and small size varchar. I ran an update on the tables, using recommendations I found here in SO and other sites, putting the db on simple recovery mode, disabling indexes, setting transaction isolation levet to read uncommited and updating in chunks of 1K rows. Updates takes some times to run, about an hour. Here is my code: 

If there is a database with data on it, someone, at some point, will consider it important. So best approach would be run a full backup to avoid any hassle. Maybe weekly with couple of daily differentials. If there are frequent changes then maybe a daily full backup and one differential in the middle of the day. If the data is temporary and can be recreated somehow, then I would configure the recovery model to SIMPLE and take a FULL backup daily. And to avoid spaces issues, if the data is not so important and can be recreated afterwards, I would add a process to empty all tables before running the backup. Then configure another step after backup process in order to add fresh data once again so anyone using the database has data to continue working. And last, but not least, you could just script the database and recreate it from those scripts. Do you have your database code under source control? That would help a lot on this case. We have couple of databases with similar characteristics and we only save the recreation scripts, data can be easily recreated with couple of scripts. 

I'm slowly collecting and trying to analyze lot of information on all of our instances (2008R2 SP1). I'm now going through the Error Logs and I've couple of question. There is a lot about error log size and found different recommendations on using to recycle the logs on say, weekly basis, to avoid having very big error logs and be able to handle them adequately. So... 

Another option, if you expect to be "playing" around with the jobs order very often, you can follow the following steps. We use it extensively and it works flawlessly. 

That's why when running it, tried to automatically create the snapshot. And as last snapshot is from couple of months, there is lot work to do in the middle. Ran with false value and now everything is ok, runnig smoothly and without issues. 

We have several db servers on production, 4 of them with very similar hardware configuration. Dell PowerEdge R620, only difference is that 2 newest ones (bought and configured 3 months ago) have RAID controller v710, 256GB RAM and CPU is 2 physical Xeon E5-2680 2.80GHz. Old ones (bought and configured about 1year ago) have RAID controller v700, 128GB RAM and running on witl 2 physical Xeon E5-2690 2.90GHz. BIOS is updated, all drivers updated to last versions, etc. All runing SQL Server 2008R2 Enterprise (SP1) updated to last CU and Windows 2012R2 Standard. Both running on 200 GB SSD x5 RAID10. There is only one database running on each of them, synchronized using a job that calls an SSIS package. Our sysadmin has run lot of performance and stress test to rest assure we don't have any hardware or network miss configuration or failure. As expected, newest ones show better performance results. So far so good. The problem we have can be seen on the screen capture from Kibana. Yellow and orange are the 2 newer servers (6,7 on tables) and below all the other servers. Is perfectly visible that those 2 new server have a slower response time. And not only that but also those 2 servers have a bit less of load than the 2 older ones (light and dark blue lines- 4,5 on tables). 

According to comments the inner is not needed. But then, following OP question and provided sample data as the results don't match the Output suggested by him. Only if the OP is mistaken and all the time he is asking for datetime, as on the first sentence of his question. 

I'm configuring a job to get the SPID of an offending process and kill it. I'm using a mix between the great sp_Whoiscative from @AdamMachanic and joining with sysprocesses on login name and SPID and session_id values as those are supposed to be the same (or I'm wrong and then that's my error?). I'm getting something I don't understand: for a same SPID from sysprocesses I'm getting different session_id values from whoisactive results. What is incorrect here? Here is the code I'm using. The filter to get values 10s before current datetime is because the whoisactive is running with the parameter @deltainterval that takes about 10s to execute on each run. 

You can limit the connections by using a specific login for that application. Any other app with a different user will just not have rights to even connect to the server. 

I'm trying to find when a database on our instance was configured as but can't find that information. Either using tsql or the GUI. Maybe it is not possible to know that information? Are these actions logged somewhere? Or do we need to force this logs? 

One thing is to compare raw hardware performance, for that you have tons of tools around. Just type "server hardware performance" on your preferred search engine and you will get lot of options. Other case is to compare SQL Server performance. Then, for starters, you will need to be sure that both have same configuration parameters. You can use FirstResponderKit for that task. It will give you a handful scripts to monitor and and do a first triage of your servers, so you can have data to compare. You could run a series of test and measure different performance parameters with tools like Perf Monitor and alike. I like to use SQL Stress to run a group of queries in loops, at the end it will also give you some useful data. 

As a recommendation, I would suggest switching to Ola Hallengren maintenance solution for the backups and some other administrative tasks, is really easy to configure. Run the main script Maintenance Solution, schedule required jobs and off you go. I hope that backups are copied to a different server than production server. Then, schedule restore jobs from the test server to restore those daily backups. The job would have minimum 2 steps, drop database and restore it. Maybe you will want to add some cleaning step to clean up or mask some critical data that shouldn't be available for testing, replacing emails to avoid sending emails to clients and stuff like that. Maybe even deleting some data from bigger tables to have smaller databases. Heck, maybe for some cases is enough to just have the skeleton of the databases. If there are few databases, you can handle it manually. If not, you could make use of dbatools as commented, is a set of powershell scripts really helpful for administrative tasks. Or you can search online, there are plenty of options available, 3rd party tools, scripts that you will need to tune for your needs, etc. 

You will certainly need the right permissions to view the views definition. This should work for you. Please note that I'm using the undocumented stored procedure. This means that it could change without notice so is not advisable to include as part of your production/live tsql code as it could fail. Check Aaron comment about it. 

And second part was to modify a windows registry and restart SSRS afterwards: Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Lsa Create a new DWORD registry key DisableLoopbackCheck and set value to 1. In case you are wondering, here are all the references consulted: 

If I clic on a tab on the left pane (for example in Security), SSRS ask me to reconnect and jump to an http page. This is a real problem for us. Any suggestions? 

As Shanky already commented, running scheduled shrinks is wrong. There is ton of info online about it. Find the root cause of that growth and solve it. I would get rid of it immediately. As for the error, seems related to db ownership. If you run a search with the error text your will find several related questions here, here and more. As a recommendation, I would suggest switching to Ola Hallengren maintenance solution for administrative tasks as DBCC, index maintenance, backups, etc. Buena suerte, feliz navidad y felices fiestas. 

When an error log is considered to be "big"? Some of our logs take long to load, close to a million of rows. edit: one has 2 034 546 rows I'm trying to find some tsql to get error log size, but can't find it, it is possible to do it? I see the recommendation to recycle the logs and to configure more than just the default number of 6. Some authors recommend 50, others 10. I guess it depends on the environment, but is there a recommended number? 

We have on several databases on same instances, some stored procedures have references to tables on other databases. We are in the middle of a big project to take our database under source control. Creating the baselines is not an problem. But after baseline creation, when trying to deploy on other environments, say a local developer instance or staging or alike, this cross reference issue is causing us several headaches. After doing some research and checking on possible approaches I still can't find a reliable way of managing this issues. One approach proposed is to split baseline creation into "modules" that could be run individually so the cross referenced objects remain in one single script and be run alone. But what if the referenced objects on the other database has changed, then how I can track those changes? Maybe an index was added on referenced table and directly affects performance on the current sp? Or for whatever reason the referenced table changed column name, even deleted that column that is referenced and added a different one? What a nightmare. So, question is, is there a recommended solution for this issue? For solution I mean, methodology, step-by-step approach, or whatever else that helps getting close to a solution with the deploying issue when having cross reference between database objects. And for the moment, removing the database cross references is a no go. Way to much application dependent stuff to be corrected/updated and can't be done for the moment. Not completely sure if this question follows the standards, as it can maybe trigger some opinion based answers. But I don't have more information to throw in for the moment. If you feel it should be flagged, then go. 

That's the equivalent of checking on the publication creation wizard the checkbox Create a snapshot immediately and keep the snapshot available to initialize subscriptions. 

So, after running out of possible solutions we opened a support case to Microsoft. They asked to run a tool to gather some info while the process was running and afterwards they analyzed it. Here is their answer: