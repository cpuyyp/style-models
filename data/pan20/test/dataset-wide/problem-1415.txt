This is shorter and clearer, and lets JUnit both easily verify that the correct exception was thrown and give you a useful error message if it wasn't. 

you can then just print that list to print the (filtered) row. Note that you could also do this filtering yourself, using a list comprehension; however, has the twin advantages of being common (hence readable) and carefully optimized (hence fast). 

Improved version of original program To start with, I just applied some polish to the original program, maintaining the approach and spirit of the algorithm while tidying up the code. Starting immediately after the line: 

Then, you have a bunch of linked-list functions which operate on pointers, and use an macro to get back from the to the enclosing structure. Not to say that this is better, but it's arguably a more type-safe way to get 'generic' linked lists in C. Both approaches are valid, though (and yours is more 'classic'). Const correctness I would generally say that things should be whenever possible, because it makes programs easier to reason about. I generally encounter the opposite problem to the one you're describing, though: something is conceptually const, but for incidental implementation reasons it needs to be mutable. DLL naming Well, 'dll' isn't the clearest name, because of the conceptual collision with Windows libraries. It probably isn't a big deal unless you're writing software on Windows and using dlls. I would be tempted to just call this module "list", or "ll", though. The only real constraint on the include guard macro is that it not collide with any other names in your program. I suspect that is probably fine. (Obviously, if you have some style guide or standard, that probably specifies a convention). Implementation This is pretty nice code overall. If I were feeling picky: 

Presumably so that you get an unanchored match (i.e., one which can start at any point in the string). has a method which will do unanchored matching by default; you can anchor the match to the start of the string with , as you would expect. See search() vs. match() in the documentation. Manipulating whole data structures There are a number of specific points below, but the broad thrust of them is that you will have a much nicer time doing large-scale operations on whole data structures, rather than fiddling around with indexes and individual elements. This is true in most languages, but one of the strengths of Python is the rich set of data structures and operations which are available built-in or in the standard library. 

would in my opinion be easier to read (you might choose to make the log level names a little shorter, e.g., , , in that case). Naming consideration: prefer systematic naming This is probably quite subjective, but: in the case of a module such as this one, my inclination would be to prefix all of the names with , like so: 

I think I've previously explained all of the things that go into that final version. This version seems to have good performance overall; the thing which costs it most performance seems to be increasing the size of the input sets. Increasing the size of would also cause problems. Playing around with various input-generation parameters, it seems to generally handle inputs of a few hundred thousand sets in a few tens of seconds. It should also be straightforward to parallelize this version: the expensive operation is generating the index; partial indexes can be generated independently, merging indexes should be quite cheap, so it's a good candidate for multiprocessing. Having the indexes available would also enable more interesting behaviours, particularly if the element->set index from the previous version was also available. For example, it would be straightforward to keep some filtered sets (say, the single largest filtered set) to avoid dropping elements entirely. Perhaps I got a little carried away with that, but it was enjoyable to poke at. 

This is still quadratic (sadly), but the early exits combined with the other improvements does speed things up somewhat (about a factor of two on my machine with my test data). Using Whenever I have a problem that looks somewhat like this one (fiddling around with sequences and collections), I wonder whether the standard library module could help. It usually can. In this case, we can use to generate all possible pairings of sets (not including pairings with themselves). Then, we can intersect each pairing, and keep both sides; doing this for all pairings gives us the set of sets which we wish to exclude from the output. The full program looks like this: 

This is still quadratic, but now the quadratic thing has been moved inside a single generator expression, with the quadratic expansion coming from a part of the standard library; I'd guess that this would be a relatively good place to start if you wanted to use multiprocessing to speed this up. The greater use of optimised standard functionality also gets us a performance win: about a factor of two (so 4x faster so far than the original). Indexing Constant-factor performance improvements don't win us very much for \$O(n)\$ problems with largish \$n\$; some kind of algorithmic improvement is necessary. My initial thought when trying to come up with an improved approach was that matches were likely to be quite sparse---that is, most sets won't share three items (or whatever is) with each other; I would expect that most sets will share no items with each other. This is true of my randomly-generated sample data; it may not be true for your real-world data. If that assumption holds, then the obvious place to start seems to be with a simple index, mapping elements (ids/whatever) to the sets which contain them. Then, we can perform our existing quadratic comparison, but only on sets which we already know share at least one element. If most elements only participate in a small number of sets, we are then doing our quadratic process many times with small \$n\$, rather than once with large \$n\$; this should improve performance. Starting immediately after the line from the previous version (also needs an for the ): 

Your first pair of loops builds the list . The length of this list is \$O(n^2)\$ in the length of the input. In your second set of loops for each element in you all of to find those elements which match it. The runtime of this filter process is linear in the length of , and hence quadratic in the length of the input. You perform this quadratic process once for each member of the input list; the search loop is therefore cubic in the length of the input. Your algorithm as a whole is therefore \$O(n^3)\$ in the length of the input. To make the algorithm as a whole quadratic, you need to make the second (match) stage linear in the length of . Natural approaches would include: 

this could be useful in some situations, although you might want an option to turn it off, or to adjust the order (lots of tools can be configured to reference pairs in various formats back to the source line, as a way of handling e.g. compiler messages). Naming consideration: and are very similar. The visual difference between and is tiny. Using more different names (e.g., , ), or passing the log_level as a parameter to a single function: 

Bug: Use of in does not check length It's possible for to be longer than ; in this case, your call to will smash memory beyond the end of . You should at least use to avoid out-of-bounds accesses; preferably, you would ensure that the sizes all add up, by either: 

(for erate character), but that's open to debate. Redundant s The statements in don't hurt, but they are dead code, so they take up mental space. I would be inclined to drop them---and to align the from the default case with the others, to emphasise that all of the cases return directly. No from main I know that the C standard allows it, but it will still always look wrong to me. Depending on the context for this program, other people might look at the source and have mental space occupied by the sense that the lack of a from looks wrong. But that is essentially entirely a matter of opinion, and arguably I'm the one that's wrong there. As I said though, overall it's really rather pleasant. 

For most inputs I tested, this approach has both better constants and scales better than the previous programs. But it is possible to generate pathological inputs. The index built here also introduces the possibility of doing some kind of clever filtering---for example, to avoid filtering out all the sets which mention some element (of which we have a convenient list). ngram Indexing Having had some success with indexing, it's reasonable to ask whether there's more to be gained from that approach. The problem with the element-based index was that it could only tell us whether sets had any elements in common; a quadratic-time operation was still necessary to filter the sets with common elements. So a natural question is whether a different kind of index could do more work for us in the data structure. I can't think of a better way to transition this discussion, so I'll just say: we can get what we want by using ngrams as keys for our index. This requires only a small evolution of the previous approaches: we can use with as the length parameter to generate all of the possible -length subsets of each set. (We need to sort the input to to get consistent output). Then, we can just look up each key (unique -length subset); whenever a key maps to more than one input set, those input sets have at least elements in common. The code is straightforward: 

Once you can easily generate lists containing exactly the elements you want to output, you don't have to fiddle around doing and similar; you can just do: