Works perfectly fine on Debian/Ubuntu with the nVidia drivers. You have given us very little info but I think I remember having this problem with ATI drivers a while back. 

After you have a bridge set up, all you have to do is put the public IPs on your domU's and that's it. 

Yet another option is suPHP which allows you to define a php.ini for each VirtualHost. It also allows you to specify a PHP version for each VirtualHost. 

What you want to consider as well is Private VLAN. You put all your users in an single "normal" VLAN but only allow them to speak between specific ports. Basically, you emulate point to point between gateway and PC. Much more simple then any other solution mentionned here. 

If these were HDD-based, I would run SMART monitoring tools to check for bad sectors and general disk health. Except, due to them being CF cards, I am in the dark and have difficulty measuring how bad (or good !) the situation is. What can I do to monitor the health of these cards and measure their health ? I insist on "measure" as I need to give some hard facts that will eventually motivate the change of all the CF cards. And to make things a little more complex, I do not have physical access to the Soekris boards so all this needs to be remote. 

In order to avoid having to setup a NAS system, I am considering the possibility of setting up a dual-primary with DRBD to handle RAID1 over IP. Concerning filesystems, I know of OCFS and GFS as distributed filesystem. I am leaning towards OCFS as that seems like the more reliable option. Do you have any production experience with such a system ? Am I heading towards trouble ? What should I expect/plan ? 

Personnal VPN server definently on port 443 TCP gets you out of any network restrictions. HTTP server for you blog/wiki/other Transmission Torrent box (watch out you're warned) 

I agree with Zephyr but I must add that it is possible for a provider to do IP route load balancing. Let's say you're a random Internet provider and you have 4 equivalent links to the same point. You can chose to load balance the traffic in between those links. This means that the hops will not be the same. 

The user has not already been created on the host as it is Puppet's job to do so. The examples I have found all over seem to be the same as this but since this isn't working, I am definitely missing something. 

This is actually the same solution. You add more balancers and insert them in round robin DNS. Or as I said before, the smarter alternative is to use geographical load balancing. 

I am trying to configure Apache Virtual Hosts with Puppet and have been trying different things with little success. I have defined a node as the following : 

You're lucky that RHEL4 uses LVM by default. LVM is a flexible partitionning tools that allows you to modify partitions really easily. You can modify the size of a LVM partition by using the following commands. The first one modifies the size of the partition (replace X by the new size) and the second adapts the filesystem. 

I second what ConcernedofTunbridge said. Forget about low end SAN or NAS. The performance will most likely be worse then with regular DAS. If you need fast disk access in DAS, go the SSD route. The performance of SSD beat any hard drive disk by orders of magnitude. 

It depends what you mean by trunk. In Cisco language, a trunk is a 802.1Q port that will tag VLANs. You can do this by activating 802.1Q. In rest-of-the-world language, a trunk is an agregate of two ports. You take two physical ports and bond them together to have one logical port with twice the bandwidth. This goes along with protocols such as LACP. 

You can always replicate the SAN data indeed but considering the price and specs of SAN boxes, there are highly available to themselves. This is only true if you actually get a decent SAN obviously. This does not mean that your disks become more reliable though... Always use some sort of RAID technology to protect (i.e. not RAID0) your data accross disks. 

This behavior is a very good way of finding a potential duplicate IP. If your computer gets no answer, then it is the only one with that IP. If your computer gets an answer, there is another computer with the same IP which is a problem obviously. Concerning RFCs, I find them horrible to read. I only use them for reference concerning specific problems. I have probably read just one from start to finish. The rest I read bits by bits. IMO, I find that the best way to learn about something is to pick up the O'Reilly or similar paper book and read it. There can be more then one RFC for a single protocol. For example IPv6 has 10 different RFC just concerning transition mechanisms from IPv4 to v6. There are many others for such things as neighbor discovery. SCTP is covered by 4 RFC also. 

With Citrix XenServer, you have the control interface called XenCenter with which you can connect to your hypervisor and do such things. With OSS Xen, you can use the xm console command. 

Hyper-V is a hypervisor solution only available with Windows Server 2008. It puts an hypervisor in between your hardware and the OSes which leads to modification of the kernel of the guests OSes. With a hypervisor solution, everything is virtualized except the hypervisor itself. This results in far better solution in exchange of a higher complexity. Virtual PC is a standard virtualization that virtualizes an OS in an application. It's more simple but performance is not as good. Hyper-V is only for Windows 2008 server on the "host OS" 

The only specific configuration I can think of is allowing any IP to connect to your squid proxy. In "Proxy server", you have the "Access control" tab. You can put 0.0.0.0/0 and see if that helps. If it doesn't help, you're going to want to create a specific rule in your firewall to allow access to squid. This should look like allow any to squidIP on tcp squidport on interface WAN. That should do the trick. 

Are you sure they are using your webmail ? Webmail is a web interface used to send emails, we agree on that ? I advise you to remove your webmail asap because someone is most likely using some sort of vulnerability in your webmail software. Upgrade it and put it back online. If the spamming continues, try to see which user is sending email and disable their account. Check if someone hasn't inserted a malicious web page that sends out email. I had this exact situation 2 weeks ago. If so, delete it asap and seriously reconsider reinstalling your server. Also, is the webserver local to the webmail ? If so, are you sure you're not an open relay ? There are tests that allow you to test this. Big ISPs only allow the IPs of their clients to send email via their mail servers. If you are not an ISP, you want to force your clients to authenticate (see SASL authentication for Sendmail) or allow specific IP which you know are good. 

I think the main difference between TCP/IP and OSI model is that one is protocol-specific and one is as-generic-as-can-be. The question is not TCP/IP or OSI as they are not incompatible. TCP is a layer-4 protocol in the OSI model and IP is a layer-3 protocol in the OSI model. But there are many other protocols that can be adapted in these layers. For example IPX, IGMP and ICMP are other layer-3 protocols and UDP is another layer-4 protocol. Also the OSI models covers more as it includes lower layers which are very important to the networking world. One problem with the OSI layer is that it is a little bit too "extensive" as layers 5 through 7 are often merged into one. Not contradictory. Just different. 

The setup We currently have a Freeradius server used to authenticate our Wifi users against our Active Directory server. The link between Freeradius and the Active Directory is done by Winbind. In order for the user to be able to obtain authorization, it needs to be belong to a group in the Activer Directory. This is done by adding an argument to the ntlm_auth command. What we are trying to achieve We are now adding 802.1X to our cabled networks and would like to re-use the existing Radius server to authenticate against the same Active Directory. Everything will be the same except the authorization will need to be based on whether the user belongs to a different one than that of the Wifi networks. What we have already tried I have read many things on freeradius in the documentation and have seen that it is possible to use conditionnals and variables. My plan therefore was to put a variable in the ntlm_auth command that would contain the group SID (as suggested on Freeradius mailing-lists). The group SID would be dependent on the IP of the network device which should be contained in "NAS-IP-Address". This should just be a case of writing a simple conditionnal statement and setting a variable. Nonetheless, I have not been able to do this as Freeradius will not start everytime I try to add a conditionnal to the configuration files. So my questions are : 

I have the exact the same problem as you but for a slighty bigger network. I would suggest you to use Pfsense which is based on BSD and is configured via an extremely powerful yet simple and clear web interface. It is firewalling a zones of 70 servers on a Celeron 3Ghz processor with 2GB of RAM (largely unused). Configuring it as a transparent bridge was the most efficient setup as it allowed to modify practically nothing on the actual architecture. I therefore suggest you either get one nice Dell server (low end will be far enough) for reliability of the hardware components and install pfsense on it. Or you can reuse two older servers on PFsense with redundancy (CARP) which is really simple to configure. 

You must be able to send out ARP requests for sure. What you can do is ARP all the IPs in your network one by one and see who answers. 

bigmem = PAE which will allow you to use more then 4GB with 32-bit. It's not the best idea but it should work. 

Smells like you have a cheaper NAS. It could also be from your network bandwidth... "Standard" consumer NAS are really weak when it comes to heavy IO which is what you are trying to do here. It could also be a cheap switch connecting your PC and your NAS that is not strong enough to handle all the packets correctly. 

DRBD only works with two nodes. If you use it in a primary/primary setup, it will indeed allow you to have some sort of high availability. I wouldn't recommend GFS, it is really slow. DRBD is slow because it needs to replicate data over the network, if you add on top of that a slow FS, you're looking for trouble. By slow, I mean slower then DAS. The replication features included with MySQL are a mess. It's already quite complicated with two servers but with circular replication, it's just crazy. For DB servers, I find that the best solution is to use the replication features included with the DB system. Using block replication could potentially lead to data corruption which would most likely crash the server. Imagine this, you have a master/master setup using DRBD/OCFS. What happens if you do an insert in the same table of both hosts at the same time (+/- 1ms) ? How is DRDB/OCFS supposed to know how to order the tuples ? The replication features of the DB software will know how to handle this while DRBD/OCFS is not meant to handle such things. Therefore, use the MySQL replication features and crash test them. Try to pull the cord on one server and see what happens when it comes back online. Try to pull the cord on both and see what happens when they come back online. Do many simultaneous insert and updates on both at the same time. For more then two nodes, consider MySQL Proxy.