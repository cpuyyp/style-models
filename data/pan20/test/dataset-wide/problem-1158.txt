Yes, that's what will happen if you change nothing. No, the transaction log will still fill up. Regardless of synchronous or asynchronous modes the primary must keep all log blocks that are required for the replica that is the furthest behind. In your scenario, there are only two replicas a primary and a secondary. The secondary will be offline for a while. You've already correctly stated what will happen, the send queue will get larger as the replica is offline and stays that way. This happens no matter the mode. The mitigation is to remove the replica that will be down for the extended period of time. This allows the primary to properly truncate the log with proper transaction log backups. Once the secondary is back up, since it was kicked out of the AG all of the databases participating in that AG will be in a restoring state. To get back to a good steady state and bring the replica back into the AG, take the log backups that were run during the period the secondary replica was offline and apply them to the databases, leaving them in a restoring state (with no recovery). Once you get close, pause all transaction log backups on the primary and join the replica back into the AG. Resume the log backups on the primary. 

If your min/max memory settings are set properly then SQL Server shouldn't be the main cause of memory pressure according to Windows, especially since you said it's dedicated with nothing else running. It could be internal memory pressure, where clock hands ae sweeping caches (inside or outside hands) and that would be due to workload/amount of memory available. That should be investigated. How do you know there is pressure? Could you update your post with what you've found? 

In-Memory objects (in this case, tables) are not just pinning data into memory. They have entirely different memory structures and are lock and latch free. There are additional differences in how the data is stored, how it affects database startup, etc. 

However, this will not take an actual lock on the resource and is an application logical lock only. When I read the OPs statement I did not think that actually locking the tables would be a good idea as there is a script or application that is attempting to do something. Since locking the tables will effectively cause serialized access to the tables - or worse, result in deadlocks in the script effectively denying everyone access - I gave this application lock example as a possible alternative. This way the application lock can be checked and/or taken for the parts in the script that were necessary but leaving the system in an otherwise functioning state. If the OP truly does need to lock tables and provide serialized access, it may be worth changing the isolation level to or re-thinking how the script or application functions. 

Again, depends. Will this ever be failed to? Maybe just in the instance of a true regional DR event where RTO trumps RPO? Will there be any reporting on this new replica? If so, does the reporting require a specific data freshness SLA? Synchronous will obviously have an overhead and your round trip time (rtt) latency will play the largest factor for long distance AGs, followed next by disk. Asynchronous will obviously not have those problems in terms of reducing the throughput of the workload but will still feel it in terms of data freshness if it is used for reporting. Remember, if it is a planned downtime you can change the availability mode before performing any activities. 

The easiest way is to use sp_getapplock but you're forcing serializable concurrency when you do this for exclusive access and it's a terrible idea. 

While the IP may or may not need to be public (different architectures at different companies' may or may not require different things), it seems in your case it does... though I'm not a networking architecture guru so I can't comment on how/why this was implemented or any technical challenges observed. 

Should you setup a witness? Yes, always with 2012R2+. Why? WS2012R2 added dynamic witness on top of the already included dynamic quorum, both of which are enabled by default. Dynamic witness and quorum work together to give you cluster the highest availability possible given the infrastructure it has. How dynamic witness works is as follows: 

I covered this through two main posts. The first gives you an idea how to see what is being used, currently. The second tells you how to find which connections are read only routed by exploiting the fact that SQL Server can listen on multiple ports and that read only routing accepts whatever endpoint url you give it which is directly given back to the client - this means you can setup specific items just for read only routing and report back on it. It is much more accurate and no need to go through extended events. You're specifically looking for the second post, but they are related and will give you a better picture when combined together. 

When using SQL Server 2016 and multiple automatic failover targets, you can set the preferred list through powershell or the gui but do not touch anything else such as possible owners. Again, I would stay clear of autofailback for both FCIs and AGs. 

I don't know if it's local to the server (does the FTP program really know who is primary?) or if it is a "local" share/Dfs. If that server can "see" the file, it'll probably try to run as per normal. While I highly doubt synonyms are used, I never want to assume. Additional Thoughts It would make sense to run this with some debug output/tracing. Put some print statements around your internal checks. I can only theorize the code path it might take not knowing the environment. If you don't want to go through that, doing the checks for your agent job steps as I have above will stop this from happening. 

You should be suspect that it may or may not improve performance, however the reason isn't because the data is already in memory. It is mainly due to being lock and latch free and less code execution with native compilation. Thus, if your bottleneck isn't around locks or latches... the actual gain you receive will be minimal to none. This can be exacerbated if the actual bottleneck is the client. There is a great article about use cases for IMOLTP, and while we are at it you may want to look at 2016 and the uses of IMOLTP for advanced real time analytics.