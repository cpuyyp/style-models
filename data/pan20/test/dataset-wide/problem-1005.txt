"Most secure" is an ambiguous statement that needs more explanation. You need to think about what kind of attack vectors you are trying to mitigate. For example, what if a malicious user trys to brute-force your built-in account? It's going to be locked out. This creates a denial of service scenario, as normal users cannot receive a list of valid accounts either. If you were trying to mitigate a DoS scenario by using hard-to-guess logon names, you just shoot your own foot. Hard-coding a password in your application will make it insecure. First, it can be extracted from the application, no matter how you encrypt it. That's exactly the way Blu-Ray encryption was broken. Second, should you need to change the password, you need to ship new binaries to all the users which is somewhat a hassle. How about creating a SSIS package that publishes a list of valid logins on a network share? This moves some security configuration from the database to your AD group membership maintenance. 

Not at all. A SAN snapshot is a bitwise copy of raw disk state. If and only if all the responsibility for business continuity is on SAN storage team, this might be acceptable. For most of the business cases, this will present such RPOs and RTOs that the solution is not going to be feasible. What's more, restoring a SAN snapshot to a stable state is problematic. If the server was up and running during the snapshot, the server will return to state like someone hit a reset button. That is, a dirty filesystem, incomplete transactions and stuff. To get a clean snapshot, the server must be shut down a priori - and that's a business outage. With SAN snapshotting only, you'd miss at least point in time recovery, as all the databases must be in simple recovery model due missing log backups. This is a killer for most recovery objectives. Denny Cherry has written a whole article about failure points. 

This happens (based on the discussion in comments) because the tables exist in the system database. As per the documentation (my emphasis): 

The data about connection aliases is stored in the Registry, so either a file or a script that updates registry would do. Updating the Registry can be done with or Powershell. The tricky parts are syntax and the fact that 32/64 bit registry keys are not the same. That is, 

I really wonder if you got a solid business case for worrying about performance issues in rounding. Unless you are doing thousands of conversions per second, this really is non-issue. The standard approach is to measure the performance differences. First you got to make sure that your and actually do the same thing. Are you sure about that? After you are comfortable with the idea that both algorithms return same result, run your test cases. On my test system, looping the round/convert 10 million times each, measuring the duration in milliseconds and running the test set five times provides results as such: 

Marked transactions make it easy to do point-in-time restores without worrying about actual time. Instead, one restores to logical event. As a practical example, consider an update that needs to be undone. Now, the update was such that a database restore is needed as wrong value was set to all the columns and original data is lost. Maybe a business analyst set VAT to 24% for all the products instead of a subset. We don't know what the previous VAT values were, so the update cannot be undone. Restoring the database is simple, but how much of the transaction logs would be applied? At which point-in-time would one stop the log restores? Does the business analyst keep a detailed journal that records the beginning of the update? Maybe all that is remembered is that "it was done Monday afternoon". If there are lots of changes in the database, recovery to right spot is trial and error. Should the update be done within marked transaction, the DBA will simply query table and pick symbolic name for the transaction. Log restore is then done with parameter to stop at the precise point before marked transaction was started. 

Another a way would be based on a view. If you can't modify the source table - often the case for 3rd party products - a view can save your day. Like so, 

Shrinking a database is not a good idea. There are specific cases, but they require a DBA to carefully evaluate the situation. Sql Server cannot have an idea if a query in, say, next five minutes is going to hit tempdb heavily. Without such a knowledge, shrinking and expanding tempdb is going to be unnecessary and, to be honest, stupid too. TempDB is re-initialized whenever Sql Server service is restarted. In most of the cases, this effectively shrinks the tempdb data and log files. 

works as follows. First, the inner will find the 1st dot. As index is 0, it starts from the beginning of the string. +1 will point to next character after the dot and pass that as the starting index to the outer . The outer will find index of the second dot. Finally, will return a substring from zero to the 2nd dot. Edit As per ypercube's request, let's handle some errors. First, add some data with invalid values: 

In theory, yes. There might be firewalling or company policy that prevents such an action. Do not try to work around policies or technical barriers on your own! You might get into serious trouble. 

Please do not change Sql Server's TCP port unless you know what you are doing. This needs to be done only in specific scenarios like manual SPN registrations with failover cluster setups or restricted environments. Sql Server installations contain a special service called Sql Browser Service. When a client tries to connect to a named instance, it will ask connection details from the browser service. As per the documentation: 

By this test, it looks like performs slightly better on my system. Your mileage is quite likely to vary. For ten million iterations, saves on the average 250 milliseconds. That is, performs some 250 nanoseconds faster than . I highly doubt if this is relevant to most real-world issues - unless one is developing something like high-frequency trading algorithm or the like. 

By splitting the delete in batches, you should lock way less data than with a all-encompassing single-pass delete. YMMV, depending on the selectivity and other activity in your table. In addition, keep in mind that is logged and you might get a nasty surprise with your transaction log size. This article has nice an overview about how to batch delete data and mind the logs too.