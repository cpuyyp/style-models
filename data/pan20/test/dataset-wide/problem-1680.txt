You don't need LVM. LVM just uses the device mapper, too. You can do that yourself. That may be a bit tricky with the root fs, though, as you probably have to modify the boot scripts. You just need to put a DM device on top of the root device, e.g. 

You have to put both the respective users and the php call into sudoers. It would be risky to allow php calls globally. Instead just the call to this specific script (which really should not be writable by users, the same for all parent directories). 

either don't use the system for anything else and connect it directly (or via VLAN) to a router which prevents network traffic to anywhere else but the gateway (and perhaps DHCP server) or (if performance is not a problem) try to put VMware itself into a VM. Please mind that I know that this is possible with hypervisors but I don't know whether it is possible with VMware. 

BTW: There is no need to give the destination port for if it is not changed. Edit 1: You can do that if you do without allowing the connections explicitly, if you are fine with allowing everything that has been DNATted. In that case you would stick to your 

The Xvnc session and application startup: I would place all this in a script and start it from xinet.d The tricky part is to prevent users from re-connecting to an existing session. This is a unusual requirement since that is one fundamental feature of VNC. You may be able to get away with parsing the output of the Xvnc process and killing it (with the app) when you see a disconnection event. For killing the Xvnc when the app terminates, just wait for the appication to terminate in your script and kill Xvnc if it is still running at that point. 

Postfix Virtual Readme is the mandatory starting point and covers what you are trying to do, then google for more (ie: distro specific instructions, etc). There are many tutorials out there, all slightly different, most use a database like mysql as backend, but if you understand the postfix concept of maps then using plain text files is no different. 

You may want to look at Virtual GL Quote: "VirtualGL is an open source package which gives any Unix or Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration" 

if didn't happen to be empty. The kind of stupid but unfortunately correct answer to your question is simply: Sorry, your question does not make sense. Why? You define (i.e. accept the definition of that web site) "secure" as "no write access for user X". And now you want both to be "secure" and to have write access for that user. That is obviously impossible. Your question can be answered only if you give a both precise and usable definition of "secure" in this case. My first proposal would be to seperate the users. Why shall manage the files and not an additional account? Does it make sense to have this requirement? The next point is: Not having write access to a directory just prevents you from deleting and creating files but not from modifying those files. You just need write access to those files (not necessarily all files). Creating and deleting files via a sudo script doesn't look like a security risk to me; especially then if this is not done as root but as a user whose only purpose is the ownership of this directory tree. 

We are trying to set up a failover ASA configuration at a colo who is only providing us with one network drop. Given that with one network drop, we are not able to completely eliminate all single points of failure, it was our hope to still be able to use the failover ASA config that we had initially envisioned. We do not have a router at the colo, as we were hoping to use our 3750 switch infrastructure to do the routing for us. The network provided by the colo looks something like this (obviously these are not publicly routed IPs, but this is an example): 

The traditional NTFS kernel driver is read only. If you use in your fstab it will attempt to use that kernel driver, and mount your NTFS as read only The is the newer FUSE driver (as you mention) and will mount the partitions as read-write is you use it in NTFS. Some distros have started treating them as one and the same, but as of about a year ago, using in fstab in Ubuntu would result in the file system not being writeable. I guess it might depend upon the version of Xubuntu you have installed. 

Short answer is that you cannot do this with VNC. Each TCP port will be bound to a unique session. As far as I am aware you will have the same conundrum with Xpra, VNC, NX, etc. (going via a server, like freenx, and connecting via ssh is a workaround but not an ideal one!) You may be able to workaround this by writing a simple load-balancer type of application, but this will still require one port per client. As for the rest of your question: the dimensions are specified with -screen. For just starting Xvnc with these options, something like this should work: 

Are you aware of this root filesystem site? All the filesystems on there were originally developed for use with UML, but they should work with any virtualization solution. Note however that there is no bootloader installed as the images are made of a single loop mounted disk (without a partition table). You can still boot them with kvm using its command line option, for the others you will need to boot another image (recovery cd/image perhaps?) and install the bootloader yourself. Obviously you may have to convert this raw format into whatever format you need (vdi/vmware/..) using the relevant tools. The scripts are included should you want to create the filesystems yourself. 

some network event (that does not open a connection) which is logged by Netfilter () a wget call which is triggered by the logging and gets the file from some web server (the OpenPGP file probably needs to always have the same name then) 

Something like this has to be put into the initrd boot scripts. Instead of /dev/sda3 you would then mount /dev/mapper/rootfs to /. Then you can make snapshots of the root fs (but have to do all the steps by hand with dmsetup; no black magic though). You just can't merge them back but that should not be a problem. 

Use the output as new key file (). Edit 1: Please mind this: "This option will read an unencrypted private (or public) key file" So probably the file has to be changed to one without passphrase by a program which understands that format. 

Your SSH key is stored in the wrong format. OpenSSH uses keys which are put in a single line. You need with and options, see . Probably one of these: 

When I have had errors like yours they were normally fixed by replacing the drives (even though smart did not report errors - it's not always 100% accurate and I prefer to be safe). However, since this is a recurrent problem, you should consider the possibility that it is the cables (already changed so probably not) or the controller (try to add a PCI/PCIe controller and see if that helps?). Maybe upgrading the OS kernel would help too if interrupts get lost because of buggy chipset support. 

Probably a silly question, but have you verified that the is also sufficiently high? 4000MB/s would be blindingly fast, that must be the cached speed not the actual disk speed. 

You cannot do that with bare UML, it just does not provide this feature. So you will have to install some kind of server inside the UML guest so it can be accessed from the host. Generally one would go for SAMBA (fairly straightforward to setup) or NFS (harder imo), but there are a multitude of options there, even some esoteric ones like sshfs which are very easy to setup and may just be the ticket for you. 

In addition to that, we also have a point to point back to our primary site, which is delivering layer 2 connectivity. In our initial design, we had envisioned that the 3750 would be the router for all of our devices, which is to say that everything at the colo would use a Cisco 3750 as it's default gateway. The 3750 would decide if the traffic was internal (i.e. locally switched), point to point (i.e. switched back to the main office), or external (routed out). The issue that we hit was that we wanted all of our externally routed packets to be sent through the firewall for the purpose of filtering. Once these packets left the firewall, they would be routed out through the OB connection, which would send them back to the 3750 (over a different VLAN) from whence they had originated, and then out the colo connection. What a configuration like this would allow us to do would be to configure 2 of our routed IPs (192.168.200.1 and 192.168.200.2) on each of the ASAs for the failover. The 3750, in the extent that is acting as our edge router, would have our colo provided customer IP (10.100.200.202) and would also represent our single point of failure. The issue that we hit, as we started to draw out the traffic path decision tree was that we essentially needed the 3750 to have 2 default routes, one for traffic coming in from the internal network -- default route to the firewall -- and one for the traffic coming in from the ASA -- default route out the colo provider connection. Is there any way to accomplish failover ASAs using publicly routed IPs using a single Cisco 3750 as the only path out of the ASA? I know I could do it all with a router (which would become my SPOF), but I don't have one handy, and I'm not extremely keen on purchasing one. 

You can run two instances of sshd with different configurations. In an extreme case: Use one patched version (as this is probably not possible via the config file) which uses a different PAM string than ssh so that you can create different PAM configurations for both instances. Or you start one instance in chroot / lxc so that it sees a different PAM configuration. That might be easier (to maintain). The client selection can be done by iptables (DNAT) then. Just send the respective clients to the port of the second instance (or to the lxc IP). 

Without this the client tries to connect to 1.2.3.4, the gateway deflects the packet to 10.0.43.113, the webserver sees a connection from the client (e.g. 10.0.43.112) thus sends its reply to 10.0.43.112, and the client having sent to 1.2.3.4 and getting a reply from 10.0.43.113 just goes "WTF?"... But with SNAT the webserver responds to the gateway, the gateway rewrites both source and destination address and everyone's happy. 

Can you check (with tcpdump) what happens on the webserver? I assume that such a rule is missing (assuming eth1 is your internal interface): 

You can run it before or after calling maildrop, important note: the script will receive the email via stdin, so you will have to buffer it if you intend to pass it to maildrop later (which the example above does not do). I would probably recommend using perl for this as there are more mail handling libraries there than in bare shell. Edit: If you only want to do this for a single user then this is overkill, have a look at maildrop - in particular the section about "external commands" via backticks. Define a maildroprc for this user with the rules required. 

I have to say that this list here is helpful, if a bit confusing: it includes low level protocols (like NX, VNC, and now xpra) as well as high level wrappers (like neatx, freenx, and now winswitch). Also it points to some VNC implementations, but not the more recent TigerVNC fork... 

If advanced routing is a bit difficult for you then I would not recommend this approach. The problem is that by default each dhcp lease will claim the default route and also update the list of dns servers. Then there is the firewall issue: how to detect when the it is down, etc Buying a cheap and easy to configure router might just be better for you. 

I wouldn't call it "traffic shaping" with a time resolution of one month... You do not want to impose any restrictions before this hard limit is reached? I think you need to watch the traffic and disable the account when the limit is reached (or activate traffic shaping then, making the connection quite slow). You may add rules (without target) for each of the connections (after configuring static addresses as mentioned before) in order to see the amount of traffic from and to this user. Every hour or so you can call a script / program which reads this amount of data, adds it to the user's traffic log, resets the counter (iptables --zero), sum up the traffic log and take the appropriate action if the user's limit turnes out to be reached. 

OpenVPN is not an option because you don't know how to use it? You have told us what your requirements are. Leave it up to the answers what turns out to be an option or not. First point: You can use iptables CONNMARK and MARK targets for marking everything from a certain client. This can be used for firewalling and routing. I doubt that you need several DHCP servers as you can set each clients configuration based on its L2 address. If you really need one interface per DHCP instance this should be possible with virtual interfaces: Enslave veth0 to the VPN-tap bridge and bind the DHCP server to veth1 or similar. The separation of the bridged VPN clients can be done by ebtables. It's hard to believe that anyone could seriously consider writing a VPN software because he wants to use only one UDP port (why?). Even this could easily be realized by NAT. You just have to be able to associate client configurations to source addresses. And even with completely dynamic client IP addresses this could be enforced by creating a non-encrypted IP tunnel first.