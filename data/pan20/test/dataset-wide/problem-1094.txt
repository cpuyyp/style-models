I agree with Rick and I Michaels answers, and would go further. In my opinion you are talking about the difference between tables and the interface. I would question if you need to have a schedule table at all. I'd store each vehicle's details and service requirements. Personally id have different tables for make/model and the details of the items that need to be serviced and at what distances/intervals. I would have a form that looks up the make/model and works out what needs to be serviced depending upon the mileage. You need the details related to that specific car and component. Why not store them together? Unless you need to store the details of jobs done why bother storing all of the possible iterations of service schedule? Can 1 schedule be reused for many cars? Another thing to consider is where you will get all of this data from. Is it in a format that can be easily imported? Text data? Microfiche? Without data your system isn't going to be much use. And hundreds of hours of data entry is boring and expensive. Do you need to the data in database fields so you can perform calculations and queries on it? Or does the user just need to be able to lookup the details. (Ie pdf/microfiche ) If you haven't done so already i suggest that you look at a few different service manuals for different cars. You may have some different ideas after that. 

I believe what you are describing is technically ETL. However I don't think it is what people in the industry would typically consider as ETL. And Excel would not commonly be considered an ETL tool. There is a difference between experience with ETL and experience with a specific ETL tool or product. However I do think that if you explain yourself carefully that it would be beneficial to your CV. Eg: "Automated Extract, Transform & Loading of data between databases using VBA and Excel" is honest and unambiguous. In my opinion it shows a degree of understanding, technical proficiency, problem solving and experience. And I think that has merit. 

I think you should be thinking about what your end state is going to be. I.e: "What is the requirement"? The requirements should be driving the solution. Not the other way around. 

There is no perfect answer for this only you can decide the best option. And it has a lot to do with tradeoffs, and preferences. How much time you have now vs maintenance in the future. You could have: 

The links and information are confusing but we did find a solution. The short version is you need to install Visual Studio 2015. I used the free Visual Studio 2015 Community Edition. That includes the components needed to run Team Explorer. Once we installed VS the Team menu is now visible in SQL Server 2016 SSDT. You can then add and configure TFS as before. 

Running SQL 2012 SP2 (11.0.5343) on Win 2008 R2 Standard SP1 on VMWare. Edit: Updated to SQL 2012 SP3 (11.0.6020). Edit: have added more context and clarified. Following suggestion from @TheGameisWar, I queried ssisdb SELECT * FROM [SSISDB].[catalog].[event_messages] WHERE event_name = 'OnError' Returns no error messages for that date. 

Create a user db with a decent file size, and set the file growth to be 100Mb or 1Gb. Ensure each table has a primary key defined in sql server. Not just a unique index. Access wont allow you to update a sql table without a primary key defined in the linked table definition. The equivalent of an access autonumber is an identity column. If you have staging tables with duplicates its worth adding an identity column as a pkey. 

You will need to write a query or report that joins between the tables so you can select the description. The Image below shows the MS Access query designer. It displays both tables and the join between them. Below that it shows the fields I want to return in my query. 

For simplicity if this was for a limited time frame, i would go with backup and restores. Unless you are required to respond within minutes you could probably get by with daily backups. You could even set up a job to take a backup and FTP it on demand, after they have reported a defect. I wouldn't even bother restoring daily unless the issue was specific to newly created data. In my experience UAT is more about functionality. So you test with existing data and create new data. Having a db a few days behind theirs means you should be able repeat the test and reproduce the defect. This approach would get slower, more difficult and less feasible the bigger the db is. If you will be providing ongoing support, the db is huge, and you have SLA saying you have to respond within minutes, replication/synchronization of some sort might be worth the effort to set up. The best solution for you may be dictated by client security or technical policies. Work with them to ensure what you are proposing is acceptable. If not keep in mind there are lots of variations on this theme. But it all boils down to: 1. you need something to take a back up, and 2. you need something to FTP the files either on a schedule or on demand. If I was doing this personally I would begin with Ola Hallengren's backup scripts. After installing that I'd create a SQL Agent job which invokes the backup. I would then edit the agent job to add a step to FTP the files. I just had a quick search and found this example of how to use SSIS to FTP a file and I found this example of an FTP script for SQL Server. My recommendation is that you set this job up to run on a schedule, and only run it on demand if truly required. If you don't have remote access and permission to execute tasks, you can request someone on the client site execute the task for you. 

If no attributes apply the XML field is NULL. If any of those attributes do apply then the application inserts an XML document and relevant nodes. A valid example looks like this: 

SSIS and SSAS are SQL are all running locally on the same server, so I don't believe it's network related at all. The same package has been deployed and has run successfully for weeks. If we manually connect to the cube it processes without any problem. Windows Server 2012 R2 6.3 (Build 9600: ) on VMWare 64gb of RAM. Microsoft SQL Server 2016 Enterprise (SP1-CU1) - 13.0.4411.0 SQL Max mem = 48gb My gut feel is that this is more of an SSIS issue than a SSAS issue. Has anyone else encountered this? Can anyone reccomend a solution or workaround? Is it possible to get additional debugging information? 

As i understand your question you have 2 unique ids in your table. If they are both truly unique and have no other purpose (ie no business meaning) you have a redundant field. If you're developing a system that will merge data later and you want to assign a persistent unique id now then use a GUID. If you want performance and simplicity use an int. Other than enforcing uniqueness... Putting an index on a column that you won't search by is pointless. Searching on columns that aren't indexed will perform poorly. Ideally if you can enforce uniqueness and improve query performance with a single column you will have a more efficient design. 

As others have mentioned it would depend upon what you need to be able to query on. Do you need indexes? Aggregates? Foreign keys? Joins? Personally, would go with the approach of a single table with individual fields for important attributes and the rest I would store in an xml doc in the same row. Xml is flexible and therefore is ideal for optional attributes. 

The 3 options you have suggested are correct. Another option is simply upgrading SQL Server in place. If this a development instance on the same machine I would consider that as an option. Which option is best will depend on your preferences, the size of the db's and whether you need to move machines and/or whether it is important to have a fresh installation. As the difference between 2016 & 2017 is comparatively small, you could detach, move and reattach or backup and restore. In each case you should check the compatibility level and raise is it if you wish to use new 2017 features. If you are moving to a new SQL instance logins will need to be reattached. Yes you could also script the the data, export/import, use a wizard or a data comparison tool. However scripting comes with some risk depending upon the tools and formats you use to export and import the data (eg .csv BCP etc). The bigger and more complex the db is the less feasible or attractive this becomes. 

I dont think there is 1 right answer to your question. Best for what? The right answer for you will depend on factors like cost, scalability, platform preference and compatibility. And ultimately what you want to use the data for and how you expect to use it. I have done similar things in the past. In my case i've used SQL Server and MS-Access. But any database should be able to do the same or similar. Ive used powershell to iterate through folders and generate a csv file and then load into SQL. A path and file name can be stored in a single varchar field, or can be split into multiple fields. In my case i wanted the data split into multiple fields. This suited me as it allowed me to report and filter by any level very easily and the data was used by a multiuser app built in access. The important questions to ask yourself now are, what do you want to do with tha data once its within the db? Do you want to query it to produce reports? Do you want 1 row per file? What other meta data do you want to store? Size? date? File owner? Depending upon what you are trying to do you might not need to store the data in a db at all. One issue you might face is trying to keep your data in Synch. As the db will be unaware of new files, file changes, moves or deletes. For this reason you might be better off not using a db but instead reading the data on the fly as required.