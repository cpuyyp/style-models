On the other hand, without an arbitrary definition of personhood, the evolutionary algorithm would not work. You need some way of measuring how close someone is to "personhood". In addition, the Wikipedia article for the "Talos Principle" does not mention anyone ever questioning the "use" of the evolutionary algorithm, so it's possible that humans have came to a consensus about how to define personhood. We may therefore wish to defer to their consensus. (Wikipedia calls this appeal to consensus as the "Group consensus" approach to solving the Paradox of the Heap.) So, back to your question. Is Elohim a person? My response depends on whether you trust the arbitrary definition that humanity has given to personhood. 

There is a 'marketplace of ideas', a wider cultural discussion where people take up different ideas and try to show they are more effective, meaningful, or coherent, or in some other way worth consideration. People have tried to justify slavery and fascism and genocide, in cultures where it was socially acceptable. To say they can't be considered wrong or even problematic because they were socially acceptable, puts you in a hole. It's interesting to look at Marx & Nietzsche as examples of philosophers who have been more and less socially acceptable, and popular, at different times. Their ideas in some sense have to be addressed now, even by their opponents. That seems a more important marker than social acceptability. 

It is, of course, debatable whether a Strong AI even can exist. After all, philosophical arguments about the Chinese Room and Philosophical Zombies abound. However, it is possible to imagine a world where "Strong AI" is possible and yet people adhere to a philosophical argument that denies the existence of "Strong AI". It is also possible to imagine a world where humans fall prey to the AI Effect. According to Wikipedia, 

Could this scenario occur? Or would it is possible, as soon as Strong AI exist, that humans immediately come to a consensus that it does exist? 

Of course there are limits to free will. Pushed off a skyscraper you can't choose not to fall. You seem to be asking really, if we are rational beings. I have issues with how you define and explain Total War, but I would point out the decisions involved have been based on lacking or incorrect information. Like, a massive under-estimate of Russia's population, and of the ruthlessness of their leaders. Consider if the Cold War had gone thermonuclear. It's likely Total War would have ensued, until the destruction of one or both states, because of the unforgivable nature of the numbers dead. $URL$ Stupidity can absolutely trump rationality (pun intended). There are limits to individual rationality, and of collective organisational rationality. We are animals; just ones with a high opinion of ourselves. 

I have not played "The Talos Principle", so all my information came from the Wikipedia page of this game and your question. According to your question, humans developed an 'evolutionary algorithm' with the goal of developing some entity that could be classified as a "person". According to Wikipedia, an evolutionary algorithm works as follows: 

According to the 'Mario Lives!' video, researchers have been able to develop an AI unit that is able to experience emotional states, such as greed, hunger, and curiosity. If the AI is currently experiencing an emotion, it will engage in certain behaviors. For example, if Mario is very greedy, Mario will look for coins. This approach is very similar to the Sims video game series, where emotional states are also represented, and AI characters will attempt to fulfill those drives. Now, I can concede that the Mario AI is effective at simulating emotions. The media, however, has gone beyond this. Two articles, in particular, troubles me. 

Vincent VanGogh, painter of the world's most valuable body of paintings (judged by insurance valuations because no one sells them anymore), never sold a single painting in his whole life. I saw Andre Geim speak, who shared in a Nobel Prize for discovering graphene - terrible public speaker, clearly didn't want to be doing that. Dirac is probably the greatest scientist you've never heard of, because we was so disinterested in talking to those outside his field. Darwin wouldn't publish until forced to, and crucially required popularisers and advocates like Huxley, to be heard. 

I can accept the idea of Mario 'learning'. I can even accept the idea that Mario is 'self-aware' within his own environment. But I cannot intuitively accept the idea that Mario is feeling any emotions. It seems weird to think that EA programmers and German AI researchers were both able to accomplish the dreams of science fiction writers all over the world, and without anybody ever noticing. Maybe my intuition is wrong though, and maybe Mario is indeed feeling emotions (even if they are not the same emotions as humans feel). 

You might very much equally ask, why do scientists disregard philosophy, even as it directly relates to their subject? Scientists like Pinker, Hawking and Dawkins, go way beyond their subject, without the background knowledge to make real sense of what positions they take up philosophically. There is no excuse for the failing to understand the wider field, for either side. But there is a great difference in temperament, that makes it unusual for thinkers to cross and excel on, both sides divide. 

The key aspect here is the existence of "fitness", to help measure how close someone is to reaching the desired goal (in which case, the entity that could be classified as a person). This "fitness" must be a quantifiable number, so if the AI is not "fully" a person, then how close to "personhood" the AI is (so that the evolutionary algorithm could decide which AIs to breed and which to 'replace'). In this specific case, the humans have chosen to measure the "fitness" of the AIs through the use of puzzles...if AIs have successfully solved all puzzles and defeated Elohim, then the AI is considered a person. If the humans already solved the hard questions of defining personhood, then why would Elohim be intentionally designed to be a person? Elohim's goal is to supervise the evolutionary algorithm...you don't need "personhood" for that. You don't even need intelligence for that. If you can come up with a way of measuring fitness, you can attempt to run an 'evolutionary algorithm' that will maximize this fitness. The Wikipedia article does not claim that Elohim is able to reprogram itself. So if it was not intentionally designed to be a person, it never had the chance of ever becoming a person. Now, Tenrec77 argued that the players "have every indication Elohim was a person, just as much as Soma, Milton, or even you and I". That may be true. There's no reason to assume that these future humans' attempts at measuring personhood would be at all sensible. It's possible that the humans built Elohim (the evolutionary algorithm) with several "features" such as the desire for self-preservation and the ability to feel emotions such as fear, claimed that Elohim was only "20% person", and simply used Elohim as a way to build "real, true, 100% persons". The humans has tried to define a complex topic such as "personhood" by creating an arbitrary boundary between a 'person' and 'not-a-person', and then used an evolutionary algorithm to try and reach that boundary. This idea of setting up arbitrary boundaries does seem very similar to a possible solution to the classical philosophical problem: the Paradox of the Heap... 

David Deutsch argues that epistemology, theory of knowledge, is an essential pillar to our understanding of the world. Specifically (from Wikipedia) : 

In this Zen perspective, truly 'free' unconditioned action, can be recognised by others who have it. Buddha developed the reordering of mind, and then in the Zen phrase there has been a 'direct transmission from mind to mind'. That is not the whole of the Zen perspective. In the Zen view, we all have Buddha nature, the capacity for freedom and to act in an unconditioned way, in any given moment. So, also surely to recognise and transmit such, in principle. But the elusiveness, the apparent subjectiveness of this, makes it unclear whether it fulfils the questions criterion. It has been characterised as intersubjective transubstantiation [can track reference down], a depersonalisation with absolute empathy that enables a direct insight into another's mind. It is interesting to compare this to the Turing Test and it's limitations. Natural language processing has developed hugely in recent times, and this has been considered to have allowed the passing of narrow versions of the test. It is made more difficult though when we have to consider the implications of humans failing such narrow versions of the test. It seems our minds are capable of generating tangled hierarchies, reinterpreting statements and ideas by stepping outside of dialogues and their assumptions, that is we are Strange Loops. Can we evaluate when one occurs? Only with the next challenge, some question and answer which has no precedent. Like the Zen master and student, saying redefine yourself, in this moment. Can we establish an objective measure of passing this? I would say no. But it can be falsified. Freedom cannot be unambiguously demonstrated, but unfreedom can. When a statement or an act fails to manifest a redefinition of yourself towards freedom.