IMHO you can reasonably live with those limitations. If you are looking for simple mirroring, PXC would be just fine because all DB writes would go to just one server and synchronous replication would take place at COMMIT time. I have evaluated this product and it seems very sound so far... 

I am sorry to have to inform you, but cannot be rolled back. In fact, triggers an implicit commit. According to Page 418 Paragraph 3 of the Book 

Either the rules have changed, or somebody missed something in compiler builds. According to the MySQL Documentation on ENUM 

IHMO Although budgets may be a driving force to design/infrastructure decisions, I would easily be in favor of separates databases per client. 

This may not be all that useful because may be about the same size as since has no value. Notwithstanding, you would run that query to see the temp table size in gigabytes. 

EPILOGUE Sorry about the corrupt file. You must see to that. Yet, these steps will help clean up the data dictionary and restore the to some sense of normalcy. 

YOU ABSOLUTELY, POSITIVELY, CANNOT DO THAT. Why ??? Back on , I wrote the answer to Cannot GRANT privileges as root I carefully explained how the grant columns existed from MySQL 4.x to 5.6. I have explained this many times (See my others). The columns in the table need to be in the exact position they are published along with the exact column names. On , I answered MySQL service stops after trying to grant privileges to a user where I gave an example of how you can hack repairs into to comply with the next version. I highly recommend you remove your custom column today !!! Otherwise, you will NEVER get all your grants back. Please go manage your database user identification in another database. 

There are two things to consider Stored Procedures/Functions are physically stored in mysql.proc as a MyISAM table 

If you see multiple entries in the , you have multiple MySQL instances running. NEXT STEP Judging from the date and time of each ibdata1 file and their respective folders, my guess is that you have MySQL Enterprise running and it has its own MySQL instance. You need to check the MySQL Enterprise Documentation on moving Enterprise Monitoring Information to anther disk location. As for , you need to locate the being used. You should be able to have MySQL Enterprise Monitor tell you. In the event MySQL Enterprise Monitor does not any mechanisms for editing the datadir or migrating the MySQL data folder, you could also get it from the entry: 

Look at the Picture. What components are essentially for InnoDB's self healing (sounds better that crash recovery)? 

Once you do this, you should have SELECT access to the table. To make sure, run You should also be able to see what table-level access is granted to 

TWO TABLES IN A SPECIFIC DATABASE If you want to know if two tables are different in database , run this 

From the output in your question, does not occur three times consecutively. I have an iterative solution using user-defined variables MY PROPOSED QUERY 

This query will not get every . Why? If there is an in that is missing in , that does not write a zero in the column. To cover for values missing in , run this one: 

The DB Connections 12287 and 12279 ran one INSERT each Here is what I notice: In each INSERT, you are running to fill in . This would cause additional locks because a query's result is needed to fill while other indexes are made to wait for the materialization of 's primary key. This deadlock lasts a little longer that normal because of this intermittent need to run a . SUGGESTION Instead of doing 

Whenever you did the compression this way, find out what data_length is for . This will give you an idea what the actual size should be. 

Run this same query against more recent tables and see if the slowness is just O(n) (order n) running time based on the amount of recorded data. 

I have MySQL 5.6.21 on my laptop running Windows 8.1 and it is doing nothing. It still has that state. Please keep in mind that this state exists in the , in the table , in the column . Please note the MySQL Documentation on PERFORMANCE_SCHEMA.THREADS.PROCESSLIST_INFO 

Based on this, you must increase the join_buffer_size in your session just before running your . From the look of the join's clauses, I don't foresee indexes ever being selected by the query optimizer. I cannot tell you how big to make the join_buffer_size. You will have to experiment with it and run the plan to see if the will avoid writing to disk. WARNING : According to the last paragraph of MySQL Documentation on : 

Epilogue Bottom Line: foreign_key_checks is not meant of use in . That make the most sense because (if allowed) starting mysqld with that would be damaging to referential integrity from startup. 

You may not find any such information for GCP because Aurora is a storage engine native to Amazon. It can be used with MySQL or PostgreSQL. You only have three(3) options here: OPTION #1 You need to read the Amazon Documentation on Aurora and try to locate the same storage options and features in CloudSQL. Thoroughly scour the GCP Documentation for any new storage options for CloudSQL similar to Aurora. OPTION #2 You should also focus on what Google provides for CloudSQL High Availability 

I tend to be a little conservative and cautious when using FULLTEXT indexes because FULLTEXT indexes has a nasty habit of playing head games with the MySQL Query Optimizer. Try getiing all monkey rows first in a subquery, and then eliminate "monkey business": 

Since the short_url is a keyword, maybe refactoring the query could be a game changer Start with getting the top 10 keywords whose count > 50 Then, join the 10 words to the other tables 

What is so special about the number 251 ? According the book Understanding MySQL Internals, the last paragraph on page 74 says: 

These 10 tables are rarely used. Their contents are loaded into RAM on startup and then referenced from RAM. There is no need for them to be InnoDB because they are system-level tables that should not be changed via transaction. Look at the potential problems that could occur if these 10 tables were InnoDB: 

are there for preparing for the Developer and DBA Certification Exams. Get the book, study those chapters well, and you could do great things with MySQL's INFORMATION_SCHEMA. The INFORMATION_SCHEMA database as of MySQL 5.5, now features plugins, global variables (status and static), session variables (status and static), storage engine status, performance metrics instrumentation, trigger map, events (programmable) and much more. Sorry this may seem like WTMI but I am a big proponent of using the INFORMATION_SCHEMA database. 

Give it a Try !!! CAVEAT Since the default for innodb_file_per_table is ON, each InnoDB table being optimized will actually have its file shrunk. Any MyISAM tables (which I hope you don't have) will also have their and files shrunk as well. UPDATE 2012-09-18 18:19 EDT I just tried the following: 

Don't worry about putting 106G inside ibdata1. MySQL was running with innodb_file_per_table=0 before MySQL 5.5. It can still handle it. Give it a Try !!! 

Then, his algorithm worked perfectly. SUGGESTION Instead of putting single quotes in , do it without the single quotes. Do the delimiting before processing the query with this 

I am just giving ideas. I'll leave it to you to implement. Perhaps, to give you some UI ideas, see the site I use to commute : $URL$ . The result of the route you pick in that site shows route departures within a 5-hour window (2.5 hours before and 2.5 hours after). Give it a Try !!! 

Since you are using Amazon RDS, ask your DB Administrators to disable it. If your DBs are in a Shared Resource, my condolences... UPDATE 2012-07-25 14:45 EDT If you can download the RDS CLI, you can adjust in a customized DB Parameter Group. You will have to do the following: 

You are not given SUPER privilege and there is no direct access to my.cnf. In light of this, in order to change my.cnf options for startup, you must first create a MySQL-based DB Parameter Option List and use the RDS CLI (Command Line Interface) to change the desired Options. Then, you must do this to import the new options: 

By means of a full table lock as well as the Clustered Index, all INSERTs, UPDATEs, and DELETEs are held in abeyance until the completion of the index creation. There are also too many moving parts to the lifecycle Can I setup a temp table for index creation? One could attempt to use an external table as follows: 

You could run against all your InnoDB table to provide some shrinkage. See my 5 year old post Why does InnoDB store all databases in one file? for ideas on how to shrink your tables. Unfortunately, you cannot do that in you present state. See this YouTube Video. As for your not being able to list your databases, please note this: 

The mysqld process will rotate to the next log by itself when the current binary log reaches 1G (default is 1G, but you can change that by setting max_binlog_size). Unless you set the expire_logs_days option (By default, it is 0), no logs will disappear. Keep in mind that restarting mysql also causes log rotations. If you want to delete logs manually, DO NOT DELETE THEM IN THE OS. Please use this command : All logs before mysql-bin.000200 will be deleted. You could also delete logs by timestamp: There are no other drawbacks other than wasted diskspace, especiall if the logs have outlived their usefulness. Storage Engine usage in independent of Binary Logging. DDL for MyISAM and InnoDB can both recorded in the binary logs. 

UPDATE 2014-07-23 16:20 EDT I just realized, your units are in minutes not seconds Here is the quick fix 

The error message you are getting comes from that validity check by the Replication Threads. What's funny about this is that the book I quoted was made in April 2007. There have been more binlog events added to MySQL since that book's publication. Thus, there will be occasions when a Slave cannot interpret a binlog event and just consider the relay log corrupt. (It's like a conversation between two people from England and Brooklyn USA, or Brazil and Portugal, or Spain and Puerto Rico). The slight shift in vocabulary between binlogs from two different versions of MySQL is just being misconstrued as unintelligible between them. About six(6) weeks ago, a client was using 5.5.30 Master and 5.6.21 slave (On the path of migration to MySQL 5.6.21 (Master and All Slaves)). Although this replication setup works at present, there have been occasions when the Slave would break because it could not unpack a binlog event from an older Master. I learned of this phenomenon from my boss when he showed me the error in the error log. It had a rather cryptic error message (when MySQL Replication would either start or rotate relay logs) with the word (You should do and see if this is the case for you) Here is a snippet from that error log: 

Connecting to the databases Schemata Retrieval and Selection Reverse Engineering Object Selection Migration Manual Editing Target Creation Options Schema Creation Create Target Results Data Migration Setup Bulk Data Transfer Migration Report 

OMG 220G out of 240G ??? WAY TOO BIG !!! Take a look at this Pictorial Representation of InnoDB from Percona CTO Vadim Tkachenko 

Since COMMIT and ROLLBACK govern data visibility, READ COMMITTED and READ UNCOMMITTED would have to rely on structures and mechanisms that record changes 

Other than doing point-in-time restores, there are two additional reasons a Slave could have binary logging enabled. REASON #1 If a Slave is also a Master 

EPILOGUE All files will vanish and all InnoDB tables and indexes will exist inside ibdata1 Give it a Try !!! 

This shows that could reduce the amount of network traffic by cutting out requests. If you have a good network with 10Gig E, this may not be an issue. Summary Think of the tradeoff you were proposing: If you are qualifying all tables with the database name prepended, then you are transmitting more bytes in the SQL text and giving the Query Parser more to work with anyway. Another way to approach it would be to have a different DB Connection connecting to a separate database. That way, one DB Connection would not have to switch databases and run up the counter and add to the network traffic. The tradeoff would be to have multiple DB Connections per client (requiring more memory). In other words, something has to bite the bullet in the end. Again, I suggest leaving everything alone because all the alternatives require major code changes without knowing if the enhancements were worth it in the end. 

See the Pictorial Representation of InnoDB's Architecture to see the Buffer Pool and Transaction Logs Every time you read from or write to an InnoDB table, the is checked (disk I/O), the data dictionary in is checked (disk I/O), and MVCC for transaction isolation is written in ibdata1 and transaction logs if there are incoming writes to the same table (disk I/O). Even a read-heavy InnoDB environment incurs disk I/O. EPILOGUE You could easily expect some pages to be rotated out. This is especially the case when the caches are configured too small. In the case of MyISAM, since data pages are read from disk each time, index pages can rotate in and out. As for InnoDB, it's data and index pages. The result sets may have to be recreated each time. Sometime, this contribute to all caches activity. 

According to the MySQL Documentation on SHOW SLAVE STATUS\G Either SUPER or REPLICATION CLIENT should do it. I would go with the minimum: 

You can set up a crontab job to do this every hour You can also schedule a nightly job to defrag the table with this: 

You could try the following to improve things skip-host-cache and skip-name-resolve Please follow these steps: Step 01) Add the following to /etc/my.cnf 

Keep in mind that these are living, breathing entities. Do no bother run a . By the time you do, queries have disappeared and other queries have materialized. Here is a sample: 

You really need to unify all Date and Time Stamps into one format for both and . Just to give you an example, I'll compute the time difference for the first row. Here is the code to compute it: 

While the query works for your sample data, you have to pay close attention to how name relates to country. You would have to make sure of three things in order for my query work