Let $m$ cobb-douglas. This means $m(u,v) = A u^\alpha v^{1-\alpha}$ for $\alpha < 1$ and $q(\theta) = A \theta^{-\alpha}$. Then $$\frac{\theta q'(\theta)}{q(\theta)} = -\alpha > -1$$ To what extent this holds for other functional forms I'm not sure, but I haven't seen anything but Cobb-Douglas being used as the matching function. 

So they're interested in two related elasticities, one of which was the one you initially suspected. Their table 2 will give you some estimates of those elasticities. 

One approach: Look at the unscaled preferences $$U(c,l) = \left(c v(l)\right)^{1-\sigma}$$ A log transformation reveals $$ (1-\sigma)\log\left(c \right) + (1-\sigma)\log v(l)$$ Let $v(l) = l^\gamma$. Then, $\log v(l) = \gamma \log l$. As $(1-\sigma)$ premultiplies both terms, the degree of curvature in $v(l)$ is the only way to shift the level of leisure. 

That's at least what I thought. Denote $P(M) = 1$, people toss a ball that always lands in one urn. Say $g(x) = 1/4$ for $x\in [0, 3]$: The tossing is in a way that applications are equally distributed over vacancies with boundaries 0, 3. I think that $$ E[x | x_h == 0 \wedge M] = \sum_x P(x | x_h == 0 \wedge M) x \\ = \frac{1}{P(x_h == 0 \wedge M)}\sum_x P(x \wedge x_h == 0 \wedge M) x$$ Where we can rewrite $$ P(x_h == 0 \wedge M) = \sum_x P(x_h == 0 \wedge M \wedge x)\\ = \sum_x P(x_h == 0 | \wedge M \wedge x)$$ As I said, $P(M) = 1$, and $P(x_h == 0 | \wedge M \wedge x)$ is the binomial chance of drawing $0$ high-types out of $x-1$ applications with probability $1-P$. All sums start at $x=1$. With the numerical example I gave, and $P=0.5$, I have 

What are you trying to do? I hope you're not after finding evidence for or against the Laffer Curve, because your approach pretty much will not be able to say anything about that. 

I have the following system of equations $$ \rho V(u, \epsilon^i) = F(u, \epsilon^i) + V_u(u, \epsilon^i)g(u, V(u, \epsilon^i) + \lambda^i \left(V(u, \epsilon^{-i}) - V(u, \epsilon^i)\right)$$ with $i \in \{0,1\}$, so that $\epsilon$ can take two values and $g(u, V)$ known and initial $u(0), \epsilon$ given. $g(u,V)$ is the law of motion for $u$, $\dot u(u, \epsilon^i)$ if you will - but it depends on the value of $V$. This looks like a HJB, but crucially, there is no optimization involved. This is a macro-labor model, where the amount of job openings depends on the value of a filled position ($V$), which is why the law of motion for $u$ (unemployment) depends on the equilibrium value of $V$. How could I compute (numerically?) the value of $V$ that solves this ODE? 

Kimball (1995) defines production of the final good ($Y$) with intermediate goods $y_l$ in his equation (1) as $$ 1 = \int_0^1 G\left(\frac{y_l}{Y}\right) dl $$ with $G(1) = 1$, $G'(x) > 0$ and $G''(x) < 0$. I have never seen such a construct before. What does the $1$ on the left-hand-side mean, is that a normalization or some cost construct? An introduction into this type of production function would be greatly appreciated. 

An additional reason why house prices are not included in the inflation measure is that some housing contracts are indexed by inflation. I seem to remember that this was/is the case for the UK [citation needed]. If then you include housing prices in your inflation measure, you get loops/amplification of nominal inflation: 

draw samples from the full annual table, each row with equal probability Stop when number of rows is equal to the actual data I then apply the same estimator as before to the sample, each row weighted with 

What is the data corresponding to that? Since his publication was from 2005, he must be referring to data on vacancies before the big recession. However, the standard data on job openings, JOLTS, is only available from 2005 onwards. 

1. is generally not accepted, both empirically and theoretically (remember, we're here not talking about individual preferences, but under aggregation). 2. is more disputed, but elasticities of substitution found typically range between $0$ and small positive amounts, not strongly negative as required here. So, to conclude, temporarily, $C$ responds positively to a positive TFP shock. Permanent Shock Yes, a temporary shock could end up however, given how we calibrate it. But here's the gist: 

Denote the total size of the world as $N$. Denote urns as $U_N$, balls as $B_N$, and say that we randomly toss every ball into an urn. Urns and balls are "scaled", i.e. $U_N = uN$, $B_N = bN$, for two constants $u$, $b$. Then the distribution of balls for any urn is Binomial $$ Prob(k \text{ balls arrive}) = Binomial(k; n=B_N, p=1/U_N)$$ In the next step, I want to let the number of urns and balls go to infinity. Note that if $np$ converges to a finite number as we let $N\to \infty$, the distribution of ball arrivals for any particular urn is approximated (pretty accurately) by $$ Prob(k \text{ balls arrive for N ``large''}) = Poisson(k; \lambda=np)$$ And indeed, due to the setup, $np = B_N/U_N = b/u$ converges as $N\to \infty$. 

I'm using CPS monthly individual data which comes with as a float weight. I'm computing some annual statistic for unemployed. I do that by 

No you are not. Preferences determine the equilibrium, even if they are identical, because they determine the value of the endowments. Consider two agents, one ("A") having ice cream only, and one ("B") having lava only. Case 1: Both hate lava Assume lava burns tongues and is useless (typical Economist). Then, the initial endowments of B have zero value, as no one wants them. The equilibrium allocation equals the initial allocation. Case 2: Both find lava useful Now, imagine lava being useful for lava lamps (whatever). They still have identical preferences, but now they attach some positive value to lava. The value of B's initial endowment has changed, and hence he can trade at least some of it for ice cream. The equilibrium allocation no longer equals the initial allocation. 

An even better example is airplanes travelling around with emty business / first class seats. They could always move someone from economy for a small fee, but prefer "losing out". Why? Because if people anticipate that they might get lucky and get a low-price first class offer, they would not pay around 7-10k for an intercontinental flight. In order to control these expectations, you better not do this at all when you can help yourself. 

I don't think I've seen cases with a kink in continuous time that are solved numerically. Aguiar & Amador (2014) have a kink in their HJB, but they solve their HJB analytically. Validation I don't know to what extent Moll et al's algorithm is going to get that right. You will most likely get weird behavior around the kink, but whether it's the real functional form is to be doubted, and you'll have to argue ex-post that "it looks okay alright?". If you want to get a better grip into whether the kink looks alright, make it more extreme. Without going through your equations, it seems that the first part is convex and the second part is linear. Increase the curvature for the first part so you can "see" more easily whether it still looks correct around the kink. Also, you definitively should look at the numerical counterparts to the first-order and second-order derivative, the first-order and second-order difference of the approximation divided by the step size. Do they have the correct signs around the kink? One thing I'd emphasize is having many grid points around the kink. However, since the algorithm gets tougher when you have a nonlinear grid (because the grid step size is now not a constant, but a vector), it might just not be worth it and you could rather jack up overall grid points and keep it linear. Speed Finally, you really should look into having sparse matrices for the A and B matrices. 10 iterations per minute is really slow. It takes you perhaps 30 minutes to understand and implement (if you've never worked with sparse systems before), but already after the second time you run the code you've made up that time. If the problem persists, have a look at profilers for your coding language. For Python, this is a nice solution. Even if you have everything sparse, perhaps you forgot some matrix that you don't even need in the iteration loop. That was the case for me, and with large grid sizes that single reinitialization took up a lot of time. Profiler will tell you what's what.