Define the seminorm on the space $S=[0,1]\times[0,T]$ $$\mid u\mid_{\alpha} = \sup\frac{|u(x, t) - u(y,s)|}{(|x-y|^2 + |t-s|)^{\frac{\alpha}{2}}}.$$ Define the norms on the same space $$\lVert u \rVert_{C^{0, \alpha}} = \lVert u \rVert_{C^0} + \mid u\mid_{\alpha}$$ and $$\lVert u \rVert_{C^{2, \alpha}} = \lVert u \rVert_{C^0} +\lVert u_x \rVert_{C^0}+\lVert u_{xx} \rVert_{C^0}+\lVert u_t \rVert_{C^0}+ \mid u_{xx}\mid_{\alpha} + \mid u_t\mid_{\alpha}.$$ Suppose that $\lVert u \rVert_{C^2, \alpha} \leq C$ where $C$ is a constant. Let $a, b, c \in C^{0, \alpha}$. How can I show that $$\lVert au_{xx} + bu_x + cu\rVert_{C^{0, \alpha}} \leq K\lVert u \rVert_{C^{2, \alpha}}$$ for some constant $K$? Or equivalently, want to show that $$\sup_{\lVert u \rVert_{C^{2,\alpha}} \leq C_1}\lVert au_{xx} + bu_x + cu\rVert_{C^{0, \alpha}} \leq K_1$$ (ALL the above norms are over the compact set $S$). Thanks for any help 

Hi, I am interested in learning a bit more about this space. I have exhausted all the books available at my disposal, and none of them explain much of the basics for me. Here's a definition of this space. The seminorm is $$[u] = \sup_{(x,t), (y,s) \in Q} \frac{|u(x,t) - u(y,s)|}{(|x-y|^2 + |t-s|)^{\frac{\alpha}{2}}},$$ and norm $$ \lVert{u}\rVert_{{C}^{k, \alpha}(\overline{Q})} = \sum_{i+2j \leq k} \lVert{\frac{\partial^{i+j}u}{\partial x^i \partial t^j}}\rVert_{C(\overline{Q})} + \sum_{i+2j = k} \bigg[\frac{\partial^{i+j}u}{\partial x^i \partial t^j}\bigg]. $$ Would someone please explain to me why the parabolic Holder space norm is chosen in the way that it is? For example, why aren't we interested in the quantity $u_{xt}$? Because it doesn't pop up in PDEs very often? Why only take the highest order seminorms in the norm? Also, in the denominator of the expression for seminorms, usually we have the spatial $|x-y|$ term to a power higher than the $|t-s|$ term (eg. $|x-y|^2 + |t-s|$). Why is this? Also, there are a number of different definitions for the norm of these spaces. Since these are norms we equip these spaces, are they somewhat equivalent? Does it really matter which one we use? 

Suppose $N=\|(I+K)^{-1}\|$. In order to calculate $N$ one can do the following. Write $$N^2= \\|(I+K)^{-1}(I+K^\*)^{-1}\\| = \\|(1+K+K^\*+K^\* K)^{-1}\\| = 1/(1+\lambda),$$ where $\lambda$ is the smallest eigenvalue of $K+K^\*+K K^\*$. Try to calculate it using your explicit formulas. 

I don't know the answer, just some thoughts. Suppose you have $n$ vertices of the first type and $l$ of the second. Of course, you know the answer if there is no restriction on the number of edges (you want it to be odd). Therefore equivalently I can calculate cases, when it is odd with coefficient (-1) and cases, when it is even with coefficient 1 (I denote this amount by $N$). Let $y_1,\dots,y_n,z_1,\dots,z_l$ be elements of $\mathbb{Z}\_2=\mathbb{Z}/(2\mathbb{Z})$, corresponding to the vertices of the graph. Edges are given by $n\times l$ matrix $A$. Then $N$ is the coefficient of $h^m$ in $S(h,h)$, where $S$ is defined by $$S(h,g)=\sum_{y\in\mathbb{Z}\_2^n,\;z\in\mathbb{Z}\_2^l} h^{|y|} g^{|z|} (-1)^{\sum_{i,j}y_i A_{ij} z_j}.\tag{1}$$ Here $|y|$ is amount of components in $y\in \mathbb{Z}\_2^n$ which are equal to $1$. One can get rid of the sum over $y\in\mathbb{Z}\_2^n$ in the following way: $$S(h,g)=\sum_{z\in\mathbb{Z}\_2^l} g^{|z|}\prod_{i=1}^{n} (1+h(-1)^{\sum_j A_{ij} z_j}).\tag{2}$$ This for example solves the problem in two cases: 1. we fix the number of vertices only in one part of a bipartite graph and the map $\mathbb{Z}_2^l\to\mathbb{Z}_2^n\colon z\mapsto x$ with $x_i=\sum_j A_{ij} z_j$ is surjective (in this case we can omit $g^{|z|}$ in $(2)$, make described change of coordinates and apply again the trick we used to get (2) from (1)); 2. amount of vertices in one part of a bipartite graph is small enough (in this case the sum (2) has small enough number of terms and each of them can be calculated in a polynomial time); 

Let $x,y$ be two nonzero complex vectors, let $\hat x=x/\|x\|$ and $\hat y=y/\|y\|$, and consider the parabola $$\phi(t)=\|t\hat x+(1-t)\hat y\|^2=1+2(t^2-t)(1-\Re(\hat x \overline{\hat y})). $$ You easily check that $\phi(t)\ge\phi(1/2)$ for all $t$. This gives the inequality $$ \|t\hat x+(1-t)\hat y\|\ge \sqrt{\frac{1+\Re(\hat x \overline{\hat y})}2} $$ for all real $t$. This is stronger than your inequality, which can be obtained by choosing $$t=\frac{\|x\|}{\|x\|+\|y\|}$$ at the left hand side, and noticing that $$ \sqrt{\frac{1+\sigma}2}\ge\sigma $$ for all real $|\sigma|\le1$ at the right hand side. So yes, the correct extension is using $\Re(x \cdot\overline{y})$ instead of $x\cdot y$. 

Let me try again (I deleted an earlier wrong post). First of all, a sufficient condition for subadditivity on $x>0$ is: $f(x)/x$ nonincreasing. This is easier to work with since it is a local condition. The proof is elementary (take $x\ge y>0$, then $f(x+y)\le f(x)(x+y)/x=f(x)+yf(x)/x\le f(x)+f(y)$). For smooth $f$ this is equivalent to $f'\le f/x$. Thus it is sufficient to look for a non concave function $f(x)$ which is smooth on $(0,\infty)$, has right limit zero at zero, and satisfies on $x>0$ the inequalities $$ 0\le f'(x) \le \frac f x.$$ Now, take $f=x^a$ with $ 0 < a < 1 $; we may start with any example, but just to fix the ideas. This is a concave function satisfying all the requirements of the problem. Notice that actually there exists $d>0$ such that $$ d\le f'(x) \le f'(x)+d \le \frac f x \text{ on } (0,1].$$ Any function satisfying this condition is a good starting point. We shall modify $f$ on a compact subset of $(0,1)$ so to make it non concave near a point. Let us take a smooth non negative cutoff $g \in C^2_c(0,1)$ such that $|g'|\le 1$ and define $f_t=f+tg$, for $t$ a small positive constant. We have $f'_t=f'+tg'$, so if we restrict $t$ to $ 0 < t < d $ we have $$ 0 \le f_t'\le \frac {f_t} x $$ everywhere. It is clear that $f_t$ for all $ 0 < t < d $ satisfies all the requirements of the problem. Can we choose $g$ so to make $f_t''(1/2)>0$? We have $$ f''_t(x_0)=f''(x_0)+t g''(x_0) $$ so in conclusion we are looking for a smooth function supported in a compact subset of $(0,1)$, non negative, such that $$ |g'|\le1 \text{ and } g''(1/2)\ge N $$ for a fixed $N$ arbitrarily large. This obviously exists, e.g. take a piecewise linear function and approximate it with test functions. 

$$q^{-[n/2]/2}\sum_{k=0}^{[n/2]}(-1)^k \sum X_{j_1}\dots X_{j_{n-2k}},$$ where the second sum is over sequences $0=j_0,j_1,\dots ,j_ {n-2k},j_{n-2k+1}=n+1$ such that $j_0< j_1< \dots< j_ {n-2k}< j_{n-2k+1}$ and $j_{l+1}-j_l$ is odd for $l=0,\dots,n-2k$. Is not this a simple expression you are searching for? 

I've tried to fit a $n(x)$ by $\Re(f(x))$ with $f(x)=1+\sum_i\frac{c_i^2}{x-a_i-i b_i^2}$ with $c_i,a_i,b_i\in \mathbb{R}$. Here is the result of the fitting: 

Suppose, for simplicity, that $N=\infty$, so you are just taking values from a distribution. In order to answer your question, you should have some prior knowledge about this distribution (i.e. a probability distribution in the space of all distributions). I will give you two examples of this. Example 1: Suppose, you are doing (many times) some experiment, which has 2 results: 0 or 1. You don't know the probability p of "1", any value $p$ from 0 to 1 is possible. Then you can formalize your knowledge as the following: $p$ is uniformly distributed on $[0,1]$. Suppose, that after a few experiments you have got the sequence w="1010110111", you can write the formula, for a posterior density $f(p)$ of $p$. In general, if you have $n$ zeros and $m$ ones then $$f(p)=\frac{(1-p)^np^m}{\int_0^1(1-p)^np^mdp}=\frac{1}{n+m+1}\begin{pmatrix}n\\\\n+m\end{pmatrix}(1-p)^np^m.$$ This formula is just a continuous version of Bayes' theorem Because mean of the result of experiment is exactly $p$, the formula, written above, is exactly formula for the distribution, you are searching for. If w="1010110111", then n=3, m=7 and this distribution looks like this: 

(I asked this question on MSE but I did not receive an answer so I hope I can post here.) Let $S$ be a compact set in $\mathbb{R}^2$ and let $C^{k, \alpha}(S)$ denote the usual Holder space with $k$ continuous derivatives and finite $k$-th order seminorms with exponent $\alpha$. 1) Is it true that if $f \in C^\infty(S)$ and $u \in C^{k, \alpha}(S)$, then $f(u) \in C^{k, \alpha}(S)$? I don't know how to show that the seminorm part (which involves supremums over the composition divided by a distance involving the arguments) is finite. 2) Is it true that if a sequence $u_n \to u$ in $C^{k, \alpha}(S)$, and if $f \in C^\infty(S)$, then $f(u_n) \to f(u)$ in $C^{k, \alpha}(S)$? I think so, since this is true for ordinary $C^k$ space so the "norm part" of the $C^{k, \alpha}$ norm converges, but again I am not sure how to show that the seminorm part of the $C^{k, \alpha}$ norm converges. And I guess if this works for Holder space, it'll work for parabolic Holder space too. Parabolic Holder space is defined as follows. The space $\widetilde{C}^{k, \alpha}(S)$ has the seminorm $$u_\alpha = \sup_{(x,t), (y,s) \in S} \frac{|u(x,t) - u(y,s)|}{(|x-y|^2 + |t-s|)^{\frac{\alpha}{2}}},$$ and norm $$\lVert{u}\rVert_{\widetilde{C}^{k, \alpha}(\overline{S})} = \sum_{i+2j \leq k} \lVert{\frac{\partial^{i+j}u}{\partial x^i \partial t^j}}\rVert_{C(\overline{S})} + \sum_{i+2j = k} \bigg[\frac{\partial^{i+j}u}{\partial x^i \partial t^j}\bigg]_\alpha.$$ I'm grateful for any help. Thanks. 

One of my favourite applications of 'basic' distribution theory, which actually requires most of the tools in your list to be fully apreciated, is the Malgrange--Ehrenpreis Theorem on the local solvability of arbitrary constant coefficients PDE. There are a few proofs, but the proof in M.Taylor's book on Pseudodifferential Operators (last Section of first Chapter) is especially suited for your course I think. It is a substantial theorem though, so it might or might be not a good idea to include it, depending on the length and size of the course. 

To my taste, the cleanest approach to the extension problem is contained in Stein's 1970 book "Singular integrals and differentiability properties of functions". For a bounded open set with Lipschitz boundary, he constructs a 'universal' extension operator which, when applied to a $W^{k,p}$ function on the open set, produces a $W^{k,p}$ function on the whole space, with support in a fixed neighbourhood, and suitable bounds on the norm. The method also works if the set is unbounded, provided some uniform bounds at infinity are assumed on the coordinate patches at the boundary. It is not necessary at any step to assume continuity or smoothness up to the boundary. As Pietro mentions in his answer, the extension property is blatantly false if the boundary contains inward cusps, but I understand from your question that your boundary is smooth hence at least Lipschitz. 

Assume we have a $3\times 3$ grid with rows and columns being short exact sequences of $C^*$-algebras. This gives a grid of 6-term exact sequences: 3 "horizontal" sequences and 3 "vertical" sequences, forming a torus-like diagram with 18 groups and 36 maps. Commutativity of 16 out of 18 squares follows from naturality of functors $K_0$, $K_1$ the index map $\delta_1$ and exponential map $\delta_0$. Two squares remain, and it seems, that they are "anti-commutative", but I didn't manage to prove this. These squares are the following. 

P.S. Here is a proof I came up with after some trial and error. We would like to define $F=f(PP^*)$, where $f(x)=0$ if $x=0$ and $f(x)=1$ otherwise. To check, that $F$ is well-defined we need to check, that $f$ is continuous on the spectrum of $PP^*$. To do this it is enough to show, that the open interval $(0,1)$ doesn't intersect with its spectrum or, equivalently, that $(2PP^*-1)^2\geq 1$. Expanding we see, that this is equivalent to $PP^*PP^* \geq PP^*$. Define $A=P-P^*$. Then $A^*A\geq 0$. On the other hand expanding we get $A^*A=PP^*+P^*P-P-P^*$, so $PP^*+P^*P\geq P+P^*$. Multiplying by $P$ on the left and by $P^*$ on the right we get $PP^* + PP^*PP^*\geq 2 PP^*$ or $PP^*PP^*\geq PP^*$. Thus $F=f(PP^*)$ is indeed well defined. Since $f(x)$ is real valued and $f(x)=f(x)^2$ we have $F=F^*=F^2$. To show, that $FP=P$ notice, that $f(x)x=x$ for $x\geq 0$. Therefore $FPP^*=PP^*F=PP^*$ and $(FP-P)(FP-P)^*=(F-1)PP^*(F-1)=0(F-1)=0$. Thus indeed $FP=P$. Thus we have $$P=F+(P-F)=F+F(P-1).$$ Finally, notice, that $(P-1)F=(P-1)PP^*F=0P^*F=0$, so $P-1=(P-1)(1-F)$ and $$P=F+F(P-1)(1-F).$$ 

I'm not sure I understand your question since the answer seems too simple. You must first choose a space of vectors for the matrix to act upon. A natural choice is $\ell^2$, then the natural assumption is that the operator induced by $A$ has a selfadjoint extension, in which case standard spectral theory applies, in particular the spectral theorem gives you what you need. This makes the meaning of 'diagonalizable' precise. Notice that your assumption is that the operator $A$ is symmetric, which is weaker than selfadjoint (but not too much weaker). Other choices are possible of course. 

Very interesting question. As a prominent harmonic analyst told me recently, when I asked him where I could learn to make explicit computations on hyperbolic spaces: "not easy to find references, and it's all Sigurdur Helgason's fault". He was joking, of course, but basically he meant: there is now an implicit understanding that for each one of your questions there's a formula somewhere in some book of SH, so why are you asking? read the books. But on the contrary, those elegant and general formulas are of no help if you really want to compute something: basically you still need a lot of work, choose proper coordinates, write down explicit formulas for every Harish-Chandra thing and so on. A slower development of the subject would have been more helpful, by now we'd have available books on special cases with explicit formulas and so on. More to the point: a beautiful example of an explicit computation using the Haar measure on $SO(3)$ is this paper on endpoint Strichartz estimates for the cubic Dirac equation. The computation is quite elementary, so you will not have troubles in reading it in case you're interested. I find it a compelling example of how useful it would be to develop some more machinery to work with Haar measures. 

Consider a space $X$, whose points are right and left limiting points of $[a,b]$, i.e. points of the form $x+$ and $x-$, where $x\in(a,b)$ and points $a+,b-$. If $f\colon [a,b]\to \mathbb{R}$ is a Cadlag function, you can define $f(z)$ for $z\in X$ by formulas $f(x+)=\lim_{y\to x+} f(y)$, $f(x-)=\lim_{y\to x-} f(y)$. Moreover, you can define a topology on $X$ such that function on $X$ is continuous iff it is obtained by the procedure described in the previous sentence from Cadlag function on $[a,b]$. Space $X$ with this topology is compact and you can apply Arzela-Ascoli theorem. The result of this is the following: 

Not a good fitting :(. This is actually why I ask this question. But note, I haven't used the data I have for k(x). Let's check, how is it approximated by $\Im(f(x))$ (with coeffitients from the previous fitting). 

This is not a complete answer, but a collection of observations (mainly about part (1)), which may be useful for the problem. Let's denote maximal determinant with $D_n$. Case $n=1$ is a trivial exceptional case, where determinant is uniformly distributed on $[a,b]$. So we can assume, that $n\geq 2$. In this case the minimal value of determinant is $-D_n$, because we can always permute two rows. Moreover, by the same reason, the distribution of the determinant is symmetric with respect to 0. Lemma: the answer to 1 has the form $D_n = (b-a)^{n-1} \max_i (c_i a + d_i b)$, where $c_i,d_i$ are some integers, $i=1,\dots,i_{max}(n)$. Proof: First, note, that the determinant is linear in each of matrix elements. Therefore any local (and hence global) maximum and minimum will be achieved for the extremal values of the elements. That is, we can assume, that each matrix element is either $a$ or $b$. Let $A$ be such a matrix. Do the following: