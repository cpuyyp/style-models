The general answer is "no". uid and gid on the filesystem will be as set at the moment of the write and if they don't match on a different machine, then privileges won't match either. If you do not want to make a small revolution with uids/gids on several machines, you could try using acls to set the desired permissions for all desired users on all machines. I suspect this will use numerical uids internally, so it could happen, that giving access to your account foo on machine A, uid a, will give access to your files to a random guy bar on machine B, uid a. It also seems like it's more hassle than it's worth. I think that saner approach would be to use tar to migrate your development tree. I have also had a half-baked idea of carrying around a Subversion repository (with files writable only to root and appropriate access configuration files), and relying on svn server being present on all machines which you are going to use, but I do not think it's excessively sane. 

Keep current with bugfixes. Compile and package the upgraded software. Verify that it is stable, does not behave differently than the previous version and generally doesn't break anything in your system. Verify that the upgrade procedure itself works. Distribute to the field. Goto 1. 

Ad 1) To the best of my knowledge it does not. You will have changed rules for creating new passwords, not requirements for accepting already set passwords. If you want to force users to change their passwords to one complying with the new rule set age their passwords. Ad 2) pam_cracklib doesn't give you that kind of flexibility. You either enforce minimum number of characters of given type (Xcredit<0) or you give extra credit (+1 "length") for up to N characters of given type (Xcredit=N>0) and set minlen high enough to be satisfactory even if user choses characters of one type only. 

I use robocopy for rsync-like behavior in windows. Basically, I wrote a backup.bat file that I have on an external drive. I regularly run the file to back my desktops up to the external drive. Then I store the external drive in a fireproof safe. 

I have a server running some VMWare VMs that I built a couple of years ago for around $700 in parts from online vendors. I just built a gaming/development desktop that I run several VMs on. Case Shuttle is fine. My VM server is in a shuttle case stuffed under a table behind me. CPU Cheapest 64-bit i7. Memory 1G per Core. The new i7's are hyper-threaded, so it looks like you have 8-cores. Motherboard If you're not testing graphics intensive OSes/Apps, get a motherboard with integrated video. I've found an eSATA connection to be handy. Drives Largest you can buy for less than $100. Operating System Windows 7, Fedora 10, or CentOs 5.2. I've found that VMs are a bit faster under windows than Linux. Also note that regardless of OS, if your VMs are all doing IO operations, they may slow your system to a crawl. Virtualization Software I prefer VirtualBox because it's lighter weight than VMWare server or player. If your new machine is headless, I'd recommend VMWare. 

If the host machines are all on the same network, you may be able to configure DHCP to assign the MAC addresses used by VirtualBox (or VMWare) VMs to a different 'virtual' network. For example, say all your host machines run on 192.168.0.0. You could configure DHCP to assign all computers with MAC addresses beginning with 80:00:27 to the 192.168.99.0 network. Hope this helps. 

I would like to tinker with OS X in a VM. It doesn't have to be VMware, but I do want it to run under Windows. I tried the instructions at: $URL$ but couldn't get the OS X install disc mounted. What's the best way to get OS X to run in a VM? 

You are not doing number-crunching and you use plural "sites". That points to the AMD setup for several reasons: 

When the installer starts to boot hit . This will cause it to show messages normally hidden behind the "pretty" blue screen and will give you more information. I think you may be on the right track, though. This $URL$ HOWTO could help you. There's a section on labelling a partition on your USB too. Use output of mount and/or dmesg to determine, which partition is your USB stick. 

Here's a description of a process. They used a large swap partition to host root filesystem during the change. You do not need this trick, because you have 2nd HDD. A trick question: does your server support booting from the 2nd HDD? Can you set it up to boot from 2hd HDD? I think you could simplify the solution and not change the /boot partition. Bootloaders do not like LVM yet, so you still need a regular partition to host your /boot. Leaving /boot be and only changing the root file system location simplifies the process. You can have 2 entries in grub.conf mid-migration: with root in the old place and on LVM. In case something goes awry, you can boot to the rescue image, change the default entry and have another go at the migration. And yes, I wholeheartedly agree with poige. Test in a virtual environment before you start playing with real system. 

I'm not an Ubuntu user, so I cannot comment on completeness of patches you'd get through your option 3, but if you have any doubt, I'd assume you won't have complete coverage. The best solution is to move to a LTS version of Ubuntu, which will give you support for the given package versions for some time to come. In time, some of the packages will be outdated, but your environment will have security patches and will be stable (no package version bumps). From my experience, stability of a known working environment is usually more valuable than new features. It seems, that your current position is not maintainable, and you have to move. The only safe way is to get a second machine (or a virtual machine) and to test migrations until you have a repeatable successful procedure, then apply it to the production machine. If you use your backups to do test-migrations you'll have a good opportunity to test your backup procedures too.