Neologism. Speaking for myself, though, ‘nonce’ is fine for things that are not strictly ‘one-off’. Everything starts with a first time, and 'accepted productive rules' are often precisely how a nonce word would arise, right? To the point that it seems odd to have a specific term for it. 

Yes, there is a lot of research on phoneme confusability. Here is just one of many similar papers on the topic: Cutler, A., Weber, A., Smits, R., & Cooper, N. (2004). Patterns of English phoneme confusions by native and non-native listeners. The Journal of the Acoustical Society of America, 116(6), 3668-3678. The PDF is available at $URL$ You may find Table I and Table II helpful, specifically. 

That can be a little trickier, and it's language specific. For French, I believe Lexique is a good choice ($URL$ Researchers have also had success using eSpeak ($URL$ to convert orthography to phonemes, for French and other languages (for eSpeak performance figures, see Marian, V., Bartolotti, J., Chabal, S., Shook, A. (2012). CLEARPOND: Cross-Linguistic Easy-Access Resource for Phonological and Orthographic Neighborhood Densities. PLoS ONE 7(8): e43230. doi:10.1371/journal.pone.0043230). 

They're talking about section 1, not the complete set of all reality. This is called context. Presumably, the book goes on to explain why this rhetorical point is false. 

We study prosody because we observe that it is a meaningful, functional component of language. We transcribe it with the goal of abstractly understanding the contrastive elements that speakers seem to be perceiving and producing (and the system of interaction for those elements). This is analogous to other aspects of linguistics; e.g., we could roughly say that (segmental) phonology seeks to understand the system of sound elements in human languages. 

For many languages, SUBTLEX is considered a good source for realistic frequencies. There is an Italian version, SUBTLEX-IT, available at $URL$ (Crepaldi, Keuleers, Mandera, & Brysbaert, 2013). 

The NS would be defined so that it derives only on negative sentences, and NCoord would derive only on conjunctions that "can be negated" and thus cause the observed ambiguity. However, I find this a bit awkward, and I wonder whether it would not be more natural with a Tree-Adjoining Grammar. Then, the question remains of when such an ambiguous analysis is acceptable. What are the conjunctions that allow the observed ambiguity. We have so far examples involving exclusively conjunctions with a causal undertone, though the direction of the causality may vary. I found another example, without causality, which may shed some light on this. Unfortunately, it is based a conjunctive form that is understandable, but rarely used (according to my search engine). 

As I recall, the Harvard dissertation written by Goodenough (circa 1969) is a very big document, but I was not able to retrieve details on Harvard site. Internet search shows that this paper is cited by several others, but it is not clear that it all makes a significant body of linguistic literature on programming languages. 

You already have two answers. One says yes, and the other says no. Both give you arguments to support their view. Maybe the problem is with the question. Your question is, regarding English grammar: 

Now, If you wanted to draw both trees in a single diagram, that is possible, but significantly more complicated. But I doubt your book does anything of the kind. 

I think common names for what you look at are "frozen phrase" or "idiom" or "fixed/frozen expression". Maybe you are a bit more general than that, since you seem to include verbs associated with a specific preposition (I am not too sure from your example). There is some literature on this, though it seems that most lists of such units have been produced by hand, somme being quite large (Gross produced a list of 25000 frozen verbal expressions in French). I do not know however that there have been mechanized attempts to extract such lists automatically from copora, as is already done for other units such as verbs. One problem is that the characterization of these "frozen expressions" is semantical rather than syntactical. Though they may be apparently following usual syntax (maybe not always), the meaning of the expression does not derive from the meaning of individual components (other than possibly through etymological history). So I would first classify frozen expressions into two categories, those that are grammatical, and those that are not. Typically, in your example, "all right" and "come on" are not grammatical when appearing alone, or followed by a comma with nothing in front. (I hope that my knowledge of English is not betraying me). For example "this is not a station for the bus you come on" is syntactical, and is not relevant for the frozen expression "come on". Other frozen expressions are fully syntactical: "kick the bucket" and can even take inflection "as he was kicking the bucket ...". They may possibly be identified by lemmatization of constituents and semantic analysis, checking in particular the relevance of the different parts of speech to the various context in which the expression appears. But I am only guessing. Whatever the case, I doubt you will get very far without serious morphological and syntactic analysis. Frozen expressions, or semantic units may well be composed of words that are not adjacent in the sentence, but correspond to a connected subpart of a tree representation of the syntactic structure. (well, tree is the simpler kind of structuring technique). I do not think that simple adjacency of words will give you the more interesting results. So building this list and trying to extract semantic units witout the (syntactic and/or semantic) context of your fragments may not get you very far, and may not be very meaningful. Another point is that you may also need larger semantic context, to see whether it has an impact on your frequencies. Your corpus could be divided into sub-corpora corresponding to different sub-languages (i.e. spoken by different communities: lawyers, medical doctors, scientists, journalists ...). This might bring information, as can the specific topics of the differents documents in the corpus. Of course, this is all of the top of my head. I have not done the research and some of my suggestions may be ill advised. But I do doubt you can do anything worthy without syntax analysis, and probably lemmatization. This seems a worthy research project, but you may have to put more technology in it, imho. 

I’m not currently working with Praat, but I assume you’re calling something like string$() for the filename at some point. (Or, it’s being called implicitly to coerce your numerics into strings.) Can you use this function instead? 

This is not an authoritative answer. But, I'm not sure one exists. I'd never heard a neutral term for it, but I like your suggestion. It looks like people could almost use 'retelling error' ($URL$ but haven't. I think it's a good candidate, though: 'retelling' seems to be the most precise term for what you describe, and it is already in use in academic papers about this. The 'error' half is sometimes referred to as 'mutation' or just 'change', but I think you're correct to go with 'error' (and for the reason you mention). 

In fact, it seems to be the case that you do get /r/ in those contexts and specific words (also, 'squash', etc.). It may be difficult to find them in text (e.g., for 'posh': 'porsh' and 'parsh' are both problematic for the speller), but they likely exist. I was able to find Google hits for 'jarsh'/'jarshing'/'jorshing', for example. It is also possible that this kind of regularization rule is applied more for high-frequency words than for low-frequency. If 'wash', 'water' are much higher frequency than 'posh', 'josh', then it's reasonable to see fewer instances of 'jarsh'. 

TextGrids are still just text files. The simplest thing for a non-scripter is probably to open it in your favorite editor and find-replace them there. Try TextWrangler or SublimeText 3 if you don’t have a good one you like.(I’m not sure that this is even the correct StackExchange for this, or if the tags ‘phonetics’ and ‘computational-linguistics' are relevant.) So, the tiers probably contain intervals like: 

While a dedicated script (sed, awk, Python, …) would be better if this is a common operation, you can indeed do this in e.g., SublimeText. One method is to first select all the ‘text' lines (e.g., Find All with '.+text.+' is fine); then, Replace All '\\n' with ‘ ‘. This method assumes that you can't simply do a global replace of '\\n' with ' '. I think this latter, simpler option should be tried first, though. 

You see that you have upward monotony in the first case because the more general phrase is existentially quantified. You have downward monotony in the second case because the more general phrase is universally quantified. Your first example is upward monotonic for that reason. If you expland the semantics of "I eat fruit", it is really "There is a fruit I eat". And we have seen your second example is universally quantified, hence downward monotonic. Negation does reverse the monotonicity, as it exchanges universal and existential quantification. But I do not think negation has itself an intrinsic role, at least in your examples: "Driving is not dangerous" is negative, but has exactly the same meanind as "Driving is safe" which is not negative. Recall that the general/specific opposition here is on "driving". 

I have not worked personally on machine translation, but as I recall, some have been built, or experimented, using syntactic trees, and tree-to-tree translation mechanisms (I no longer have the references in mind). Trees are of course syntactic, rather than semantic, though they can be decorated and usually are. However they can help with semantic ambiguity resulting from a syntactic ambiguity when languages have similarities in some of their syntactic structure. There are clean formal ways to extend tree structures so that they encode several trees when there is syntactic ambiguity. Then you can consider translating this structure to the (extended) tree representation for the target language, and thus simply preserve the ambiguity. I remember people discussing this, but I have no idea whether it was actually ever implemented. This could for example be used for ambiguous prepositional attachment. But there are probably other ways. 

This is part of the problem you encounter. Many natural language statements are actually predicate over some entities that may be element of sets. For example you may use the predicate Eat which takes two arguments, the subject who eats and the object that is eaten. Then the sentence "I eat banana" can be interpreted semantically as Eat (I,banana), which establishes a relation between I and banana. But when you say "I eat fruit", "fruit" stand for Fruit which is the set of all fruits, that include banana and many others. But, how do you interpret this sentence: does it mean that you eat all fruits or that you eat some fruits. This is not obvious though it seems that most people interpret that as meaning that you eat some fruits? Furthermore this has to be your interpretation since you deduce it from the fact that you are eating banana, which cannot imply that you eat any other fruit. All you know is that you eat one fruit: banana. So for you, "I eat fruit" must mean that there is a fruit that you eat, which can be formally written as 

In American dialects, the sound in 'hat' is generally /æ/, not /a/. The use of /i/ (meaning /ai/) in the 'fight' vowel would be too long/close; you probably do not pronounce 'might' exactly the same as the phrase 'ma eat'. The vowel in 'might' is shorter/opener, and so /ɪ/ (meaning /aɪ/) is appropriate. (See $URL$ 

Moved from comments. You might consider generating such lists yourself. I'm not saying asking if that work has already been done is wrong. :) Computationally, it's not a trivial job. This link (billposer.org/Software/minpair.html) purports to provide an open source (GNU license) program to find minimal pairs in a provided dictionary. You'd want to provide a phonemic dictionary, of course, for phonological minimal pairs. If it's not adequate, you could also examine the algorithm and modify it to suit your needs. You might also be interested in generating your own words (according to some pattern of phonemes, presumably), and then simply checking if they are in the dictionary. Or, and this is based on your original question, you can test people on minimal pairs of nonwords (in something like an AXB task, I guess); for the purpose of assessing their phoneme perception, this option is the typical choice. It helps to minimize the influences of semantics, frequency, lexical neighborhoods, etc. 

This is a fairly hard problem (well, 2: the g2p problem, and the word-formation problem). Per-grapheme:phoneme (fun fact: such mappings are sometimes called ‘graphones') dictionaries for English don’t (can’t) exist, because there’s lots of ambiguity and issues of multiple n-gram sizes (as in your example, ‘ie’ = /IY/, but sometime ‘i’ = /IY/, or /IH/, or whatever). Dealing with stress placement is even worse, and possible not solvable by the method I suggest below. I had some success with Phonetisaurus, a more generic g2p (and, if you reverse the inputs, p2g) tool. It needs to be trained on a dictionary like you describe (orthography list with matched phonemic list). I guess you’ll also have to match the spooky versions for stress and rhyming, but I don’t know of an existing solution for that. The most realistic solution is for humans to prepare these spooky versions ahead of time (by hand); the distribution of names is such that you could get reasonably good coverage for most people with a reasonable amount of work. 

Then chart parsing can be applied to such an automaton to produce a shared forest for all the grammatical sequences in the language of the finite state automaton. Of course, you may actually put more special symbols than you actually need. This will only result in more candidate sentences. Applying a chart parser to such an automaton can be done in the same way that chart parsers can be applied to word lattices in speech recognition. A word lattice can also be viewed as a finite state automaton. Indeed, all this falls into a more general framework that understands parsing as a way of computing the intersection of a phrase structure language (not necessarily Context-Free) and a finite state automaton representing a set of candidate sentences. The result is a parse forest for all the grammatically acceptable sentences that can be generated by the (non necessarily deterministic) finite state automaton. This represent a basic skeleton for the sentence completion procedure. Then you may have to use other devices to actually choose the words you want to insert in place of the missing parts of speech. But that may depend on your lexicon and semantic issues. It become more an issue of sentence generation, based on the result of an initial parse of what is given to you. You may have to worry about semantics, but syntactic correctness is ensured. Regarding the use of NLTK and Python for the purpose, I am not sure of the adequacy of it as currently programmed, since I am not a user of NLTK. However, since NLTK contains a general context-free parser, it should be possible to modify it to get the result described above, as general CF parsers all work more or less on the same principles. There may be one subtle point about handling "input loops", which amounts to handling infinite ambiguity, often ignored by general CF parsers.