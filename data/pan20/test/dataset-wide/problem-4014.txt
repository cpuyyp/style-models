I'm interested in various areas of complex systems, and I often come across articles like these: $URL$ $URL$ The main points are accessible in each (much less so the 2nd one though), but I'd like to be able to understand this sort of writing deeply, or even be able to do it myself. What sort of studies would I need to undertake? Would a standard thermal/statistical physics class do it, or do I need something more drastic? Are there any resources along the lines "statistical physics for the social scientist" that are still rigorous and high-level? (There's a question about "statistical physics for the mathematician", but this is almost exactly the opposite of what I need, funnily enough"). 

I am an undergraduate CS major with strong interests in applied math and theoretical computer science. In the past, I've done reasonably well grade-wise in all math-related (that is, pure math, applied or theoretical CS) classes, but I feel that I still haven't taken away as much as i could have from most. As people who have often taught math courses and had to deal with the inevitable fact that no lecture will be universally effective, what are your suggestions for how I (as a student) can best learn in these classes. A few problems I've experienced regularly: When professors try to present long and difficult proofs on the blackboard. I always find it ridiculously hard to understand proofs in real time or to understand verbal and visual explication of the proof simultaneously. I have to look the proof up in a textbook, and the comprehensibility of textbook proofs varies widely. More generally, accessing the "kernel" of the proof that really makes it comprehensible is sometimes difficult, especially when it's presented more formally. I tend to think of proofs in terms of algorithms, and proofs that don't fit this well tend often evade me. Definitions, even, (especially in pure math) tend to blend together and become obscure. I've re-learned the basic definitions of probability waaaay too many times. 

Linear Programming is strongly entwined with linear algebra, as are many of its generalizations under the heading of mathematical programming / convex optimization. What analogies are there for convex optimization techniques outside of vector spaces dealt with in linear algebra. For example, Gaussian Elimination is generalized by Buchberger's algorithm for finding Groebner bases (or so I'm told); is there any algorithm that has a relationship with Buchberger's analogous to the Simplex Method's relationship with Gaussian Elimination? 

Let $M$ be a smooth manifold. A smooth distribution $D$ on $M$ is the union of a family $\{D_p \leq T_p M : p\in M\}$ of vector spaces such that there is a family $\mathcal C $ of smooth vector fields on $M$ satisfying $D_p = \text{span}\{X_p : X\in \mathcal C \} $ for every $p \in M$. Remark that we do not ask the dimension of the fiber $D_p $ to be constant: we call a distribution regular if the dimension is constant, and singular if it is not. We call a distribution $D$ integrable if for every point $p \in M$ there is a submanifold $S\subseteq M$ which is tangent to $D$ and satisfies $T_q S = D_q $ for every $q \in S$. In this case, it can be proved that the maximal connected integral manifolds of the distribution form a partition of $M$ into weakly embedded submanifolds of $M$, which we call the foliation associated to $D$. Typical examples of integrable distributions are given by Lie algebroids: if $A\to M$ is a Lie algebroid over $M$ with anchor map $\rho : A\to TM$, then the image of $\rho $ is an integrable distribution on $M$. For example, every Poisson manifold has an integrable, possibly singular distribution given by the image of the Poisson bivector field $\Pi : T^*M\to TM $, and the induced foliation is precisely the symplectic foliation of the Poisson manifold. The integrability problem for regular distributions is solved by the Frobenious theorem: a regular distribution $D$ is integrable if and only if it's involutive. A singular version of the Frobenious theorem can be stated in the following way: a (possibly singular) distribution $D$ is integrable if and only if there is a family of vector fields $\mathcal C$ which span $D$ pointwise, such that the flow of every element of $\mathcal C$ preserves $D$ (see Theorem 3.5.10 of this book for a more precise statement and a proof). A sufficient condition for the integrability of a singular distribution $D$ is the following: there exists a module $\mathcal C$ of compactly supported vector fields spanning $D$ which is locally finitely generated and involutive. Some people calls such an object a Stefan-Sussman foliation. I have two related questions: 1) Is it true that every integrable distribution is spanned by a module $\mathcal C$ of compactly supported vector fields which is locally finitely generated and involutive? 2) Is it true that every integrable distribution is the image of the anchor map of some Lie algebroid? Clearly, (2) implies (1). There is people which believe that (2) is true, and I would like to know if this question is still open. Thank you! 

The truth is, that almost just by being around an educated parent, children grow up to be smart/successful. Pushing them in certain directions, or trying to teach them may be effective, or it may backfire. Being hands off, likewise, may be a good or bad decision. Either way, they won't be failing 4th grade arithmetic. If you want all your kids to grow up to be multimillionaires and senators, then you're probably going to have to push, and push hard. If you're fine with them being content but unspectacular (with the option to go for spectacular if they're inclined), then take a more relaxed stance. Of course, I've never had kids, only been one. So take this with a grain of salt. 

I want a list of the sort of mathematics/mathematical tools that are applied to the study of complex and probabilistic systems in order to make quantitative and qualitative observations about their behaviors. These mathematical tools need not be exact (in fact, I expect most will draw from statistics), but I want a list of methods that are rigorous and useful beyond a specific problem domain. For instance, what are the mathematical tools behind: -Statistical physics (Renormalization group, for instance) -Large-scale differential equation solvers used in modeling -Image processing techniques in computer vision (for instance) -Social Network Analysis -Compression technologies -Visualizations of large-scale data In other words, what are the most interesting and rigorous mathematics applied to large-scale information processing? A list of techniques and tools: Sampling techniques in Applied Probability and Stochastic Modeling Principle component analysis based on eigenvalues of matrix representations of data (plus loads of other stuff related to matrix representations, like singular value decomposition, rank minimization, etc.) Belief propagation in neural networks The PageRank metric on networks Random Matrix Theory (for statistical physics) The finite element method (much more of engineering flavor, but mathematically based and enhanced in its efficiency by all sort of deep mathematics, e.g. from differential geometry) 

Convex Optimization is a mathematically rigorous and well-studied field. In linear programming a whole host of tractable methods give your global optimums in lightning fast times. Quadratic programming is almost as easy, and there's a good deal of semi-definite, second-order cone and even integer programming methods that can do quite well on a lot of problems. Non-convex optimization (and particularly weird formulations of certain integer programming and combinatorial optimization problems), however, are generally heuristics like "ant colony optimization". Essentially all generalizable non-convex optimization algorithms I've come across are some (often clever, but still) combination of gradient descent and genetic algorithms. I can understand why this is - in non-convex surfaces local information is a lot less useful - but I would figure that there would at least be an algorithm that provably learns for a broad class of functions whether local features indicate a nearby global optimum or not. Also, perhaps, general theories of whether and how you can project a non-convex surface into higher dimensions to make it convex or almost convex. Edit: An example. A polynomial of known degree k only needs k + 1 samples to reconstruct - does this also give you the minimum within a given range for free, or do you still need to search for it manually? For any more general class of functions, does "ability to reconstruct" carry over at all to "ability to find global optima"? 

If I add to the hypothesis that the ring is a domain, then (I think) the statement is true. I'm trying to figure out if this must be true (I suspect not). Is there a nice example of a local Noetherian ring whose maximal ideal is principal that is not a domain? Is there a better, weaker condition to add to the hypothesis so that sufficiency holds? In other words, "if R is a local ring whose maximal ideal is principal, then R is Noetherian if and only if R is [what is the best thing to put here]?" 

For a $p$-group $P$, the number of maximal subgroups is $\sum_{k=0}^r p^k$ where $r$ is the minimum size of a generating set for $P$. You can see this from looking at the maximal subgroups of $P/\Phi(P)$, which is elementary abelian of order $p^r$. What I can tell you is that there is at least one normal subgroup for every power of $p$ up to the order of the group. Sylow theory style orbit counting gives us that the number of normal subgroups of each order $p^k$ is going to be congruent to $1 \mod{p}$, so the total number of normal subgroups in a $p$-group of order $p^n$ will then be congruent to $n+1 \mod{p}$. EDIT: I thought of a bound. $n+1$ is the lower bound, attained by the cyclic group of order $p^n$. There must be at least one normal subgroup for every prime power divisor, so this is the lowest it can go. On the other hand, I claim that elementary abelian groups $E_{p^n}$ contain the largest number of normal subgroups. This is because it has the maximum rank of all groups of order $p^n$. Thinking of $E_{p^n}$ as an $\mathbb{F_p}$-vector space, we obtain the number of subspaces by $$\mathcal{N}(E_{p^n})=\sum_{m=0}^{n}\prod_{k=0}^{m-1}\frac{p^n-p^k}{p^m-p^k}.$$ Here we count the number of ordered combinations of $m$ linearly independent vectors in $\mathbb{F_p}^n$, then divide by the number of possible bases of an $m$-dimensional subspace. Summing over $m$ we have the total number of normal subgroups in $E_{p^n}$. 

The only sneaky groups that I can think of are similar, in spirit, to the following example. Let $\langle a_1 \rangle \times \langle a_2\rangle = C_2\times C_2$ act on $\langle b\rangle \times \langle c \rangle = C_3\times C_5$ so that $a_1$ commutes with $C_5$ and acts fixed point freely on the $C_3$, and $a_2$ commutes with $C_3$ and acts fixed point freely on the $C_5$. We end up with $$\left(C_3\rtimes C_2\right)\times \left(C_5\rtimes C_2\right) = \left(\langle b \rangle\rtimes \langle a_1 \rangle\right)\times \left(\langle c \rangle \rtimes \langle a_2 \rangle\right)$$ which has elements $ca_1$ of order $5\times 2$, $ba_2$ of order $3\times 2$, and $bc$ of order $3\times 5$. But, since $a_1$ doesn't commute with $b$ and $a_2$ doesn't commute with $c$, we can't have any element of order $3\times 5 \times 2$. Sneaky. We can generalize this construction by replacing the $C_2\times C_2$ with a product of (solvable) Frobenius complements, and the $C_3\times C_5$ with a product of Frobenius kernels, with actions defined in such a way that each complement commutes with one kernel and acts fixed point freely on the others. Furthermore, we could expand one more time to using 2-Frobenius groups of type $(p,q,p)$ (see p.5 here for definition), for example by replacing the $C_3$ and one of the $C_2$s with an $S_4$. So, my question is,