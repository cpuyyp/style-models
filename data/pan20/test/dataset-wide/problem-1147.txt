I'm importing a large set (126M rows) of file data from CSVs. Import works fine, except I'm trying to derive a column as I import rather than trying to script it out afterward (due to the number of rows). SSIS is not terribly helpful with string manipulation functions, and the possibilities I have are pretty complicated as well. The derived column needs to reflect the file extensions. File extensions range from 1-10 characters (anything longer can be truncated at 10), and are (of course) separated from the file name and path by a period. They don't correspond to a concise list of file types (something like "docx, xlsx, accdb, msg" are Office types). The file path may also have periods in it. And in some cases it will not have files at the end of the path, just a folder name itself. Some examples: 

I'm asked to help a developer figure out why the "random" data we should be getting from Oracle is exactly the same results each time you run it, rather than being actually random. SQL 2012, Oracle 11g, linked server is using OLE DB connector type. When they run the following query directly in the Oracle DB, they get proper "random" results. Through the linked server in SQL, they'll always get the same rows (and I've verified this by running the linked server query myself). 

I wouldn't have the ContactInformation linked 1-to-1 to Person anyway; storing several addresses like that in ContactInformation violates normal form. Instead I'd have ContactInformation with its own PK, an FK for PersonID back to the Person table, and store as many Contacts as necessary for each Person. 

No, the applies to the query and not to the table created by . Using is unnecessary in this statement, because it doesn't even guarantee the rows are inserted into the new table in that order. See INTO Clause (Transact-SQL) 

First, I'll say that is frowned upon for production code; if you're just testing or running a user query periodically I can overlook this, but if this will be part of a stored procedure or something lasting, you should replace with the list of columns you're looking for. Second, you have a couple of options. You can exclude '%text%*4' like Michelle pointed out using WHERE NOT LIKE. If you're going to try to include it, you'll need to cast or convert the normal contents of the column to a string type like . That should allow the '%text%*4' to come through in the results. The other way which would retrieve the linked comment you're looking for could look something like this: 

I have a BI admin that created a tool-generated query from Tableau, connecting to a custom database application that tracks issue reports for follow-up action. I can make almost any changes to the database, though I'm not the owner. The requirement we're having trouble with is that certain "admin users" need to be able to see ALL database items, while "normal users" should only see items that pertain to them (that they reported or they're responsible for resolving). The BI admin handles this with a Cross Join to the "Groups" table, filtering for the Admins key - and this takes the result set from 94K rows to 2.4M rows, and the query response time from 12s to 5m12s. Obviously Cross Join is to be avoided wherever possible, but I'm not familiar enough with Tableau or other reporting suites to suggest an alternative way to handle admin access vs normal user access. What's the proper way to do something like this? 

The easy solutions fall into "push" or "pull" strategies, which each have their benefits and drawbacks. 1) Push data from the Oracle database into SQL server, via a scheduled job or table triggers. Doing this, you'll have to remember to update the links to SQL if that database ever moves. With a table trigger, you'll also have some performance overhead on writes, updates or deletes to the table. 2) Pull data from the Oracle database using SQL server, via scheduled Merge job or SSIS ETL package. Using either kind of scheduled job you'll be slightly behind the live table updates, which is usually not a problem. The job can run as often as necessary and possible to keep the gap small. A DBA shouldn't have a problem setting this up with you, but if you're unfamiliar with database internals it can be difficult to choose and implement an option. Third party tools might be able to help if a DBA is not available. You should also be able to find guides on each of these options if you're going to implement one yourself. 

23068672 bytes = 22 MB, not 20 GB. If it were really 20 GB the only thing I could think that would make it that big would be reusing the same file for multiple backups and appending each backup to it. 

Edit: After reading initial comments and looking into this closer with the BI guy, the query Tableau generated is very poor and could use significant cleanup. The database has a total of ~3900 rows in all tables, so even the 94k means a significant amount of crossing and duplication is already occurring from one-to-many relationships and Left Join. I simplified the query a bit, removing about half the columns in the SELECT portion. 

Short answer: test your backups, and validate the backup frequency. It's difficult to write a long enough answer to properly address this topic; I'll explain a little on my short answer and give you some links to more information. Testing backups is extremely important; merely taking backups will not ensure that they're useful. Most DBAs will periodically restore the backups to another database or another server - for practice and to test the validity of the backup. The frequency of backups is important to address "minimal loss" - it depends on how frequently the database is used, and more importantly, how much data loss is acceptable. If the users/analysts can lose an entire day of data and be okay with re-entering it, you only need daily backups. If they can only lose 5 minutes, you need to be taking log backups every 5 minutes to satisfy that requirement. Communication with the database stakeholders is key to identifying a proper backup strategy. The other thing they can tell you is how long ago they will need to restore. If there are occasionally mistakes they need "rolled back" up to 3 months ago, your backup plan needs to keep backups around for 90 days so you're able to pull from those. Check out Brent Ozar's site for a more complete discussion of backup practices and why and when to employ them. 

Populate the ContactInformation table with the PersonID, then an email address/phone number/mailing address/twitter handle/etc and mark it with a ContactType. This could be 'MA' for mailing address, 'EM' for email, "TW' for Twitter, 'TT' for Term Time as in your question. Triggers could validate any inserts to make sure contacts don't have a TT (term time) address unless they're a student. Hope that helps you think outside the box. It may be too hard to make these changes at this point, if you're already deep in the application. Edit: I didn't feel I properly addressed your direct question. If I were you and making deeper changes isn't possible or feasible at this point, I would add TermTime address to the Students table. It belongs there because 1) it only applies to Students, so having it in the Person or Contact tables (as they exist now) is less than ideal, and 2) Each student should have one so it's already part of their student record and 3) this can be done with the least disruption to the existing schema. 

Note: I'm not an Oracle DBA but I work with one; he didn't see an issue with the Oracle part of the above query. Also, I edited a lot of stuff out and replaced with XX for privacy. Each time we run this query in SSMS, we get the same 200 rows. I even connected from another SQL server that has the same linked server and ran this query, and got the same 200 rows (the same Oracle user is used for both connections). This leads me to believe Oracle may be caching the results; but somehow it's not doing this when the query's run in Oracle directly. Is there a way to force it to return new results through openquery? (so that the 200 rows it returns are a truly random sampling each time) 

You're getting the from the clause of your Case structure because NULL doesn't evaluate. NULL bypasses the test. Null values can be difficult to navigate. Most recently I've seen this article from Brent Ozar's team discussing NULL values and their effect on join logic and performance. For most evaluations you can use ISNULL but keep in mind that may affect query performance. 

I have 3 servers, one of which has a linked server configured that points to the other - I'll call this Server A. Server A has over 100 user databases for various purposes. Server B is running SQL 2005 which we're trying to eliminate. Server C has copies of some of the databases from Server B, and we're migrating applications from Server B's database copies to Server C's. When I'm on Server B, I can see connections from Server A to a certain database but I don't know how to tell what procedure, task, or job from Server A is using that linked server connection to Server B. In order to retire the database on Server B, I need to re-point Server A's connections to a database on Server C; but in order to do that, I need to know what procedures, tasks, or jobs on Server A are using that connection so they can be updated. Is there a way to see the dependencies on a linked server without disabling the linked server to see what starts failing? 

I'm going to disagree that SSDs are the de facto standard, though they certainly are far more common than a short time ago. I know my company doesn't set any standards, but nearly all of our virtual database servers still use an auto-tiering partial-flash SAN, against my better judgement And unfortunately, I'm going to start my answer with an "it depends." In general, index maintenance is far less important than it was in prior years and older versions - specifically, the long and deep index rebuilds that used to be offline-only. The days of "reorganize everything and rebuild if > 30% fragmented" are pretty much over. I think most would agree that rebuilds are no longer worth doing, and SSDs do play a part in that. On the other hand, statistics have less to do with fragmentation and physical distribution of data on disks and more to do with logical distribution of data across/among the table(s), so this can still be important. Whether it's important for a given workload depends on a lot of factors. If you can test stopping these maintenance routines in a non-Prod system, you can look for poorly-performing queries, then analyze the DB engine's steps in the Execution plan. All of the statistics issues I've seen (and I've been dealing with some recently) have been easily visible by looking at the operators in an execution plan and seeing huge differences between "estimated number of rows" vs "actual number of rows". If the queries perform poorly enough, execute a quick statistics update and try again. It's a bit of work going through all of these steps, but once you have a handle on the system you can tell if statistics maintenance jobs are worthless overhead or important and necessary. Erin Stellato has several great blog posts about statistics that can help with background information on why fresh statistics can be important, and also when they're most useful and how to detect if they're not updated frequently enough. 

Your function, is not able to open the file you're passing to it with the variable. I don't know if you redacted the filename, or if it actually has an empty string in single quotes, which would mean the variable was empty. You probably should debug and check the results from . Also note, these functions are deprecated and it's recommended to use Extended Events instead. sys.fn_trace_gettable (Transact-SQL) 

So I need to be able to extract the extensions up to 10 characters. I tried TOKEN() but with folder names possibly having periods, that didn't work well - adding as a token delimiter helped some, but I still got odd extensions from folder names. I can't find a combination of SUBSTRING(), RIGHT(), FINDSTRING(), and/or TOKEN() that will meet the rules and the derived column definition doesn't allow logic like IF or IIF. Some false positives are expected and I plan to sort them out after importing. If it helps here, I have a second column in the CSVs which is extracted by SSIS, it's the size of the file (for folders it populates a 0). I haven't gotten this to matter either, because of the lack of IF or IIF in derived column definition.