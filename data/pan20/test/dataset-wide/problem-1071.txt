It is because you need to create the breakpoint using the database name and object id rather than setting a breakpoint in a random SQL file which is what you get by choosing modify. See: $URL$ Ed 

The default options that ssdt chooses often aren't best practices, instead you should choose what you want. For page_verify I agree checksum but it is up to you: $URL$ Incidentally if you import from a production (or similar environment) and you choose "Import database settings" the actual settings will be imported from the database - if it is a new database I would get your dba's to create an empty one on a dev instance with their standard build etc. Ed 

I have table blobtable which has a BLOB column blobcol. A user updated the table using a command similar to this: 

Note that the file filename.txt existed in the directory before this update was performed. Now, the user is saying that when they select from blobtable (using PL/SQL developer), blobcol contains "Value Error" for this row. Normally, blobcol contains the value "BFILE". Out of curiosity, I tried selecting the same record in SQL*Plus and I get the following value for blobcol: bfilename(directory/subdirectory', 'filename.txt') I'm not very familiar with BLOBs and how they work, so my questions are: 

Are these files used by any database processes, or only by client programs like SQL*Plus? If these files became unavailable, would existing client connections be affected, or only new connections? Would database links using the TNS aliases be broken while the tnsnames.ora file is unavailable? 

CSV files are not included in the dacpac that you deploy so you will need to make sure that you copy it with the dacpac and that if you use relative paths the working directory is what you think it is. What I do for extra files is set the "Build Action" of the file to "Copy if Newer", when the dacpac is built in the bin folder you will also have a "Data" subfolder - just make sure you copy that with your dacpac always. ed 

Release 1 - create column hello on table Release 2 - rename column hello to joe_blogs Release 3 - rename column joe_blogs to hello Release 4 - create column joe_blogs 

In the interest of keeping the database as secure as possible, I'd like to lock the SYS and SYSTEM accounts so that no one can login with them. Assuming that: 

There are no OS scripts/cron jobs logging in as SYS or SYSTEM There aren't any applications or outside utilities using either of these accounts I can always login "/ as sysdba" with the proper OS account 

Did the user update the blobcol properly? What could cause the "Value Error" and how can this be corrected? 

Backing up to local disk isn't an option because there's not enough storage available locally. Now, the strange thing is, when I enable compression the backup magically works! Whereas if I don't, I see the backup file appear on the share and after about 2 minutes the file disappears and the backup fails with the error above. I'm at a loss and looking for any ideas about what might be causing this. 

If any of the releases are missed, none of the next ones can continue. Benefits of upgrade scripts (Liquibase, DbUp, etc): 

SSDT is comparable to Liquibase/Flyway as it does what they do but by taking a different approach. With SSDT you have the development environment so you get things like go to definition, find references and intelli-sense as well as the ability to compile a project into a dacpac and then deploy that dacpac to a database. The SSDT way (and redgate sql compare way) to do a deloyment is to declare what you want so if you want to change a table that looks like: 

Will locking these two accounts have any adverse effects? Has anyone done this before who can comment on whether or not it's a good idea? 

I don't think there is a "Replace" option for non-table objects like procedures, packages etc. The best option would be to drop the schema entirely before the datapump import. This way, datapump will re-create the schema and all of the contained objects. 

This happens on just one database out of 88 databases on the SQL server. All of the others are being backed up daily - to the same location - with no problems. What I have tried: 

Personally I really think SSDT is a professional development environment and it means that I can concentrate on writing useful code and tests rather than writing upgrade scripts which are in themselves just a means to an end. You asked for opinions so there you go :) ed 

It is normally because there is a difference between how they are written and how sql stores them. As a one time thing to a compare back from the db to the project and you should see some difference on the keys, apply them and it should stop doing it. 

Let's say that I want to change a SQL configuration setting like MAXDOP. I know that changing this on an active instance will severely impact performance, as it will clear all of the cached query plans. Doing this while the instance is running is simply not an option. Is there some way to modify the value now, but to "defer" the actual change until the next time SQL server is restarted? What I'm looking for is an option like Oracle's "scope=spfile" for parameter changes. Does such an option exist in SQL Server? 

You should be using the system user for full datapump export/import operations. Using "/ as sysdba" or "sys as sysdba" may give you unpredictable results.