Try rewriting the query in different ways to see how the execution plan and times change. Changing the order in which tables are accessed, using set operators, indexing, etc... can make a huge difference. The two approaches suggested below are using the MINUS set operator and using NOT EXISTS instead of NOT IN. In both cases, the queries are driven by the SHU_STUD_TERM_SUM_DIV table. This will reduce the number of rows joined to NAME_MASTER. The MINUS set operator works by filtering the results of the first query with he results of the second. This allows the MySQL tables to be joined directly. The NOT EXISTS will behave like the NOT IN (each row will query the MySQL tables) but does have the potential of comparing a smaller set of values (only those that directly match NM.ID_NUM). Using MINUS: 

I've been seeing some SMTP errors in my database mail logs. So I have a set up a test SQL Agent job that runs every ten minutes (it's only action is to SELECT NULL). It is set to send email notifications whenever the job completes. The addresses are internal email addresses. I have configured database mail to retry failed emails up to 4 times. In the last day, almost half the emails encounter an error at least once but eventually get delivered. 15 of the emails failed completely. A little more than half get sent without any error at all. Here's a list of each job execution since midnight, along with the number of errors encountered by each attempt to send mail: $URL$ (note: there are gaps in the mailitem_id sequence because we have other jobs that also send emails). The errors are a mix of the following two messages: 

"show create view" will provide you the DDL to recreate the view exactly as it is (complete with select statement). 

It seems the most important thing is matching time out with the time in. The query below shows one way this can be accomplished. The approach below attributes time worked to when the individual "clocked" in... there is no attempt to attribute the time to the day in which it occurred (there are other questions on this site that address that need). 

I highly recommend partitioning your table now. I'd create a new table partitioned by year that looks just like your current table then insert into it. Going forward, you can more easily prune your table. Here are several links to a good partitioning discussions... $URL$ $URL$ I'm not a SQL Server guy - I saw an MS SQL Server 2016 documentation suggests there are some differences in 2014 features - but I can't find 2014 documentation. Here is a link to MS SQL Server 2008... $URL$ Good luck! 

Seems to me that according to the question, simply sorting by EndDate would do the trick... The OP does not specify that the results are to be returned in descending order. In fact, he does not state that the rows need to be ordered by StartDate at all. Since NULL dates will be ordered before dates with values, an fulfills the requirement. Otherwise, should be sufficient. Using @Julien's sample code, the final SELECT could look like this: 

In SSMS 2014, when I connect to a server in Object Explorer using my network credentials (I am a domain admin), I can do pretty much everything I would expect as a DBA. But I cannot start/stop/restart services. For instance, if I right-click SQL Server Agent, the options to Start, Stop, and Restart are greyed out. Using SSMS 2008R2, from the same client machine using the same credentials connecting to the same server, the options are available and functional when I right-click. This is true for all servers in our environment. What do I need to configure in order to be able to manipulate SQL services in SSMS 2014? 

This is an ugly solution that may introduce other issues. It is offered as a means to eliminate the lock you are experiencing. Auto_increment is preferable to managing numbers in this manner. The number could be managed in its own table incremented as needed. This table could be locked at no detriment to the main table. When write locked, other sessions are prevented from using the table, the locking session can then increment the number (and store it in a variable - something meaningful like @next_position) then releasing the lock immediately after the update. The number is then used in your main query - no join, simply "LQ.initialPosition = @next_position". You may still have contention on the position number table - but these locks shouldn't be held too long - only during the incrementation to ensure only one process increments the number at a time. The locking scheme could be eliminated, but this would require knowing the current number first which would then be tested in the update's where clause to eliminate race conditions with other sessions. A loop will be necessary to try again if the update doesn't work (another session made the update first). This complicates the use of the table but reduces the potential for trouble. 

BIG QUESTION: Why are these emails failing? Smaller question: What steps can I take to further troubleshoot this to ultimately answer the big question? 

We have a business-critical stored procedure that normally runs daily at 2am from a scheduled job (in an SSIS package) on the production DB server. The same procedure/package is called from a second job, 15 minutes later, from a SQL Agent job running on a different server (as an emergency failover in case anything goes awry with the first job). The procedure is defined WITH RECOMPILE. The procedure normally executes in about 45 seconds. Last Wednesday, and again this morning (also Wednesday, coincidence?!?), the 2am procedure took 90 minutes to execute. While it was executing, the 2:15am job ran and that execution took the usual 45 seconds. I have the execution plans from both situations. There are some table variable processes that should include estimated row counts in the neighborhood of 200K rows. The faulty plan reports these table variables with an estimated 130 billion rows. [Side note: I have already rewritten the code to use temp tables instead of table variables, based on this discussion and will be moving it to production in the near future] Our monitoring software (Solar Winds DPA) reports excessive CXPACKET waits for the 2am execution. This seems to indicate issues with parallelism and is likely related to the table variables being used in the procedure. There is still user activity on the server during this time, and some scheduled jobs, but nothing that I see that would affect this procedure or its execution plan. An index maintenance job is run at 2:30am. I understand the poorly-performing execution plan is related to the temp tables, but why would this same procedure executed 15 minutes later have such a drastically different execution plan (and why does the 2am execution run fine the rest of the week?) Here are links to the .sqlplan files: The Bad Plan and the Good Plan. 

What is the fastest way to determine if an IP is contained within a CIDR block? At the moment, whenever I store a CIDR address I also create two columns for starting and ending ip addresses. The starting and ending ip addresses are indexed. If I want to see which network contains an address then I look which seems less than desirable. It occurs to me I can store the right shifted number and could match similarly shifted IP address (660510 in the case of @cidr)... 

The following is an untested attempt at distilling the problem into its basic components then unions the results together. The last query (with the correlated subquery) can be turned into a join if the introduction of orders does not cause a Cartesian product. I can not test with what is provided to see if this is a valid assumption.