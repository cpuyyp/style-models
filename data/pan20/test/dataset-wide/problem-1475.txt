A common pattern to avoid these instances is to create a 'default' abstract class for the interface. One that comes to mind is the SAX parser's DefaultHandler Consider an abstract class (marked abstract even though it implements the method): 

It's not complete BS. Your use-case is somewhat surprising, and your disregard for using a Map is also surprising, but given those, you're left with little option other than a completely custom implementation.... but, let's look at the issues you currently have... People need to do a round-trip query-before-inserting operation - they need to call before setting a new value. People can't use all the methods available to them. For example, they can't call: 

There's two things in there, firstly, the comment is a lie ;-) The code does nothing with 8 or 9 decimals... it rounds "half-up". The second issue is that the rounding is inaccurate if/when the is something like , which your code will round-up to 60. This is a problem, because it should round up to 0, and the minutes should be increased (and, worse, if the minutes is 59, you need to round up the degrees... for example, rounding the value 8°59'59.8" should be 9° 0' 0" seconds - but your code gives 8° 59' 60") Half-up rounding There's an easy trick for half-up rounding when dealing with integer-casting in programs, the trick is to add a half-unit to the source value before doing any calculations, and then truncating the result as an integer. For example, to half-up round a float value to an int, you do You can use this in your code quite nicely. I would also reverse the logic you have in your function... convert everything to seconds (half-up rounded in ) and then extract the values from that. Note that you can express the location in total seconds, total minutes, and total degrees, and then get the "remainder" (modulo) to get just the incremental part: 

Your title specifically lists efficiency as being significant. If someone were to call your sort as: 

leave the code as is, and assume it does something good, or, at least does nothing bad benchmark the code yourself, and confirm the benefits and then fix it, or move on (satisfy your curiosity) speak with the programmer who added the code, and see if there is something you are missing. 

Actually, that's a good example of a bad failure. All the dd commands will fail, doing nothing, and then, at the end, you will have: 

The concept here is great, I like it, and I can see real usefulness for it. The Content class is unnecessary, it is just a logic container that does not add value. All the methods in Align simply delegate to Content anyway, and there is exactly one content per Align. Instead of Content, you could simply have and make the logic in there more private.... I am not sure why was protected anyway. There may be a slight performance benefit to tracking the max-sizes of each column as the data is added, and conditionally re-calculate it if data is removed, or set (and the removed value is the same size as the max for that column). You can probably save a fair amount of memory churn if you were to pre-calculate the size of the in the : 

Style Why make the an input parameter, but not the group length (3)? The is also... out of place, either you have DEFAULT values for both, or you have for none, but the mixed concept is cumbersome. Apart from that incongruity, the code is otherwise really quite neat, and understandable. For what it's worth, I tend to use magic numbers, and not constants, when confronted with these situations. I am aware that magic numbers are not generally welcomed, but your code here: 

but that is overkill, and unnecessary. Edit: In your comments you raised the issue of having multiple different-length values. I would recommend that you create a constant for the longest one (whether you use the String-constant or some other generator method), and then do a on that to get the shorter constants... alternatively, I would consider a helper function as useful for this problem.... 

always put each SQL clause on it's own line (the FROM on a new line). Try to indent each clause in the where clause so that the precedence of the logic is represented by the indentation. 

The first answer I gave was based on the premise that the data could be 'cloned' out of the HashMap. The alternative way for processing the data, as suggested, is a form of serializing the data away to a slow target (disk, network, etc.). That serialization cannot happen while holding a lock on the source. The current implementation accomplishes this by 'freezing' the underlying datastore, and then dumping that datastore to the output. To keep the system available, it also creates a 'hold and store' mechanism that tracks changes made, and then, when the serialization is complete, it 'replays' the changes to the underlying store. The problems with that system are numerous: 

Review You have posted one method for review, but that method has multiple responsibilities. Good code follows the "Single Responsibility Principle". Your method does the following things: 

About your question for "looping" the values, and adding each of them... well, that will be clear in the logic, but will also be slower. , and both use low-level code to make the memory transfers happen quickly, in bulk, and will outperform a higher-level loop. 

Your regex is overly complicated, I must admit. The negative lookahead is going to do a lot of work to identify all the negative cases before even looking for (nearly) positive matches. I think the trick you are missing is the word-boundary anchor. Consider the following regex: 

the name - it is not a "Suffix" Tree, it is an "Infix" tree. It does not match only suffixes. the overhead - The HashMap instances and the Character and Node classes, are a problem from a memory perspective. Sure, the count of these instances will be relatively small, but, for "bananas", you are creating about.... 28 HashMaps? Each HashMap has a significant memory footprint. It is expensive. 

There is always a way to improve things. But, sometimes improvements are opinion based... ;-) In general I dislike solutions that use generous amounts of AutoBoxing (int to Integer, boolean to Boolean, etc.). In your case, the actual work that Java does (and memory it uses) to manage the objects required to solve the solution is far more than the actual work of the problem itself. I believe the following solution is faster, smaller, and all those things, but it may need a little explaining. I also like that it returns the values in sorted order, and as an int[] array which is the same format as the input. It's only heap-space data usage is for the copy of the input array and the small result array... (unlike all the Integers and Booleans and Map.Objects of the Collection-based solutions. 

The above query will do no direct joins on either the supply or the history table, and will just do a join on the aggregated results. There may be many items, but it should be a lot faster than the many subselects you currently have. The case-statement makes it work. Even though the two aggregating statements are represented as subselects (in from clauses), they have no references back to the outer table so there is no need for the database to do the join to the outer until after the aggregation. I have put this together as an SQLFiddle 

Separate Static class, and interface In this case, there is normally a class with only static methods, and an interface that is returned from these methods. For your code, it would require three classs (the enum would be separate).... 

The idea was that if any one disk failed, I could replace things with minimal loss. The OS was supposed to be stuff that was easy to reinstall. The 'valuable' things are data that is irreplaceable (e-mail, photos, documents, etc.). The backup disk contains a copy of the valuable data. I also have learned (a long time ago) that keeping backups of your current data is not very useful if you corrupt (or delete) your current data, and then replace your backups with the corrupt version. As a result, I keep 'snapshots' of my valuable data at regular intervals, and I can go back to any snapshot to retrieve the data as it was at the time of the snapshot. If I delete a file now, I can go to a previous snapshot, and restore it. I started using a script written in bash to do this for me.... Mike's handy backup script! This worked for a while, but I discovered it had problems: 

although I imagine this is done as an education ploy. Integer as the example Because of the int auto-bocxing I worry that Integer may not be the right choice for data type. You may want to consider a non-primitive as the data. Autoboxing is the sort of thing that will confuse. Conclusion Otherwise, I don't see much in the way of problems. 

This ensures that there are an even number of 0 values, and all other values are 1's. The second regular expression is the opposite, it ensures there's an even number of 1 bits, and the other bits are 0. Combine them together and you can count them easily.... See the pattern working in Java here. Note that my preference solution would simply be to ensure there are an even number of characters in total (a ) and then to just ensure an even number of the 0-bits: 

The GC will only nequeue newly cleared soft references. Further, from the SoftReference.get() method, we have: 

pre-process the 'term', in to it's length, and the count of each letter. read the entire words file discard wrong-sized words check that each letter in the right-size word is in the term make sure the letters appear the right number of times report the matches. 

Ignoring Generics warnings normally means you are doing something wrong. In this case, you are. You are doing a cast to a type where you have no certainty, in fact, you are certain that it will fail.... .... the solution here, is to not have a default constructor. It does not make sense for you. Why do you think you need one? Also, why does extend ?