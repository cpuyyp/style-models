One must first distinguish two different senses in which the Allais Paradox can be seen as a "contradiction" of vNM independence; these correspond to the two different interpretations which can be given to any model in Decision Theory. (von Neumann-Morgenstern (vNM) utility theory is one such model.) According to the descriptive interpretation, a model in Decision Theory is supposed to provide a description of how real, actual human beings actually make decisions. According to the normative interpretation, the model is supposed to tell us how an "ideal rational agent" should make decisions. To put it another way, the descriptive interpretation matters to you if you are trying to construct a predictive model of insurance markets or financial markets or some other social phenomenon where people make risky decisions; the normative interpretation matters to you if you yourself are making risky decisions, or if you are trying advise someone who is making such decisions. The Allais Paradox certainly falsifies the descriptive interpretation of the vNM model; this simply means that, as a matter of empirical fact, most people (including, apparently, many economists) will choose 1A and 2B, which cannot be explained as maximizing the expected value of any possible vNM utility function. Whether the Allais Paradox falsifies the normative interpretation of the vNM model is a more subtle and controversial question. Basically it depends whether or not you think that {1A, 2B} is really the "correct" answer. Some people strongly feel that {1A, 2B} is correct; for them, this is evidence that vNM is not the correct normative model of decision-making under risk. Other people are initially tempted by {1A, 2B}, but once they recognize that it is inconsistent with vNM, they are willing to "bite the bullet" and reject their own intuitions in favor of the theory; their ultimate position is that the normative implications of vNM are counterintuitive, but correct. (The fact that a theory has "counterintuitive" implications does not make it wrong; modern physics is full of theories like this. Indeed, there are even mathematical theorems which are "counterintuitive", but evidently they are correct as a matter of logical necessity.) In your post, you wrote: 

Just to synthesize the other two answers together a bit and "join-the-dots". Setting the Scene Since 2011 SNB had decided that it wanted to prevent the CHF from strengthening past 1.2 CHF per EUR. In order to do this it had to do either or both of: 

Firstly there are services like this in Spotify, and even radio and tv, but it sounds like you are talking about downloading the material with ads in. That causes a problem. Revenue from ads relies on giving many ads to many people. Each time you listen to a song the provider needs to be able to provide a new ad. If you download a song or book with ads then those ads are fixed. So you might listen to it 100 times but no advertiser will pay 100x as much because it is not valuable to give the same ad to the same person many times. Secondly you would need some added security on the files to make it hard to simply remove the ads. That is easily as difficult and expensive as DRM anyway. The third problem is ads are annoying. I think for many people an ad would be more incentive to pirate than a small cost. Early music DRM eventually failed (in part) because it was to motivate piracy in itself. Xkcd had a nice comic on that idea. And that leads to the forth point. Much of economics is about equilibria and long run expectation. In reality everything takes time and big bold change is even harder to start. Initially music was DRM protected but they eventually realised most people will pay 99c to avoid the hassle of piracy, but will pirate to avoid DRM Nothing changed in the economics, it just took time for the industry to realise that was a better way yo do things. There may well be a book publisher out there working on an ad supported eReader library, but it will take time. To put the price of a song in context: The US median hourly wage is \$17.091, the median song length of the top 100 iTunes songs is 223 seconds2. So 50% of US workers earn \$1.06 in the time it takes to play a song. 

I don't think "approximately the same utility" is good enough ---as long as the agent has any nontrivial vNM utility function, the choice pattern {1A, 2B} is impossible. Perhaps what you mean is that the choice pattern {1A, 2B} can be reconciled with vNM theory if the agent's vNM utility function is trivial (i.e. constantly zero); in this case, the agent would be indifferent between all possible lotteries. As a purely mathematical point, this is correct. But it is not consistent with people's preferences; most people experience a strict preference for 1A over 1B, and a strict preference for 2B over 2A. So we cannot "explain away" the paradox by positing trivial utility functions. Your post ends with the question: 

Perhaps you will be interested in the Ellsberg Paradox. (I won't bother including a complete description here, since there is a very nice Wikipedia article about it.) The Ellsberg Paradox differs from the Allais Paradox in that the probabilities of certain events are left unspecified; the decision-maker must construct her own "subjective probabilistic beliefs" about these events. But the interesting fact is that there is no set of "subjective probabilistic beliefs" which can explain the pattern of choices most people make in the Ellsberg Paradox. So it definitely falsifies the "descriptive" validity of expected utility (EU) maximization as a model of human decision-making. More importantly, it presents a strong challenge to the "normative" validity of EU-maximization; even many economic theorists who "bite the bullet" in the Allais Paradox are willing to acknowledge that in the Ellsberg Paradox, the choice pattern which is inconsistent with EU-maximization is much more compelling than a choice pattern which is consistent with EU-maximization. However, I should acknowledge that the Ellsberg Paradox is properly understood as a challenge to EU-maximization in a setting of "ambiguity" (where probabilities are not known a priori), whereas the Allais Paradox is already a challenge to EU-maximization even in a setting of "risk" (where all probabilities are known). It is a perfectly consistent position to accept the vNM axioms in a setting of risk (and hence, "bite the bullet" in the Allais Paradox), while rejecting EU-maximization (i.e. rejecting the Savage Theorem) in a setting of uncertainty. Another, entirely different normative challenge to vNM independence arises in a setting of social choice. Suppose that you have an indivisible resource (e.g. a hard candy), which you must allocate to one of two claimants (e.g. two children, Alice and Bob). Suppose that, a priori, both claimaints have an equal claim to the resource (they are equally deserving, equally hungry, etc.). The resource is indivisible, so you can't "split" it between the two claimants. Question: should you allocate the resource by some random mechanism (e.g. by flipping a fair coin)? Or should you allocate it by some deterministic mechanism (e.g. give it to the older child, or the one whose name comes first in alphabetical order, or the one who is standing further to the left at the moment the decision is made)? Most people have a strong intuition that a random mechanism is more "fair" than a deterministic mechanism. So they prefer to randomize. But at the same time, they are indifferent between giving the resource to Alice and giving it to Bob (recall: Alice and Bob are equally deserving of the resource). So if A is the outcome "give it to Alice", and B is the outcome "give it to Bob", then we have a situation where the decision maker is indifferent between A and B, but strictly prefers the lottery (1/2 A, 1/2 B) to either A or B. Clearly, this normative position cannot be explained by the vNM theory. This is sometimes called the Diamond Paradox, since it was originally pointed out in a 1967 article by Peter Diamond. 

An exchange can handle more than one currency, but this would be uncommon for equities. An example is the ICE futures exchange in Europe. Multiple currencies doesn't cause any particular problems - transactions are carried out in currency and it is up to you to convert to whatever currency you prefer. Usually done by fx sweeps by your custodian bank. For margin purposes combined exposure (e.g risk netting) can be calculated at prevailing fx rates. Each security only trades in one currency, and each security trades basically like a mini exchange so there isn't really any problem. Think of it like two exchanges run by the same company. 

Keeping rates low at the short end is easy enough - that's the base rate. But that was scuppered by the low EUR short rates. Keeping rates low at the long end is trickier, but can be done by not issuing too many bonds. Selling CHF is fine, but that means buying other currencies. By the end of December the SNB had bought 85% of GDP worth of foreign currencies. At this point the SNB decided that it did not want to keep expanding the balance sheet. Why it decided this is a different question (as the question points out). The question is how to end a cap once you hold a huge Euro position. The Sudden Act Once the SNB had decided it could not defend the cap, it had three ways it could act. 

The microstructure noise is, roughly speaking, the small-scale noise introduced into the market as a result of the way the market is designed. For example, suppose that there is an asset and the "real" price for this, is 126.6. That is, if we magically knew everyone's honest, perfect maximum buy and minimum sell prices and could match them off and arrive at the equilibrium price, then it would be 126.6. However, imagine that the market only quotes in whole numbers. That means you can either buy at 127 or sell at 126, but it is physically impossible to trade at 126.6. Intuitively we expect to see a sequence of trades switching between 126 and 127. This is called "bid ask bounce" and is perhaps the simplest example of microstructure. Even with such a trivial example, we can start to do interesting things. For example, since the price is closer to 127 than 126, we might expect to see more 127s than 126s. We could use a simple logit function or something to select the probability the next tick is on the near or far side. Also we can "what-if" the market has a different tick size? Clearly we expect in our model the bid ask bounce to reduce. In our example the latent price is a constant, but that is not very realistic. For a better model one would usually assume some dynamic model for this hidden price process. Perhaps Brownian motion, for example. Now you can see how with our model from above, we will see trade prices bouncing either side of our hidden price. As the price gets closer to a whole number, more trades are done on that, until it crosses and then we start seeing trades on the next tick along. As you can imagine, you can continue to play this game, making the latent price process more complex, and adding more complex microstructure models to generate the trades around that price. And all along we have assumed independence of latent price and microstructure noise. That is to say, our microstructure process does not impact the latent price, and the microstructure process stays the same, regardless of what the price is doing. This is a useful assumption to make, since it separates the problem out - without it you probably wouldn't be able to solve much and the usefulness may be limited. That said, it is easy enough to think up possible models that would break this: e.g.