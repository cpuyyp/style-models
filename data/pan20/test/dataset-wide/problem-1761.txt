If you're good on the above, a too-deep file/folder path might be giving you some grief. Try navigating several folders towards the root, then cutting and pasting them to reduce some of that path. Alternately, you could rename the folders in the path to "1", "2", "3", etc. Cutting down on some of the extraneous characters. 

Oh, also.. Be forewarned, if you're using AD-integrated dynamic DNS, where your machines automatically update themselves in your DC's, once you kick down that lease time, you're going to need to compensate by scavenging those DNS entries. Maybe you could append that line up there and add, "&& ipconfig /registerdns" HTH.. 

Let's try the easy ones first: (In Internet Explorer; and I'm assuming you're using IE because you're running IIS on 127.0.0.1 and you reference the hosts file in \drivers\etc.) 

You can use the "s3cmd" utility with the "sync" option, although I stumbled on your question because I'm trying to figure out if this syncing mechanism is screwing up my duplicity backups. 

are there any shortcuts that can be taken on this initial "resync", since there's not (AFAIK) any data I need copied between the empty disks can I blithely format and treat it like a normal disk while it's in the middle of its initial sync, as long as I'm okay with it being in a "degraded" state for a bit? 

We have a web service running on Amazon EC2. Currently we have some live user data stored on a single disk (EBS). We are considering moving to a RAID0 setup (we don't have to be concerned about the increased failure rate). If we do this migration, what is the quickest (to minimize site unavailability) way to reliably transfer the user data to the RAID array? One idea I had was to take a recent snapshot of the data, copy it over to the new RAID array, then when the site goes down for maintenance use rsync to copy only the changed data over. I'm not sure if this would actually save time or ensure data integrity though. 

Try restarting the netlogon service on your DC? (Or probably in your case, just giving it a good reboot wouldn't hurt) 

We enforce a 90-day password expiry on everyone here, (including ourselves.) Mostly because it's simply best-practices. Chances of someone using a "weak" password, vs. a stronger one is greater and the longer you leave it the same would possibly result in a long-term, undetected security breach. 

Two suggestions: 1.) Make sure that ANY local volume shadow copies you're taking, are turned off. We've had QUITE a bit of conflicting information from Symantec regarding running local VSC, but have found that when we talked to the techs, they've all advised we don't run VSC on the same server. 2.) If you absolutely, positively have to run VSC, (like, there's a group of users who love deleting things and making you restore them via VSC and they're holding your family hostage or something) you could try resetting the entire kit-and-caboodle by using "vssadmin" to delete all current present volume shadow copies, then delete them. In addition to the above, have you presented/removed a disk, (such as from a SAN via fiberchannel, etc?) recently? Make sure it's no longer present in your system, and that you haven't inadvertently attempted to take a volume shadow copy snapshot of it. Along the same side of the coin, you may have "stale" volume shadow copy snapshots present for disks that are no longer there. That could be exacerbating your issue. Suggest same above, to delete those, since I believe that vssadmin is the only way you can see some of them. Don't quote me on this, but I think that you also have to expand the default size of the VSC from 300 to 500, however I'm not entirely sure. I do remember that it took me quite a bit of googling though and in the end, this made Symantec happy. Sorry, ordinarily I'd be more clear and try to give some steps but I can't seem to find where I documented this, the last time it worked and the primary backup guy is out for the day, (only other guy who'd have them in his email.) Good luck! 

I'm using fabric for provisioning instances from scratch (a stock ubuntu image) on EC2; it works great. There are a few basic functions for appending to and commenting lines in a file that make modifying config files workable. 

...to change my passphrase for my key, but I'm not sure what this means. If I'm encrypting data on box A and decrypting on box B (say with duplicity) do I have to change the passphrase on both ends? Will previous backups still work? Is the passphrase just the key to a sort of encrypted wrapper around the key file? Dumb question, but I don't want to screw this up. Thanks! 

EDIT: It turns out by adding a after the SSH server comes up and before trying it works fine. This seems to be an issue with the configuration of apt on this AMI combined with perhaps some strangeness on Amazon's end, or DNS issues. Here are my entries in : 

We use the default Microsoft quota system on the file servers, specifically when it comes to home drives. Other than that, one of our admins down south of us has a tendency to let the users' project spaces fill up, then encourages them to run duplicate finders and large-file/last-accessed programs such as "Doublekiller", "Easy Duplicate Finder" and "JDisk Report", which is one of my personal favorites. Another solution we're looking at implmenting is Symantec's "Enterprise Vault" especially if you're doing disk-disk-tape or have the tape storage available to shuffle files off to. 

First of all, you've got good backups, right? :) Always a good idea to make sure that you're all backed up, just in case a disk decides to not spin up, etc. when you bring everything back online. Nothing like breathing easy after a power outage then realizing that one critical SQL server never came back. :) Second, you're right, gracefully shut down and turn off all servers, etc. If it's not a modern building and/or you don't have a UPS in-line with surge protection, (which it sounds like) or anything like that, it's always better being safe rather than sorry, and I would suggest unplugging equipment. Bringing power back online could result in a power spike. Also, a little FYI; bring your core servers down last, (such as DNS, etc.) You don't want to bring down your DNS servers and find yourself unable to resolve the rest of the servers if you're shutting them off remotely :) When bringing everything back online, bring the network equipment up first, then your core critical servers, then finally your app/file servers. Good luck! 

I'm using an Alestic image which disables root SSH logins, but provides a user "ubuntu" with NOPASSWD sudo privileges. See here. In the course of trying to add a new user to the sudoers file I inadvertantly created another line for the "ubuntu" user, this time without NOPASSWD. I have now apparently lost root access to this machine. Is there some way to mount the EBS root volume on a different instance (fixing the sudoers file) and then re-launch the server? Or am I totally screwed? 

I'm experiencing a strange issue with a fabric script I'm using to bootstrap a server on EC2. I launch a stock Ubuntu 12.04 AMI (ami-3d4ff254), wait for it to start, then proceed with: 

Shouldn't be too many to change by hand, and hopefully your system doesn't differ too much from mine. 

Another suggestion, (although it's a little risky/beta right now) I've been experimenting with Windows Live Mesh. It's got 5Gb of storage for free and I could see a setup of something like this: 

With those, you can likely derive what you need. I'd hazard an educated guess that your answer lays within the "FROM" and "TO" .cfg entries. As one more alternative, you could always skip those above and simply write a batch script that will move/archive or delete the previous week of logfiles -out_ of the default "\LogFiles\W3SVC1" location to say, something like "D:\IIS-Logfiles-Archive", and then run the analog analyzer for the day. Some pseudocode that might help you get started and moving in the right direction would be: (and again, this is PSEUDO-code. Rough, ugly and fits like a wet wool sweater. There probably isn't a need to delete the directory, but I'm cribbing from another script that I use, mmmkay? :) 

I understand it displays the command with arguments, or when unavailable the command in square brackets. But where do the names come from for processes such as passenger worker ruby instances, which show up as: 

I have a command that runs a disk snapshot (on EC2, freezing an XFS disk and running an EBS snapshot command), which is set to run on a regular schedule as a cron job. Ideally I would like to be able to have the command delayed for a period of time if the disk is being used heavily at the moment the task is scheduled to run. I'm afraid that using nice/ionice might not have the proper effect, as I would like the script to run with high priority while it is running (i.e. wait for a good time, then finish fast). Thanks. UPDATE: This is what I ended up going with. It checks /proc/diskstats and runs my job when the current IO activity hits 0, or we timeout. I'll probably have to tweak this when I look at what kind of IO activity our servers actually get in production: 

We run firewalls on all local workstations where I work and we view it as a "good thing", insofar as has been mentioned previously, some lunkhead could bring a personal laptop in that's infected with a virus or worm and release it into the `trusted' corporate network. Running a solution like Symantec or McAfee, you can centrally manage the firewall rulesets on all clients to quickly respond to a fast-spreading worm, (then again, if you're a quick thinker on your feet, you can also create pre-set ACL's on interconnecting switches and routers within your LAN to potentially block a malicious code from spreading, once you know how it traverses a network.) That being said, be advised that you MAY POSSIBLY run into application issues where your clients need to communicate with servers on specific ports, etc. To summarize; Firewalls on XP, Vista, etc. workstations = good thing. Do not deploy onto servers, unless you're ready to document and tweak for clients connecting to applications. 

The appears to run fine and gives no errors, however (2/3 of the time or so) installing throws a "no installation candidate" error. When I ssh into the server and run I get the same error. Running by hand, then the package installs fine. Any ideas on what might be happening, or ideas for debugging? 

What specificaly is involved in an Elastic Load Balancer health check on an instance? I know that it performas an HTTP(S) "ping"; does it just deem an instance "Unhealthy" if that HTTP request returns an error status number of times in a row? Or does it take other factors into account, such as CPU usage? 

I would like to configure sudo such that users can run some specific commands without entering a password (for convenience) and can run all other commands by entering a password. This is what I have, but this does not work; a password is always required: