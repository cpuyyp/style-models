Try running on the slave to make sure it has got the you think it does. If not you can run and make sure it is set up in the file for when you restart the database. Also check the error logs as there may be additional info in there that can help. 

But I am using MySQL 5.5. Is this the correct version of innobackupex to use with my databases, and if not which version should I use? The documentation isn't very clear. I installed it using the following process: 

Is there a way to do this? as I really need the datetime field in the database, so I don't have to do the every time I run a query. I was also looking to partition the table by date, as there will be a lot of data produced, and so I only want to keep the minimum amount of data, and then drop the oldest partitions once I am done. 

1 & 2) You should look at and variables. This prevents the two masters from creating the same Primary Key (assuming you are using Values). 3) Not Natively, for this you would need to look at moving to one of the Galera Cluster variants. As it stands I wouldn't recommend an Active Active environment because you will almost certainly run into problems when the network drops out / replication stops on one or both sides. 

I'm currently investigating moving our database to an (vs buying bigger servers (again) and moving the master / slaves to these). So far my testing on a small cluster has been fruitful, and suggests that we would benefit from this (especially in data resiliency / write access). However I have one small niggling concern. We have one large table (apx 30% of the whole database size - or 60GB apx at present) which is comprised entirely of BLOB fields (80+), and a single field at the start to identify the row. Few of these contain very much data (mainly some text/ numbers/ dates etc), many are actually empty (NULL). None of these are indexed. We add new rows constantly, and read the old rows frequently too. Updates are few and far between. Is this likely to become an issue when running this in a cluster? From my reading I understand that BLOB fields are held in separate tables/pages in the background, so need to be read separately to the main data. But can't see if this is like INNODB, where the first x data is stored in the main table, the rest held separately, or if the whole thing is separate. I ran some simple tests, inserting records with similar data, (both disk based and normal), with various joins etc. which showed no differences. But I am concerned that my small test system simply isn't stressing it enough, and don't have the resources (or finances) to run a full size test yet. Can anyone with some experience of this tell me if I am going the right way with this, or am I just walking into a nightmare? 

If they are not, then you will need to add some logic to the interior select statement to convert Ageing from whatever it is into those string values (e.g. '0-30', '365+'). As an example to check this, I did the following: 

I have a database with its primary file and log file stored locally. One table is split into 101 partitions which are stored on a Windows share on another server accessed by a UNC path. Each partition filegroup file is 1GB in size. Yesterday, I ran a bulk process to sequentially load data into partitions 2 through 58. The partition schema ensured that none of the files were filled to the 1GB capacity. The load did not report any errors. Today, when I attempted to retrieve data, I began to see three distinct errors: 

I was able to query all of the empty partitions (1, 59-101) without error. In addition, I was able to query partitions 25, 26, and 49-58 without error. When querying partition 39, I received the third error message each time. When querying any other partition, I typically received the first error message, but sporadically received the second error message. After repeating my queries several times, the second error message went away, and I only received the first and third error messages. [The numbering of partitions corresponds to the order in which data was inserted into them, with 2 being the first data-containing partition, and 1 being an empty partition reserved at the beginning.] DBCC CHECKDB also failed, and it returned the following: 

Assuming that the Ageing values match the names of the columns in your expected output, then this should work: 

Based on your sqlfiddle link, here's a brute force way to get what you're looking for. I inject an ordering value for each result set being unioned, group on matching values to get the minimum order, and then order by the minimum order for each grouped value. 

(-sorry if I have misunderstood) (-assuming you are using binlog file and pos). The key things to make sure of are that your servers have unique (if you have replication running already then this should be ok). You also probably want to set: 

Or would an index on only suffice? It's quite a large table so I don't want to spend time adding the index if it is unlikely to help. 

I'm trying to install a Plugin on my MySQL database, but each time I try I get a Duplicate Entry error: 

I'm guessing I'm doing something that's prohibited by MySQL, but can't see what. Basically, I have a set of tables that hold lists of things (, , etc.), and I need to create a joining table to hold each of the ID's (e.g. , , ). At present there is no real relationship between the various lists. This table needs to be automatically populated each time I add a new . To get the matching entry, I need to use a statement to match the first letters of the , and get the relevant entry from the other table(s). My plan is to have a set of Procedures (one for each table I need to join): 

I'm trying to install the Percona Audit_log plugin for my instance of MySQL. I ran it previously so I know it works ok with MySQL (I'm sure it says so in the documentation anyway). I have MySQL 5.5.47, so I downloaded the files for Percona server 5.5.47-37.7 and extracted the file. I put that file into my plugin directory on the server (debian), but when I try to install it I get: 

values so that the two Databases don't generate overlapping primary keys. $URL$ After that on your Slave DB (B), simply run . Use those details to issue the statement on DB A. What you should find is that anything executed on A is then replicated to B and vice versa. However anything executed on A shouldn't be replicated back from B to A. And as such, the only queries written back to A will be those that are executed directly on B. Finally, you will want to set B to read_only to prevent anyone accidentally writing to it, until such time as you want them to. 

Unfortunately I no longer have an access to old servers. I have a backup though. I checked mentioned configuration options and our traffic. Is there something that could increase amount of binary logs I didn't think of? Please, point me somewhere :) Edit 1: I doubt that traffic profile changed a lot. Only a fixed number (fixed = controlled by us) of devices are communicating with this database. Those devices are logging but I might say it's a stable amount of traffic. Unfortunately I'm new and I don't have much data to compare with. Database is working for over a month, so there shouldn't be any leftovers from migration itself. Maybe I should ask a little different question. Is there anything in database setup that might change weight of binary logs? Some additional metadata? Verbosity? Edit 2: Yes, server id's are different on both machines: 

or similar do the trick (some data de-synchronization between master ans slave is not a big issue for my app)? I was looking into some alternatives like or to make a backup and simply copy files later but: needs at least twice as much space as database and, because of that, I cannot use it. restore time is huge comparing to copying files. Taking above under consideration I would love to stick to 'cold' backup with snapshot or a solution like that could provide compressed data without intermediate steps. 

We were moving our servers to newer machines. And now I spotted that binary logs on master database takes about 18G of space. Previously it was about 3.5G, so change is quite big (~5 times). I didn't notice traffic increase since then (maybe 10%, but not 500%!). There were only small changes in configuration as our goal was to migrate our software. We changed: 

I'm trying to find a way of comparing two tables in two databases (master and slave), as I have applied some slightly different settings to one, and need to see what affect it has (if any) on the data etc. Ordinarily I would use which has worked well in the past, but this doesn't seem to have the option for a single table, and is too limited. So I downloaded to my Debian Server (the slave) and am looking at . But the Page on the Percona website is very little help, instead filled with all manner of options, but seemingly no examples. I set up a schema, with one table, and diffent data. So far I have got as far as: 

Has anyone found a way yet of circumventing MySQL 5.7s insistence that it's Master must have a . I currently have multiple instances of MySQL 5.5 over various sites, and want to look at upgrading to MySQL 5.7. To do this I was planning to set up a test-site using some MySQL 5.7 databases running as a set of master > slaves from my main site. But this failed at the first hurdle, as MySQL 5.7 insists that the Master must have a !! Depsite the official documentation telling us we should upgrade our slave databases before we upgrade the master. 

In the end, I renamed the entry in , and then I was able to install it. Once I checked it was OK I then removed the entry in . So far it seems to be working fine, 

I'm just testing MySQL CLuster 7.3.8. According to the documentation, after adding new Data Nodes I need to ALTER my tables to spread the data to the new Data Nodes. So, I run: 

Note that this answer actually gives six full years of data - the year selected and the five preceding years. 

In my first attempt to fix things, I rebooted the remote storage server. This did not change any of the observed behavior. In my second attempt to fix things, I restarted the SQL Server instance. This eliminated all of the error messages mentioned above. After doing this, I was able to complete a DBCC CHECKDB successfully, and I was able to retrieve data from each partition of the table. Two things that stood out to me while researching this: 

Okay, I think that I'm beginning to understand the nuance of your question. Given that space efficiency is a high priority, I think that you can strike the varchar(max) option from your list. Since your minimum file size is 11K, nothing is going to fit on a single page. All the files will be stored as BLOBs. SQL Server won't compress that (aside from compressing the backup), so you're not going to save any space with that approach. Next, I'm going to make the assumption that processing time can be traded for space efficiency. In that case, I would recommend that you compress the files yourself before storing them and decompress before displaying them. This goes for any solution you end up going with. You should end up with better compression than letting the file system handle it (which applies if you use a Filestream). So I think that your question boils down to Filestream (and possibly FileTable) or VARBINARY(MAX). The standard answer provided by Microsoft is: 

The database is now online on the second server, and I can query data from filegroups PRIMARY and C. The database is aware of filegroups A and B, but they are marked as offline. Is there a way to restore the remaining two read only filegroups directly from the .NDF files? 

Looking at you config, it seems you are missing in your my.cnf files. You will need to add this in to both masters files and restart them. This tells the slave database to take anything it receives from it's master, and copy from the relay log to the binary log so it can be replicated onwards. e.g. Master 1 Updates a table. It gets written to the binary log on Master1 This is replicated to Master 1's slaves (which are slave 1 and Master2) In order for it to proceed to slave 2, master 2 has to be told to add it to it's binary log. This is what does. 

In the given example each has two data elements and . I want to get the data elements for and compare them to So with the above Test Data I would end up with: Desired Result: 

Bear in mind that there is apx 1000 entries in the first section, and apx 1000 entries in the second section, how do I identify which row was missing? Is there perhaps a better tool than mysqlbinlog to help in these situations? 

I'm trying to put together a function to convert a size from one type to another (e.g. bytes >> gigabytes). An extract is below 

The main errors I saw were: and - no need to use - no need for I included the line as this caused an error on my side otherwise. I believe this is related to replication, so depending on your set up you may not need it. 

The config items you added in should have little to no impact on the situation. n.b. you will probably also want , (details) but leave that out until after you've imported the data, of you'll have 100's of GB's of binary logs created. I think the real question is whether your OS is happy with a single file that size, bearing in mind that it wont get smaller, but may get larger. If it's going to be an issue you may want to look at (details) which 'basically' splits the data between and a file for each table. (This involves reloading the dumpfile to take affect) (this is on by default from MySQL 5.6.6) There are pros and cons for both options.