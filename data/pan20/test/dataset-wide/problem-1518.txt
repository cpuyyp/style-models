If you are overestimating the upper bound by heaps, which causes your branch and bound to never "bound". Your second relaxation is not an upper bound I'm assuming your linear relaxation is for the upper bound. As such your first function to calculate the upper bound is good (bar the bug above). The second function is no longer an upper bound because you are essentially taking a greedy solution as the upper bound. Example that triggers faulty upper bound: Define the value and weight of item i as: and respectively. Define the ratio of value to weight for item i as: . The knapsack can carry at most units of mass. We have three items as follows: 

You can be certain that they have constructed a test case which never terminates early from the returns and you must infact run all the \$O(n^2)\$ iterations. So without reading the problem description, nor the rest of the code. I'm going to guess that there's your problem and they expect you to do better than \$O(n^2)\$. 

This whole allocation and disposal of objects seem to be centered around the fact that you're using a to represent your live-state of the game. This may also be a cause for your performance hit, every access you make to the map requires that you must calculate the hash-code of these positions over and over again. It would probably be faster if you just used as key and used a custom hash function like so . The function is collision free and faster to calculate than the of your class. Also you avoid straining the GC. Another thing that might be even faster is to simply have a large array and let dead cells be . Access is done by which avoids the lookup of the map and the computation of the hash code. It also will allow you to ditch allocating objects all over the place. However it does mean that the resize behavior of the game changes and you will have to reallocate and copy the field when that happens. However that is a rare event compared to how often the simulation is ran so I believe you will gain performance on this. Summary This is as far as I feel it is useful to go in this review. The best thing you can do is to separate your model, view and controller and reduce the load on the GC in your game code. The frequent allocation and disposal of objects and the expensive hashCode function is a likely suspect for your performance issues. 

The singleton pattern has all those properties with the benefit of automatic destruction when your application terminates by normal means. 

This reduces from 4 mult + 3 add to 3 mult + 3 add. Here you need to make sure that the compiler actually emits instruction and not : 

and so forth. See documentation for Pattern. As for your capturing groups I'm quite sure you can do some clever back referencing there but I will save my sanity and not try to parse the regexp. :) 

Late or spurious echoes One thing that can happen is that during one ping interval of 50ms you can get echoes from objects up to 8.5m away ( ). If the object is a little further than that, say 9m, you will get the echo early during the next ping period instead. As you can guess, this would be bad, it would wrongly appear as being a close echo while it in fact is distant. I'm not familiar with the strength of the ultrasonic on your chip; 8.5m may be out of range in which case this isn't the problem. But if it isn't then your library should send each ping request with a different frequency and check the frequency of the echo to know if you got the right echo, it should also check the amplitude of the echo for feasibility. Again I don't know if your library does any of this but you should check. Otherwise this might be causing your troubles. If 8.5m is just on the edge of the range it is conceivable that some times you get the echo and some times you don't. Filtering If you can rule out the above and you're certain your implementation is not at fault, then you can simply assume that it's a transient at work. They happen when dealing with sensors, and sound is finicky that way with echoes and everything. Best way to deal with transients is to use some kind of filtering. Median might work well for you and I see that NewPing has a "ping_median" function, try this with a filter size of maybe 5 or thereabouts see if that helps. Variance Using two samples to estimate any kind of variance is way too little, the result is not in any way reliable. You are also very sensitive to transients and errors are you are calculating the square of the values. I would suggest another approach. Another approach If you want to detect movement you want to detect changes in the echo delay. So what you do is setup your timer to consistently ping at a fixed rate then keep a low pass filtered running average of the echo delay and look at the differential of this (the rate of change). Some pseudocode: 

This will make sure you get the correct rounding and remove one division per channel. Also when running the code un-optimized (while debugging) this should perform slightly better due to elimination of some subexpressions. 

Your A* is incorrectly implemented You are using the total number of moves explored as the priority for each search node. See here: 

How large buffer size are you using? What is the hit/miss frequency on your buffer in ? If the hit frequency is less than 90% and you're doing as you say 80E6 per second that means you have 8 million buffer allocations per second which is going to cause the GC significant head aches. AFAIR the JVM can stall all threads while doing GC under certain circumstances (I had a similar problem developing a high-performance parallel java application). So I would change: 

Don't roll your own math libraries When making a game you will need a lot of math, rolling your own is time consuming and error prone and unless you've got a very strong background in computational mathematics likely to be slower than using a library. Personally I often use Eigen. Use default constructors to reduce code You can replace this: 

How about initializing the compensation factor so that it doesn't have to do the convergence from an undefined state? 

In a merge sort (and quick sort) you need to stop your recursion when there is a handful of iterations left (say 32 elements) and then use an in-place sort (some even use bubblesort shrugs) to sort the last elements in that range. The reason is that the recursion and splitting overhead becomes too large when the number of elements are small. 

Avoid unnecessary memory allocations. Memory allocations take time, significant time if you do them in your inner most loop. For starters, change: 

Just a note for the curious. String comparisons are memory comparisons which are slow as you have to check all bytes in the source operands for a successful or near-successful match. But they can also exit early. So comparing one \$k\$ character string to \$n\$ strings of \$k\$ characters is anywhere between \$k\$ (match on first) and \$k\cdot n\$ character comparisons (all strings differ on last character). The JIT can optimize memory comparison to use native words instead of chars in loop unrolling. The array approach suffers from the same problem. Using a computes the hashes once for all \$n\$ strings as a pre-processing step. Then comparing one \$k\$ character string is equivalent to computing the hash for the string, which is \$\mathcal{O}(k)\$ character operations and then a lookup in the hash table is just an indexing operation, \$\mathcal{O}(1)\$. So ignoring the setup cost, the runtime improvement would be \$\mathcal{O}(nk) \rightarrow \mathcal{O}(k)\$ which seems like a good deal to me. 

Style In Java it is common practice to have class names to use PascalCase (a.k.a. UpperCamelCase). Also the name doesn't make sense to me and is confusing. Also, why is the instance called ? Readability I prefer to prefix my formal parameters with for argument, for example . I believe it would help readability of your code. Please prefer descriptive names. Currently I have no idea what , and are. Nested for loops You are basically implementing a multi-byte counter. You can do something like this (not tested, but you get the idea): 

Usage remarks If you're a single developer using this in your own project then I do not see any problems with the usage. After all it's a convenience for you. I can not comment on the design on the macros as I find them harder to read than what is worth spending my free time on (this is not specific to your macros, but to complex macros in general). However if you are working in an environment where there are more developers than yourself, then I believe that this type of convenience macro is rather an inconvenience. In general I find that the further one goes to try to make C++ look like something it isn't by "inventing/implementing new syntax" using various macros, the more difficult it becomes to maintain over time. Developers come and go and the closer you stick to standard approaches to solving problems the better you will be able to handle new programmers on your project. As Michael Urman said in a comment on OP: 

chance of a re-roll. In closing That all being said and done, linear congruential generators are generally poor choices for any kind of serious pseudo random numbers overall, you should look into a better generator like for example the widely used Mersenne Twister. 

Your general approach is sound When your throws, the throw will be propagated out of the , you catch it and cancel all remaining futures. I believe it is safe to cancel a completed job so you might not need the check. But that's nitpicking. Then you shutdown your thread pool to clean up. Your implementation is a bit messy The loop where you iterate over all the futures seems a bit messy to me. For example: 

Java GUIs: Swing & AWT In standard Java there are two ways to make GUIs: Swing and AWT. To be short, Swing is platform-independent and AWT is platform-dependent. Typically you pick either AWT or Swing and stick to it. I believe Swing is the more popular choice today as it is easier to work with. You can tell Swing code from AWT code by the fact that Swing component classes are prefixed with a . For example is AWT and is the Swing equivalent. The Swing components inherit from the AWT components so a is a . Windows and Frames Now the difference between and (and and respectively) is that a is a plain window without any decorations; no borders, no title bar, no window management buttons. A is a with all these decorations. And I put emphasis on is a in the above as actually inherits . So the inheritance tree looks like this: 

Yes, I do believe that this is a bad practice. And as a presumptive employer I would be worried and put your application towards the bottom of the pile. What you are doing is essentially a singleton pattern but you're not treating it as such. The singleton pattern is one of the most basic design patterns (and probably most frequently abused). This tells me you have limited knowledge about design patterns and code design. You are also just taking procedural code and trying to make it look like it is object oriented while it really is not. Which tells me as an employer that "you just don't get object oriented programming". The exact same API and features could have been implemented with a namespace like this: 

Algorithm What you have implemented is called (linear) insertion sort. It is very old, sorry your method is not new or novel ;) It has average case O(n^2) time complexity which is basically unusable for anything with more than a few thousand/tens of thousands elements. I recommend that you study the classic algorithms: Bubble Sort, (Cocktail) Shaker Sort, Insertion Sort, Shell Sort, Merge Sort, Heap Sort, Radix Sort and Quick Sort. There is a classic video from 1981 that is part of many CS curriculums called Sorting out Sorting which is very illustrative and a recommended watch. Code You never use remove it. Once the unused type is removed you can change your template parameter to the iterator type directly and get rid of a lot of typing. This also allows template parameter deduction so that you don't have to explicitly state the container type as a template parameter. Please don't reuse the argument as a variable. It is hard to read when your iteration variable is named begin with another being original begin. Simply make a copy of begin, call it for iterator or . Typically when two iterators are used to represent a range we use the names: and . To not confuse with and and avoid a name collision when some one uses (which you shouldn't!). In general I'm not fond of your naming. For example could simply be . You don't need the inline specifier. You need a better algorithm. The compiler will automatically inline if it makes sense. Bug In the code here: 

Again YMMV the compiler may have been able to do the above transform for you, or it may not be any faster at all. It is only my gut feeling telling me that the above might be slightly better. I have not profiled and you should test to see if it gives any improvement at all. Better formulation for Lerp The current code is: 

requires the user to know that is a this isn't ideal. I believe that it's a good idea to hide the implementation class. You should put your class in an implementation namespace: 

You're almost there. You rebuild the entire HashSet for each of the i-m sub arrays resulting in (i-m)*m worrk. Change the HashSet to a HashMap and then imagine the sub array as a sliding window. When the window moves one step to the right, count down number at the far left of the old window position in the HashMap and if it becomes zero, remove it. And then add the number to the far right of the new window position to the count of that number in the HashMap (or add the number with count one if it didn't exist), and at each window position take the size of the HashMap. This is (i-m)+m work which is much faster.