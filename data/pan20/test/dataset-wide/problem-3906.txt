I got some good answers to my question, both here and in private e-mail to Bill Casselman. But in the end I decided to make my own diagram (with the aid of TikZ and Python SAGE). Here it is. alt text $URL$ 

The reason that I want a short name is that the object X itself is used to make a moduli space or an algebraic variety. The moduli space is easy to describe: Assign fixed lengths to the edges of X and look at its rigid embeddings into a metric space. So we wouldn't want to say "contractible plane continuum variety". Also, for no particular reason I've been thinking of the triangulation as an extra decoration and instead name the underlying topological space. I can certainly think of livable names, but the general idea appears in several places in mathematics and I would prefer a good name. Could someone with a good sense of the literature argue for a particular term, not necessarily one of the ones listed above? Or any useful opinion would be welcome. 

One way to confirm that a quiver has a rigid potential is to find a J(S) that contains all paths of some length. I(S) then automatically contains all loops of that length or greater. Moreover, it does not matter whether S includes any terms beyond the cutoff length. If a rigid potential always has this property, then determining whether there is one is recursive. 

See also this, which is a more modern work, in the form of a book, discussing (non-commutative) geometrical ideas related to supersymmetry although from a more -imo- phenomenological viewpoint (emphasizing the supersymmetric standard model, supersymmetry breaking, elementary particle interactions , etc). Edit: After giving some further thinking on your question and since you are asking for some method 

According to nLab, such an action is called a Hopf action and your data specify a left $B$-module algebra. Such a structure is also referred to in the literature as an algebra in the category (of left $B$-modules). Note also, that, if instead of the bialgebra $B$ we consider a hopf algebra $H$ acting on $A$ and satisfying your condition supplemented by: $$ h\triangleright 1_A=\varepsilon(h)1_A $$ where $\varepsilon:C\rightarrow k$ is the counity map of $H$, then under such an action, $A$ is called a $H$-module algebra or a Hopf-module algebra or an algebra in the category (of left $H$-modules). (You can also check section 3.1 of this article, for a review on this and other similar kinds of actions and coactions on algebras and coalgebras). Edit: If we drop the demand for the bilinear map $\triangleright: H \times A \to A$ to be an action, i.e., if we drop $(h_1h_2) \triangleright a = h_1\triangleright(h_2 \triangleright a)$ and simply require $h \triangleright(ac) = (h_{1} \triangleright a)(h_{2}\triangleright c)$ and $h\triangleright 1_A=\varepsilon(h)1_A$, then we say that the bilinear map $\triangleright$ is a measuring or that $H$ measures $A$. If, furthermore: $1_H\triangleright a=a$ for all $a\in A$ then we are speaking about a weak action of $H$ on $A$ (see def.1.1, p.674 of the linked article). The same terminology is used for the bialgebra case as well. 

Actually, the previous isn't the whole story. If $E$ and $F$ are both transcendental, then they are extensions of purely transcendental extensions $E'$ and $F'$. $E'$ and $F'$ are only weakly linearly disjoint, and therefore $E$ and $F$ are too. So the distinction is always moot. Pete and Andrew's intuition was more correct all along. The correct statement is that when $E$ and $F$ are both transcendental, linearly disjoint extensions have different behavior. 

As I see it, this posted question and some aspects of the answers turn an important but straightforward fact into something needlessly complicated and less general. Let $\mathcal{A}$ (Alice) and $\mathcal{B}$ (Bob) be $C^*$-algebras of observables, or better yet, von Neumann algebras of observables. Let $\mathcal{A}^\#$ denote the dual space of (finite but not necessarily positive) states on $\mathcal{A}$, and in the von Neumann algebra case let ${}^\#\mathcal{A}$ denote the predual space of normal states. Then a quantum channel, to model a message from Alice to Bob, is a completely positive, unital map $$E:\mathcal{B} \longrightarrow \mathcal{A}.$$ The corresponding CPTP map on states is the transpose: $$E^\#:\mathcal{A}^\# \longrightarrow \mathcal{B}^\#$$ in the von Neumann algebra case, $E$ should be normal and have a pre-transpose: $${}^\#E:{}^\#\mathcal{A} \longrightarrow {}^\#\mathcal{B}$$ Yes, quantum channels should form a category, and yes they do. Yes, you can restrict to the one-object subcategory where the object is $B(\mathcal{H})$. You need to check that quantum channels include the identity (they do) and you need to check that they are closed under composition. It is immediate that preserving 1 (the unital condition) is closed under composition. As for complete positivity, the condition is that $$E \otimes I_\mathcal{C}:\mathcal{B} \otimes \mathcal{C} \longrightarrow \mathcal{A} \otimes \mathcal{C}$$ preserves positive states for all $\mathcal{C}$. Closure of this condition under composition isn't quite immediate, but it's still very easy. Associativity is immediate because quantum channels are functions. 

No, in general the claim is not true: To see why, consider a field $k$ of characteristic $p$ and take the polynomial hopf algebra $k[x]$ (in a single variable). Then $x$ is primitive and so is $x^p$: $$ \Delta(x^p)=1\otimes x^p+x^p\otimes 1 $$ (because in characteristic $p$: $\binom{p}{i}=0$, for $1\leq i\leq p-1 \ $). However, in characteristic zero, $k[x]$ is generated by its primitive elements, which are in fact the homogeneous linear polynomials, in the sense that: $P(k[x])=kx$, as sets,and $k[x]\cong U\big(P(k[x])\big)\cong T(kx)$ as Hopf algebras. ($P(\cdot)$ denotes the Lie algebra of the primitives, $U(\cdot)$ stands for the universal enveloping algebra of $P(\cdot)$ and $T(.)$ the tensor or symmetric algebra of the one dimensional vector space $kx$). Since you are also asking for some reference, the last statement (on characteristic zero) can be found cited explicitly (apart from the celebrated Milnor-Moore paper already cited at the comments to the OP) at the following sources: 

I want to prove that the cocommutative finite dimensional Hopf algebras over an algebraically closed field of characteristic zero are group algebras (for some finite group) and that the commutative f.d. Hopf algebras (over an algebraically closed field of characteristic zero) are dual to group Hopf algebras (for some finite group). More precisely, I want to prove the following proposition: 

Here's what I have done: $\bullet$ Let the Lagrangian $L(q_{i},\dot{q}_{i},t)$, which under the point transformations $$ \{q_{i}\}\leftrightsquigarrow\{Q_{i}\} $$ given by the invertible relations $Q_{i}=Q_{i}\big(q_{j},t\big)\Leftrightarrow q_{j}=q_{j}\big(Q_{i},t\big)$, $\ \ i,j=1,2,...,n$, (i.e. $\det\Big|\frac{\partial Q_{j}}{\partial q_{i}}\Big|\neq 0$), can be written as: $$ L(q_{i},\dot{q}_{i},t)\stackrel{}{\leftrightsquigarrow}L(Q_{i},\dot{Q}_{i},t) $$ $\bullet$ Differentiating the point transformations $Q_{i}=Q_{i}\big(q_{j},t\big)$, we get that: $$ \frac{dQ_i}{dt}\equiv\dot{Q}_i=\frac{\partial Q_i}{\partial q_j}\dot{q}_i+\frac{\partial Q_i}{\partial t}\ \ \ \ \ \Rightarrow \ \ \ \ \ \frac{\partial\dot{Q}_i}{\partial\dot{q_j}}=\frac{\partial Q_i}{\partial q_j} \ \ \ \ \ \ \ \ \ \ \ \ \ (1) $$ $\bullet$ On the other hand, differentiating the Lagrangian we get that: $$ \frac{\partial L}{\partial\dot{q}_i}=\sum_{k=1}^{n}\frac{\partial L}{\partial\dot{Q}_k}\frac{\partial\dot{Q}_k}{\partial\dot{q}_i} \Rightarrow $$ $$ \Rightarrow \frac{\partial^2 L}{\partial\dot{q}_j\partial\dot{q}_i}=\frac{\partial}{\partial\dot{q}_j}\Big(\sum_{k=1}^{n}\frac{\partial L}{\partial\dot{Q}_k}\frac{\partial\dot{Q}_k}{\partial\dot{q}_i}\Big)= \sum_{m=1}^{n}\frac{\partial}{\partial\dot{Q}_m}\Big(\sum_{k=1}^{n}\frac{\partial L}{\partial\dot{Q}_k}\frac{\partial\dot{Q}_k}{\partial\dot{q}_i}\Big)\frac{\partial\dot{Q}_m}{\partial\dot{q}_j}= $$ $$ =\sum_{k,m=1}^{n}\frac{\partial^2 L}{\partial\dot{Q}_m\partial\dot{Q}_k}\frac{\partial\dot{Q}_k}{\partial\dot{q}_i}\frac{\partial\dot{Q}_m}{\partial\dot{q}_j}\stackrel{(1)}{\Rightarrow} $$ $$ \stackrel{(1)}{\Rightarrow} \frac{\partial^2 L}{\partial\dot{q}_j\partial\dot{q}_i}=\sum_{k,m=1}^{n}\frac{\partial^2 L}{\partial\dot{Q}_m\partial\dot{Q}_k}\frac{\partial Q_k}{\partial q_i}\frac{\partial Q_m}{\partial q_j} $$ Thus: $$ \Big[\frac{\partial^{2}L}{\partial\dot{q}_{i}\partial\dot{q}_{j}}\Big]=\Big[\frac{\partial Q_{j}}{\partial q_{i}}\Big]^{t} \Big[\frac{\partial^{2}L}{\partial\dot{Q}_{i}\partial\dot{Q}_{j}}\Big]\Big[\frac{\partial Q_{j}}{\partial q_{i}}\Big] $$ where $[..]$ stands for the respective Jacobian and hessian matrices and $[..]^t$ stands for the transpo-se matrix. The last relation clearly tells us that the non-vanishing of the hessian determinant is invari-ant under the point transformations, i.e. $$ \det\Big|\frac{\partial^{2}L}{\partial\dot{q}_{i}\partial\dot{q}_{j}}\Big|\neq 0 \Leftrightarrow \det\Big|\frac{\partial^{2}L}{\partial\dot{Q}_{i}\partial\dot{Q}_{j}}\Big|\neq 0 $$ which concludes the proof. P.S.: We can easily see why the above, is a sufficient condition for a Hamiltonian description to exist: Generalized momenta $p_{i}$, are defined by \begin{equation} \label{7.55} p_{i}=\frac{\partial L}{\partial\dot{q}_{i}}=p_{i}\big(q_{j},\dot{q}_{j},t\big), \ \ \ i,j=1,2,...,n \end{equation} If the Jacobian of the generalized momenta w.r.t. the generalized velocities, or equivalently the Hessian of the Lagrangian w.r.t. the generalized velocities \begin{equation} \label{7.57} \det\Big|\frac{\partial p_{i}}{\partial\dot{q}_{j}}\Big|=\det\Big|\frac{\partial^{2}L}{\partial\dot{q}_{i} \partial\dot{q}_{j}}\Big|\neq 0 \end{equation} is non-zero, then, due to the inverse function theorem in $\mathbb{R}^{n}$, the above relations can be solved w.r.t. the generalized velocities: \begin{equation} \label{7.58} \dot{q}_{j}=\dot{q}_{j}\big(q_{i},p_{i},t\big), \ \ \ i,j=1,2,...,n \end{equation} Then, the Hamiltonian function is defined through a Legendre transform: \begin{equation} \label{7.59} H=H\big(q_{i},p_{i},t\big)=\sum_{i=1}^{n}p_{i}\dot{q}_{i}-L\big(q_{i},\dot{q}_{i},t\big) \end{equation} with $i=1,2,...,n$. The Hamiltonian function $H:\mathbb{R}^{2n+1}\rightarrow\mathbb{R}$, is a function, of generalized coordinates, generalized momenta and time. 

Since you asked for a problem in algebraic geometry, here is a popular result whose proof in modern terms is very short. It could be called a one-step problem: Harnack's inequality on curves: Prove that a smooth algebraic curve of degree $d$ in $\mathbb{R}P^2$ consists of at most $(d^2-3d+4)/2$ circles. (1,1,2,4,7,11,...) 

There is a unique formal power series solution with $f(0) = 0$ and $f'(0) = 1$. I had supposed that the coefficients would all be positive, which would imply that they are smaller than for $\exp(x)$ itself and thus that $f(x)$ is entire. No such luck. Maple gives me this: $$f(x) = x + \frac{x^2}4 + \frac{x^3}{48} + \frac{x^5}{3840} - \frac{7x^6}{92160} + \frac{x^7}{645120} + \frac{53x^8}{3440640} + \cdots.$$ This doesn't say much about the possible radius of convergence of this series. On the other hand, expecting it to be entire may have been naive from the beginning, because it seems unlikely that $f(f(x))$ would be periodic in the imaginary direction. 

In a recent comment, bpowah says "All I wanted to know is whether or not it was faster to compute the given integral numerically or exactly." Here is a discussion. Computing an integral, or any transcendental quantity, "exactly" is an illusion. Transcendental functions are themselves computed by approximate numerical procedures of various kinds: Newton's method, power series, arithmetic-geometric means, etc. There is an art to coding these functions properly. A competitive implementation of a function even as simple as sin(x) is already non-trivial. Even so, I'm sure that it's faster in principle to evaluate the integral in question in closed form using elliptic functions. It could be hard work to do this right, because the first step is to factor the quartic polynomial under the square root. That already requires either the quartic formula (unfortunately not listed in the GNU Scientific Library even though it has the cubic) or a general polynomial solver (which is in GSL but has unclear performance and reliability). The solution also requires elliptic functions with complex arguments, even though the answer is real. It could require careful handling of branch cuts of the elliptic functions, which are multivalued. With all of these caveats, it doesn't seem worth it to work out an explicit formula. The main fact is that there is one, if you have elliptic functions available but not otherwise. The merit of a numerical integration algorithm such as Gaussian quadrature (or Clenshaw-Curtis, Gauss-Kronrod, etc.) is that it is vastly simpler to code. It won't be as fast, but it should be quite fast if it is coded properly. The only problem is that the integrand becomes singular if it reaches 0, and nearly singular if it is near 0. This makes convergence much slower, although still not as slow as approximation with chords. With special handling of the near-singular points, it should still be fine for high-performance numerical computation. For instance, a polished strategy for numerical integration might well be faster than a clumsy evaluation of the relevant elliptic functions. 

Now, my question is whether there is some criterion (necessary, sufficient or both) regarding to when an algebra is admissible (or non-admissible) in the sense of the above definition. In other words: 

The case of irreducible, cocommutative Hopf algebras, over a field with $char(k)> 0$, is discussed in Sweedler's textbook on Hopf algebras, Ch.$XIII$, sect. $13.2$. (See prop. $13.2.2$, $13.2.3$). For the case of cocommutative Hopf algebras, over an algebraically closed field $k$, with $char(k)\geq 3$ you can have a look at: Modules of solvable infinitesimal groups and the structure of representation-finite cocommutative Hopf algebras, R.Farnsteiner, D.Voigt, Math. Proc. Cambridge Philos. Soc., v.127, p.441-459, 1999 and the references therein (among them, there is also an interesting older paper by the same authors discussing the case of cocommutative hopf algebras of finite representation type, over an algebraically closed field of $char(k)>0$). 

If the definition of a finite quantum group, you use, is a pair $(A,\Phi)$ of a finite dimensional $C^*$-algebra $A$, with a comultiplication $\Phi$, such that $(A,\Phi)$ is a Hopf $*$-algebra, then this paper may be helpful. . 

This follows easily by a trace argument on both sides of (1) (or of $[A,B]=1$) as long as Planck's constant $\hbar$ is different than zero. So, from the beginning we have to focus in infinite dimensional representations of the Weyl algebra and since the problem is motivated by physical considerations it is natural to consider its generators acting on Hilbert spaces. An old result (see 1, 2) tells us that when dealing with representations of (1) in Hilbert spaces, the generators $q$ and $p$ cannot both be bounded: at least one of them has to be unbounded. Given that a self-adjoint (and thus symmetric), unbounded operator in a Hilbert space cannot be defined on the whole of the space but only on a dense subspace of it (this is a direct consequence of the Hellinger–Toeplitz theorem), one can see that the above remark poses delicate questions on the domains of the above operators (and thus the representations of the Weyl algebra) and at the same time underlines the importance of the use of unbounded operators in the formulation of QM. Initiating from the above remark, Stone and von Neumann studied the integrated forms of $p,q$ i.e. the unitary, one-parameter groups $$ \begin{array}{cccccc} U(s) = exp(-i s p) & & & & & V(t) = exp(-i t q) \end{array} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2) $$ The above expressions are well defined because $p, \ q$ are self-adjoint (see 2,3). $V(t)$ acts on $f(x)$ by multiplication with $exp(-itx)$, while $U(s)$ acts by horizontal translations (shifts) of $f(x)$ i.e. $$ \begin{array}{cccccc} (U(s)f)(x) = f(x-s) & & & & & (V(t)f)(x) = e^{-itx}f(x) \end{array} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3) $$ for any square integrable complex function of a real variable, i.e. for any $f(x) \in L_{2}(- \infty, \infty)$. The operators $U(s)$, $\ \forall s \in \mathbb{R}$ and $V(t)$, $\ \forall t \in \mathbb{R}$, are unitary and thus bounded. Their domain is now the whole of the Hilbert space (and not only a dense subspace as was the case for the $p,q$ generators of the Weyl algebra). Furthermore, $U(s)$, $V(t)$ satisfy $$ U(s)V(t) = e^{ist} V(t)U(s) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4) $$ for all $s \in \mathbb{R}$ and for all $t \in \mathbb{R}$. (4) is frequently called the integrated form of the Weyl relations or the integrated form of the CCR. The Stone-von Neumann uniqueness theorem, determines conditions -the main one is (4)- which guarantee the uniqueness of the representation (3) (and not directly the uniqueness of the usual Fock representation of $p,q$). In other words, it tells us that, given the operators $U(s), \ V(t)$ on $L_{2}(- \infty, \infty)$, satisfying (4) (plus some technicalities on the domains) then their action is unitarily equivalent to the representation (2), (3). There have been lots of studies (and references) on the Stone-von Neumann uniqueness theorem, its origins, its extensions, its descendants and on the exact relation between the Weyl generators and their integrated forms. If you are interested in such aspects, I could provide further references on such topics. Remark 1: Formally, the transition between the CCR (1) and their integrated form (3), (4) can be understood as follows: $\bullet$ If we start from the CCR (1) together with (2) written in power series: $$ \begin{array}{cccc} U(s) = exp(-i s p) = \sum_{n=0}^{\infty}\frac{(-isp)^{n}}{n!} & & & V(t) = exp(-i t q) = \sum_{n=0}^{\infty}\frac{(-i t q)^{n}}{n!} \\ \\ \end{array} \ \ \ \ \ \ \ (5) $$ then one can deduce the integrated form of the CCR (3), (4) by applying the Baker–Campbell–Hausdorff formula. $\bullet$ For the converse, the CCR (1) follow formally on taking $\frac{\partial^2}{\partial t \partial s}$ of (4) at $s=t=0$. However, this description is rather an indication than a strict proof of equivalence between the Weyl algebra and its integrated form: The reason is that $p, \ q$ are unbounded operators and hence the power series expressions cannot be valid on the whole of the space. Consequently, in general, the integrated form of the Weyl relations is not equivalent (at least not in some obvious way) with the Weyl relations themselves (i.e. the CCR). Remark 2: For the case of the infinite degrees of freedom Weyl algebra, there is no analogue -at least to my knowledge- of the Stone-von Neumann theorem (its applicability is limited to the finite generators case). On the contrary, it has been shown (see 4) that, the CCR admit infinitely many, non-equivalent, irreducible representations on a Hilbert space, which are in bijection with the set of real numbers. For interesting examples of irreducible CCR representations, which are inequivalent to the usual Fock space representation, one can look at books on the Foundations of Quantum Mechanics such as: 5, vol. II, p.251-252. References: 

I am not (yet) getting voter support, but I stand my ground! :-) First, clearly if $E \otimes_k F$ is a field, then it is isomorphic to every compositum. Second, if $E \otimes_k F$ is not a field, then there exists a compositum in which $E$ and $F$ are not linearly disjoint. It has a non-trivial quotient field, and that field can serve as a compositum. As Pete Clark points out, there is a difference between the case that $E \otimes_k F$ is an integral domain and the case that it has zero divisors. (And Pete is right that I forgot about this distinction.) In the former case, there exists a compositum in which they are linearly disjoint, namely the fraction field of $E \otimes_k F$. In the latter case, $E$ and $F$ are not linearly disjoint in any compositum. If $E$ and $F$ are both transcendental extensions, then there are two different criteria: Weakly linearly disjoint, when $E \otimes_k F$ is an integral domain, and strongly linearly disjoint, when it is a field. Which you think is the more important condition is up to you. In Andrew's examples, $E$ and $F$ aren't both transcendental, so the distinction is moot. (I needed to think about this issue in this paper.) 

Conway and Sloane, Sphere Packings, Lattices, and Groups, is one of the best survey-style mathematics books ever written. It certainly does extend the discussion to error-correcting codes, even though the main theme is Euclidean sphere packings. The most important relationship between sphere packings and codes, but not the only relationship, is as an extended analogy that leads to uniformly argued bounds and constructions. The Hamming metric and the Euclidean metric are both metrics, and in both cases you are interested in minimum distance sets which you then call codes. But the analogy is more than just that. The Hamming cube is a normed abelian group, and Euclidean space is a normed abelian group. In both cases, there is a special interest in codes that are subgroups. If a code is a subgroup, you only have to check the minimum distance from the 0 vector. Also, recall Pontryagin-Fourier duality: If $A$ is a locally compact abelian group, it has a dual group $\hat{A}$ which is its group of characters. The Hamming cube and Euclidean space are both canonically self-dual in the sense that $A = \widehat{A}$. If $C \subset A$ is a subgroup, it has a dual code $C^\perp$ which is by defining the subgroup $\widehat{A/C} \subset \widehat{A} = A$. (In other words, it is the group of characters which are trivial on $C$.) $C$ has a weight enumerator, which in the Euclidean case is called a theta series, and the weight enumerators of $C$ and $C^\perp$ are related by a transform. The transform is called the MacWilliams identity in the Hamming case and the Jacobi identity in the Euclidean case. The transform is possible because of another fundamental common feature: The Hamming cube and Euclidean space are both 2-point transitive metric spaces. When a metric space is 2-point transitive, there is a construction due to Delsarte for finding upper bounds on the sizes of codes. The construction is a certain relaxation of the distance distribution of a code that reduces the bound to linear programming. The linear constraints come from harmonic analysis. It is easier in the compact case, and it is explained in SPLAG in the Hamming case and the spherical geometry case, where you get bounds on kissing numbers and other spherical sphere packings. The Euclidean case of the linear programming bounds were later developed by Henry Cohn and Noam Elkies. On the construction side, the analogy is sometimes more direct. A sphere packing in $\mathbb{R}^n$ could be both a subset of $\mathbb{Z}^n$ and union of cosets of $(2\mathbb{Z})^n$. When it is of this form, it comes from a binary code in $(\mathbb{Z}/2)^n$. Sometimes this leads to the best known sphere packing. In fact one of these cases uses the Best code with $n=10$ (found by Marc Roelant Best). A simpler case is the $D_n$ lattice, which is the best packing when $n=3$, the best lattice packing when $n=4,5$, and thought to be the best packing in these two cases. It comes from the parity code. This transfer of codes extends to another important case, codes over $\mathbb{Z}/k$ in the Lee metric. The Lee metric on $\mathbb{Z}/k$ is just the graph metric of a $k$-gon, i.e., $d(a,b) = |a-b|$ if you choose residues so that the answer is at most $k/2$. The standard Lee metric on $\mathbb{Z}/k$ is the $\ell^1$ sum of the Lee distances on the factors, but you can view this as an approximation to $\ell^2$ and again lift to the Euclidean case. You can obtain the $E_8$ lattice with way with $k=4$. $k=4$ is also separately because $\mathbb{Z}/4$ is isometric to $(\mathbb{Z}/2)^2$, and this isometry leads to what are called $\mathbb{Z}/4$-linear binary codes. (The Best code mentioned in the previous paragraph is one of these codes.)