Background: Let $\mathbb{F}$ be an algebraically closed field. Let $X \subset \mathbb{F}^n$ be an affine variety. Let $\pi(X)$ be the projection of $X$ to the first $m < n$ coordinates, $$ \pi(X) = \{(x_1,\ldots,x_m): x \in X\}, $$ and for a point $a \in \mathbb{F}^m$ let $\phi(a,X)$ be the fiber of $X$ over $a$, $$ \phi(a,X) = \{x \in X: x_1=a_1,\ldots,x_m=a_m\}. $$ It is known that $\dim(\pi(X)) + \dim(\phi(a,X)) \ge \dim(X)$ for all points $a$, and that equality holds for all $a \in U$ where $U \subset \mathbb{F}^m$ is a Zariski open set (so, dimension equality holds for "typical" fibers). Question: Are "a-typical" fibers, where the dimension equality doesn't hold, have lower degree than $X$? That is, for all fibers we have that $\deg(\phi(a,X)) \le \deg(X)$ since they are the intersection of $X$ with the degree $1$ variety given by $x_1=a_1,\ldots,x_m=a_m$. Can it be that when $\dim(\pi(X))+\dim(\phi(a,X))>\dim(X)$ it implies that $\deg(\phi(a,X))<\deg(X)$? Example: Let $X$ be defined by $x_1 x_3 + x_2 x_4=0$. Then $\dim(X)=3,\deg(X)=2$. The projection of $X$ to the first $2$ coordinates $x_1,x_2$ has dimension $2$. Fibers over $(a_1,a_2)$ if $(a_1,a_2) \ne (0,0)$ have as expected dimension $3-2=1$. However, the fiber over $(0,0)$ has dimension $2$ (which is $>1$) but degree $1$ (which is $<2$). 

I am curious if there is a relatively simple explanation of what is the difference between strong convergence and conditional convergence for Spectral Sequences? (Hopefully a simpler explanation than this info here: $URL$ For instance, I am aware if all the differentials are zero after a certain index ($d^r=0$ for all $r\geq r_0$ for some index $r_0$), then the spectral sequence converges. Is this considered strong convergence? Thanks for any help. 

Is it likely that in the future, there will be interest in computing persistent homology over the integers (or other PIDs)? Currently, persistent homology is usually done over a field (like $\mathbb{Z}/2$), as the algorithms for producing the barcode only work for a field. However, working over a field loses a lot of information. Also, there are algorithms that can compute the persistent groups over a PID ($URL$ Section 5). Thanks for any help. References will be very welcome. 

What is the maximum number of spheres that can be placed in 3D such that all inter-touch? One can of course place four unit spheres tetrahedrally and then add a smaller sphere in the middle, so this number must be at least 5. [By the way, I was trying to extend the "five points in 2D cannot be inter-connected without a crossing" limitation to 3D with a simple statement, but this was sadly the best I could do. If anyone knows a better simple extension, please comment.] 

Dirichlet's theorem shows that, for any fixed prime integer a, "big prime numbers mod a" are uniformly distributed between 1 and a-1. If we similarly pick different prime integers b,c,..., are these uniform distributions independent of each other? 

I understand that the differential $d^k$ of the Bockstein S.S. (mod p) is nonzero iff the homology $H_*(X;\mathbb{Z})$ has summand of the form $\mathbb{Z}/p^k$. How about for multiple summands in the integral homology, say $\mathbb{Z}/2\mathbb{Z}\oplus\mathbb{Z}/2\mathbb{Z}$. How does the Bockstein Spectral Sequence detect them? Merely knowledge that the differential is nonzero doesn't seem to be enough to detect the multiple summand. Thanks for any help. 

May I ask what is the basic motivation behind studying bounded cohomology? Is there a simple reason why bounded cohomology is more interesting / useful than usual cohomology? Also, is bounded cohomology only interesting for infinite spaces (with infinite simplices/singular simplices)? From the definition, it seems that for finite spaces (e.g. finite simplicial complex), the definition reduces to the usual cohomology. Is there any variant that is interesting for finite spaces? Thanks. 

Is there a positive 128-bit integer whose square has all middle bits equal to 1? (The "middle bits" are naturally the 65th bit through the 192nd bit, defining the 1st bit as the least significant bit of the full integer.) 

How can I most quickly find a big prime, p, for which 4p+1 is also prime? For example, p=37 works. I wonder if these special primes have been researched and some characteristics are known. Are there infinitely many of these primes? 

Since Gerry Myerson is too humble to take answer credit, I'll make it even simpler and close out my stupid question: $0^2+(x+1)^2-x^2=2x+1$ ranges all odds, so $1^2+(x+1)^2-x^2$ ranges all evens. Just 3 integers are enough (a is 0, b is 0 or 1, c is x+1). 

I refer to this paper: $URL$ According to the results in the paper, especially the experiments in page 15 it shows that persistent cohomology is faster than persistent homology by a factor of around 30 to 50. That seems quite amazing to me, considering that over fields, homology and cohomology are dual. The paper does explain the reason why, but I don't really get the key idea how it accounts for a 3000% to 5000% improvement over persistent homology. The paper's explanation (also on pg 15) is based on the difference between row operations and column operations. Apparently, row operation is supposed to be the better one, and since persistent cohomology can use the row operation (while it is difficult for homology), it results in better results. Also, the column algorithm (the worse algorithm) has to store all dead cycles, while for the row algorithm we can delete those cycles that died. Also, theoretically, I am curious if such wonderful optimizations can be done for persistent cohomology, why can't the same be done for the dual persistent homology? Is there any theoretical reason for the impediment in persistent homology algorithms? Ideally I would imagine that the "best persistent cohomology algorithm" would perform as well as the "best persistent homology algorithm". Thanks for any enlightenment. 

Is there any integer N such that 2^N=3 mod N? I understand that N must be an odd non-prime. I checked up to a million with no success (but, FYI, 2^N=5 and 2^N=7 have solutions). 

I have many polynomial equations in many variables which I want to jointly minimize (in a mean square sense, but you could pick a different reasonable measure which favors anything where all quantities go to zero). For example, I am looking to make the 6 equations below as "small" as possible (a-j are unknown real numbers). This example probably actually has a solution where all equations are zero, but I also have cases which have no zero solution, so I'd rather not do the "repeatedly eliminate variables and solve for the quadratic root" approach (also, this approach takes too long; is there even any machine which could find a full zero for these equations within 10 minutes?). I'm thinking there might be some software tool that considers the "terrain" smartly and is locally minimizing on many global fronts...or maybe that is impractical. So, is there a free math tool (like Sage) which can minimize things for me (and be certain that no other point is better within some tolerance)? I'm open to theoretical advice, but feel like the options will all look like brute force. Should I give up if I need to minimize a similar set of equations within 10 minutes on one machine?