I have a backup running on both my master and slave servers using pg_dump, piped into "gzip -c" command. The result file sizes are quite different. My slave backup is 1/3 size of the master backup of the same database. Replication seems to be running OK. I ran a query of table list with rowcounts and the two databases seem to be the same. As far as I understand a pg_dump is a text file with copy commands and schema defines. They should be the same size. Am I correct? PostgreSQL 9.5 on Ubuntu 16.04 The backup file is too big to eyeball 

After enabling TDE Encryption my database backup file .bak is 3x larger than before. Is there a way to improve this? Is this due to compression not being as efficient on an encrypted file? 

I have two Postgresql 9.5 servers configured in streaming replication. There are no errors in log files and the servers seem to be in sync. MASTER postgresl.config 

Does anyone have preferences or biases? For myself, I use most (4): I create a new empty database using SSMS, then populate the schema of new database using RedGate. Perhaps, it makes no difference. But I would like to know from the more experienced DBAs what their opinions are. Thank you. 

I currently have Mirroring set up between two production SQL Servers. One of these servers is a hardware SQL server (principal) and the mirror is a VM (on different server). I need to migrate these servers to Availability Groups (HA cluster NOT using shared storage). None of this is my choice. I would personally prefer two VMs, or completely identical systems. My question is: has anyone come across issues for AG/clustering between a hardware SQL Server and a VM SQL Server? Or is there any issues that you can imagine with that setup? I have had no issues with Mirroring between a hardware & VM. As long as the servers are very similar (ie: same version of SQL Server, same RAM given, data files are named & stored in the same locations, etc). My concern is AG utilizes windows failover clustering - an unfamiliar territory for me - so I'd like to know if there's anything additional to look out for. 

I found a solution - the issue is I tried running perfom not locally to the server but from another server. Locally it works. (sigh) 

I am attempting to setup a Linked Server from MS SQL Server 2012 to PostgreSQL 9.3 via Linked Servers & ODBC driver from PostgreSQL. Everything works, until a given query invokes MSDTC, at which point I get an error like this on the SQL Server machine, and the query utterly fails: 

I would like to add non-clustered indexes to my Publication for Transactional Replication. I am aware that replication works by reading the transaction log for the published database and sends the updates to the subscribers. My question is: does transaction log include updates to indexes and therefore would this double the updates sent to the subscriber? Will it send updates to subscriber when I run Index maintenance ie: Rebuilding/Reorganizing indexes? If so, then perhaps creating non-clustered indexes as part of post snapshot script application is a better way. If not, then I prefer to keep it as part of Publication. 

I fixed the problem by changing folder permissions on the Postgresql ODBC driver folder and giving read/execute access to Network Service. Because MS DTC runs under network service, which I confirmed by looking at the service properties. That got rid of the error! 

I have shut down SQL Server services and renamed the files to something different and restarted the SQL Server. I was then able to take the database offline. And then I was able to drop it. I restored the database from Primary and was subsequently able to set up mirroring. I still do not know why Error 3456 happened during OS losing network connectivity. If I find out (which I hope I do) I will update this answer. Thanks everyone who tried to help. 

I am using SQL Server 2012 and I would like to keep my pre-existing data but to cause the ID (identity) column to skip by 2000 rows. Why is this happening? I have a scenario where I may have transactions happening on a standby server which I will need to copy to current server. I want the IDs to match. Therefore I do not want any new transactions on this server that would have the same ID as my standby server. Hopefully this makes sense!! Thank you 

The DLL is in fact that location, so the registry seems to be pointing to the right file. The ODBC driver is 64bit and so is my OS. "File=%2" is pointing to something on the d drive, which doesn't make sense to me, since d drive is a DVD. MSDTC is running... what am I missing? I have toggled Linked Server Properties "Enable Promotion of Distributed Transactions for RPC" to both "True" and "False" and this doesn't change the issue and does not produce a different error. Otherwise, scouring the Internet has brought me nothing. Last thing to point out, my query isn't actually doing any updating - it is just pulling data. So I'm not sure why MSDTC get's invoked in the first place... 

What is a good way to manage developers' data changes? I am using RedGate SQL Source Control to monitor changes to tables and stored procecures, so that when we do a production build, I can include all the changes in a SQL update script. My question is: what is a good way to consolidate and monitor changes made to the data? (As this is something RedGate Source Control cannot track) For example, new records included in lookup tables? And new records included in tables used for routing procedure calls. Thank you! 

It seems AdeptSQL Diff is not compatible with SQL Server 2012 (at this time). And it is my preferred tool for deploying changes. I realize this is a shopping list but I hope SE won't mind. I am wondering what are the best alternatives out there and wanted to query the DBAs. Currently I am evaluating RedGate SQL Compare. It doesn't include data comparison (separate product) though it does generate deployment scripts which is nice. Just want something quick and painless for launching schema changes to production servers. What do you recommend. Thanks! 

I am wondering what are preferred methods to copy a new (Development) database into a new environment. The database will be empty, so essentially I am copying only the schema of this database and not the data. Options include: 1) SSMS (Backup/Restore). I do not use this one because it will copy test data into Production. 2) SSMS (Generate Scripts) 3) Writing T-SQL myself 4) RedGate (or other 3rd party tools out there) 

I am new to postgres, I have a 20 million row table that is on a live server - I need to remove most of the rows but not all. I want to do this without impact to other read/write processes accessing this table (very frequently). I have a way to delete in about 100-400K row chunks at a time. Between each delete, I want to make the query sleep - so that other operations can get a chance to access this table. I have the code, but I believe in this version, it locks the table the entire time the query runs (with all the sleeps). How can I actually release the table while the process sleeps? Thank you!! My code so far: 

Everything else is pretty much default. And yet I have 1600+ files in the archive directory! Because of space constraints I have dared to delete some of them. I would like to figure out what is causing this. I also see that there are only very old files on the slave for the same directory. The only thing I can think of is that I recently did a restore on a database. I dropped the database, created it and did a backup restore. I did this several times. The slave seems to have caught up with the changes. But I have a ton of WAL files in the archive folder for each of the backup restores. Would appreciate some insight into what could be happening. 

For some reason my mirrored servers triggered an automatic failover. I have 2 databases that both think they are the principal server, one is "Principal, Synchronized/In Recovery" and the other is "Principal, Synchronizing" I figure this is bad. My plan is to rebuild mirroring by dropping the database on one server and restoring it again based on the backup of the other. I have removed mirroring on one of the servers. And took a backup. However, I cannot drop the database on the other server. I have tried single user mode, detaching and taking database offline. I have tried to kill process using this database but it doesn't work. The command it is on is "KILLED/ROLLBACK" but nothing changes here. 

I want to monitor CPU usage before implementing TDE on several production SQL Servers. My plan was to use user defined performance counters, over a period of a couple of days, counters as defined here: $URL$ I can view data on these counters when just looking at the Performance Monitor graph but the Data Collector sets just show datetime stamps and no counter data. I'm on a time sensitive project and so I am looking for an alternative to perfmon.... and any advice. I suppose the cause of the empty counters belongs on serverfault - but I would appreciate a solution if anyone ran into this issue and solved it. 

Using SQL 2012 Enterprise edition Following instructions here for restoring a backup: $URL$ I need to be able to rename the database because it is the second copy on the instance (testing purposes). But I cannot rename it nor change the folders that it will restore the MDF/LDF files to. For whatever reason the "Relocate all files to folder" checkbox is not available. Restoring form a SQL 2005 backup. 

I am migrating from Mirroring to AGs on SQL Server 2014 / Windows 2012 R2. I will have a 2 node cluster. I need this cluster to be online at all times for a production environment. I am reading Microsoft documentation for Availability Groups but I cannot find anything along lines of "Best Practices" for having a 2 node AG/WSFC, except that not having a witness is NOT "Best Practice". I am simply exploding with questions. If I implement a File Share Witness What is the best place to put my (file share) witness? Is putting it on a DC a good idea or a security risk? Why does it have to be Windows Server as opposed to (say) a linux/NAS? How do I prevent someone from simply deleting the file share because they don't know what it is or what its for, while not locking myself/all sysadmin out of it? If I don't Implement a File Share Witness Could I just remove dynamic quorum and exclude witness altogether? Is that even possible? Is it better to just add another SQL node to my cluster, if I have one, to act as a 3rd voting party but not include any of its databases? Is that perhaps more durable? 

I am trying to understand best uses of PostgreSQL replication and how it works so I can troubleshoot in a production environment. I am having a hard time understanding the differences between these 2 types of replication in terms of (1) Configuration (2) How the 2 servers Master/Slave perform in each scenario Replication on PostgreSQL (9.2+) is essentially XLOG files of 16MB in size (depending on frequency settings for creating each file) are being created on Master and sent by some method to the Slave. My Setup (for purposes of this question) Configuration of Postgresql.conf on Master archive_command= 'rsync -av %p postgres@[SlaveIP]:[wal_archive_folder]/%f' Configuration of Recovery.conf on Slave to read log files restore_command = 'cp [wal_archive_folder]/%f \"%p\"' primary_conninfo = 'host=[MasterIP] port=5432 user=postgres' My question is what part of this configuration makes this "streaming" replication versus "log shipping"? My master is configured to use rsync to send logs to the slave (is this log shipping?) My slave is configured to be able to connect to the master in recovery.conf (is this streaming?) Second part of the question: What is happening? I understand there is another protocol on PostgreSQL via WAL_sender & WAL_receiver. But I am not clear if this is used for streaming only and if so, how is the rsync being used in the Master? :) Thank you!! And sorry if this is an obvious question. I've been doing a bunch of reading blogs/books but having a hard time understanding. Postgres wiki is so in depth that it takes a long time to get through it all (and I have deadlines) 

I an new to Postgres (9.3 on Ubuntu 14.04) monitoring and I have been using Zabbix/PG_monz. I am seeing errors that my database has "too many temp bytes" and that my database is "too large". My actual backup files are 400 MB daily. I am wondering if anyone is familiar with PG_Monz or just what this error might be about. Thank you in advance! 

I am using Availability Groups (database level) SQL Server 2017: I have set my availability group to "prefer secondary". I have allowed read access on replica. I have created a maintenance plan with full backups on all databases on replica using "copy only" and then also on primary. But now I'm wondering does AG actually make any decisions? Both Mainenance plans will run and from what I can tell won't change "copy only" to primary's maintenance plan if there's a failover. Besides just preventing me from running backups if I change the setting..... besides enforcing rules for what [another] dba may or may not be able to do: what is the point of this? Or, did I setup my backups totally wrong? 

I am changing a database's compatibility mode from 90 (SQL Server 2005) to 110 (SQL 2012) and I am wondering at what point would I see errors from breaking changes if there are any. I ran Upgrade Advisor 2012 which spotted several Stored Procedures that needed updating. An example was: "In SQL Server 2005 or later, column aliases in the ORDER BY clause cannot be prefixed by the table alias." However, when I run this stored procedure in SQL 2012 (without having made changes) it doesn't show any errors. Also, when I changed the database compatibility level from 90 to 110 there were also no errors. Nor when I restored the database. If I am not seeing errors anywhere, I am hoping that Upgrade Advisor caught everything. Is there any other ways I should check for compatibility errors? Also, how is it possible that this stored procedure ran successfully even though Upgrade Advisor told me it would fail? Thanks :)