ETL is not a tool, but the process or group of processes intended for data integration from a source system to the destination system, generally a Data Warehouse. So if you are doing an extraction, transformation and further loading of that transformed data, you are performing de facto ETL. Someone could argue that it is harder to maintain, and more prone to errors. And they'd be right. But a poor code does not convert it into something different than code. Same applies. The tricky part is the final load process in the sense of being moving Excel files in a file system. We use at work a MapR DB based on MapR FS made out of independent files and yes, we do data integration and we finally "load" those files into their final destination, despite they are actually just files. In the end they are accessed by a Query Engine. So in the present day I would consider it an heterogeneous database and your system an ETL. Don't see why not. $URL$ 

What I would do is to create a table with the stock_id (that can be the alphanumeric code or a integer), the timestamp of the measurement and the current value. That is your entry data, 3 columns. From that point you can add columns for calculations (the difference absolute or percent) with the previous value. Having all in the same table will simplify the model and ease your queries. Try to create a date (not timestamp) column and create a partition by it. It may lighten a bit the access to the table as long as you set it in your queries. 

As you cannot use the vendor name as identifier, since it may be duplicated (as it happens with the name of a person), you need a true identifier to be set upon creation. When someone identifies themselves in your system, they cannot identify by their name, since it is not enough to uniquely identify them. Thus you don't have any other option than assigning them any proper id when you create the record on your vendors table and identify them by that id. For this purpose you can create an ad-hoc column (filled in its most simple approach with an auto incremental series), or use a contact email, Tax identifier or any other key that you know is unique, instead of the name. 

Normalization are a set of techniques that, in a theoretical scenario, are intended to pursue consistency and ease of management and save costs. However, since you cannot work with a theoretical scenario, but with a real DBMS, you may find that a database with a certain high degree of normalization may be hard to use, and hard maintain in terms of coding and daily maintenance in a real-world scenario. Said that you may find that, under certain circumstances, you can achieve the same goal of normalization by means of other techniques while not compromising computation cost or maintenance cost, and with an acceptable increase in storage cost. This is why, along with Normalization, there is another technique called Denormalization. Of course you should do specific benchmarking tests and case studies to know which queries are the most common ones, so you can choose which columns to denormalilze. For example, if you have a country with several different country specific columns, and you have the country id (say, an INT) as a FK in the rest of your tables, as probably that id is meaningless (as intended, since it is an ID), you may foresee that every single query will need to join with the country table to retrieve the country name. So you may want to denormalize the common country name, which is used for daily reporting, while you may find that you don't need to denormalize the formal country name, which is used much less common. So analyzing the real use of your database you may try to improve their computational costs. After 17 years designing and managing databases, particularly in the DWH world, my rule of thumb is start at 3FN which to me is a good trade off between usability, consistency and computational, maintenance and storage costs. 

You don't specify if you want to count the number of rows in the step_count or summing it. I've assumed that it contains a previously calculated count and thus I've used the SUM function. It would work the same with a count() instead a sum(). If you need to add the high_fives as a general total (at member level, instead of at activity type level, you need to do an outer query. Try this. This will not work in SQL Fiddle as MySQL does not support full outer joins, but it should work in Teradata. 

The basic question I ask to myself or the user when I gather the requirements in order to create a model and solve your doubt is "How much information do we want to keep about this now and if it is possible to foresee, in the future?" If it is incidental information and you will not need to store much about it apart from the name, maybe you can keep it as an attribute of the corresponding entity. If you need to store specifically related information that, otherwise you would need to denormalize it heavily to be stored, then maybe better create a separated dimensional table with all the related information and then keep just the foreign key. Anyway, as you ask for the case of a SCD, in either case you can historicize your table no matter if you are storing the information as an attribute or as a FK of a proper dimensional table. So for me the necessity of storing such changes wouldn't play any role in the decision making of having it as an attribute or a dimension. 

Patients table - ID as the key, and GroupID to differentiate groups of patients. Also has an IsDeleted field (bit) to indicate soft deletion.t item Phone table - ID as the key, and has three business fields: AreaCode, PhoneNumber, and Extension. Also has an IsDeleted field (bit) to indicate soft deletion. PatientPhone - Has an ID as the key, but also uses the combination (PatientID, PhoneID) to link these tables. 

I would tend to believe the data (phones being shared) is likely the cause of this problem, but I am having trouble proving that. Can someone help me with this? Thanks for your time. 

I am planning a migration to 2016 Reporting Services and have noticed a particular problem. On two separate new installs for SSRS 2016 Developer Edition, I am unable to save a subscription to a report. The report was downloaded as RDL from the current 2008 R2 server, then saved to the new 2016 environment. The new server is build 13.0.4466.4. I also installed the same version of SQL onto a second server and have reproduced this problem. All servers are running in Native Mode. Within Report Manager, I am able to view the contents of the report (on-demand). When I try to create a subscription, the web page is stuck with the "Loading...." prompt after I click 'Create subscription'. It does this regardless of the Render Format I choose. I am attempting to deliver via e-mail. I have domain credentials for the SSRS service account configured, and I have put the IP address for the SMTP server in RS Configuration Manager, so I know it is configured for email ok. In the SSRS log file, I see these types of errors at the time I try to create the subscription: 

I then run this update statement against the Phone table to default every other patient's phone number to 555-5555: 

The problem: I am trying to add some logic to a database refresh job that will de-identify PHI-related fields, and have come across a strange side effect. I am attempting to exclude a group of patients, in order to allow that set to see their true information, and mask all of the other sets. When I run my update statement, it also updates a large portion of that set I was trying to exclude. Why is that, and how can I change my update statement to avoid that behavior? The data model: 

In theory, the data model allows for phones to be related to more than one patient. We did have some issues in the past, and most of the phone numbers have been denormalized. I need help to understand: 

Thanks to the suggestions from others, and I appreciate the time anyone took reading this post. So after looking at this different ways, and taking several breaks, I now see that my validation query was at fault. The join was incorrect. The Update statement was designed correctly, I verified the data was valid in the tables, the relationships correct, and I was able to verify at a detailed level the update worked. Something about my validation query (that did aggregations) seemed off. My validation was incorrectly joining against the wrong foreign key. The join: 

PowerBI Integration is not enabled on either of the 2016 servers, and is obviously not a part of the existing 2008 R2 configuration. I do not wish to use this feature. How do I solve this problem and begin to create subscriptions, so I can ultimately migrate all of the reports? 

The relationship: Patient to PatientPhone - 1 to Many Phone to PatientPhone - 1 to Many In order to validate before and after, I have been using this query to sample the phone numbers by grouping the first couple of digits. It gives me a wide distribution of values, as I would expect: 

My understanding of SQL security is such that in order to have access to the server, you need a login. To access a database on that server, your login needs to be associated with a user in that database. Over time I have been consolidating security so that developers, QA, etc. are in various roles implemented as active directory groups. Those groups have logins on the server, so individual windows logins are not so prevalent on SQL instances. I grant proper accesses to the databases through these roles. A common service I provide is to refresh non-production versions of databases from production. I'll do a backup and restore of a production database onto a development server, de-identify the data, rename it, etc. I have noticed something recently in my environment that is troubling. I was granting access to a key table in production for a development team, when I noticed by using 'fn_my_permissions' that one of the developers has read and write access to the entire production database, rather than being restricted to read-only as I designed. I compared all of this developer's group memberships and found another group he was a member in that was assigned read/write - but only as a database user. There is no associated login for that group on that SQL server. It is typical here to copy a 'baseline' production database and repurpose it elsewhere - whether as another production database, or a non-production version. I am starting to find other examples of this anomaly. To sum it up, the problem I see is that when a database is copied to another server, the users in that database that were previously linked to logins on the original SQL instance seem to retain the same level of access on the new SQL instance - even if there is no related login for them on new SQL instance! This strange effect seems limited to users representing windows group logins. Here is a summary of what I am seeing: Server1 has a login tied to windows group A, which has been granted read/write to database X. Database X is then backed up and restored onto Server2, with logins but none related to group A. The database still shows a user for group A, and when I run 'fn_my_permissions' for a user in that group he has read/write to the database. This particular server is running SQL Enterprise 2012 SP3 with the latest CU. I've seen the same thing on servers running 2008 R2, as well as a current build of SQL 2016. Why is this happening? And what can I do to ensure that users do not inadvertently circumvent the controls I have put in place? It seems obvious I need to remove all of the unnecessary users after the databases are copied, but why?