If you want aliases to work from the client then you have to create them on the client. Install the SQL Server client (from memory the feature is called Client Tools) and use Configuration Manager to create the aliases (32-bit or 64-bit to match the client application). 

First, if it ain't broke, don't fix it. If no-one is complaining about the speed of the system then leave it alone. Spend time and money on other things. Second, no, I don't think it is possible to over-normalise. That is, I think it is possible to spend more time on the normalisation process than your application might need, but unless you've spent tens and tens and tens of hours doing so I don't think the time is really wasted. The time getting to 3rd normal form is most certainly never wasted. In my (admittedly limited) experience, the problem is usually under-normalisation, otherwise known as "laziness". Database designers often claim it is for speed purposes, throwing the term "denormalised" about like a holy mantra, but if you investigate closely, all they've actually done is half a job. Denormalisation is not "stopping before you reach a certain normal form"; it is "reaching a certain form (usually 3NF or BCNF) and then undoing some of that normalisation for well-documented, well-benchmarked reasons." Links Tony Davis, The myth of over-normalization. Louis Davidson, Can you over-normalize?. 

Make sure your SQL Server services and machines are well secured - anything published on the Internet will get attacked. You should have good pass phrases, renamed administrative accounts (if possible), regularly-updated anti-malware software, etc. 

I've just tried creating and dropping a table using Notepad to enter the command. It worked fine for me (Server 2012 R2, SQL Server 2014). In Notepad, I typed I selected all of that text, copied, and pasted into the create table and drop table statement in SSMS. In SSMS I could see that the nonprinting character was there, because using the arrow keys paused at the underscore. 

Remove the square brackets. Apart from that, the PowerShell code looks fine (and works on my test machine - the code I used is below, copied from my SQL Server test machine SERVER1). 

Short answer: Don't use or or their deny equivalents. They are for backwards compatibility only. Using them will cause issues like the one you are facing. If you want to give principal Alice the SELECT, INSERT, UPDATE and DELETE permissions to all table-valued objects in schema Sales then use the following. 

My first thought was to create columns for each part of a date , , , and so on, where if the date of birth is only known to the year then month and day and so on contain a null marker. Yes, I know this could be considered a violation of 1NF. A bit of playing around convinces me that it causes the constraints to make sure no-one put a null marker in DOBMonth and a value in DOBDay get a bit insane. After a bit of thought, I'd go with what you have. I wouldn't care too much about validation of the data stored, I would limit the data returned by means of a case expression, something like the following (typed off the top of my head so untested). 

Fabian Pascal identifies four pragmatic criteria for choosing a key: familiarity, irreducibility, stability and simplicity. He also notes that sometimes you must make trade-offs. During the logical design phase you will identify all the candidate keys for an entity. During the physical design phase you will decide which of them will be the primary key (the one usually used in referential integrity, in other words, foreign keys). If you don't think any of the candidate keys are suitable for the primary key then you choose to use an arbitrary key, and then decide what mechanism will be used to generate the arbitrary key, perhaps the application generates it, perhaps the server does (identity, guid with newid, sequence, something else). If your systems will be multi-master in some way then application-generation might be better, as most of the server-side mechanisms don't handle this. The choice of primary key is determined by size and type - you will be ing on it, so you want it to be small. During the physical phase you will also be doing index design, which includes deciding which index will be the clustered index. You want that to be small as well, since the clustered index key is included in all non-clustered indexes. You want it to be unique, otherwise SQL Server will add a uniquifier, which has a small impact on performance. On the other hand, this is not the 70s. We now have computers that don't fill entire warehouses and take all week to run the payroll. If won't kill your performance to use a guid or a char(8) either as a clustered key or as a primary key. Unless you are UPS or AIM Healthcare (two winners of WinterCorp's 2005 TopTen survey). Edit: I just had this link arrive in my mailbox. Simple talk Primary Key Primer for SQL Server Edit: Joel's comment below is very important. Natural keys often aren't stable. Even government-issued keys (tax numbers, drivers licence numbers) that should be static can change. For example, ask Mrs. Hilda Schrader Whitcher about her two SSNs. You control the arbitrary key so you control its stability. In my (admittedly limited) experience, I can't remember a time we used a natural key. I've done work in healthcare, where there were government-issued id numbers (like NZ's Community Services Card) but not every person had them, so they were not suitable for PKs. 

Another alternative is to write a scalar function that checks if a value already exists in the table and then call that function from a check constraint. This will do horrible things to performance. 

If you are using SQL Server 2005 or later then to create an table in a schema you need both CREATE TABLE at the database level and ALTER at the schema level. Ownership of a schema covers the ALTER permission requirement but not the CREATE TABLE one. CREATE TABLE is also granted by membership in the fixed database role. This role is for backwards compatibility - do not use it for new security implementations. 

As stated by other posters, datetime values are stored as datetime values, not as strings. If you want to get technical, they are stored as two integers - one representing days and the other representing clock ticks (3.33ms for the datetime datatype). I agree with you in not wanting to use CONVERT() on every field. I feel that the correct place for formatting region-dependant items is on the client. The web page or application should be formatting dates and times as appropriate to the user's locale. This can take advantage of any localisation features of the client programming language - something that T-SQL CONVERT() can not do. As an aside, in case you didn't already know, the format for inserting dates can be controlled with SET DATEFORMAT. As another aside, SQL Server 2008 added the date datatype which may make your life a lot easier. 

Two drives, RAID 1 - Operating system, executables, pagefile. Four drives, RAID 5 - All data files (alternatively, RAID 1+0) Two drives, RAID 1 - All log files 

If you have restored all of the log backups and forgotten to use on the last one then run the following. 

In SQL Server Configuration Manager, enter the username as MYDOMAIN\Fred, not fred@mydomain.co.nz. You do not need to give the account any permissions at all - Configuration Manager will take care of the user rights and the group memberships and SQL logins and server roles, as well as other tasks like service master key protection. This, however, only works if you use Configuration Manager. Do not use the Services tool to change SQL Server service accounts. Additionally, verify that you are running Configuration Manager with elevated permissions (Run As Administrator). I strongly, strongly suggest you take the account out of the local Administrators group. If anyone somehow compromises your SQL Server then they have just compromised your server. TechNet: Configure Windows Service Accounts and Permissions. 

A wild guess - are the triggers based on CLR code? If CLR Integration has not been enabled on the machine you are restoring to then the triggers won't work. CLR Integration can be enabled with the following code. 

The integrity rule we are modelling is "the Code must be unique". The real-world situation violates this, so the database shouldn't allow both items 2 and 4 to be in the table at the same time. The safest, and least-flexible, approach would be to disallow null markers in the Code field, so there is no possibility of inconsistent data. The most flexible approach would be to allow multiple null markers and worry about uniqueness when values are entered. The Sybase programmers went with the somewhat-safe, not-very-flexible approach of only allowing one null marker in the table - something commentators have been complaining about ever since. Microsoft have continued this behaviour, I guess for backwards compatibility. 

This would not remove the data and log files relating to the old databases - it just removes all references to them. You would have to manually remove the files to free up the disk space. You must also install all service packs and hotfixes again. 

Microsoft SQL Server Analysis Services uses the term "cube processing". Kimball seems to usually use the term "dimensional model loading". Accordingly, I use the term "ETL" to refer to copying data from the OLTP systems into the staging database (or for copying data from one OLTP database to another) and the term "cube processing" to refer to copying data from the staging database into the OLAP databases. 

The usual warnings about undocumented features apply. You can look at the source code of the procedure in master if you are curious or if you want to be certain it has no nasty side effects. It uses dynamic SQL to build a cursor, which is bad for performance (cursor=slow!), so only use this procedure for a one-off task. Additionally, is not available in Azure Database. 

Configure both of your SQL Server machines with static ip addresses. Configure your SQL Server named instance with a static port, for example tcp45001, using SQL Server Configuration Manager. The default instance is already using a static port of tcp1433. Open the static port numbers using Firewall with Advanced Protection on both your SQL Server machines. Publish the port numbers on your firewall. Incoming traffic on port tcp1433 should be forwarded to port tcp1433 on the default instance machine; incoming traffic on port tcp45001 should be forwarded to port tcp45001 on the named instance machine. 

Alice attempts to select from SchemaA.Table1: No change in ownership so permissions are not checked on the table. Alice is allowed access. Alice attempts to select from SchemaA.View1: No change in ownership so permissions are not checked on the view or on the table. Alice is allowed access. Bob attempts to select from SchemaA.Table1: There is a change in ownership on SchemaA.Table1 (Bob is attempting an operation on Alice's object) so Bob's permissions are checked on SchemaA.Table1. Bob attempts to select from SchemaA.View1: This is a change in ownership on SchemaA.View1 (Bob is attempting an operation on Alice's object) so Bob's permissions are checked on SchemaA.View1. There is no change in ownership from SchemaA.View1 to SchemaA.Table1 so Bob's permissions are not checked on SchemaA.Table1. Bob attempts to select from SchemaB.View1. There is no change in ownership on SchemaB.View1 (Bob is attempting an operation on Bob's object) so permissions are not checked on SchemaB.View1. There is a change in ownership from SchemaB.View1 to SchemaA.View1 so Bob's permissions (not Alice's) are checked on SchemaA.View1. There is no change in ownership from SchemaA.View1 to SchemaA.Table1 so Bob's permissions are not checked on SchemaA.Table1. 

So, in the OP's sitation, the view depends upon the table . The view and the table have different owners so there is a change in ownership (something some SQL Server books call a "break in the ownership chain"). To query the view, JoeUser needs to be granted the appropriate permissions to both the view and the table. 

Use a UNC path (\server\share). Using a a mapped network drive requires that the drive be mapped in the profile of the account running the backup (usually the MSSQLSERVER service account). THe account running the backup will, of course, need write permission on the shared folder on the file server machine. As an aside, you say this is running on Windows XP. I have two comments on this. First, push back on your customer/employer as hard as you dare that if they try and run server services on a workstation then they will get grief. There is a reason Microsoft have server and workstation versions of their OS. Second, it might pay you to check the edition of SQL Server (). If it shows "Developer Edition" then your company might be in violation of the license agreement. 

The keyword in the clause can, as far as I know, only be used to put the results into a table-typed local variable. 

Thus if the precision is "years" then the actual date of birth is "1985". If the precision is "months" then "March, 1985". If "days" then "25 March, 1985". And so on. Something to consider is whether it is an error to have a precision column of "months" with higher precision actually specified? That is, is ("1965.04.25 5:43:28", "months") valid or does it have to be entered as ("1965.04.01 00:00:00", "months")? If the first answer is invalid then the check constraints get a bit lengthy. As an aside, if this particular situation happened in many of the entities in the system and was used in a large number of select statements then I would consider creating a user-defined type (if using SQL server then it would be a CLR user-defined type) to avoid having to enter the same case expression over and over. 

No it is not, because identity does not guarantee a unique value. The identity property can be bypassed with (in SQL Server - you didn't specify what RDBMS you are using). A primary key constraint (and a unique constraint) uses a unique index to enforce uniqueness. 

The Policy Management feature of SQL Server can do some of this. The Table facet has fields @HasIndex and @HasClusteredIndex (as well as other ones that may be useful, like triggers). A policy can be created to check conditions on all tables, in all databases, in a number of servers (using the Central Management Server feature). It can't, however, check the existence of a primary key index or constraint. I would have sworn that there was a field @HasPrimaryKey but it's not there in MSSQL2012. I'm either misremembering or going mad. Note: Policy Management is included with SQL Server 2012 Enterprise, Business Intelligence and Standard editions. It is not available in Express edition. 

If you want read and write to the local server at all three branches ("multi-master") with site independance (that is, losing the link to a site does not stop that site from operating) then you only have one solution: Peer-to-peer Transactional Replication, an Enterprise edition feature. Merge Replication is a multi-master solution but has a single point of contact - the distributor. If the site containing the distribution role server loses its link then the other two sites will stop replicating. They will continue to serve queries but will not replicate any changes in or out. There are a couple of HADR tools included with SQL Server - database mirroring and availability groups - however with both of these there is only one writable copy of the database. If the site containing the current master loses its link then the failover process will cause that site to stop serving requests to the database and another site to become the new master.