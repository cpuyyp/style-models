Is there a way to determine the hostname of the server you're running in w/ in a stored procedure? You can run system hostname from a client but you can't make system calls from SP. I don't see it any of the global variables or statuses. 

Looks like you left part of your command off. To be clear, you'll need to use the mysql client for the import. But, in general that should work. Another option is to just connect to the client (same command as before, but with out the the 

Stop slaving on S1, M2. Flush tables with read lock on M1. Start slave until M1's master log pos on S1,M2 Stop slave on S2, M1. Flush tables with read lock on M2. Start slave until M2's master log pos on (S2,M1) Start slave on M1, M2, leaving s1,s2 stopped 

It depends on how long you want to measure. If you won't be recording any times longer than 2^32, or 4294967296 ms than a regular unsigned int is just fine. That corresponds to about 50 days of clock time, btw. If you were storing unix time stamps or larger intervals then look at using bigint. This gives you 8 bytes of space, or the ability to record times as long at 1.84467440737096e+19 ms (which is roughly 500 Million years) 

You can use merge tables, however they're more antiquated from the 4.x versions. Given your application is manually partitioned as it is either a) you are running a really old version or b) the original developer wasn't aware of table partitions. In short if you're running 5.1+ you can let mysql do this partitioning for you. See $URL$ If you're using 5.5 you should check those specific docs as you will find some differences. There are many advantages to partitioning. However it really depends on the dataset at hand, access patterns and how it is to be indexed. Also, keep in mind my following comments are in the context of mysql 5+ partitioning, NOT older mysql Merge tables; although they are sometimes discussed in terms of partitions. Some examples: 

We use cake for some internal applications. And yes it does at times, pardon my french, suck. This isn't anything to do with mysql per se. Once you start defining more than basic models and go down the "has many"/"has many belongs to many" relationships, the underlying queries it generates can be less than sub par. Do you have table with multi column primary keys? Forget about it. Frankly there are just times you need to break the molding your framework is supposed to shield you from and write your sql more directly :-\ That being said, as with anything else interacting with your DB you'll want to ensure you have proper indexes and appropriate buffer pool/key buffer. 

Can you be more specific on "quite large"? In general you're right, you shouldn't just 'do it live' in production with any change. Do you have any kind of QA or test environment you can do a run on first? This isn't the most sexy ninja one line approach but you could 

Running db.system.replset.find() returns the documents from both with out error. They are different versions, one showing the current membership, another an older setup. I haven't seen this before. Is this expected? I'm not really seeing any problems from the mongo or application side, this just seems strange. The duplicates are present on the primary and secondaries. Should .remove({"version":}) be safe? 

In addition to Rolando's general description I would recommend your script maintains and expected configuration definition to compare against the active process list. Rolando's steps only very connected slaves. A likely scenario a slave would fall so far behind many bin logs is replication broken or was administratively stopped and not in the masters process list at all. Treat an event of a missing slave as an error state in which no binary logs are purged from the master. This does add the additional admin overhead of needing to keep that config list in sync when adding/removing slaves but required for your script to work as desired. Further consider your retention policy. If you want to spin up a new slave you might do that by restoring a recent backup and restarting replication from when the backup was taken. For the scope of this discussion we'll assume you have daily backups and are certain the binlog position stored with the backups have been proven to be working correctly. If you have daily backups you'd probably want to keep at least 2 days of bin logs. Edit: After rereading your post it sounds like your question is more a long the lines of "how to write scripts". Python is certainly a viable option suited for the task. Whether it's perl, python, php or other you'll need to look up how to work with their MySQL API clients. More specific questions of how-to-code in these languages are probably better placed in stackoverflow.com after googling some basic intro's to the language to gain that literacy. 

I've read the docs saying log_slave_updates is required. It makes sense for a general promotion situation. But is that really still required for something you're intending to be a read only slave that will never get promoted? 

The general rule is you do not replicate from a newer version into an older versions. Replicating into a newer version is the exact upgrade path you should do. If you only have one slave then you would just promote that to master. You'll want to be sure to retain the output of show master status before promotion so you can reconnect the old master as a slave. If you have multiple slaves have only the new master replicating from the old master and then have the other slaves replicating from the to be new mater prior to promotion. 

It's not unheard of to offline old historical data. However your description makes it seems like this becoming active is some what common. I can't imagine any on demand process where you have to rebuild a table before being able to service the request. Rebuilding a table and it's indexes is much slower then a raw copy of a file of similar size to the final table. You say it's being "packed" and "unpacked" indicating it's still being stored somewhere. If it's going to be needed to be accessible by an application just keep it in the database and let old pages fall out of the buffer pool. When requested again the information needs to be read from disk versus having a table completely rebuilt. 

If I'm reading your query correctly your asking for anything NOT in domain_setting EXCEPT for those with is_keyword_checked = 0. You should be be able to just get rid of that first not in check. Further subqueries are not always as efficient as you might think. You could rewrite the query to be 

What's the exact version of 5.5 you're running. If you're running 5.5.24 you might be running into this bug $URL$ If you are running 5.5.24 ensure userstat=OFF. Really though this is annoying so you might look at just upgrading beyond that. 

I'm in the process of decommissioning an old mysql server. Of course there's a few things left connecting to it periodically people had since forgotten about. I turned on general query logging to get a better view of what was still going on there to track down the culprits and noticed something odd. There was an entry like: 

sdiff will give you a side by side comparison with differences paired w/ a > or < indicator. I've found this to be useful in the exact scenario you describe. The example supposes you have the new schema initialized on a different host, but this certainly isn't a requirement. It could be a different instance (e.g. port) on the same host or the same instance with different database names. 

You're using mysql 5.1, This means the only engines you'll have available are MyISAM or innodb. in both cases adding a new column, regardless of nullability will require a complete, blocking, table rebuild during alter table. You maybe be able to use the pt-online-schema change tool that does some tricks to rebuild the table in a less blocking manner but may not work out if you have complicated composite primary keys. If you have a more recent version (5.6.22 Percona Build) you could use a tokudb storage engine that allows you to to "instantly" add columns. The way it's indexing works it propagates changes down as they're accessed. 

I have a binary log that mysqlbinlog chokes on with the error in the title. The file itself has much more activity after the cited position. Doing some basic confirmation it's not all garbage by running it through the strings command shows theres legit traffic until the end of the file when it got rotated. I've seen a similar post about using hexdump to get past an error related to event too large, but in my case mysqlbinlog chokes to continue to get further information. I'm not familiar enough with the binary format to look for what might be a position of a next event it would recognize. It gives a starting position it can't get past so I have a script running to basically mysqlbinlog --start-position=X incrementing X by one until it returns with a 0 exit code but that looks like it's going to take a month to completely get through everything at this rate. I tested the POC of this idea on "good parts" by starting it at weird offsets and it returned correctly at the next one it found w/o error. I'm running percona 5.6.20 for this instance. I realize this report might be lacking in information needed to answer the question so I'm happy to edit with comment requests as needed. 

Well much to my surprise the script I mentioned did find a good starting position and I was able to successfully recover a large portion of the binary log after the problem point. Not the most elegant script but in case it's of use to anyone else. Note it is very slow restarting a new process to scan byte by byte but got me results. And yes I recognize the potential of infinite looping by never finding anything, might want to add additional check based on start position vs filesize