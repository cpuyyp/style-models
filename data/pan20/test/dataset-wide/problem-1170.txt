The default values for these two fields indeed increase monotonically, which will lead to a hot shard for writes if you shard an collection using the default ObjectID value for . The chunk number is expected to be a sequence, but you can (and should) provide your own custom IDs when uploading files if you want to improve write distribution for GridFS in a sharded deployment. Official MongoDB drivers should provide an API for setting the when creating a new GridFS file. 

The specific warnings you highlighted are for read preferences of (only read from a secondary) and (read from a secondary if available). Both of these read preferences exclude reading from a primary. With either of these two read preferences and a healthy three member replica set, your reads could be split between the two secondaries. However, if one secondary is unavailable the surviving secondary will need to service 100% of the secondary read load rather than 50%, which could overwhelm your deployment without careful planning. This is exactly the read preference you've asked for, but perhaps not the outcome you expected. A better alternative would be to use a read preference of , which may read from the primary or from secondaries. 

The WiredTiger internal cache is used for uncompressed data which is a different representation from the compressed data format on disk. Memory outside the WiredTiger cache is available for filesystem cache (which caches the on-disk representation, including compression) as well as other temporary RAM requirements such as per-connection overhead (1MB per connection), in-memory sorts, aggregations, and JavaScript contexts. Your working set is the total set of data and indexes that your application frequently accesses. For best performance this should ideally fit into RAM. A larger WiredTiger cache will allow for more uncompressed data in RAM at the expense of memory available for other purposes. If your data compresses significantly, more free memory for the filesystem cache will allow for a larger total working set in RAM (uncompressed + compressed data). 

MongoDB's dynamic schema supports different fields for documents within the same collection, so there is no strict requirement for all documents to have an field. The main consideration here will be how that affects your application logic; it may be simpler to have an explicit field present and check the value. If you have an index on a field, documents that do not have that field present will have a value included in the index by default. This can potentially be useful if you want to save some bytes of storage by assuming that missing values are . If it only makes sense to store a value for your use case (and you do not want to search on values), you can: 

Note that doesn't have any data yet, because it has just been started up with the new WiredTiger dbpath. Use to load the config database backup you created in step 3. At this stage you should have: 

Install MongoDB 3.0 or 3.2 for compatibility with your existing MongoDB 3.0 replica set members. If you add a new member with MongoDB 3.2, I would recommend upgrading your other replica set members to the same release version. For more information see: 

The default configuration for a replica set is designed for automatic failover: if the current primary becomes isolated from a majority of replica set members, a new primary can be elected to continue write availability. If you do not want automatic failover, set the votes for your secondaries to 0: 

Assuming this node is definitely an arbiter, I would expect those files are unused (check the timestamps?) and are either: 

As at MongoDB 3.2, there is no option for the Windows MSI to only install the and command line tools so a default install is the most straightforward option. 

You can perform a rolling upgrade of your replica set from MongoDB 2.6 to 3.0. While a mixed version 2.6/3.0 replica set is supported for the purposes of upgrading, you should not run with this configuration for longer than is required. You should also read the 3.0 upgrade and compatibility change notes carefully, as important considerations will be included there. As with any upgrade, you should test in a development/staging environment before prior to your production release. A few particular points to be aware of: 

The MongoDB Extended JSON format allows representation of additional BSON data types that are not part of standard JSON. There is a subtle distinction between: 

Add a "space" between the and keys and their values YAML requires a space between key/value pairs, so reports: "error at line 4, column 8: illegal map value". Remove the double quotes from your path values YAML interprets backslashes inside quoted strings as introducing an escape character, so reports: "error at line 3, column 16: unknown escape character". As an alternative, you could also leave the path quoted but either escape the backslashes () or use forward slashes. 

MongoDB doesn't have an automatic backup feature built-in -- you have to choose an appropriate backup strategy using command-line or third party tools. The backup procedure will vary depending on your deployment type (standalone, replica set, sharded cluster), disk/dbpath configuration, and backup requirements. Unless you are using a third party tool, it's best to follow a supported procedure from the MongoDB manual. 

This is actually not fine, as far as replication goes. If you restart a that is part of a replica set without the parameter and start writing in standalone mode, the standalone data will diverge from the other members of the replica set. Any standalone writes are not noted in the replication oplog, and there is no way to reconcile local changes with other writes that may have happened in the replica set. 

Ideally this sort of metadata would be maintained by the database server in a standard format that tools could also choose to interpret. There is a relevant open feature request that you can watch/upvote: SERVER-10021: Add timestamps to user documents. If you are using MongoDB Enterprise there are a few extra features that may help you with a workaround in the interim: 

Low cardinality shard keys will definitely cause issues with data distribution. The most granular chunk range possible will represent a single shard key value. If a large percentage of your documents share the same shard key values, those will eventually lead to indivisible which will be ignored by the balancer. 

Validate behaviour is storage-engine specific, but in general walks data structures and does not attempt to repair any errors. As at MongoDB 3.4, the one exception to being read-only is for the WiredTiger storage. Some WiredTiger collection statistics (eg. document counts) may be inaccurate in the event of an unclean shutdown and will be corrected when a collection is validated. The collection stats are metadata rather than structural changes to the data files. If you need to repair/salvage data, the relevant command is . In general you should consider this a last resort as many forms of data corruption are not repairable and the end outcome is salvaging data that can be read successfully. For production deployments the first recourse for fixing a corrupt database should be re-syncing from a healthy member of the same replica set. 

You currently have a Primary/Secondary/Arbiter configuration. Instead of compromising replication by stopping your only secondary in order to copy the files, I recommend adding a new secondary with increased storage and dropping the arbiter (since it won't be needed if you have an odd number of voting nodes). Once your new secondary completes initial sync you can then upgrade and resync the other secondary, and finally step down the primary. At this stage you could either drop the former primary and add an arbiter to return to your Primary/Secondary/Arbiter config, or consider adding another secondary to the replica set so you have a more robust Primary/Secondary/Secondary deployment. For a critical production environment I would encourage you to use three data-bearing replica set members instead of two plus an arbiter. A main consideration when using a three node configuration with an arbiter is that if one of your data-bearing nodes is unavailable, you no longer have replication or data redundancy. 

A COLLSCAN operation is a collection scan, which indicates that no index is being used and the collection is being iterated in natural order. If your query had no criteria but requested a sort by , the index would be the best candidate to return results in the expected order. In MongoDB 3.0 I would expect this to be indicated as an IXSCAN rather than a COLLSCAN. 

From your profile output the query held a read lock for 334 microseconds and yielded once, so it would appear to be well below the threshold. Effectively, this isn't an inherently "slow query" to execute but it is likely being affected by other activity on your system. A properly indexed query returning a single result is unlikely to be slow; the option is much more useful for a poorly indexed query or a long running query that returns many results. For a forced example of a slow executing query to test you could use a JavaScript function, eg: 

Roughly translated: "find all documents where it is not the case that the array includes a value other than one in the provided list". 

As at MongoDB 3.4 there is no equivalent of "initializing" a MongoDB data directory beyond copying data files from another deployment before starting with the appropriate configuration file. There are also no standard set up scripts that would be analogous to , which does preparatory tasks such as creating schema and user accounts. MongoDB production deployments are generally configured as replica sets or sharded clusters, so set up requires context of the overall deployment. Typical solutions for deployment automation include MongoDB Cloud Manager (SaaS) and MongoDB Ops Manager (on-premise). New AWS environments provisioned using MongoDB Cloud Manager will include O/S settings as per the Production Notes in the MongoDB manual. 

This indicates the start of a new BSON object, which would be represented as an opening curly brace ( ) in JSON. You will see this at the top level for each new document and for every embedded document/array. 

By default, queries return all fields in matching documents. If you need all the fields, returning full documents is going to be more efficient than having the server manipulate the result set with projection criteria. However, using projection to limit fields to return from query results can improve performance by: 

Your chosen shard key for a collection defines the granularity of possible chunk splits. Chunks represent a range of values for the shard key. With a single field in your shard key (), that means the smallest possible range is a single value such as . The chunk size setting determines when MongoDB will try to split chunk ranges in order to help distribute data in a sharded cluster, but does not put an upper bound on the size of the data in a chunk range. When the maximum chunk size is reached for a given value, that chunk will be marked as an indivisible chunk and the balancer will no longer attempt to split or migrate that chunk. New data will continue to be added to that chunk leading to an imbalance in data distribution as you observed in your testing. 

As at MongoDB 3.2, you are correct that the collection does not maintain metadata such as created or modified timestamps. You can add metadata manually using , but as you noted this requires coordination with any applications or tools that are allowed to create users in your MongoDB deployment: 

Note: the HTTP and REST interfaces do not include support for authentication and are unsuitable for production environments (they are disabled by default to prevent potential data exposure). For more information, see Network Exposure and Security Checklist details in the MongoDB manual. 

I believe the issue you were experiencing here was SERVER-2592: fields in a document are reordered (sorted alphabetically) when setting a field value. As of the MongoDB 2.6 production release, the existing order of fields is now maintained on updates. 

MongoDB 3.0+ will log a warning if you have an even number of voting replica set members, but does not prevent you from using this configuration. The log warning will be similar to: 

This error indicates the process is unable to open any more file handles because the configured operating system resource limits (aka on UNIX-like operating systems such as Linux) have been reached. The settings are meant to constrain individual users or processes from consuming excessive system resources. Verifying and increasing ulimits is an admin task outside of MongoDB. See: UNIX ulimit Settings in the MongoDB manual for more information. 

I haven't personally tried Hydra (and there is currently a caveat in the Github repo that the authors only tested with MongoDB 2.2.3 on Ubuntu 12.04), however it seems like a promising approach. 

The GeoJSON spec doesn't support multidimensional coordinate positions in the format you are suggesting: 

A standalone server limits your backup options if you also want to keep your deployment available while taking a backup. Here are a few suggested approaches in order of most to least recommended: Approach #1: Use a cloud backup service For the easiest short term solution, I would consider using a commercial cloud backup service like MongoDB Cloud Manager. MongoDB Cloud Manager provides continuous backup with scheduled snapshots and a retention policy (see Backup Preparations for more info). A cloud service also avoids you having to deploy any extra servers/infrastructure, so even if you plan to do so in future this is a helpful short-term solution. The general approach would be: 

It looks like the current MongoDB 3.0 upgrade instructions are missing mention of two important parameters for backing up and restoring users and roles: 

There isn't an absolute number. General factors include resource challenges such as the size of your data relative to RAM, available network bandwidth, and how quickly your data changes. Typically if you have sufficient data or workload to warrant sharding, you've also outgrown as a backup approach. is going to read all data into memory which will have significant impact on working sets for shards if your data is much larger than available RAM. You also need to have enough disk space to save a complete backup (or compressed backup for MongoDB 3.2+) of the data dumped via a single , enough network bandwidth to cope with the increased traffic, etc. For your specific use case definitely isn't a recommendable strategy for several strong reasons: 

A MongoDB arbiter cannot automatically become a secondary or a primary node, as it does not have a copy of the data set. If you try to manually reconfigure the arbiter as a regular node via you should get an exception similar to: 

The keyfile is used to secure intra-cluster communication, so all components of a sharded cluster ( and ) must use the same keyfile. The documentation you referenced is for a single replica set deployment. For a sharded cluster see: Enforce Keyfile Access Control in Sharded Cluster. 

The only other marginal benefit is if you are using a 32-bit build (which is also not recommended for production use). Since 32-bit builds are limited to ~2Gb of addressable data for memory mapped files, the journal is off by default to allow for more data (with journal enabled, the 32-bit data limit is halved to ~1Gb). In all other cases, the journal is enabled by default. For 64-bit systems, the addressable virtual memory is not a limiting factor for data size. Any performance gain from disabling the journal is a tradeoff for data & operational risk. I would strongly advise against disabling the journal unless your data is ephemeral (i.e. you don't care about losing it or can easily recreate it from another source of truth). If you are concerned about possible contention or overhead from disk I/O of journaling, you could always move the journal to a separate volume. NOTE: moving the journal to a separate volume may also change your backup strategy, particularly if you were relying on filesystem snapshots to take a comparatively quick backup without having to and quiesce your database. 

As further preparation for successful initial sync with WiredTiger you should review the MongoDB production notes for your operating system. Since you mention using Linux, I would recommend checking: 

Arrays which potentially grow without bound are an antipattern for performance and usability. I would recommend having each quiz in a separate document for reasons including: