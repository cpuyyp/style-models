Computer says no. Conclusion Can MariaDB or MySQL cast as bigint? As we have seen, yes, MariaDB 10.3 can (though it's not GA yet ... but surely any day now it will be). However, it can only cast as bigint implicitly. You can't explicitly cast as bigint, just like you can't cast as varchar (but see MDEV-11283), text, float, tinyint etcetera. There are only a limited number of datatypes you can cast to. This is not well documented, but CAST appears to support only the same datatypes as listed for the CONVERT function. 

Your application will then have update the every time an article is viewed. If the record for that particular week doesn't exist, the application will have to create it. If you want to delete old data, which may not be necessary: Instead of a cron job, you could use a MariaDB event. Set it up to only delete records older than some limit, e.g. 365 days. 

You can use Percona Xtrabackup / innobackupex to do partial backups of MariaDB databases (as long as you haven't used compression or at-rest encryption). I've personally used this with MariaDB version 10.1. MariaDB Backup () is a fork of Percona Xtrabackup (with added support for compression and at-rest encryption), so (logically) this should therefore also support partial backups. MariaDB 10.1.23 is the version where MariaDB Backup was first introduced. Since this is an alpha release of the tool, it's not recommended for use in a production environment. See this MariaDB blog post for details. It would seem that partial backups are supported at least in later versions, as options to specify particular databases and tables to back up as well as tables and databases to exclude from backup. 

I think you've already established that the master server is running. You can check whether you're able to connect from the slave to the master with: 

I think we can safely assume the performance would be worse than using regular columns. The way to index JSON attributes is to create a virtual generated column for the particular attribute you want (company_id), then index the virtual column, and use that column to filter your query. 

XtraDB (which is InnoDB with a few patches from Percona) is a tried and tested technology, and XtraDB / InnoDB is the default storage engine in MySQL, Percona and MariaDB. InnoDB is being developed and supported by MySQL (owned by Oracle). TokuDB is newer, it doesn't have as many years of usage and development. If I understand correctly, TokuDB is developed by Tokutek, now owned by Percona. With these considerations, in a choice between these two technologies for a system for financial transaction data, I would probably go with XtraDB. 

Check your config files and and look for the socket parameter. Make sure it's the same for client and server. A good place for the socket parameter is in the [client-server] section in . The socket file is created by the MariaDB server when it starts up. Note that there are other places where MariaDB config files could reside, and these could potentially override your settings in the two files mentioned above. For more details, see this page in the MariaDB KB. Also note that you may be able to connect to MariaDB through TCP rather than through the socket. You can specify that you want to use TCP rather than the socket with . If this is successful, you can then do to find out where the socket file really is (or should be). You can then connect with or edit the config files so that the client reads it correctly from there. In some cases you may also be able to figure out what socket file location is used by listing the processes: which could output something like 

Thanks to the auto-incrementing component of the primary key, you don't have to worry about populating that. 

(Use if you want to change it for the global scope rather than just the session.) If you want to set in the MySQL config file, then I think you'll just have to "hard-code" it, i.e. just find out what the default modes are for the version you're using, remove the undesired modes, and add the resulting string e.g. to the section of the .cnf file: 

(Edited to address comments by @RickJames, plus a bit more on NDB Cluster and general improvements.) There are several issues with your plan: 

Then make an alias (assuming Linux-like OS - depending on shell, you can perhaps put the alias in ): 

So you will insert a record for each product in a certain combination. You can then select from this table, and join with your product table, to get a listing with all the combinations where a certain product is involved: 

(Note that it's bad practice to use "SELECT *" except for ad-hoc queries. So you should instead enumerate the exact columns you want to select.) 

Xtrabackup is a physical backup tool, and is meant to backup and restore to the same MySQL (or MariaDB) version. It's not designed to work with migrations. It might work, it might not. So if possible, if you want to restore the backup to a new instance, I think the best approach is to restore it to another MySQL 5.6 instance, and then upgrade that instance to 5.7 afterwards. 

The InnoDB buffer pool is essentially a huge cache. (A variant of LRU - 'Least Recently Used'). If your working set data fits into that cache, then queries will usually be relatively fast. On the other hand, if the working set data doesn't fit into that cache, then MySQL will have to retrieve some of the data from disk (or whichever storage medium is used), and this is significantly slower. Running can bring huge amounts of otherwise unused data into the buffer pool, and at the same time the (potentially useful) data that is already there will be evicted and flushed to disk. There are ways to avoid or minimise this problem: 

It's because you're calling the stored procedure with empty string values rather than NULL values. The IFs and ELSEIFs in the SP body are testing for NULL, not empty string. 

Galera can't do that, unfortunately, although you can configure it to handle shorter network outages. (See Galera's config tips for WAN replication.) Nodes have to be connected for Galera's global ordering to work so that it can merge writes from multiple nodes. See e.g. here Maybe what your application could do instead if a node loses network and becomes unavailable (which could be detected by the application or a DB proxy) is to automatically fail over to a different cluster node. However, that still requires network connectivity to this other node. So if your architecture needs to be tolerant against network losses between nodes, then you may have to look for a different solution. 

... where refers to a file . If the user then tries to set their password (I've tested this only on MariaDB with the unix_socket plugin): 

Disclaimer: The below solution is probably unnecessarily complicated - the obvious solution to this would perhaps involve a cron job or maybe a database event running at some interval to execute and extracting the output into your table. Nevertheless, here is what I think is an interesting solution which gives you instant access to the output through a query. However, I've only been tested this with MariaDB (version 10.1.32), and it may or may not be portable to MySQL. It requires the CONNECT storage engine, and I'm not sure you can get that for MySQL. First install the connect storage engine package in your OS. Then in the mysql client: 

This should be correct for MySQL 5.7. When upgrading to the next version, you'll have to update the setting again. Note that stored procedures run with their own s: 

The SQL:2011 standard has a feature known as temporal tables or system-versioned tables which might be helpful for this type of use-case. Basically, whenever you update or delete a record in the database, the database automatically holds on to the previous version of the record, including the time period when it was valid. PostgreSQL doesn't support this feature natively, but there's an extension called temporal_tables. A tutorial can be found here. A few other DBMSes support temporal tables natively, including SQL Server 2016 and MariaDB 10.3. 

Use the UUID() or UUID_SHORT() functions to generate globally unique primary keys instead of auto incrementing primary keys. Your application then needs to either explicitly call one of these functions when creating the records, or you could use a trigger. (I personally think it's bad design to use triggers this way, but it does work.) Assuming you know a fixed upper limit to how many users there will be (n): You can use auto incrementing primary keys and set auto_increment_increment to n, and then set auto_increment_offset to 0 for the first user, 1 for the second user, 2 for the third user, and so on. This way you will avoid collisions in the same way that auto_increment is used in Galera clusters and other master-master replication configurations. 

You have another .cnf file where the variable is being set to 4895. The MariaDB Systemd unit file (or SysV init script on older Linux versions) is overwriting the variable value you've set. You're running a non-standard MariaDB server that's been compiled with a hard-coded value for . (Unlikely!) 

One way to achieve this could be by using pluggable authentication. This way the password is stored externally and therefore can't be changed from within mysql. This allows you to use PAM, LDAP or other authentication services. For PAM, assuming you have configured PAM already in your OS (assuming Linux below), then in MySQL do: 

I don't know if this is likely, but if we imagine an additional record, essentially a duplicate of the data in record with id 6: 

At the moment you're searching for e.g. "Barney Likert" (the whole string) in both the and the columns you've specified. You need to either have two search strings, one for searching , and one for searching . Or if your users can't be bothered to use two separate fields, then you could parse the string before it's sent to the database, split it up if it consists of two words, and execute a particular query depending on what the search string is. E.g. you could have one query for input of one word, and another query for input of two words. You may also want to consider changing the to , otherwise you will only get results from studies that have at least one element. If you know that all studies have at least one element, then you can keep it as is, as this type of join is faster. 

And the column is indeed created as bigint(10). MySQL 8.0, both the default strict and non-strict sql_mode 

The relationship between and is one-to-one, so if you wanted to simplify your table structure, then these two tables could be merged into one. Are you 100% certain that every question will have only 5 (or less) options? For flexibility reasons, it might make sense to have only one answer option per record. In that case, the table must remain separate from the table, and it becomes a one-to-many relationship. Which is the right answer option? You should probably indicate that somehow. In the table, the is obviously the primary key. The is a foreign key. And you seem to have forgotten a foreign key to , as well as a foreign key to a table to indicate who gave this answer. A minor issue: In the table, the column is a . This is up to 16 million bytes. Do you really need that many? A column can store up to 64K bytes, maybe that would be enough? Also, you would want to use for your columns. The reason for this is that auto incrementing ints start at 1, so by allowing signed s you're effectively wasting a bit and limiting the range of values you can use. There may be other opportunities for improvements, these are just a few I noticed. 

Although not quite yet stable (GA), MySQL 8.0+ (and MariaDB 10.2 - stable (GA)) supports CTEs and window functions, so we can do: 

Yes, I think this could be the way to go as I don't think there are any stats in MySQL for historic status variables. 

Even though only 0.1% of the articles have a duplicate, the cost (in terms of wasted space) of a NULL int value (for a column) in the table is small. (In fact, if using the InnoDB storage engine, you will waste no space at all if the value is NULL. (See e.g. Bill Karwin's SO answer here as well as the MySQL documentation about InnoDB field contents.) However, if the column needs to be indexed, then there is a performance penalty. A separate table () is the more normalized approach. This will add another join to your queries, but again the cost of that (in terms of extra memory and processing time) is small because the table is small, especially if best-practice indexing is used. You should therefore have a compound index (article_id, redirect_to) on the article_redirect table. For the separate table approach (), it's good practice to define an explicit primary key (in every table). Assuming an article can only be a duplicate of one other article (but many articles can duplicate the same article), then the primary key should be . As an aside, I will mention that it's usually not good practice to have columns ( is a type of ) together with other "meta data" columns such as title, timestamps and so forth that you might retrieve in queries. Instead, consider moving this to a separate table with a foreign key pointing back to your article table. See the MySQL documentation about optimizing for BLOB types for details.