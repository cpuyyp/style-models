You should start with partitioning the new server (including mkfs on the volumes for the main system) and installing a service Linux. Then you should boot the old system from CD/DVD and mount the partitions you want to copy. You need a working SSH connection from the CD boot on the old system to the new one. On the new system you have to mount the target volumes. Then you can copy the data, e.g. using tar: cd /path/to/source_volume_1 tar -czf - --one-file-system . | ssh -e none root@newserver 'tar -xf - -C /path/to/target_volume_1' 

There is one more extreme level of process control than (re)nice: chrt. You may set the process to SCHED_IDLE. In combination with ionice IDLE that should do the job. There is a really device mapper target which unfortunately didn't make it into the mainline kernel yet which gives you even more control: ioband Another option would be: Put this process into a VM. Direct VFS access is possible in KVM and you can precisely define how much I/O the VM gets. 

Edit 1 Calling this trivial answer a "great solution" makes me feel obliged to take it to a higher level. The basic problem cannot be solved "better" but you can try to get used to always making an argument. And some shell code (replacing rm by a shell function) can help you with this: 

This may sound stupid but the truth (your truth) is benchmark results. There may be file systems which are faster than others in every case but the optimal structure most probably depends on the speed characteristics of your disks and the amount of RAM and the cache effectiveness. What happens if you use smaller directories with a deeper hierarchy? Less data has to be read to find a directory entry but maybe (if that directory's entry in its parent is not cached any more). Let's assume a directory entry is 50 bytes. That's 15K for the whole directory with 300 files. When doing consecutive reads your disk probably delivers 150+ MiB/s. Thus the difference between reading 300 files or 600 files is 0.1 milliseconds. The positioning time is 4ms at best (if that's not an SSD). I.e. for each saved directory lookup you can read the entries of at least 12.000 files. That makes me assume that your directories are rather small. But maybe all your directory entries are in the cache (I don't know how to monitor that, would be interesting though) so this calculation is irrelevant. Perhaps it helps to keep a script in the background which accesses all directories once every few seconds so that none of them gets thrown out of the cache. I assume that the problem is not the lookup time for the file inode. Probably a lot of processes try to do I/O simultaneously. If this leads to the files being read in several steps then performance is dead, of course. The same is true for file fragmentation. Have a look at and your cache files. And have a look at . You should adjust that to your average file size (or the size which is more than 90% of your files) and check whether this has any influence. I also found the hint (several years old, though) to set this value to zero for all devices except for the topmost: 

This would fail if there are files with ".matthew05012013" occurring twice in the name, once at the end. This is probably not the case and could be prevented by using another tool like mmv or a simple shell script. 

Would this cause a delay as the app waits for a IPv6 timeout before it tries IPv4? Some domains seem to be mixed that it looks like chaos. Separating google.com and youtube.com seems like something you don't want to do if you can avoid it. 

I don't know how much you are willing to do but I can imagine that a FUSE module would help in your case (depending on the file size and read-ahead effectiveness): This module would have to make sure that files are read in one step and that (within the limits of userspace) these accesses are not interrupted. The next step would be sorting file accesses by position on disk, i.e. do on the file level what the kernel (and the disk itself) does with single I/O operations. Instead of having a big file system with directories you could create smaller LVs. Thus you could sort file accesses by name and would get accesses sorted by disk area. If you are willing to change your hardware then this may be interesting: Putting just the metadata on a SSD. And you should try to get write accesses off your cache disks. This may be mainly log files. They are usually not really important so it may help to put them on a file system with long commit time and . If (some of) your cache data is static (and you don't need ACL) then you may test the performance if you move it from ext4 to squashfs (compressed read-only FS). Even transparent compression (FUSE) within ext4 may help if the problem is reading a file in several steps. The file system (and disk-internal) read-ahead would get more of the file (if it is compressable). 

have a look at the disk and see the LVs there. But as the kernel cannot provide access to them in the configured way they are not available for access. 

Usually this is simply done by access rights in the filesystem... (except for the visibility) OK, more detail: 

A memory jump sounds like an bug. If that is the case the problem may be solved by caring about this inode manually (with ; I am not familiar enough though to explain how to do that). If it's not a bug and just needs more memory then there is a "solution" which does not require adding RAM but it would certainly "redefine performance"... The problem is that e2fsck does not use swap memory as equivalent. At least that was the situation in a similar case. Does your swap space get filled completely before crashes? Probably not. You can trick into accepting swap as real RAM: You can boot a second Linux in a VM and export the block device to be checked to the VM. You configure the VM with more memory than physically available. in the VM will see more RAM. Of course, this does not make the use of scratch_files unnecessary. I had problems starting a VM with more memory allocation than there was physical(!) RAM available but according to the Fedora docs this is supposed to be possible (maybe that was not even a KVM/QEMU problem but some fancy kernel stuff). 

Have you at least read the Wikipedia article about MitM? MitM means that you encrypt to the wrong key (and/or accept signatures from the from key). If you use public keys for a VPN then this is theoretically possible (but I assume that every serious VPN software takes care about that). You cannot compromise key validation by ARP spoofing. 

The problem is that you have read access to the directory but no execute and no write access. Execute access will replace the "?" with useful data, write access will allow you to delete the file. Write access is not enough to delete a directory entry. Execute permission is required, too. 

Have you experienced that permissions are set to 644/755 or are you just afraid of that? If the former: What have you done? BTW: If such a problem has occurred: It is possible to just replace access rights: 

either adapt the access rights of the files (think of default ACLs for their parent directory if new files will be created) run it in a chroot / lxc environment 

puts the rule at the end of a chain. Thus your one never matches (or at least with no effect) because the second () already got those packets / connections. Instead of you need . Or you create chains to make the structure easier to understand: 

It does not matter which system gets this IP. You can give it to your gateway and use NAT internally, and you can give it to the server. In the second case the gateway needs to know where to route this IP to. 

Problem and aim We don't get IPv6 from our ISP thus I have an IPv6 tunnel which works fine but is, of course, not very fast. And not really reliable. I like to have IPv6 available "just in case" but I want certain hosts (domains) to be connected with IPv4 only. Default protocol It seems to me that all applications try IPv6 first; this is probably a glibc setting. I would be fine if this default would be reversed (for all applications). Netfilter It would be possible to block IPv6 addresses / networks with Netfilter but there are two problems: 

Edit 1: You can check your (file system) access rights by changing to the Apache user and trying to read the files then: 

vnet* are the virtual interfaces of the VMs. You can configure their names (numbers) otherwise QEMU just counts them up. These virtual interfaces are bridged to either physical or virtual interfaces on the host. You can have a look at what the VM sees on that interface by 

Not throwing all capacity in /dev/root was a goot idea IMHO. You can either extend (, ) the existing LV (and after that the file system in it) or create new ones (preferably). Use or (better) your distro's tool for that. And have a look at your disk partitioning: 

if you ensure via SGID bit that a newly created subdirectory belongs to the same group. I do not consider your decision not to create a new group a good idea. 

On the gateway you must allow forwarding within the LAN net, of course. But then you can use SNAT even locally. That's probably more fun than testing with the connection to the customer. :-) 

This is executed so fast that the connection didn't break (not even a LAN connection). Even safer would be to do this within screen (which I did first but which turned out not to be necessary). The safest solution would be (within screen, of course) to drop everything in the firewall during these commands. So insert after the first line (before ip addr del) and add at the end (after ip link set) 

No. Only hosts on the same (physical or virtual) link can do that. The reason is that you must create a layer 2 (e.g. Ethernet) packet which is aimed at your system but contains an IP target address which is not that of your system. Hosts which need at least one gateway to reach your system need the help of at least one (maybe all, depending on the circumstances) of these gateways to get this done. 

And, of course, you must configure the OpenVPN server for routing to the WLAN network (if you don't do NAT in the local router). Edit 1: If all Internet traffic is supposed to go through the OpenVPN server then the configuration becomes very easy. As there already is a host route for the OpenVPN server you only have to change the default route from to the OpenVPN server's tunnel IP. 

See iptables -j SNAT --to-source [ipaddr[-ipaddr]] --persistent I don't know though "how static" this is. Maybe the mapping is lost on reboot. If that is a problem then you might configure a static mapping by an explicit rule for each address. IIRC there is a tool (unfortunately I cannot remember the name) which can map source or destination addresses to chains so that you don't need a chain with 254 entries: 

It seems to me that something went wrong there. Listing should show a symlink and a real directory . should be shown in a listing of only. So I recommend you remove everything from and show us the commands you used (if it does not work again). 

I think that e2fsck cannot correct certain errors on the volume it is run from. This is why I always have a small additional Linux installation. How do you make changes ("set FSCKFIX=yes in /etc/default/rcS") on a corropted filesystem? 

You can do this if you have access to the webservers (or their gateway), too. The only solution that comes to my mind is a tunnel between router and webserver. Then the webserver (gateway) can send all reply packets to the router without having to adjust the destination address. If that is an option for you then I am going to have a look at the details. 

Automatic sync is limited to spare devices. It is of limited practical use to expect a disappeared drive to come back without manual intervention. If the device (or its connection) has become unreliable then a time consuming sync may not even be what you want. If this is important to you then you may regularly check via cron for such strange occurrences or trigger the appropriate mdadm action via udev. 

As voretaq7 correctly points out: "something going drastically wrong with the server" includes the loss of the controller. So if you don't go the "official" desaster recovery way he suggests then it would make sense IMHO to copy the contents of the hardware RAID to a software RAID (so that you can easily add a second disk for mirroring while getting a replacement for the hardware). This means that the target disk must be a few sectors bigger than the source disk. And maybe the boot loader configuration cannot be simply copied; depends on your partitioning. But you can reinstall Grub on the backup disk before you need the backup disk. In that case you should ensure that both your controllers module and mdraid are part of your initrd. Just try to boot the backup disk after you're done.