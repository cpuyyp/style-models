This seems like it should work because the group deploy can write and mima is in that group, but I do get a permissions error. 

I'm running an EC2 instance. According to EC2 specs, it should come with . When I SSH into my new machine, I don't see any of these drives. All I see are my EBS volumes: 

I have no idea where to go from here. If anyone knows the root cause of this or has good instructions on upgrading from postgres 9.1 to 9.3, please let me know! 

The plan for the two WAN interfaces is as follows. All outbound traffic will go to the primary, with exceptions based on destination IP/subnet or possibly on src+dest IPs/subnets. Such exceptions should be routed to the secondary. It would be very nice if, should the primary go down, the secondary would automatically take over for all outbound traffic. I am reasonably sure that I can put something together based on dd-wrt. However, I'd like to hear from you what alternatives are out there (especially something easier to set up for my use case, even if it means paying more for the hardware.) 

I thought restarting the server would work but no luck. Even trying to SSH in with root gives me the same error. It seems a bit weird that I would get completely locked out of a server due to this and it would never reset. Is there something I'm missing about how to reset this? It's actually hard to google for information about this because everyone experiencing this problem seems to have a bunch of keys in that a dumb client in cycling through but I am setting my key very specifically: 

The measured throughput is 1.3Gb/s. This is 7.5x below the theoretical maximum, and only 30% faster than 1GbE. What steps can I take to troubleshoot this? 

I am seeing the following messages in my server's . They look like stack traces and are not preceded by any narrative (e.g. "such-and-such has gone wrong"). I am almost certain they are related to I/O problems I am experiencing, but it would be instructive to understand what exactly these messages are and what triggers them. 

This is a Mongo server and I'd like to use instance storage as temporary space while backing up Mongo. How do I locate the 420GB instance stores and mount them? I've read the EC2 docs and I am still unclear. 

I have a file on server that can be written by a group. The group is called "deploy" and the user is "mima." I have verified "mima" is in "deploy" with: 

I have two Linux machines, each equipped with a Solarflare SFN5122F 10GbE NIC. The two NICs are connected together with an SFP+ Direct Attach cable. I am using netperf to measure TCP throughput between the two machines. On one box, I run: 

As you can see, is 100%. At the same time, the aggregate I/O throughput () is about 18MB/s, which is 10-20 times slower than what the disk is capable of. This, and the ratio of to , lead me to conclude that instead of reading large chunks of each file at a time, NFS ends up reading the files in smallish chunks with lots of interleaving of chunks between different files. This in turns leads to lots of disk seeks, killing performance. Would you say the conclusion is justified by the evidence? What would you recommend as a way to address this? I can change the reading app, and can tweak NFS settings on both the server and the client. I am using RedHat 5.6 with kernel 2.6.18, which I believe limits to 32KB (I'd be happy to be proved wrong on this). edit: This is how things look when there's only a single client reading a single file: 

I recently created a ZFS volume to test its compression capabilities. I'm comparing it side by side to an ext4 volume. After creating the new volume and turning compression on with I copied a ~3GB file from the ext4 volume to the ZFS file but the file is the same exact size on the ZFS drive (I used to see this). I gzipped the file manually to see what the compression should be (I understand there are different levels but just to get a ballpark) and just using the file size was cut in half. My ZFS settings also show compression is turned on: 

What exactly is the meaning of and ? for writes is 72 seconds(!) -- would you say this is abnormal and, if so, how do I go about troubleshooting this further? 

I have an NFS3 server with multiple clients. Each client is sequentially reading a different large file, and performance is very poor. Here is what I am observing in on the server for the disk where the files reside: 

My question is more of a ..."where should I look" question. If supervisor isn't logging anything the only other place I can think to look of is the syslog which doesn't indicate anything crazy to me. 

I'm on my server and I can't ping anything outside. I tried for example google.com. How can I diagnose this issue? I can ping my localhost (ping works) This is my traceroute to google.com: 

I am trying to configure Postfix so that it would accept mail from authenticated clients outside . When I try to send a test email from my iPhone, which is configured to use port 25, SSL and password authentication, the mail gets rejected by Postfix. I get the following in : 

How do I establish that the session is indeed authenticated? How do I go about troubleshooting this further? 

There is nothing in the logs (My global config has ) and there is nothing in the "program logs" either (). I have a similar provisioning process for other servers that run ruby daemons so to dig in a bit further, I even creating a very simple ruby script and made the thing so there are no permission issues: 

I'm following these instructions to upgrade postgres from 9.1 to 9.3: $URL$ When I restore (Step 8), I get a ton of messages like this: