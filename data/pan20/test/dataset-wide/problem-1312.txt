And then only if a branch has a upper bound that is better than the best solution's partial value do you continue evaluating the branch: 

In C++ we usually use for generic template type parameters. Nest implementation classes The class is an implementation detail of and should thus be a nested class. Normally we prefer terse but descriptive names, would be preferred to but even better, if is nested you can simply use as it will always be clear from context that it is a binary tree node. General comments on : 

has even worse entropy as it is only using the lower bits due to the modulo operator. Using modulo skews the distribution As if low entropy wasn't enough, using modulo in this way also skews the distribution of numbers. To see this, imagine and you do then the possible outcomes are: 

We have to be careful about the inequalities and off by ones here as any error adds a bias. Lets check: 

This also goes for your visited list. For the love of ${DEITY}, don't encode coordinates into strings Instead of simply use . Much faster, especially since you don't have to parse out the x and y values when you use it. Use the correct data structure and algorithm for unvisited nodes You are storing your unvisited nodes in a list and doing a linear search through it for the next unvisited node. This is what is ruining your algorithm performance of Dijkstra, going from to and why it takes 30 seconds for your machine. First off you only need to store the currently accessible, unvisited nodes. This alone would significantly speed up your search for the unvisited node with smallest cost. You can do this by adding the neighbours of the current node to the unvisited nodes set only if they don't already exist in the set. (You can keep an additional matrix with nodes that have been added or you can query the set if it contains the node provided you have lookup). Next as you will be taking the smallest element as the next node, it makes sense to use an ordered data structure such as a heap or also known as a PriorityQueue in Java. Note that if you do use the priority queue you need to remove and re-add elements as you change their cost so that they get re-sorted in the queue. Use an Object Oriented Solution You have many matrixes (arrays) of the same size that represent different properties of each node. It makes sense to just consolidate these arrays into one array of nodes where each node has properties. Like this: 

Unnecessary work Consider a list of \$n\$ elements, then will move approximately \$2n\$ elements in the array when all it really had to do was swap the two elements. Note that an optimised only needs to move elements. As this looks like self-learning or possibly homework, I'll leave the details to you. Use binary search You can binary search for the insertion point to improve the performance even further. 

Also add some checking to see if you have enough "free" numbers of the type in the hashmap. The time complexity is amortized \$\mathcal{O}(n^2)\$ as lookup in is amortized \$\mathcal{O}(1)\$. 

You are hitting a special case for your array as the hash function of by default is just the integer value itself. As soon as you start having a non-trivial hash function your containsAll implementation begins to be slower. See for example the following code which uses strings of 100 characters. 

Note that I also showed you how to get rid of the branches in this function. You'll have to benchmark and see if there is any use from making a template value. I'm suspecting it wont make a difference. Consolidate branches Note how the branches for and have almost the same code? 

Work in logarithms You can simplify your determination of the correct suffix by using the logarithm of the file size. First of note that \$1024 = 2^{10}\$, \$1024*1024 = 2^{20}\$ and so forth. You're checking if \$x < 2^{10}\$ and then if \$x < 2^{20}\$ etc. It's much easier to take the two-logarithm of \$x\$ and then check \$\log_2(x) < 10\$, \$\log_2(x) < 20\$ etc... Now if you take \$k=floor(\log_2(x) / 10)\$ you will end up with \$k=0\$ for bytes, \$k=1\$ for KiB, \$k=2\$ for MiB etc. Which means that you can simply index into an array of the suffixes to get the right suffix, then upscale the result to get the correct number of digits. Like this: 

Not everything needs to be object oriented. But don't use a if you are not writing object oriented code. 

While this looks innocent, even for small you will iterate a really long time. For you will need one call to . For you will need one call to (which calls ) and one to which totals 3 calls. For you will need one call to (which then does 3 more calls), (which does 1 extra call for you) and i.e. (3+1) + (1+1) + 1 = 7. Continuing on will need (7 + 1) + (3 + 1) + (1+1) + 1 = 15. I think you can see where this is heading. For you will need calls. So for you have calls to . To put that into perspective considering that it's been estimated that there are about atoms in the observable universe. I wouldn't want to stick around and wait until that completes. Not even if every atom in the observable universe was a 1 THz computer working optimally in parallel to calculate this. Your code exhibits the same type of pattern, you make many recursive calls per call to so you are probably even worse off. Comparing to: 

The above has O(nklog(k)) worst case and O(n + log(k)) best case behaviour. You can improve the worst case by using a max heap for k_smallest instead of a vector. This would have the promised O(nlog(k)) run time. I leave that as an exercise to the reader ;) And you should read this is bad practice. You can see from the benchmark here: $URL$ that the code above is twice as fast as your original code for the given values (you need to test with typical values for your application to verify) and the heap version is even faster. 

This would have been \$\mathcal{O}(n\cdot \log (n))\$ for a binary tree implementation or \$\mathcal{O}(n)\$ for a hash-based implementation. Sorting and ditching order If you sort the array first (as you suggested), you can then iterate over the array and copy the first element and then any remaining elements that differ from the previous element. This algorithm will have \$\mathcal{O}(n\cdot \log (n))\$ runtime. The sorting dominates the run-time here. For small lists, the code by Janos is adequate. If you want performance for large lists you really do need the sort. This solution will not preserve order of elements but will have faster run time for larger lists: 

To answer your question about which of the methods is better. Note that is defined in and inherited by , and . Calling will get all windows, even owner-less dialogues and system windows associated with the application, regardless of if they have decorations or not. On the other hand is defined in so calling will get all windows with decorations (frames). If your application doesn't have any frame-less windows, the two pieces of code you posted will be equivalent. The Code Your code is likely fine as it is wither either approach as plain s are kind of rare and often transient. The approach you're using is the standard one. If you want to be absolutely sure you get every window there is and be picky about it, this is how I would write it: 

Number 1 and 2 are bad in my humble opinion. You might want to do something else with the compressed data at some point, like encode it in base64 and transmit to a webpage in text mode. Number 3 is so-so depending on how you use the Huffman coder. I disagree with the accepted answer that the code is clear. There is a limit when small functions become too many and I believe you have crossed that line. It is difficult to follow the algorithm in order to verify correct implementation (I understand Huffman it's just difficult to follow with jumping across all functions). Test that your compressed data is smaller than the source data Your tests should really be unit tests as palacsint says in their answer. Also the probably most important test that you should do is to see that the compressed data is smaller than the original data. Specifically, any correctly implemented Huffman coder should perform very closely to Shannon's Source Coding Theorem. For sufficiently many samples you should be within 1% of the limit in that link. Other comments You are generating your prefix codes as strings and by doing string addition. I find this kind of code difficult to follow as you are covering up the fact that you are doing bit arithmetic. I would use a natural ordering for the nodes instead of implementing a comparator class. I do not see any mention of a stop symbol which prevents accidental decoding of junk when your output bitstream is not an even multiple of 8 bits. And this: 

This is the only use has in C++, it means that writes and writes to memory will always happen and will happen in the order specified. For example the compiler can, and frequently does re-order stores and writes. The compiler can remove memory writes and reads if it can determine that they will not affect any memory read or write. The compiler can even remove memory allocations if it so pleases (clang does this some times). For completeness, all inputs and outputs (files, , , , , keyboard drivers, graphics display, sound buffers, networks packets) are volatile either directly or transitively. So the compiler cannot remove inputs or outputs of the program, but anything in-between basically. When should I use then? Volatile should be used when you must be certain that a certain write or read is not removed by the compiler as dead. For example when you are writing a hardware driver, you would set control variables that must be written to some bus as volatile. But works for synchronising my threads just fine! That is pure luck that it is working on your compiler. The compiler is allowed to change the order of reads and writes AROUND your volatile accesses as long as they don't affect the values of the volatile accesses. Also the CPU may or may not re-order some stores or loads depending on the CPU (x86 has a conveniently strict memory model which only has one case where memory accesses may complete out of order). What you need is a memory barrier. A memory barrier forces both the compiler to generate code in such a way that all writes before the barrier happens before the barrier and tells the CPU to make sure all writes before the barrier has happened before the barrier completes. The above is a bit simplified, there are different memory types of barriers which say what must be completed before the barrier completes. If you're interested, see . Does affect my performance? Yes, the compiler can do quite some fancy optimisations with re-ordering stores and loads and instructions so that they will effectively use the CPU's registers and to make sure that multiple issue kicks in on some CPUs. By using you are prohibiting the compiler from doing some of these optimisations as you are forcing it to emit a read or write in a specific order. In summary, don't use unless you're absolutely sure that you need it. Over-aligning basic types You specify: 

Some general comments not already mentioned. Java Doc Java has a documentation standard for documenting members, classes and methods called "JavaDoc" most editors support this documentation and allow you to auto-generate stubs which you can just fill in. It is highly advised that you stick to this standard instead of ad-hoc formatted comments. Unit Tests You really should convert your method to be a suite of unit tests. There are many unit testing libraries out there but in my opinion the most common one is JUnit which is integrated in eclipse and probably other editors as well. 

Why do you not want to use a virtual destructor? This would make all your headaches go away. Yes you need a v-table, yes it's an extra indirection to figure out the correct destructor to call. But unless this is in the 1% of code that is your inner-most time critical loop, it will rarely matter on PC platforms. 

Visual Studio 2013 still doesn't support the keyword in C++11. This causes some problems with alignment of types in various situations. Thankfully the compiler will generate an error when it can't guarantee the alignment or at least give a warning. To solve these errors and warnings I'm implementing a wrapper type that will wrap another type with some alignment restriction regardless of how the wrapper is aligned. Consider two wrapper objects and that themselves have arbitrary alignment. Let the wrapped datum be and respectively. Then will assign while still keeping aligned regardless of how was aligned because in itself is aligned. I'm looking for a review on the code on the account of syntax and possible weaknesses in the design and implementation. 

Locality of reference You are using linked lists. Linked lists have notoriously bad locality of reference and this will severely impact your CPU cache hit/miss ratio and is likely a part of your performance problem. The best thing you can do is to use a float buffer directly and build the data in it, I'm assuming the is basically just an array of s. This will have the best locality of reference you can get. The fact that you are allocating a new for each vertex is likely causing significant stress on the GC. Another good reason to use directly. Use fixed sizes With some math you can figure out how large your index list needs to be, this means here too you can use an directly and save lots of work of adding to a list. Avoid unnecessary branches This here: 

I'm aware that this is not perfect and will probably fail for types other than but it does illustrate a technique. It can probably be done in a prettier way too. It doesn't provide exactly the same convenience as OP but it is close IMHO and it doesn't use macros. If one wishes to get the same simplicity as OP one may use a macro in the tune to: