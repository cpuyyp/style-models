This comes from this MSDN link found almost instantly after a quick web search. Bear in mind there are a few obvious and not-so-obvious functional reasons why you would not want to failover from Enterprise to Standard as some features are not supported in both. The idea of a clustered environment should be that you maintain integrity of the database environment upon failover, so why compromise that in any way, whether talking about support or features? 

Checkdb creates a snapshot in the background. Snapshots are supported by sparse files (they look large in Windows but are typically almost empty). Could it be that you are looking at this file? 

Just my 2cents from my own experiments on 1-2 year old hardware: Read-only operations (DW-style scans, sorts etc) on page-compressed tables (~80rows/page) I've found to break-even at compression size reduction of ~ 3x. I.e. if the tables fit into memory anyway, page compression only benefits performance if the data size has shrunk by over 3x. You scan fewer pages in memory, but it takes longer to scan each page. I guess your mileage may vary if your plans are nested-loop and seek-heavy. Among others, this would also be hardware-dependent (foreign NUMA node access penalties, memory speed etc). The above is just a rough rule-of-thumb that I follow, based on my own test runs using my own queries on my own hardware (Dell Poweredge 910 and younger). It is not gospel eh! Edit: Yesterday the excellent SQLBits XI presentation of Thomas Kejser was made available as a video. Quite relevant to this discussion, it shows the 'ugly' face of CPU cost for page compression - updates slowed down by 4x, locks held for quite a bit longer. However, Thomas is using FusionIO storage and he picked a table that is only 'just' eligible for page compression. If storage was on a typical SAN and the data used compressed 3x-4x then the picture might have been less dramatic. 

I have two computers. Computer A is dedicated to collecting data and storing it in its local database. Computer B is for analysis. A is running flat out just with the data collection, so I want to replicate the collected data onto B. The catch is, analysis will generate its own derived data which should also be stored in the database (but only on B). My understanding is that the off-the-shelf replication mechanisms require the recipient database to be read-only. Is it possible to do something like streaming replication, but which allows the recipient database to be read-write? Ideally, the schema containing the tables replicated from A would be read-only to analysis users, but separate schemas owned by those users would be read-write for them. It is okay for the replica to be hours out of date. I want to keep data collection on A writing to a local database, so that it can continue whether or not B is available. (Currently I'm on 9.3 but about to upgrade to 9.4 betas for unrelated reasons.) 

Long shot perhaps, but worth checking file \Program Files\Microsoft SQL Server\100\DTS\Binn\MsDtsSrvr.ini or the equivalent on your setup. You may have to manually edit it with the instance name. Otherwise SSIS connections might be looking for an msdb of a default SQL instance that doesn't exist. 

SSDs tend to have a natural 4K sector size as far as I know, but would it be beneficial to format Windows cluster size at 64K? This is good practice for spinning rust, but is it relevant on SSDs? 

And this returns: 32 How can that value actually be contextualized? Is it an "average of 32 batch requests per second in the last minutes"? Or is it "in the last second there were 32 batch requests" 

Unless you have Systems Centre Operations Manager with the SQL Server Management Pack, a good way is to set up some sort of data collector that queries the appropriate DMV's. Assuming you are running SQL Server 2008 + you can set up a Management Data Warehouse on the instance using SQL Server Management Studio. The result is a graphical user interface in the style of a performance dashboard. Link: Generic How To Guide on MSsqltips Set up out of the box is easy and informative, and when you get bored of the default measures, it can be easily extended. 

This query is fast when and are both empty, but if there are even a handful of rows in either, it takes several minutes to run. It needs to be efficient across the entire spectrum of possibilities for what's in -- from zero to 50 million rows; from almost no rows weeded out by the anti-joins, to almost all of them. There will typically be order of 5000 rows in , but they change constantly. (The next step is to insert all of the jobids that this query returns into that table; when the worker machines report results for those jobids, they are removed again.) Adding indexes on the various columns did not help, so I presume I need to structure the query differently, but I'm out of ideas. Please also feel free to tell me to reorganize my tables; this is the first time I've tried to do something this large with an RDBMS so I can easily imagine those being poorly laid out. Here's an EXPLAIN ANALYZE dump (sorry it's so wide) taken with 4000 artificial rows in and nothing in : 

Setting the database to single user mode will close/rollback any existing connections except the current session Set it back to multi user afterwards to return it to normal. 

I know people will recommend Ola Halengren's scripts but I've always sidestepped them (as good of a resource as they are) simply because I want to be the architect of our maintenance scripts and creating them by myself furthered my understanding of indexes and maintenance plans in general. So with that in mind, here is a pretty flexible and lightweight script that will rebuild (or reorganize) indexes only if necessary, as defined by the percentage fragmentation thresholds. Using a more selective script as per the below, I have been able to reduce index maintenance times massively. We've seen reductions of 15 hours in some of old MP's designed in Management Studio on large databases. 

Yes, but only from SQL2012 onwards, if I remember correctly from Bob Ward's 2013 PASS session (gave me a headache!) 

Just came across this 2-year old link: $URL$ It implies that 64K NTFS cluster size is still recommended for SSDs To improve this answer it would be ideal to hear from real-life experience with latest generation SSDs (FusionIO or SATA-controlled). Maybe 256K is even better for columnstores on SSDs! 

I'd be very careful using this flag on a VM, as their memory has an extra level of abstraction. Had more than enough trouble with it on physical servers with lots of RAM dedicated to SQL. Example: with 3 2008R2 instances co-hosted when restarting one of them it took forever to come back because it could not find contiguous memory segments anymore. The performance benefits were neither here not there (lets say 'statistically insignificant overall). I treat it as a 'TPC special'. Also consider that 834 doesn't play nice with columnstores either. 

To me that means the instance wants 4GB and can only consume 878MB. There are no other memory intensive processes running on this server. The output of dm_os_process_memory is as follows: 

Can anyone tell me why Instance 2 is not using the memory it has been allocated, and how best to troubleshoot? 

Here's an example which does what you want and will get you started with using PIVOT. Sorry it is a bit rushed, but hopefully it will get you started and show you how it can be done fairly easily. There will be limitations and I would fully advise researching and playing with the pivot functions because they are really powerful. 

To give an idea of the scale, there are currently 5,361,151 unique values in , and each one of those will ultimately generate order of 10 rows in . Now, populating involves, as I mentioned, a large distributed computation. There is a single dispatcher process for that computation. Periodically, it needs to retrieve the next block of job IDs which have not yet been processed in a particular "locale". The query I have for that right now is 

I have a weird problem I am struggling to troubleshoot. I have a development server with 18GB RAM and two SQL Server 2012 SP3 instances with @@version output: 

As far as I can see from checking gpedit, Lock Pages in Memory is not enabled on the server. Each instance has the following memory settings: Min: 1024 Max: 7168 I'm getting performance issues with Instance 2. Slow queries etc. Instance 1 is fine. To do a basic check for memory usage I'm running: