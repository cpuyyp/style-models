Your property implies in particular: "Every nowhere dense closed subset of $X$ is compact." (A nowhere dense closed subset is the boundary of its complement.) This in turn is equivalent to compactness for $T_1$ spaces with no isolated points, as shown by Katetov in 1947. Interestingly, the result holds as well for Lindelöf spaces, or more generally for $[\lambda,\kappa]$-compact spaces (a space is $[\lambda,\kappa]$-compact if every cover of it by $\le\kappa$ open sets has a subcover of cardinality $<\lambda$). This is due to Mills and Wattel, a very nice short proof can be found in Blair's "Some nowhere densely generated topological properties" (easily found online). Completely unaware of these results, I actually recently published a paper whose main result is weaker than Mills and Wattel's, and with a more complicated proof than Blair's. 

Let $T$ be a Suslin tree and $f:T\to Y$ be continuous. ($T$ is endowed with the order topology.) Assume that the image of $T$ is contained in a Lindelöf subset of $Y$. Then, force with $T$. Which properties of $f$ and/or $Y$ ensure that the image of $T$ is still contained in a Lindelöf subset of $Y$ in the forcing extension ? I know that spaces which remain Lindelöf after a countably closed forcing are called indestructibly Lindelöf and have been recently well studied. The preservation of Lindelöfness after adding Cohen or random reals has also been investigated, with some interesting results. It might be that some of these results hold as well when one forces with a Suslin tree, but I must admit that my knowledge of forcing is not very "hands on" and I am not able to see quickly if this is the case. My motivation is to try to extend an old result of Steprans which says that a continuous real valued map on a Suslin tree has a countable image. 

The reverse implication can be proved as follows: first notice that $\forall n\in Z$ $AC(\{n\})$ implies $AC(\{p\})$ for each prime $p$. Then, apply the following general result (Theorem 7.15 in Jech's book): 

I think a keyword to help you here is Rademacher Complexity. Learning theorists know a lot about these kinds of questions. In particular, for the intervals-on-a-line case, the high-probability bound for the maximum discrepancy should be $O(1/\sqrt{n})$, where $n = |O|$. More generally, if $O \subset \mathbb{R}^d$ and $T$ must be the intersection of $O$ with a halfspace, the bound should be $O(\sqrt{d/n})$, I think. [I could be mistaken here, I didn't look at the details very carefully.] 

The classic example, given in all complexity classes I've ever taken, is the following: Imagine your friend is color-blind. You have two billiard balls; one is red, one is green, but they are otherwise identical. To your friend they seem completely identical, and he is skeptical that they are actually distinguishable. You want to prove to him (I say "him" as most color-blind people are male) that they are in fact differently-colored. On the other hand, you do not want him to learn which is red and which is green. Here is the proof system. You give the two balls to your friend so that he is holding one in each hand. You can see the balls at this point, but you don't tell him which is which. Your friend then puts both hands behind his back. Next, he either switches the balls between his hands, or leaves them be, with probability 1/2 each. Finally, he brings them out from behind his back. You now have to "guess" whether or not he switched the balls. By looking at their colors, you can of course say with certainty whether or not he switched them. On the other hand, if they were the same color and hence indistinguishable, there is no way you could guess correctly with probability higher than 1/2. If you and your friend repeat this "proof" $t$ times (for large $t$), your friend should become convinced that the balls are indeed differently colored; otherwise, the probability that you would have succeeded at identifying all the switch/non-switches is at most $2^{-t}$. Furthermore, the proof is "zero-knowledge" because your friend never learns which ball is green and which is red; indeed, he gains no knowledge about how to distinguish the balls. 

They are different since there is (for instance) a model of $\forall n\in \omega$ $AC(\{n\})$ which does not satisfy $AC(\omega)$, see Theorem 7.11 in Jech's "The axiom of Choice". Notice that $AC(Z)\Rightarrow\forall n\in Z$ $AC(\{n\})$ and if $Z$ is finite, then of course both are equivalent. Will Sawin gave the correct answer for Question 1: 

As I wrote in my comments, D.N. Sarkhel constructs such examples in Section 4 of "Some generalizations of countable compactness" (Indian J. pure appl. math 17 (6), 1986). It works as follows. Fix some $n\ge 2$. Take a partition of $[0,1]$ into pairwise disjoint dense subsets $A_i$, $i = 1,\dots, 2n$. If $x\in[0,1]$ there is a unique $n(x)$ such that $x\in A_{n(x)}$. Then set $E_i=A_i$ when $i$ is odd and $E_i = A_{i-1}\cup A_i\cup A_{i+1}$ if $i$ is even, where $A_{2n+1} = A_{2n}$. Then $G$ is open if for each $x\in G$ there is an interval $I(x)$ such that $x\in I(x) \cap E_{n(x)} \subset G$. Sarkhel proves on page 782 that this space is Hausdorff and $n$-star compact but not $n-1$-star compact. It is not possible to obtain regular such spaces since $2$-star compactness is equivalent to $n$-star compactness for any $n\ge 2$ in this class, and to pseudocompactness in the class of Tychonoff spaces. This is shown by van Douwen, Reed, Roscoe and Tree in "Star covering properties" (top. app. 39 issue 1, 1991). 

Any Lindelöf non-metrizable Hausdorff space will do (EDIT: you need that the space is C-closed as well, see below), but more generally, a space is called isocompact iff every closed countably compact subset of X is compact, cl-isocompact iff the closure of a countably compact subset is compact, and C-closed iff any countably compact subset is closed. This paper by Cho and Park contains various results on (cl)-isocompactess, as well a other references: $URL$ And this one by Ismail and Nyikos has good informations on C-closed spaces: $URL$ 

The sharpest multidimensional Berry--Esseen Theorem I know is due to Bentkus and appears in the paper "A Lyapunov type bound in ${\mathbb R}^d$". $URL$ It does not use the characteristic function, though. 

The problem must always involve an accuracy parameter $\epsilon$, since in general the answer will not be rational Even in the case of a standard 1-dimensional normal, the question is not so easy. I believe this can be done to accuracy $\epsilon$ in $\widetilde{O}(\log^2 (1/\epsilon))$ time -- i.e., essentially quadratic time in the number of digits of accuracy. The best reference here (I think) is Brent and Zimmerman's book "Modern Computer Arithmetic"; see their discussion of the erf and erfc functions. In the general $n$-dimensional case, I'm not sure whether you can get a $\mathrm{poly}(n, \log(1/\epsilon))$ running time. I'd possibly guess 'yes', but I don't know any reference. I would say that "in practice" the easiest solution would indeed be a naive Monte Carlo estimation. Assume first that you can generate standard Gaussians and do real arithmetic in constant time. Then you can get the answer (with high probability) to additive accuracy $\epsilon$ with $O(1/\epsilon^2)$ samples. Assuming you have the square-root of $\Sigma$, you could then probably get the answer in $O(n^2/\epsilon^2)$ time. (But then one technically has to worry about assumptions like generating standard Gaussians to sufficient accuracy, doing the real arithmetic, factorizing $\Sigma$ if necessary...) 

I refer to the mathematician described here: $URL$ I am interested in learning, e.g., his full name. 

This 74-page paper in Journal of Machine Learning Research (by Huang, Guestrin, and Guibas) -- $URL$ -- is an amazingly useful and undergrad-friendly intro to the representation theory of the symmetric group (in a very surprising venue). Also, the fact that its applications are to machine learning and statistics looks very appropriate for OP's question. 

Max-Cut, at least, in cubic graphs is NP-hard even to approximate to some factor .997. This is due to Berman and Karpinski, 1999: On some tighter inapproximability results. In Proceedings of the 26th International Colloquium on Automata, Languages and Programming, Prague, Czech Republic, pages 200–209, 1999. I wouldn't doubt it if the optimal cut is a bisection in the "yes" case of the reduction. 

Given a linear objective function and a system of linear constraints, is there any known closed form lower bounds for it? to clearly express the problem assume that $$ z(\mathbf{a,B,c})=\mathop {\inf} \left\{ {\bf{a^Tx} |\quad\bf{Bx-c}\le0} \right\} $$ is any closed form (rather linear) function $y(\mathbf{a,B,c})$ versus $\mathbf{a,B,c}$ such that $y(\mathbf{a,B,c})\le z(\mathbf{a,B,c})$. As I know, any feasible dual solution for the above problem is a lower bound for it. However, the dual problem take the similar form of the primal problem. How can I obtain a close form lower bound for the above problem? 

Consider the following constrained optimization with the integral objective function $$ \min_{x_i\in D} \int_{t_1}^{t_2} \frac 1 {t - \sum\limits_{i = 1}^N x_i f_i (t)} \, dt $$ where $t - \sum\limits_{i = 1}^N x_i f_i(t) > 0 $ and $f_i(t)$ are polynomial functions. $D$ is a convex set. Is there any numerical method to solve the problem? 

Assume that ${\bf{z}} \in {\mathbb{C}}^{n \times 1}$ is a CSCG random vector denoted with $\mathcal{C} ~ (\bf{\mu} _0,\bf \Sigma _0)$ where $\mu _0$ and $\bf \Sigma _0$ are mean and contrivance matrix, respectively, and defined as $\mu _0=E({\bf z})$, $\Sigma _0=E({\bf zz}^H)$. How can I obtain the following expectations versus $\bf{\mu}_0$ and $\bf{\Sigma}_0$.? 1) $E\left( {{\bf{x}} \otimes \left( {{\bf{x}}{{\bf{x}}^H}} \right)} \right)$ 2)$E\left( {\left( {{\bf{x}}{{\bf{x}}^H}} \right) \otimes \left( {{\bf{x}}{{\bf{x}}^H}} \right)} \right)$ where $\mathbf{x}\triangleq\left[\mathbf{z},\mathbf{z}^*\right]$ and $(.)^*$ denote for complex conjugate. 

Consider the following mixed semi definite and second order programming: $\begin{array}{l} \mathop {{\rm{min}}}\limits_{\bf{X}} \,{\rm{Tr}}\left( {{\bf{XA}}} \right)\\ {\rm{s}}{\rm{.t:}}\, & {\rm{Tr}}\left( {{\bf{XA'}}} \right) + \left\| {{\rm{vec}}{{\left( {\bf{X}} \right)}^H}{\bf{A''}}} \right\| \ge a\\ & {\bf{X}} \ge {\bf{0}} \end{array}$ where ${\bf{A}}$,${{\bf{A'}}}$ and ${{\bf{A''}}}$ are $M \times M$ positive semi definite matrices, $a$ is a positive constant. $vec(.)$ is the stack column operator. By assumption of feasibility of the above problem, how can I obtain the order of complexity of the mixed semi definite and second order optimization problem? 

suppose we have the following optimization problem: \begin{array}{l} \mathop {\min }\limits_{{\bf{X}},{\bf{x}}} \,\,Tr\left( {{\bf{XA}}} \right) + 2{{\bf{a}}^H}{\bf{x}} + b\\ s.t:\,\,\,\,\left[ {\begin{array}{*{20}{c}} {\bf{X}}&{\bf{x}}\\ {{{\bf{x}}^H}}&1 \end{array}} \right]\succcurlyeq {\bf{0}},\,\,\,\left[ {\begin{array}{*{20}{c}} {{c^2}}&{{{\bf{x}}^H}}\\ {\bf{x}}&1 \end{array}} \right] \succcurlyeq {\bf{0}} \end{array} the matrix $\bf{A}$ is a hermitian matrix but not necessary to be semidefinite. I want to know how can I write the dual form for the above optimization problem?