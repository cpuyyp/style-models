Limiting a program to just one instance isn't an OS function: it has to be coded right inside the code. This means the correct answer is: it depends on how the program performs the single instance detection. There are several ways of doing this: 

I was tasked to port and move an old ASP application from a Windows 2003 server (x86) to a Windows 2008R2 server running IIS 7.5 A number of things needed to be changed (mostly related to registry access) but I stumbled across one problem that I couldn't solve in a satisfying way. The application makes use of an external COM object (in-process dll). It will create an instance of that object during the login phase and store it as a session variable. This used to work quite nice and worked well in my test but when I deployed it on the first production server, I started to have random "500" errors when users tried to log in. The error was really not consistent so I used procmon to trace the source of the problem and got this: when a user tries to access the web site, the IIS worker process tries to open the Dll that hosts the COM object but fails with an access denied error. Now, I don't understand this: the web application uses a service account for accessing the local resources. That service account has explicit rights on the directory where that COM library is located. Yet, the access to that file is still denied. I "fixed" the issue by giving "everyone" explicit read and execute access on the dll and it (seems to) have solved the problem. But I don't know why. The strangest thing is that is I use a local administrator account to connect to the web page once (and just once), then the issue was fixed for as long as the application wasn't recycled. I would appreciate if anyone could shed some light on that issue. I can live with the fix, but I really would like to understand. Edit To clarify: the web site is NOT set to impersonate the connecting user. In fact, the only authentication scheme enabled is "Anonymous Authentication". The basic settings, however, are set to use the service account. Edit The identity reported by procmon during the access denied is "IIS APPPOOL\(application name)" 

Check if, by any chance, there isn't already a computer account with the same name in the destination domain. 

First, I would like to point out that you're probably taking the problem from the wrong end: in theory, you should write down a business case for a better backup solution, have your management submit that to the central IT and then let them sort it out themselves. Now, this being out of the way, you're going to face a number of issues with that proposal. The first one being that Mercurial isn't built for what you want to do and neither is robocopy. You can find ways around you problem by deslaring the backup location itself as being a repository but that's not really efficient. I would instead investigate two different tracks. The first one is to use Volume Shadow Copy to create a snapshot history that you store on a different disk array on the server (you seem to be running windows on your file server and I'm assuming you have a recent version 2008 or higer). That will give you an easy way to recover user files (actually users will be able to recover their files themselves) while keeping the backup size close to the data size. The downside of this is that you will need admin access on the server to set this up so there is no way around involving your IT administrators (which I personally think is a good think, but that's only me). The second option is to bit the bullet and get a "real" backup software. If your data is important enough, you can probably make a business case for that investment. If not, you can always use Cobian backup to create incremental copies of the files as a user-level task or, if you really want to use mercurial, use Cobian backup instead of Robocopy to create the repository copy. 

In theory, you can query the DNS server for a zone transfer (AXFR) request to get the whole zone file. In practice, however, this is not going to work against a domain that isn't your own: this option is usually not enabled from Internet at large. 

Bad clusters are typically detected by the hard drive itself and then re-localized transparently. If your OS reports bad c√©lusters, it means that there is no more reserve cluster available and that the disk REALLY has gone bad. Now, Windows RAID will alert you in case there is a bad cluster because the raid will then fail and the disk will be marked as bad. Typically, you don't wait for that stage, though: you use the disk's SMART status report to see how many bad cluster relocation the disk had to perform and pro-actively change the disk when it is too high. 

(I picked 911 here as event ID but you can use whatever you see fit as long as it doesn't conflict with something else). 

It's a bit hard to answer that without knowing exactly what you want to monitor and why: bandwidth use ? number of transactions ? Number of connections ? Debugging an application ? Security check ? All these can be done but requires different approaches. Anyway, if I where you, I'd start by going to the DB server and checking the SQL performance counters to see if you don't have what you're looking for in there. 

I also made sure that the other app is NOT bond on 0.0.0.0:443 but uses the specific IP address. Finally, I ran and got this: 

I have written a very simple batch to perform that analysis. It will grab the appropriate symbols from MS's web site so you don't have to download them beforehand: 

How do you plan on accessing the table data ? If you're using something a bit flexible (your own code or simply SQl queries) then an easy way out is to copy the needed table to another database and the detach it from the server. Afterward, truncate the table (or delete the unwanted rows, although you might want to check your T-Log size of you do that). When you'll need it, you can always remount the relevant database from storage in read only mode. If you need to run queries on multiple backups at once, you can create a view regrouping all the data. If you're using a software that won't allow you to customize how that table is accessed, you can still copy the table to another database on its own but restoring that backup can be problematic because you'll need to merge the existing context (the other table states) with the old data. Plus, the data structure might have changed making it next to impossible to restore. So in that case, I would suggest that you simply take a backup of the full database and store that (preferably along with a version of the software that accesses it). 

You can setup a WebDAV server on the receiving server, map it as a dive of the source server and use robocopy or whatever file copy tool floats you boat. Of course, without some detail about what OS you're running and what you call "an intranet", there is no way to be more specific (or even tell you if you have the option to setup WebDAV) 

As far as I know, you cannot do that unless you write a script to do the lookup and adjust the rule. But in all honesty, you shouldn't do that: DNS isn't really a secure service unless you're using DNSSEC (which I doubt). In adition, you shouldn't need to do that, really, unless you have restriction on your outbound connections from an external machine. What are you really trying to do here ? 

Edit: actually, there is an even better way: First, you need to isolate the app in its own apppool. Then you just issue the following command: %windir%\System32\inetsrv\appcmd.exe recycle apppool "MyApplicationPool" (use %windir%\System32\inetsrv\appcmd.exe list apppool to get the name if necessary) That should recycle the app gracefully (unless it's really stuck). 

Frankly, I don't think you'll find any product that does what you request, mostly because the best way to handle bandwidth management is at the network layer. In your case, I would setup a QOS rule at the perimeter and define bandwidth reservation for the specific path you want to prioritize. 

Changing the dynamic port range in windows will not help. The only thing it does is limit what port will be used for the client side of a socket connection. What you need to do is limit the port range used by IIS's FTP server. There is a pretty good blog post about that on MSDN but here are the high points: 

If you did not make a backup copy of the private keys attached to your SSL certificate, then your only option is to generate a new request and ask your CA to sign it again. Some commercial provider will do it for free the first time (for the same FQDN), others will ask for a small fees, some will require a completely new payment. Also, next time, generate the key outside your production machine and keep a backup of your private key in a safe place (i.e. not in a networked location: that key is your reputation). 

(where 1.2.3.4 is actually the IP address of your destination machine) Save the file, and you're done. Note, however, that this will just change the IP address of the machine that you'll connect to when using the dns host name. It will not change anything in the content of these connections so if you're using a protocol like HTTP (or SSL) where the host name is embedded into the data, the target machine might not respond properly. 

Have you considered using VMs instead of trying to stuff everything on the hardware ? That would make your system easier to backup and manage. You might need a couple of additional windows server licenses but not only will you get around the issue of having multiple services running on the same OS but you'll also gain the ability spread the load more efficiently between physical machines. It will also make DR much easier assuming you have a good backup policy. 

I would do two things to try to fix the problem: First, remove the AV. Completely. Don't just disable one part or another, uninstall it. Second, assuming it still fails from time to time, change the NIC and cables. 

If you want to use the event log instead, then you can use the built-in eventcreate command like this: 

It should bring the DB online. Read the integrated help on the restore command. It really contains information you cannot ignore. 

I woudl suggest that you check your path MTU and adjust your NIC accordingly. I've seen this happen when routers would drop oversized packets without sending back the proper ICMP messages (or when these messages would be dropped by a firewall on the path and not acted upon). 

Actually, you can and it usually works without any issue (although not always) What you do is rename the file without moving it and move the new file over it. That will keep the handles to the file valid and working so that pre-existing instances still will be able to access the file properly and new instances (or new handles) wil go to the new file. Obviously, if a program re-opens the same dll file and expects it to stay exactly the same (for instance, if there is resources to be loaded from the dll and that references to these resources is extracted from code running when the dll is loaded), this will cause issue but this is definitely not the norm. 

For creating self-signed certs, you have plenty of options. The simplest one, if you're a windows shop, is to do it through IIS (see this: $URL$ You can also do it with OpenSSL (quite messy but works), with the makecert.exe tool that comes with the .NET SDK or with a number of similar tools (I use my own tool for this but, that's just me). For integrating a CA with AD, the simplest way is to install the certificate services role on a machine and configure it for AD integration (although in your case, it doesn't seem to be a necessity unless you want to use it for other things). Finally, you might want to create your own root that isn't integrated with AD. Unless you have to work with client certificate authentication, have many different servers (with different names) that you want to use in testing (and perhaps with automated testing) or if you want to be able to test some aspect of your application that uses special certificate properties or chaining, it's probably not worth the trouble. In your case, assuming I understood it correctly and all you want to do is test your web app with a certificate, all I would do is generated a self-signed cert (using whatever tool you like best) and then install that certificate in the correct store on your test machine (to avoid certificate warnings and errors) 

That solution has one drawback: DNS answers are cached, usually for several hours. This causes a situation where users will not really be able to navigate back and forth between internal and external domains. The typical solution is to setup a relay server that can be reached both internally and externally with the same IP address. 

Assuming you are using a Windows 2008R2 system and a 2008R2 AD, you can use a managed service account for this. This technet blog entry has a pretty good summary of how to use managed service accounts but, here are the basic principles: A managed service account is an AD account that is strongly tied to a computer and that has an automatically managed password. You don't create the password and nobody needs to know about it but since it's an AD account, you can use it for network ACLs which makes it perfect for your scenario. Using a MSA for a scheduled task will, however, require you to use the command-line to create your tasks (see this and this thread for more details). 

Check your machine's BIOS. There is usually an option that controls how the system will behave when a power failure occurred. Typically, you want to change that setting from "do nothing" to "automatically turn on when power is applied". Unfortunately, details changes from BIOS version to BIOS version so I can't give you more details about what that option is called, but it should be in the power management options. 

I have seen such a behavior already (and apparently, you and me are not the only ones). In my case, the problem was caused by having machines with duplicate SID in the domain due to a HDD clone followed by a sysprep without having checked the "generalized" checkbox. You can get the same issue if you're running cloned/P2Ved VMs. 

It's a really good time to check your backups, though: a single failed read on any of the disks in the array during rebuild and your data is toast. That's why I wouldn't recommend rebuilding the arrays any more frequently than you absolutely have to. 

That should do it. Edit: that's the "scripting" syntax. If you want to type this at the command line, you must remove one of the % sign each time they are doubled, linke this: 

To answer your question directly, the thing that can best improve your system's tolerance to hard drive failure is to use your extra hard drives to hot spare parts. With such a low disk count, the chances of having two simultaneous drive failure is low enough that you can rely on backup. Having a hot spare, however, will allow the system to recover much more quickly (and safely) from a single drive failure. You'll need two spares, though: one of each size. I have to add, however, that it sounds like your heading in the wrong direction here. First, you need to understand that RAID is not a backup system. There are many things that can go wrong with your RAID that will cause total data lose: manipulation error, controller failure, viruses, fire/water on the server itself. You need to use a real backup system. Second, the best way to increase redundancy for an AD is not to improve the disk subsystem or have better backup but to add a second domain controller, if possible at a different physical location. It will improve uptime much more than anything you will do for a single server setup. 

Intrusion incident analysis is never easy, in particular if you haven't followed standard procedure for recovery (take physical server off-line, grab a sector image of all file systems it has access to, rebuild machine completely from install media, restore data). Typically, you would review all the logs files fist, starting with the Apache logs. The most likely attack vector is, in your case, drupal but it's by no mean the only possible one so all logs should be checked (I mean ALL logs). Depending on the file system you're using, additional steps can be taken to identify what activity took place at the time of infection and figuring out both the attack vector used and what was done. Meanwhile, check all the software your machine is running and make sure it's all up to date. That includes all your drupal modules, themes, etc. I'd also check any custom code with a comb. Edit: I forgot to mention the fact that, in your case and since you apparently don't have the relevant expertise in house, you might want to consider contracting the incident to a security company that does forensic analysis: it might be a bit on the expensive side, but you'll get clear answers about what happened and how to prevent it.