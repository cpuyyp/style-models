You should have a logical model because it lets you think about your database design without having to get bogged down in physical constraints. You need to have the physical model because that will be where your data eventually lives. 

No, it is absolutely not proper normalization to move a name out to another table. Normalization reduces redundancy based on functional dependencies, not based on coincidental repetition of data values. Let's say there are 100 Joe Smiths in your database. If you move the name "Joe Smith" out to another table and have 100 records point at this name, what happens when one of the Joe Smiths decides to change his name to "Bob Smith"? Unless they all change their names in unison, you can't work with the data that way. 

I agree with Aaron Bertrand that pre-optimization is not a good idea. Relational database management systems are built for joining data. Don't presume a performance problem, observe it, preferably through thorough load testing prior to moving to production, then deal with it if necessary. As to your question about when does it make sense to duplicate data for querying, the classic scenario is in a data warehouse. Ideally, you want the data to be static, in other words, read only. This can be the case for historical transactional data, for example. In a data warehouse the data is written as close to once as possible and is read many, many times. If this is your scenario, then denormalization for reporting may be a reasonable design choice. However, any time you introduce redundancy in your data you open yourself to the risk of loss of data quality. This is what the Normal Forms are designed to prevent. Redundancy introduces risk that you need to manage. As long as you go in with your eyes open, you will be OK. 

This will allow you to work with measurements that need time-dependent conversion calculations. 2. Units of Measure Have Attributes Each unit of measure has a few different attributes. The obvious ones are indicative, like a code and maybe a descriptive name. There are also a couple of critical other attributes to keep for each unit of measure. (i) Unit Type and (ii) Conversion Factor to the Base Unit. The first tells you whether your unit of measure is a length, a weight, energy, power, currency, etc. etc. It should also tell you what the base unit of measure is. You should pick exactly one for each unit type. You can use things like kWh if you like, but I'd stick to the base SI units (as applicable) if I were you. The second tells you what your unit of measure needs to be multiplied by to get it to the base. I mentioned that this is an attribute of your UOM, but in fact it needs to be in a child table. The business key of the child table that holds this base conversion factor is the combination of the UOM, its base unit type and a date/time. I would keep both an effective and an expiry date/time on the base conversion factor table. This allows you to quickly find the right rate that applies at any particular point in time. If it happens to be a rate that doesn't change, that's OK. Just use a min-collating effective date and a max-collating expiry date for the one record. 3. Trying to Table-Drive Everything Will Make You Nuts The last piece of the puzzle is determining the calculation for moving from one kind of unit to another kind of unit. You could try to table-drive this kind of calculation but in the end the tricky ones are going to make the design so general (read complicated and slow) that it will be impractical. Instead, create a code-table of conversion calculations and use it to link one kind of Unit Type to another kind of Unit Type. Perform the actual calculations in some code somewhere. Which piece of code you use for any given conversion is what the code table tells you. How the calculation is performed is just in the code. You can have one calculation each for the various easy things, like area needs two lengths and volume needs three lengths as well as the harder ones like work needs energy and time. When you get the details of your design figured out you should blog it and come back here to post a link! 

There are two ways of approaching the answer to your question: First: Is pre-optimization a good idea? As a general rule, don't pre-optimize on the assumption that you will have a problem. Use volume testing to determine if you have a problem and denormalize for optimization purposes if that is the best of your available solutions/compromises. Second: Is this a good case for denormalization? Having said that, there is a practical limit to how much you want to be fussy about functional dependencies. Are your type A and type B messages really that different? They both seem to quack like a duck, as it were. Having only a single attribute different, and that difference being whether it is null for one set of records and not null for another set of records isn't necessarily a good reason to implement two distinct message tables. You might want to have a logical model that makes the distinction between type A and type B messages, but it doesn't necessarily follow that your physical model has to implement these two entity-types as separate tables. You have the option of using a constraint to enforce the relationship between message type and section number. You don't have to implement your constraint through normalization. 

Use a variation of number 2 - a location table with a involuted (self-referencing)relationship, but add something to make the hierarchy management easier. For example, use nested sets/visitation numbers (see here) or an adjacency list. The advantages of this approach are that: 

To achieve multi-tennancy you should limit the number of relationships you have to the factor that determines the tennant. For example, each of , and have a foreign key to . However, these child tables are also interrelated. Is it necessary for to have a ? You can get to the school a staff member works for by following the foreign key to . Similarly, you have grand-child tables ( and ) which are relating tables in curious ways. What has "months indicating duration of the course" to do with which staff member is teaching the course? Is it really necessary for to relate to ? Also, you appear to have repeating groups defined in your and tables. I believe you may have a normalization issue. By over-relating tables you create cascade cycles. Consider the following ERD as an alternative: 

The first thing you want to do to keep your thinking straight is to choose some clear terminology. You are using the word "assessment" in two different contexts: (i) a Test and (ii) the results for a student taking the test. I understand that "test" may not be the best term, since you might be assessing based on lots of different kinds of work. The important thing is that you need to clearly differentiate between a measuring instrument and the results of an individual student against that measuring instrument. Keeping these concepts separate will help you to keep your thinking straight when designing your database. I'd recommend that all of your RESULTS be kept in a single table. That will make things like report cards easier to handle. Whether the result is a percentage, a letter grade or a pass/fail, you probably want to have a common representation (percentage is a good lowest common denominator) and a weight to help you aggregate marks across assessments. 

You need to learn the difference between repetition and redundancy. Sometimes a field value is repeated coincidentally. This kind of repetition cannot be normalized out. Normalization is about studying the functional dependencies between key and non-key elements. It is not about putting everything that might occur twice into a table with a surrogate key. For example, you say that a phone number may be used by multiple companies and have modeled phone number as an independent table. This would prevent update anomalies if when one company changed its number all of the other companies using that number changed theirs at the same time and in the same way. Does any of that actually make business sense? It doesn't to me. Also, normalizing geography into a hierarchy is a questionable proposition. However, you haven't even normalized into a hierarchy. Is region ID not determined by country ID? If so, your LOCATION table is not 3NF. Similarly, are there postcodes that bridge states? I don't know the Australian system well enough, but I know that in Canada, the US and the UK this doesn't happen. You've designed everything like its a star schema and company is the fact. Facts in star schemas are usually transactions, not static entities. Transactions don't tend to change their properties, because they usually represent something that actually happened and one doesn't generally go back in time to change history. Static entities live for a long time in your data (maybe forever) and change their properties fairly often. Star schema is not a good, efficient model for this type of data. What you should do is run queries on the frequency distributions of each column and on combinations of columns that you think may be keys combined with columns that you know aren't keys. This will help you test your assumptions about which columns are truly able to act as keys. Then you should temper those findings with some common sense around what could potentially happen to your data in terms of changes to column values. Once you know your actual functional dependencies found in your data, then follow the relatively mechanical process of moving your relation to 3NF. 

In the above ERD, notice how the extra information that is not author-dependent remains at the level, while the new intersecting entity is added to record the connections between authors and a particular instance of the submission of a paper. This four table model is more robust because it allows for papers to be resubmitted multiple times with changes in authorship each time. If this kind of real world nuance is important to your system then you would be better off with the four table approach than with a three table approach. Here is the four table model with more column details at OP's request: 

There was a school of thought that said you should try to make user credentials hard to find in your database to make it harder for hackers to get at this information. I think that this is ultimately wrong-minded, because once a hacker gets into your database it's not going to be much of a problem for them to unload your data and figure out what it is, regardless of how many tables you've got and how much trouble you've gone to spreading columns around. The right place to thwart hackers is outside your firewall. You should keep your database schema as simple as possible to make it efficient and maintainable and take all of the necessary precautions to keep hackers from getting any access to your database. One other thing you should do by way of keeping sensitive data safe is to encrypt sensitive data. Don't keep things like passwords or credit card numbers in plain text. Use strong encryption for these kinds of data elements. 

Year number is part of a (three column) composite primary key. There will be one record per employee + year + type of leave (vacation/sick/...) Year number probably means the calendar year stored as an integer. means the total of leave actually taken, which would be derived by summing the days between and where those dates fall within the year given by for the same and . is next to pointless because it would be less . A real scenario would be that an employee is allowed three weeks vacation (15 days) and has taken two weeks so far (=10 days) and so they have 5 days remaining. The point of keeping these precalculated in this way is that you can refer easily to these values in a query or report. Notice the field. This implies that there is a regular (perhaps scheduled) batch job that calculates these derived values periodically, or perhaps they are recalculated when changes are made to either leave allowed or leave taken. 

Keep you list of people in one table. Then keep another table with the list of things that various people might need. Normalizing people and needed things is important for being able to write simple, reliable queries about your data from the perspective of either an individual person or a particular needed thing. Use a third table to keep track of who needs what, and how badly they need it. 

Normalization is not done to make reading data easier or more efficient. Normalization is done to make changing data simpler and less risky. It's when you insert, update or delete data that you will see the benefits of normalization. Therefore, if you are going to update your data much less frequently than you are going to read it, for example, if your data almost never changes, then optimizing for reading might make good sense. However, if you are going to be adding new data and changing existing data pretty often then you will be better off with Third Normal Form (3NF) as your starting point for your database design. 

Note that this is good for a point in time. If you need to track changes over time then everything becomes many-to-many with start/end dates in the intersection entities. 

In your case you could pick either approach. Which might be better depends on your preferences and on exactly how many distinct columns there are per sub-type. 

The best approach for this type of situation is to use logical sub-typing. Keep columns that are common in a central table which also contains a partitioning attribute. Then keep sub-type specific columns in sub-type tables which have identifying 1:1 relationships with the common table. In your case, this would look something like this: 

The problem is that some of these goals compete with one another. Sub-Typing Solution You could choose a sub-typing solution where you create a super-type that incorporates both corporations and persons. This super-type would probably have a compound key of the natural key of the sub-type plus a partitioning attribute (e.g. ). This is fine as far as normalization goes and it allows you to enforce referential integrity as well as the constraint that corporations and persons are mutually exclusive. The problem is that this makes data retrieval more difficult, because you always have to branch based on when you join account to the account holder. This probably means using and having a lot of repetitive SQL in your query. Two Foreign Keys Solution You could choose a solution where you keep two foreign keys in your account table, one to corporation and one to person. This solution also allows you to maintain referential integrity, normalization and mutual exclusivity. It also has the same data retrieval drawback as the sub-typing solution. In fact, this solution is just like the sub-typing solution except that you get to the problem of branching your joining logic "sooner". Nevertheless, a lot of data modellers would consider this solution inferior to the sub-typing solution because of the way that the mutual exclusivity constraint is enforced. In the sub-typing solution you use keys to enforce the mutual exclusivity. In the two foreign key solution you use a constraint. I know some people who have an unjustified bias against check constraints. These people would prefer the solution that keeps the constraints in the keys. "Denormalized" Partitioning Attribute Solution There is another option where you keep a single foreign key column on the chequing account table and use another column to tell you how to interpret the foreign key column (RoKa's column). This essentially eliminates the super-type table in the sub-typing solution by denormalizing the partitioning attribute to the child table. (Note that this is not strictly "denormalization" according to the formal definition, because the partitioning attribute is part of a primary key.) This solution seems quite simple since it avoids having an extra table to do more or less the same thing and it cuts the number of foreign key columns down to one. The problem with this solution is that it doesn't avoid the branching of retrieval logic and what's more, it doesn't allow you to maintain declarative referential integrity. SQL databases don't have the ability to manage a single foreign key column being for one of multiple parent tables. Shared Primary Key Domain Solution One way that people sometimes deal with this issue is to use a single pool of IDs so that there is no confusion for any given ID whether it belongs to one sub-type or another. This would probably work pretty naturally in a banking scenario, since you aren't going to issue the same bank account number to both a corporation and a natural person. This has the advantage of avoiding the need for a partitioning attribute. You could do this with or without a super-type table. Using a super-type table allows you to use declarative constraints to enforce uniqueness. Otherwise this would have to be enforced procedurally. This solution is normalized but it won't allow you to maintain declarative referential integrity unless you keep the super-type table. It still does nothing to avoid complex retrieval logic. You can see therefore that it isn't really possible to have a clean design that follows all of the rules, while at the same time keeping your data retrieval simple. You have to decide where your trade-offs are going to be.