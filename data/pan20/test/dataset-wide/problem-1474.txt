When you are collecting the unique integer values to a sorted list from the values of the original , you are doing work that has already been done, namely the elimination of duplicates by calling on the stream. You have already filtered out the duplicates when you populated the , so instead of collecting the values of the , you could instead collect the keys of the . Also, you are creating a , which stores its values in s (so it's like a ). This creates unneeded overhead, because the s containing the values of the multimap eliminate duplicates. But in this case, the multimap can never contain duplicate values in the first place, since its values are obtained from the key set of the original . It would therefore suffice to configure the multimap to store its values in s instead of s. By the way, the documentation says that the methods of the individual implementations will be deprecated in the future, and that should be used instead. Apart from that, you can omit the entirely by configuring the multimap to behave like a instead of a . This might also save a bit of performance, because you have two functionalities implemented in one data structure, as opposed to using a for associating each integer with one or more Strings, and an additional for sorting the integers. Of course, a does not index its keys, so you would have to maintain a counter yourself when you iterate over the keys of the multimap. Finally, the overall structure of your code sample is strange. Unit tests are meant to test whether some code works correctly. But your method not only performs the tests, it also contains the code to be tested. Of course, there's nothing wrong with placing assertions in code to test it. But what you wrote is not a unit test, because it doesn't test any defined unit (for example a method). It just executes some code and then uses assertions to check whether the code did what you thought it would do. Instead, I suggest that you first write a method that accepts a (or any where ) as a parameter and returns a "compressed" version of this map: 

Besides, I would suggest renaming the variable to something like , which would describe its purpose more clearly. Finally, I think there is still a bug in the method. By putting the line 

Just for the sake of completeness, there are things that this parser doesn't support other than comments and elements with mixed content, such as processing instructions or character references (e.g. or instead of , i.e. references to unicode code points rather than entities). Processing instructions are meant to carry information relevant only to the application receiving the XML document and are not part of the data stored by the XML document, but the parser should recognize them nonetheless. And since you support references to the five predefined entities (i.e. , and so on), it would seem natural also to support character references. The parser also doesn't read the prolog, which consists of an optional XML declaration and a likewise optional document type declaration, although, admittely, the XML declaration only contains information specific to the process of parsing the document itself (such as the XML version, or the character encoding), and the document type declaration defines such things as the stucture of the document and entities to be referenced by an entity reference, so it might not make sense for the parser that parses the XML data to also parse the prolog. So now about what you have implemented: 

Your code does a relatively simple task in a horribly complicated manner. It accomplishes this by two means: First, the logic itself is slightly more complicated than it needs to be. But the main problem is the code's design, which is so obscure that the actual purpose of the methods, fields, variables, classes etc. is almost incomprehensible without going through every line of code and seeing what exactly it does. The names of your methods do not help in this regard, either. I will first try to address the second issue, that is, make the code clearer and more managable, and then review the algorithm itself. The Code 

The same can be applied to as well. This reduces the number of methods needed in to 2, namely the two "deserialize" methods. Maybe you would agree that, with such a design, the class can be dropped as well, because now it is nothing more than a container for 2 methods that both merely wrap another method. 

I don't know why the HashMap algorithm takes so long here. It could have to do with the need to increase the capacity of the HashMap multiple times, but experimenting with different values for the initial capacity didn't make much of a difference. But anyway, I think this makes it clear that you shouldn't rely on the Big O notation when comparing the speed of different algorithms to each other. However, apart from the performance, your method returns a , which is a class, so whoever calls your method won't be able to do anything with the returned value. But regardless of that, I don't understand why, in your code, you invent something that already exists, namely a linked list. Why don't you just use ? In any case, you also asked whether there would be a better data structure than a linked list to return the found pairs. I think there is, namely a . After all, the array might contain duplicates, so it might also contain duplicate valid pairs. Of course, in order to take advantage of a , you would have to implement and in your class (which would then only contain the two integers and no nodes). Seeing as, in your algorithm, the first number in a pair will always be less than or equal to the second number, it would suffice for to compare and from both s to each other without sorting them first, thereby considering to s with the same numbers but in a different order unequal, but this depends on how you want to use the class further (in case you do want to use it outside the context of this algorithm). Note that, if you use a , the algorithm will naturally take longer than if you use a , because removing duplicates is an additional task that was not carried out in your original code. Edit: Actually, there's a much simpler and more efficient approach for avoiding duplicate number pairs than using a : When adjusting the array counters for the next loop, you could simply in-/decrement them until the integer they point to is different from the integer they previously pointed to, instead of in-/decrementing them only once. 

Also, there is no need to handle the case (which, after deleting , would translate to ) separately. The result of the method will be unaffected if you remove this extra and simply let the loop take care of it. In fact, I don't see anything special in this case in the first place, so I wonder why you make a special case for it. Besides, I think it is strange that you let the main method read the number of test cases from the command line, but delegate the reading of the individual test cases to the method, not least because this requires you to create a new object for every single test case. Why don't you do all the reading from the command line in one method, and then pass each test case to the method as a parameter and let it do only do what its name suggest, namely calculating the result? Anyway, I would suggest separating user interface from program logic, because right now, the method not only calculates the result, but also prints it to , which goes a bit against the Single Responsibility Principle. I think the code would be clearer if returns a representing the result, so that the calling method can decide what to do with the result. 

Depending on what you want to do with the class, you could also consider overriding to generate this . 

Your double pointer array beats the hash map again. Finally, increase the array size to 5,000,000 (the program ran out of memory when I tried to run it with 10,000,000-size arrays, probably because I generated all 10 test cases in advance instead of only generating one for each benchmark). 

Instead of using two s, you could use two instances instead, because duplicates in any of the two arrays can be ignored. Also, the declaration of and in could be moved into the innermost loop to reduce their scope to the smallest extent necessary, which would make the code a bit clearer in my opinion. Other than that, your code seems to be succinct and working. 

I would suggest making , but not , a . You only perform operations on , and is O(1) with an . Removal operations, however, can be very costly with an , because all the subsequent elements have to be rearranged. The worst-case-scenario is, of course, if you remove the first element of the list, which you are doing here. So would benefit a lot from becoming a . The trick is that removing an element from a is only O(1) if you do it via instead of one of the two methods defined in . So instead of calling and then calling , I would manually create an by calling and use this iterator to find the first occurrence of in . If you have found it, you can simply call on the iterator and remove the element from in O(1). Here is a summary of the advantages and disadvantages of and . In fact, does not need to be a at all. All you are doing with is retrieving a character at a specific index, and you can do that with a too. The time complexity of is O(1), just like that of , because uses a internally to store the characters (or apparently a before Java 9), so it should not be noticeably slower than , and since you would not need to create the from the in the first place, you can even save a bit of performance. Apart from that, here are some other suggestions for your code: 

Here, both and should be a that only contains options (a fits here perfectly, because it cannot contain duplicate elements and the order of the elements doesn't matter). The number of serial numbers containing this combination is only a value that is associated with this combination and not a part of the combination itself, and an empty belongs here even less, because it is only relevant for the output format and has nothing to do with the combinatorial logic itself. If you apply these changes, the above code could be replaced by this: 

You can prevent duplicate pairs by incrementing/decrementing your counters not only once, but until they point to a number that is greater/lesser than the number they previously pointed to. That way, you can store the pairs in a instead of a , which will probably be faster, because the doesn't need to check whether it already contains a pair that is equal to the one being added. Also, there is a typo in your method names . 

Using to look for a in a seems to be faster than searching for a in a using . On the other hand, requires the to be sorted, which is why we need a second that stores all distinct characters in the order they were first encountered in the original , assuming the returned must fulfill this requirement (if it doesn't, then the array is actually not needed and you can return directly at the end of this method, which might speed up the process a little bit). To ensure that stays sorted, it is important to insert new s at the correct position when updating , which is why two calls to are needed. I did some simulations with random arrays, each containing 100000 random characters. Here are the results of a set of 10 simulations: 

This seems to work, but I have no idea whether it really is faster than your code, or whether the caching and looking up of indexes of known palindrome substrings takes more time in the end than simply iterating through all possible substrings. 

I think this better represents the intention behind the code than using a (or a , which, I agree, is more appropriate than a here), because collecting the elements to a is not an aim per se, but just a means to accomplish something else, whereas the stream describes exactly what you want to do, seeing as streams are meant to represent operations on data as opposed to storage of data. 

This does not address all the issues with your code, like the countless unused variables that might be relics from earlier attempts, but it might give you an idea on what you can improve with your code. I would advise you to return to get another review if you have reworked your code (simply start a new question and create mutual links in the old and new question), since, as you say, the code is really very messy. 

Update So I've tried to implement the wraparound suggestion myself, and the code now seems to be about twice as fast as your version when removing and inserting from/to random positions in the vector. When only operating on the upper half, it is, of course, a little bit slower, because the overhead is never compensated for by shifting fewer elements than your original code. Here is my implementation: 

Now, I also have some points of criticism on the code design prescribed by your instructor, which I'm going to mention here as well for the sake of completeness. 

Admittedly, this is more code than your version, but then, you don't consider the possibility that the range is smaller than the velocity, and if this is the case, an object might bounce off an edge more than once. The above code could be optimized by first calculating the velocity modulo twice the range, because after two range lenghts, the position and velocity of the object will be identical to its initial position and velocity. Also, you should validate the arguments. This not only entails checking whether the arguments are valid with respect to each other (e.g. whether lies within the permitted range), but also handling special cases like the infinities or . E.g., if you check whether , then you automatically have covered for and , because the comparison operators and the equality operator always return if one operand is (even returns , which is why there's a method ), but might still be if is also . Also, should probably be greater than (which would not be covered by the aforementioned condition either). 

A final remark about the field . Technically, the size of your is not really a state of the object per se, but rather a value that depends on another field, namely . So you represent the same property of your object in different fields, which is basically an invitation for bugs. You might forget to update when you add or remove an element, or you might make another mistake that causes to contain an incorrect value. The problem I mentioned at the beginning of this answer is an example of this. Even if the exposure of the list's head node and its method is intentional, your object can still become corrupted because will not be updated if someone simply modifies the list's head node. If I were you, I would remove this field and instead make the method calculate the list size based on the field . True, this might make a call to more expensive than if the size were cached in a field, but this is not your problem but that of the caller. And if that caller is you because you want to ensure that the index parameter passed to , and is valid, you can instead rely on to throw an exception if the index is out of bounds, so you don't have to traverse the node tree multiple times. 

My suggestion would be to rework your code and request another review, since correcting some of the issues mentioned above could possibly require radical alterations to the code (it is perfectly acceptable to post a new question with a reworked version of a code from another question, but if you do, you should mutually link the questions). 

Finally, if you don't intend to change the values of , and , you can make these fields , so will be immutable, meaning that, once a instance is created, it will never change, which can make a lot of things easier simply because you don't ever have to worry about the possibility of a object being modified. 

Of course, you can replace the variable assignments with additions of a cell to , I just wrote it this way so that it is easier to understand. The indexes for the lower and upper neighbors are still calculated even though they might not be needed, but for the lower neighbor index, the code is still at worst just as fast as yours (if the index isn't needed), and faster if the index is needed. So the question of whether this code is in sum faster than yours ultimately hinges on the performance impact of calculating the index for the upper neighbor in advance. This probably depends on the number of rows in the list, in that if there are more cells with an upper neighbor than without, I'd guess this code is faster, but you'd probably have to try it out, because, as J H said, who knows what optimizations are already done by the JIT, and in the end, you might only save a couple of microseconds (or so).