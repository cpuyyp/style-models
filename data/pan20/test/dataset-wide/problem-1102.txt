One of our developers created a view whereby one of the select columns was wrapped in an RTRIM function. That same column on the underlying table has a non-clustered index on it. Now queries using this view and a where statement on this column are failing to use the index due to the RTRIM. I need the view's output to be consistent as unfortunately this developer is no longer with us and I can't read his code. What are some options for optimisation? I've got one, but I'm hoping there is better. 

My scenario is I have a thick client side app using a SQL DB. The use of the app is divided into 10 "sites" mostly by Geography but some by Usage. I've written an SP to archive selected data out of the DB and put it elsewhere as the DBs are growing to nearly 500GB in size at this point. Further whilst most servers host one application DB there is one server (and instance) that hosts 3 DBs. My dilemma is I want to write code that is the same everytime. Ideally I would write something like 

So it looks like the database context was set in the 'background'. What would lead to this circumstance? (Of the GUI showing blank) It's a minor mystery that is nagging at me now. 

In looking at a historical SQL Agent job on my employers server I noted a job with 4 steps. The Databasefield was set as master for the step 1 and not set at all for the other 3 steps. Yet each of those 3 steps contained simple SQL statements (exec SPs, inserts etc.) that all worked on a specific database. When I scripted the job out to a query window each of those 3 steps had the element: 

Scenario: In production we have the same functional DB at different physical sites. The column order on a table don't match each other so I am writing a script to amend this (a 3rd party application requirement). Action: To do this I copied the table to a test server, made the changes required in the GUI and then used the save to script functionality. The generated script essentially creates a new table with the proper order, copies the data into there, deletes the original table and then renames the new table to the old/proper name. Query: The generated table definition includes in part column definitions such as: 

Is it possible in SQL Server to set collation on individual rows in a table (as opposed to by column, database or server)? If not, perhaps does anyone know if this is a planned feature for SQL Server 2016? The only problem this solves is curiosity between two colleagues. 

I am getting failures on insert with the error: Msg 233, Level 16, State 2, Line 1 The column 'ID' in table 'dbo.pre_case' cannot be null. The insert statement is of the form 

Create a new computed column on the underlying table that is an RTRIM of the column in question. Add an index on this column. Adjust the view to use this computed column. 

I only noticed this was for Firebird part way through so sorry if that doesn't work. You could also do: 

I would like to move the tables in my database from one filegroup (Primary) to a another (a new one). I can find lots of advice on this concerning tables with clustered indexes but all these tables in question are heaps. So far I implemented the solution found here: $URL$ which essentially provides a script to create a clustered index for the table on a secondary filegroup (which essentially moves the data) then removes the index to return it to being a heap. This seems to work fine but may be too slow for our production requirements. Are there any alternative solutions? This doesn't feel quite elegant enough for me to believe as the only method. Note1: The tables must remain heaps. A 3rd party app enforces this. Note2: The DB is about 150GB in size across 200 tables. 197 tables and 99.9% of the data will move to the secondary filegroup. 

Possibly the ANSI 92 definition held at $URL$ gives the answer. Section 3.1 Definitions seems to draw a distinction between the idea of a result set and a result sequence. The distinction was not in my awareness when I wrote the question using the former phrase 

I recently got my first taste of the error "Heterogeneous queries require the ANSI_NULLS and ANSI_WARNINGS options to be set for the connection." which I hadn't come across. Nearly every resource I found on this site and elsewhere dealt with this occurring in a stored procedure. My specific scenario is a Vendor supplied app and DB, I can change the DB but not the app. I created a view on the DB to another server (via a linked server) but when I attempted to use this in the app I got the aforementioned error. I used SQL Profiler to prove the app opens its connections with ANSI NULLs and WARNINGS OFF. I used SSMS to prove the link works with the default values there. But unlike an SP I can not set ANSI WARNINGS on a view. I set ANSI WARNINGS on the DB as a test, the app now worked, but I couldn't afford a full regression test of the app so this solution does not work. I also tried many different ways of implementing the linked server including created an ODBC connection on the server and then using that. But it's like the app is explicitly setting ANSI WARNINGS to off. Not sure if matters but the source server is SQL2000 and the remote server was SQL2008R2. 

Where databaseName is somehow set at a server level. I understand synonyms are the wrong solution for this. Is there something similar we can use that doesn't include writing a synonym for every single object? 

It is acceptable for the last person aspect of the answer to be fuzzy or loose. i.e. as long as it's someone who went near it sometime vaguely recently that is "OK". 

As I said in the question a lot of resources dealt with this issue but not when it pertained to a view. The solution I found was to 1- create a 2nd DB on my source server. 2- create a view on the 2nd db using the linked server to the remote DB 3- alter the view on the 1st DB to use the view on the 2nd DB 4- alter the 2nd DB to use ANSI WARNINGS ON at a DB level 

This should modify your execution plan. If it doesn't help could you post your plan for both this query and your original query (minus the Option line). Actually, eitherway I'd be intrigued to see this data. 

If you wanted them randonly side by side you could use an adhoc ID value. If there are different numbers in each table then some will appear at the end without a partner. 

Or you could compare the length of data against the length of data when '"' is replaced with an empty string (''). 

So the best result I've managed to come up with is adding the following improvements - optimising the queried table (creating a new temporary table with suggested indexes) - adding a path element to check I'm not revisiting an existing part of the path - adding a depth counter as a limiter. This does however mean I'm consciously choosing not to have the full result set 

Shortly our development team will be moving an existing SQL Server 2008R2 instance into our Production environment. Ahead of this I have been asked to review all the code on it for anything malacious (e.g. wiping their own order history from the sales database). This has come out of the blue and I'm not sure it's particularly physically feasible but I'm willing to give it a good go, at least from the SQL side of things. The question is, where can code exist? My draft list is 

Whilst reviewing code on a SQL 2008R2 Instance I inspected the Date Last Modified of the System Stored Procedures of the databases. The ones in the DBO schema had a datetime that predates the installation however the ones in the dbo schema come later than the installation date. Specifically the machine was built December 2013 and the date of these SPs is 09/07/2014 17:16 (both for Date Last Modified and Create Date). The DB in question was also created December 2013. Now I assume this was something benign, but how can I understand and determine what caused this date stamp? I've checked the Windows logs but can't find any installations going on at that time. 

I am running an old production application which has a backend DB hosted on SQL2008R2 in Compatibility Mode 80 (SQL2000). When I run bits of TSQL to view actual execution plans SSMS is giving me suggested indexes which are only legal for SQL2005+ e.g. ones with included columns. How can I prevent it from doing this? I would like to view sensible indexes it might suggest. NB: The Compatibility Mode can't be changed. Yes we are replacing the application, this will remove the need for the DB entirely but not for some time. NB: Already tried, SSMS is configured "Script for server version" as SQL Server 2000. 

No value/reference is made to the ID column. On the underlying PRE_CASE the column ID is an integer primary key (PK, int, not null) with a valid identity seed. Direct inserts onto PRE_CASE work, but not ones onto the new view and trigger. What have I overlooked? (My end solution will be deployed onto SQL2000, SQL2005 and SQL2008R2 boxes, but I'm testing/developing on SQL2000) Edit: I created this model to test in a more isolated way. Turns out the error occurs on SQL2000 and not SQL2008R2, personally I'm giving up at this point. 

The boss has asked for a list of DB names along with size and name of last person to use that DB. I've found resources for when a DB was last accessed but for by whom. How would I solve this? SQL2008R2 being used and so far I have this: 

This is my current answer in my environment. This works for me as the greater script this is a part of runs on a daily basis. It does feel like a better answer or explanation should be available. 

There is a further method that will give output in a single semi-merged result set. First open Registered Servers and create a new group under Local Server Groups then register your server once for each DB, in each case setting the default DB to the one desired. Once complete right click on your group and select New Query. The query window that opens will have "multiple" where you would normally see a server name on the status bar. Any queries run in this window will operate on each registered server that was in the group. The first column of the results will be the name of the registered server. The result set will be fragmented by that first column and order by wills only operate within that fragment. Very powerful but overlooked piece of functionality for when you routinely have to run the same SQL on multiple servers. 

I'd have a table sit between users and messages with a one-to-one many relationship to both. Call it users_messages for example. Each row in users_messages would describe all the users who have an involvement in any particular message. For example if a message had 3 users involved then the table would have 3 rows for this. Each row linking to the same message but linking to a seperate user. When a row in users_message is deleted/deactivate you could check if any active entries for that message still exist and if not delete that message. Edit: Missed the 2-user requirement but this is still valid and more future proof. 

You could join the label table multiple times under different aliases specifying in the where clause that entries for the 2nd join be not equal to the 1st. Entries for the 3rd be not equal to the 1st or 2nd etc. e.g. Say we have a table of people 

To understand this error you need to understand that the SELECT is actually being parsed last. At the time WHERE is being parsed the SELECT statement hasn't been run and so valid_license has not been set up as an alias/computation. Discussed more here: $URL$ or google Logical Query Processing 

1) Why is there a difference? 2) Does it matter? Environment: Server SQL2000 SSMS 2008, Script for server version set to SQL Server 2000 Production destinations SQL2000, SQL2008, SQL2008R2 

My scenario is I look after an old system with a VB6 based front end and a SQL back end (good news, the replacement is being developed but in the meantime I need to keep this up). Now the application is used at a dozen or so sites most of which are on SQL2000, one is on SQL2008 and one on SQL2008R2 (all Standard). Changing versions/editions would be a big ask. The one hosted on SQL2008R2, as it happens, is also the largest site. We now have a table with over two billion rows and 700GB of data. It's a small number of columns, one being a 256 char varchar used to store serialised blob data. This is now of a size that when people try to insert, delete and update to it (via the app) users occasionally lock each other out. This can only get worse as time goes on. Also however, the next largest is on SQL2000.. I need to get the size of this table down. What are some methods or techniques I could use to achieve this? Ideally the technique would be compliant with SQL2000 Standard edition so I could use it at all sites. However just fixing the biggest offender on SQL2008R2 Standard would be something. The table does have an identity column and as the table definition is controlled by the app this can not be changed (I mention this as I believe this rules out a partitioned view- the rows are INSERTed, UPDATEd and DELETEd by the app). My current thought is to delete all data past date X months in the past (50% of total) and hive it off to a table not exposed to the user or the app. Then in the app I can put a method for a user to request items of data to be copied back across and this would be done via a scheduled job (overnight possibly). This feels clunky to me however.