In the above code, filegroup [DATA_1995] would retain the values beyond the last partition boundary. So, if I elect to add new filegroups and split the function for additional boundaries in this fashion... 

Have you opened the window in the Normal mode (instead of Dialog)? Have you tried explicitly changing the Height and Width properties of the form in the OnLoad event? You're correct about the selecting the main window and maximizing it. You could create an AutoExec macro that will select that window and then issue Maximize. 

I'm not familiar with this particular error but I've encountered situations when a two-node cluster had multiple failovers due to MPIO issues with the SAN LUNs. More often than not, it was resolved by updating HBA drivers. One other thing to look for is to ensure that the disk dependencies are properly set. The SQL Server service should be dependent on all that disks that are hosting the DB files and the backups as well as the disk with the drive letter acting as the mountpoint host. I've run into a few hosts where a missing disk dependency caused a disk to go offline before SQL could close out the DB files. 

During debugging, the OLEDB connection was created from scratch using the SQL Server Data Tools GUI. The notable difference in the connection string was the the addition of a space to have parameter now read . Connections to the secondary are now successful. It appears that the OLEDB driver accepts both variations but only the one with the space is implemented properly with SSIS packages. Is this an anomaly with SSIS or with the SQL Server Native client/OLEDB driver? 

I have a table with a PK based on a identity and will have a clustered index on a column. I've created a partition function based on calendar quarter and was planning on two partition schemes (one each for clustered and non-clustered indexes) to able to spread out I/O. However, if I plan to implement a sliding window to age out older data, do I need to keep all indexes in the same partition scheme or it doesn't matter as long as the partition function is the same and using the same key column? 

The use of Save Transaction provides you with a mechanism for rolling back portions of a transaction. For example SP A starts a transaction which then calls SP B. At the start of SP B, a Save Transaction Start Processing could be created. If an error then occurs in SP B you could just rollback the change in the SP B allowing the changes in SP A to be committed. 

I don't know of a specific product called "Amstrong Database" but I have heard of Armstrong's Axioms which are a set of axioms relating to the functional dependencies of relational databases. The axioms have been around a long time (mid 1970's). There is an IBM Research Paper available. I would think a Computer Science or Maths Department in your local University would be able to help. 

Take a user created Stored Procedure, MySP and as part of the SQL Script to create the SP add a SQL Statement to Grant execute permissions on the SP. 

In order to get the desired result you need to join your table to itself and then ignore those rows where c3 is null. The SQL below produces the required result. I'm not a PostgreSQL person so there may be a better way to do this. 

If you have users who are updating/insert records then I am sure they are also deleting records most likely with no or a very limited audit trail. If this was my database I would 

Leave the SQL 2014 Database running in Compatibility Level 120 and add a dbcc Trace Flag to force SQL Server 2014 to use the prior CE <-- I would not recommend this Leave the SQL 2014 Database running in Compatibility Level 120 and Tempoarily add OPTION (QUERYTRACEON 9481) 

SQL SERVER SCOPE_IDENTITY/IDENT_CURRENT/@@IDENTITY. While SCOPE_IDENTITY/IDENT_CURRENT/@@IDENTITY perform similar functionality the values returned from each differs depending on the current scope such as a stored procedure, trigger, batch each. The documentation should be read and understood before determining which one to utilise. CREATE TABLE Test( TestID INT IDENTITY(1,1), TestName VARCHAR(20) NOT NULL ) INSERT TEST VALUES('Test1','Test2','Test3'); SELECT @@IDENTITY,SCOPE_IDENTIY(); In this example both @@IDENTITY,SCOPE_IDENTIY() will return 3 as they are in the same scope. Depending on scope though they may return different values. SQL SERVER Output allows you to access all of the rows returned from an INSERT/DELETE/UPDATE or Merge statement. You may need to use a variable to access the identity. The links provide more comprehensive information. 

Can you confirm with a Profiler trace which credentials are attempting to execute sp_start_job? I'm curious if it's the credentials of the client using the browser or the underlying application pool. If the account in question has sysadmin, it should already have the permissions to execute jobs. 

It'll involve 1) removing the user from any database fixed roles like (only will remain), 2) creating a custom role in the DB, 3) granting the new role specific permissions on the one table, and 4) adding the user as a member of the new role. You'll also want to make sure has no grants to user objects as all users in a DB are a part of the role. 

All of the DBs were in read-only mode and I have validated full backups I can recover from (already restored to an alternate server and ran DBCC as part of the test). I tried running DBCC CHECKDB, toggling the DBs to the offline state, emergency state, detaching them and lastly a DROP DATABASE command but am blocked by the same error. The files are still on the NAS media with the last date modified equal to the time they were put in read-only mode weeks ago. I figure my next (and last) option would to be to schedule a downtime to restart SQL services to see if the read-only state protected the data files. If not, I figure I need to stop SQL, move the damaged files out of the way, restart SQL, drop the now-Suspect DBs and recover from backup. Is there another option I may have missed that will keep the instance online? We have a live DB on a local volume for the VM that we'd like to keep up but a short service restart isn't out of the question. 

I'm implementing PBM on a SQL Server 2014 instance and want to enforce a condition that all objects are created outside the schema and all object names are created in title-case (i.e., OrderDetail). I have the regular expression drafted but am not able to create a satisfactory object name with it. An example output is here. 

SQL syntax. This syntax creates a new table and insert the resulting rows from the query into the new table. 

As you are looking for the Maximum value for Sequence for each Booking_ID you could write some SQL similar to that below: 

In general there two different ways to obtain an Identity value after performing an insert, using the SQL SERVER SCOPE_IDENTITY/IDENT_CURRENT/@@IDENTITY or SQL SERVER OUTPUT. 

By opening SQL Enterprise Manager, locating the database in which the SP resides and viewing the SP under Programmability. Use sp_helptext 'MySP' which will return SQL Servers stored version of the SP 

From reading the YQL Documentation and viewing the data sources it appears that while the yahoo.finance.quote does provide data on all the stock data, running a query on a large data set make take some time and the data may be returned in multiple pages which means that your application will have to handle the pages. It also appears that the YQL data requires a where clause in order to run so you would need to know the symbols for all of the stocks. There is a YQL Console which allows you to access the YQL data and run queries, view the output etc. I would suggest reading the YQL documentation and running some sample queries in the YQL Console to see if the data you require is available. To access the YQL Console you can 

The Connection timeout is always set by client. The timeout is normally configured when the connection is created. In order to prevent recompilation of client code the timeout interval is often stored in a config file, registry entry etc You cannot configure the timeout on the database itself. 

When the SQL is run the SP is created and stored in SQL Server. Once the SP is created the Grant execute statement is executed. When the SP is executed only the contents of the SP are run. The Grant execute statement is NOT stored with the SP in SQL Sever. This can be verified in a number of ways 

I think this is a pitfall. I recall SQL 2005 and older versions requires active nodes to be updated. SQL 2008 and later versions allow passive node updates like the walkthrough you described. A posting from Linchi Shea explains it well. 

If you can define the T-SQL query to filter the rows to a specific date range, create a view and choose the view object within Excel instead of the source table. 

In the process of removing an usused disk resource from cluster, I would follow these steps: 1) remove dependencies on the disks by the SQL Server resource, 2) take the disk resource offline, 3) delete the disk resource from the SQL resource group, 4) delete the disk resource from the Available storage pool. I was proceeding to do this on a two node cluster with two SQL 2008 R2 named instances (running Win2K8R2 SP1 build 6701, 64-bit). On deleting a resource in step 3 above, SQL would go offline, all the disk resources would go back to the Available pool, and the SQL resources (Net Name, SQL Server, SQL Server Agent) would disappear. SQL services do restart OK after via the services console as all the disks are still on the same node. I ran from the command prompt and I see the SQL service resources are still there, in Available Storage, and offline. To bring the resources back to the appropriate group, can I move them via commands? 

I am using a COTS package to perform index rebuilds across my SQL Server portfolio. The package unfortunately doesn't have an option to set index fill factors globally for an individual instance or DB (I can set them when scheduling rebuilds on an index-by-index basis, but with 1,600+ indexes, I'd like to set them in advance in bulk). Outside of an ALTER INDEX command, is there a way I can redefine existing index fill factors? 

A data warehouse ETL process is querying a read-only secondary in an availability group. The ETL process queries a single table incrementally using datetime range criteria of a minute and read committed isolation level. At the time of execution, 5 records that meet the criteria are committed on the primary, but another 3, with slightly earlier timestamps than the first 5 (but within the criteria range) are still in open transactions. Does the nature of availability groups require all transactions to be applied in LSN order (delaying the visibility of all 8 records until all are committed) or do the delayed 3 records get later LSNs and are applied as soon as they are committed, potentially after the ETL process has adjusted its date criteria? 

If you do create a table using this syntax then you should be aware that Indexes, Constraints and Triggers are not added to the new table. 

Once you have encrypted the data using an AES key the data is encrypted (the point of using AES). The only way to change the key would be to decrypt the data with the old AES key and then re-encrypt the data using a new ASE key. Some good MySQL AES documentation. 

Which would output the row numbers. You could then use the SQL Modulo function, % and perform a %3 which would give you 1,2,0 repeating as the row output which gives you the grouping you want. 

if TableName == NULL then "0" is returned if TableName !==Null then TableName is returned You could rewrite the ternary expression as 

Create a SQL Server Account and use those credentials to connect to the SQL Server <-- this might be the easiest Trust the domain in which your GIS Programme is running <--I would be loath to do this 

The issue is that you are running your GIS programme from a domain that is not trusted by the domain in which SQL Server resides. That are numerous methods you can use to correct this. You should discuss with your network admin/sql server admin as to the best approach. 

Using the above example a new table, #tmpTable will be created with the data from coulmnA, columnB and columnC from tableA. The data types in #tmpTable will match the datatypes of tableA. You can of course create a true physical table and have multiple tables in your select statement. 

Allowing even trusted users to login to what appears to be a production database is in my mind a big no-no. The solutions you mention above may provide you with a sense of security/relief but they their own have side effects. I would be particularly worried about your statement 

to the offending SP to force just that SP to use the prior CE. The SP should now perform in Compatibility Level 120 as it did with Compatibility Level 110. <-- I prefer this approach as your Database/System gains all of the other advantages of SQL 2014 while the "fix" only impacts the offending SP. Irrespective of the approach I would then profile the execution plan/read/writes etc of the offending SP on a test system in SQL Server 2014 Compatibility Level 120 and rewrite the SP to cater for the new CE. Once tested and verified I would then apply to production. Microsoft have a good article on the Cardinality Estimator changes in SQL 2014