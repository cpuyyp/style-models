Here is a script I have used on Oracle 9i and 11g that grants more than I've ever needed. Further pruning of the permissions on a production system would be a good exercise in security. Edit @ miracle173 asks what is the difference between the script below and just granting DBA privilege? The most important difference is that by granting individual privileges you can remove what is unnecessary. If you grant a DBA role then you cannot pick and choose unless you edit the role which is not advisable. The requirements change from when you are developing a database where you could need everything to a database that has been deployed to production where you want to have the least privileges necessary. Best practice is exactly what Leigh Riffel answered: do what you need to do with the least privileges. Edit @miracle173 correctly points out that SYSTEM tablespace is not recommended for a user. I have changed it to USERS in the script and on my primary development database. Edit @miracle173 also makes some good points on the distinction in privileges between a DBA who maintains and a developer. I do DBA and application development for the databases I work on so you could break down the script according to what you are doing. Practices and standards vary by industry, organization and habit so what you need to work with can change from instance to instance. It's common in my shop for the development database to have more privileges for admins and developers than production. 

Customers come and go. Sometimes they have the right to take their data with them. To extract a subset of data from one schema where the data could be anywhere and the only marker might be a tenant number is a lot harder than putting each customer's data in one schema. One solution I have seen used is to have a master schema with the metadata concerning the unit of work that is relevant to your application. So if your application sells something you have tables that list the types of sales, types of sales terms, lists of provinces, states, countries. Then in each customers schema you have child tables which can extend the parent types with additional columns. These tables have a primary key which is foreign keyed to the master tables. If your ORM solution supports inheritance this can work quite well. Reporting gets a bit tedious as you have additional joins to the master tables. 

Although this may look like table churn it allows you to calculate the price and discount that was applied today and in the past. You can change your discount every day by inserting a new row in ITEM_DISCOUNT 

Edit: @Justin Cave Good point. The query starts out using bind variables and to store it in a table for reuse will product a static query. I guess I want the best of both worlds. I have a few users for this application who do a lot of querying so it would be a nice feature to save their input. Maybe I should save the the bind variables. Edit @Justin Cave If you were to suggest that I hold the user parameters in a collection I would accept that as an answer 

Now that virtualization is common and accessible there are no factors that I can think of that would force a business to put all their databases on one server. I see cases where there is more than one Oracle database in use on the same server but if you are talking Oracle and MSSQL and MySQl all on one server this constitutes a single point of failure. 

(available from 9i but improved for 10g and 11) If you have packages that are not valid and will not be valid, for whatever reason, you can use this package from Martin Mares which will recursively compile all invalid objects except the ones you specify. As far as best practice goes I try to limit the dependency chain. It's always a toss up between "Don't repeat yourself" which encourages dependencies and having stand alone packages. My rule of thumb is that if the package/trigger/object interacts with another database try to reduce dependencies so if there is a issue you have less places to look for sources of the problem. 

The Action is admittedly crude but versatile. The requirements were to funnel changes from multiple tables to another database where further processing was required without touching the application code. A typical call is to a package from a trigger, other procedure or job. 

of course this requires privileges too, unless you have the CATALOG role, if you select from this table you can alter a user 

This method needs downtime. A weekend should be more than adequate and only costs you a new server, Oracle 12 licence and some VMWare software licenses. Keep in mind that testing and verification can take more time than importing the data. 

It's not clear from your question what brand of database you are connecting to. I will assume it's Oracle. You need this information: 

It appears that you don't need to worry too much. Install XE first. You don't get to choose the database name (XE) or the port (1521). Next, install 11g to the default app folder. Modify the TNSNAMES.ora files for both databases so they know about each other. Test connections and create database links and you are good to go. 

you need a package to read a long column. Sadly Oracle continues to use this datatype but does not recommend anyone else use it. 

Not that unusual for a best practice if done well. I see read only service accounts and more powerful accounts for web services to connect with 

When you look at the view the data type is Number with no precision for all of them. This appears to be a consequence of the UNION as just 

I think you are over complicating this. Grants and roles govern access by users not what table space and data files where the data is located in. Yes, backup, recover and export are much easier if schemas have their own table space but that doesn't seem to be your question. 

If the databases are on the same network and are not too many versions out this should work: You have to use the export utility from your older database to get the data out of the newer database. (this might not work when trying to use an oracle 8i exp on an Oracle 11g database) Add the information about the new database to the TNSNAMES.ora file on the older database Run the exp utility from the command line or script on your old database 

There is no in-place upgrade possible but the existing exp utility is still supported for importing dmp files. Some of the issues you need to research are: 

I have a number of tables which hold metadata about the tables and packages in the system. This is intended to be a resource for analysts and documentation so it is not used to create data. One of the columns that appears in a few of these tables shows the user or "Owner" of the object. The data should only contain user accounts in the database and it seems like a good practice to have a foreign key constraint from the metadata tables to sys.user$. However I have always approached SYS tables with caution and I was surprised to see that there is no primary key for SYS.USER$ only a unique index on the NAME column. As far as I know I cannot create a foreign key on sys.all_users. Or I could create a table as Select username, user_id from all_users so the data entry is checked on insert but it all seems very clumsy. Is there a better way to access a table that has a primary key and the names of all schemas? I ended up adding a check constraint as a hard coded solution that won't mess with system files. I still think there should be way to get a list of the schemas or "owners" that can be used as a foreign key constraint. @Phil, why don't you post your comment as an answer? 

This question is more complicated than how SGA relates to cpu utilization. Many other factors can cause a high CPU usage even if there is adequate SGA. Start looking at the other Oracle diagnostics for the usage of the shared pool, large pool and buffer. Even more important you need to investigate the application front end, number of sessions, type of data operations your users are up to... What kind of SQL is getting fed to the database, does it use bind variables, does it require full table scans? Oracle tuning is more successful when you look at the whole picture. Tell us more... 

Oracle can connect to MySql with an ODBC connector. This is described in some detail here. Also see the official Oracle documentation here. This will work for Oracle 10 and 11g to MySQL 4.1, 5.0, 5.1 or 6.0. There are limitations both in performance and what you can do. My understanding is 

With this structure the ITEM_DISCOUNT table is a poor mans version of a temporal table as it stores all the discounts that ever applied to an item and the time frame that they were applicable in. Instead of using a trigger put your business logic of selecting which discount to apply in a package To calculate the price for a sale you would use a package 

The trick is get the packages and dependent code recompiled in one go. If you are refreshing a development database with new data you will have an enormous list of invalid procedures. As well, with dependent code you find yourself recompiling over and over. You can use this command located in $ORACLE_HOME/rdbms/admin/utlrcmp.sql: 

Or when using the sqlplus interface EXEC DISABLE_TRIGGERS; I have to hope that this is not production code as a procedure that disables all the triggers could have many consequences. This code as originally posted should be more comprehensive if used in production, particularly for a migration. Triggers do things, whether you agree with their use or not, they can enforce business logic and stitch together poorly implemented business logic. 

One solution is to have both databases using the same character set. Considering your destination is a free XE version why not do a new install? Just make sure you use EL8MSWIN1253 character set and the same UTF version. 

Based on Phil's test case I backed up the the production database using exp. I dropped the editioned user, recreated them and imported the data with no issues. The user had views and packages which were versioned but as long as you set the edition to the most current one you are good to go. This query showed that the recreated user was not using editioning 

"Safe" is relative and involves a tradeoff between access and privileges and application and data security. The factors that you would night want to consider are: 

Sounds like a data pump using a network link would do the job. It's always preferable to run it during quieter hours but if your source database is not under powered then you might be able to run it during regular hours without your user's noticing any difference in performance. From the article 

I work with a database that has this solution that I outline below. I don't like it because the base table has an entry for each type and it ends up being a huge table that is slow to query. --base table 

This is for Oracle Grid Control 11g OEM. The listener is considered to be a target for monitoring. Go to the Hosts tab, select a server and click on the Targets tab. This will normally list all the database instances, the listeners and the OEM agent. The status is indicated by the Availability column. You can configure OEM to send you an email if the listener is down under the custom alerts configuration. 

Oracle has had a long standing limit on table names of 30 characters. I suspect this is a legacy issue based off an original 16 bit environment. The length of a table name could have some minuscule effect on performance as all the names have to be stored in a data dictionary and also parsed for queries but I don't think you could measure the hit. A more important effect of short table names is that it's hard to work with. I too have to maintain an enterprise database schema with short names. There is no good reason to have short table names. Ease of maintenance trumps obfuscation or old DOS habits every time. 

Looks like there is already a job on the imported database with that job number. Just recreate the job on the import database. The job looks to be a queue so you could, depending on your business procedures, probably run the new job to test that everything is working. 

I have to extend a .Net 4/C# case management application that uses Oracle 11g to handle legal cases. I have minimal budget, my experience with Oracle and a very junior C# developer to accomplish this. The additional module will be used by no more than 20 people on an intranet. The existing rules are generally like this: