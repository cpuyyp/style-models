* I'm not an ORACLE Dev, but the function should look something like the one above... Once you create the function, you will create a functional index that will use the function to look for the rows you need: 

Just for fun, and the sake of your question, if are using MySQL, this would be a stupid hack to check if the hourly file has been received on the server: 

Why not recreating the primary key as (nid, vid), and creating a new index on the column "order" (just for fast retrieval / ORDER BY clause). --> The worst that can happen is to have 2 ingredients with the same "order" value, but the order-change-logic should be already correctly defined in the application. 

This can and will work (theoretically :) ...), but you will need a recursive function (in application or in the database) that can return the parent group id (ex. B-1), or the top group id (ex. A-1) for any group id value given as an input parameter (C-1). But ... like I said: The more info you give, the better responses you get. Hope that helps. 

Replication was made for small changes, so you're on the right track. To keep the systems in sync, at the database level, you will need to setup a mysql master-master replication scenario. I've used the tutorial from this linke ($URL$ to setup such configuration. Depending on how much data is changed, you can choose between [statement] or [row-based] replication. More info about the differences between the 2 types: $URL$ 

In your first query, both valid values + null values are returned from the database, but because you use the "limit 50", and because NULLs are displayed last, you don't see the rows containing NULL Price. The correct for for the first query should be: 

However, the procedure raises the error and does not get trapped by the error handler at the bottom. Questions 

Background I am working on fixing a large query that I inherited. The query consists of over 200 lines of DML statements. The search queries the database restricting the data to a single month. Most months work just fine, but there are a few months that generate an error. I suspect the problem is one of the following: 

Are there any methods or database variables that I can set to help isolate this error? Why isn't the error being trapped by the error handler? 

I am trying to alias some columns that are being used to interface with a 3rd party data aggregation system. This company has stated that they might change their naming scheme in the near future, so I preemptively created constants to be used as column aliases in my PL/SQL package. However, it doesn't seem like constants can be used to alias columns. After quite a bit of searching I found that variables can't be used to alias column names dynamically (there are lots of questions about dynamic aliasing), but I wasn't able to find anything stating whether or not they can be aliased using constants. My attempt to do so failed and Google search results returned negative, so I assume the answer is no. If the example below isn't feasible are there any alternative ways to alias columns, so that they only need to be changed in one location? Example PL/SQL Code 

This makes it really difficult to make changes that are still compatible with the application that is using these PL/SQL packages. I read that by using the error can be avoided by: 

Because both of them are virtual servers (you didn't mention the technology used (ESX/KVM...), you can try this: 

In the first use case, you (or the application) do a select to retrieve some metadata about a file; In the second one, you enter an ID (select query) and the application does the archiving (insert statement into table_b); What exactly do you need to automate here ? 

FRA will be purged automatically when free space is needed Put the archived logs in FRA as well Use RMAN to make the backups and use RMAN commands (REPORT OBSOLETE / DELETE OBSOLETE) to manage the purging of backup pieces + archived logs from FRA 

you can partition the table by certain criteria. Because your using the table for a "weather station", and your data is time series values, your best option would be to partition by [station_id] then by [created] Create a Table_A_Archive, where you can move data that would be to old to keep in Table_A. How long do you intend to keep data in Table_A ? would it make sense to delete old rows that become obsolete for you application 

Your free space in the current innodb file (ibdata[1] ?) is decreasing. It doesn't necessarily mean that your website is slow. Maybe the number of users (concurrent users) has increased in the last few days ? (also causing lots of inserts int he DB?) There is no error in the print screen. it's just your monitoring system, reporting a value that have reached the critical threshold. 

Also, I believe that the application that's using your database already knows what those values mean. So what is the real use case for your constants ? If you really want to go with your approach, without changing the existing column type, you can create a function that will return the STRING value for each integer code: 

So I know it's not a proper shutdown of the database service that is occurring. I am logging to syslog on a FreeBSD system. The log is very basic and essentially only shows me shutdown and startup attempts. 

No differences. Eventually the memory usage will cap and will kill the database backup (among other processes). I've also tried tuning some system variables. There was Managing Kernel Resources and also Resource Consumption. These have affected the running postgres processes that are used up by the server, but it had no impact on the process uses. It still rose to 2.2 GB before FreeBSD started killing processes. I've read through the pg_dump manual and haven't really seen anything that will help besides the multiple jobs. At this point I'm not really sure what to do. Is there a way to cap the resources that is allowed to use? I don't mind if the backup is slower. Just don't run the system out of resources where the OS has to start killing processes. 

As the backup is being completed the associated postgres process keeps rising in memory usage. It gets to the point of 2.2 GB for that process alone. At this point memory is all but used up and starts killing processes. I've tried to change how the backup is done. For example, I tried using multiple jobs (and thus changed the format to directory). 

I am currently using default logging options. What do I need to change in order for PostgreSQL to log errors? Requested Information /etc/hosts: 

I'm running a PostgreSQL database and from what I read, the database is unlimited in size. Is there any material I can read to understand what occurs when the HDD fills to capacity? What sort of monitoring must be done in order to insure continuous operation? One solution would be to create an arbitrary limit of free space and once that free space has been reached, you would force the user to purge off older data. Is that a proper solution? I'm interested in learning what other solutions exist and how databases are properly maintained. 

I was looking at a tutorial on triggers for SQLite. The tutorial said that a use of triggers could be for data validation. The following code was given as an example: 

I need to make packages for Oracle 10g that can be changed while users are still logged in an using an application that makes database calls through PL/SQL. Unfortunately, anytime a package is updated while users are using it they get the following error: 

I did a little bit more research and found the answer to the problem. Even though the first parameter for is listed as in the 10g documentation for DBMS_OUTPUT. The actual name for the parameter is . You need to bind the name using . The actual parameter names for stored procedures can be discovered using the following query: 

a column defined as VARCHAR contains alpha characters but is being used where a number is required a column is null and is being used where a number is required 

If I can find the row that is causing the problem, I can change the query to handle said data appropriately. However, I don't know how I can isolate that row. I have attempted to break the query into parts and see where null values are being generated, but the entire query is dependent on a number of outer joins. I have been able to isolate the range to few days, but that doesn't allow me to isolate the row in question. Also, the query has been defined as a stored procedure. At the end of the procedure I have defined an error handler to catch all errors and display a message: 

The values , , and can be replaced with the values of the the stored procedure that you would like to look up.