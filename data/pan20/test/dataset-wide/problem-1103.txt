Databases will typically issue a checkpoint prior to dumping the data to ensure all commits are written to storage. The checkpoint can cause the database to truncate the transaction log, depending on database settings. According to Microsoft, the following conditions will automatically trigger log truncation: 

The transaction log can be dumped indedpendantly of the database using the BACKUP LOG command: Backing Up the Transaction Log (full and bulk-logged recovery models) 

Without an ORDER BY clause, the order the values are returned in will be unreliable, as they can be influenced by indexes and the data pages the individual rows are stored on. If you require the data to be in a particular order, use the ORDER BY clause. 

It's typically good practice to seperate your OS installation from your database installation to isolate OS disk issues from database disk issues. The primary reason for this is to reduce the change that a problem with the OS could prevent recovery of the database, or vice versa. If your database fills up the free disk space, it can crash both the database and the OS. This also allows you to have different recovery options available for the different pieces. We typically run RAID 1 (Mirroring) on our OS disks, and RAID 5 (or some other type of parity + striping) on our Database disks. Since the OS disk has fewer writes, we can take the performance hit of mirroring, to reduce recovery time in a crash situation. But on the Database side we want to maximize the performance of reads/writes at the expense of recovery time in a crash situation. 

You should probably create a .sql script that can run and return the value for multiple commands. A script can be easily created using the following format: 

In Advantage/SQLAnywhere, the database names are stored in sysdbspaces To access the name of the database you are currently in use the function. 

Sybase ASE retrieves port numbers for the listeners from the file. On *nix systems, this is typically in the Sybase Home directory ($SYBASE/interfaces) On Windows systems it's called and can be found in %SYBASE%, which is likely to be c:\sybase (I think..been a while since I've run on Windows). I believe you can launch the GUI interfaces editor through Sybase Central, if you are using that to manage your ASE databases. and are the two utilities that can be used to edit the interfaces file. You can also manually edit the interfaces file using a text editor such as but be sure to pay attention to the format of the entries and permissions, so as not to accidentally muck it up. 

Sybase ASE will not allow you to directly delete rows from multiple tables using a wild card, but it's pretty simple to create a script to find the table names, and loop through them. You can find the tables names by querying within the database. To find all the tables with the prefix you would do: 

It seems that the 5GB limit is per server, so you would not be able to have multiple 5GB databases. As for migration, it should be just a matter of shutting down the Express Edition, and using the Developer Edition binaries to start your existing data server. It may be as easy as copying then RUN_[servername] file from the old directory to the new one, if all of your system and user databases exist outside of the installation directory. The link above is worth checking out, as it details the restrictions of the different versions. It's also a great resource for ASE knowledge, as Rob Verschoor is one of the most knowledgable Sybase/SAP database guys around. 

If you execute this from and using the flag to supress headers, and flag to redirect output to a file, it will build a script that can then be executed directly. 

Sybase ASE does not support a direct upgrade from version 11.5 to version 15, so you will have to export your database from the ASE 11.5, and import it to ASE 15. From the $SYBASE/$SYBASE_ASE/scripts/ directory in your ASE 15 installation, run the installupgrade file into your ASE 11.5 installation, to install some utility stored procedures to assist with the migration. 

Now login to each database, and run the sp_checkreswords command to make sure none of your DDL is using words reserved for the system. If reserved words are found, you will need to alter the code, and remove/change the reserved words. Use the ddlgen utility to export your full database structure (Tables, indexes, stored procedures, defaults, user defined datatypes, etc.) At this time, you may want to review the code generated by ddlgen, to make sure it makes sense in your new environment. You may want to make changes to device locations, device names and device sizes. For example, ASE versions prior to 12.5 had a device size max of 2 Gb, so if your database was 10 Gb, you were forced to have 5 devices. ASE 15 will allow device sizes of up to 1 Tb (on a server with a 2k page size), so you would only need one device to store the entire database. You may also want to seperate the index and trigger creation scripts from the rest of the DDL, to allow you to run those independantly. This comes in handy when you import data into the new server. Once you are satisfied with the scripts, you can run them against the new server, and it will create all of your database structure on the new server. At this point it's a matter of transitioning the database, using the bcp utility, you will need to export your data on a Table by Table basis. Depending on the size of your databases, and the number of tables in each database, this can take a while. I would recommend trying the -n (native) version of bcp instead of the -c (character) if it's possible. To import the data into your new database, you will also want to use the bcp utility, but you will likely want to make sure that your databases do not have triggers or indexes as this will enable the 'unlogged' version of bcp to run, allowing it to run faster and not fill the transaction log as it runs. If you can not run the unlogged bcp then you will want to make sure you have the appropriate database options ('truncate log on checkpoint', 'abort tran on log full') and set the batch size for your import to a reasonable value. At this point, you can create in indexes and triggers (if they haven't been created up to this point), and start testing the server. I would also recommend reviewing Sybase's Migration Guide, as it will likely go more in depth, and cover things I may have forgotten/skipped over. A lot of the details will depend on the migration environement, so planning and preparation are key. 

I believe the basic issue is that the query is doing multiple heavy table scans due to no index support. Try adding the following indexes: CREATE NONCLUSTERED INDEX TEST ON STOCKDEBUGTRIGGERED (ChangeDate) CREATE NONCLUSTERED INDEX TEST ON STOCKDEBUG(ProductID, StockOld, StockNew) There may be further tweaks, so please post the stats and execution plan with these indexes added. 

From your comment update, it sounds like Merge replication could work well for you, mostly due to the minimal updates. It's a lot simpler to set up and manage than two-way transactional (which is what the scary link I mentioned deals with--I should have left that for later). In a nutshell: $URL$ 

You can't use a SP in a default, but you can use a function. But presumably your SP code has to increment the value in the NextNumber table, so that won't work for you. Your best bet for doing this within SQL is probably to have an INSERT trigger on MyTable which calls the SP and sets MyColumn. However you'll have to carefully consider the behaviour of your SP code when multiple concurrent inserts are involved. For example, you'll probably need to select the current value from the NextNumber table using an UPDLOCK hint to prevent another user reading and using the same value. (Note that the UPDLOCK hint will only prevent reads in this way if it's within a transaction [which triggers run within in by default], and if other selects on the NextNumber table use UPDLOCK as well.) 

Since the three operations ended within 3 miliseconds of each other, it looks to me like the first two simply were able to wrap up and report their completion just that little bit faster than the third. (Note the End_Time is the end time of the operation, not the time the operation was logged, which would be slightly later.) Note row E is the only stored proc call. It's possible there's slightly more overhead in wrapping up such a call. 

Provided you're using bulk delete/update where appropriate, and have indexes to support such operations, I don't think you'll run into trouble due to the PK standard you use. It's possible that if you later have EF generate queries with joins etc, that they won't be as efficient as they would be with a natural key based repository, but I don't know enough about that area to say for sure either way. 

Create a login that has only Public and DBcreator Server roles. Map the new login to user DBO in all existing databases. 

It's not clear to me how your first paragraph relates to the second, but I can answer the following bit: 

Assuming that: - You have the looping and sleep code in the CLR code - You know how to call said code from SQL agent (a stored procedure wrapper if nothing else) - Your process is ok with the occasional sub-minute downtime - Your process will not lose any data if it fails part way through (i.e. the next run will process the data the failed run was trying to process) then I would propose a SQL agent job that is sceduled for every minute, and the job step itself have say, 10 retries set (advanced page) so that it will usually restart itself immediately if it fails. Why just 10? Because if it starts consistently failing, you will want to get notification quickly (via agent job failure notification settings). That will be an email a minute. Of course, if it fails 10 times sporadically, you'll get a false alarm notification, but you can quickly see from the job history whether it's failing consistently or just sporadically. 

A comparison of the values in the inserted and deleted tables is your only option, because UPDATE() and COLUMNS_UPDATED will be true even if the value hasn't actually changed (i.e. has simply been overwritten with the previous value). To (largely) future-proof your trigger you could dump inserted and deleted into temp tables, and generate dynamic SQL for the comparison based on the temp table structures (in conjunction with Update()) which you can obtain with this: 

But if you can instead wrap steps 1 and 2 in a stored procedure or ad hoc query that handles the retry logic without duplication, then your solution will be much more understandable and maintenance-safe. 

This may be a file permissions/uac issue. To prove/disprove that, create a new folder and copy (not move) the excel file into it, then give the windows security group "Everyone" Full control on the folder and its files. Update your export query with the new file location and try running it again. If that works (or if you at least get a different error), then bear in mind that wherever you want to have the export file in the end, the file will need read/write permission assigned for the user account your SQL Server service runs under. (Or the "Everyone" group, but that wouldn't be best security practice.) 

$URL$ shows SQL 2016 Dev as supported on Win10 Home, and I don't know of any explicit feature limitations--though you are right in general that you're limited to what the OS supports. Have you tried to set up replication using the built-in service logins? I believe the instructions around adding Windows logins are for security best practice, which may not be an issue for a dev environment. 

Since both servers show similar reads and similar CPU "seconds", but the new server example shows that the duration was the same as the overall CPU time, the spikes you're seeing are likely due to less-parallel execution plans sometimes being generated--perhaps simply due to the CPUs being unusually busy at those times. Note that the CPU "time" shown is the total CPU milliseconds used over possibly multiple cores--which would have to be the case in your old server example where the CPU time was 6 sec but the actual duration was 1.5 sec. 

You're right to consider both Avg_fragmentation_in_percent and Avg_page_space_used_in_percent when considering whether to do reorganise, rebuild, or nothing. Even if Avg_fragmentation_in_percent is low, a low Avg_page_space_used_in_percent may gain benefit from reorganisation (due to the extra IO and cache resources used by unfilled pages). And the "greater than 30% = rebuild" advise may be better read as "if Avg_fragmentation_in_percent > 30% and you have been regularly reorganising, then rebuild". If you haven't been reorganising, then try that first, since as your test shows, that may be all that's needed even with very high Avg_fragmentation_in_percent. As for the exact trigger levels to use, that really depends on your data and how it's used, and on various other factors like those jesijesi posted.