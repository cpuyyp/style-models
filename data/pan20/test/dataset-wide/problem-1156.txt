You could "optimize" the above with making only those indexes unusable, that have extents in the second datafile by checking for different values, though I would not put too much effort in that, because that is very unlikely for a tablespace having multiple datafiles to have extents only in 1 datafile + the total size of your indexes is only 23 GB at most, even if you rebuild all of them, it should not take long. 

You need to create an external job for that. Techniques for External Jobs First, you need to create a credential, with the OS user and password in whose name the job will run: 

SID identifies the instance, not the user. Use for SID - based on your output. This is not a solution, it is a cheap dirty hack: 

Get the data, order by . Compute a temporary column: if equals to in the previous row (rows ordered by ), let that column be , otherwise . This column is used for the following: when is the same as in the previous row, we increase the value in by , otherwise we increase the value by . Finally, get , , and the of the temporary column up to the current row (this requires the analytic version of ), this will give us . The above translated to SQL: 

is the default block size of the database, but it does not necessarily mean all your tablespaces have the same block size, you can have tablespaces with different block sizes. You need to define seperate cache areas for tablespaces with non-default block size. For example, if your is , and you want to create a tablespace with block size of , first you need to specify a size for the "16K" cache with the parameter , and after that you can create a tablespace with block size. 

Now if you don't insist on naming the columns LIST_47, LIST_91, ..., the version peforms worse, but it can be easily reused, by changing the 4 parameters, you can even use bind variables there. The performs better, but if you want to use different values, you need to rewrite the query, because you can not dynamically provide the values for the . needs to know the column names it generates at parse time. 

Exadata uses or redundancy diskgroups. It is possible to use redundancy diskgroups as well, but that is not supported. Even if you drop a griddisk, and one of the ASM failgroups becomes incomplete, you can still mount the ASM diskgroup and use the mirror side(s). But this will not happen automatically, because the usual command throws an error in such a case, and will not start the database because of that. You should be able to mount the diskgroup manually with the option. Log in to the ASM instance as : 

If they still remain there, just simply delete them at OS level. And no, there is no such command thats deletes datafiles from the database that the database does not know of. Reference: Drop Tablespace Including Contents And Datafiles The Datafiles Are Not Automatically Deleted (Doc ID 389467.1) 

Still, if you configured everything properly, you may have hit: BUG 19383839 - UNIFIED AUDIT - NO LOGON OR FAILED LOGON ACTION CAPTURED Edit: Ok, so I had to test it myself, else I would not have believed it. Opposing to what the documentation states (AUDIT_TRAIL), setting the to does have an effect even when using pure Unified Auditing. But this is not intended, it is a bug. The fix is not included in the latest PSU (12.1.0.2.170117), but installing the above one-off patch (19383839) indeed resolved the issue. The problem is, this patch is not available to Windows platform. (I have tested this both on Linux and Windows, because Windows is always a factor you need to consider when working with Oracle.) Setting to does not reenable mixed-mode, relinking (renaming the DLL) enables pure Unified Auditing and that overrides this, you can confirm this by selecting from or . So I suggest that you set to . 

If the standby site becomes unreachable, the primary site will not delete archivelogs as they will be still needed by the standby site. If you try to backup and delete these archivelogs, you will receive an error, and eventually your archive area will become full and your database will stall. Setting to lifts this "restriction". If the stanbdy database becomes unreachable, you can manually set to , with that, you disable that destination, and the primary site will be able to delete the archivelogs. An other way to work around this is to write your backup scripts in a way that is prepared to handle this situation. By adding the option to a operation, RMAN skips the above check and deletes the archivelogs specified regardless of the state of the standby database. For example adding this line to the end of your backup script: 

You do not even need the database to register itself, as you have static registration defined - also in listener.ora 

Columns referenced by a foreign key constraint should have a PK or unique constraint in the referenced table. This is not true for , , . To be honest, those FK constraints are unnecessary, remove them, and keep the FK constraint on . Also: 

The naming of archivelog files also confirms this, as archivelogs in the FRA are use OMF format, and they are actually placed in seperate directories (named based on the date) each day. So instead of this: 

Do not use for querying the current SGA size. in the output of is computed as , you can easily confirm this by enabling SQL trace. 

Data inserted after will be automatically partitioned and you can leave your old data as it is, or split into partitions if you want. Partitioning data is easy, the cumbersome part may be dealing with the constraints and indexes that need to be global. You may need to change your PK and Unique constraints (and indexes), as you can not have such indexes locally partitioned without the partitioning key included in them. Also because of dropping the old table and having a new one in its place, you need to sort out the FK constraints pointing to this table. 

parameter value is empty - in this case, the database registers itself into the listener running on port 1521 parameter value is something like - this is a TNS entry that resolves to the address of the listener. If resolves to a valid address, the database should be able to register itself to the listener specified by that parameter value is something like . No name resolution is required, database registers itself in the listener.