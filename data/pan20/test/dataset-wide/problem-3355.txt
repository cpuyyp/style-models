The complement of saw, Jane reading the book, is, categorially speaking, an IP whose I(nfl) head is the non-finite affix -ing, with a subject Jane in Spec I position (but not a CP, i.e., not a complete 'clause', against what jlawler's comment claims). That IP is, indeed, theta-marked by saw with the role traditionally called 'Theme'. It is not, however, Case-marked by saw, because IPs cannot absorb Case, which entails that the accusative Case feature that saw can assign remains available to be absorbed by some available DP (i.e., Jane, in this case). As non-finite I(nfl) -ing cannot assign Case (except 'null' Case to PRO in constructions like I enjoyed [ __[PRO reading the book]], where enjoyed takes a full CP complement), in I saw Jane reading the book, Jane then 'exceptionally' receives accusative Case from saw (one of the 'exceptional Case marking' (ECM) verbs - which, at bottom, means a small number of verbs that may select IP instead of CP complements). The difference is that the DP occupying the Spec I of IP complements is not 'protected' by the CP barrier and remains accessible to Case-marking by an external verb, saw in this case, whereas the PRO subject of full CPs is inside the CP barrier and inaccessible to the higher verb (enjoyed, in my example above). Evidence for the classical analysis just offered comes from at least two simple facts: one is that * I saw [PRO reading the book] is ungrammatical (whereas e.g. I enjoyed [__ [PRO reading the book]] is not), and it is ungrammatical because its PRO subject is doubly Case-marked (as 'null' Case by the IP-internal -ing Infl, and as accusative by the governing verb saw; the other is that when the higher verb see appears in passive form and so loses its transitivity and its capacity to assign accusative Case, as in __ was seen [__ reading the book], the lower subject Jane can no longer receive accusative Case from see and, to avoid violating the Case Filter, must raise from Spec IP into the Spec T of the main clause and become its subject, as in Jane was seen [ t reading the book]. This, in turn, entails that the ing-clause cannot have a PRO subject after ECM verbs like see, since it has a trace-of-Jane one, and so that it must be an IP, not a CP, Q.E.D. 

There are solid reasons to conclude that in DPs/NPs like they both, they must be the 'head' and both must be an adjunct. This stands whether you adopt the (in generative linguistics) now standard analysis of NP's as DP's (i.e., projections of a Determiner) initially proposed in S. Abney's (1987) thesis, or prefer a more traditional analysis of NP's as just NP's (one that can also be easily defended, even within P&PT). Of course, if you adopt Abney's proposal, they both must be a DP, its head must be a D, and the only problem is that, under Abney's assumptions, both they and both can arguably be considered 'determiners'. Thus, are there reasons to choose one of them as head of the whole DP in your example? If you opt for Abney's analysis, you must also adopt the rest of P&PT, and, in that case, the answer is 'yes, there are'. The most obvious one is that the P&PT of that period had already adopted the 'VP(/XP)-Internal Subject Hypothesis', according to which the subject of your sentence, They both, must initially be attached and receive its Theta Role as the highest argument of the VP fighting about corn in Spec VP. In other words, in PPT's 'underlying syntax' your sentence would start (with irrelevant structure omitted) as ...[ __ were [they both [fighting [about corn]]]. Since they is inserted from the lexicon with a nominative Case feature activated, it must eventually land in a specifier in which it can license/check... that feature, and, in your sentence, the only position in which it can do so is the specifier of Tense, i.e., at the overt level of representation, the position immediately preceding the finite verb were. Hence, they must 'move' upwards from Spec VP into the specifier of Tense, it cannot remain in its initial position under VP (as Economy would otherwise dictate!) or it would violate Case Theory, cf. * __ were they both fighting about corn; thus, your sentence will only be grammatical if at least they raises into Spec of Tense and gets its nominative Case feature licensed there, as in They were __both fighting for corn. On the contrary, the determiner (or pre-determiner, or adverbial-quantifier, but let's leave that aside) both need not be assumed to have been inserted from the lexicon with either nominative or accusative Case, because, contrary to they, it can represent either (cf. I saw both [Accus.] vs. Both [Nom.] saw me). In such circumstances, a lexical item may be inserted with an unvalued Case-feature, and, as a consequence, need not occupy a structural position in which any particular Case is licensed. Consequently, contrary to they, both need not independently move anywhere to satisfy Case Theory and can stay in situ, as in They were __ both fighting about corn. What's more, since both need not check Case, granted Economy, it shouldn't be able to move on its own anywhere, and, indeed, if it does move into the available Spec of Tense, the result (i.e., * Both were they__ fighting about corn) becomes ungrammatical - on two scores, a) because the raising of both would be gratuitous (and violate Economy, in current terms), and b) because it would also prevent the necessary raising of they into Spec Tense in search of a legitimate nominative Case value, which will violate Case Theory. As a consequence, either both stays behind in situ (as a so-called 'floating quantifier') and the result is They were __ both fighting about corn, or it accompanies they when it raises into Spec Tense and the result is They both were __ fighting about corn. If it does, They both must have been a unitary constituent at underlying structure, or Move would not have been allowed to raise them together. If it does not, and only They moves into Spec Tense, then they both cannot have been a unitary DP at underlying structure, because, if it had, they (a D/DP) would not have been able to raise on its own out of a higher DP without violating the A-over-A Condition (aka 'minimality', aka 'shortest movement' and, ultimately, Economy). It follows that they both must be analysable as a unitary DP or as two separate constituents DP + QP? depending on whether they both raises as a unit or they raises on its own leaving both behind. That is why not all linguists accept Sportiche's Q-Float analysis, but we can ignore that detail now and concentrate on the former case, because, if they both is not a constituent, your problem (which of them is the head?) automatically dissolves. When they both is a 'deep' unitary DP and lands in Spec Tense, then, there remains the question whether the head of that DP is they or both, and why, i.e., what primarily motivated your question. Now, suppose the head is both. In that case, given the surface order, they must be a specifier, but then its Case feature will remain unlicensed, because although Tense licenses the nominative Case feature of its DP specifier, if the head is both, that licensed Case feature will 'percolate' down into it, but not into its specifier. On the contrary, if the head of they both is they, the nominative Case feature that Tense assigns to the whole DP will 'percolate' (cf. the Head Feature Convention of GPSG/HPSG) into its head, the Case feature of they will be licensed, and the sentence will be grammatical. The fact that it is, then, forces us to conclude that the head of they both cannot but be they.Q.E.D. If,instead of Abney's theory of the NP as a DP, you prefer the traditional NP analysis, a very similar reasoning leads to the same conclusion: if they is the head of they both, the Case feature assigned/licensed on the whole NP in Spec Tense will 'percolate' down into its head, they will satisfy Case Theory and the sentence will be grammatical. If, on the contrary, they is not the head, but a specifier, it will violate Case Theory and the sentence will be out. Finally, to the extent you should entertain alternative analyses under which both is an adverbial quantifier attached to the VP but does not form a unitary constituent with the subject they, both they and both are heads of their respective phrases (DP, QP) and your problem disappears. 

The question "What is the relation between lambda calculus and LF?" is comparable to "What is the relation between F (any formal system of representation with an 'alphabet' and a set of 'formation rules' that can recursively generate the expressions of LF) and LF?, or even to "What is the relation between a natural language L and the LF of L?, and the answer is rather trivial: the relation is usually understood to be 'representational' (basically: one of 'translation'), i.e., the lambda calculus can (among many other things!) represent the Logical Form of natural languages, as you know very well. However, what I suppose you want to understand better is under what assumptions and in what sense can lambda calculus be considered a better representational system for the LF of natural languages than, say, Predicate Logic, or Predicate Logic 'enriched' with events (plus tense logic, modal logic, possible words, etc.). Such formal languages are more or less 'powerful' (and more or less flexible and adequate) to represent the semantic structure of natural languages in a 'compositional' way to the extent that they acknowledge the existence of additional 'entities' (events, time intervals, etc., and new variables that can be quantified over).As you know, the most austere ontologies acknowledge only individuals (and sets) as in first-order Pred Logic, but Russell, Church, Reichembach and others 'enriched' that ontology with 'higher types', then Davidson added 'events', and others even more generously acknowledged 'times' or time 'intervals', modalities, propositions, possible worlds, etc. Montague, of course, has it all. You can take 'lambda calculus' as a sort of limit, in this respect, since, in principle, due to the properties of functional analysis, it can acknowledge and operate with an infinite number of types of entities. The enormous advantage of lambda calculus over elementary logics as a matelanguage for the compositional representation of natural language LF comes from the fact that, assuming the analyst accepts a very rich ontology (e.g.,Frege's, for example, in which entities are either 'functions' or 'objects' of all types it may be analytically convenient to acknowledge!), it is possible to consider any non-atomic expression 'E' of any NL as the 'product' of two 'factors', a function 'F' and its 'argument' 'a', and say that F yields E if 'applied to' 'a'. What's more, since any function can be trivially turned into an argument of a higher order function ('Type Shifting', 'Type Raising', 'Function Composition'), the analyst can recursively 'reanalyse' functions as arguments (and viceversa!) at will, depending on the type of the 'domains' of the functions he may be interested in acknowledging in order to exhaustively analyze E into just two 'factors', an F and its argument. And by 'any non-atomic expression E', I literally mean 'any expression', i.e., not just sentences, words, or intermediate phrases of any level of internal complexity (NPs, VPS, APs, AdvPs, PPs, QPs...) but also STRINGS (continuous or not!) of linguistic material that under other analytical approaches (e.g., phrase structure grammar, with its constituency tests) would not be acknowledged as relevant syntactic constituents and therefore would not be eligible to undergo operations (either in syntax or at LF). Actually, since lambda calculus is based on functional analysis and the latter is so powerful a tool, it can in principle be applied even to sub-morphemic units. In other words, 'features' (= [attribute:value] pairs) or even feature-values can also be analysed as 'arguments' (or functions!) if necessary, although, in practice, most syntactic-semantic theories treat minimal 'signs' as unanalysable atoms and seldom or never perform sub-atomic analysis (the event structure of verbs, mainly, excluded). That makes lambda calculus an extremely powerful and flexible metalanguage for the representation of the 'factors' that compositionally contribute meaning to NL expressions of any level of complexity. You can take the sense of a DP expression like 'the definition of syntactic categories in English' and analyze it into a function 'the definition of syntactic categories in __' and an argument 'English', just as easily as you can take the function to be 'the' and the argument the rest of that DP (the NP), or the function to be 'the definition of syntactic __ in English' and the argument to be 'categories', etc. In principle, given an LF object O you can 'abstract' ANY component C of O and analyse the remainder as a function F( ) = O. One of the consequences that have attracted more attention is that if lambda calculus is available to compute LF, grammars can be monostratal, i.e., it is possible to compute 'meaning' directly from 'surface syntax'. Thus, constructions that in early PSG or 'Chomskian generative grammar' are supposed to involve 'displacement' (e.g., WH-Movement, Topicalisation, Subject Raising, Extraposition, Right Node Raising, etc.) can be computed as if everything was 'in situ' provided certain kinds of PSG (GPSG, HPSG, etc.) or Categorial Grammar work in tandem with Lambda Calculus to generate syntactic structures and calculate compositional LFs, respectively. Also, since in lambda calculus virtually anything can be treated as a syntactic/semantic 'factor' ('constituent'), coordinative or elliptical constructions (e.g., Gapping, Sluicing, Right Node Raising) that under standard PSG approaches create constituency paradoxes, no longer do in Categorial Grammar cum Lambda Calculus. Mark Steedman's Surface Structure and Interpretation(MIT 1996) or The Syntactic Process (MIT 2000) are authoritative and readable classical statements in this respect. At a more elementary level (and under a Chomskian view of syntax), I. Heim & A. Kratzer's Semantics in Generative Grammar (Blackwell, 1998) is also worth reading. Of course, all that immense analytical and computational power has its negative side, too: grammars must not only be simple and elegant, but also, and above all, empirically adequate, which means able to generate enough without overgenerating, and, as you can imagine, unrestricted categorial grammar cum lambda calculus overgenerates massively, it acknowledges wildly extravagant LF/semantic entities ('objects' and 'functions') which must ultimately correlate with just as extravagant extralinguistic 'entities', and, needless to say, raises formidable linguistic-theoretic, metaphysical and philosophical issues.