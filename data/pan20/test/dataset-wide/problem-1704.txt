Do you have this job in a user's crontab (Can you see it with ?) or is it in the system crontab ()? The system crontab (On linux at least, don't know about other systems) requires an additional user argument that the user crontabs do not. In a user specific crontab, it should look like: 

Basically what this is doing is looking at the url of every request and if it is matches, inserting some config rules on the fly before the authentication and authorization stage of the request. To modify it, change the bit, the regex match , and the list of directives to insert. This will leave you with a user file to maintain, where the username is the directory name from the url. If you want multiple users per directory, you'll have to use groups and maintain that file as well. For that, change to and add . 

So, the virtual hosts - local and on dev - are pointing to the "root", while on the live site the domain appears to point to the "public" folder. And so, when I put the rules on the "public" .htaccess file - then it all worked on the live site. (And what was confusing for me was that when I break the "root" .htaccess file - the live site gives Error 500 ... and that's why I thought the "root" .htaccess file is the one that I should use ...) 

And then pass this new array of JSONs to the insert method of the new collection: (copied the array in the clipboard, directly from the text editor I used to make it) + Shift+Insert + And voila :) ... all your data is transferred :) ... 

I just did this install myself recently. The Erlang included in Hardy is too old. Simple as that. You need to build Erlang as well as CouchDB. 

and follow the instructions to reload apache. If it still doesn't work, add a comment below and possibly update your question. I'll take another look and maybe set up a test system. 

The other answers here are correct, you need to change what files are being matched by your config. The same problem could occur with the files in /var/log, but the combination of compression and the 10M size limit is keeping it from happening. There's another problem you're going to run into that I'd like to point out. Logrotate is built to rely on a consistent log file name to rotate out old files. When it processes a log file, it uses that as a base name to find all old versions of it by searching for the specific extension that old versions would have. That's what the glob stuff in your debug log is looking for. Since your filename changes every time because of the date, it will never go back and look at those old ones because the filename it's working on now doesn't match. There's two things you could do. First option would be to set up postgres or whatever script is creating that dump file to not use the date. Keeping the filename consistent will let logrotate do it's thing and clear out the old ones. Alternatively, you could skip logrotate entirely and put something like this in your crontab: 

I've been googling and searching MS's website for their MSDN or technet page on this cmdlet and can't find it. Google keeps wanting to point to the Get cmdlet. I used the MS link for the Get: $URL$ And tried to use: $URL$ But it fails. The cmdlet shows up in the ConfigurationModule and because of where the server lives I can't use Update-Help. There's also no "link" section in the help to point to a technet article. Anyone know where the MS documentation is on this? 

My answer is for the "Is there a smarter/simpler way to do this?" part of your question. This script was successful in removing a go daddy cert for me 

The three addresses you listed there will work with a certificate for *.example.com. Be careful with other names you might add in the future if they have more words separated by periods. The meaning of * for certificates is inconsistent between browsers. Some will match anything, others will match only one word separated by periods. 

This is not at all clear in the documentation. What I believe is happening here is that causes the session to read only the rows or bytes or whatever that existed when the table was locked, regardless of what size the table actually is when it gets to it. Rows can continue to be inserted into the table, but only at the end, past the locked portion of the table. 

I had to add a rule to the .htaccess file of existing site, which redirect some old images urls to the new ones. So, directly after the 

The local machine (the source) is accessible from the outside, but only with the IP. The version of the local mongo is 2.0.4, while the version of mongo on the dev server is 2.2.2. 

I have a collection in a mongoDB, on a project that runs locally, and have to copy the collection to the dev server. Can you tell me how to do this, considering that: 

I installed and setup with virtual users, and each one of them has folder in . The ownership of is . When I point a subdomain to open with from one of those folders - I get . What is the best way to solve this? 

Powershell treats "\" as a reference to the root of whatever context your operating from. So if you're working from the standard provider (e.g. "c:\") "\" is the root of C:. If you're working from the certificate store provider (e.g. "cert:") then "\" is the location container for the user and machine cert stores. Another nuance to Powershell is the way it works with network resources. You typically experience greater success when operating from the file system provider (e.g. "filesystem:") than you do from the standard provider. You also have to specify the network location "\server\shareroot\etc". Your path variable needs to reflect either the mapped location (e.g. "Y:\Trunk\bin") or the network location (e.g. "\servername\folder\trunk\bin"). Hope this helps. 

Well, after some experiments I did - I figured out what the problem was :) ... The structure of the application is this: 

I made a little research, but I'm not sure for some things ... like for example which credentials should I choose for the source DB ? ... the ones for SSH connection? ... or ... ? ... Thanks in advance :) 

Locally and on the dev server - all is fine. But when I deployed it on the live site - it's not working :( ... Locally I'm on OpenSUSE, the dev server is on Ubuntu Server, and the live server is FreeBSD. I ran phpinfo() and it showed that on the live server (shared hosting) I have mod_rewrite, so I don't know what could the problem be. Tried to add some "Options" to the .htaccess file, tried with flag for redirect in the end of the new rule ... but nothing changed. All the other rules are working. When I break something in the .htaccess file - it gives me "Internal server error", and if I write "Deny from all" - it gives me "Forbidden" - as it should. Can you help me? ... Thanks in advance :) On another forum got some advices and tried: 

You can use the auto_increment_increment and auto_increment_offset settings. Each server will have a different offset value, and they will all increment enough each time to "hop over" and leave room for the numbers your other servers will generate. There's some good docs about it here: $URL$ 

I think you need to set up a display manager such as GDM. When Xvnc is starting up, it tries to communicate with a running display manager via XDMCP. That's what the option is in your xinetd file. I've set up GDM on Hardy before, and it worked pretty well. Give this a try: Install GDM if it's not already installed. Edit /etc/gdm/gdm-cdd.conf: 

The directive can be used several ways, depending on the syntax. To execute a file the way you want, you must use the full path relative to the DocumentRoot, preceded with a /. Otherwise, it's taken to be a literal message to be displayed, which is what is happening for you. Try this: 

Given how WMI was built and implemented you're a bit SOL on a magical solution. I can help you some of the way, but you're still going to have to eyeball the results. Essentially you need to start with a list of providers which you can get with the following code: 

The automatic distribution option in Exchange 2010 is performed by random selection. However, you do have some options to control which DB's are randomly selected. The following technet article will help you: $URL$ 

None of the properties there appear to map to any tickets (run a gm on it too). Granted, this could be ignorance on my part and the method to produce the tickets just isn't aptly named or described. I've tried looking at the SMlets source code to glean some insight, but I'm not a C# developer so some of it goes over my head. I've also tried to get some understanding and help from the msdn page for EnterpriseManagementObject Class, but to no avail. Hopefully there's a really simple solution to this and I'm just over thinking it. But, how do you get the Affected User for a SCSM ticket using powershell? Preferably a service request ticket.