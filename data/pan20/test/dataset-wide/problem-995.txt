The sp_addlinkedserver statement requires the ALTER ANY LINKED SERVER permission. New Linked Server GUI requires sysadmin...the sp does not. Just verified with a new SQL login that has pretty much nothing but ALTER ANY LINKED SERVER. $URL$ 

My Google-Fu is failing me... Is there now, or possibly in the works a way to tell SQL Server to stay inside one NUMA node, either server wide or at the query level? Basically the same functionality as MAXDOP, but MAXNUMA? Right now, unless I am totally missing it, when a query goes parallel it can use all of the processors it can see. Just wondering if there is a way to restrict it to one or two or 'x' NUMA nodes. I may not really want my queries spread out over 80 logical processors :) This is mostly a curiosity rather than a "help me break my broken thing" question. Thanks, Kevin3NF 

The Dev license is not about where, but rather how the installation is used. If you are not using it for Production, its fine. Separating out by domain is great, but not required. 

Update: Konrad corrected my misunderstanding of his question. The goal was to count queries, not transactions. How to count queries? Method 1 Use pg_stat_statements contrib. Method 2 Enable full logging of queries for a representative period of time. To enable full logging, for PostgreSQL 9.0 - 9.3, change following settings in 

Decide your vertexes (nodes, objects) and edges (relationships). Convert relational data to cypher, declaring all items and all relationships explicit. 

No it's not. Unless your field set is very dynamic (no single authority, people can invent fields on the fly). 

Solution 2 Using more ANSI-compatible SQL, like UNION and LIMIT. It will work on MySQL, DB2 and some others. Similar solution can be done on Oracle, just replace LIMIT with ROWNUM. 

Notes on performance With small tables (less than 1000000 rows), any solution will work. is slightly faster: 

All operating systems and all applications use a concept called "caching". It means - when the data is first read from a slow memory device (like, a hard disk), it is saved in a fast memory device (like, RAM) for some time, to facilitate faster lookups. The same applies to RDBMS. First time the data blocks that build up your query results are read from disk, second time they are read from memory. Details can be explored using OS and database tools. If you specify what RDBMS and what OS you are on, we can help you get the details. For PostgreSQL it's about EXPLAIN command. 

Having connection issues out of nowhere in my lab (Azure VMs): SQL 2016, sp1, cu4 all the way around, Windows 2016 Datacenter Windows Failover cluster for SQL AG testing. SQLServer-0: Default instance (port 1431) Inst1 (port 1499) SQLServer-1: Default instance (port 1431) Inst1 (port 1499) From SQLServer-0, I can only connect to the remote instance(s) using tcp:SQLServer-1, 1431 (or tcp:SQlServer-1.kevinsdomain.com,1431) Same thing for the named instance. Same thing from SQLServer-1 to SQLServer-0 Ping of just the NetBIOS name works just fine, ping -a resolves the FQDN None of these are running on port 1433, both server are running SQL Browser under 'NT Authority\Local Service' This is my test domain, along with a domain controller, DNS, etc. Windows firewall on each has all of the ports open that are needed, including UDP 1434. I’m sure there’s something simple I’ve forgotten to check, flush, whatever. Just can’t think of it. Kevin 

The restore from 2014 is an upgrade, not just a restore. You should actually see this in the messages window, assuming you are using T-SQL to do the restore. You don't mention what Fast and Slow are, or the differences, such as 5 mins vs. 5 hours, so we can't add much to this. 

You can use string escape syntax and function like below. Please note it's heavily dependent on setting. Should be . You can find details here, here and here. 

To redirect traffic from primary to standby you need some external tool which will do the failover procedure - using either dns-based or IP-based or other failover method. PostgreSQL itself does not know how to redirect traffic or do anything outside the database scope. Popular tools are pgpool (in layer 7) or Linux HA or corosync and friends (in lower layers). 

Yes you can hide those messages in the log. In the calling session, before running the statement, issue this statement: 

Internal representation of larger attributes will be sometimes compressed. More specifically, what works here is the TOAST (Oversized Attribute Storage component used in PostgreSQL). The threshold when values are considered for compression is 2000 bytes. is not a logical length, but the size (in bytes) of actual internal representation of the column/variable. It is documented. PostgreSQL stores array values in a custom, internal, binary format. Command line example below. Details also here. 

(I've tried to somewhat format it to make it cleaner, but I don't think it really helps) Running this query with an execution plan shows that for the majority of these tables, it's doing a Clustered Index Seek. There are two operations that take up roughly 90% of the time: 

Now, for the actual problem: As part of our software, we need to select random products, given a store and a general category. However, we also need to ensure a good mix of manufacturers, as in some categories, a single manufacturer dominates the results, and selecting rows at random causes the results to strongly favor that manufacturer. The solution that is currently in place, works for most cases, involves selecting all of the rows that match the store and category criteria, partition them on manufacturer, and include their row number from within their partition, then select from that where the row number for that manufacturer is less than , and use to clamp the total rows returned to . This query looks something like this: 

On categories without a lot of products, performance is acceptable (<50ms), however larger categories can take a few hundred ms, with the largest category taking 3s (which has about 170k products). It seems I have two ways to go from this point: 

in pg_hba.conf, in the lines with localhost IP, replace "ident" by "md5" and restart - then you will be able to use password logins if (1) is not acceptable, sudo to "postgres" user and create another superuser login - then use this login to connect from PgAdmin. 

In current version (Pg 9.0-9.3) there is no fast "fail-back" mechanism. You will have to follow official guidelines which state: 

I'm not yet sure if this is doable in pure SQL (probably - yes), but here is the brute force solution using cursors. This is just a (working) draft, with some twiddling and dynamic SQL you could add more parameterers, like function name. 

RULEs or triggers are a performance overhead, and can be avoided. Consider something along these lines: 

Only thing I do not understand in your question is "database agnostic" - what does it mean, precisely? 

You do not need any triggers / rules to maintain it. There are open ends here - that's just a draft... Some issues: 

This answer assumes that you want to connect via TCP to localhost. (for local socket connection see Erwin's answer) Two options to ease your pain: