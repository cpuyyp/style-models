I have often found the IN operator to be a source of performance problems in the past, and avoid it whenever I can (which is almost always). 

At 14 million rows in your largest table and 16GB of RAM devoted to SQL Server it sounds like your entire DB almost fits in memory. If that is true the problem is likely to be quadratic (or worse) behaviour in your queries, which can only be truly fixed by rewriting the worst offenders. That said, I would try one or both of: 

As Joe Celko never tires of noting, the concept of Primary Key is an artifact of pre-Relational database technology. As such it is wholly within the Physical Design of your database. At the Logical Design level it is necessary to have a Unique constraint on the column token to preserve the integrity of your data, as that column is a Natural Key for your table. Then the question is (a) whether or not to have an additional surrogate key to assist in performance of Foreign Key lookups; and (b) if yes, then should the table be clustered on the Natural Key; the Surrogate Key; or in some cases on neither (heap storage, using an implied RowID as surrogate Key). In only one case will it be necessary to specify as part of Logical Design that a Surrogate Key be present in the table - when it must be allowed for the Natural Key to be editable. These two questions are not part of the Logical Design of your database, so need not be addressed early if your development methodology allows for convenient schema updates to your DB. As an aspect of Physical Design, the answers to these two questions will be properly based on the estimated table sizes, and the frequency of FK lookups into this table. In order to make a sensible design one should be well read on the affect of index structure and construction over time. It is unfortunate that the term Primary Key, which originates as an artifact of pre-relational database technology, was first co-opted into the Logical domain by Date, Boyce, Codd, et al; and then co-opted back into the Physical domain by the major RDBMS vendors. I avoid the term completely when I can, and specify all my indices as being simply Unique or Non-Unique, and decide which one will be clustered as late in the development cycle as I can. 

You can use the "free" MySQL workbench if you are using MySQL $URL$ to reverse engineer the model from an SQL file then re-organize the model into multiple pages. 

Remember these are just starting points you can fine tune specifics as you move along, but before you fine tune, measure tune then measure again 

How about making each of the elements of your XML a table or child-table and using in-built stored procedures to generate the XML from an SQL query result. This provides you with the data consistency and validation of the relational model plus additional database functionality with the ability to generate your XML document on the fly. If you need the XML documents handy you can create the equivalent of a "reporting" table into which the XML is stored ... 

UPDATE: To cater for the computation of the room coefficients with dependence on the building in which the room is located To compute the different room coefficients you can have either: 

In EXCEL, from the Data ribbon, Get External Data tab, select From Other Data Sources -> From SQL Server. Follow the wizard to connect to your server and create a query. 

If you decide to keep a single entry for both sides of a transaction, then by definition you are engaging in single-entry bookkeeping. This may be the most appropriate solution for some simple applications, but be clear that you are losing all the functional and robustness advantages of double-entry bookkeeping, in exchange for a simpler design. Note that when viewed stand-alone Subledgers (though not their corresponding Journals) are often implemented as single-entry, since a control-account in the General Ledger captures the subledger total and the balancing side of the transactions are in the General Journal (GJ) and General Ledger (GL). You also appear to be confusing the distinct concepts of Ledger and Journal in traditional double-entry bookkeeping. The various Journals (of which there will be numerous specialized varieties for specific common transactions of the business in addition to the General Journal) is a chronological history of all transactions entered into the system. The General Ledger is an ordering by account of all transactions entered into the system, and the various subledgers are an ordering by subledger-code of all transactions entered into the corresponding Journal. Two examples of common Ledger and Journal combinations: 

I would recommend modeling it as a sale with an adjustment for the trade-in, assuming that as a car dealer you accept trade-ins without a related sale of a car. Therefore within your system you re-use the purchase features, and can later report on metrics like how many sales are linked to trade-ins, what values of sales are linked to trade-ins, whether trade-ins boost sales, etc 

The composite primary index will always ensure that the combination of col1, col2, col3 values are unique. If the purpose of the index is to ensure that the values are unique then you will have achieved that A composite index also provides for index matches on any combination of the columns col1, col2, and col3 You would create separate indexes on col2, and col3, if you join or filter using any of the columns without the others. I always prefer a numeric primary key (with no business association), and unique indexes over a composite primary key where necessary. 

(My apologies for the SQL Server syntax - I don't have MySQL. Please treat as pseudo-code.) A scheme such as the following should work 

Put the list of values into a temporary table, and then perform an INNER JOIN to it. Most SQL optimizers and engines can handle joins much better than they can handle an IN operation. If the list is long, this also allows you to define an index on the temporary table to further assist the optimizer 

Every user in the system will immediately start seeing the three default bookmarks, personal copies of which they can then edit as they see fit. 

so NOPL is another possible candidate key, CK2. However CK1 and CK2 have columns NOP in common, making them overlapping candidate keys. 

You have forgotten the join criteria . If you had used the modern JOIN syntax this would have stood out like a sore thumb. 

First of all since you are using MySQL, go for InnoDB which allows you to create relationships between the different tables (not dbs) You need three tables: 

How about this: a) Export the table structure from SQL Server and create the tables in the MySQL database - this is a one off b) Setup a scheduled job to export the statistics data from SQL server into an SQL file c) Setup a cron job to apply the SQL file to the MySQL database created - you may run another script prior to this if you need to empty the tables first 

I would suggest that you indexes to job_id assuming there are many events for the same job and event_date assuming that there are many events on the same date assuming that there are queries which use these columns for filters. However see answer by @ypercube below which has a better explaination for indexes PS: Updated to remove erroneous suggestions which are left below for reference: b) In the query I can see that you are restricting by a date and time yet event_create_date is a date field, you can improve the comparison by using TO_DAYS(event_create_date) = TO_DAYS('2012-12-18 00:00:00') c) Additional advice on indexes - add indexes to those columns with a lot of repetitive data, indexes are not very effective if the data varies widely ... (That is why @Rohan requested for more details on the data) 

The Shipping Address is a property of each and every order, not of the customer. Even if it was a property of the Customer, it would be necessary to know where past orders were shipped to if a customer relocated. Therefore all that can be stored as a Customer property are the Default Shipping Address (and Default Billing Address), for pre-populating each order and invoice as they are created. This is not a denormalization, because it directly reflects and captures the required business rules. Now that the historical accounting records are taken care of, there should be a CustomerLocation table with FK's to three Address records in an Address table: Physical Address; Default Billing Address; and Default Shipping Address. The first of these records should be associated with an EffectiveDate, so that again a historical record exists as changes occur. 

A Non-Clustered Index on the child table by ParentID and TypeID will cover the subquery. An Indexed View on the subquery is possible also. 

I would recommend that you have a Pages table that contains both Static, Blog and any new pages you may have. The Pages table would have the columns page_url, page_content and pagetype (a tiny integer) which will help you differentiate between different pages. Later u may end up with other page types which have content hence leaving page content as a field in the pages table and may also allow you to have custom PHP or markup content in a blog page for customization (thinking aloud) You can then have a BlogPages table which has the information for blog pages linking to the categories and posts. 

The balance is a computed amount based on certain business rules, so yes you do not want to keep the balance but rather compute it from the transactions on the card and therefore the account. You want to keep track of all transactions on the card for auditing and statement reporting, and even data from different systems later on. Bottom line - compute any values that need to be computed as and when you need to 

The theory is all fine and good, but it only starts to really make sense once you understand the practice. The Lawyer's version of the first three Normal Forms is: 

Amend your table definition as following to add two additional indices (My apologies for the SQL Server syntax - it's all I have here at the moment): 

Adding some SSD RAM to the server devoted to the indices of the largest tables; and Adding more disks (not more disk space, more actual disks) in order to have more heads and axles servicing the massive table scans. 

This has the additional advantage of being data-driven; in the case that your masking needs to be amended, only a data change is required instead of a code change. 

Working from Aaron Bertrand's Script to create dynamic PIVOT queries in SQL Server and with a sample data set defined thus: 

a) categories (db3) - I think you will use this for the tag cloud b) products (db1) - name and details of each product c) product_categories (db2) - a single row for each product and each category 

For access control use the groups instead of individual users. Create a group called public to which all users belong therefore any graphs assigned to public will be visible. Look to Role Based Access Control (RBAC) for more on this Each graph can be accessed by one or more groups, which allows you to have graphs that are visible to different cross sections of users (through groups). This means your access table will be graphid, groupid, edit, view with a unique key constraint on graphid and groupid. Answering the question can a user edit this graph will require a join from User_Group to Access table There is no special meaning for nulls, as none exist Handling users with no account is easier since no authentication is provided For the graphs, you need a graph table, and also a graphid added to the node and relation tables to simplify queries. 

the General Journal and General Ledger are an aggregated summary of all transactions used for generating Trial Balances and the Financial Statements. The Account Receivable subledger receives entries from the Invoicing Journal (debits to AR by Customer Number) and the Cash Receipts Journal (credits to AR by Customer Number). 

Use a CROSS APPLY (or perhaps OUTER APPLY) to replace the case statement in your aggregates like this: 

in your inner WHERE clauses is non SARG-able, so no index can be used. Change both occurrences as shown below, to make this term SARG-able: 

As an aside; Why on earth do you want to insert a two-character table identifier into every instance of this new field? The table that the row came form is immediately obvious from the table being inspected. If you really think that your data collision rate across a large number of sites is so extreme as to justify a scheme such as this, at least use a GUID, an already implemented technology, instead of this NIH design. Update The monotonic nature of the autoincrement key is made use of in some very efficient tally-table algorithms for running totals( at least) to enforce proper sequencing of running-total calculations. This scheme would invalidate the use of those algorithms.