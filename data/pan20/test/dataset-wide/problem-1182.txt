What sticks out to me here is the failure of grabbing the resource group "Available Storage" using WMI; so I thought I would play around with writing some powershell to collect resource groups using get-wmiobject to see if I could track down any possible errors there. When I run the following snippet 

Try enabling the DTC on the sq202 box, since that is the server initiating the distributed transaction. You will also need to have Enable Promotion of Distributed Transactions for RPC option set to True as well for the sq009 linked server located on the sq202 box; when this option is TRUE, calling a remote stored procedure starts a distributed transaction and enlists the transaction with MS DTC. Helpful information on T-SQL Distributed Transactions and the requirements: $URL$ Linked Server Options: $URL$ 

I do not receive any values; if I run this same script against Node 2 and any other node in all of my other failover clusters, no matter if it's active or inactive, I'm able to pull back the 3 resource groups (Available Storage, Cluster Group, and SQL Server (MSSQLSERVER)). I've done basic research to see if the WMI repository is corrupt, but nothing is pointing me in that direction. Any ideas? Thanks! 

Then you can query the Excel file using the 4-part Linked Server syntax (the DB name and schema part are shorthanded here with ...): SELECT * FROM [YourLinkedServerName]...[Sheet1$] 

In order to try and solve this, I have rebooted PG, the machine, and vacuumed the DB. I believe there is an error in the CREATE OPERATOR code. If I can index an array of custom type of (int, int4range), that would be even better. I've spent quite some time (a full day) wading through documentation, forums, etc., but can find nothing that really helps me to understand how to solve this (i.e. create a working custom operator class). 

I defined as 16 and therefore created 16 indexes. I also created an index that contained all rows (ignoring ). In my tests, I get an almost 13 times speed-up by having 16 indexes vs 1. Great! However, this is test data (1 million random entries), which is somewhat meaningless. Nonetheless, it proves smaller indexes provide tremendous speed-up. Close to linear in fact. My question is, is there a way to calculate an optimum number of partial indexes without going through a tedious trial-and-error process? I have searched, but can't find any best practices regarding the number of partial indexes. If there is no "formula" to determine the answer, can anyone share their experiences with multiple partial indexes? 

But when I try to create the index, it fails (error below). However, if I call the create from within a DO $$ block, it executes. If the create index executed, I get the error on INSERT INTO instead. 

I have a 2 node failover cluster that I'm trying to patch. The initial patch level of the nodes was SQL Server 2016 SP1-CU1 (OS: Windows Server 2016 Standard) and I'm trying to upgrade to SQL Server 2016 SP1-CU3. I started by patching the inactive node (Node 2) and I was able to install CU3 without any issues. After performing a failover to Node 2 and determining that the patch was successful, I tried patching the newly inactive node (Node 1). The installer fails a few seconds after hitting "Update". Here are the Summary results: 

You'll need to use a more recent driver since the excel file was created in Excel 2016. Install the following driver on the machine running the SQL Server Database Engine: Microsoft Access Database Engine 2016 Redistributable (64-bit) $URL$ After the driver is installed, open up SSMS and navigate to: Linked Servers -> Providers -> right-click Microsoft.ACE.OLEDB.16.0: Enable "Allow In-Process" From here, you can use the Microsoft.ACE.OLEDB.16.0 in your OPENROWSET command. I prefer to setup linked servers to these files, as follows: 

Having generalized security across all servers / applications would be a bad approach. With your suggestion, once you let John Doe from Customer Service have read access the Customer database (which he needs in order to accomplish his job), you effectively give him read access to job applicants in an HR database. The HR database would contain Personally Identifiable Information (PII). Since John Doe is not apart of HR, he shouldn't have the permissions to access this data. Allowing him to read out of the HR database would cause compliance issues and could result in major fines when those auditors bust through the door! John Doe would also be able to read salaries out of the Payroll database, and ohh boy, once that info goes public you're bound to have some tension in the office; employees will question why the new guy is being paid more than them, etc. Now that I've beat that horse, here is an article I ran across recently discussing how to setup AD groups with specific levels of access to specific databases. This goes into a little bit more detail than you may need, such as creating scripts to create the AD groups, but should be a helpful start. $URL$ 

Each role has its own login and has access to its corresponding schema. Roles may have access to certain other roles (simple ). All objects within a schema are owned by the schema (except for the schema 'core' where the owner is 'postgres'). Any given role can do the following only: SELECT from relationships and EXECUTE functions. Obviously, access to sequences, types, triggers etc. is required. All schemas have access to 'core' and 'public' (since types, functions, etc. are therein). Certain relationships will be individually set to have specific priveledges (such as no ). 

I've never attempted to define roles and permissions in PG. Please bear with me here as this question has a few parts to it. This is my setup: 

In my experience, a recursive hierarchy is the most practical way of tackling this. It offers the following advantages: 

By contrast, it takes an extra table for each level of "-to-many" joins. This is hard coded and difficult to maintain against schema updates. By using filtered indexes, a large table of hierarchical joins can perform at superior speed to dedicated tables. The reason is each join is only "parent-child" compared to "to join table to data table". The latter has more indexes to process and store. I've been trying to solve this problem for many years. Recently, this is what I came up with.