The book to check is once again Corbett's "Number", from page 78 and onwards. Unfortunately this is one of the features that WALS ought to have but doesn't. Doing a typological study (that is: actually going through grammars and counting which have the distinction and which do not) sounds like good and suffcient material for at least a master's thesis. 

By relying on inference, parataxis can do the job. Compare the a-examples (parataxis) with the b-examples (subordination) below. 

There are as many dialectal differences as there are differences between languages and generally a more interesting and unanswered question is: where is the cutoff between dialect and language. The old joke that "A language is a dialect with an army and a navy" doesn't hold since German German and Austrian German aren't considered two different languages, and there's a movement underway to reunite Brazilian Portuguese and European Portuguese (makes for a bigger market, that.) 

I don't know of a specific tool for making dialect maps but it does sound like something suitable for OpenStreetMap (OSM) and its many map-making tools. Egotistically, it would be a great overlay to put on top of another map on my GPS, which uses OSM =) It would make for some silly trips: Are we at the isogloss yet? This is retroflex /l/-country! 

The "goodness" depends on the purpose. There are several purposes. Some are meant to be used by people in a similar way to how natural languages are used, and meant to be used by several people, like Esperanto. Others are to give flavor to an artwork; a book (Lapine), a movie (Na'vi), a tv-series (Dothraki), a franchise (Klingon). Being used to communicate is then not one of the primary goals. Then again some are experiments: "Can a language with X be mastered by humans? How will this feature Y change when used? How will this feature Z affect its users?". Some are meant to be artworks in and of themselves. Some just happened and later give birth to conventional works of art (Quenya). Some are meant to be secret, used only be the creator or a small group. Some are made to communicate with the divine (not so uncommon in Wicca IIRC). Some are political statements, IMO (LÃ¡adan). Some are an attempt to fight depression (Toki Pona). Some are made for role-playing purposes (Tsolyani), as part of the world-building. Some are made to learn/discover the principles of linguistics. Some are made as a source of examples to teach linguistics. Some are made to serve as a contrast to languages already known, in order to better understand the mechanisms of those. There are probably fewer purposes than there are conlangs but I don't think there are that many fewer. Some have a goal of passing as a natural language. Then it becomes possible to use the normal linguistic arsenal of tools to analyze them and discuss them. Those who do not have this as one of their goals is probably a bad fit for this site. Some are meant to be a complete system usable for communication, others are not. As with naturalism, the former can lead to fruitful discussions, the latter are a lot less interesting, at least to me. 

If what you want is to teach them about lexing and parsing, how about SWI-prolog? It even has a limited online version. Though, showing off lexing and parsing with prolog might ruin them for other solutions... Been there, despaired that. 

Using "to have" to show perfect is a key feature of the European sprachbund. It's happened repeatedly in European languages. For instance in Latin, than later with a new word for "to have" in descendant languages when the last round of suffixes have become worn down to invisibility. It's a "language hailing from Europe" quirk, and I think "sprachbund" is explanation enough. I wouldn't call it widespread though: $URL$ 

As you practice what @JoFrhwld is saying, do the finger-trick: put a finger close to the lips and say [b] [p] [ph] [p']. Expect just about no tickly air on [b] and [p], lots of soft air on [ph] and a very concentrated, short hard puff on [p']. 

I'd say a natural language has (or had) native speakers. Natural languages don't cease to be natural when their last native speaker dies. A native speaker is one that learnt it as a child. This means that it had at one time been learned as the first language of children (one child is not enough). So if adults got together to learn a language, it couldn't be a natural language until their children could use it as their sole and first language. A language that is no longer learnt by children will go extinct. The test of a definition of natural language is what to do with Esperanto. It now has at least three generations of native speakers (though they are all multilingual). Can a constructed language become a natural language by being the first language of children or will a constructed language always be a mere constructed language no matter what. It should be noted that the Esperanto these people speak is different and more complex than the Esperanto of the parents, who learnt it as adults. 

In addition to precision, jargon is used to mark status and what social group(s) you are a member of, or wish you were a part of. Such jargon then coincides/overlaps with sociolect. For instance, the Alice of "Alice in Wonderland" used "looking glass" for "mirror". See also U and non-U English at the Wikipedia. 

Logic is a big deal in linguistics (especially semantics), and unification is an important idea behind several grammar frameworks like HPSG and LFG. I did both a bit of math and statistics to support my Master's, and unification was only taught at the ling. dept. Anyway, browse through the table of contents of "Mathematical Methods in Linguistics (Studies in Linguistics and Philosophy)" (1990) by Barbara B.H. Partee et al., that should give you a good overview. 

Yes, there are two main approaches to machine translation. Rules-based (as in babelfish) and statistics-based (as in google translate). I don't think that statistical methods are ipso facto better than rules-based methods in all things. You get different types of errors, as you'll see if you try to translate the same text to the same language with both engines linked earlier. It'll depend on the recipients what types of errors still leave the text understandable. I use Babelfish when I need translation-help for English -> German because I consider the errors of Google's translation service to be worse than those of Babelfish, for the kind of texts I need to translate. Rules-based approaches takes time, money and trained personnel to make and test the rules. Statistics-based approaches needs large equivalent* corpora for each pair of languages it can translate to and from. Sometimes the former is easier to arrange than the latter, because of for instance copyright. For languages with few speakers rules-based is the only possibility since there exist no suffciently large corpora. That rules out statistical translation for most of the languages on this planet. * Equivalent corpora: same genre of text. Corporas that have the same texts in different languages is even better but there aren't enough of those.