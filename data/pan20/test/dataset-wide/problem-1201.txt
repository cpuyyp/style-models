Our database Server is being hosted by another company. They provide a file level backup that occurs once per day. They also allow for Log Shipping. My company is taking advantage of their Logshipping option as a major part of their backup and recovery solution. I would like to have a better backup system that would allow us to restore data up to the point in time of failure. Typicly this would be full backkup model for the database and backups of the transaction logs as well as the full and incrimentals. However, the vendor has told me that providing us with transaction log backups would cause issues with the Log Shipping. Is there a backup strategy that will allow me to restore to the point of failure and also use logshipping? 

SQL SERVER 2008 R2 When attempting to connect to the default instance of sqlserver on our local server from SSMS on my desktop I'm receiving the following error. 

When using RESTORE FILELISTONLY to restore a database using code the logical_name for the files is coming back as 423. However when I do a manual restore of the database the logical file name appears to be 3497. I'm running SQL Server 2008 R2. Has anyone run into anything like this before? How do I repair this? 

This did not occur near the time of the Snap recreation job. Several of the table data extracts (that point at the same snap) prior to this one worked fine. And the job appeared to recover for the next table data extract. I'd like to find out what caused this job to fail. Any help on places to look would be helpful as it doesn't appear that there is any reason for the snapshot to have been offline. 

Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: 

When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

So we're wondering if there's a lower-cost solution that would store as smallint but expose the colunms as ints to readers. Like this: 

We have an application user which has been given read write to the appropriate databases. This user has the ability to query the sys tables by default. I've also heard that there are times when this user needs to be able to obtain a listing of tables and columns that exist in the database. Are there any security risks to having the user have read access to the sys tables? 

when I run the procedure to do the restore. Here is the procedure code. Can anyone spot what my error is? 

The short answer would be when you have no set based way to get the information or have exhausted all other options and rejected them based on their performance being worse. The reason for avoiding scalar sub queries is that they force SQL Server to perform the scalar query once per every row that is returned. There is almost always ways to get the information back using a set operation that will perform and scale better. 

I'm receiving the data 1558.39999999999 However, investigation of the data shows that all data points only go out to two decimal places. In this case it should be impossible to receive the number above. using another query: 

We are migrating our SSRS reports server database from SQL SERVER 2008 R2 to SQL SERVER 2014 and as I understand it this means all SSRS reports will receive a new id. Hence any SSIS packages that reference a report would break. Is there a way to identify all SSIS packages that make reference to SSRS reports? 

Our SQL 2014 server has a blazing-fast tempdb drive (2800 IOPS) but a much slower data drive (500 IOPS). Our application runs a few long-running reporting queries that are I/O-intensive and we'd like to avoid them starving our server for I/O capacity when they run. Ideally we'd be able to limit these queries to 50% of available I/O capacity. Unfortunately SQL Server Resource Pool's IOPS throttling is not percentage-baed nor volume-specific. If I limit to 250 IOPS, then it will unnecessarily slow down performance of queries that make heavy demands on tempdb. Slowing down these long-running queries if the server is busy is OK, but slowing them down by 10x+ if they need lots of tempdb access is not OK. So we're looking for workarounds that will defend other queries from these lower-priority, long-running queries, but without unnecessarily hurting performance of these long-running queries if they happen to use lots of tempdb. It's not practical to change the queries themselves to reduce tempDB usage-- these queries are generated by a custom reporting feature that may sometimes generate really complex query plans that spill results to tempdb. So far the best idea I have is to remove IOPS throttling and instead use the "Importance" of a workload group to defend the rest of the server's I/O capacity from these queries. Is this a good solution to the problem I'm trying to solve? What are the pros and cons of using Importance? Or is there a better way to achieve our goals? 

I'm attempting to determine root cause for an issue that caused an outage for my company. Essentially the site connections to our database server were timing out after not receiving a response for 5 min. A check of perfmon counters showed that the database server was using about 10% of processor, memory was under utilized, and our hard disks were not being stressed. A check of long running queries using the following statement showed that there were a lot of queries queued up with a status of suspended the longest of which had been running for 50 minutes. 

However when I try to pull the sqlhandle out for use in sys.dm_exec_sql_text() I get an error "The handle that was passed to dm_exec_sql_text was invalid." Or no value is returned. If I pull the handle from the xml and put it into the query manually it returns just fine. What is the proper way to store a varbinary value that comes from a file so that I can pull the value out and use it with dm_exec_sql_text? 

I have a sql server user that our application is connecting as. The user has been granted db_datareader and db_datawriter as well as Grant Execute on each stored procedure that it needs to use. These are the permissions I see when I veiw the usermapping on the Login and when I view the user under the database. Given these permissions I don't think the user should be able to alter a procedure; however, when my developer logs in as that user they are able to update the stored procedures. Am I wrong about what these permissions allow? Is there someplace I can look to find out about permissions that may not being showing up in the UI? Or do I need to explicitly deny alter on the stored procedures for that user? 

When setting MAX_IOPS_PER_VOLUME in a Resource Pool, what exactly does "volume" mean? Specifically, how many "volumes" would be the following cases: 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten? 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it. 

A multi-billion-row fact table in our database has 10 measures stored as columns. The value ranges for some of these columns won't ever be above the +/-32K range of a . To save I/O, we're investigating whether it's practical to store these columns as instead of . But we're concerned about what problems might crop up from doing this, including: