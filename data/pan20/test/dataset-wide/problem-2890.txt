In the spirit of Nietzsche, if your scepticism is preventing you from functioning, you need to take it farther. If you cannot accept that truth exists, you should not rely upon it for making decisions. You need to find a motivation for action that matters. The two he puts forward, which to my mind are really the same, are Power and Art. Which of these does his racism offend? Does it undermine his effectiveness? Or is it just tacky? Yes, that first question is framed the right way around. If you expect him to act in ways that are not playing to his strengths, you are being a hypocrite: You have some devotion to some morality based upon the religious history of truth and the 'slave morality' that it leads into. So you should expect him to do what is good for him. Is racism good for him? If you think so, you need to just acknowledge that you are fighting with success, and nothing succeeds like success. If not, you should be able to make the case based upon the odds of being on the losing side of history. Stereotypes do not predict outcomes very well, and racism is ultimately not really a moral position, it is bad science with a religious anchor. If it is in his best interest, what in it offends you? How does it lack art? Are you adopting the prevailing modern taste for equality and peace? If so, again, you are being a hypocrite. Modern politics is stil Enlightenment thinking, highly over-constructed, not open to deep skepticism, and if you lack faith in "Truth", you need to discard it. So aside from the trappings of cultural pressures, what in his racism offends you? What is the aesthetic you want to adopt toward politics, and what would be its standard of judgement? For example, I can look at Donald Trump's racism and sexism, and hate it because he is presenting a false image of masculinity. (As a gay man, masculinity is something I value for its own sake, at least in other men.) His position is presented with bluster, but it is ultimately cowardly. It is not pride or tradition he is appealing to, it is simple fear. If you believe a group belongs in a subordinate position, but you fear you cannot keep them there, then they don't belong there, and they won't stay. So accept history. Do what works, but don't whine about it. Letting yourself be afraid of them, and being dishonest or duplicitous about it debases you. It is tacky. 

I think the general notion I would call upon here is that "Whoever does something first, very seldom does it best, and almost never does it most efficiently." But they will be the reference, as the source, for all time. Information is ecological. A major work of philosophy places ideas in the public domain that are then evolved and assimilated, and ideas related to or consequent upon those become broadly distributed and easy to understand. But when those very same root ideas are first presented, they need to be laid out in gory detail and expressed from multiple perspectives, or they will not be communicated. Once the notion is refined by passing through many hands and has become accessible by standard examples, reading the original presentations that considers them new is often painful and annoying. At the same time, those ideas are compromised by their assimilation. So you also cannot necessarily expect any secondary source to capture everything that might be relevant to you. The author will unwittingly put a given 'spin' on things that may be foreign to the original. Unlike philosophy, one very seldom reads mathematics in the original, which makes it an even better example of why doing so in philosophy is often not efficient. Turing's conjecture of the existence of the universal machine, for instance, is painful and bizarre, given that we all use computers daily, and the people most likely to read it have written in assembly languages and understand mechanical logic quite well. We have no trouble accepting his premise, and the actual construction is hard going for no gain. We want to get on with the actual ideas, as if this wasn't one. Reading Galois' notes from the eve of his death is almost impossible, he is so intelligent that he makes tremendous bounds, and you cannot follow him, in certain places, but so little of basic combinatorics existed that in other places he torturously belabors points we all know. You cannot choose a speed at which to read. Some primary-source philosophy is equally pointlessly demanding. (In my experience, the closer to math it is, the more likely this is to be true. I have found it easy to read Plato in the original, but not Kant or Hegel, despite that Greek itself has always been much, much more demanding for me than German was at the time I was trying to do such things. I think this is not just the form, but the distance of the ideas from delicate processes and intricate balances.) 

@stoicfury is right, you answered your own question. The rules of deduction and the dogmatic contents have to be considered a priori truths for this kind of mind. Don't they? The form the basis for its reason, and they will have had their effect before they could possibly be modified by experience. But let's go down your list: 

Capital is held wealth in a form which can be liquidated or invested. So capitalism is the preference for control in the public sphere to devolve into the hands of those who hold this variety of wealth. This is distinct from the emphasis on Real Estate, as a means of distributing control. Those medieval trades took place somewhere, and the participants' ability to make decisions were always trumped by a local Lord, however poor he might be in money or materials, he was in control because of title to the land, however little value that land might have as capital. Neither could they set up their own rules, nor could they just move and do business where they wished. (Jews, "gypsies" and expatriates in general, and the Silk Road in particular, were something of an exception, continually lying in a sort of DMZ between various different Feudal systems). To the degree we are not free of jurisdictions or of Lords and heredity, given inheritance, traditional jurisdictions, etc. in Western Law, this contravenes strict Capitalism. The economic system of the West, therefore, to the degree economics is subject to a set of controls that has vestiges of Feudal Law (local authority and bylaw), Civil Law (public police and statutory enforcement) and Common Law (our legal procedure itself), is not pure Capitalism. It comes close, for instance at the nadir of government effectiveness that led up to the 'Robber Baron' era, but Progressive movements always arise to reverse this trend. To apply this to another of your examples: Antonio had to go to Shylock because the Church said financial speculation was a sin and could enforce its will and have any Christian who made high-risk loans formally ostracized. They derived this power largely from Feudal Law. Today, anyone can be a venture capitalist, and he could have found a less bitter man to borrow his money from. But it would be someone whose wealth was liquid, and who therefore, in our system, has substantial power -- much more than an equal quantity of wealth might have granted Shylock. 

Apocrypha has it that Quine used to demand he be given the collected whole comprised of the parts of a chicken (i.e. what a chicken consists in). Given the normal thing, a slaughtered and partitioned chicken, he would complain that it was not a collected whole. Given a live chicken he would complain that these were not the parts of a chicken, this was a chicken. (Presumably given a dead chicken, he would claim this was not even a chicken, it was the corpse of a chicken.) It is impossible to produce what the chicken consists in, without including all the things beyond that that the chicken consist of. If you have any real object, and it is not a chicken, you do not really have everything that a chicken consists in. But a real chicken contains the incidental aspects of the chicken, in addition to the aspects that really make it a chicken, so a real chicken is not what a chicken consists in either. Various logicians, including Quine himself have tried to make sense of this idea, going back beyond Parmenides and Socrates (in Plato's dialog named after the former) discussing whether a hand is one thing or many things. Is it really made up of the fingers or are they just incidental aspects of it? To me the entire issue is an overestimation of our power to define things with a single notion. We assume that there is some (extraordinarily complex) statement that would perfectly capture chickenhood or hand-ness. But we are wrong, the definition of a chicken, or a hand, is a convention arrived at by iteration and approximation. The notion of something consisting in its principal aspects is itself a weak approximation to how meaning works. The statement itself never means anything without example, refinement or reframing. So there is no room here for a rigorous definition. 

The maxim behind vigilante justice that involves action rather than mere prevention seems to be 'Everyone should enforce the moral rules for themselves.' This would scare me. Many people think they know what the rules of morality are, and I would not want them to impose that misunderstanding upon me. I am pretty sure, given his notion of nations and stable societies Kant would prefer that we choose the maxim 'We should choose who will enforce the moral codes and empower them to do so.' If that means you have to tolerate theft right under your nose, sobeit, the State monopoly on violence is more important than your pique. 

The 'multiple drafts' model that Daniel Dennet puts forward in "Consciousness, Explained" suggests a novel way of describing this disconnect: There is a problem created by the fact the brain is a massively parallel processor, and the memory has evolved primarily to turn everything into a story. Our memory, and thus our 'mind' has contents that take the form of narratives, or audiovisual scenes, which are the equivalent of verbal narratives. Narratives are basically serial. Qualia are referents in these narratives, which are already recorded at the time we experience them. But our brain, the machine that 'executes the mind' is not a serial device, it is a threshold-based neural network -- a massively parallel machine. The model he proposes for how these things interact is that the parallel process constantly edits the serial narratives. So 'qualia' and 'cognition' cannot happen separately. If there were no narrative, there would be nothing to edit. And starting to write the first draft of an experience narrative is itself editing. But the are entirely different things. One is the state, and the other is the process, in the brain's sort of 'pre-Von-Neumann' architecture. It is impossible to experience cognition, because the narrative needs to have a state in order to be experience. But cognition has no state information that is not recorded in the narratives themselves. There is no experience of cognition, there is the experience of the constant change of state: "This event was not the same by the time I had a clear grasp on it as it was when I chose to reference it. I don't seem to be able to reference it and have it be the same thing it was that I chose to reference." So from this point of view, all your processing is subconscious. Conscious processing is just subconscious processing that has a narrative recorded to explain it, so that it can be audited later. 

Statistics are not reliant upon having correct meaning in order to be true, and this is the salvation of the experimental method. You can look at existing data, determine what is statistically likely and then explicitly run randomized experiments and determine the odds of your results following the noted trend. This does not validate your theory, but it does verify its predictive power. Your theory is not true. It never will be. That is the nature of science. Successive theories in any domain tend to get more and more different over time. They do not converge to some stable truth. Human creativity and desperation increase along with our observational power. Still, we measure each supplanter's predictive power to explain what we have already observed. And we are safe relying upon the proven predictions. What you are relying upon, in measuring that likelihood is not your set of facts, it is the law of large numbers. The actual facts may be misrepresented, inappropriately framed, of may have many, many other reasons for simply not being true. For example, psychological theories are quite weak, and many generations of different theories along many disparate lines of basic theory have been used to explain the same reproducible outcomes. But the outcomes remain reproducible. And you can argue based upon the outcomes. If vaccination causes autism, then we should see the correlation. Not a retrospective one, but a real correlation in current populations. If you find populations who are not being exposed to the vaccines and are still seeing the same increase in autism, there is no reason to follow that predictive thread. It is not disproven, but it is unsafe to rely upon. Whereas the ability to suppress disease is safe to rely upon. Public policy should be safe. The problem with some of these positions, for example, global warming, is that we cannot run experiments that have real statistical validity. So there really is no version of the argument that is not thoroughly theory-laden. And the kinds of science that produce these arguments have a long history of terribly weak theory and vast, contentious impasses. Geology, the overall science that includes climatology, achieved its "DNA moment": its grand unification as a science, with plate tectonics -- after continental subduction was clearly identified and filled in the remaining holes in the approach. That happened after most of our current legislators had already left school. In such a situation, good science needs to have some humility. All we can argue is 'Shouldn't we avoid the possible worst case, especially if the cost is reasonable?' And for some people, the cost is not reasonable. 

This kind of approach is, in the tradition of Wolfgang Pauli, "Not even wrong." Like the dog with Buddha-nature. 'Nothing' is not 'vacuum'. If I came up to you and brought nothing with me, few drastic results are likely. If I came up to you and brought vacuum with me, you would likely die, as all your cells decompressed. In that sense, emptiness is a physical force, and empty space is a real thing, not a 'nothing'. At the same time the Parmenidean 'Nothing' as a supernatural construct also just seems to be a misunderstanding waiting to happen. One version of the Bogomil heresy goes "Deposed, Satan had nothing. He therefore rules the world. For nothing is more powerful than God." This may be deeply motivating to Satanists in a religious sense -- but it is not logic. Negation is a verbal convention, not a Category, as Kant would have it be, or some other deep force of nature. To me, the fact that it has intrinsic problems like Russell's Paradox indicates that it is not a reality, just a human convention. LeDoux's "The Emotional Brain" points out that we have one entire layer of memory processing that we share with lower animals which is, in a very basic sense, incapable of processing negation. It is a common theory that the gap in speed between this layer and our fully engaged frontal lobe leads to things like phobias, PTSD, and Tourette's syndrome. If negation were so basic a part of actual reality, we would not have evolved it so late that the system that implements it lags behind our more basic processing. -- Sorry to add so much later. I always get to that point where I think I made the answer obvious only to later find I have not stated it -- As the last thing we property evolved mentally, I think negation is not quite right, not complete. Trying to push evolution, humans have a bit of an obsession with 'nothing' as a concept. But urgency is not importance, and our occasionally-urgent feeling that these 'deep' questions about 'nothing' matter is misguided. If we look at this less obsessively, Krauss is free to work from a model that the world originates in vacuum, and it is obvious what he means by nothing is not what someone more careful means by nothing. But even that is silly. The oversimplification just makes him bound to an incomplete paradigm, one without a more basic notion of nothing, and not wrong. (Ellis can judge this paradigm non-falsifiable, and thus not science. From a Kuhnian perspective, that only makes it non-normal science, not inadmissible as science, because 1) paradigms are not falsifiable by nature, only contrastable with comparable alternatives; and 2) I do think there are clear alternatives, and that it is productive to consider those scientifically as well as philosophically. There are competing models, and I think those do have potential test cases. For instance, some predict very specific ways to send information backward in time (e.g. this crazy man $URL$ which would break down the notion that extrapolating time backward linearly to some beginning has value, and make Krauss irrelevant -- but still not "even" wrong.)