I search a db schema to store the process tree of linux every half hour. Platform: PostgreSQL on Linux. I am unsure where to split all the data into columns or to use a json column. Which one is better, and why? 

I have two postgres databases which should have an equal db-schema, but have not. I want to see the difference in the column order. Columns of table1 in Dev: id, foo, bar Columns of table1 in Prod: id, bar, foo Strategy 1: list of columns This is my current strategy to solve this. Dump all columns of the database like this: 

The size of the our PostgreSQL data directory is 100 GByte. Up to now we use traditional hard disk drives and hardware RAID 10. Which hard disk drives and RAID setup would give us maximum performance? The budget is at maximum 5000$. The load is like in most average databases. Maybe 1/5 writing (update or inserts) 4/5 reading (select). 

How can I dump this statement again? I looked at but only found a way to do a data-only dump or a schema-only dump. If I do the following, then I get both (CREATE TABLE and GRANT statements) 

Script creates some dummy tables Scripts inserts data to the tables Script does some queries Script shows matching values for random_page_cost and seq_page_cost Human or an automated system takes these values and updates the config. This step is not part of the question. 

One work-around: Change the port of PostgreSQL on . This way it is unlikely that clients connect to the DB during . 

I read this article about PostgreSQL performance on SSD: $URL$ These two configurations seem to be important vs Since both parameters need to match the particular hardware I wonder if it is possible to automatically detect the matching values? Update I have these steps on my mind: 

How to realize these improvements? Are there other things which could get improved? I am using PostgreSQL 9.3.13 

We have a PostgreSQL based system which is installed at 20 customers. Sometimes I run a SQL query over all systems. Up to now I do this with the command line tool like this: 

I am missing the ability to sort the output. I would like to avoid dirty shell scripting, and use SQL wisdom. How to get the N SQL queries into PostgreSQL, so that I can run ordinary SQL on it? (Sub question: Is there a matching tag for this kind of question? I mean questions where the SQL spans several databases?) 

In my environment most PostgreSQL queries are generated from http requests. In my case every http request has a unique header: X-Request-ID If there is a error or warning in the PostgreSQL logs, I would like to see the related X-Request-ID to see where the sql query came from. How to add an extra message like X-Request-ID to log messages of PostgreSQL? Related: $URL$ 

I had created table with engine BLACKHOLE basically the BLACKHOLE storage engine acts as a “black hole” that accepts data but throws it away and does not store it. Retrievals always return an empty result. I heard that we can retrieve the data by creating a new table same as the old table with storage engine as innodb or myisam. but i had tried that also but unable to get the result. Can any one pl help me on this issue to fix it. 

If you anticipate that your database file(s) will exceed more than 1TB in size you should configure the server to put the data files across multiple VHDs. How can spread my databases across multiple VHDs? Create one or more dynamic volumes consisting of multiple VHDs at the operating system level and put your database files on these volumes. Split your tables and indexes into separate database filegroups and then place those filegroups in different files (MDF + NDF) which are spread across separately attached VHDs. Note that this will not offer any benefit for transaction logs because SQL Server does not stripe across transaction log files but uses them sequentially. If you are migrated to Azure and want to compare the performance of your databases before and after the migration then take a SQL Server baseline(enter link description here) 

WITH (NOLOCK) is the equivalent of using READ UNCOMMITTED as a transaction isolation level. So, you stand the risk of reading an uncommitted row that is subsequently rolled back, i.e. data that never made it into the database. So, while it can prevent reads being deadlocked by other operations, it comes with a risk. In any application with high transaction rates, it's probably not going to be the right solution to whatever problem you're trying to solve with it. SQL Server 2005,2008 and 2008 R2 will support Nolock. Pl look on the below link $URL$ 

If the tables were dropped, ApexSQL Recover can recover them even from databases in the simple recovery model. ApexSQL Recover can recover both table structure and table records On the other hand, ApexSQL Log cannot recover records lost when a table was dropped, it can only recover the table structure In case the records that were lost using DELETE (not DROP TABLE), both ApexSQL Log and ApexSQL Recover can help The advantages of ApexSQL tools over recovery to a point in time is that ApexSQL will recover just the tables you specify (creates CREATE TABLE and INSERT INTO scripts) , while a point-in-time recovery will roll back all the transactions that happened in the meantime A DROP TABLE statement marks the MDF file pages used by the dropped table for deallocation. These pages are actually still in the MDF file until overwritten by new operations. To prevent new operations overwrite the data necessary for successful recovery, we recommend creating a copy of the original MDF and LDF files immediately 

How to solve this is a different question, not this. This question how to call this in general. In this example is about sending mails. But the same problem as soon as you do modify something in systems which are outside the transaction boundary. Is there a name for this problem? Roughly the same problem arises if you want to import files from a directory. If you delete the file inside the transaction, then the transaction might fail and the file was deleted but never imported. Or you delete the file after the transaction. Then the delete of the file might fail and the file gets imported a second time. I don't want to reinvent a solution for this. That's why I need the matching term for this problem. Then I can read some papers and learn what's "state of the art" in the year 2018. 

We use check_postgres.pl to monitor our database. We use this to check the count of the locks: $URL$ We often see more than 150 locks. The question was: What is going on? We patched the script to output this sql statement, if the lock count was exceeded: 

Since I only used plpythonu up to now, I solved it like this. It works, but I see two things to improve: 

I want to understand a database layout. It has 180 tables. I only know the high level use case: Owncloud (File Hosting Server. Can be compared to DropBox). How to understand the database layout? How to identify the most important tables? Is there a tool which helps me to understand the layout? At the moment am interested in two facts: 

Is there a way to write this statement so that it is easier to understand? Yes, "easier" is a matter of taste. In this question easier is means: prefer sub-selects and union over joins. In this context it does not matter if the performance gets slower. 

"Avoid redundancy" is important to me. I want to store the input data for configuration management in a relational database. My input data: 

$URL$ row2: I think every person has exactly one date of birth. I don't understand this row4: I think every person has exactly one birth place. I don't understand this. What am I missing? 

If I would store this data in YAML, then I could use variables to avoid repetition. AFAIK this does not work in relational databases. I could store this in DB, but AFAIK I need to evaluate it myself: 

If you only look at the database everything is fine. You have transactions and if somethings goes wrong everything gets rolled back. That's nice - I like this. BUT: I want to send mails. Now I am in trouble because I can't rollback. example: 

After pg_upgrade one check fails. This SQL gets executed to list tables and their sequences. In comments to this question it is called "above sql". 

This error happens when Full back up is not restored before attempting to restore differential backup or full backup is restored with WITH RECOVERY option. Make sure database is not in operational conditional when differential backup is attempted to be restored. Please have a look on the below blog. $URL$ 

You can create a username and password also provide grant access to them. so that all clients can access the mysql from their local by using the username and password provided by you. 

The error (2003) Can't connect to MySQL server on 'server' (10061) indicates that the network connection has been refused. You should check that there is a MySQL server running, that it has network connections enabled, and that the network port you specified is the one configured on the server. Start by checking whether there is a process named mysqld running on your server host. (Use ps xa | grep mysqld on Unix or the Task Manager on Windows.) If there is no such process, you should start the server. If a mysqld process is running, you can check it by trying the following commands. The port number or Unix socket file name might be different in your setup. host_ip represents the IP address of the machine where the server is running. 

Your BHCR is 98.92% its perfectly Good and Great. If the value of the Buffer Cache Hit Ratio (BCHR) counter is "high", then SQL Server is efficiently caching the data pages in memory, reads from disk are relatively low, and so there is no memory bottleneck. Conversely, if the BCHR value is "low", then this is a sure sign sign that SQL Server is under memory pressure and doing lots of physical disk reads to retrieve the required data. Prevailing wisdom suggests that "low" is less than 95% for OLTP systems, or less than 90% for OLAP or data warehouse systems. You can also test the true nature of the BCHR 

Try the above query in terminal or check the below link to repair table or databases via phpmyadmin $URL$ 

I had some changes in that table and it got executed. mainly the primary key. pl go through the same, 

In create table you have set teams as primary key, and also you are aware that primary key does not allow duplicate values. 

Check the insert query for teams field you have mentioned values as 1 and if you insert the same value for second record it will behave as below. 

It will ask for password don't enter anything first time because it will use blank, n just press enter you are done. N later you can set password too...:) 

Error Cause: The control file change sequence number in the log file is greater than the number in the control file. This implies that the wrong control file is being used. Note that repeatedly causing this error can make it stop happening without correcting the real problem. Every attempt to open the database will advance the control file change sequence number until it is great enough.