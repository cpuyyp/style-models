What you want to look at are the words with an "" in front of or at the end. The "" is my token that color is involved, and a modifier is needed. If the "" is before the word, then I need to add a color before it. If the "" is after the word, then it is a color word and it needs a modifier. I also created a table of colors only for the times I need to add a color to the front of a word. The word can be a noun or a verb. "White lightning," depending on how it is used, can be a noun or a verb. If you said "White lighning struck a tree," it is a noun. "Dear ole Fred, lay dead, when met by white lightning" could be considered a verb/adverb use. The second thing is the "+" modifier. In the same DATA statement, the word "voice+" means it needs a modifier after it. I created a specific table for this type of modifier. You will see in my code that since I used a random generator to start either with an article or verb, every word is in or converted to lowercase. I had to create another modifier, "@" that is placed in front of proper nouns or words that must be capitalized, no matter where in the sentence they appear. @moon and @sun are two examples in my DATA statements. The third thing that shows promise is to create files that you can read into your VAR tables, but with a twist. I am currently creating theme-based poems. So I created files with words that are used in a particular theme. For example, "hope" is one of the themes I am developing, so I am using everything from other poets to a thesaurus to even the Bible to gather as many words, Articles, Nouns, Verbs, Adjectives, and Modifiers that fit the theme of "hope" along with some generic words that would work. This is where you asked if it would be better to have .CSV instead of tables. It will add flexibility to what your computer program writes about. You would have a VAR table with just themes, and .CSV file names for each theme that holds your vocabulary for that theme. You would first randomly pick a theme, load your tables, then can randomly pick your words. By moving your tables out to .CSV files, you allow for many different types of dictionaries. And you can even get as complex as having a main theme and a sub-theme and load the appropriate dictionaries. Another improvement I demonstrated in my code is that sentence structure does not always have to be article-noun-verb-adjective. It doesn't even have to always be subject-noun-verb or noun-verb-subject. Try starting one of your lines with a verb instead of an article. Then you can use verb-noun-subject or even article-verb-noun-adjective. For example, "Stepping on Mike's foot felt great" shows how a verb can start a sentence. I mentioned subject. For now, you can use nouns, but you may want to eventually create another table with just items that are considered "subjects." "Science" is a subject. "A Dear John letter" can be a subject rather than a noun. A "Divorce" can also be a subject. I tried to keep to the same logic you used, but I wrote as vanilla a BASIC program as I possibly could so those with other dialects need to change very little. Like statement separator (!) or some of the statements for lower and upper-case. With minimal changes, I had this working in techBASIC. But Javascript, being a C-based language, there are enough similarities to BASIC that you should have no problems adapting the routines in my code below into your program. Not a plug, but incase there are those who never heard of SmartBASIC, it is an app (less than $5 in iTunes) that runs a pretty robust BASIC dialect on iPad and iPhones, and comes with the ability to create Xcode apps for distribution on iTunes. You also do not need access to the internet to run and test your programs. This code works when run in SmartBASIC and is not broken code. Here is it with the changes I made (commented): 

2) Error handling function does not handle any incorrect formatting. Any line which is not in the format name=value will not be imported correctly. You can explode into an and check if elements are defined and exactly two. 3) A more powerful config reader This is a good exercise, but ini files have a more complex format that seems to be handled by parse_ini_file function. 

An alternative to this approach is to have custom map these properties when transferring data model -> view model and vice-versa (so, this also supports data persistence, not just data fetch). Actual mapping As already suggested, AutoMapper was specially designed for this kind of chores. Its usage is simple as: setup the mapping: (this is deprecated in the last version, as described here (and also mentioned by the compiler, if last NuGet package is referenced)) 

I just wrote this pool to avoid calling and when I have some code that frequently allocates and deallocates chunks of same-sized memory. I would like to know if there are any bugs I didn't notice and what would be the best solution to achieve this goal. I'm using some small functions that aren't really required because I find the abstraction nice when using the code later and because I noticed the compiler will optimize them away when link-time optimization is turned on. I'm using about the same code I posted before for the stack. 

Complexity should be as all dictionary operations are done in . If your documents were copied from some other data structure to your response, I think it is better to construct the dictionary from that data structure and obtain from the merged data. Otherwise, you have to use a loop to remove documents already merged (somewhat more convoluted than version). 

I would also mention a little about your naming. C# uses a CamelCase notation for most identifiers (class names, public methods etc.), so you should try to use also for your structures: 

I am not well versed in coding node.js as I'd like to be (I can read and debug, but learning to program in it is still ongoing) and I did not see anything that said I could not present an answer using a compatible language, so I converted your code into SmartBASIC. If you were expecting node.js code, I will understand. I needed a quick way to show you several areas where you can improve on your own program, and suggestions that will smarten your program's AI. For yes, SIRI seems to be human like, it is not perfect. And it is harder to have a computer write something in one of the arts, whether a poem, lyrics or even a short story. I did notice one slight error. In the statement below, you have a period instead of a comma, which will get you strange errors (had been.will be): 

I have also added some defaults to make it easier to use for the caller. 2. Changed function to support back-off [edit] Changed from to as suggested by 

' advice is applying DRY principle to your code. It can be extended to also cover and functions, since they are almost identical: 

I will try to cover several points: 1) Naming Try to use homogeneous naming. Pascal/Camel case seem too be used most often, so stick to it. E.g. should be , should be . 2) Improve readability of your declarations Your table variables could be declared like this: 

I was reading about opaque pointers and decided to try it with queues. Please review my code and let me know if there are any errors and how to improve code quality and performance. It uses lines to enqueue and dequeue. All lines have fixed sizes and dequeue/enqueue never share a line. New lines are added only when all lines are full, not counting the dequeue line. The lines are stored in a linked list. queue.h 

I wrote some code for linked lists and would like to know what things could/should be done differently. I'm using an index to keep track of the most important information and have 2 sets of functions, one to use the list just to point to data and the other to store the data in the list. Here's the code so far: 

I experimented with several ideas, and several show promise. I incorporated them in my SmartBASIC program, which I commented thoroughly. Your dictionary is limited, but I found out that this is not always a bad thing. I experimented with nouns, verbs, adjectives and articles where I created several hundred in each, and while it was still hit or miss, it was more miss than yours hit. Quantity does not always equate to quality. One of the problems I noticed is when you want to use a color. Rarely does a color stand alone, nor does it always work with every word. A "white beard" is fine, but what is a "white crash?" But a white crash can be poetic if the theme is right. That later. So with colors, I created in my table colors with modifier tags. These are color words with a tag at the end so the program knows that another word is needed after a color word is used. Here is a sample DATA statement that you should be able to convert to a "VAR[" Table: 

Besides the naming that should more C# friendly (PascalCase), the application should have a clear separation between data models and view models. Data models are usually generated automatically in database first scenarios or POCOs classes defined by the programmer (code-first approach) and should be separated from view models or data returned by the service layer which usually consists in aggregated information. Ok, for your particular case, data model is : 

Other aspect is the harcoded number of possible graphs. Are they so different that they must be defined one by one? Do they share common functionality? 

I'm using these 2 functions to handle file input/output and would like to know if there's anything that should be changed. For content retrieval, enough space is allocated and the file content is copied. To make sure saving a file either saves all or nothing, I'm writing to a buffer then renaming it after all content is successfully written. I'm particularly interested in knowing whether these functions are safe or not since is considered dangerous. I read the main risk is a file with the name returned by being created before we are able to do it, so I'm using to make sure it doesn't happen. Is this enough or there are other security concerns that I'm not aware of? files.h