That should work fine, however, I'd probably use an A record, rather than a CNAME record to save that extra lookup. Also, bear in mind that this will resolve them to www.domain.com. but not redirect. So, for example, if a user starts accidentally using ww.example.com - they will be lead to believe that this is the correct web-address, which could cause issues further down the line. You will need to resolve this on your web server, though. 

I apologise if I'm oh so wrong here, but I don't think there's any such thing. You've always got to have a TLD / DNS root or whatever. I think what you're used to seeing is companies having things like $URL$ - HOWEVER, this is really resolving to $URL$ This is done by the machine knowing which domain it's in and /or by using the "DNS Suffix" in the network connection settings. Standard practice tends to be to create a ".local" zone for this stuff, otherwise it just goes under the companyname.local name like in your first examples. In other words, add projects & admin alongside the others and users will NOT have to enter .company.local as long as they're on the domain or have the default DNS suffix entered. 

No, because snapshots are block-level delta disks so this has no way of working. Think of a snapshot like a "transparency" that goes over the original disk. If the original changes, then the snapshot will make no sense and the machine will be in an inconsistent state. This isn't a small topic, but essentially, when a snapshot is made - from that point onwards the original disk is "sealed" and changes are written to the snapshot. For completeness, I know for a fact that you can do what you want, manually, by copying snapshot files and altering the meta-data, however 

Not with any of the vanilla Windows tools, I'm afraid. This is where classroom management solutions (Tutor, Impero etc) come in, 

And all that happens is that the very last policy to execute will have the final 'say' on what the final setting with. With the exception of 'Not Configured' where no changes are made. 'Not configured' is the default for all options within Group Policy when you create a new GPO. So, if your current policy has a setting that is "Enabled", you need to create a GPO with the same setting "Disabled". 

Have you tried disabling windows firewall on the server? Can you ping (Or otherwise confirm connectivity) the proxy from the machine? What error message do you get from Internet Explorer? 

It looks like a U.FL connectors - they're commonly seen on embedded wi-fi cards in laptops etc. If it were me, I'd be looking to take an internal antenna out of a laptop with the connector intact, and then modifying it. $URL$ 

There's absolutely no vulnerability here - you simply need to ensure your shares are secured properly. 

But, as that's a computer policy, will it not apply to all users including administrators? If so, is it possible to exclude admins somehow? 

Every other company manages to get people to use their own user account and password, and you should be no different. 

The system proxy (This is the one you see when in the Internet Options panel) An application embedded proxy. Obviously this is reliant on a developer implementing such a feature. 

This is probably nothing to worry about - this will occur when the original local profile folder doesn't unload or is otherwise corrupted. Basically, the first profile will be 

You can't have two default gateways because it's an oxymoron. Another phrase for Default Gateway would be "Gateway of Last Resort", and two things can't be last. It may help you visualise this if you remember that Windows routing (And that includes your default gateway(s)) is a global setting for all NICs, IP's and interfaces. You may need to add the and option to the command. Please try with those options and then post the results of here if that doesn't help. Additionally, if you want this setting to persist across reboots, you will need to add after : 

The first option merely stops a particular logical drive from utilising it. The third will allow the cache to function, but you'll lose the battery backup which is inherently dangerous. 

Even if you can install it, how do you expect to use it? If the server's that important, do you really want to be administering in a foreign language? 

As this is the hive for the Local System user account, despite its confusing name. (MSDN Reference) To edit the Default User Profile, you need to change the NTUSER.DAT from the Default profile directory (Location varies - C:\Documents and Settings\Default or C:\Users\Default etc) You can do this by using the Registry Editor as follows: 

How on Earth do you expect these 2 servers to share information if they can not talk to each other? Serious question. The answer is to set your network so that they can communicate properly... Serious answer! 

Not in hindsight, no. There are a ways of auditing in future, but unless you have previously configured some kind of monitoring then, no, this will just remain one of those mysteries. 

I think you should go the multi-VLAN route - and not just because of the DHCP server issue. At the moment, you have one big flat network and while to some degree, users should be expected to take care of their own security, I'd personally find it a pretty unacceptable setup. The only switches that need to be managed are yours. Beyond that, you give each apartment a single port on a specific VLAN - anything downstream of that will be completely unaware of the VLAN and you can function normally. In terms of your switches - the switch-to-switch ports will need configuring as trunk ports and you will need to be consistent with your VLAN ID's. In other words, VLAN100 MUST correspond to VLAN100 everywhere else on the network. Other than that, you can set up a "Router-on-a-stick" configuration, with each VLAN (And it's associated pool of IP's*) configured only to route back and forth to the internet and NOT to other internal networks. *I couldn't think of anywhere else to stick this, but remember that ideally you should be giving your VLANs their own pool of IP's. The easiest way to do this is to keep one of the octets same as the VLAN ID, e.g. 

This isn't really a XenServer specific question. XenServer comes with XenCenter which is its GUI management tool, but how you connect to your server is completely situation dependent. XenCenter connects directly to XenServer using its management IP, but you certainly wouldn't want a public facing IP. A VPN would be ideal for this, but again, it all depends on what facilities you have at your data center. 

You can't, because there's nothing to say an application has to register with .NET or anything. It just uses it - the application doesn't even need to be installed. Besides which - having multiple version of .NET is absolutely fine. 

Sorry if I'm teaching you to suck eggs, but are there other EXEs associated with the application (Do a search in all of its folders). That's normally the cause in my experience. You might also try using a path exception rather than hash rules. 

Given that you've tried two techniques without success, I'd say this is almost certainly going to come down to a permissions issue. Go back to using Group Policy Preferences - you can then review the event log to see why they're failing. Edit: Please don't use the Default Domain Policy for this stuff, if you break it you'll be stuffed. Create a new GPO and link it as appropriate. 

If it's greyed out then it's almost certainly being overridden by a Group Policy setting. I can't think of any other scenario where the GUI would not allow you to select the desired setting. 

There is a built in "Event Log Readers" group on the local machine. Add your users to that. Of course, if you want to use an AD Group, then just add the AD Group to it. You could also use Group Policy Preferences to add users and groups to this group on your domain computers. 

Fundemantlly, your AppPool is wrong a different .NET level than your development environment. You could try changing this, asking your provider to change this or you could try removing that line, changing to or perhaps simply Googling the problem. 

is ultimately only ever going to be relative. However - you need to be aware that failover / HA will only protect against host failures - not Operating System / Application level failures. It's key that you take into account the different failure modes and the different ways to protect. For example, HA will also require shared storage - this in itself could become a single point of failure for your server. You need to weigh up the likelyhood of such failures along with how detrimental downtime will be. 

I've never seen this particular issue, but you can sort out file associations with Group Policy Preferences. Either: 

This is going to get closed, but it's a design decision like anything. For example, a 10Gb NIC is only useful if you have a 10Gb port to plug it into. 10 1Gb NICs may plug nicely into your core switch. Additionally, you'd need two (on separate switches) for redundancy. Taking your virtualisation route, it's very useful to have multiple NICs because it allows for more flexibility in carving up your network. VLANs are great, but ultimately you're forcing everything down one pipe and it's easier said than done to manage that traffic. And from a VMware specific standpoint, it's far far easier to manage your virtual networks with multiple separate NICs. For example, you could have 2x for iSCSI storage, 2x for virtual machine production VLANs, 2x for backups (So as not to impact on the network if you want to run daytime backups, or the service is 24 / 7) and 2x for hyper visor management. That's 8, independent NICs right there that I can absolutely hammer without affecting the other services. 

Computer config gives you full configuration options similar to manually changing it in Folder Options on the machine. User config just gives you an "Execute" option which you can set as default. Computer config would be the preferred - just go onto a working machine and duplicate the settings. I think this would work for you, but you'll need to experiment: 

WireShark sits too low in the network stack to have any information on which processes sent or received traffic. The trick is to know something about what you're looking for - it shouldn't take that long to isolate traffic by Source IP, Destination IP, Source Port or Destination Port. 

Just follow the blooming best practices from the vendor. They're there, written in black and white - I don't seen an addendum that says "Or, just disable a load of services if you don't fancy following the above." Or, to clarify, you may be putting yourself into an unsupportable situation. 

I respect you for thinking critically and trying to learn, but honestly, you should work on your research skills. A simple Google would have taken you to Wikipedia which has this excellent explanation of why it's correct, and how it happens. Source: $URL$ 

Yes, absolutely, this is the very foundation of Group Policy hierarchy. Group Policies are applied in the following order: 

Well, the built-in C$ share is an "Administrative Share" - so the clue is in the name there. If they really need access to that one, then I think they need to be in the Local Admins group or similar. You can add users (Or Groups) to a PC's Local Groups by using Group Policy. Alternatively, you could create a specific share on each PC under whichever Folder he needs to access. That's a manual task on each PC. 

MAC addresses are only relevant on a local subnet - your VPS can't see your remote devices MAC address. In short, you can't achieve what you're trying tobdo. 

I'm not sure about your original question, but this is exactly what Group Policy is designed for. Create a GPO and either apply it to the relevant OU's or apply it higher up the tree and use Security Filtering to only include a Security Group containing the people who need this. This will be neater and more manageable than manipulating the individual user objects. 

I think to achieve this, you need to do it with groups, rather than individual users. For example, the Server Manager task, which causes the Server Manager to appear on start up, is configured like this: In particular, using the "Administrators" group, and configured to "Run only when user is logged on". So, for your case, simply create a group called "ProcessExplorerUsers" or whatever and assign that as the user of your task. I presume you've already found the "At Log On" Trigger. 

I think you're technically right in thinking that you can't set it to run every 15 minutes after a boot, but why does it matter so much? E.g Will run the task every 15 minutes of every day. You can add in a new schedule alongside it to run on startup aswell. Worst case scenario there being that the task may run twice in less than 15 minutes for the first time after boot. If that really is an issue, then the only alternative way I can think of is to come up with a script which runs at startup which creates the new scheduled tasks. You may be able to use this page as a starting point: $URL$ So, to clarify: 1) Server boots 2) Scheduled tasks runs batch file 3) Batch file deletes the previous boots scheduled tasks (You can use an ID number to differentiate) 4) Batch file creates a new task to run at 15 minute intervals today 5) Batch file runs the task You may be able to skip some of that by just having the bootup scheduled task run the 'real' task for the first time. It will then run every 15 minutes from that point. However, it still doesn't preclude the possibility of running it more than once in 15 minutes.