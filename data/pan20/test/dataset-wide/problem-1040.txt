In this case, the table has 22,000 rows, SQL Server estimated that you are going to use 7,700 of those rows, and you actually only used 1,827 of those rows. Because SQL Server uses statistics in order to allocate ranges of rows to threads, the poor cardinality estimates in this case are likely the root cause of the very poor distribution of rows. Thread skew on 1,872 rows may not matter much, but the painful point is that this then cascades down to the seek into your 1.5 billion row fact table, where we have 30 threads sitting idle while 600 million rows are being processed by 2 threads. 

However, the output for should show in the field in this case. I've seen this a few times, so I think that a value is more likely reason #1 in your case. Reason #3: The query plan XML is too complex This doesn't seem likely in your case if the query text is as simple as your example. However, it looks like there is also the possibility of a query plan being shown due to the XML being too complex. However, from looking through , it looks like this case is handled with a message and instructions to convert the raw XML into a .sqlplan file. 

Based on the documentation, it does not seem like -1 is an expected value for . But it does seem to appear somewhat frequently, and the procedure does not make an attempt to handle this situation. In my testing, I'm not even sure if it would be possible to do so; even when calling with the default values (to find the whole plan, not the specific plan based on the offset), I found that no plan was provided when was -1. I think there is the possibility of a situation where the request exists, but the plan and current statement offset are not yet available. I do not know the full explanation for this behavior though. Since a is available (at least in my cases with a plan), I don't think it's the case that plan compilation is still happening. Reason #2: It took more than 5ms to find the query plan The full implementation of actually uses a cursor to look up each plan one at a time. There is a of to avoid delaying the entire procedure too much in case a lock is encountered when looking up a particular plan. 

How about the following query? This query will take the exchange on the query date if it exists. If not, it will find the most recent exchange date prior to the query date and use that date's exchange rate. 

No, they are not necessarily "treated the same by the database engine." A test below shows that you might get different query plans. In many cases, the difference between query plans may not matter for you. But in some (likely rare) cases it could matter significantly. For example, if SQL Server has a very poor cardinality estimate for one branch of your UNION, calculating the unique rows within that branch could result in a spill that would not have happened if the query only calculated the final set of unique rows. 

What are the side-effects of CHANGE_TRACKING_MIN_VALID_VERSION? As far as I can tell from the documentation, there aren't any "side-effects" that will meaningfully impact you. As Aaron mentions, you'll get the same error if you try to use in a function. In both cases, I would suspect (but am not sure) that and are accessing and/or modifying an internal data structure (e.g., a random generator in the case of ) and SQL Server is biasing towards caution in rejecting their usage in a function. 

The execution plan From the execution plan, we see that the original index proposed by Paul is sufficient to allow us to perform a single ordered scan of , using a merge join to combine the transaction history with each possible product/date combination. 

I think this looks pretty reasonable for a system in which a user can add comments or attachments associated with a contract, a specific line of the contract, or a proposed modification to a line of a contract. Since you asked for potential shortcomings or pitfalls, here are a few you might consider: 

Using , this approach reports , which confirms the "single pass" over the table. For reference, the original loop-seek query reports . As reported by , the CPU time is . This compares favorably to for the original query. CLR quick summary The CLR summary can be summarized as the following steps: 

The most likely situation is that the new SQL 2014 Cardinality Estimator is yielding a poor row estimate for one or more joins in your query and this has led SQL Server to choose an inefficient plan. If you are able to run the query in SQL 2014 with "include actual execution plan" turned on, you can use the query below in another tab to view the real-time progress of rows flowing through each query operator. I noticed that you only have an estimated plan for 2014 (compared to an actual plan for 2012), presumably because you cannot run the query to completion in SQL 2014. So this could give you more insight into the actual rows flowing through the query in 2014 and may lead you to a way of tweaking the query that runs efficiently using the new Cardinality Estimator. In the meantime, until you are able to optimize the query you could use with trace flag 9481 for this query or you could follow Brent Ozar's advice of running the database at the SQL 2012 compatibility level, carefully testing your queries with the new Cardinality Estimator, and only updating the compatibility level to 120 (SQL 2014) once satisfied with these results. 

Using , this approach reports that no logical I/O has occurred! Wow, a perfect solution! (Actually, it seems that does not report I/O incurred within CLR. But from the code, it is easy to see that exactly one scan of the table is made and retrieves the data in order by the index Paul suggested. As reported by , the CPU time is now . So this is quite an improvement over the T-SQL approach. Unfortunately, the overall elapsed time of both approaches is very similar at about half a second each. However, the CLR based approach does have to output 113K rows to the console (vs. just 52K for the T-SQL approach that groups by product/date), so that's why I've focused on CPU time instead. Another big advantage of this approach is that it yields exactly the same results as the original loop/seek approach, including a row for every transaction even in cases where a product is sold multiple times on the same day. (On AdventureWorks, I specifically compared row-by-row results and confirmed that they tie out with Paul's original query.) A disadvantage of this approach, at least in its current form, is that it reads all data in memory. However, the algorithm that has been designed only strictly needs the current window frame in memory at any given time and could be updated to work for data sets that exceed memory. Paul has illustrated this point in his answer by producing an implementation of this algorithm that stores only the sliding window in memory. This comes at the expense of granting higher permissions to CLR assembly, but would definitely be worthwhile in scaling this solution up to arbitrarily large data sets. 

An example of different query plans On my SQL 2014 instance, the queries have (slightly) different query plans and the query plan for the LEFT OUTER JOIN ON version (top) is superior to the ORM-generated LEFT OUTER JOIN WHERE OR IS NULL version (bottom). In particular, the seek into should be a seek on both and , but in the LEFT OUTER JOIN WHERE OR IS NULL version it is a seek only on and a subsequent operator tests all matching rows in order to apply the appropriate restriction. This difference may or may not be meaningful in your specific case (especially if is unique without adding in the ). However, if your real version actually uses table partitioning by (and this just isn't represented in the simple sample query), you will likely be losing the benefit of partition elimination for the seek into . 

If you can post the specific data and queries you are using, that is probably the only way we can help answer the question in the context of your specific case. You can use a script that generates anonymous data in roughly the same scale as your real example. However, I went ahead and created a similar type of script myself. For the sake of simplicity, I am using fewer than 225 columns. But I am using the same number of rows and random data (which is unfavorable for columnstore) and I saw results that are much different than yours. So my initial thought is that yes, you do have some sort of problem with either your configuration or your test queries. A few of the key takeaways: 

When finding distinct rows across two tables where we can't necessarily ensure are pre-sorted, is it a good idea to use a rather than a ? Are there any downsides to this approach? If it is consistently faster, why does the query optimizer not choose the same plan for UNION that the would use? We were able to bring a specific production query from ~10 minutes to ~3 minutes by re-writing a as a . A seems like the more intuitive way to write the logic, but upon exploring both options, I have observed that the is more efficient in terms of both memory and CPU usage. See the following scripts if you'd like to run a simplified and anonymized version of our production query: Setup script 

Looking up exchange rates in batch If you need to find the exchange rate for multiple rows at a time, you can use to find the most recent exchange rate for each row. If you make sure that you have an appropriate index on that includes the column, looking up the exchange rate for each row will perform a single seek with no additional sort needed. You can see a full demonstration in this SQL Fiddle. 

I agree with Aaron that can be very useful and is probably the way to go if you have full control over the script that is being generated. However, if a long script is currently executing and you don't have the ability to change the script in order to add calls, there are also less direct ways to get this information. Test script Here is a test script you can run to help demonstrate the two approaches below: 

And finally, if you are interested in going a little bit deeper into how the query optimizer works, I have found Paul White's blog to be a great resource! 

I ended up with an approach that yields the optimal solution in this case and I think will do well in general. The solution is quite lengthy, however, so it would be interesting to see if someone else has a different approach that is more concise. Here is a script that contains the full solution. And here is an outline of the algorithm: 

I wasn't able to find a formal guarantee of this behavior, but I did find multiple examples — including a modified version of your original example — in which query optimization decisions appear to be made based on a guarantee that partition numbers are in order by value. Here's what I found: Your example Your example code doesn't currently compile, and also uses an index that does not cover the sample query. Here's an updated script that contains working code for your original example. With these updates, we can see that the query plan for your ORDER BY query does not include a sort operator; query optimization appears to rely on the fact that partition numbers are ordered by value in order to omit the extra sort. 

However, there are a few things that remain confusing and leave me wondering if I am interpreting the problem correctly: 

View You could rename the table (e.g., to ) and then create a view in order to provide the following API: 

I wasn't able to find any good resources online, so I did some more hands-on research and thought it would be useful to post the resulting full-text maintenance plan we are implementing based on that research. 

Why is the second statement ~5x slower than the first? From the amount of log data generated, I think that the second is not qualifying for minimal logging. However, the documentation in the Data Loading Performance Guide indicates that both inserts should be able to be minimally logged. So if minimal logging is the key performance difference, why is it that the second query does not qualify for minimal logging? What can be done to improve the situation? 

Using sys.dm_exec_requests.statement_start_offset Alternatively, Conor Cunningham also has a post on extracting the statement from AND . I don't believe this has been incorporated into yet, but you can use a query like the following to see both the current executing statement and the overall batch. 

Without these filtered statistics, SQL Server will take a more heuristic-based approach to estimating the cardinality of your join. The following whitepaper contains good high-level descriptions of some of the heuristics that SQL Server uses: Optimizing Your Query Plans with the SQL Server 2014 Cardinality Estimator. For example, adding the hint to your query will change the join containment heuristic to assume some correlation (rather than independence) between the predicate and the join predicate, which may be beneficial to your query. For the final test query, this hint increases the cardinality estimate from to , but is still quite a bit shy of the correct row estimate produced with the filtered statistics. Another approach we've used in similar situations is to extract the relevant subset of the data into #temp tables. Especially now that newer versions of SQL Server no longer eagerly write #temp tables to disk, we've had good results with this approach. Your description of your many-to-many join implies that each individual #temp table in your case would be relatively small (or at least smaller than the final result set), so this approach might be worth trying. 

What about batch mode? Based on a quick test, it seems that the batch mode hash join operator will also fully process the build side of the hash join. 

Given that the data is changing very infrequently and there will therefore be very little overhead in maintaining the indexed views, an approach using indexed views could help performance. As you likely noticed when trying to create an indexed view that uses , there are a lot of restrictions on what syntactic constructs can be used in indexed views. To get around this limitation, you can use a combination of aggregation (via ) and statements. However, you'll then run into another limitation, which is that a view that uses cannot be materialized. However, it is easy to make a separate view for the aggregated inputs and outputs in this case, and that approach is worth testing on your specific workload to see if it yields a performance benefit. Below is a sample script that illustrates both approaches: