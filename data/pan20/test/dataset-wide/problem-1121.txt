You can delete databases with DBCA which takes care of most of it. Or you can do as below, but this will do the same as removing the datafiles, redo logs, controlfiles manually. 

is the "physical" location of a row, including the internal ID of the object, file number, block number, and position in the block. This is just a pseudocolumn, and it is not exported. It is not even stored in the database. When you move a table, ROWIDs change. So no, do not use the above. ROWID Pseudocolumn 

But this above will affect only sorts and not the comparisons. You can set comparions to use the sort settings: NLS_COMP 

In your case, the backup is created before RELEASE CHANNEL. The autobackup would happen after COPY, BACKUP ... ARCHIVELOG and BACKUP ... CONTROLFILE, but since these are in the same RUN block, why perform the work of creating the autobackup 3 times unnecessarily, when it is enough to perform the autobackup after the last BACKUP command? The size is different because the autobackup contains the SPFILE also. The autobackup finishes after the controlfile backup, so the autobackup contains the entry pointing to the previous controlfile backup, but the previous controlfile backup doesn't know about the autobackup, that is another difference between them. When the controlfile grows, it gets more space allocated than it immediately needs, leaving some empty space in it that can be used later. Because of this, the controlfile and the controlfile autobackup can be the same size (when no SPFILE in use), but their content will be still different. Outdated entries can also be reused in the controlfile to prevent the controlfile growing too big. 

No grant needed. You do not see any results, because output is disabled by default. Enable output before running your code: 

Byte order is reversed, swapping the bytes and the octets: 0x004054c7. Doing this on a non-standby database, with minimal supplemental logging (0x4000000) and force logging (0x1000000) enabled (for GoldenGate replication): 

Trigger is the only way to do it. Virtual column is different from what you want, and that will not accept , because that is not deterministic: Column Expressions 

If the database homes were installed properly, the central inventory has a list about them. The central inventory on Windows is located at . On Linux/UNIX platforms, the location of the central inventory can be found in . In the inventory, in , there is a list of installed homes in XML format, e.g: 

22839608 is the prerequisite patch for 22839614. If you try to replace it with a newer one (23530402), the prerequisite will not be fullfiled for 22839614 -> conflict. I would try to rollback the conflicting patch (160419 JavaVM) and install the newer version of it: 

Oracle by default does not cache query and function result, but both cache do exist. $URL$ The importance of these: there is more than caching blocks. Even if you process everything in memory, why process the same requests again and again, if you already know the answer? What you are referring to when mentioning autotrace and physical reads, is the buffer cache. 

(By default here you should see and , and to be honest, I can not remember a single case where the database had different values - except the one I have just created in my sandbox.) Given the above, you will get the same execution plan as in your question. You can restart the instance, or set and at session or system (instance) level to the same values, it will not 'fix' the execution plan. To modify the above setting, it is technically possible (but do not ever do this in a real database) to update these values manually (re-running the dictionary scripts will not update this): 

This includes , and that is not the current usage. Use . For an even more detailed list, use and , I will not post output of those here. 

By the way the functions in your list are unnecessary, because you filter data with and , so you can have only 1 possible value, and you could just write: 

I assume you have a Basicfile LOB, because shrink is not supported with Securefile LOBs. The segment is not being moved, so the default tablespace and the tablespace of the segment is not relevant now. So a simple test is: 

is not a reserved word. The best way is not to use reserved words. If you insist on using them, you can put them between quotation marks. This does not work: 

Normalized means that the timezone information is not stored. When storing such data, the timestamp value is automatically adjusted to be in the same time zone as the database server. When retreiving such data, the timestamp value is automatically adjusted to be in the same time zone as the client. For example, I simulate that my client is in a different time zone: 

Just a few possibilities with not 100% accurate explanation. Let's say you have 100+ GB SGA (with 100 GB buffer cache) and 20 GB PGA. 

Then use localhost:12345 in SQL Developer. With the above, you log in to Machine B from Machine C using SSH, and create a tunnel to the listener port of Machine A through Machine B. 

Tempfiles can be dropped then recreated without any hassle. I would just try dropping the tempfile and then run nid again: 

You can specify the block size at the tablespace level at the time of its creation. Different tablespaces can have different block sizes. A conventional table must have the same block size for all of its segments, even if it is partitioned and partitions are in different tablespaces. 

Here you can see BMW, SILV row was updated ( in column ), and BUIC, BLAC row was inserted ( in column ). (Note that a may change.) 

Basically, if the result of this query is greater than 0, than the Data Guard is in use as per the procedure: 

The above drops the index, so when you enable the constraint, the index will be rebuilt. The below preserves the index: 

You can select from the below views. Historical SQL statistics: DBA_HIST_SQLSTAT All SQL text: DBA_HIST_SQLTEXT Map snap_id to actual time: DBA_HIST_SNAPSHOT 

Ok, so you have no backup. You made sure it is not a permission issue. But you have Data Pump dumps, at least that is something. First try to recover the datafile without data loss: 

Then force log switches until the new logs are used (CURRENT status) and old logs become ACTIVE/INACTIVE by: 

This returns a where the item is not on the list, otherwise it returns the . The actual value does not matter, just the fact that it is not null, so providing the desired output: 

If you want to decrease , you need to decrease as well. Still, if you decrease processes, you will have plenty of 'free' sessions, and you will need to open several sessions until you receive the error. Easiest way to simulate the error without messing with and opening several connections: 

Looks like the JDBC 11g NAT+RAC issue. JDBC Connections Using SCAN Fail With ORA-12516/ORA-12520 (Doc ID 1555793.1) 

There is no such thing in the database itself. You can perform something similar in SQL Developer, with the Database Export tool. How to Export Data using SQL Developer With the Tools / Database Export tool, you can specify the objects you want to export, then SQL Developer generates an SQL file for recreating them, with the data as insert statements. 

That is not possible, the database does not support such grants. You can grant the required privileges on each object individually in the schema. For example: 

This is called Enterprise User Security, which is available in Enterprise Edition databases. You can do this with Oracle Virtual Directory or Oracle Unified Directory (preferred). They both require the Directory Services Plus licence. The required steps for this can be found at for example: Enterprise User Security (EUS) with Active Directory (AD) Integration Using OUD Proxy (Doc ID 1571196.1) A public walkthrough: Configure EUS with OUD, AD and DB12c 

Assuming your trigger is enabled, I am surprised at your trigger working at all. Usernames are case-insensitive by default and treated as uppercase strings internally. If you create a user called or , the actual usernames will be and . Normally, a condition like the one in your statement, will be false. Sure, it is possible to create case-sensivite, lowercase usernames, but that is quite rare and I consider it bad practice, because it has no benefits, it just makes administration cumbersome. Instead of: 

After this, continue as you normally would. Update: For 9i, the above is not possible, there is no such option in 9i, you can not catalog backup pieces, this feature was added in 10g. The solution is in the below note: how to restore or clone a 9i database to different host when backup location is different (Doc ID 1451140.1) It suggests mounting the database (controlfile) with a 10g software, use the above command, then shut down the instance, and continue with the 9i software. I think it is easier to create a symlink pointing to . 

Notice the line . This is obviously incorrect, as A is an unknown identifier. Either type , or put apostrophes in the code: 

Oracle Recommended Patches (PSU) for Enterprise Manager Base Platform (All Releases) (Doc ID 822485.1) There is no PSU released for 12.1.0.5 yet. 

The same can be specified in the standby database. RMAN Configurations at a Standby Where Backups Are Not Performed 

Too broad. Anyway, in this specific example, the best reasonable index is the PK itself, because its unique, and you get your 0-1 row (block) immediately that you can visit and evaluate the rest of the conditions. Of course, you can create an index as (id, name) which in some cases saves you the table visit because of the short-circuit evaluation, but in my opinion the performance gain with this would be marginal. Sure, you could also create an index as (id, name, lastname), so you would not have to visit the table at all, but there is no point doing that, since you dont have any other columns, so you would index the whole table. It would be better to just create the table as an index-organized table in that case. 

This is for the 1st query, the database calculates the cost of the transformations, then later decides to use it (did not post everything, as that would be much more than the above). Now for the 2nd query, this is what the optimizer did: 

Notice how the optimizer completetely eliminated the DISTINCTs in the first and second queries, but not the GROUP BYs. Unfortunately in these cases, since the DISTINCT elimination happens in subqueries ("views"), this information is not present in the optimizer trace, just like for the original queries in the question. So now we know that DISTINCT and GROUP BY are handled indeed differently, lets go back to question 1. To be continued in the next post... (Both answers together exceed the 30000 characters limit.) 

There is a Readme for each Oracle Template download. Here is the related part from the Readme for your template: 

You can create an AWR/Statspack report of the problematic time period, it is quite easy to recognize if it was the culprit (CREATE TABLE statements with weird/specific names that was not executed by the DBA/application/regular jobs). 

I fail to see the point of a requirements such as avoiding , but here it is, without , or hardcoding a constant (still, hardcoding is done by using ): 

If you see a capital R in the output, then the read-only attribute is set for that folder (which is the default setting for the Documents folder). Either remove the read-only attribute from the folder (I do not know what this will mess up in Windows), or specify a different folder for diagnostic dest. For the latter, on this screen, go to , and set the value of to a directory that has no read-only attribute set. 

is not an error, just an informational message. Actually, when you use EXECUTE, you implicitly use an anonymous block. If you want to get rid of that message, you can turn it off by: 

The 12c Multitenant option is a nice tool for this. Create a "master" pluggable database that is your starting point, and you can create other pluggable databases by cloning this PDB. When you clone a PDB, you have the option to clone the PDB as a "snapshot copy". This does not actually copy the files, but uses storage snapshots, and that is what makes this feature great. For example, cloning a 111 GB database took 20 seconds in this example: Snapshot clone of a PDB using ACFS When I experimented with it, smaller databases took even less time to be cloned. When you do not need the database anymore, you can just simply drop it. Also you are not limited to 1 schema, because you can create many clones and use them at the same time concurrently, up to 252 PDBs. Note that, the Multitenant option is an extra cost option for Enterprise Edtition databases. So you need the proper license to use it. SNAPSHOT COPY