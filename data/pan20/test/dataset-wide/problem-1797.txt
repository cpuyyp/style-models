You try to install package postgresql-libs-8.4.4-2PGDG.el5.x86_64 and postgresql84-libs-8.4.7-1.el5_6.1.x86_64, at the same time. This indicates, that you have incompatible repositories enabled. If it is a RHEL installation, I would recommend disabling everything but the default repositories and once again running . If you want postgresql that's newer/different than the one provided by Red Hat (warning: this makes your config unsupported), then you'd have to play a bit more with disabling some/all RH repos and enabling non-RH repositories one at a time, and run . At one time you'll get combination of enabled repositories with no conflicting postgresql packages and the command will succeed. 

Is the controller supported by your server? Check e.g. here: $URL$ Is the controller supported by your cloning software? Check the producer's website and check hardware compatibility list for your version of the software. 

NAT definition rules; routing rules (if some packets leave via eth0, and some via eth1 things may get strange); 

Expanding on Raphnik's comment, you can save yourself a pipe through grep by puting into the regex part of locate or find commands: -- it will match files with .avi and .flv extensions. 

The typical problem is that the user Apache runs as (let's call him apache) cannot access the directory. If the apache user has a shell to that user, cd to and see if you can to the directory pointed by DeocumentRoot. Each directory on the path from to has to give at least and permissions for the apache user, apache group or "the world" in order for the Apache process to be able to cd into this directory on its way to . 

followed by the results I wanted to see, in this case the properties of the user. But recently the same PS message has not been followed by the results I wanted, but instead I see, 

My account is a domain admin, I often log onto a particular server in our network (not the DC), and until recently I could query AD with PS commands, e.g. 

Uninstalled all obvious adware type programs Removed all obvious adware type toolbars from Internet Explorer (I am not using Chrome or Firefox) Download and run free editions of the following anti-virus/anti-malware programs, run them repeatedly until they all find nothing. 3.1 AVG 3.2 Malwarebytes 3.2 Spybot Search and Destroy (None of the updaters will work first time) Reset the TCP stack to cure printing and updating: 4.1 Type CMD in the Windows search function, right-click the Command Prompt item found, and click Run as administrator 4.2 Type and hit enter for these two commands: netsh int ipv4 reset netsh int ipv6 reset Re-run the above listed free editions of anti-virus/anti-malware programs, this time they should update, re-run them repeatedly until they find nothing Run the free edition of the ESET online scanner (it found a few more threats) Replace the two dnsapi.dll files listed above in the introduction to this reply Flush DNS in another elevated command prompt using command: ipconfig /flushdns Re-start Windows 

The usual suspect would be synchronous write. Try mounting the NFS with async option. I've never played with this big wsize and rsize. Try something about 8k and see if it maybe helps you. Edit: Can you verify on the NAS, that it's exporting the filesystem with async option? I would also try different option, starting with reducing their number: 

If your servers all the same hardware, then the reason must lie within software. Older version of CentOS means older kernel and older LAMP stack (unless you compile your own). CentOS 4.8 runs kernel 2.6.9. CentOS 5.5 is 2.6.18. 2nd factor would be tuning. If you fine-tuned 5.5s, but run an out-of-the-box 4.8, this will factor in as well. Developers strive to improve speed both of kernel and applications, so yes, OS version may play important role in the speed delta. 30% improvement on identical hardware? Not impossible. OTOH if your servers don't run on identical or very similar hardware, or if the 4.8 one is the one connected to the rest of the net by that ancient over-heating half-duplex 10 Mb hub locked in the broom shed, then hardware may be the dominating factor explaining the speed difference. CentOS 5.5 on quad Nehalem with 32 GB RAM should be significantly faster than CentOS 4.8 on a P IV. 

Not sure if I understand the question but you wont be able to have high availabilty of your virtual machines running on a host system. If you're already running an OS, then you need to add on a virtual machine service to windows. this would include: - virtualbox (free) - vmware server (free but being discontinued) - vmware player (free) - vmware workstation (paid) You can run VMs on each desktop, however you WILL notice a slowdown of the host system even if the VM isnt doing anything. CPU is only one concern - HD activity, network activity, ram, and most endusers will notice if you "sneak" a vm onto their workstation. If you're looking for high availability - to move vm around from machine to machine seamlessly - you're looking at commercial software from vmare/xen/microsoft. Using existing workstations is a bad idea as it will be slow and hassle to admin. What you're asking about isnt done in the real world. If you're wanting to play sure go for it, but if this is for a business/school/nonpersonal network it's bad idea to try to harness the "spare" cpu power, cannot stress that enough. It doesnt scale. 

So... was trying to add in a 2008 DC to a single 2003/exchange 2007 setup. ran adprep and updated schema, and joined the new DC to the AD... then 6 hours later noticed everything was not working. Restore tapes are offsite and not available for a few days so no easy option. The gist is the GC is not locatable and the sysvol isn't being shared. If you connect directly to the DC you can query all objects inside the AD properly, but nothing that queries the root domain itself works. went through the dns tree and eveyrthing seems proper. the server is pointing to itself for dns. dcdiag shows: Starting test: FsmoCheck Warning: DcGetDcName(GC_SERVER_REQUIRED) call failed, error 1355 A Global Catalog Server could not be located - All GC's are down. Warning: DcGetDcName(TIME_SERVER) call failed, error 1355 A Time Server could not be located. The server holding the PDC role is down. Warning: DcGetDcName(GOOD_TIME_SERVER_PREFERRED) call failed, error 1355 A Good Time Server could not be located. Warning: DcGetDcName(KDC_REQUIRED) call failed, error 1355 A KDC could not be located - All the KDCs are down. i have done ntdsutil and seized all roles anyway, confirmed under sites that the DC is a GC. it should work...google doesnt show what i want.... i'm good with AD but not good enough ;) Where do i go from here? 

If named doesn't run, then stop it (just to clean up), then start and see what it writes to log files (/var/log/messages, /var/named/data/named.run). There may be some typo that prevents the daemon from starting successfully. Then I'd strip as much as I could from the named.conf. Minimal, basic config should be easier to troubleshoot. Then I'd add entries bit by bit, to see, which entry causes the server to balk. Also try separately to resolve things locally from the server, and from a client outside. I hope you'll find what's wrong. 

Personally I prefer piping output of a command I want to examine through . records too much information, including timing of key presses, and a lot of non-printable characters. What saves is much more human readable for me. 

Here's a description of a process. They used a large swap partition to host root filesystem during the change. You do not need this trick, because you have 2nd HDD. A trick question: does your server support booting from the 2nd HDD? Can you set it up to boot from 2hd HDD? I think you could simplify the solution and not change the /boot partition. Bootloaders do not like LVM yet, so you still need a regular partition to host your /boot. Leaving /boot be and only changing the root file system location simplifies the process. You can have 2 entries in grub.conf mid-migration: with root in the old place and on LVM. In case something goes awry, you can boot to the rescue image, change the default entry and have another go at the migration. And yes, I wholeheartedly agree with poige. Test in a virtual environment before you start playing with real system. 

Check the properties of two files C:\Windows\System32\dnsapi.dll and C:\Windows\SysWOW64\dnsapi.dll, if you find they have a "Date modified" around about the time you got the infection then those files are infected and you need to replace them with uninfected versions. The infection does not change other properties like file size and version, you can only tell they are infected by the Date modified. That means you can use the infected files to find out what version of the uninfected files you need, because the version numbers are unchanged. Once you know the version numbers then go and find those uninfected files on a different uninfected system, making sure that the Modified dates look correct e.g. compare to other system dlls. These were the full steps I used to disinfect a Windows 8.1 system manually, it was also suffering from not being able to print because the print spooler would not start, and Windows Update and anti-virus update programs would not update saying there was no Internet connection. Also Ads by Jabuticaba was occasionally replaced by Ads by Shopperz, and Ads by LaSuperba, but all in the same format. As you will observe fixing this took considerable time - a couple of days - perhaps some of these steps can be shortcut but this is how eventually I fixed the problem. 

Frist one is much less hassle, but you don't get some of the benefits of ext4 for files already present in the filesystem (i.e. they won't be extent-based). Backup-mkfs-restore will give you all features for all files, and mkfs.ext4 will make sure that alignment of data is playing well with your underlying software RAID. If you got the alignment right when creating ext3 filesystem, then there will be no gain. If you made a mistake, potentially you can gain a lot. Which method to choose? Up to you. If your filesystem works fine now, and you just want to run current file system, you can convert in-place. If you experience performance problems and hope that filesystem change will help you with these, I'd go for a method 2. 

You have better part of the solution figured out, the only thing left to do is correct permissions for things created by root. There are at least two approaches 

into cron to be run every 5 minutes or so and after a crash have a look what was eating your CPUs just before server crashed. 

Your traceroute to the twitter.com ruled out DNS and routing problems on your side. If you do not block twitter on the firewall, then the problems are outside your systems. 

Unfortunately this is a vague issue and it needs to be narrowed down. One of the first things that needs to be done is to make a chart clarifying what the problem is, and for whom. Chances are fixing the wireless network will improve your network greatly, but that might not be the real problem. One of the main questions would be , are the people who are physically wired in experiencing problems? If so, then that shifts the focus from the wireless issue to a server/application level issue, perhaps upgrade from older switches to gigabit etc. Wireless is not as good as a physical cable. Especially with consumer wireless routers/access points. They're designed for a few people to surf the internet, not a business network. Chances are the wireless network isn't operating as well as it should be. Few options: 

Unfortunately that's a bit vague to start with. Define "performance drops a lot". This is one of those situation wheres there's about 20 different things to check, both hardware and software. Hardware: - do you have a good decent nic on both ends? (intel/broadcom), not realtek - do you have a managed switch? - Is the managed switch maybe unavailable to do all the process switching if small packets? - have you tried swapping network cables - confirmed it's gigabit? - perhaps your harddisks can't keep up with the data streams? generic basic harddrives can be maxed out on gigabit. - do have a network card that does TOE? Software: - are you using mtu 9000 aka jumbo buffers in card? - have you looked at tuning receive windows or buffers? - What OS? - If windows, do you have av/firewall software running? Applicationlevel/Data: - is it being encrypted? (tunneled over ssh?) - What protocol? ftp/cifs/rsync/http/nfs - what are the size of files? thousands of small files or one really large one? There's so many places to start, but those are some questions you need to answer to yourself. Once you get to the software level, I would recommend using iperf between the 2 machines and seeing what the maximum of raw data you get. That will tell you the highest possible speeds. Then you can compare it to what your application is giving you.