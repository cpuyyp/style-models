@Alecos has already mentioned one of the two points I wanted to make, here is my extension. Econometrics has no value per se, it always comes from the data we use it on. In microeconomics, with many studies that are natural experiments, quasi-experiments or where we are somewhat sure that we are not capturing endogeneity, we could estimate many interesting parameters. One of the standard references would be the many wage regressions that have delivered fairly consistent and reproducible estimates. On the other hand, in Macroeconomics, we have no hope of getting clean identification. If you assume every country to be identical, all we have is a panel of around 60 panels and 30-50 periods of observations (mostly of aggregates), depending on what you want to observe. Moreover, these are often not perfectly measured, and most importantly we cannot create random variation here. Of course, when data fails, one can try to tweak the estimators, but even with partial identification as some VARs do it, we can only hope to get short-run identification, and still have to worry about endogeneity. Yes, I agree. Economists have not consistently and univocally warned us about the financial crises and other macroeconomic shocks. But this does not necessarily mean that the structure that macroeconomic models assume are bad, nor does falsify any other section in Economics. This is good news for Microeconomics: We can somewhat rely on Econometrics to reject and accept theory. It is bad news for Macro: We can almost never reject plausible models just looking at the data. 

No it usually does not. There is an intuitive reason why: We interpret the technology shock as a residual, the unexplainable part in TFP changes. If we knew that there was a specific causal relationship between monetary policy and future TFP, that part of future TFP would now was explained. It's true that the MP shock is excluded in standard Solow Regressions, but if you believe there is a causal effect, you need to regress it out of it. Since after all, we understand $A_t$ to be independent. Note that still, implicitly, through its effects on capital accumulation (which does not exist in standard NK models), monetary policy should impact future TFP. Then, you could make $A_t$ dependent and argue it being a shortcut in the absence of capital. 

Where row , column gives you the probability of transitioning from to , and it satisfies that the sum of each row is approximately one. I am wondering how I can transform this to a continuous time equivalent of the transition matrix; a set of poisson probabilities controlling the flow rates between the states. All I remember in this regard is that we can get the linear approximation to the poisson probabilities using $$Prob(i \to j) = \lim_{\Delta\to0} \exp(-\lambda_{ij}\Delta) \approx 1-\lambda_{ij}\Delta $$ But I can't see how that helps me transforming that former matrix to the $\lambda$s... I'm looking forward to any suggestion. 

tl;dr There is a lot of demand for what they produce. A proper answer should have some evidence, perhaps I will add that when I have more time. Even within a category, say, football (soccer) players, you see a lot of discrimination: Top players at top clubs in top leagues get most of the cake. This is anecdotal evidence for the demand side story: 

This is relevant because if the households receive utility from what the government is doing with the money, they will "feel richer" - the expenditure has a wealth effect (and is neutral in case of no other rigidities). In the other case, even with lump sump taxation, households will feel poorer and respond differently. If you want to look more into this, solve the following simple optimization problem: $$ \max_{c,l} U(c + G(T),l) \text{ s.t. } c = w(1-l) + a - T $$ where $a$ is assets, $T$ is the lump sump taxation, and $c$ and $l$ are consumption and leisure. Look at the impact of different functional forms of $G(T)$ onto the optimal choice. For example: $G(T) = T$, $G(T) = 0.5 T$, and $G(T) = 0$. 

Extensive margin models use Rogerson (1988) style lotteries in order to simplify the savings problem under non-full-insurance. However, one could still write down the model in that style but instead of assuming full insurance (that is, marginal utility of consumption equalized among all individuals in the household, no matter whether working or not), do it with partial insurance, in order to match the data better. I however have not seen this approach so far. Did I miss something? Or what would be arguments against this approach? Besides the obvious one: If you don't have full insurance but forbid savings at the individual agent level, you are an evil person. 

I would say that calculating cost of manufacture is too trivial to fall under this category - unless it's a monopolist who has to take into account equilibrium responses of his suppliers and demanders to his choices. 

I suppose one way to solve this - but I'm still looking for a proper way without - is to fall back to matching. If there is matching between the intermediate and the final stage, at zero search cost and symmetric arrival rates, the surplus will be split given the Nashbargaining weights - which then determines the price. 

Not really. The long term upwards trend is what we deal with in growth theory. And the reasons that are brought up for that trend typically are inventions/technology, population growth, increases in human capital. There are reasons to believe that technological growth - an important driver - may stop, and hence long term growth may flatten out. But there's nothing inherent in these drivers of growth that necessarily implies mean reversion (and eventually negative growth). 

It is agreed upon among economists that trade, mostly through specialization and economies of scale, is welfare improving. Hence, if you want to look at self sufficient countries, you rightfully look at those which are involuntarily in that situation. North Korea receives supports from China. Cuba receives support from Non-US. Iran has self-sufficiency as one of the big targets of its politics, to be less dependent from international sanctions. While Iran is still trading with other countries, and getting a lot of resources through informal channels, I believe it i the country closest to self-sufficiency, disregarding undeveloped economies. 

We live in continuous time $t$ and something terrible is happening at a poisson rate of $r(t)$. How can I compute the length $T$ such that with a probability of $P$ (for example, 0.99), at least one terrible thing has happened? I know that for a constant rate $r(t) = \bar r$, the expected value is $1/\bar r$. How can I go on? 

This is question about Kaplan and Menzio's shopping time model. Pages 7,8: Unemployed search once or twice (for a seller). 

Your formula is overly simplified. The elasticity is supposed to be "how much does supply change if the price changes". Now, the natural way of looking at this is $S'(p)$, the derivative of $S$. However, the units of that are hard to make use of. Hence, we normalize $S'(p)$ by multiplying it with $P/S(p)$, hence getting the "percentage response of $S$ to a percent change in $P$". Long story short, your $k$ should satisfy $$ k = S'(p)$$ Note that this is the price elasticity of supply. For the price elasticity of demand, you would instead use $D'(p) P / D(p)$. 

What are public sources for US firm/plant sizes, in order to characterize the firm size distribution? And what are stylized facts about the firm size distribution: What does it approximate to? Somewhat related: What about the firm age distribution? That is, the distribution of firms given their age. I understand the latter is a soft concept in the data, but are there first-order approximations for age, and how does its distribution look? 

Where again, if you want to do purely theoretical work, your knowledge (in particular with probability theory) needs to be much more profound. 

I'm interested in solving the elasticity of $x,y$ w.r.t. $z$. I guess I need some sort of concavity requirement in $f \circ g$. How do I approach this? 

Side note: This is one way of solving it - the alternative would be formulating a Bellman equation and iterating on that. If you assume that the real economy is on or sufficiently close to the steady state, you can also infer about responses to shocks. That is, you can look at the impulse response functions to a change in whatever interests you, and see how the model economy changes. Arguing that we're close enough to that steady state will allow you to see how the economy responds to certain shocks. Also, in general, you can simulate the economy with (for example TFP) shocks, and look whether the simulated economy looks similar to the real economy. Using this comparison you could judge the model. This needs arguing that we are close to the steady state - or that convergence happens really fast. Generally, the growth literature around Solow has provided arguments for this. But your argument is very present in most extensions of the basic RBC model: especially when it is important how close we are to the steady state - when models are more nonlinear. There have been many papers showing that this is the case for standard Neo Keynesian extensions of the RBC model. 

Let's split each period into $n$ intervals. There's a continuum $u$ of unemployed and $v$ of vacancies. During each interval, there is a total of $X/n$ job offers. That means that each unemployed gets a job offer with probability $X/(nu)$ (assuming that $n$ is eventually so small that the chance of multiple job offers to the same person will go to zero). Single individual Number of job offers - for a single individual - in the interval is distributed $Binomial(k, n, \frac{X}{un})$. Letting $n\to\infty$, the distribution in the continuous-time analog converges to $Poisson(k, \frac{X}{u})$. I'm interested in the probability of $x$ individuals getting at least one job interval during a whole interval. That is $(1 - Binomial(0, n, \frac{X}{nu}))^x$: $$ (1 - (1 - \frac{X}{un})^n)^x $$ If I'm not mistaken, the continuous-time analog is $$ (1 - e^\frac{-X}{u})^x$$ Is that correct? The combination of exponential and power function is making me quite uncomfortable. 

If instead you're going to continuous time (this will be the case for Macro-Finance, also likely Growth. For mainstream macro, it seems to be the trend, but is yet unclear), you will need knowledge of differentiable equations, in particular 

I'm looking for empirical evidence (optimally from natural experiments, as described below) of the impact of working capital constraints onto firm output, and the differentials of firm size. I suppose that generally, stricter working capital constraints are bad for output, and typically these constraints are stronger for smaller firms (measured in output) and younger firms. However, I couldn't manage to find (a) paper(s) to establish that fact yet. Where by working capital constraint I mean something along the lines of Kiyotaki&Moore (1997) $$L, K: f(wL, rK) \leq \psi(F(K,L))$$ , for example, for some constant $\psi_0$, $$wL + rK \leq \psi_0 \cdot F(K,L)$$ where $F(\cdot)$ is the production function, and $f$ and $\psi$ can be whatever. I have, in other environments, seen that some people take bank liquidity to approximate $\psi$ and argue that for example the last recession was a natural experiment on a change of $\psi$. So, one type of useful regression table would be (on the firm level) 

A competitive equilibrium is specific to the economy you're modeling. Your economy has three markets: 

I don't think that on yearly returns, a day more or less matters too much. In the unlikely case it actually does, you could interpolate the final day. A first approximation would be the average over June 12th and June 11. 

Some people from the more extreme microfoundations/internal consistency camp keep bashing the Neo Keynesian model for the Calvo pricing being not micro founded and hence the whole model not being immune to the Lucas critique. However, one could argue that we don't observe inflation rates far away from the steady state in the US, and, as long as this was true, the lack of micro foundations was minor. Which argument dominates here? Are there other arguments to be made around NK and the Lucas critique? What is the literature's conclusion? 

It's his doctoral thesis, and using Swedish data. Is there any recent evidence on this for the United States? 

The optimal level of inflation is very debated with unclear answers. There are many reasons, and a great answer would be very long. It should also distinguish between expected inflation and surprises. I'm not going to do any of this, but giving you three reasons for a desirable positive level of inflation. This list is of course incomplete, also there are many reasons against too high inflation. Downward-rigidity of wages It is really unclear to economists why it is happening, but nominal wages seem to be downward rigid. It appears to be a behavioral thing (and might not be true at all, see Barro (1977)), but it appears that in crises, once presented with the choice of more firings or reduction of wages, most firms/workers decide to not cut wages but rather respond to the slump with increased separations. To the extent that we believe this lack of reduction in wages is suboptimal, the central bank can enforce a reduction in real wages through increased inflation rates, and thus preventing separations to some extent. Redistribution This argument is based on Keynesian theory. Keynes claimed that the marginal propensity to consume out of income is smaller for rich households (and indeed, we do find this in the data to some extent). Unexpected inflation is similar to redistribution from debtors to borrowers, as long as debt contracts are not indexed to inflation. To the extent that poor people consume more out of this unexpected wealth shock and than rich people decrease their consumption, this redistribution will lead to an increase in aggregate consumption. Room to cutback interest rates This argument was most prominently brought forward by Krugman around 2008. In recessions, you want to be able to decrease nominal interest rates in order to punish households and firms for "holding cash" and incentivize them to spend it instead. If you start with low (say, 2-3%) nominal interest rates during normal times, you don't have a lot of space to cut back nominal interest rates during the crises. If, instead, you would have higher nominal interest rates (Krugman argued for around 8%) and relatedly higher inflation during normal times, you could easier cut back nominal interest rates during busts and stimulate the economy. 

It sounds tricky: Even if you find the most efficient way of not-recycling trash (be it burning, dumping on a lake, on the moon, or in a third world country), how do you evaluate the costs of doing that? Coase: How can we evaluate the costs of dumping the trash? Without loss of generality, Say the most efficient solution is to drop the trash into the sea. If you are familiar with externalities and the Coase Theorem, you will believe that the obvious solution is to designate ownership: Under certain circumstances, if we allow someone to own "the sea", and then allow trading the sea, the person/coalition that "cares most" about the sea, given same initial endowments, will own it. If you want to trash the sea, you will have to pay them a fee, which depends on 

Definition In economics, stagflation, a portmanteau of stagnation and inflation, is a situation where the inflation rate is high, the economic growth rate slows down, and unemployment remains steadily high. Stagflation hence requires a high unemployment rate. The unemployment rate can be defined as $$ u = \frac{U}{POP}\\ u =\frac{U}{LF} $$ Direct increase of unemployment? that is, either the number of unemployed over population (more common) or over the labor force. Removing women and teens from the labor force does not affect anything in the first definition, and decreases the unemployment rate by the second definition. Indirect increase of unemployment? As stagflation requires a high unemployment rate, reducing the labor force size cannot directly create stagflation. Your only argument could then be that a reduced labor force somehow leads to a higher unemployment rate. Most likely, this is orthogonal. Equilibrium unemployment is a composite of frictional unemployment and voluntary unemployment. I cannot think of a reasonable argument why a reduction of the labor force should increase the relative share of either of these (unless of course, voluntary unemployment is higher among men than women, which is false). Level effects and growth rate effects Even more importantly, there is an important distinction between shocks to the level versus shocks to the growth rate. To the extent that - after women and teens have left - the growth rate of the labor force is the same as before, long term effects are negligible. In a world with exponential growth, a shock to the level is - in the long run - negligible, as we will catch up quite quickly. A shock to the growth rate however, is permanent. Of course, "in the long run" is relative, but there is some idea of persistence behind stagflation. An exit of women from the labor force would, in my opinion, cause a sudden drop in output, but would not affect growth rates.