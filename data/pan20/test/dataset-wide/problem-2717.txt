I have not played "The Talos Principle", so all my information came from the Wikipedia page of this game and your question. According to your question, humans developed an 'evolutionary algorithm' with the goal of developing some entity that could be classified as a "person". According to Wikipedia, an evolutionary algorithm works as follows: 

I can accept the idea of Mario 'learning'. I can even accept the idea that Mario is 'self-aware' within his own environment. But I cannot intuitively accept the idea that Mario is feeling any emotions. It seems weird to think that EA programmers and German AI researchers were both able to accomplish the dreams of science fiction writers all over the world, and without anybody ever noticing. Maybe my intuition is wrong though, and maybe Mario is indeed feeling emotions (even if they are not the same emotions as humans feel). 

It seems to me that the value of this painting lies completely in the identity of its creator and the context of its creation. Technically speaking, a complete amateur like myself, or even a young school child would easily be able to produce the same painting. Yet if that had been the case, no modern art museum would be showing it, and people wouldn't be paying money to see it. To give another extreme example: A nude picture or painting of Kim Kardashian would be softcore pornography, but a nude picture or painting of a similar looking Saudi or Iranian woman would be subversive, radical, revolutionary, and definitely worthy of our attention. In both examples, the context in which the art work was created is what determined its value, not the actual content of the work. Is there any way of determining the inherent value of a piece of art, regardless of how it was produced or who produced it? Or is art always dependent on the context in which it is produced? 

It is, of course, debatable whether a Strong AI even can exist. After all, philosophical arguments about the Chinese Room and Philosophical Zombies abound. However, it is possible to imagine a world where "Strong AI" is possible and yet people adhere to a philosophical argument that denies the existence of "Strong AI". It is also possible to imagine a world where humans fall prey to the AI Effect. According to Wikipedia, 

Replace "heap" with "non-person" and "non-heap" with "person", and you can see the similarities. Now, one way to solve to the Paradox of the Heap is to just create an arbitrary definition of "heap" and adhere to that definition ("a heap is a collection containing 10,000 or more grains of wheat"). Wikipedia doesn't seem to like this type of solution: 

Non-reductive materialism (or materialism) is considered a from of monism, in the sense that ontologically everything is considered to be made of physical substances. It is non-reductive only in that it holds that some properties can never be explained in terms of lower level physical properties, even if they pretain to material objects. A non reductive materialist holds for example that mental properties, even though themselves physical, can never be explained in terms of neural states. Another example of non-reductive materialism would be our inability to explain biological causes in purely physical and chemical terms. I've seen this position also described as naturalism (in the context of John Searle's theory of mind). More recently, there was this paper on the undecidability of certain quantum hamiltonians, challenging the reducibility of the macroscopic to the microscopic. My question is: does this really count as monism? Isn't it for all practical purposes dualism? I've seen Kant described as a dualist because of his noumenon/phenomenon dichotomy. But then based on the same logic, isn't dividing the world to ontologically identical, yet logically non-reducible, and therefore epistemically independent realms, amount to a form of dualism? 

The key aspect here is the existence of "fitness", to help measure how close someone is to reaching the desired goal (in which case, the entity that could be classified as a person). This "fitness" must be a quantifiable number, so if the AI is not "fully" a person, then how close to "personhood" the AI is (so that the evolutionary algorithm could decide which AIs to breed and which to 'replace'). In this specific case, the humans have chosen to measure the "fitness" of the AIs through the use of puzzles...if AIs have successfully solved all puzzles and defeated Elohim, then the AI is considered a person. If the humans already solved the hard questions of defining personhood, then why would Elohim be intentionally designed to be a person? Elohim's goal is to supervise the evolutionary algorithm...you don't need "personhood" for that. You don't even need intelligence for that. If you can come up with a way of measuring fitness, you can attempt to run an 'evolutionary algorithm' that will maximize this fitness. The Wikipedia article does not claim that Elohim is able to reprogram itself. So if it was not intentionally designed to be a person, it never had the chance of ever becoming a person. Now, Tenrec77 argued that the players "have every indication Elohim was a person, just as much as Soma, Milton, or even you and I". That may be true. There's no reason to assume that these future humans' attempts at measuring personhood would be at all sensible. It's possible that the humans built Elohim (the evolutionary algorithm) with several "features" such as the desire for self-preservation and the ability to feel emotions such as fear, claimed that Elohim was only "20% person", and simply used Elohim as a way to build "real, true, 100% persons". The humans has tried to define a complex topic such as "personhood" by creating an arbitrary boundary between a 'person' and 'not-a-person', and then used an evolutionary algorithm to try and reach that boundary. This idea of setting up arbitrary boundaries does seem very similar to a possible solution to the classical philosophical problem: the Paradox of the Heap... 

Per this principle, sexual pleasure/connection, although desirable, is not as valuable as friendship at the level of ideas and emotional connection. Then, based on utility, one could argue that sexual relations might interfere with real friendships, and so friendship should be prioritized over sex whenever the two compete. Evolutionary perspective One could argue that there is nothing morally wrong about "cheating" but that fidelity makes sense as an evolutionary strategy. According to Strategic Pluralism Theory for example, it makes sense in many environments for people to restrict themselves to one partner and focus on maximizing the care provided to the offspring of that relationship. In other environments, other strategies might used. See Gangestad, S. W.; Simpson, J. A. (2000). "The evolution of human mating: Trade-offs and strategic pluralism". Behavioral and Brain Sciences 23: 573â€“587. Religious/Social perspective I can't provide a source for this. It was an answer provided by my high school religious studies teacher. The Abrahamic religions had very strong rules against promiscuity because of the need to preserve family lineage. In societies were family and clan relationships were very important, people had to be sure that siblings and cousins were indeed who they claimed they are. At a time when contraception or DNA testing weren't available, the only way to insure the "purity" of family ties was by restricting the sexual partners that people could have. 

(As a side-note, the fact that the humans believe that it is possible to measure personhood by their external response to puzzles rather than philosophizing about the internal state of the AI means that the humans adhere to either Behaviorism or Functionalism. If you don't agree with two philosophical approaches, then it's possible that this entire question is moot and that it's impossible to build an AI that would reach "personhood" status.) 

On the other hand, without an arbitrary definition of personhood, the evolutionary algorithm would not work. You need some way of measuring how close someone is to "personhood". In addition, the Wikipedia article for the "Talos Principle" does not mention anyone ever questioning the "use" of the evolutionary algorithm, so it's possible that humans have came to a consensus about how to define personhood. We may therefore wish to defer to their consensus. (Wikipedia calls this appeal to consensus as the "Group consensus" approach to solving the Paradox of the Heap.) So, back to your question. Is Elohim a person? My response depends on whether you trust the arbitrary definition that humanity has given to personhood. 

Yes he did. He has a whole section on the inhabitants of other planets in a book he published called Universal Natural History and Theory of Heaven (Allgemeine Naturgeschichte und Theorie des Himmels), published in 1755. Keep in mind that he was an astronomer and that given the level of scientific knowledge of the day (with the still relatively recent discovery of the heliocentric system, limited knowledge of conditions on other planets, limited knowledge on the nature of organic life and oxygen, etc...),that there was no a priori reason to expect that other planets weren't inhabited. And the belief was wide spread among philosophers of the period: Rousseau and Voltaire for example, also believed in extraterrestrials. 

I don't really get Kuhn's approach to the demarcation problem. I can throw around terms like "paradigm shift", "normal science", "revolutionary science" and "incommensurable", but I don't really understand how Kuhn arrived at his result that science is defined sociologically. In this vein, I'd like to know what is the problem with falsificationism that Kuhn's approach solves? 

According to the 'Mario Lives!' video, researchers have been able to develop an AI unit that is able to experience emotional states, such as greed, hunger, and curiosity. If the AI is currently experiencing an emotion, it will engage in certain behaviors. For example, if Mario is very greedy, Mario will look for coins. This approach is very similar to the Sims video game series, where emotional states are also represented, and AI characters will attempt to fulfill those drives. Now, I can concede that the Mario AI is effective at simulating emotions. The media, however, has gone beyond this. Two articles, in particular, troubles me. 

Could this scenario occur? Or would it is possible, as soon as Strong AI exist, that humans immediately come to a consensus that it does exist?