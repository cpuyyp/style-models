It will not answer your question directly, but I wanted to notice you about this ibdata issue in general. Unfortunately, there is no magic solution to decrease the ibdata file size... It is the central point of the InnoDB engine, removing it (even when database is shutdown properly) will cause data inconsistency/corruption. The only way to restart from a healthy state (with a small ibdata) is to reinstall a brand new MySQL instance and load the data from your origin, and PLEASE use innodb_file_per_table to avoid having all you data in one single file (MySQL doc here). To load you current data into your new instance, you can use mysqldump if your dataset is small (few GB) or use "Transporting tablespace" procedures to cleanly import table by table your dataset (example on Percona Blog here). You can also read this article form Percona Blog which explain many things regarding these ibdata issues. Max. 

I worked for a while on your request dude... your problem is that it's not really adapted for relational database (POOR PERFORMANCES), you will be obliged to use some scripting language, but there is some interesting results: I've created a tale with one line: 

Why do you mean by "certain time"? Replication Lag? Query Response Time? In a simple Master/Slave(s) infrastructure, the distribution of the reads/writes operations should be done with a load balancer or directly in your apps. Max. 

If you have more than one secondary index in your table you should have been to drop all but one and show the : 

Why MongoDB created 9 chunks of a collection that contains 85MB? I didn't change the defaut chunk size parameter: 

Yes it's the average size of each row in your table. row size * N rows = table size Row size = 83 B Rows = 22,000,001 Row Size times Number of rows: 

On your mysqldump did you dump your "mysql" database ? (it contains users tables) If you can connect to your master check your users: To see declared MySQL accounts use: 

I don't think there is a similar command, but if your keys are simple (not composed) you can do the trick. I usually generate a DELETE script with an SELECT CONCAT statement You have your "IDs" file : 

will replicate statements on ads database (your context should be ads, ) and ignore the statements on other databases: The context is : REPLICATED 

will replicate statements on ads database even if your are in an other context ( for instance), and ignore the statements on other databases. The context is : REPLICATED 

So we change the auto_increment value without ALTER the table, note that as the transaction was rollbacked there is still 0 line in my table: 

Instead of loading your MyISAM table and ALTER it after, change the MySQL engine directly in your SQL dump file (on table definition) : The table will be restored directly in InnoDB engine : MyISAM 

Don't do large restore with phpmyadmin, PHP will block you, you will have something like "Allowed memory size" after few megs loaded... Max. 

You can also generate this type of script with OS tools (awk or notepad++) You can run this type of script safely on a production master, you DELETE rows from the PK and you limit with LIMIT 1 so you do not risk to have replication lags. Best Regards. Max. 

Imagine you want to replace fname by firstname, lname by lstname and add a birthdate field. Create your new structure: 

No need to ALTER table its quite overkill. I use to do a fake insert in rollbacked transaction to do the trick. 

I've an MySQL 5.1 slave for our BI team. They need to make some CREATE SELECT with big select queries (several million lines). As CREATE SELECT is a DDL, if the replication attempts to update some rows in same tables than the SELECT statement, replication is blocked until the freeing of the CREATE SELECT. Do you now a good non-blocking alternative to thoses CREATE SELECT statements? I thought to an SELECT INTO OUTPUT FILE then LOAD DATA INFILE but they will fill out our disks as BI guys like to do... :) Max. 

This is a good question. You have several solutions but your table is quite big so none will be without pain :) You have three solutions to "shrink" InnoDB tables: 1. OPTIMIZE TABLE You can use as you mentionned it but you should care about the variable : 

I'm testing the MongoDB Sharding with a Sharded Collection and a forloop to insert arround 1M documents to see how splitting and moving works. I'm surprise that after few documents MongoDB starts to split chunks (well before 64MB), At the end of the 1M (and some) inserts i have thoses stats : 

There is one parameter doubtful for dev server in the default "Config Type: Development Machine", it's the table_definition_cache. I'm sure that yours is set to 1400 no? Comment it on your my.ini file or just set it to its default value : 400. The table_definition_cache parameter is an dynamic variable, so you ca set it without restart MySQL with: 

Replication status and lag are vital monitors you should take care of. Before start a backup you must know if your slave goes well. A simple will show you all needed infos: 

I inadvisable to use Datetime type for your answer_time, change it to TIME type because the 000-00-00 will fail with all datetime functiosn (look at the MySQL documentation : link). So, when your table looks like : 

Make a generic table call Users with a unique ID by line (user_id)and put your generic infos (email, password...), used for both Registered and Unregistered users. Next, create a table "Registered_Users" to store specific info, with a column user_id (which point to Users table : foreign key). If you want to query just global info about all your users, query : 

The mains "counters" are and for replication status and for lag (ideally at 0 second). If you have a slave dedicated to backups (that is a good practice), I recommend you to make a binary copy of your datadir instead (or in addition) of your mysqldump. The restore will be much more easier and quick. However mysqldump is good if you want to restore a partial backup (especially InnoDB tables) or restore a clean shrinked dataset. If your are afraid by corrumption or delta between Master and Slaves you can use the Percona tool pt-table-checksum (available in the Percona Toolkit) that "Verify MySQL replication integrity" easily. Max. 

The "the closest matching" constraint make me think to the MySQL function SOUNDEX. I never used this feature but maybe it's well designed for you :) Max. 

Then, with your mysqldump command, use the option to specify explicitly your conf file has your are in crontab. 

Let me explain: The whith InnoDB tables, locks the table, copy the data in a new clean table (that why the result is shrinked), drop the original table and rename the new table with the original name. That why you should care to have the double of the volumetry of your table available in your disk (During operation you'll need 2x700GB). When you are in the innodb_file_per_table = ON. All tables has it proper data file. So the statement will create a new data file (~700GB) when the operation is finish, MySQL will drop the original one and rename the new one (so at the end the 700GB -- probably less because it will be shrinked -- of data generated during the operation will be released) When you are in the innodb_file_per_table = OFF. All data goes to one data file : ibdata. This file has a sad particularity, it can't be shrinked. So during the process, your new table will be created (near 700GB), but even after the drop and renaming operation (and the end of phase) your ibdata will not released the ~700GB, so you wanted free some data but you have 700GB more, cool isn't it? 2. ALTER TABLE You can also use an statement, the will work in the same way as . You can just use: 

Note that your syntax is correct, all statements will be written on binary log (master) and the slave will ignore the and databases BUT YOU SHOULD EXIT THE CONTEXT and before run queries on other databases. The context is : REPLICATED 

Note that my note on innodb_file_per_table is true also for this solution. 4. mysqldump The last solution is to recreate all databases from a dump. Terribly long, but terribly efficient. Note that it's the only solution to "shrink" the ibdata file. Max. 

I dont know about feed_id but your timestamp column by definition have probably many differents values. I will advise you to try to create a simple index on timestamp column. Do you have "disparate" values too on feed_id column? For me you should take your attention on the timestamp column (WHERE + ORDER BY). Maybe it'll not a index problem, maybe your MySQL instance have some difficulties to sort the result (bad temp table management) so you can try without ORDER BY clause to see its impact. Let me know. Max.