There are two major concerns here. First you are using significant server resource (memory, CPU, disk) on a produciton box which, presumably, is busy. This will be delaying other work. In the extreme, your huge query may fully consume some shared resource and cause other work to fail. Second, your query may be locking data others are trying to use. There are ways around this which either compromise the integrity of your results or use further server resources. Mitigations include 

By growing upwards the tree maintains identical depth in every branch. This is important for predictable performance. (Some say the B in B-Tree stands for "balanced" for this very reason.) 

The estimated and actual number of rows differ wildly. The plan has a table spoo, likely as a result of the recursive CTE. Most of the action is in a worktable coming off that: 

Yes, deleting the unused images will free up space inside your DB files. To actually reduce the size of those files you will have to them after the images have been deleted. Here's a link to the MS Technet site for this process. Shrinking the files will most likely introduce internal fragmentation into your tables and indexes. A thorough would be appropriate afterwards. On a 10GB database, once a year, this shouldn't be an undue burden. 

If you compare any two authorative references for a single subject they will employ different words even though they convey the same meaning. This is just the nature of English - it has a very rich and overlapping vocabulary, accumulated from many sources over centuries. Looking at the examples listed in the question they all contain the concepts of data, collection and organization. To me they seem to be describing the same thing, even if they describe it in different ways. As used specifically in digital technology the term "database" covers a great many things. An IMS hierarchical database looks very different to a relational database, which is different again to a sharded, clustered JSON document store. Yet all these products use the same word to describe one tier of their offering. Writing a set of words that covers the usage these technologies make of the word "database" would be impossible. Yet all of them allow the collection and organization of data, so meet the definitons listed. 

Yes, an index will help. Without an index all the RDBMS can do is start at the beginning of the data and work its way toward the end, stopping when it finds what it's looking for. This is a O(N) operation. With a B-Tree index in place it reduces to O(log(N)). This is true whether the data is held on disk or in memory. 

To be effective there must be an index on the reversed text. This is an overhead at write-time and will use additional storage. Waiting until read-time would require a scan of the data, which rather defeats the purpose. See examples here amongst others. 

You may need to add some square brackets, depending on your naming convention. Write a similar query for each object type you're dealing with - constraints, indexes etc. There is a or table to cover all of them. At the end you'll have a script which will capture the current schema. Run it once per target database. Pipe the output to a file (or in SSMS use "results to file" setting). That file will be a new SQL DDL script which will effect your change and restore the database to its previous state, including all customisations. The meta-script and produced scripts can be pushed to source control for tracking. 

A columnstore arranges the data on disk differently to how a "normal" table does it. The column's values are split into segments of just over one million values. Each segment is compressed. Since a single column's values can show a lot of repitition (think "country code" or "product name") the compression ratio can be significant. Read performance can improve from several factors. First, only the columns required in the query are read off disk. Second, compression means much less IO for a given number of values compared to rowstore. Third, aggregate functions can be performed in what's called "batch mode," which is optimised for CPU cache utilisation. Compression is also available for rowstores. My experience is that CPU utilisation increases but IO drops, for a nett improvement in elapsed query time. This was for moderately large databases performing reporting & analytics. Of course, your mileage may vary. 

Do the same thing for monitors and use the local table variables to populate the mapping table. Of course you want to have appropriate validation, duplicate checking and error handling in the body of the SP, too. You don't say what scripting language you use. The documentation for it will tell you how to declare and populate stored procedure parameters for SQL Server. Response to OP's EDIT #2: First, a few tips. Please post the full error message; it helps immensely with debugging. Second, if you're using SSMS you can double-click an error and it will highlight the code in error. Third, get in the habit of closing your statements with a semicolon. It is not required yet but it will be soon. If all computers have exactly one monitor then the TVP is not needed. You are correct. How many developers have only one monitor these days? I've seen finance traders' stations with eight. In these cases you do want a TVP. Please, please, please do not be tempted to write . Your code will throw the error This is because of your third INSERT statement: 

SQL Server has various flavours of replication with different capabilities. It sounds like peer-to-peer transactional replication best fits your needs. 

The same argument holds for Relationship 2 and 3. has only two columns - the ID and the number. I'd suggest quite strongly that it would be an error to have two rows in this table with the same number. So a unique contraint on the number would be appropriate. The ID column, which is the primary key, will have a unique constraint, too. So we have a table where each row consists of a unique number (ID) paired with a unique number (the phone number). It seems to me one of these is redundant. You may choose to retain the ID as a surrogate key since an integer (the most likely data type) will be shorter than a full international telephone number, especially with formatting characters embedded. Adopting this would give the table 

creates a new execution context which ceases to exist when the executed statements return. The USE only has effect within the . Try this 

One risk mitigation is to use a signed stored procedure. Here's an example of this in action. In summary, you package all the sensitive action (table access etc.) inside a stored procedure. You create a certificate in SQL server to sign that SP. You create a new login who's only ability is to execute the SP and you send the password to this low-privilege login in plaintext. Depending on how sensitive the data coming from the SP are this can limit damage from a password leak. 

This is possible using piecemeal restore. The source database's objects and filegroups have to be organised in a way that supports this. The RESTORE requires additional specific keywords. While the concept may take a little bit of getting used to, the amount of scripting to implement is no more than that for other solutions suggested. 

There may be similar concerns for dates, depending on the chosen character representation. Again type casting can cure this with the costs mentioned previously. Occasionally you will find strings which contain digits only. Examples are national identity numbers, bank account numbers, phone numbers and such like. The common characteristic of such values is that it does not make sense to perform calculations on them. Other things being equal it may be OK to store these as columns, especially if they have optional embedded alpha characters, but the above considerations still apply. 

You may be able to optimise further by referencing only once in each part; I'll leave that to you to decide according to your needs and business rules. Oh, and stop using and . 

Storing the submissions is easy. Any persistence software with sufficient bandwidth will do. Heck, even the OS's native file system will do. From this perspective JSON in MySQL is excessive as you have the DBMS overhead. Similarly with an update. The system need only overwrite the current submission with the new one. The real constraint on your design will be on retrieval. You use the word "filter." I presume you want to see all submissions with a given value in a particular form field. To do that efficiently will require indexes. Capability in this area varies widely across the various NoSQL and relational products. You must learn what each can provide, gain a good understanding of your requirements, and choose. If you adopt a relational store you will likely gain a lot of advantage by shredding the JSON into table(s). This is how an RDBMS is meant to work. It may be difficult to settle on a schema, however, if there is great variabilty in your forms. There may be some advantage to holding, in addition to the submissions, a separate list (document / table) for each answer and pointing to all submissions which contain that value. This may become unworkable at scale because there will be a document keyed by answer holding an array of submission IDs. When any of those submissions is updated this document will also have to be updated. This will be a bottleneck when there are a large number of updates per second. It will require careful programming to keep the submission document and answer cross-reference document in sync during updates since Mongo does not have transactions that span documents. 

So the sub-queries may well be producing the extra rows but they are then filtered before the results are seen. Try UNION ALL. 

Deletes could be trickier, depending on how the chains interact. If they are all independent (A->B->C and X->Y->Z) then it is easy - simply remove the row and close up the pointers. If the chains merge (A->B->C->P and X->Y->Z->P) it gets more complicated. In extremis, they can be re-calculated from scratch, of course. Without pre-calculation a lazy evaluation may be in order. Once a path has been enumerated, it can be stored against the starting point (and all intermediate nodes) so the expensive path walk need not be performed again. Whether or not this is cost-effective will depend on the read/ write ratio. I will note in passing that this is textbook material for graph processing. If a move to another tool were possible, or when SQL Server's graph processing matures to a sufficient level, I would recommend considering that approach. 

MySQL supports columns of type bit. These only use one bit each, in sets of 8, as you would expect. This will give the same disk and memory density as you're currently getting, but with much easier query writing. 

The database designs for relational and document stores will be entirely different. You will not be able to take one and simply implement it in the other. What will endure, however, is the data model - the logical representation of the entity types, connections between them, cardinalities, constraints, attributes, domains and rules governing them. The data model represents what your customers know and how they act. The database is one physical representation of this, limited by the product chosen and hardware. Getting the model right and keeping it up to date will aid current development and smooth any future transition. The data model is a logical construct. It should not, in theory, be affected in any way by the features of the chosen storage product. From one data model several database designs can be derived. For a relational storage product the translation is quite direct. An entity type from the data model becomes a table in the database. A relationship from the data model becomes a foreign key in the database. Take the example of an order processing system. The entity types "Order" and "OrderLine" become the tables Orders and OrderLines, with OrderLines having a foreign key column of OrderNumber. For a document store the translation may be more complex. The designer has more choices to make. Order and OrderLine may be mapped to different collections, they may be mapped to different documents within the same collection, or order lines may become sub-documents nested within the order document. The relationship between the logical entity types Order and OrderLine can be implemented either by copying the order's number into the order line's document, having an array of the order line's IDs in the order document or by nesting the order lines as sub-documents within the order document. Which to choose will be determined by the application's requirements. Whatever the physical implementation the logical analysis in the data model remains constant: one Order, many order lines. The next obvious point is to be scrupulous in separating to data access tier from the others. This way future rework will be focused on that area. Having good test scripts will pay off now and will be doubly valuable after the rewrite, to speed regression testing. Finally, the chance of you migrating is slim. I do not doubt you will be successful. I do doubt you can guess now what your business and technical environment will look like several years from now. Everthing I have read by successful software companies says they rewrite the product three times before they get it "right." Use one of those re-writes to change persistence product and concentrate now on delivery and customer service. 

You never ever need the id column in any table. It is a surrogate key which replaces the natural key for pragmatic reasons. Often this is because a narrow, monotonic key gives better performance, or because join clauses become cumbersome with multiple natural key columns. Some shops have an id on every table as a standard. Consistentcy is good, so that would suggest having the id in this table, too. If this table is the parent of a 1:m relationship it can help performance to have the parent keys directly in the child table without having to follow surrogate keys. It does take room on disk. If you have a gazillion rows that may be an important performance consideration. This would suggest omitting the id. Broadly, in my experience it is best to omit the id from intersection tables, and add it later if the model changes. 

Since you seem to be primarily interested in stock levels varying over time for SKU/ store combinations I would suggest a time series database. There are many available. Some are propriety, some open source. Many are built on an existing storage engine, often with a SQL programming interface. They are deployed widely in finance, for listing stock prices, and science for collecting measurements. Knowing these industries and the data volumes they can produce I'm confident this technology will be able to handle your use-case (providing you can supply the IOPS and RAM). KDB (one of the best-established) has a specific retail solution. 

No, the way you've written it is the "best" way. Unless, of course, that way doesn't work. The joy and frustration of using a declarative language is the optimiser. It is your best friend when it works and worst enemy when it doesn't. One way to kick the optimiser into doing the right thing is to re-write your query in a semantically identically way such as: