This is happening on the primary (if it truly is blocking), you'll need to find what sessions are blocking it at why (on the primary for the primary). 

It is not possible to connect to SQL Server with a username/password combo and setting the trusted connection setting. This is because trusted connection doesn't mean "encrypted and secure connection" it actually means "use windows tokens instead of a username and password". It doesn't mean the connection isn't trusted it just means use my token which will either end up being protected through ntlm or Kerberos. 

Autogrow settings aren't your issue, sparse writes and running out of space on the volume, is. Update: 

When changing any of the services for SQL Server, always use SSCM. Always. Period. It will set the permissions for the new account to the basics. If before the local system account was used and unrestricted permission to everything on the system was had, I would expect something to fail permissions after the change due to tighter controlled security. That's not a SQL Server SSCM fault, that's an admin fault of not granting proper EXTRA permissions (such as accessing a network share, restricted folders, items outside of the SQL Server install purview, etc.) 

If that's the case, and you are either running 2017 or can upgrade to 2017, there are read scale availability groups where you don't need a cluster. 

No, it's protected but as shown in the article you need to have access to all of the keys in order to decrypt the values. The article takes a bunch of liberties and glosses over quite a few things when it comes to the protection of the keys. Try doing this without being a sysadmin, knowing the DMK password, or using transparent decryption hierarchy. Could you still do it - most likely not. The secrets in SQL Server are generally protected by levels of encryption - where one key encrypts the other which encrypts the other. If you can unwind all of those levels (which again the article doesn't even really talk about and just kind of glosses over) then absolutely you can decrypt the data. SQL Server doesn't use any homemade encryption, it's all industry standard algorithms which are publically documented. There is nothing stopping anyone from implementing something to reverse it if they had all the keys. This is why over provisioning permissions is a bad thing. 

We'd need to see the cluster logs to see what it thought happened. Other logs of use would be the output of sp_server_diagnostics in the logs folder, the failover policy level, and potentially the always on health extended events files. There also started to be issues with the hardening of logs: 

This totals out to 10 seconds. Now, remember the Time Out I was talking about earlier. If the application attempts to connect and receives a login timeout, that means we waited 15 seconds and didn't get a response. This doesn't mean we constantly tried for 15 seconds, it means we sent a request and patiently sat there, waiting. Adding in a single timeout wait of 15 seconds now brings our total up to 25 seconds. That's assuming a whole bunch of things. Wrapping up In the end, the actual failover time will be held in the SQL Server errorlog by finding the start of the failover and the database online messages. Additionally, extended events can be used to find the actual failover times. The application may (most likely will) see a longer "downtime" due to the way it is coded. In this instance, a default timeout value of 15 seconds is most likely adding and additional overhead. The code also sleeps for 1 second between retries which makes a total of 16 seconds. The connection string has a default database which means it takes us even longer to connect as the database is required to be online for the connection to be established. It takes time to detect and arbitrate the failover, this needs to be taken into account. More information: $URL$ 

This shouldn't be a problem if the node ip itself is changing. It's only a slight issue if cluster resource ips are changing. 

The initial size is going to be whatever Model is set to, in this case your model is 3 MB. Who knows why, could be a host of reasons which in the end aren't very important to be honest. Set Model to the starting size you'd like for all new databases or specify the size in the create database command/window. 

Availability Groups don't have preferred replica. They have primary and secondary replicas, but not preferred. Windows Server Failover Clustering has preferred owners. Are you looking for who was the primary replica or who was a preferred owner? If it's the WSFC preferred owner you are looking for, then you'll have to scrape the cluster log. If it's the primary and secondary replicas, then nothing exists out of the box - you'll need to write your own. This data can be gathered from a few different sources: 

REDO requires that LSNs be applied in sequence order, this doesn't change inside or outside of an AG. When the REDO thread is working, it'll have to do the log records in order. 

"It dependsâ„¢" Again, what are you trying to solve? I wouldn't over-engineer a solution to try and solve every possible edge case or problem. Have your top 3-5 true issues and solve for that. 

I'll reiterate that once a year is fairly standard, but it still isn't a great story. You have to change the account, there is a service restart required, and in general no one is happy about it except InfoSec. This is why you should modernize by using Managed Service Accounts and/or Group Managed Service Accounts (or virtual accounts). In MSAs, the password is automatically rotated and is not known by anyone, gMSAs work a bit different but you can think of them the same as MSAs for use with multiple computer objects. The automatic password rotation does not require a service restart. 

Now, you're probably thinking - ok that's great but I don't know the magical limit so it really isn't helpful. Well, it is and it isn't. If you specifically look at it from a numbers point of view then yes it isn't very helpful... however this is a terrible way of looking at it. It should be looked at as, "Am I collecting only the data I need?" and in most cases you'll never run into an issue with this error. If we take the definition in the question that doesn't work, some of the information collected seems as though it really isn't needed. Do you really need callstack, current thread id, cpu cycle time, worker address, and scheduler address? Callstack is variable, the rest are fixed, so just eliminating the callstack you could fit in more columns if needed. I'm not saying you need any more but you could. The whole point is to limit the definition to be as small as needed. Collecting everything is going to either result in errors (as you've had here), system slowness, too much data for analyzing, or even system halting. Just because you can doesn't mean you should. There is nothing stating that these limits will or won't change between major or minor versions, so keeping the true minimum need is the best prevention. Please don't just check every box (gui) or add every action possible. 

You have a few options, some of which were already pointed out. First, though, if these events haven't been captured so far there is no "historic" way of obtaining the data. There may be ways to get some of the data but chances are you won't get all or even most of it. Going forward, however, it seems you only want those who successfully connect to the instance. This can be complicated if something such as connection pooling with a sql login, but I'll leave that as something for you to decide on how you want to track it back to the users. 

You can do a copy only full, but unless C is the primary you cannot do a differential database backup. 

SQL Server and the SQLCMD utility do not, by default, track any of this. If you haven't yet setup customized logging then you won't be able to get any information. SQLCMD is a command line utility and does not keep any history itself. You can setup extra command line logging outside of SQLCMD, but there is no switch or setting to turn on to default log any and all SQLCMD input or output. The closest thing would be to create an extended events session or server audit that looks for connections through SQLCMD... However this would not be extremely worthwhile as application name and information can easily be spoofed. In short - if you don't have anything setup, there is nothing. If you did set something up, there is no guarantee that you'll get everything. If the question changes to "Who is executing what on my SQL Server?" this becomes an entirely different discussion. 

No, there is not. This always points to the default database path. If the path is removed, it will attempt to be created and have security applied to it a few times before returning an error. This is true as of build: 13.0.4422.0 and older If you feel that this should be changed to a configurable value - I'd invite you to create a request in Connect. 

Adding a listener does not require the AGs to be set to a specific availability mode. AGs can have 0 or more listeners (only one available to be added via the GUI) and be in any availability mode. 

If you want something straight out of the box, that's going to be difficult to find. However, 2016 does bring Standard Edition Availability Groups. Standard Edition before 2016 does have Mirroring. Additionally if you're using asp.net and using the session state databases, there may be additional considerations (like not mirroring or putting it into an AG as the data is volatile and not worth/needed kept). 

Secondary user databases in an availability group are not available to be written to (readable at best). Thus the upgrade failed as the database is part of the AG and not able to be written to. 

Yes, really, you do. In fact, you'll need a unique IP address for each listener for each subnet that listener could potentially live in. Why? Well it's not very helpful to have a listener that can't listen. This is true whether Basic AGs are used or not. 

Based on the curated errorlog output you gave us, shrinkfile was waiting on a current open snapshot transaction to finish: 

SharePoint is extremely hoggish when it comes to logging, depending on the features of SharePoint being used and the number of users. Chances are that the database is in the "FULL" recovery model. Depending on how heavily the users are adding/changing lists, documents, when the reindexer jobs are running (for SP, no SQL), etc, will all add to this. The FULL recovery model allows point in time recovery, which I am assuming is what your SLA is set to. If you can't change your SLA and this is needed, you don't have much choice. Switching to the "SIMPLE" recovery model may be possible, again depending on your SLA... but I doubt your users would be happy with it. In this recovery model, transaction log backups cannot be taken and thus you won't have any to deal with. To be honest, though, 10 GB a day for an active SharePoint database isn't that bad! I'm not sure if you're lumping any "extra" logging needed or done outside of SQL Server per the STIGs for your government (as most are just security policies and don't need to log anything) but if you are, there is nothing we can help you with in that respect. Here is an overview of the recovery models. 

That defeats the purpose of using Always Encrypted if the server that is running SQL Server also has the certificate to decrypt all the data. Please, please, please don't do this! Put it on the client machine or a "tools" or "intermediate loading" machine to get the data in and out. Then remove the key and keep it only on the client machines that are locked down. I'd also keep, in the back of your mind, a solution for when a client machine is lost/stolen/breached (laptop), or a server is compromised. How do you find out, how long do you have to rotate CMKs, do you know how to rotate CMKs, etc. 

Three may in fact be redundant, or it may not. It completely depends on how your host cluster is setup (physical interfaces and vSwitches) and how the physical network is setup (physical switches and networks). If the host has a few physical interfaces that all go to the same vSwitch, etc., then you'll get isolation at the windows level (more RSS queues, etc.) which can help identify issues or allow for other QoS options but it won't magically give you more throughput. This all assumes you'll be going outside of the same host for networking. 

Since these are secondary replicas the databases will return to a state. This will be helpful in the future. Allows the primary and any unaffected secondary replicas to stay in the availability group and continue with log backups and re-use. 

Probably my least favorite option but here for completeness. The logins could be logged to a table, but why re-invent the wheel? Trace and extended events will do this for you. $URL$ 

Yes, it should. We'd need a network trace to really dig down deep (from the app and the server side) and see what's happening if you're not having this behavior. The main point is that even when we use read only routing, the client will always contact the primary first. Period. The only time this wouldn't happen is if the client connection string pointed specifically to a node name instead of the listener name. Assuming that isn't the case (you made no mention if you're using node or listener name or how the test was run [could you add that please?]) it will always connect to the primary first. 

If you want it set to 1, then you should configure your backup preferences properly. The reason it is not returning a is because it shouldn't be based on your current preferences. Right now, all of your replicas have the exact same backup priority which means we're going to next look at replica names. Since sorts first, that should be the secondary (given that is your primary) that the function returns a on. If you want the backups to happen on a different server, then set the priority appropriately. Here is an extremely simple repro showing that given your current settings, should be the replica which a is returned. 

No, there is no trace flag or way to override this behavior. If any statement has a restricted word in it, it will be overwritten with "-------" or "** restricted **". This is the same whether it is a trace, in cache, or using XEvents. 

Through many hours of painstaking work. You can use sys.fn_dump_dblog() to look at transaction log backups that are taken after redo and undo has run. Since the SQL Server Log records and files aren't documented, except for physical and logical architecture, you'll just have to search the internet and make your own best guesses. If you're asking because you'd like to make software to deal with the log, then you'll need to get in touch with Microsoft Partner Resources. Otherwise, you're on your own, armed with the collective knowledge of the internet and volumes of your favorite beverage. 

Personally the name of the nodes shouldn't really matter. Yes, I get naming conventions and whatnot, but at some point (just like which node in a cluster owns a specific role) there is diminishing returns. This is one of those times where you're going to put way more effort into changing it than I personally believe you'll get out of it. 

Using AOAGs could potentially help with this, depending on how much downtime is considered to be "too much". There is still a blip associated with switching the primary to a synchronous secondary as the clients will be disconnected (this may or may not be an issue in your environment) as the move happens. They will need to reconnect using the listener name (so no connection string changes should be required) and be using a library that supports this. 

Stop trying to use the transaction log for these types of situations. Take any notion you have of using the log for data mining and defenestrate it. 

The SQL Server log isn't really human readable, though the functions help. I stated before, it's not always that easy as change 100 to 150. Let's say you changed 'Hi' to 'Sean'. This might cause a forwarding record, or a page split. It might be the first record in a table, or a heap, or be changed because of multiple indexes. I'm not trying to put you off, just that academia and the real world and generally two very different things. 

I would not stop taking log backups but you could put into place an aging mechanism in your backup software or process to eliminate files that are no longer needed. Using your example, you have many paths but the basis is as such: Always take a tail of the log backup first