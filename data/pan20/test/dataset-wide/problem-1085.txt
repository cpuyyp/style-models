Assuming you aren't using any newer features or index/collection options from MongoDB 3.2+ that aren't supported in MongoDB 3.0, you should be able to restore the backup. Before doing so, I would review the downgrade instructions in the MongoDB documentation to check for potential issues: 

MongoDB does not return the results of field-level updates like an operator; the overall document update either succeeds or fails. However, assuming you are using MongoDB 2.6 or newer, you can infer whether the update was a no-op by inspecting the returned on update/upsert. Example below using MongoDB 3.2. Test data: 

If you later rejoin that to the same replica set, the assumption will be that no writes have happened outside of the oplog. The member may also have an older view of the replica set configuration if there have been changes since the member was offline. Replication will try to sync to a common point in the oplog of a current replica set member (assuming there is one), but you will have introduced data inconsistency via direct updates in standalone mode. Local writes will not be rolled back (or replicated) because the oplog has no record of the changes. This can lead to data loss and other conflicts, because breaks the requirement that all members have the same data (aside from oplog entries not applied yet due to replication lag). If you follow the forced reconfiguration approach noted earlier, you would end up re-adding former members to your new replica set configuration and would not lose any data. 

A normal replica set failover situation involves one or more members being unavailable, subject to the fault tolerance of your replica set configuration. As long as a majority of configured voting members in the replica set are healthy, they should be able to automatically elect a new primary without manual intervention. If a majority of members are unavailable and you need to manually reconfigure the replica set to recover write availability, the correct process to follow is a forced reconfiguration with the surviving member(s). This process will ensure that the replica set version information is updated so when former members rejoin they will detect the new configuration and resume syncing if possible. In this disaster scenario, you are effectively reconfiguring your replica set to have a single member (), and later re-adding other members to rebuild the replica set. 

You should definitely follow the authentication schema upgrades and other procedures as documented in the relevant version upgrade instructions. The documented procedures are the only supported (and tested) upgrade path. Upgrade steps for authentication schema changes are particularly essential, but skipping any required steps or versions on the upgrade path is very likely to cause issues with your deployment. You should also take a backup before each major version upgrade. If something goes awry in your upgrade process or some required steps have been skipped, you will then have the option of restoring a previous known-good deployment. 

The error message in your screenshot indicates you are trying to add identical members to your replica set under different names: 

If you want to migrate with minimal downtime, a straightforward approach would be to use MongoDB replication: 

You can use the command line option or the config file option to provide extra storage (i.e. a second volume) for working space during the repair process: 

This query isn't covered because the full document has to be fetched to discover the value. The field is included in query results by default since it is the unique identifier for a document, but won't be included in a secondary index unless explicitly added. The and metrics in results will show how many documents and index keys were examined: 

You can increase settings to try to support this use case, but with millions of collections you are likely to run into other performance issues (for example, commands like would have to iterate stats for millions of files). Unless the millions of collections and indexes were created accidentally, I would consult with the developer and advise them to rethink their data model. 

The strict mode is only only intended as a data interchange format (for example, transferring a collection via and ). For queries in the shell you should use the representations as noted in the MongoDB Extended JSON documentation. 

The provenance of information on the MongoDB production notes isn't apparent at the moment, but WiredTiger and the recommendation to use XFS was definitely added much later than the Linux kernel details. The production notes share collective experience from known issues, but are typically recommendations rather than strict guidance. Most notes are added at the time a widespread problem is observed, but circumstances could certainly change. 

These are both options for custom point-in-time snapshots, which can be time and resource intensive for Ops Manager to process. The backend processing involves restoring the nearest (older) stored snapshot and then applying the oplog changes up to the specified point in time (as marked by a date/time in seconds or an oplog timestamp). Both are limited by the available oplog history as specified in your configuration (i.e. 24 hours). The restore point creates a custom snapshot up to the selected date and time; the is a more precise custom snapshot which presumes you know the exact oplog timestamp you want to restore to (for example, up to and including the entry preceding an accidental collection drop). 

Given your current replica set configuration, this behaviour is expected. In order to be elected (and remain) primary, a replica set member must have connectivity with a majority of voting replica set members. Your configuration has two members which have and cannot be elected as primary, so there is only one candidate for primary (the member ). However, all of the members in your replica set are voting so the required majority to elect a primary is 2/3 configured replica set members. Based on the log information you have provided from the member you prefer as primary, the replica set member state timeline looks like: 

If you have a sharded cluster with multiple hidden members, you can only read from one hidden member at a time via a direct connection. Note that bypassing with a direct connection to a hidden secondary may return stale or orphaned data for a sharded collection. If you want to dedicate nodes for specific read use cases in a sharded cluster, instead of making these hidden you should use secondary read preferences with replica tag sets. Be aware that reading from secondaries may return stale, duplicate, or orphaned data for a sharded collection as at MongoDB 3.4. For more background, see SERVER-5931: secondary reads in sharded clusters need stronger consistency. An improvement for SERVER-5931 was recently committed to the MongoDB 3.5 development branch, so secondary reads in sharded clusters should be safer in the MongoDB 3.6 production release (expected later this year). 

You can only connect to a hidden member directly (i.e. by specifying a single hostname and port). Read preferences and tags are ignored for direct connections. For example, any of the following should work assuming is a resolvable hostname and you aren't blocked by firewall configuration: 

Once the shard has a primary, normal operation can resume. You should also remember to re-enable the balancer if you disabled it during your outage. 

This is incorrect. Without a primary you cannot write to a replica set, but reads are still possible using non-primary read preferences such as , , , or . The default read preference is , which provides strong consistency (versus eventual consistency when reading from a secondary). The read preference reads from a primary if available, otherwise from a secondary.