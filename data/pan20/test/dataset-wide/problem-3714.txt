If you write the problem of finding the closest point to $x_0$ on the cone with a Lagrange multiplier, the solution must have the form $x = (\lambda A + I )^{-1}x_0$. If you start by diagonalizing $A$, the inverse can be computed efficiently and you can search for $\lambda$ by dichotomy. The algorithm will run in $O(n^2 \log{1/\epsilon})$ 

cis.syr.edu/~wedu/Research/paper/duthesis.pdf My favorite protocol is the 2nd one. One variation of it would be: Alice breaks down u as $u_1 + \ldots + u_p$ and transmits set $( \{u_1^0, u_1^1\}, \{u_2^0, u_2^1\} \ldots, \{u_p^0, u_p^1\} )$ to Bob where $u_i^1$ are just a random vectors. Bob computes the dot product of each vector with a $v$ and adds random number $\epsilon_i$. Using the oblivious transfer protocol Alice gets back $(u_1^0.v +\epsilon_1, \ldots, u_p^0.v + \epsilon_p)$ and thus can compute $u.v + \sum \epsilon_i$. Bob can then divulge $\sum \epsilon_i$ and $u.v$ is known. I'm a little disappointed there isn't an answer that doesn't rely on modular arithmetic. I was hoping for a purely geometric protocol. 

The solution is the Poisson estimator. Let $a_i$ = $\log x_i$. We would like to approximate $\exp (\sum_{i=1}^n a_i) = \exp (n \hat{a})$ Draw $\kappa \sim \textrm{Poisson}(\lambda)$, then draw with replacement $a_{i_1}, \ldots, a_{i_\kappa}$ and compute $y = e^{\lambda}\prod_{j=1}^{\kappa} n a_{i_j}$ $$E(y) = \sum_{k=0}^{\infty} \frac{1}{k!} E\left(\prod_{j=1}^{k} n a_{i_j}\right) = e^{n\hat{a}}$$ 

Let $X_1,...,X_m$ be connected, simply-connected CW sub-complexes of a CW complex $X$. Let the symmetric group on $m$ letters, $S_m$, act on $P:=X_1\times\cdots\times X_m$ in $X^m$ by permuting components. Let $Y$ be the smallest subspace of $X^m$ that contains $P$ that is also stable under this action of the symmetric group; the saturation. Is $Y/S_m$ simply-connected? If $X=X_1=X_2=...=X_m$, then $Y=P$ and $Y/S_m$ is the usual symmetric product construction. Then the answer is yes by Dold and Puppe since the $m$-symmetric product of $X$ has its fundamental group equal to $H_1(X)$; which is trivial since we are assuming $X$ is simply-connected. This newly published paper addresses a different direction, namely, restricting the action of the symmetric group to a smaller subgroup. I have some ideas of how to proceed subject to certain conditions: first show the saturation is simply-connected using van Kampen, and then using some kind of path-lifting property show the quotient is $\pi_1$-surjective. However, I would like to know about references, interesting examples, or if someone has a solid proof under some specific conditions. 

This question is inspired by this MO question; indeed it is a special case on which to focus. An exotic affine space is an affine variety $V$ whose $\mathbb{C}$-points are diffeomorphic to $\mathbb{R}^{2n}$ yet $V$ is not algebraically isomorphic to $\mathbb{A}^n$. Say that two varieties are count equivalent if they are both polynomial count varieties with the same counting polynomial. As shown in the comments here, the Russell Cubic is count equivalent to $\mathbb{A}^3$ although it is not isomorphic to $\mathbb{A}^3$. 

Suppose I have a random variable $X_0$ with a p.d.f $f_0$ supported on the real interval $[a_0, b_0]$. $X_1$ is the restriction to $[a_1, b_1]$ of the sum $X_0 + g$, where $g$ is normally distributed $g \sim \mathcal{N}(0,1)$ $$f_1(y) = \frac{\int_{x=a_0}^{x=b_0} f_0(x) e^{-(y-x)^2/2}~dx }{\int_{x'=a_1}^{x'=b_1}\int_{x=a_0}^{x=b_0} f_0(x) e^{-(x'-x)^2/2}~dx~dx'}$$ or $$f_1 = \frac{1}{Z} L( f_0, (a_0,b_0,a_1,b_1))$$ Where $Z$ is a normalizing factor. $$Z = \int_{a_1}^{b_1}L( f_0,(a_0,b_0,a_1,b_1))(x')~dx'$$ $L$ is a linear operator over functions in $\mathcal{L}^2$ What are its eigenvectors? What happens when I replace the interval with a $n$ dimensional box and the normal distribution with a multivariate normal? Thanks! Clarification: I'm looking at f in $\mathcal{L}^2(\mathbf{R})$, not $\mathcal{L}^2([a_0,b_0])$ otherwise $L$ is obviously not an endomorphism. 

Consider the family of distributions having the form $$f = \frac{1}{Z({\alpha,\beta,A,B})}\prod_{j=1}^{n}\left(\frac{1}{1+e^{-x+A_{j}}}\right)^{\alpha_{j}}\prod_{j=1}^{m}\left(\frac{1}{1+e^{x+B_{j}}}\right)^{\beta_{j}}$$ $Z$ is the normalizing constant. Typically $n$ and $m$ are both around 3. The $\alpha_j$ and $\beta_j$ are in $\mathbb{R}^+$ and the $A_j$ and $B_j$ in $\mathbb{R}$. I believe a sufficient statistic for this family of distribution is the set of expectations $$\left\{\mathbf{E}(\log(1+e^{-x+A_j}))\right\}_{j\leq n} \cup \left\{\mathbf{E}(\log(1+e^{x+B_j}))\right\}_{j \leq m}$$. This motivates the following questions. (1) Is there an analytic form for $\mathbf{E}(\log(1+e^{-x+A_j}))$? if so (or if not), is there an efficient way to numerically approximate it? (2) What of $Z$? Calculating $Z$ would be a good first step it seems, as (if I'm not mistaken) $\frac{dZ}{d \alpha_i} = -Z\mathbf{E}(\log(1+e^{-x+A_j}))$ 

I am hesitant to try to add anything further to the solid answers here, but here it goes anyway. In practice, here is what I do: 

As shown by David Speyer in the comments, if $\dim G=0$ then yes. For example, let $G$ be the solutions to $z^3-1$. Then over $|G(\mathbb{Q})|=1$, but $|G(\mathbb{C})|=3$ and hence $G$ is not trivial. On the other hand, the comments by Brian Conrad show that if $\dim G \geq 1$, then $|G(k)|\not=1$. I think this proves it: Since the identity component of $G$ is a connected affine algebraic group over $k,$ it suffices to prove this for $G$ connected. Then, since we are in characteristic 0, $G$ is isomorphic (as a variety, but not as an algebraic group) to $(G/G_u) \times G_u$ where $G_u$ is its unipotent radical. The unipotent radical is likewise isomorphic to an affine space, and $G/G_u$ is reductive. By the Bruhat-decomposition $G/G_u$ contains an affine open subset whose $\overline{k}$-points are isomorphic to $(\overline{k}^*)^n \times \overline{k}^m$ where $\overline{k}$ is an algebraic closure of $k$. 

If the surface is not a pair of pants, there are infinitely many different isotopy classes of pants decompositions (which, as mentioned in the comments, can be made compatible with a constant curvature metric). They are all "connected" by a finite sequence of "moves". The collection of the isotopy classes (thought of as 0-cells), connected by edges (1-cells) if they are related by the aforementioned moves, and with 2-cells added according to relations among the moves forms a 2-complex that Hatcher has shown to be simply connected. See here for precise definitions and the proof. On the other hand, the mapping class group acts on the 1-skeleton of this complex (see here), and the quotient is a finite graph. So there are finitely many homeomorphism classes of pants decompositions. As far as I can tell, the precise number of homeomorphism classes of pants decompositions is not presently known. See here for a lower bound however. 

So you want $$\forall x,\\, x^t (\Lambda^{-1} - \Sigma^{-1}) x \geq 0$$ $(\Lambda^{-1} - \Sigma^{-1})$ is a symmetric matrix, and the condition expresses that it must be a positive matrix too. Let $T = \Sigma^{-1}$ and $M = \Lambda^{-1}$ we want to minimize $\det(M)$ with the constraint that $M-T$ is semi-definite positive. This constraint can be expressed by calculating the Cholesky decomposition of $M-T$, giving $n$ inequality constraints. At this point, I would suggest resorting to quadratic programming to solve the KKT system. (See for instance this). The Cholesky algorithm can be adapted to compute derivatives in the constraints. Edit: removed a system of equation that only relied on the (necessary) constraint $\det(M-T)\geq 0$ We can deduce a couple simple bounds. For instance, the non negativity of $M-T$ implies that all its element are themselves non negative, thus $\frac{1}{\lambda_i} \geq \Sigma^{-1}_{i,i}$ and $$\prod_i^{n} \Sigma^{-1}_{i,i} \leq \det(\Lambda^{-1}) \leq \hbox{Tr}(\Sigma^{-1})^n$$ 

The Viterbi algorithm works by maintaining, at all times between $[t,T]$, for all states $S^k_t$, the likelihood of the maximum likelihood path between this state and any state $S_T$. You can run it a second time, but this time, maintain the likelihood of reaching any state $S_T$ following a path that maximizes likelihood while being less likely than the one previously calculated. To avoid dealing with exact duplicates, you can add a very small noise to every edge. In maybe-compiling C++, 

Also, deformations of $(G,X)$-structures on 3-manifolds motivates studying these spaces. For example, $\mathbb{RP}^3$-manifolds; all of Thurston's eight geometries admit $\mathbb{RP}^3$-structures. 

Let $V$ be an algebraic variety. If there is a finite ascending chain of Zariski closed sets $\emptyset=V_0\subset V_1\subset \cdots \subset V_n=V$ such that $V_i-V_{i-1}$ is a fintie disjoint union of copies of affine space $\mathbb{A}^i$ we say $V$ is affine paved (so $V$ is "algebraically cellular"). Note: there are non-equivalent variations of this definition (see here). One can deduce that an affine paved variety (over $\mathbb{C}$) has no odd cohomology and its even cohomology is free abelian. Examples: 

Algebraic Systems Biology: A Case Study for the Wnt Pathway by Elizabeth Gross, Heather A. Harrington, Zvi Rosen, Bernd Sturmfels. A Hilbert Scheme in Computer Vision by Chris Aholt, Bernd Sturmfels, Rekha Thomas. 

Computational Aspects of Polynomial Identities by Alexei Kanel-Belov, Yakov Karasik, Louis Halle Rowen Polynomial Identities And Combinatorial Methods by Antonio Giambruno, Amitai Regev, Mikhail Zaicev Polynomial Identity Rings by Vesselin Drensky, Edward Formanek Razmyslov, Yu. P. Identities of algebras and their representations. Translated from the 1989 Russian original by A. M. Shtern. Translations of Mathematical Monographs, 138. American Mathematical Society, Providence, RI, 1994. xiv+318 pp. ISBN: 0-8218-4608-6 Beidar, K. I.; Martindale, W. S., III; Mikhalev, A. V. Rings with generalized identities. Monographs and Textbooks in Pure and Applied Mathematics, 196. Marcel Dekker, Inc., New York, 1996. xiv+522 pp. ISBN: 0-8247-9325-0 

Using the formula for the pdf of the Irwin-Hall distribution one gets $$S_n = \frac{\sqrt{n}}{(n-1)!}\sum_{k=0}^{\lfloor \frac{n}{2}\rfloor}(-1)^{k}{n \choose k}\left(n-2k\right)^{n-1}$$ It's fairly straightforward to see why, imagine you're drawing random point in your cube, how many will have coordinates that sum to less than $\epsilon$ in absolute value? This gives you a $2\sqrt{n}\epsilon$ thick slice of hypercube around the hyperplane. Take the limit as $\epsilon \rightarrow 0$ The first values are, $1,2\sqrt{2}, 3\sqrt{3}, \frac{32}{3}, \frac{115\sqrt{5}}{12},\frac{88\sqrt{6}}{5},\ldots$ Using the central-limit theorem gives the asymptotic $$S_n \sim \sqrt{\frac{6}{\pi}}2^{n-1}$$ This paper proposes an algorithm for a slight generalisation 

One road is for Alice and Bob to generate vectors $\epsilon$ and $\eta$ where every entry is a random normal number with a very large variance $\nu$. Alice sends $u+\epsilon$ to Bob who computes and publishes $u.v + \epsilon.v$ Bob sends $v+\eta$ to Alice who computes and publishes $u.v + \eta.v$ They also compute $(u+\epsilon).(v+\eta)$. With all that, they can compute $(u.v - \epsilon.\eta)/N$. There remains an error term, $\langle \epsilon.\eta \rangle/N$ which has standard deviation about $\nu/\sqrt{N}$ The problem is that there is a tradeoff here. If the $\nu$ is too small, too much about the vector is divulged, if it's too big there is too much noise in the result. Another idea would be for Alice and Bob to agree on a random base of $\mathbf{R}^N$. Alice could then pick $\epsilon$ as a random linear combination of the first $N/2$ vectors, and Bob could pick $\eta$ as a linear combination of the last $N/2$ base vectors. This guarantees $\epsilon$ and $\eta$ will be orthogonal, but now Bob would know the projection of $u$ over a very large subspace. Not that good. Thougths?