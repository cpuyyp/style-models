This will mean that you can restore to the point in time before the upgrade (last full backup and log backups until the last one) and if you restore the last full backup and the differential made after the database was put back into the full recovery model and any log backups made after that. And finally the best solution: Schedule your log backups to run every 10-15 minutes during the upgrade and dont change the recovery model to simple. 

In both cases you end up with a CRT that you can import to the machine store of the SQL Server and then use the SQL server configuration manager to encrypt the connections. You will have to trust the root certificate in the latter case though and in the former case, if you are not running on a domain you will have to install the root certificate for the windows CSA on both the machines 

There are two f3 values maximum per each f1,f2 combination. For this specific example, I want to get top-2 minimum values per f1 and range (f2 - max(f2) per f1). Example output: 

The public schema is supposed to be visible by all users. You should not restrict rights of public schema to just one group. So, if you do not use: 

Please be more specific: "It will contain a lot of rows". How many? Millions, thousands or billions. "Is it a good idea?" It depends. If your queries are like the ones you mention, you should create two b-tree indexes, one for each field. Instructions are here: $URL$ You should only create one index for both fields, ONLY IF all your queries are like: 

also has the same values for dbFields A and B. Is there any efficient way to actually retrieve all records that have the same value as the top-k records? For example, when I search for the first 100 records to get instead 102 records if the last two have the same values as the 100th record? Is there any index to accelerate such queries? I do not mind if it has to be done with pl/pgsql (and not plain SQL) if the implementation is efficient. 

AFAIK there are three major structure types used for storing Oracle index data: B-tree, R-tree and Bitmap. All indexes use one of these structures. However, not sure about CONTEXT, CTXCAT and CTXRULE Oracle text indexes... 

We can create the database trigger on concrete schema event (ON SCOTT.SCHEMA) or on all schemas (ON SCHEMA). However, we can also use ON DATABASE when creating database trigger. What is the difference between them? Is it some legacy stuff? ON DATABASE should be used when using AFTER STARTUP or AFTER STARTUP because it's definitely related only to database but the same stuff that is done using ON SCHEMA might be done using ON DATABASE, so what's the difference? I can't find references in Oracle docs about that. 

What is the difference between Oracle clusterware and Real Application Clusters? If we have 2 servers and want to join them into a cluster why there are two separate software products? 

If the query has not got any specific ordering, it returns the first 10 rows it receives. But if there is a "" or "" clause, it must first get the full resultset and then fetch the first 10 rows. If a simple "" with no "" or "" clause takes that long, I would probably suggest to ANALYZE the table $URL$ Also you should check if this DB table has a primary key. If not, setting one would probably speed up the query. 

To create the field RNG is not required. I only added it to show that for , there are 3 ranges: created by the distinct values of for . For each such range I want to calculate the top-k minimum values per f1 and range. SQL Fiddle here: $URL$ Building the Ranges may be done by: 

The trigger does not contain autonomous transaction procedure with commit inside that, so who is commiting the insert? This triggger works like a charm and inserts new record into log table after user logon. It smells like hidden Oracle functionality and I can't find any reference in Oracle docs about that. I'm using Oracle11g. 

Suppose we have a server that has only 1 CPU with 1 core, so we need 1 Oracle database license for installing a database into it. Now, if we would need to add a second server, combine them into cluster and install database into it, then how many licenses do we need - 2 database licenses and 1 Clusterware licenses or 1 database license and 1 cluster license? I'm not sure, how Oracle treats joined into Oracle cluster servers - as one server or still as separate servers? Because we would need to buy expensive Oracle cluster software, then it would be logical for Oracle to treat joined servers as one. 

I want to generate 10 sets of such integers. Each set must have an integer identifier, something like that: 

If this calculation is always the same you can add an extra DB field (sclr5) that holds this information. You may update the DB table with one query, if the table is static (no new rows will be imported) or with a trigger if you regularly add new rows. Then you can add a B-tree index to the sclr5 DB field to accelerate the and the clauses. 

First of all, for 2000 records (as others have stated) everything will work. So, for the OP the natural key will still work. On the other hand VARCHAR fields for primary keys are (in most of the cases) a bad idea. They are inefficient, hard to index and provide slow performance. In most of the cases a numeric field ( int / bigint) will probably work better. So, in the argument natural vs surrogate, the correct answer is: It depends. Stick with the natural key if it fits your and your app purposes. If not, use a surrogate. 

Hash indexes are usefull when you have a large table with URLs and you need to query by them. So, a solution would be to have an additional column "url_crc32" that will be filled with hashed URL value via trigger on inserts. An index on url_crc32 would be definitely faster than index on URL column that is a type of text. It's not very common case to query data by URL. The more frequent case would be to query text data by fragment and hash indexes are useless in such case. So, I'm curious do you use such hash indexes, and if so then when do you use? IMHO Oracle does not have native hash indexes, so that must be done manually. 

Is it possible to monitor role and privilege grant/revoke using triggers? I' aware of doing that using Oracle audit tools however, it's interesting is it possible to do that using triggers. 

You will have to call /pages/folder.aspx with an ItemPath parameter as absolute path /Test which url encoded will look like this. 

For each file which is in SIMPLE recovery and not having wait_desc = "active transaction" you can simply do 

Now if you only have access to the unc path but not the linked server then the @backupfile becomes like this:. 

Yes it is possible to set one of the instances to run on port 1433. It is also safe as the dynamic ports will be set in the range of 49152 and 65535 as stated above. You use the configuration manager to set one of the instances to start at port 1433 (clearing the configuration for the dynamic port) and it will run on that port afterwards. 

DBNETLIB is an old library, create a client alias by running sqlcliconfg and create an alias for the old servername pointing to the new servername and correct port 

Oracle has partitioning and clustering features. Partitioning enables to split table or indexes into multiple tablespaces and store on various servers. So it's done manually. Clustering enables several servers to make operate as one. This IMHO is handled transparently. However, it's possible to have several tables (in different server databases) and group them as one, and use it. From the perspective on table partitioning and table clustering, IMHO both ways implement shared everything because the data is split into the different locations. The theory about shared nothing architecture is that each database node is independent. But how this looks in practice? Both ways (partitioning and clustering) splits data into different sources and could be also described as horizontal partitioning. However, if we had for example, two different database servers we would need to synchronize the data between them... Anyway, any thoughts on that would be appreciated :) 

For that purpose you can download the pre-built virtual Machines (VMs) provided by Oracle. They only need Oracle Virtual Box in order to check them out ($URL$ You can download them here: $URL$ This is the best solution for learning Oracle DB, without actually messing up your system. 

You may safely work with the public schema with all accounts. If you do not want specific accounts to mess up your public schema tables, then create a new role (for users of your app) and revoke rights from this particular role inside the public schema. Something like: 

I believe that PostgreSQL can log slow or unsuccessful queries. Probably I can set also PostgreSQL to log all queries executed. Contrarily, I am interested to know if there is a way that a malicious attacker can get access to all the queries successfully executed on the PostgreSQL server, if I have disabled logging as much as possible. Is this possible? Or once a query has been successfully executed (might be a SELECT or UPDATE query) I can be 100% sure that the DB server has no memory of successfully executed queries and therefore no one else can get access to this information. I am using PostgreSQL 9.3. 

The database server is being backed up by a third party tool not the maintenance plans. The following query will show you the backup vendor information 

Install SQL Server on the new nodes Use SP_HELP_REVLOGIN to copy the user account to the secondary node Take a full backup and a single log backup of the primary server and restore on the secondary with no_recovery Break the mirror and remove the secondaries from the primary server Create the endpoints with TSQL or use the Mirroring wizard to configure them and endpoint security start mirroring 

For most parts I'm referencing Paul Randall's Inside the storage engine blog series. The only way to reclaim unused space from database files in SQLServer is using the DBCC SHRINK command which reallocates data within the database files freeing pages and after removing them from the Global Allcation map removes them from the database file. This operation is slow, creates fragmentation within the database and is even slower when dealing with LOB pages as those are stored as linked lists within the database files. Since you are dropping the NTEXT column you will have to wait for the ghost cleanup process to drop the data before shrinking. Now having lots of free space in the database files will actually do you no harm, if you have the disk space, backup compression will take care of the free space within the files. If you absolutely want to make the files smaller you can create a new filegroup with the database and make it the default and then move the tables into the new filegroup but this can take time and cause downtime. I have used the technique explained here by Bob Pusateri with good results.