This indicates that that the IO Thread is still downloading entries from its Master's binary logs. The longer you take to address the SQL error, the more the will pile up. If the Master is in a heavy-write environment, relay logs will pile up quickly and puts the Slave at risk for disk space issues. If you do not want to monitor , you can always set the relay_log_space_limit. This force a cap on the amount of disk space allow for relay logs. For example, if you set this: 

I would upgrade away from MySQL 5.0 altogether. Please note you are using mysqld-nt.exe It has been a very long (probably over 1.5 yrs) since the NT-featured mysqld was distributed. The only benefit to using mysqld-nt was if you are using shared memory or named pipes, which were just nuances when MySQL 5.0 came out for Windows. The MFC C++ runtime distributables have been updated several times over the years. If you have been running updates against Win2003, things are probably very different for mysqld-nt.exe now. Before upgrading, try using mysqld.exe instead of mysqld-nt.exe. If you still get the same crashing behavior, switch out of MySQL 5.0 for Windows and go to MySQL 5.5 because MySQL Support for Windows 2003 went EndOfLife back in April of 2011. I would also think about using a Windows 7 Server instead of Windows 2000/2003/NT. 

MySQL restart not needed. See MySQL Documentation about join_buffer_size OPTION #3 : Make sure is indexed Since you have a reference, this is a rather moot point. If you do not have the , make sure link is indexed: 

Here is an explanation on how works vs how works flushes all data buffers of a file to disk (before the system call returns). It resembles but is not required to update the metadata, such as access time. Applications that access databases or log files often write a tiny data fragment (e.g., one line in a log file) and then call immediately in order to ensure that the written data is physically stored on the harddisk. Unfortunately, will always initiate two write operations 

This will give the Recommended Setting for the size of the InnoDB Buffer Pool (innodb_buffer_pool_size) given your current data set. Don't forget to resize the InnoDB Log Files (ib_logfile0 and ib_logfile1). MySQL Source Code places a cap of the combined sizes of all InnoDB Log Files must be < 4G (4096M). For the sake of simplicity, given just two log files, here is how you can size them: 

The easiest way would be to change the global setting for long_query_time to a ridiculously high number, run the mysqldump, and change long_query_time back to its original value. Perhaps something like this 

EXPLAIN was meant to do queries, not functions or procedures. When it comes to procedural code, you must think like a developer all over again. For what purposes would you optimize code, especially for a database ? 

As long as you do not perform any INSERT/UPDATE/DELETE statements directly on the Slave, your Slave should be just fine. Otherwise, MySQL Replication could break if you INSERT an new row in mydb.mytable on the Slave and, via Replication, the Slave later detects an INSERT of a row to mydb.mytable with the same PRIMARY KEY. This produces error 1062 (Duplicate Key). The only way you could write to Slave without breaking MySQL Replication is this: 

The index has to be utilized in this case. Why did it not use the index with your original query? I blame it on the WHERE clause because the Query Optimizer saw sale_id lookup first and then probably decided that with the remainder of the WHERE clause that a full table scan was the path of least resistance. Give it a Try !!! 

Interestingly, @DTest gave a good idea in his answer. Make a Stored Function. However you have to pass in three parameters: NetID,UserID,EventID. Here is that Stored Function DoesUserHaveEditPrivs using the new refactored query: 

There are no options for handling the output of NULLs. You may want to experiment with . This will dump BINARY, VARBINARY, BLOB fields in hexadecimal format. This should make the data portable. You can see what this option produces for NULL values. 

The Problem The server option needs to be enabled on both m1 and m2 The Solution Please add the following in /etc/my.cnf on both m1 and m2 

This index should now be a permanent part of the table until you decide to remove it, which I don't think you want to do. To see the table and the indexes, just do this: 

I have another solution that combines the two ideas You will not need to do joins as my suggestion will not involve another table. You will need extra fields to monitor public columns. Actually, you only need one field. 

It will show the table structure and associated indexes In phpmyadmin, there is a Tab to view the Table Columns To run the commands I mentioned above, try executing them in the SQL or Query Tab. If phpmyadmin does not like my command line versions of the commands, then run them in the SQL or Query Tab with semicolons: 

You could switch to and deal with it as a Table to Query. STEP 01) Convert mysql.slow_log from CSV to MyISAM 

If all the tables are MyISAM or if all the tables are InnoDB and they do not have any foreign keys references, I got just the thing for you: NOT EVERY TABLE HAS universityID = 7 

I did not move the and into subqueries because they are bit fields and will just do a table scan at the point. Even if the bit fields were indexes, the Query Optimizer would simply ignore such an index. Of course, you could change to if your SELECT does not need anything from Perhaps involve a,b in the JOIN between lookup and main if this makes sense 

CAVEAT Make sure the network connection is clear (with no dropped packets) between Master and Slave. Give it a Try !!! 

In the upper left corner, there is an illustration of the InnoDB Buffer Pool. Notice there is a section of it dedicated to the insert buffer. What does that do ? It is ised to migrate changes to secondary indexes from the Buffer Pool to the Insert Buffer inside the system tablespace (a.k.a. ibdata1). By default, innodb_change_buffer_max_size is set to 25. This means that up to 25% of the Buffer Pool can be used for processing secondary indexes. In your case, you have 6.935 GB for the InnoDB Buffer Pool. A maximum of 1.734 GB will be used for processing your secondary indexes. Now, look at your table. You have 13 secondary indexes. Each row you process must generate a secondary index entry, couple it with the primary key of the row, and send them as a pair from the Insert Buffer in the Buffer Pool into the Insert Buffer in ibdata1. That happens 13 times with each row. Multiply this by 10 million and you can almost feel a bottleneck coming. Don't forget that importing 10 million rows in a single transaction will pile up everything into one rollback segment and fill up the UNDO space in ibdata1. SUGGESTIONS SUGGESTION #1 My first suggestion for importing this rather large table would be 

This is optional. My preference is an all-MyISAM slave to do reads because it is faster for reads than InnoDB for small datasets. Should you choose to go with InnoDB, make sure you relax the ACID compliance with this 

Then, go check the logs to see if the same errors came back or not. UPDATE 2017-08-23 11:04 EDT METHOD #4 

This will definitely give all tables whose name appears in multiple databases Form that as a subquery and join it to gather all databases the table appears in 

Then, create the other two tables. It should work. SOLUTION #2 (based on @ypercube's suggestion) Keep the table , but create the other two like this: 

The SQL Function Syntax between MySQL and PostgreSQL will never converge as you wish UNLESS you are willing to take a chance on some weird way to emulate the IF() function in a convoluted manner: WRITE A STORED FUNCTION !!! The IF conditional function could be called MyIF and do the following: 

If innodb_file_per_table is enabled, then a temp table is being populated with the new compression. The original table is currently intact, just not writable. Running on the Connection running the should do the following: 

The next paragraph says that with SERIALIZABLE, one transaction cannot modify rows if another has merely read them. SUGGESTIONS 

The only database object I can think of with the option is a VIEW To see all the views you have, run this 

You cannot use the reference for columns inside the trigger. In the code, once you get passed you must reference the new value with 

What you are asking is not really an automatic feature in MySQL 5.6, I could be wrong. Notwithstanding, you can set it up manually. Given the following information 

VIEWPOINT #2 : You may need to refactor this query Notice that query will perform the WHERE portion after all JOINs are complete. If the WHERE portion could be performed earlier that could help reduce the time. Try reorganizing the query like this: