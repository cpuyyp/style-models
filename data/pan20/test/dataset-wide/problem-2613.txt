The first one states that it isn't possible to go beyond what is possible, ie. You cannot create people without them adding to the maximum definition of how many could be made. You could call this the 'blatantly obvious' rule, if there are 100 people then having 100 people must be possible. The second one states that you could be in an environment where 100 people can survive indefinitely, and that is how many you could have there 'for very long'. You can go over this number, but you will not be able to remain above it forever. Think of a spaceship creating enough water for 100 people. You could put 200 people on there, but it would eventually decrease to 100 before stabilizing. 

In a deterministic world the mind is deterministic, but goes through the motions of reasoning to reach its conclusions. In such a universe our actions and beliefs are predetermined, and the state of our mind at the time of reasoning is predetermined, and so the path our reasoning will take is predetermined. The argument being presented is predetermined, and whether or not the mind will accept the rationale of that argument is predetermined. Taking this into account, an argument will only be persuasive if the deterministic mind is in a state where it is receptive to the argument. It relies on the listener having similar enough data patterns within its head for the sound arguments to be seen as sound - they may sound completely invalid if their worldviews are too far apart for the same conclusions to be drawn by each participant. What one mind sees as completely reasonable could be seen as completely insane by another mind. I would say when pondering reason, you should really be pondering perspectives, as rationale is built from perspectives. People will always have their own rationale for what they believe, built off their subjective perspective, and the word unreasonable merely means 'I don't understand' as opposed to 'there is a flaw in your thinking'. All this holds true in a non-deterministic universe as well, however, so deterministic or not is redundant when discussing reason. 

QM being unpredictable is likely due to the limits of human understanding, and so using it as the lynchpin in any philosophical discussion would be the same as using the sun disappearing and reappearing to confirm the existence of a higher power. The case for free will hinges upon the human mind being able to deviate from the otherwise seemingly deterministic nature of the universe, and quantum mechanics is a nice get out clause for that, however then we have to argue that the human brain can affect quantum mechanics, whilst being driven by something other than quantum mechanics, otherwise it is just QM affecting itself, and leaves us no more free than in a deterministic universe. 

... he seems to be deriding and/or doubting the goal of liberation (nirvana, enlightenment) sought through meditation by spiritual traditions such as Buddhism. Would Derrida agree or disagree? How about other so-called postmodern philosophers? 

Or how about a different scenario... Susan continues not to particularly enjoy the concerts, but she gets a big kick out of seeing Samuel happy, so it's more than worth it to her. And Samuel, being happier and recognizing Susan's sacrifices starts making sacrifices of his own, and he too enjoys Susan's happiness so much that... etc. IOW, why not assume a virtuous cycle rather than a vicious one. It is possible to enjoy making another person happy -- it's called genuine altruism, vicarious pleasure, etc. So, bottom line, your scenario is based on a set of unspoken premises, that human nature is fundamentally selfish and self-centered, that true altruism does not exist and cannot even be cultivated, and so on. These are not only mere assumptions, there is a lot of evidence that they are false. Many couples are more like my scenario than yours, though I will admit that in modern western culture yours is statistically predominant. But so what -- truth is not a statistical phenomenon. At one point a majority of the American adult population smoked cigarettes, but what did that prove about human nature or philosophic questions? And, of course, we managed to change that for the better. For another example, recent studies of Tibetan Buddhist monks and lay practicioners have shown distinct neurological correlates of genuine altruism, apparently as a result of their intensive meditation practices focusing on love and compassion, not that we need brain scans to confirm what we can observe directly in their behavior. These are not born saints but ordinary people who commit to developing their best instincts -- and succeed. Those practices as well as the practices of Christianity and other traditions are quite accessible to ordinary people. So, I'm not sure what your point is, other than assumptions hostile to altruism lead to... guess what... negative conclusions about altruism. 

I would like to clarify what Foucault means by "regime of truth", "discursive formation" and possibly related terms such as "truth game", "enunciative system". Here is my guess from looking at some sources, but it still is a bit murky... Discursive formations are images, texts etc that support a regime of truth, which is a system that determines what is even accepted as a truth in a discourse such as modern medicine, penology or science. Some questions -- is a discursive formation a process of forming a discourse, or is it something already formed ("formation" being ambiguous in English). How do these relate to "truth game", "enunciative system", other terms? This prior post is helpful -- What does Foucault mean by discursive practices or discursive constitutions in definition of discourse? -- but not quite the same question, as it asks about discursive practices rather than formations, and does not mention "regime of truth" 

I agree that they oppose each other, as you can't have liberty with equality, and you can't have equality with liberty. Equality taken as an ideal leads to communism, which is shown to fail whenever it is tried. Liberty taken as an ideal leads to anarchy, which is mostly untested but seems unworkable. Equality of liberty would be what I'd suggest we aim for. The same level of freedom for all, seems to make the most sense. 

Sight as we know it is a hallucination generated to guide the conscious mind through the world. How colours are perceived are a part of this, and so their existence is simply a different mapping within our brain. I think it's a reasonable assumption that most people see colours differently to each other - your blue is my red, etc - as otherwise individuals brains would be generating the same hallucination across the board, which would be far too regular to be realistic in my opinion. Think of it like this. Imagine a wall is made up of different coloured bricks, and they are arranged in horizontal layers so that we see stripes of blue and green. Someone comes along and rearranges the bricks so that one half of the wall is blue, and the other half is green. Someone seeing the wall prior to this event would swear it was green and blue stripes, and someone seeing the wall afterwards would swear it was half blue, half green. It's merely a rearrangement of the existing material goods, forming a different subjective experience to the individuals. 

Rationale is developed via our subjective collection of causes and effects over the course of our lifetime. If someone suspects based on their experience that what we are doing will be negative, it is possible they will claim you are being irrational. This may be due to a lack of empathy, as there is a tendency for people to assume that others should have the same conclusions as they do, despite no two people living identical lives, and so each drawing their own conclusions about what is a risk and what is not. The same would apply to being illogical, and when this is stated it is common for the person being criticized to explain why - to them - it is entirely logical. The question of whose logic is correct is a tricky one, as even the most logical and intelligent person in the world may not have the correct set of data to reach a more optimal conclusion than the least intelligent person, who happens to have experienced something which massively increases the odds of their conclusion being optimal. Person A likely does mean that they believe A has a 75% chance for success, whilst B has only a 25% chance for success, but this is not knowledge, it is prediction, and that is notoriously subjective. To answer the question in the title - it depends on the value placed on the results of the risk, and the consequences of things going badly. A rational risk can be taken, if the potential positives outweigh the potential negatives, but that is down to the individual - it's also entirely possible to take irrational risks. 

Within the context of the debate, it's essentially a non-statement. As discussed in @CortAmmon's answer, this contextual confusion. Analogy: Multiplexing serial communications Most computers can be said to have a single serial port through which they access the internet. All of the data that comes in is like a stream of 's and 's, then gets streamed to various endpoints through multiplexing:             . The above multiplexer description is flat, though in practice multiplexing tends to be a hierarchical process. Anyway, details aside, it's then important for the communication protocol to sort incoming data into the various endpoints that ought to receive it. With computers, multiplexing tends to require precise framing. Part of this is that ambiguities can be exploited, allowing hackers to get undesired effects:     . With humans, our language tends to be far less precise, so while we must still multiplex our communications, the routing process involves a lot more inference. As given in the question statement, doesn't intend for the string to be parsed within the context of the argument. Since that string should never be routed to the argument in the first place, it has no existence within the argument; it's neither a fallacy nor a non-fallacy. At most, can say (in awkwardly precise language): 

So, obviously, something must exist within a debate to be a fallacy within it. For example, someone can turn on a TV, but if that's not part of a debate, it can't be a fallacy or non-fallacy within it. The main point of this post is that folks might be confused due to the assumption that all words in an exchange in which a debate occurs are necessarily part of the debate. But, that's the basic mistake. Instead, people can exchange words, including having one-or-more debates, without all words being part of all of the on-going dialogs. This is like how a computer can play music from the internet while a user is checking their email; the email app and music app are both streaming info over the same serial line, but the music doesn't exist in the email app nor do the emails exist in the music app. 

Einstein was a proponent of hidden-variable theory. The gist is that, if something appears random, then it's really just chaotically dependent upon information we don't have. So, God (the universe) doesn't play dice. Subsets of it might, but Einstein held that a complete description of the universe would be fully deterministic. Any description reliant on randomness merely omits the hidden variables. 

Detail on the multiplexing analogy Revisiting this answer, I'm not sure how obvious my multiplexing analogy may've been, so this section is meant to explain it a bit. The basic point is that a "fallacy" is an invalid construction within a debate: 

Not fallacies Fallacies don't apply in this situation since they're not attempting to formally prove that you're wrong or even asserting that you must be. This goes doubly so for: