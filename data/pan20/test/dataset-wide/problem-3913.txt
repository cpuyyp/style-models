Let $T$ be a measure-preserving transformation on a probability space $(\Omega,\mathcal B,\mu)$. Assume that for any pair of measurable sets $A,B\in\mathcal B$ with $\mu(A), \mu(B)>0$, one can find $N$ such that $T^{-n}(A)\cap B\neq\emptyset$ for all $n\geq N$. Is $T$ mixing with respect to $\mu$? This is likely to be a simple exercise. So, apologies, and thanks in advance. 

I think Bill Johnson's answer ``just use the definition" is correct, but that he did not write what he wanted to write. A function $f\in\mathcal C([0,1],\mathbb R^d)$ is absolutely continuous if and only if the following holds: $$\forall p\in\mathbb N\; \exists q\in\mathbb N\; \forall x_1,\dots ,x_N,y_1,\dots ,y_N\in\mathbb Q\cap[0,1]$$ $$\sum_{i=1}^N\vert y_i-x_i\vert <\frac 1q\;\implies\;\sum_{i=1}^N\Vert f(y_i)-f(x_i)\Vert<\frac 1p\cdot $$ For fixed $x_1,\dots ,x_N,y_1,\dots y_N$, the set of all $f\in\mathcal C([0,1],\mathbb R^d)$ satisfying the condition written in the second displayed line is obviously open in $\mathcal C([0,1],\mathbb R^d)$. This shows that $AC([0,1],\mathbb R^d)$ is indeed Borel in $\mathcal C([0,1],\mathbb R^d)$. 

Can we say that there exist independent variables $A, B, C$ and functions $h_1, h_2$ such that the joint distribution of f(X), g(Y), X, Y is same as $A$, $B$, $h_1(A, C)$ and $h_2(B, C)$? The intuition behind the claim is that conditioned on $f(X)$ and $g(Y)$, the joint distribution of $X, Y$ should be such that $X$ depends only on $f(X)$, and $Y$ depends only on $g(Y)$. The marginal distribution of one of $X$ or $Y$ conditioned on $f(X)$ and $g(Y)$ of course satisfies this property by the given independence assumptions. The question is whether we can show such a statement for the joint distribution.. If not, then is there a counter-example? Of course, coming up with a counter-example might be hard since one needs to show that it is not true for any choice $A, B, C$, $h_1$ and $h_2$, but maybe there is some intuitive reasoning why this is not true.. 

Good afternoon. Can anybody give me an example of a continuous map $T:X\to X$ defined on a Polish space $X$ which admits an invariant Borel probability measure but no ergodic Borel probability measure? Typically, the space $X$ could be a separable infinite-dimensional Banach space, or the space $\mathbb N^{\mathbb N}$. Thanks in advance. 

A friend of mine asked me the following question (which is motivated by an image processing problem of which I am unable to say more). Let $(f_n)_{n\geq 0}$ be an orthonormal sequence in $L^2([0,1])$, and define $F_n(x)=\int_0^x f_n(t)dt$. Is it true that $\frac{F_n}{\Vert F_n\Vert_2}\to 0$ weakly in $L^2$? Thanks in advance. 

It seems that Mike's argument shows that in fact the range $R$ of the Fourier transform is $F_{\sigma\delta}$ in $C_0(\mathbb R)$. Define the $T_ng$ as above, $$T_ng(x)=\int_{\mathbb R} e^{-a_n\pi\vert t\vert}g(t) e^{2i\pi tx}dt\;.$$ Then a function $g\in C_0(\mathbb R)$ is in $R$ iff two things hold: (i) all $T_ng$ are in $L^1$; (ii) the sequence $(T_ng)$ is Cauchy in $L^1$. Condition (i) can be written as follows: $$ \forall n\;\exists N\in\mathbb N\; \left ( \forall d\in\mathbb R_+ \;:\;\int_{-d}^d \vert T_ng(x)\vert dx\leq N\right)$$ By dominated convergence, the condition under brackets is closed with respect to $g$; so (i) defines an $F_{\sigma\delta}$ subset of $C_0(\mathbb R)$. Condition (ii) reads $$\forall k\in\mathbb N\;\exists N \;\left(\forall p,q\geq N\; \forall d \;:\; \int_{-d}^d \vert T_pg(x)-T_qg(x)\vert dx\leq \frac 1k\right)$$ By dominated convergence again, the condition under brackets is closed wrt $g$, so (ii) defines an $F_{\sigma\delta}$ subset of $C_0(\mathbb R)$. Altogether, $R$ is the intersection of two $F_{\sigma\delta}$ sets, hence an $F_{\sigma\delta}$ subset of $C_0(\mathbb R)$. I would be extremely surprised if it were better than that; i.e. I "conjecture" that it is not $G_{\delta\sigma}$. 

I suppose one attempt would be to just reverse the inclusion-exclusion equation. Since P(A union B) = P(A) + P(B) - P(A intersect B) we also have P(A intersect B) = P(A) + P(B) - P(A union B) Of course, that totally ignores the analogy you want to make between disjointness and independence. Maybe I can try to reverse-engineer something. Let P(A intersect B) = P(A)P(B)/f(A,B), where f is the function giving the unknown factor. We see that f(A,B)=P(A)P(B)/P(A intersect B). For the three-variable case we have P(A intersect B intersect C) = P(A)P(B)P(C) divided by (f(A,B)f(A,C)f(B,C) times g(A,B,C). Plugging in the formula for f(A,B), we see that P(A intersect B intersect C) = P(A intersect B)P(A intersect C)P(B intersect C) divided by (P(A)P(B)P(C)) times g(A,B,C). So g is the three-way intersection times the individual probabilities, divided by the two-way intersections. I must admit, I don't see any interesting pattern developing here with f and g, the way I did in the inclusion-exclusion case, but maybe someone else does? 

Suppose $X$ and $Y$ are correlated random variables in a finite set ${\mathcal A}$, and let $f, g$ be functions that map elements from ${\mathcal A}$ to ${\mathcal B}$ for some finite set ${\mathcal B}$. Assume the following: 

Subbarao, M. "Addition chains-some results and problems." Number theory and applications (1989): 555-574. A slightly outdated reference, but seems like the best available for the problem. 

I have a couple of inequalities that I want to prove. The proof is easy using fourier analysis but I am wondering whether there is a proof that does not use fourier analysis. 1) For any $c, s > 0$, $$ \sum_{x \in \mathbb{Z}} e^{-\pi x^2 s^2} \ge \sum_{x \in \mathbb{Z}} e^{-\pi (x - c)^2 s^2}\;. $$ 2) For any $c,s >0$, $$ \sum_{x \in \mathbb{Z}} e^{-\pi x^2 s^2} \cos(2\pi xc) > 0\;. $$ Can someone suggest some other simple proof? In particular, for the second inequality, is it possible to partition the sum into parts each containing finitely many terms and each is individually greater than $0$. 

This of course is a deep question in the philosophy of mathematics. The program mentioned by Tom Leinster is certainly a very interesting contribution to this, but if it proceeds at a purely mathematical level then at most it can define an equivalence relation on the class of proofs. There's still a further question whether this equivalence relation really is "the right one" to capture the notion of "same" or "different" proofs. Also, note that there's an open question as to whether mathematical proofs really are the sort of thing studied by proof theorists. Certainly the sort of thing that is published in a math journal is not the sort of thing that is studied by proof theorists. To cite the most obvious differences, the former have words of English in them (or French or Japanese or Russian or some other language) while the latter don't. But for more significant differences, note that the former also cite well-known results from the literature, and skip steps that are sufficiently obvious to the reader, while the latter don't. You can avoid this problem by assuming that published proofs are converted into formal proofs by means of spelling out all the steps in the proof of the well-known theorem, or the obvious fact. But this might not preserve the notion of "same proof". For instance, consider a theorem that in some sense only has one proof, which happens to rely essentially on quadratic reciprocity. Do we really want to say that this theorem actually has just as many distinct proofs as quadratic reciprocity does? There are lots of interesting questions here about the relation of proof theory to actual proofs, and what light it can shed on this intuitive notion of sameness of proof. And of course, there is probably also light to be shed in the other direction too, as our technical mathematical results in proof theory and category theory absorb results from the intuitive ideas we have about proof sameness.