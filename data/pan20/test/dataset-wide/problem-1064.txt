Unfortunately you're not working for the customer, but if you were you could use an argument along the lines of enquiring just what architectural flaws the software has so that exposing the database schema might be a security risk. 

Disk Sectors A disk has a fixed sector size, normally 512 bytes or 4096 bytes on some modern disks; these disks will also have a mode where they emulate 512 byte sectors. The disk will have tracks with varying numbers of sectors; tracks closer to the outside of the disk have more sectors as they have more room for a given bit density. This allows more efficient usage of the disk space; typically a track will have something like 1,000 512 byte sectors on a modern disk. Some formatting structures can also include error correcting information in the secotrs, which manifests itself in the disks being low-level formatted with 520 or 528 byte sectors. In this case the sector still has 512 bytes of user data. Neither Windows nor Linux support this directly, although i5OS (IBM iSeries) and various SAN controllers do. Normally the sector/head/track is translated into a logical block address; due to historical issues with backward compatibility the geometry (heads x sectors x tracks) seen by the operating system (particularly on IDE and SATA disks) normally has little to do with its physical structure. RAID stripe Size A RAID controller can have a stripe size for an array using striping (e.g. RAID-5 or RAID-10). If the array has (for exmaple) a 128k stripe, each disk has 128k of contiguous data, and then the next set of data is on the next disk. Normally you can expect to get approximately one stripe per revolution of the disk, so the stripe size may affect performance on certain workloads. Partition Alignment A disk partition may or may not align exactly with a RAID stripe, and can cause performance degradation due to split reads if it is not aligned. Some systems (e.g. Windows 2008 server) will automatically configure partitions to align with disk volume stripe sizes. Some (e.g. Windows 2003 server) will not, and you have to use a partition utility that does support stripe alignment to ensure they do. File System Block Size The file system will allocate blocks of storage in chunks of a certain size. Generally this is configurable - for example NTFS will support allocation units from (IIRC) 4K to 64K. Misalignment of partitions and file system blocks to RAID stripes can cause a single filesystem block read to generate multiple disk accesses where only one would be necessary if the file system blocks aligned correctly with the RAID stripes. Database Block Size The database will allocate space in a table or index in some given block size. In the case of SQL Server this is 8K, and 8K is the default on many systems. On some systems such as Oracle, this is configurable, and on PostgreSQL it is a build-time option. On most systems space allocation to tables is normally done in larger chunks, with blocks allocated within those chunks. Misalignment of filesystem and data allocation blocks can generate multiple I/Os for a single block write, which can drive a performance penalty. I/O Chunking Normally a DBMS will actually do its I/O in chunks of more than one block. For example, on SQL Server, all I/O is done in chunks of 8 blocks, 64k in total). On Oracle this is configurable. Casual inspection of the PostgreSQL docs doesn't reveal a specific description of whether PostgreSQL does this, so I'm not sure how it works on this platform. When the I/O chunk larger than the file system block size or is misaligned with RAID stripe boundaries a disk write from the DB can cause multiple disk writes, which generates a performance penalty. Disk space usage No disk space is wasted - the database I/O will use one or more physical I/O operations on the disk to complete - but incorrectly tuned I/O can generate inefficiencies which will slow down the database. The main things that have to be in alignment are: 

If you're talking about identifying candidate keys for relational synthesis you need to know the dependencies. If you have the functional dependencies you can use the relational synthesis algorithm; a synopsis of which can be found here. Note, if this is a homework question please mark it as such in the tags. 

Never seen such an item, but having worked with a few sensitive data sets in my time, the main thing that needs to be scrambled is people's identities or personally identifying information. This should only make an appearance in a few places in the database. Your masking operation should retain the statistical properties and relationships of the data, and probably needs to retain actual reference codes (or at least some sort of controlled translation mechanism) so you can reconcile it to the actual data. This sort of thing can be achieved by getting a distinct list of the names in the fields and replacing it with something like FirstNameXXXX (where XXXX is a sequence number, one for each distinct value). Credit card numbers and similar information that could be used for identity theft are quite likely to be a no-no in a development environment, but you only need real ones if you're testing payment processing systems - typically the vendor will give you special codes for dummy accounts. It's not particularly difficult to write anonymising procedures of this sort, but you will need to agree exactly what needs to be anonymised with the business. If necessary, go through the database field by field. Asking yes/no will give you false positives that you don't want. Ask the business rep to explain why, or the consequences or regulatory implications of not anonymising particular data. 

To take a different approach, what is driving your read workload - Is it the application, or do you have a bunch of reports hanging off the system that are driving the load? Depending on the nature of your workload you might be able to re-shuffle your hardware or push some of it off onto other machines. A couple of ideas: 

N-Tier may be a disingenuous term I think the term 'N-tier' is disingenouos whe used in the context of a business intelligence system. In transactional systems 'N-Tier' describes a distributed system with an application server, ESB or some other networked middle tier. Data warehouse systems don't work in a way analogous to this, so the term is likely to breed confusion. Separating data and functional logic You could build a system in terms of data that's as raw as possible and then put a transformation layer on it, which is then consumed by a reporting layer. The transformation layer could take the form of: 

Developers have local copy of data to run development code againstThe deployment scripts or DB images mean that they can set up an environment from any version that's available. Able to rollback database structure to a previous changesetAgain, sorted by the deployment scripts. Either through DDL or test database backup images created through a controlled process, developers can bring up an environment to any specific version that you have. Able to separate new feature schema changes vs schema design fix changesPatches to a common version can be maintained in a separate fork in the svn tree. If database backups are used as the reference environments they need to be stored somewhere with the same folder structure as the branching of the source control projects. Able to modify database structure locally for testingThe simple deployment process allows devs to tinker, and easily restore an environment to a local state, or bring up a reference environment to do comparisons and make change sets against. 

Option 1 is almost always preferable unless the business cannot tolerate delay to the releases. This might be the case with a .com type business where fast release schedules matter. In the case where that impact on the release is not acceptable, then the business must either implement (3) or sign off that they are happy with (2) and accept responsibility for any downtime caused by changes that break the analytic system. Option 3 will bog down changes to the analytic systems as they may become dependent on changes to the interface and therefore require releases of the operational system with changes to populate the updated interface. If the business insists on sticking with (1) then make them sign off something accepting responsibility for unscheduled downtime on the analytics. If you don't have a paper trail showing that you've raised this issue with them then you're at risk of copping the blame for something you have no control over. 

A CTE may be called repeatedly within a query and is evaluated every time it is referenced - this process can be recursive. If it is just referred once then it behaves much like a sub-query, although CTEs can be parameterised. A temporary table is physically persisted, and may be indexed. In practice the query optimiser may also persist intermediate join or sub-query results behind the scenes, such as in spool operations, so it is not strictly true that the results of CTEs are never persisted to disk. IIRC table variables (on the other hand) are always in-memory structures. 

is showing you all of the tables you have access to, not just the ones you own. I would guess that there are three schemas on the server with your application database present. would show you just the tables you own (i.e. just the ones in the schema belonging to the user you are logged on as). If you connect as a DBA login you can see another set of data dictionary views called which have all of the objects on the instance, regardless of the ownership. 

Misalignment does not create a greater data integrity problem than would otherwise be present. The database and file system have mechanisms in place to ensure file system opearations are atomic. Generally a disk crash will result in data loss but not data integrity issues. 

Found the problem - permissions issue in disguise. The Jet driver needs to have write access on the directory so it can update the lock files. When it can't write to the lock files it throws its toys out of the cot with a misleading error message. 

The value depends very much on the individual organisation and its requirements. Depending on the level of sophistication required, a B.I. role might fall into a few different categories: 

If you could meaningfully slice the data by key1, key2 or key3 then you could make dimensions out of them. In that case you would have separate dimension tables that had all the values of each key, and any relevant attributes (even just a human-readable description of what the value represents) or groupings (roll-ups of the key values). Each dimension table would have the key value and its attributes, and would link to the fact table on its key. If your values (Var X) are additive across rows (i.e. all the values added together for Key1='Foo' mean something) then you can use the keys to slice the data. I don't think that relationships between the Var columns are really relevant to dimensional modelling unless you want to unpivot them and indentify them by a 'variable type' dimension rather than separate columns Beyond that, the question is a bit vague to really get a clear view of what you are trying to achieve and why you want to make a dimensional model of your data in the first place. Maybe if you can clarify your goals a bit we can give you a more meaningful answer. EDIT: Dimensional modelling is just identifying the axes by which you want to slice data in aggregate. Sometimes a dimension might be very simple - just a code. Sometimes it may be complex with lots of sub-attributes - such as an insurance policy. A dimension lets you slice data in aggregate, either by its key or by rolling up the data by some attribute of the dimension. If the VarX columns are additive between rows - i.e it makes sense to aggregate (for example) var2 for two or more rows - then a dimensional structure might make sense for reporting. One key point of a dimensional model is that it tends to facilitate efficient queries in aggregate, and plays nicely with OLAP tools. So, if I had values 'Foo', 'Bar', 'Wibble', 'Blarg' for key1 and values 'A', 'B', 'C' for key2 then I might have a few rows that looked a bit like: 

EAV based systems (e.g. Agresso) have query performance problems inherent to the EAV structure. The problems are really based on three underlying issues: 

Similar things can be done with base metrics on the fact tables. A bit of experimentation will show you how to programatically annotate columns onto a cube dimension or report model, so it is possible to make a template and use a metadata based system to synchronise the cubes or front-end artifacts by generating columns. If you build reports in a templatable way (base columns with particular signatures) you may also be able to do something similar with canned reports if you have a significant body of these as a part of your product. This might be getting into diminishing returns, though. One golden rule: Never, ever edit generated code, or make an architecture that relies on subsequent human intervention in generated code. That way lies madness. 

Incorporate impact analysis on the analytic system and ETL processes into their change control for the operational system. Accept that releases will sometimes break the analytic system, possibly in ways that are not obvious (unrecognised semantic changes to the data). Fixes to the analytic system may or may not be quick to implement. It is possible they could do somethng that takes days or weeks to fix in the analytics. Build a stable interface layer that the system exports data into and refactor the data mart ETL to populate via that interface. 

Then you can store one row per second with all the devices de-normalised into an array of readings. The application maintains a registry of offsets into this array by device. As you add more devices the BLOB simply expands. This changes your problem to one of storing one 50K-ish BLOB per second indexed on the time, which can be done by pretty much any DBMS platform that supports BLOBs. You might even be able to use a key-value pair system such as Berkely DB. 50K per second is 180MB per hour, 4.3GB per day and approximately 1.5TB per year. This should be possible to manage even with fairly modest hardware. Depending on how much you need to archive you can periodically clear down historical data. You will need something that supports partitioned tables to do this efficiently, though. Getting the data back out One disadvantage of this approach is that you would have to read your entire data set to query the statistics for any given device, which would imply scanning 4GB of data for a single day. If you need to support a lot of users querying the devices on an ad-hoc basis you will need to find a way to supplement this store with a fast querying capability. Some possibilities for this are: