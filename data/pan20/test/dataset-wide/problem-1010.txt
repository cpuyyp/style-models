For the stored procedure that has already been deleted, the only option is to dig into the online transaction log or transaction log backups (in case the database is in the full recovery model). The above mentioned fn_dblog provides the requested info only if the transaction is still in the online transaction log. To read transaction log backups, use fn_dump_dblog. Check out Paul Randal's article: Using fn_dblog, fn_dump_dblog, and restoring with STOPBEFOREMARK to an LSN To read the transaction logs (online, detached, and transaction log backups), besides using undocumented functions, you can use a third party tool such as ApexSQL Log. It can show who and when deleted the stored procedure, and will also provide the script to re-create it. 

Database Checkpoints (SQL Server) The lazy writer process periodically checks the available free space in the buffer cache between two checkpoints. If a dirty data page (a page read and/or modified) in the buffer hasn’t been used for a while, the lazy writer flushes it to disk and then marks as free in the buffer cache If SQL Server needs more memory and the buffer cache size is below the value set as the Maximum server memory parameter for the SQL Server instance, the lazy writer will take more memory If SQL Server is under memory pressure, the lazy writer will be busy trying to free enough internal memory pages and will be flushing the pages extensively. The intensive lazy writer activity affects other resources by causing additional physical disk I/O activity and using more CPU resources To be sure that the server is under memory pressure, check Page Life Expectancy. If its value is low (below 300 seconds), this is a clear indication of memory pressure. Check the Free List Stalls/sec value as well. If above 2, consider adding memory to the server 

It is possible to rollback wrong updates without any downtime using ApexSQL Log. For maximal UPDATE reconstruction, the database should be in the Full recovery model, and a full chain of transaction logs is needed. A full chain of transaction logs starts with a full database backup and is followed by all subsequent transaction log backups up to a point of the rollback. 

If you're not using a third party tool, I recommend the fn_dblog function. As it's undocumented, it's not easy to use and the results it returns are not easy to read. Try with the scripts someone has already tested: SQL Server – How to find Who Deleted What records at What Time How to recover deleted data from SQL Server Keep in mind that fn_dblog can read only the online transaction log For transaction log backups, use fn_dump_dblog 

Note: this feature will be removed in future SQL Server versions You can fine all arguments available for the utility here: osql Utility 

No These are the dialogs that SQL Server Management Studio shows when connecting to a Database Engine. All you have to do is enter the machine\SQLinstance names (in the example machine is Fujitsu and the instance is SQL2012) 

Before restoring from the latest full database backup, you can try Paul Randal's advise - set the database to the EMERGENCY mode and use a login that is a member of the sysadmin SQL Server role to access it. Keep in mind that you will be able only to read the data, as the database will be in the read-only mode. Search Engine Q&A #4: Using EMERGENCY mode to access a RECOVERY PENDING or SUSPECT database 

The transactions are not actually removed after a check point. Their state is only changed to inactive, so they are ignored by the function Even when the transactions are in the inactive parts of the online transaction log, it doesn't mean that they are deleted from the LDF file 

Run SQL Server Management Studio In Object Explorer, expand Databases Expand Tables Right click the table and select Script table as | CREATE TO | New Query Editor Window 

Start the Command Prompt Go to the Tools\Binn folder Run code such as osql -H TestServer -S Fujitsu\SQL2012 -i D:\Test\sync.sql -U sa -P sqladmin 

Yes, they read the LDF file (online or detached) and trn files (transaction log backups), find what transaction has happened, and create a script that will do the same, or the opposite. Note however, that the undo and redo script don't have to be exactly the same as the ones executed, but the effect will be exactly the same. For example, if the executed script was: 

3.Select Device and navigate to the .bak file SQL Server will in the Database field automatically insert the name of the original database, read from the restored backup 

Keep in mind that the frequency of transaction log backups depends on how busy your database is and how much data you can afford to lose. If your databases are quite busy, consider creating transaction log backups every 15 minutes. That will also provide that the maximum potential data loss is less than 15 minutes of data. If you don't create transaction log backups often enough, besides more data you can lose, the online transaction log might grow more than you prefer. Here's the SQL Server Management Studio Backup dialog for a database in the Simple recovery model 

Besides using the script (which is preferred in most cases), you can do the same in SQL Server Management Studio: 

ApexSQL Log can provide the list of transactions made in a specified time period. It has a time range filter where you can specify the last 24 hours, or any other period you want to read. Just make sure you provide enough transaction log backups to cover the specified time range. ApexSQL Log can also create a redo script for all these transactions, so you can execute it against the new database where delta data will be stored. What's equally important, if you want this to be a reoccurring job, you can schedule it using ApexSQL Log CLI and SQL Server Jobs. Here's an example: Automating daily transaction log reading Disclaimer: I work for ApexSQL as a Support engineer 

Note that the transaction is tied to the Primary key column, not to the column used in the original where clause. Similarly, the undo script will be: 

As steoleary said, SQL Server 2012 Express doesn't support SQL Server Agents Only the following SQL Server 2012 editions support SQL Server Agents: Enterprise, Business Intelligence, Standard and Web See all the features by edition on MSDN: Features Supported by the Editions of SQL Server 2012 

The lazy writer process is closely related to checkpoints, so I'll start with that first Best SQL Server performance is achieved when pages are read from the buffer. To provide enough free space in the buffer, pages are moved from the buffer to disk. These pages are usually moved at a check point, which can be: 

From MSDN Back Up Database Task (Maintenance Plan) Backup set will expire Specify when the backup set can be overwritten by another backup set. So, there's no deleting of old backups, just overwriting with new ones. 

For SQL Server, you can specify On success action to go to the next step, only when the job step #1 is completed. There, use a date/time condition to execute the job#2 

The generated script will be shown in the Query Editor tab. Make sure you change the table, primary and foreign key , and constraint names, as these names have to be unique. Otherwise, you'll get an error message saying something like 'There is already an object named 'Address' in the database.', or 'The operation failed because an index or statistics with name 'AK_Address_rowguid' already exists on table 'Person.Address'.' To create a script for an index Use the steps similar to the above: 

Besides , there are third party tools that read LDF files, such as ApexSQL Log. Disclaimer: I work for ApexSQL as a Support Engineer 

The transaction log will record that the row in the table with the column values 9, 'New Loc22', '41BC2FF6-F0FC-475F-8EB9-CEC1805AA0F6', and '2002/06/01 00:00:00.000' is deleted. From the table structure, the tool will read that the Primary key is the AddressType column, and will create the following redo script: 

Relational databases are designed to ensure data structure complexity and flexibility. Having all records in a single table can IMHO mean two things - a bad design, or you don't need a database at all. Why not just use an Excel table then? The example you provided shows that you want to store people and their houses, colors, etc in the same table. Having your apples and oranges in the same basket (i.e. table) is not a solution A good starting point is to check MS AdventureWorks2012 database . It represents a good example of splitting various data into related tables, simple and self-explanatory 

How to recover SQL Server data from accidental UPDATE and DELETE operations How to recover SQL Server data from accidental updates without backups Disclaimer: I work for ApexSQL as a Support engineer 

In SQL Server 2012, the Create Audit dialog enables to specify the audit files size and number. You cannot specify the time the times are saved, but the file size and number might be enough to accomplish what you're looking for Maximum rollover files - the number of files kept in the system. When the maximum number is reached, the new files overwrite the oldest ones. The default value is unlimited Maximum files - the number of files kept in the system. When the maximum number is reached, the old files will not be overwritten, and storing new audit information will fail Maximum file size (MB) sets the size of the target file. When the specified size is reached, a new file is created. The default value is unlimited 

At a checkpoint, all dirty pages are flushed to disk and the page in the buffer cache is marked for overwriting 

All can be done in SQL Server Management Studio. The Script Table As option generates create code for primary and foreign keys and constraints. It doesn't create script for the indexes, so you have to do that in another step. type, I suggest expanding all object type nodes in SSMS Object Explorer. 

This depends on how long you want to keep your old backups. Does your company have a policy for that? Do you move your backups to a storage after a while? If you do, you can specify any number of days for expiry, as long as you copy the backups before the expiry time comes. 

Start SQL Server Profiler On the File menu, select New Trace In the Connect to Server dialog, select the SQL Server instance and provide credentials Click Connect In the General tab, specify the trace name Open the Events Section tab Select the Show All Events check box. Make sure that the event type you want to audit is selected 

However, I suggest you check who has the "sa" login privileges and whether these are the right persons. A monitoring tool can be an answer, as it will show you everything that the "sa" login did on your instance. 

4.In the Set Scripting Options tab, click Advanced and make sure the Types of data to script option is set to Data only Note: If you select Schema & Data the generated schema script will be identical to the script generated in the first method in this answer. 

Note that you can restore a backup only if it has been created on the same SQl Server version, or earlier (but not SQL Server 2000 on 2012) 

I guess the problem is that the database is in suspect mode, not suspend First of all, it's not recommended to detach it, if you do that, most probably all you're left with is a third party tool, such as ApexSQL Recover that can read the MDF file to recover the table records. Use the Recover from a corrupted database or a detached MDF file option While the database is still attached to SQL Server, you can try the steps suggested by Paul Randal - use the emergency mode, switch to the single user mode, and try with ATTACH_REBUILD_LOG and DBCC CHECKDB You have detailed explanation here: Creating, detaching, re-attaching, and fixing a SUSPECT database EMERGENCY-mode repair: the very, very last resort Disclaimer: I work for ApexSQL as a Support Engineer 

You can catch values for every inserted column in a row and save them into the table where you save other audit data. There are third party tools, such as ApexSQL Audit, that create such triggers, captures data (for UPDATEs both old and new) and shows reports. When a row is inserted into a table with the following columns: All column values are shown as separate columns 

4.You have to manually type the new name here. There's a drop-down menu, but it shows only existing databases 

There are several options you can use: You can use Use Dynamic Management Views (DMVs), such as sys.dm_exec_sessions, sys.dm_os_performance_counters, sys.dm_os_memory_brokers, sys.dm_os_memory_nodes, sys.dm_exec_procedure_stats, sys._dm_os_sys_info, sys.dm_exec_requests, sys.dm_exec_requests, and many more, depending on what you actually want to monitor. As SQL Server doesn't store the performance metric values in an archive table, you can query the views on a schedule and insert the results into a table that you will use as a repository.You cfan query and analyze the records easily then. Another question is - what counters to monitor. For processor usage: Processor : % Processor Time, Processor Queue Length For memory, Available memory bytes, Total server memory, and Target server memory You can find a complete list of counters recommended for monitoring here: Performance Monitor Counters Note that when you use the dm_os_performance_counters view, it's essential to understand the counter_type. There are five different values, and the current value is calculated differently for each value type. You have examples and explanations here: sys.dm_os_performance_counters Another option is to use a third party tool that collects the metrics you want to monitor, stores them in a repository and shows historic data for the time period you select. As the history data is stored in SQL tables, you can also easily query the data yourself and create reports. Such a tool is ApexSQL Monitor, and it has built-in graphs for the last day, week, and month. It will have the reports available soon. 

Like Sebastian said, ApexSQL Recover can be used to recover from corruption, but it doesn't seem to be a right solution in this scenario. The option to recover from a corrupted database or an MDF file reads only the online databases and detached MDF files. It doesn't read corrupted database backups There is another option in ApexSQL Recover that reads corrupted backups, that's Recover table data from a database backup. However, note that it recovers only table data and creates an INSERT script, so it cannot be used to move the whole database to a newer SQL Server version There are other ApexSQL tools that can help, as they read database backups - ApexSQL Diff and ApexSQL Data Diff. As the backup is not severely corrupted, they might be able to read it. Both tools read both online databases and database backups, they can also be used if you have an online SQL Server 2005 database that you want to upgrade to SQL Server 2008 R2 ApexSQL Diff is a SQL Server database comparison and synchronization tool which detects differences between database objects in live and versioned databases, backups, snapshots, and script folders. Comparing your live SQL Server 2005 database or database backup to a blank SQL Server 2008 destination creates the SQL objects that exist in the source (i.e. SQL Server 2005 database) Once you recover the database structure, use ApexSQL Data Diff to recover data You can find the recommended steps here: Migrate a SQL Server database to a newer version of SQL Server Create a database script from a backup without restoring it Disclaimer: I work for ApexSQL as a Support Engineer 

Disk Writes/sec depends on disk specification. For an array system, the values shown are for all disks. So, there is no specific threshold, nor it's limited by SQL Server / Windows server / any other thing. More useful info here: Windows Performance Monitor Disk Counters Explained 

There are several methods to audit database transactions, but they all affect performance more or less SQL Server Change Tracking and SQL Server Change Data Capture don't show who, when, and how executed the transactions. On the other hand, SQL Server Change Data Capture shows the old and new values for the UPDATE statements. Here is a useful set of comparison notes: SQL Server 2008 Change Tracking (CT) and Change Data Capture (CDC) and Comparing Change Data Capture and Change Tracking SQL Server Auditing shows who, when, and how, but doesn't show the old and new values for the UPDATE statements The methods that read the database transaction log files don't add overhead, as there is not additional change capturing. Besides fn_dblog, that can return results not easy to understand, there are third party tools, such as ApexSQL Log There are two more auditing tools from ApexSQL - ApexSQL Audit which uses triggers, so can impact performance on a high transaction database, and ApexSQL Comply that uses SQL traces SQL Server database auditing techniques Disclaimer: I work for ApexSQL as a Support Engineer