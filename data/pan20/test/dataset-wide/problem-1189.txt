Your MDF is actually corrupt, based on that error message. This means you can't attach it. You could create a database with the same name, substitute MDF (and rename LDF), and hope it comes online or at least suspect to start recovering it. Otherwise, you'll need to find a 3rd party tool that understands MDF files to get data back. Or just restore from your latest backup...? 

a trigger is firing on the target table, and the error happens there the INSERT does some processing that adds enough data to make it too wide 

Up to you. Just be consistent though. Personally I prefer singular based on what each *row" stores: Order, Products etc. This matches my modelling ((via Object Role Modelling) where I use singular entities/types. Edit: One reason is that plural fails when you have link tables: , would give or . Neither sounds correct Or history tables (of course you can use schemas for this): -> or (no!) ? Wouldn't -> be better? 

Summary Let the linked server do as much as possible. It is impossible for SQL Server to optimise a query on a linked server, even another SQL Server Long The key factor is where the query runs. In this case, it is a trivial SELECT so all rows from a table will be sent down the wire. It doesn't matter. When you add JOINs and WHEREs then it can matter. You want SQL Server to allow the linked server to do as much filtering as possible to reduce the size of the data coming over the network. For example, the 2nd case here should be more efficient. 

Assorted thoughts... Your database code will outlive your application client technology. Think of ADO.NET -> Linq -> EF as well as assorted ORMs. Whereas you can still run SQL Server 2000 code from last millenium against all of the above client technologies. You also have the multiple client issue: I have .net, java and Excel. That's 3 sets of application logic. "Business logic" shouldn't be confused with "data integrity logic". If you do have clients starting transactions and doing assorted checks, that's a lot of db calls and a long transaction. Application logic doesn't scale for high data volumes. We have 50k rows per second using stored procs. A sister team using Hibernate can't get one per second 

Every table? This is deliberate vandalism or stupidity. It isn't a SQL Server issue There are at least 3 ways: 

there is no collation "sql_latin1_general_as" there is no collation "sql_latin1_general_ci_as" collation chosen during install does depend on the control panel settings The default collation for a clean install would be "SQL_Latin1_General_CP1_CI_AS" for US English: The default for UK English is "Latin1_General_CI_AS" SQL collations are legacy and should only be chosen for in-situ upgrades (IIRC) 

One thing to look at would be NTFS fragmentation too after all this. YOu can stop SQL Server and defrag the volumes to get the MDFs and LDFs contiguous again. Of course, when they grow again you'll get fragmentation: which merits having generous file sizes to start with (50-60% used maximum). Otherwise, your idea is sound to use different spindles to manage IO. Note the file size and NTFS stuff I mentioned above too. 

But your investigation is correct: any kind of shrink should not be a routine, scheduled operation When you do need a shrink, you'd use SHRINKFILE instead which is more targeted 

Yes, use OPENROWSET with BULK. You need a format file though. Assuming you want to attach blobs to existing records, something like: 

Yes, on any RDBMS, if you still have a unique index with non-nullable columns to identify individual rows. The difference between a "unique key" and a "primary key" is a PK does not allow NULLs. NULL does not compare to NULL so you can not uniquely identify a row with a nullable unique key. Especially if your RDBMS ignores NULLs for uniqueness (e.g. SQL Server allows at most one NULL in a unique index) Any of the indexes (A PK is an index) can be clustered: but every table should have clustered index (outside of staging table that are cleared after use). The clustered index can be the PK, the unique key, or another index. I'm sure MySQL allows separation of index uniqueness and clustering 

It depends. In a big shop, maybe not because you have 1000s of servers to look after and your tools are provided. In a small shop, you'll probably need more knowledge because you have a wider remit. Scripting languages (PowerShell, cmd.exe for example) are always useful though for monitoring, deploying etc. I've often had some Perl scripts I had to maintain (just about). Then there are the assorted ETL packages that you'd be expected to know about. Saying that, most DBAs (that I know) would be able to write basic CLR stuff or know PL/SQL well. For me, the dividing line is knowing about the broader patterns or architecture of .net or Java. I don't need it as a DB developer or DBA. In the same way that .net or PHP or Java Monkeys don't understand database design or architecture or code as well as I do. Personally, I decided to stop chasing the latest or best client language years ago and focus on database work. That doesn't make me a "non-programmer": I'd need to learn it again if I had to. 

Have you ever connected? Changed any settings? Have a backup of master? What other errors are there in the event log? Anyhow, restart SQL Server with the -f option and ensure the the 2 settings above are sensible. 

This is different to what I offered above which simply re-orders rows in various ways So, I'd consider CROSS APPLY. The WHERE clause force row by row evaluation and avoids the "folding" issue and ensures that val and rnd are always different. CROSS APPLY can scale quite well too 

The error you have is unrelated to logins as such. This is caused when SQL Server tries to "pass through" the NT login token to the remote server. It doesn't have permission to pass the token through. The remote server looks for this because the local servers connects with Integrated Security. You need "Security Account Delegation" to be configured for the local server. As for the default mapping... What MSDN says is that if you have "MyDomain\bob" locally then you have an "virtual" entry "stored" locally called "MyDomain\bob". There is no reconciliation of the 2 servers between the 2 servers. This is the same as running this: 

Quick answer: You can't. Physical access to the backup file will allow someone to restore it. The password feature isn't secure and will be removed anyway. See "Security Considerations for Backup and Restore" on MSDN To prevent someone running RESTORE and destroying a working database, then ensure that only the correct folk have permissions to do this 

Does it matter, unless it is sucking resources? As I understand it (from some searches), it will run anyway. There is an MS Connect item for SQL Server 2008 for CPU usage, but it looks unresolved 

The problem with your current model (as shown) is the Pets table: it adds no value currently. Of course, your simplified design now hides information that probably makes it useful... I'll throw in some ideas with assumptions though. Note: This is correct design, whether EF can deal with it or not. That is, you design the model, implement the database, then make your client work with it Option 1: Assuming an owner can have 0..n dogs and/or 0..n pets, then you'd need 3 object tables and 2 link tables. You don't need Pet. 

tells SQL Server to discard the execution plan You have no further need to clear buffers or caches: you are testing a query, not the IO stack to read data back into memory. If you're tuning a query that runs frequently, then data will be in cache most likely in real life. If it runs Sunday 3am once, who cares...? And maybe you can't run these DBCC commands anyway unless you have sa rights... Edit, Jan 2012 For SQL Server 2008+, you also have the hint (also see this) which gives a more general query plan 

Check constraints based on UDFs are rubbish. Concurrency, RBAR, isolation etc as you've found out. Some links: 

I'd consider using something other then SQL Server Agent, something that can connect to the Listener. We schedule jobs via SSIS for example so we don't have to duplicate jobs. If you do want to create the job on each server, you need to consider how you want it manage where it runs. It will fail on a non-readable listener etc 

There isn't enough information.. When you say "thousands of times", is that per week or per second? If per second, then the stack (OS, DB) you use doesn't matter. It's all about good IO for the logs. eg SSDs. And design+code quality. If per week, then consider your maximum size. <10GB? Then look at SQL Server Express for Windows. Or Postgres for non-Windows. And "non-profit" means "on the cheap". In which case, you really have to go Linux+Postgres unless you'd then incur training or support overhead because you're a Windows shop already. But "on the cheap" hits the "pick any 2 of fast, cheap and good" limit (good is size limit here). I would stay away from MySQL though: IMO is has too many quirks compared to the other popular systems (SQL Server, Oracle, Postgres, even DB2 and Sybase) 

There is nothing special about OUTPUT parameters when it comes to "named parameters" or "ordinal parameters". In SQL Server this terminology this applies to the EXEC call and how you specify parameters there: not direction 

You can force a ROLLBACK though by switching the DB to RESTRICTED using WITH ROLLBACK IMMEDIATE. See ALTER DATABASE. You can't force COMMIT (as per your SO question) Then... 

Yes ServerB will just replicate all databases to ServerC (ignoring replicate-ignore-db for now). There is no setting that limits ServerC to getting data that exists only on ServerA That is, is unrelated to and A and C have no knowledge or replication dependency on each other 

If you want the stored procedure to be invoked by one process only with no other concurrent callers, you'd use sp_getapplock. Note that you'd have to throw an error for a 2nd process (with a zero timeout) but you can swallow this with a BEGIN TRY/CATCH arrangement. Then you'd release it when the stored proc finishes. Note that this uses of a stored proc is basically wrong. You'd have SQL Server Agent running the stored proc every minute. You may want sp_getapplock to prevent manual execution too but SQL Server Agent won't allow overlapping calls for the same job. Or trigger is with Alerts in SQL Agent or Service Broker queues. We don't have enough info: you've asked for help with your chosen code-level solution. We don't know the problem you're trying to solve If this is a single unit of work, then you need a transaction, and a 12-24 hour transaction is plain wrong 

To run it with a SQL login, just need to specify User ID and Password in the connection string (called "init string" in BOL) 

Not with standard tools. You'd normally restore. Some 3rd party log analyzer might be able to help to pick out what you want, but you've lost your data most likely. 

So running this in batches with a good index satisfies the last 2, but is slower. However, you don't need absolute speed if it is less intrusive to other operations Otherwise, I'm not sure what you want...