I have found that a good solution to problems like this is to pretend like you're interfacing with a 3rd party data source. Create a 4th table that is specifically designed to hold the data that is being imported. A "staging table" so to speak. It's designed specifically for the type of data you will be getting. It has no foreign keys or any associations to any other objects in the database. After the data has been accepted, a follow-up process to actually consume the data into your 3NF tables is started. This process can do tricksy things like 

Again - If you try to apply it in a , you'll get a casting error. It MAY be possible to do that and have some special type of "if error ignore" statement, but I'm not familiar with what that might be. If the above query is slow, you could improve the speed of it by creating a partial index for it to use. 

In actuality, they are two completely separate pieces of instructions to the query planner.. The WHERE clause instructs the planner on which records to consider, and the SELECT clause instructs the planner for how to present the data to you. The following query is equally valid 

COALESCE takes the first not-null argument to use, so if bv.affidavit happens to be NULL it will use '' instead.. I shouldn't need to test for it being NOT NULL in the follow up.. 

This table has a lot of inserts against it and currently has about 30m rows in it. When I try to run the following query, oracle does a full table scan instead of using the index (which .. at least I believe .. it should be able to use). 

The answer can be found in the amazing documentation .. it LOOKS like you have NULL values in your tables.. When the DB checks for uniqueness, it says "does NULL equal NULL? NOPE!" and allows it. The important bit below (emphasis mine): 

If you consider that hard coding, then (as far as I'm aware) your only other option would be dynamic SQL within a plpgsql block. 

Another possibility is that your two objects are created in different schemas: If the function is created in but the table is created in .. the trigger will execute from , which will not have visibility into unless you it. Also, leapfrogging off of a_horse_with_no_name's comment .. it sounds like you don't simply want to execute the function, but actually DO SOMETHING with the result... If that's the case, you'd want to actually write a full fledged PLSQL code block for your trigger. 

If the creation of the temporary tables takes a long time, then your find_user() and your find_external_object() methods need to be tuned (as opposed to the actual insert query). One additional note .. if the or are NOT pkeys/unique of their tables, you will want to make sure that you're only getting unique values.. (If you don't, then you'll get duplicate rows when you join) In that case, make sure you do a subselect and get the distinct result (example below). 

But I can't speak to how efficient it will be.... -- Additional This may work, as well. But again, efficiency will likely be poor (since it will get all results, regardless if you use it or not) 

You can't apply a type cast in a clause .. you'll get a cast error because it will try to cast the entire column (instead of a subset of the column).. HOWEVER, you can apply some type of FILTER in the clause, and then CAST the result of your .. Postgres will apply the where before the cast, so (as long as your filter is correct) you will avoid a casting issue. Example (although there are probably other ways to do it than a regex): 

Not entirely ... pg_dump creates a file that contains the information of your whole database that can be given to pg_restore... For PITR, you actually need to create a file-level "backup" by copying your database files and WAL files (after issuing a pg_start_backup) 

EDIT oh - you added "list" requirement ... see @ypercube's answer, then. EDIT2 If the place_ids exist in a separate table, and they only DON'T EXIST in the table you're querying to get scores (which, imo, would be considered a 'good design decision'), then you should include that table in your query .. COALESCE() as used in @ypercube's answer should work fine in that case. 

can be moved to it's own table.. You probably don't need at all in the table, considering that it's always going to match the 's blood type. Likewise, you shouldn't need the in the table since it's always going to match the 's . I'm not really sure why you have a table at all ... you probably just want a table that records their name and blood type .. Then both the table and would point to the table. (although, again, that table doesn't need at all..) 

Based off what you have pasted ... It appears that, because you are using a LIMIT on each .. the second query will run faster because it only has to apply one filter (project_id = '') while the second has to apply two filters (project_id = '' and kind = ''). As a result of using limit, it takes less TIME to just spit out the first 100 results where the kind doesn't matter .... it takes more TIME to spit out 100 results where the kind DOES matter.. If you remove the LIMIT, I'm sure you'll see that, ignoring time for the query to run, the query with two filters will most likely return less rows (which, I'm guessing, will cause it to take less time overall ... but that really depends on what indexes exist. Based off of your pasted explains, though, it looks like you have appropriate indexes). If you remove the LIMIT, I -believe- you'll still see two index scans for the two-filter query .. but one index scan and one sequential scan on the query with only one filter. 

Also, there are about 1k distinct ID values in the hist table. With regard to data distribution.. Of the ~1k IDs, 50 have less than 100 entries in the table, 70 have between 100 and 1000 entries, 146 have between 1000 and 10000 entries, and the rest range from 10k to 60k entries. Over half of the entries have at least 30k records. 

This usually makes the process much faster, and also splits it up into two logical steps. Accept the uploaded file -> Process the uploaded file Rick James points out that a more detailed description of this process (along with ways to handle various situations) can be found here 

If a unit of measure doesn't have an entry in the measure conversion table for a specific other type of unit, then there would be no direct conversion available. (For example, cm -> inches would work, but cm -> hours wouldn't..) Displaying your measurements in different units should be easy. Your measure unit can be anything you want (distance/time/etc) ... but you may want to add a measurement type .. not sure if I'd worry about that - depends on use case. 

I do not agree with the approach you are thinking of using, but if you are dead set on using it, then I would probably structure the query like this: 

It sounds like streaming replication with a hot standby is the solution you're looking for. Streaming Replication details the steps of how you go about setting it up. The hot standby option that they go over (they have it put to "on" which is what you will want) will allow you to run SELECTS against the 2nd database. Under this setup, however, your production application would only write everything to the master database ... postgres itself would then stream those changes to the 2nd database. 

Not very friendly to the DB, in either case. But you only have to have your WHERE clause twice (as opposed to 4 times) 

The difference between and is that the first gives you unique rows across all selected columns .. the second gives you one unique row per column set defined within the parenthesis, but allows for additional columns to be returned. To specify WHICH additional data is returned, you need to .. will then return the first row for each DISTINCT ON set of columns. 

As you can see .. it can easily get a bit crazy. A different alternative would be to make col2wNull and col3wNull be defined as NOT NULL and provide some default value for when nothing is supplied. THIS MAY OR MAY NOT BE A GOOD IDEA depending on what you're doing. "Magic values" have a tendency to give you lots of problems later. With regard to your edit and the two strings appearing to be equal, but the database reporting that they are not - I can only imagine that there are some "invisible" characters (UTF-8?) that are in the string. Or it could be something as simple as one string has an additional space on the end. It partially depends on how you are saving it into the database. (Are you performing a trim() on them, lower(), etc..) You could try comparing the strings in various other ways (such as looking at md5 hash). I believe you can ask postgres to convert the column value into hex to view, as well, but how to go about doing that is escaping me at the moment (my apologies). 

My answer assumes that PRG_CODE is the primary key of both tables.. (or PRG_CODE + STATE is pkey of WEB_GK_STATE .. but PRG_CODE for WEB_GK) My suggestion would be to redefine the PK of your WB_GK_STATE table.. 

Postgres is smart enough to know in the that your group-by column is () ... grouping by a column that is is pointless, so postgres just gives you back what you ask for (column values a and b).. However, Postgres does NOT know that the group-by column in the is .. Thus, it's confused by your query (as well as most humans would be confused by your query - including myself). FYI - your query makes no sense. Why would you group by a column when you apparently have no interest in performing an aggregate? What would you expect a and b to give you? 

It will take LONGER than the previous query because, while it still does a sequential scan, it will most likely have to scan more than 1000 rows before it has 1000 rows to return with. All of the above holds true whether the DB needs to use a sequential scan or an index scan.. If you truly want to troubleshoot performance of the query, you need to remove the TOP 1000 and then view your query plan and see where the performance hit is... (in this case, most likely an index that would be useful is missing) 

If you want to keep the uniqueness across ALL THREE COLUMNS and, at the same time, treat nulls as equal, then you have to get creative with your UNIQUE indexes by making them partial indexes.. 

It's been a while since I've done anything like this, but last I recall - If you have ALL of the corresponding files, you should be able to copy them and start up the server .. it will go into recovery mode to validate that it's got everything that it needs. Of course, if it finds that it's missing something, it will likely either not start or complain in the log files that it's missing something (depending on the severity of what it's missing). The most likely files that you MAY NOT HAVE would be the WAL files (by default stored in the pg_xlog directory .. but could be different if it was configured that way). If the configuration was different, you will need to make sure that the new environment is suited to match the configuration that postgres is expecting (for example - if the WAL files were located in a different place than default). The postgres configuration files may also not be where you would expect them ... they COULD be in the base directory of the database, or they could be in the postgres config directory (which I do not recall the exact location of it - you can look in the /etc/init.d/postgres-9.3.sh file, iirc) 

In most RDBMSs, double-quotes are what are used to specify an exact spelling of something.. (single quotes being string delimiters). 

There is one primary difference in a trigger that is AFTER from one that is BEFORE .. but both get executed regardless. In a BEFORE trigger, the trigger is executed BEFORE the DML statement is executed.. So you have a chance to modify the :NEW (old :OLD in case of ) row it is inserted/updated/deleted... But anything you want to do you can do (like check data in a separate table and issue an update/insert/whatever in that table, etc). If, for some reason, the trigger causes an EXCEPTION, then execution ceases and the DML statement is never executed. In an AFTER trigger, the trigger is executed AFTER the DML statement is executed. You have already lost any ability to modify the :NEW and :OLD records and have it actually mean anything. However, you can still do whatever you want in the trigger. If the trigger causes an exception, for PostgreSQL, this will cause a rollback of the original DML. I do not believe this is true for all RDBMSs - but am having a hard time finding an example right now. A common use of a BEFORE trigger is to set a timestamp column to "now" before the data has been inserted. A common use of an AFTER trigger is to populate an audit/history table with the changes. In either case, you would usually want to issue a ROLLBACK if you find an exception has been raised. 

Disclaimer - I'm not a huge sys admin buff, so my information may not be correct.. I also realize that the error says "host" - but I don't think that means it has to be a host entry in pg_hba.conf When you connect to localhost, it uses the loopback address 127.0.0.1 (unless that configuration on your computer has been tampered with) to connect to your system, which counts as a "HOST" connection.. When you connect to "myComputer" (which happens to be your computer name), I'm pretty sure your OS is smart enough to say "oh hey! that's me!" and doesn't even bother resolving an ip address .. it just does a direct connection. This would be a "LOCAL" connection for postgres.. Try adding to your pg_hba.conf file.. Then either restart the server, or connect as a superuser and issue (preferred method).