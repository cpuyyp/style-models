When EXECUTE AS is used within a stored procedure to switch security contexts, it is sandboxed to the current database. This can be extended by using the database property TRUSTWORTHY or the server option cross database ownership chaining but there are security implications to those decisions. 

There's nothing inherent in Profiler that would prevent those queries from being displayed. I think you're just not catching the right events. It sounds like you're filtering for Stored Procedure events. Try also filtering for TSQL events. 

I'm not familiar with Azure but I can speak to SQL Server in general. SQL has a property called Force Encryption (which is set through SQL's Configuration Manager), which as it sounds will force clients to connect over encrypted sessions. This is done with certificates just like SSL. This guarantees the authenticity of the server as well as providing for encrypted communications. It's a bit nuanced - you can use a certificate issued by a CA that the clients trust or SQL can use a self signed certificate (less secure). There's also client side settings you can set in the connection string that will influence the connection - depending on ODBC or OLEDB the pertinent properties are something like "Encrypt" and "TrustServerCertificate". 

However, rather than grant permissions directly on the Certificate and Key, I like to have two stored procedures - one for encryption and one for decryption. We generally have different users that perform the encryption and decryption. Generally, the application is encrypting records as they are inserted into the database. Then it's a different user that needs to decrypt them for reporting purposes. Both procedures are created and the users are granted on the procedures. This way no users have direct permissions on the Certificate and Key and no user can both encrypt and decrypt. 

Secondary indexes (non-primary keys) in MongoDB and MySQL are very similar. Secondary indexes declare fields or columns to be sorted separate from the rest of the data, and use row identifiers to reference the rest of the row for a query. 

Otherwise you will have to drop the table (with the command above) and recreate it manually or from a backup. 

I would use for this. - $URL$ - The script imports only a small part of the huge dump and restarts itself. The next session starts where the last was stopped. 

For inserts, you can use . This lets you update certain fields if primary key is already used. The syntax would be something like: 

Every process under linux runs under specific user privileges. Services (like MySQL) usually need to open ports and access various system resources during startup, so they are required to be started as user. However, it is not safe to have all the processes run under as it is not required for continuous operation of services, thus it is recommended to create a special user, which will be used to run MySQL service. MySQL will only be able to access what special user can, and this is going to be limited to MySQL files on the system. This is usual practice in linux. If you, however, use you distributions built-in package manager to install MySQL, this will be done for you automatically (in most distributions at least). 

What I would do, is compare both files with tool. can come in help here as it has ability to diff word by word and if you pass it option don't even have to place them in git repository. This command can help 

This is a great example of how the Window Functions can be used. Based on your data, I'm assuming you want to do this for every unique LINE_NO. I'll leave it to you to format the percentage column. 

Dell PowerEdge 2950 with two 1 Gbps NIC's going to two 1 Gbps ports on a switch which then goes to a NetApp with four 1 Gbps NIC's that present as one virtual interface. 24 drives, 7200k SATA, NetApp RAID-DP. I've mapped each host NIC to the NetApp using MPIO in the Microsoft iSCSI initiator. Testing with SQLIO my write throughput appears reasonable at about 200 MBs, but my reads are closer to 100 MBs. Shouldn't my reads be closer to 200 MBs just like my writes? Is this a configuration problem or is there a fundamental storage issue I don't understand? 

Some comments, though: Everyone's causes for failure will be different: developer talent may dictate frequency of accidental data deletion, storage architecture may dictate whether a failed drive is fatal or just a nuisance, etc. As for SIMPLE vs FULL recovery model, that is a decision that should be made by the business folks. They need to weigh the damage to the business of downtime and / or lost data vs the cost of administration and space. Regarding data and log files on the same drive, conventional wisdom had you separate the two because access to the data file is random while access to the log file is sequential. In my experience, this works if you're talking about a couple databases on drives that you can segregate, but on a SAN with 100 databases spread across 48 disks all data access becomes random anyway. 

If you use , MySQL will use timezone which is configured in . If no value is set in , then MySQL will determine your operating systems' timezone and use that upon startup. You can see how your server is configured with this query: 

MySQL has "General Query Log". This logs everything that is going on MySQL server: users connecting, disconnecting, queries etc. This query log is a file on your filesystem or (from 5.1.6 versions) table Control the general query log at server startup as follows: 

This is likely because your User has status on your system. Users without administrator access should not be able to access these files. If you enable Guest User, you can login and try same thing with Guest. You should not get access to the files as guest or normal user without administrator rights. 

MySQL queries are not case-sensitive by default. It is possible that you have created case sensitive tables when importing data. Check if you have collation, that makes it case-sensitive. Reimport your data then using . Also, if you have collation, it will make queries case sensitive. Your collations changed when re-importing data. 

MySQL is not cutting it at 233. The problem is likely in your save method which cuts it to 233 before the data even reaches MySQL. Also, don't forget that 233 limit is not character limit, and as some character might need more han 1 byte to be stored, you might see less than 233 characters. Also please make sure that data in MySQL server is really stored as latin1, this can be accomplished with: 

I don't see why would't you just store the coordinates for every user. You can keep them either in the same table as users or in some kind of user details table, depending on how your current schema is designed. There is no point in having all geo to postcode locations mapped. To get the locations you can use something like google maps API. 

In my personal experience, if I were keeping score, causes for database restores have been: accidental deletion of data = many, restore to dev for testing = many, IO system failure / corruption = none. (Knocks on wood) You indicate you have "no need to recover to any point in time previous to our last backup". If that is in fact the case, then yes, switch into SIMPLE recovery. 

Beginning with SS 2005, there is a DMV called sys.dm_db_missing_index_details. Google that and you'll find some useful scripts. That said, you may need to look at the problem more holistically - is your hardware adequate (Perfmon counters will help you with this), what else is running at the same time, what is SQL primarily waiting on (Google the DMV sys.dm_os_wait_stats for some great scripts). And you can look at the execution plan of the specific stored procedure to find out where the bottleneck is. 

In the properties of the Alert you can set a "Delay between responses". You can set the delay to slightly longer than it takes to do the compression. 

My understanding is that online indexing acquires locks at the beginning (Preparation) and end (Final) phases. However, we see a lot of DML queries blocked during the Build phase. A sampling of the queries being blocked show they have a few things in common - the Wait Type is PAGELATCH_UP, the Wait Resource references a PFS page, and the query uses locking hints (NOLOCK, ROWLOCK, etc). Could locking hints be interfering? Can anyone shed some light here or point me in the right direction? Let me add, the actual command was: 

When you are copying these files without detaching the database first, you are risking to corrupt your backup in the event that will be synchronized during the copy procedure thus rendering the backup broken. 

Also, it is possible to set timestamp per session. is used to always get timestamp in UTC, no matter what MySQL server's timestamp is configured to. 

is file in your directory. The meaning of tilde in unix is current user's home directory. The values from different configurations are overwritten. So will overwrite was what defined in , and will overwrite what was in previous files. 

While you seem to have fixed the issue, I will quickly explain why it happened in case anyone finding this will want to understand where the problem was. When setting foreign keys, the Primary Keys Columns must be of exact same type and attributes. E.g. If you have unsigned attribute on one primary key, you must have it on another. If you have INT data type on one column, then another column must also be INT (NOT TINYINT, MEDIUMINT etc.). As you have only one ID set to unsigned, I would go and set it to all IDs. As it is usually good idea to have unsigned attribute on primary keys (if you do not use negative IDs), I would have changed all IDs to have unsigned attribute, as it will improve your query performance. Also, take a look at what values you can get with various integers (when they are unsigned). WHat you set your lenght to - does not matter: 

Yes this is normal. When RAM is no longer needed it is not freed at the same time. It is kept as cached in case the server would decide that it needs to access it again. This would save you extra time that you would otherwise need for data to appear in RAM. Cached memory is freed only when new applications request more RAM. 

The service account is a domain account. It is not a domain admin (nor is it a member of any group that is a domain admin). It has neither "write servicePrincipalName" nor "Write public information" permission (nor a member of a group with these permissions). Yet it is still able to register / deregister it's SPN upon startup and shutdown. What permission am I missing? 

For posterity, after much trial and error, we figured out how to get the expected throughput. As mentioned above, the NetApp had one virtual interface backed by four physical NICs. The host has two NICs and I had configured MPIO through the MS iSCSI Initiator so that there was a path from each NIC to the one virtual interface. The results were the throughput above - writes made sense at close to 200 MB or the speed of two NICs, but the reads were half that or the speed of one NIC. Upon closer inspection, our SAN guy noticed that traffic was only flowing through one of the physical NICs for the reads. I'm not sure if there was a configuration mistake on our end, but there were two things we tried and both got us our throughput. One was to change from one virtual interface backed by four NICs to two virtual interfaces, each backed by two NIC's. Then map one host NIC to one virtual interface. The other thing we tried was to use "aliasing" on the SAN side to present multiple virtual interfaces. (I'm not a SAN guy, so hopefully I said that correctly.) My take-away is that we just needed the SAN to present more than one interface so the Initiator truly saw multiple paths. Here is our throughput now: