OK, since you only want to reduce your access rights (rather than actually running as Peter per se) you may have some options. In Windows 7 (and Windows Server 2008 R2) the task scheduler supports this directly (via the "Do not store password" option) but I don't think there is any built-in equivalent for Windows Server 2003. Running the task this way on a different machine probably won't help because you don't get network access. It can be done in software, though, even on Windows 2003, via the CreateRestrictedToken Win32 function. A Google search found a piece of software called ulimitnt which appears be able to do what you want (via the option). Note that I've never used this program so I can't vouch for its reliability. Using this approach, the script only has access to files that grant access to both local system and to Peter. (Note that the local system account implicitly belongs to the Administrators group, so if Administrators has access that will be sufficient.) 

On Windows 7 or later, attempting to use to install a CAB file containing a Windows update returns error 0x80070002 (-2147024894), "The System cannot find the file specified". I've tried quoting the full path to the file, running the command in session zero, and everything else I could think of. Why won't the update install? 

The command allows you to specify options which will be passed to the service's function. I don't know offhand if any of the existing "run an application as a service" solutions will pass these arguments to the application, but it shouldn't be too hard to implement. Make sure you validate the folder name carefully. You don't want someone being able to reset permissions on , for example. 

They are related in that UAC relies on the existence of ACLs (and all the related security apparatus) in order to function. UAC works by removing (technically speaking, disabling) the Administrators token from non-elevated processes. This means that if the ACL for a particular file or folder only allows access to Administrators, non-elevated processes won't have access. That's why an installer can't write to Program Files without elevating first. EDIT: See this article for more information about UAC. In particular: "When an administrator logs on to a computer that is running Windows 7 or Windows Vista, the user is assigned two separate access tokens. Access tokens, which contain a user's group membership and authorization and access control data, are used by the Windows operating system to control what resources and tasks the user can access." I suppose you could argue that UAC depends on the Windows authorization model, of which ACLs are only a particular component, and that therefore UAC is not related to ACLs. I don't think that's a useful way of looking at it. Let me put it this way: if there weren't any ACLs, UAC would be pointless. You should also read this article which addresses some common misconceptions about UAC, and in particular the misconception that it is a security feature: "The primary goal of UAC is to enable more users to run with standard user rights. However, one of UAC's technologies looks and smells like a security feature: the consent prompt. Many people believed that the fact that software has to ask the user to grant it administrative rights means that they can prevent malware from gaining administrative rights." 

(This is not a question of how to run both machines from the same set of data at the same time; they would be effectrively running from the same set of data at different times. The files the servers each need to constantly modify would be on the local file system) We simply want to setup a redundant server and minimize the redundant administration. 

I am looking to begin a tape backup regimen and am looking to keep data flowing to the tape drive in a sufficient manner (120+MBs target sustained) but cannot figure out how to do so without a dedicated source drive/array that idles when not writing tapes. The documentation for our specific drive mentions no minimum throughput required. Enviroment 

I have been working at setting this up in much of a way you describe and it works great! (XenServer) I setup an old but capable server as the primary host, this runs a console only VM for DRBD. This VM then serves a "SharedDRBD" SR back to the Xen Host via NFS. The rest of the working VMs providing services run on the SharedDRBD SR. The VM's DRBD dev is on its own VDI on a MDADM RAID 1. This SharedDRBD SR hosts the rest of the VMs for various services with a local larger RAID10 array for bulk filestorage. All MDADM work is done by the host, but one side of the DRBD is in a VM. The DRBD ran in a VM gets synced with a DRBD service running on the file backup server; the file backup server is NOT virtualized purposefully so we have bare metal access to all files given XenServer is the biggest quirk we generally deal with. There is a secondary server that is virtualized but has no local storage except for what is required for the host. This server is part of a Xen pool with the primary server to simplify failover. Failover is currently manual but fast and easy. First, all VMs on the SharedDRBD SR are shutdown while the secondary XenServer host is powered on. The DRBD on the file backup server is made primary and mounted as needed. Then, the SharedDRBD SR is pointed to the file backup server and VMs are started up on the secondary server; XenCenter doesn't even realize it is serving the VMs from a new location because it sees the same SR with the same data. The VMs are fired back up and things are back and running. There is alot more to it in terms of configuration, and arrays, network topology, etc; but the jist is DRBD is served in a VM back to its own host. Overall it is HA enough for our SMB / Home use; down time during a catastrophic failure of the primary server is 10-20 min or less to fully back online and no loss of data; DRBD means the VMs are up to date! Plus, outside of the primary server which is pretty robust, there is a ton of overall redundancy. Most of the primary server is redundant in-and-of-it-self so it pretty much gives us triple redundancy or better for just about every piece of hardware you can think of (PS, RAM, CPU, HDD, Controllers, NICs, etc) besides the motherboard(s) which is only double redundancy (primary/secondary Xen Hosts). And yes, XenCenter is installed on windows sadly, the rest is all Linux. I know, this Q's is 8 years old. 

The two "Do not process the legacy run list" settings appear under Computer Configuration -> Polices -> Administrative Templates -> System -> Logon and under User Configuration -> Policies -> Administrative Templates -> System -> Logon respectively. The documentation for these settings is confusing (i.e., wrong); what exactly do these two settings do, and how do they interact? 

Note: the problem has been resolved, the admin of the parent domain managed to remove the DC object. I gather he used ADSIEdit. I'm working with an AD forest (in an isolated test environment, luckily, so failing all else we can blow everything away) and accidentally tried to create a subdomain using a cloned machine as the new domain controller. Naturally enough, this didn't work. But the new subdomain got created, even though AD was not successfully installed on the putative DC, and now I can't get rid of it. I started with this article. That didn't work because there was still metadata for the DC for the orphaned domain. So I found this. Unfortunately this doesn't work either. I opened AD Site and Services, opened the site in question, opened the Servers container, selected the DC in question, right-clicked on NTDS Settings and selected Delete. Clicked through "Are you sure", selected "This Domain Controller is permanently offline ...", confirmed that I wanted to do this even though the DC isn't in the domain I'm connected to, and confirmed that I wanted to do this even though this was the last DC in the domain. Then it said "Windows cannot delete object LDAP://... because: A referral was returned from the server." Anyone seen this before? Any ideas? 

On my system, changing the value of IEHarden (REG_DWORD) to 0 in this key appears to have corrected the problem for all new accounts (and possibly some existing accounts): 

If the service is running as Local System or Network Service, or as a specific domain user, it can either access the files via the UNC path or it can map a network drive using the function. In the case of Local System or Network Service, the connection is made in the context of the Active Directory computer object. If the service is running as a local user, UNC paths will not work, but it can still map a network drive using provided it can provide a valid username and password. (If the Win32 API is not available for some reason, running the command in a subprocess will also work.) 

Looking at using DRBD or a clustered files system to help with up-time when downtime strikes in a small business environment. We currently use a server box for a file server using Linux and samba, then running the web server and Database in a VM. Was looking at adding a second server and putting the files and the VM onto the distributed file system. The base OS is more static and easily can be managed more manually (copy config files at time of change, copy base OS if needed from full backups, etc) Question is about the fail over scenario if manually done. If server 1 goes down and fail over is manually done, is fail over completed by simply setting the static IP of server 2 to server 1 (again server 1 is down and would be in a state of needing repair), starting Samba, and starting the VM which would have the same static IP's as they had when running on server 1, and starting the backup services? This sounds like a quick and simple process, almost too simple. Am I missing something? This could easily be automated as well through a script or something that someone with little proficiency could be directed to run in the event of a failure. Down time if we have a hardware failure could easily be days without the support of on call IT support and the parts needed without a second server, but with the the second server, down time would be at the maximum a matter of hours (if no one is the office proficient enough to perform such operations, minutes if someone was) 

I've realize UREs are a bit more complex and unknown to most as they relate to array failures.. The conclusion is UREs can cause arrays to fail, but not as often as that math in the articles say. But RAID 5 still is a very failure prone RAID array compared to ALL other RAID levels. So back to basics, what are we mitigating during a RAID 5 rebuild? We are trying to get parity back before a second drive fails. THATs IT! This is a by-any-means-necessary endeavor. This leads me to solidify my list 

Given XenServer (7 currently) is based on CentOS, does that mean it works just like CentOS in terms of updating, CLI, administration (non-Xen specific like mdadm and boot loaders) etc? Basically, if I want to use XenServer, then am I committing to using, learning, and working in the CentOS "way"? We have a new (to us) server on the way and now is the time to switch hypervisors and we are set on using Xen. Our current setup that I am familiar with and can administer efficiently is a Debian host with a couple VMs using Virtual Box which is less than ideal to say the least. Due to this, I am familiar to working in Debian and have made a conscious choice to use Debian for our servers. I administer only our servers for our small business so I do not have the diversity of other setups and other distributions to work from. From my understanding, the way Redhat does things is a bit different from Debian based distributions and would require a learning curve of unknown amount; but a learning curve for sure. So if I use XenServer, am I also committing to the Redhat learning curve? I am aware that I can install Xen with a Debian based Dom0 but the consensus I have read seems to say XenServer works the best overall. However there will be a bit of configuration I will need to do such as getting our local RAID arrays up and running for the Dom0, Xen, and Network Shares, along with getting boot loaders and grub in order. I can do this configuration rather easily in Debian so I am trying to weigh the cost in time of trying to do the same configuration the CentOS way which I am afraid will add a considerable amount of time to get the new server in to Production given IT by myself for our company slowly happens in the afterhours of business; hence the question.