The site seems to go longer than before, but still dies. Checking the list of processes, I have (third column is physical mem, fourth column is virtual size): 

I have an IoT device that communicates with a remote server via HTTPS over port 443. I would like to intercept the HTTP/HTTPS communication - e.g. using Charles or mitmproxy. If it was a desktop or Android device, I could setup the proxy's SSL certificate in the certificate store. However, this device doesn't really have any HTTP settings - so I'd need to setup a transparent proxy right? However, how do I get it to trust the SSL certificates presented by Charles/mitmproxy? Thanks, Victor 

I'm trying to install memory into a Dell R610 server, and am honestly at wit's end. The DIMMS I have are: 

I have a FreeNAS server setup at my parents place. It was previously running FreeNAS Coral. This had a single ZFS volume called 'datastore'. It's a RAIDZ-1 volume, comprised of 4 x Toshiba 5TB disks. For some reason, that installation seems to have borked itself. Anyhow, I re-installed the boot USB with the latest FreeNAS 11 and booted up. I then went through the wizard to import my old ZFS volume. It seemed to import the volume fine - the GUI also prompted me to update the ZFS pool version, which I did. However, I now notice that I seem to be missing several ZFS volumes - yet no errors are being reported. Output of : 

The dialog asks me for a realm, as well as the hostname of the server, both of which I enter. However, it then seems to hit an error: 

However, this didn't do anything - and from the port 631 - is this just for the CUPS control interface anyhow? (which actually worked via ZeroTier even without this line). Is there some other config that controls how IPP works via CUP? 

I've noticed that there appears to be a delay of ~ 3 seconds for Bob seeing events. The tail -f will echo a logline, then pauses for around 3 seconds, then it spits out 3 seconds worth of log lines, then another 3 seconds of nothing, then outputs another set etc. However, if on Bob I do: 

You didn't specify your ssh key by the looks of it so it's assuming you want to use a password. Most AMIs are configured to NOT allow ssh access for root with a password. Fix your parameters to ensure the key is being specified. 

This is very doable and a great idea if you have the server hardware but not the routing stuff and don't want to make the full switch to AWS. Check out this guide. Just tweak some of it to your needs/what you already have. 

We have a bunch of existing servers in EC2. Future servers are created with Cloudformation with Cloudwatch integration. However, I need to setup Cloudwatch for servers that weren't created with Cloudformation. I have been asked to create a Cloudwatch Cloudformation stack. Is it possible to just create alarms in Cloudformation? If so, how do I specify which servers to monitor? Thanks! 

I've setup IPSEC tunnels between 3 management VPCs in 3 distinct AWS regions. Each of those regions has additional VPCs (dev/prod) that are peered to the management VPCs. It's setup in a hub/spoke like this: 

Without getting into too much detail, why not off-set that common IP to a load-balancer and then put your work-horses behind it? That'll help you scale a bit without losing that traffic. Let me know if you need more explanation. Edit: If your clients are always typing in that IP from memory, there won't be a lot you can do to switch them to a domain. As I mentioned, your best bet will to be to incorporate that IP into a load-balancer. Should be the 'easiest' way. 

We're presently using googledocs(word processing, spreadsheet and email) for everything. We'd like to move away from remotely hosted software. We have a large internal infrastructure to support this sort of thing so technical resources won't be an issue. Are there any decent alternatives to googledocs that meet the following requirements? Open-source Local Hosted option Collaboration/multi-user support Word processing Spreadsheet support Privacy features Importing/Exporting Minimal installation footprint I've been leaning towards a collaboration suite, however, I was also wondering if there is some way to collaborate within Openoffice? Thanks! 

I previously had cURL 7.22.0 on Ubuntu 12.04 Server.. but I now need to upgrade to cURL 7.30.0. I've done the following to compile this version for Ubuntu: 

We noticed one of the drive lights was not lit at all, and thought this may have failed and be the problem. We replaced the drive with a spare, and tried "F" to repair it again, but we keep just getting the same error as above. In the RAID configuration utility, all drives show as "online" and "optimal". We do have this data on another replicated server, so we're not worried about "recovering" anything, we just want to get the system back online asap. The server has 64 or 32GB memory, can't remember off the top of my head, but either way, with a 14TB RAID, I think it may still not be enough. Thanks EDIT - I checked the memory usage while fsck was running as suggested and after 2 or 3 minutes, it looked like this, using up nearly all of our servers memory: 

How do I find out what these are actually doing? Can I see a list of files being accessed by each PID, or any more info? We're on . I tried but it's not giving me much useful info about what's actually going on. Edit - Additional stuff tried based on comments/answers: Doing on each of the PIDs shows the following: 

I usually use MySQL Workbench to do the import but I wanted to use the parameter and I don't think does this. What's wrong with this syntax? I also tried: 

When it failed after 5 minutes or so with the error in my post, the memory immediately freed up again: 

However, if I run I get which is correct. Can anyone explain why there's a difference in version numbers? And how to get them all showing the correct 7.30.0? Does anyone have any tutorials / advice / any help at all on the proper way of upgrading everything cURL related to a later version. The topic seems to be incredibly lacking online, not sure why :/ Thanks Edit - Following one of the comments, here's some additional info: gives gives gives gives gives 

I wouldn't recommend it. There ARE still methods of cracking these quite easily. I personally recommend a Truecrypt volume that contains a Keepass database. It servers me well and is extremely portable. And I'm using it in an environment with thousands of passwords. EDIT: And Keepass is already well laid out for password management. With a nice GUI(i.e., easy to see what password is which type) and built-in password generators...can't go wrong. 

I've seen this done before. It's only as insecure as your network/destination servers make it. Only you know that. Are you transmitting these over a secure network? If so, you SHOULD be fine. But we can't possibly guarantee that. Why not write a simple ssh script to distribute them? That's what I would recommend. Or write a script to download the cert from a central server and distribute the script via puppet. Just an idea. EDIT: Since there is some confusion. I'm NOT saying Puppet/SSH are anymore secure. But if you're worried about unauthorized access, ensure everything is secure. This is most easily done with a custom SSH script YOU distribute. 

I have approximately 30 users on a box. Those users are in overlapping groups(about 6-10 groups). I need them to be able to land in a specific folder based on their group assignment when they FTP in. I.e., group1 -> /tmp/site1 group2 -> /tmp/site2 Is this at all possible with VSFTP on a SuSE box? Using SFTP isn't an option unfortunately. Thanks! EDIT: And in the event of a user being in several groups, just dumping them to the highest-level folder necessary to view the various folders they have access to. 

There is DEFINITELY going to be some consequences of doing this. The primary one is going to be IO read/writes. Beyond that, it's just a very scary way of dealing with that type of data(at that scale). 

I think you're going about this the wrong way. Look into tuning Apache first of all. Then, research Linux memory management. You WANT your server using the ram, otherwise why do you have it? 

I've called Dell twice to try and clarify this basic (or so I thought) question, but I've had two different answers so far. We have a Dell R710, and a Dell R720. Does DRAC come installed by default on either? Dell said it comes build into the motherboard on the R720 but the second time I rang, they said we didn't have it. Can anyone tell me if DRAC comes as standard on the R720? I'm aware we'll need a license as well, I'm just interested in the physical capability on the server. I've never used DRAC before and know next to nothing about it, so apologies if this seems blindingly simple. Thanks 

A is showing tons of activity between two of our servers, over nfs, but as far as I'm aware they shouldn't be. A lot of entries like: 

We have a Dell PowerEdge R720XD server and want to setup iDRAC, but this is a production server so we need to do it without a reboot if possible. We just need to set the IP address on the iDRAC but the servers do not have a front LCD panel so we cannot use this. The iDRAC will currently have the default 192.168.0.120 IP address but we are not on this IP range so cannot use this. We need to change it to 192.168.5.x. I've seen mentioned, but before I start messing around installing this on Ubuntu, is this what we need? and will we be able to set the IP address on the DRAC with this? Thanks Edit - The dedicated iDRAC port seems to be completely disabled until you enable the enterprise license, so I think we need to used a shared NIC for now. We don't know any way of enabling enterprise without first gaining access to it via the web GUI. The server is in production with bonded network interfaces, so we can't change the server IP to 192.168.0.x while we do this, or we might as well reboot into the BIOS and do it. 

I'd 'personally' recommend Debian i386. Its relatively simple to setup and has a great track record. Ubuntu is nice, but not ideal if you don't know how to tweak out some of the cruff that comes pre-installed. Once again, this is just my personal opinion. Edit: If you're willing to spend any money, you may want to check out a Micro EC2 instance from Amazon(aws.amazon.com) or a 512 Linode from linode.com. I've used both for dev work and they're both cheap options. 

I'd like to simulate 1000 concurrent downloads of a single file from Cloudfront. I figured I'd setup ~10-20 xlarge EC2 instances for this. Is there an obvious way I'm missing to trigger this at the same time and get the average download time while ensuring the instances aren't the bottle-neck. We REALLY need to know how much outbound bandwidth we can sustain from Cloudfront. Thanks! 

I just spun up two EC2 instances and got an elastic IP for each. I can't seem to get it to connect. Here is the bulk of my config: 

You've really over complicated your configs. First of all. Look into $URL$ Then, trim down those files to the bare minimum. Make sure to activate the sites and ensure the directories exist. That should do the trick. Also, don't forget to reload apache after all of that. 

I'm assuming you mean block access for incoming users and not for YOUR users connecting to the site(external)? If the former: Look into apache mod_rewrite. That'll do what you want. If you're not using apache, you may need to consult with someone else. If the latter: Look into setting up a proxy server. It'll depend on what OS you're using. 

For the hosting side, I'd recommend Linode or a micro EC2 instance(free tier). Wordpress can handle multiple sites with some plugins. That's probably the easiest one to setup. It's not ideal for static content, but it'll do the trick. 

Check out webmin It should allow you to do a lot of this. There are some great guides on configuring it.