I am using merge replication with SQL 2012. I have a table, and it would be useful to propogate that table to the subscribers, but just the table structure, not the actual data. The data in the table is populated per session, i.e. when you log into the software data is added into the table, and then deleted when you exit the software. What this means is that the data in the table shouldn't be replicated from publication to subscriber, or from subscriber to publication. But it would be good for the table structure to be replicated so that we don't have to run a script manually at the subscribers. 

I am using merge replication on SQL 2012. I have a very large database with a large number of tables. The subscribers are using netbooks. I have a filtering scheme that controls which tables end up being empty at the subscriber. We are saying this for instance 'the user bob is not interested in the street light table so he will receive that table, but with no data in it'. That filtering is working fine. What I am interested in is whether you can go further and not even sent the streetlight table to that user. The reason I ask is running the scripts to create the tables and add the replication triggers is quite slow on the netbook when you are talking about a large number of tables. Here is the breakdown of my sync to the netbook, 

It becomes apparent that these records do not represent actual changes to the underlying records, unless I am missing something? Why are those records there? Is it because of partitioning, or column level tracking? I am using column level tracking, but am wondering whether I need to revisit this option? 

So the only other option mentioned is a view, but I cannot get the view to work either. The first problem is that if I replicate a view it just gets replicated as a view and the underlying table needs to be there. But Simon I read your other post and that is not what you are saying. I think what you are saying is that I should be able to select a table in my replication publication, and then filter it based on a view like this, 

Why is it suggested to regenerate the snapshots every 14 days by default? With web sync you are an anonymous subscriber, so how can replication know when to clean up metadata for a user who never syncs 

Turn off all default constraints for a particular table (not ideal if some of them are not for sequences) Create the sequences manually at the client side, and then create a custom handler to apply the correct sequence values at the server when syncing 

I am using merge replication in SQL 2012. When I bulk insert rows into a table, and run this stored procedure to add references into the merge tracking table, sp_addtabletocontents Is that missing out on precomputing the partitions using the precomputed partition optimisations? 

It pivots on the DRC, but not on DISP as well. The result is like this (0, and 1 is the DRC value). I understand why that is, but how do I pivot on the DISP as well? 

I am using merge replication with SQL 2012. I am writing a custom business logic handler, and would like to know if there is a way to get a session id? For instance if the custom business logic handler catches an insert into a specific table, and then the custom business logic handler catches an insert into another table, how can you tell if they are part of the same session? Also by session_id I am referring to something like the session_id which is in the MSMerge_sessions table. 

I can never get it to do an index seek on the DispatchLink table. It always does a full index scan. That is fine with a few records, but when you have 50000 in that table it scans 50000 records in the index according to the query plan. It is because there are 'ands' and 'ors' in the join clause, but I can't get my head around why SQL can't do a couple of index seeks instead, one for the left side of the 'or', and one for the right side of the 'or'. I would like an explanation for this, not a suggestion to make the query faster unless that can be done without adjusting the query. The reason is that I am using the above query as a merge replication join filter, so I cannot just add in another type of query unfortunately. UPDATE: For instance these are the types of indexes I have been adding, 

I am using merge replication with SQL 2012. When performing a clean sync using the merge agent I am getting a timeout on this stored procedure, sp_mssetupbelongs I am wondering what the best way to diagnose this issue? When I take the SQL that was run, and run it in management studio it runs instantly. I can only assume that part of the required operations were done the first time it was run and timed out. More specifically it must be a particular group of merge articles that are slowing it down, so how can I isolate this? 

I am using SQL server, and I have been looking closely at the concept of key lookups, $URL$ So if you have a key lookup you can create an index with the 'include' columns to cover the non index columns you have in the select statement. For instance, 

I am running SQL2012 merge replication on a netbook using SQL Express as a subscriber. The netbook has 1GB of RAM. The replication process works quite well, but it is when it comes to applying the prc files from the snapshot where the problem begins. As I understand the prc files are the stored procedures for each table. At the start the prc files are applied fast, but further along it seems to slow down more for each one. At the start it seems to take less than 1 second for each, but now it is up to about 10 - 20 seconds for each. If I look back when I subscribed to the same publication on a PC (not a netbook) the prc files each applied in less than a second, and there was no slowdown over time. The SQL server is using about 453 Mb of memory. It has increased since applying the prc files. 

I am using merge replication in SQL 2012. Why can't you mark default constraints as NOT FOR REPLICATION? You can disable all default constraints for a merge article, but it is all or nothing so it doesn't seem to offer enough control. How about this scenario, 

I have a SQL express database and was wondering what options I have for securing it? More specifically it is a replicated database using merge replication. I want to stop a user with a replicated copy of the database opening the database, modifying it and running queries against it. UPDATE: Just wondering though, what happens if I have a database which is secured, but I then take it offline, copy the database, and attach it to a different database server. Does the security go with it, and can I not break into it like that? 

The problem is after step 2. We can force a software upgrade to the clients so that data is written into the new field. But some data may have been modified in the old field at one of the subscribers before they do the sync which makes the schema change. This would also be before they were forced to do the update. The migration could be done more than once, but by the time you get to step 4 its hard to know which modifications have been made to the old field, and which have been made to the new field. 

So it orders the items based on removing the leading underscore characters. It can't just blindly remove the underscore characters though because the underscores might be in the middle of the items. For instance this does not work, 

I'm not convinced about the publication article limitation of 256. If we look here, $URL$ It is talking about a particular query that causes issues on SQL 2005 when you have more than 256 articles. First I ran the stored procedure sp_MSmakesystableviews. It then generated a view called MSmerge_cont4F93AE6035D14E46ADA56D87F66E8962_90. If I take the underlying query in the view it has more than 256 tables. I run that query against a SQL 2005 database and I get the error, 

I am in the process of defining filters on a merge replication publication for our database. The problem I am coming up against is that merge replication has these rules, I don't like to call them limitations because I can understand the purpose. 1) When creating an article filter you cannot include a subquery, or at least you shouldn't. If you do then it will appear to work the first time you sync, but if something changes in the subquery table the filter will not be re-evaluated. 2) When using a join filter you can only join two tables together. The problem I am coming across is that the relationships in our database are more complicated than that. For instance here is one of our relationships, 

In merge replication if I run sp_removedbreplication will that remove rowguids or not? Will it take into account the preserve_rowguidcol values in the sysmergearticles table? 

The user inserts a record 'Shape' at a client The 'Shape' has a default constraint to set the NumberOfSides field to 4 The client user edits the value of NumberOfSides and sets it to 5 They sync with the server 

I am running log shipping with SQL server 2008 R2. I have a situation where the secondary database drive ran out of space and was not applying log shipping transaction logs. The way I want to fix this is delete the databases at the secondary and configure log shipping from scratch. The problem I have now is that my secondary databases are in the restoring state, and I cannot delete them. How can I proceed? For instance if I try to take them offline I get the error, 

So in this case if I wrote out a select statement to apply my filtering for petermc it would look like this, 

This works 100%. I have omitted the rest of it because it is just server, user, password settings etc. If I change the query to this,