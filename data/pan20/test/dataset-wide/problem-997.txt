That table originally had only 2 indexes: the Clustered PK index (that this shows it doing the Key Lookup on) and another FK index on a column not referenced here. Given that this heavy query always needs these add'l columns (DateForValue [datetime], CurveValue [float], BTUFactor [float], and FuelShrink [float]), I thought a covering index was the obvious solution here to remove the (slow) Key Lookup being performed here. So I added the following covering index: 

My background: I'm a dev/architect, not a DBA. Sorry! So we have a ~400 table 75GB database. I ran Profiler for ~24 hours ("Tuning" template minus ) and have ~7GB of usage recorded in a trace file. I then ran the Database Engine Tuning Advisor on this trace file (no partitioning, keep clustered indexes, PDS Recommend: Indexes) against our production database (after hours). I gave it ~4 hours to analyze. And here are the summary results I got: 

The script works fine as-is but the moment you uncomment out that second , all hell breaks loose. I understand this is invalid syntax because I'm mixing boolean logic with bitwise operators. But what is the right way to get this logic in there? Without going crazy with statements, is there some way to do this? 

The forecast_datetime column is a datetime for each 30 minute interval. The issue is that the forecast is for 1 month into the future which causes a lot of duplicate data. (so today, there will be a forecast for 2016-12-25 01:30:00 and then tomorrow there will be a forecast for the same interval, over and over again potentially until the period actually happens.). The view I want to create is to just take a look at the last forecast which was appended to the table. This is indicated by the file_date column (as the name suggests, I receive a daily forecast file which is appended to the database. This column indicates the date from the filename). I basically want to filter all rows for each thing_id and select the forecast_datetime columns where there file_date was the maximum date for that forecast. So each thing_id should only have one row per unique forecast_datetime interval which is based on the latest file_date. 

But it still outputs 1000 rows. I tried to increase the packet size with -a but that didn't impact it at all. 

I am using Windows 10 and running this straight from the command prompt (Administrator). Is there a way to change the row count being output to a text file? 

We're working on migrating our database (~400 tables) from SQL 2008 R2 to SQL Azure. We're currently in the proof-of-concept stage, figuring out the process we'll be following. We have a process that we think is solid but I'd like to perform some validation diffs on both schema and data to confirm that we have a successful migration. I've never done this before with SQL Azure and am looking for a good way to do this. How can I perform this verification effort on both the schema and data? Ultimately, this is a one-time migration (we'll do it a few times but the real migration will only be done once). 

The BlitzIndex tool that @JMarx suggested is working great! However, I'm also finding this additional script to make some good suggestions as well. Not necessarily using all or even most of its suggestions, but cherry-picking from the top is proving very useful! 

We have a number of tables (~1M records) that have a column on them defined as: that gets auto-populated with . We use this ID for synchronizing data across multiple systems, databases, file imports/exports, etc. For this column, we have the following index created: 

In other words, if I have only a daily tournament on schedule and run it to process the next 7 days, it needs to create 7 rows with the correct start_time on each. Consider that in a real world it will have a lot of recurrent tournaments daily, weekly, monthly... And it needs to populate everything for the next X days (7 in the example). I was wondering the best way to solve this and want thoughts/ideas about. 

None of its columns are mentioned in the query, so the question is: Why it's being used and how it's being helpful? The second question is, why it's not using this index I created: 

And I need to create a job (procedure) that will populate a queue of the next tournaments in the history table as follows. 

It runs in 64ms. (Note that I also raised the limit from 200 to 100000). Why is it trying to allocate 1GB ram to process this query with limit 200 and isn't for the subquery form? 

That would make much more sense and it's even smaller in size. The odd index is 41mb long and the second one is 30mb long. I'm trying hard to understand how indexes work. 

Does this sound right to you guys? I expected it to drop most of my indexes but then to create a ton of new indexes. Also, if it takes 4 hours to analyze 9k queries, is it even feasible for me to get this to consider a normal day's worth of usage? Compared to most large databases, ours is fairly light on consumption (~50 users total). I think I'm either misunderstanding something or am simply doing something wrong. 

I'm working with each of the 3 portions of the UNION ALL independent of one another and the other two parts are nice and speedy and executing this third of the unions either by itself or in the union performs similarly (i.e. ~30 seconds). So the UNION isn't a factor but I included it just for thoroughness sake. 

Redgate SQL Compare appears to do this decently well. I'm still nailing down the settings to ignore certain thing (like ) but at a glance, this seems like the tool I want. 

However, even after adding this index, it seems the query is still doing the Key Lookup. Am I missing something obvious here or is this the right idea and I just have a problem elsewhere? Note that all statistics and indexes have been refreshed and this isn't THAT highly dynamic of a table but it is approaching ~1M records. A simplified version of this query, focusing on this table of interest, is as follows. Nothing I removed references the PrimaryTableOfInterest.