take a look at this as a starting point: $URL$ It seems you have to change to log4j and configure it there. If you only want different access logs it is really straight forward: $URL$ I have not tried it with virtual hosts by myself, I use different instances. 

It gives you the possibility to do more fine grained security configuration with SELinux. Each user can than have only the privileges need, e.g. apache has the rights to start processes and access the network, while www can only read files. For more details, take a look at this post. 

but you can't mount a filesystem from the nfs server on the nfs client and you do not get any error message. what is the difference between your and calls? do you use ip adress in one and fqdn in the other? could you please post both commands with output and returncode? 

this will not work, because drbd only supports three nodes. for your automatic updates you should use something like puppet or a central software distribution tool (e.g. spacewalk), depending on the type of your updates. puppet would be best for configuration changes, but can also do software distribution. spacewalk would mainly be used to do software updates, but can also be used for configuration changes. 

this will redirect Request to $URL$ to $URL$ This snippets doesn't tell whether the directory behind $URL$ is / or /var/www/html. The DocumentRoot and Directory Settings is not really necessary because of the ProxyPass directive. you can find even more details in the documentation: ProxyPass DocumentRoot 

when you put this in , will check wether a user is in group yourgroup when the user executes . this way you don't have to add a new user manually to the script. 

Take a look at the docs What's the Difference between Rewriting and Redirecting?. This will explain the difference whether the client will see a change in address bar or not. IMO you neither have to put or at the end of your rule. 

try to telnet to your ssh server: . When this does not work it is most likely that you have a firewall between the two networks. do you have a local firewall on the client or the server? is there a central firewall between the two networks? 

this sounds like you need a configuration management tool. this would be a central repository where you safe the configuration and clients (your windows vms) can get new configurations. as i'm using linux, i'm not sure how smooth this runs with windows, but there are solutions out there supporting windows. take a look at this wiki article. it has a nice sum up. 

i'm afraid i can't give you a step-by-step guide, but it seems you already have the knowledge you need. just setup a new vlan for your testing environment and make sure there is no routing between production and testing. the next better step would be to setup the testing environment on there own switches to prevent the production switch from breaking by floods of packets from the tests. 

These metrics give you a good hint about how the application works and how memory efficient it is. These metrics only make sense for your customers when each application has its own process. But they are definitely very valuable for you. More application specific: 

do you have something in your scripts which depends on environment settings? the environment cron runs with is normally different from your user environment. redirect the output of your script to a log file instead of /dev/null to get an error message. 

You have to add a routing rule for each webserver you want to access with usb0. Find out the IP addresses of the webservers and add 

you can use manual fencing for testing. search for fence_manual in this doc this will not disable fencing, but nothing will happen when you not want to. 

your scheduling is right. i'm not sure why you can't edit the crontab. do you get any error message? 

i don't know of a method to group the mails, but when you add dependencies to your hosts and services you will not get too much mails. 

is it possible to replicate the mod_jk sticky session information to another apache for an failover setup? the idea behind the question is to setup two apaches with sticky sessions in front off some tomcats. when one apaches fails, the other one should take over the mod_jk session information so he knows which requests to serve to what tomcat. i know an alternative would be session replication at the tomcat level and not to use sticky sessions, but this is not possible at the moment. 

just combine the output with some other commands, like this . This way you should get the output you need. 

you could set up your apache as a reverse proxy for the shop. so apache will forward every request to the etsy system and the user will only see your domain name in his browser. for detailed configuration look at the apache documentation here. it has some examples and explanation of the concept. 

i don't know websvn, but your problem is: the self-signed certificate is not trusted. do you get this message in your browser or on the server? when in the browser, import your self-signed as trusted to the browser cert store. when it is on the server, try to add SSLCACertificateFile to your apache config. 

I don't know your distribution, but in SUSE you can edit the file and enter one or multiple proxy servers for system wide usage. Here is an example: 

Only the lightweight server needs the certificate, because it is handling the ssl traffic and the client is only communication with this server. A ssl certificate is normally bound to the fqdn of the server, unless you have a wildcard certificate, like *.example.com. This said, you can use the same certificate for multiple servers on the same machine as long as they all have the name of the cn tag of the certificate. Otherwise you would get an error message in the browser/client, telling you server name and certifiacate name do not match. 

This will give you alot of output, from apache and the system calls, but perhaps you'll find something interesting. With you can write the output to a file which makes post processing much easier. 

this should be, what you are looking for: $URL$ It is a combination of nagios plugins for windows and nsclient++. 

A lot of your questions are answered by the stunnel FAQ. There is also a special page on client certificates. 

when you want to go with the last versions, you should use fedora. centos/rhel is made for long time, stable use. there won't be the latest versions updates included, only updates for fixing bugs. 

to disable reverse lookups, disbale HostnameLookup. when this does not help, be sure to not use hostnames in , , deny/allow rules and in . 

you should use . In the file you can configure the commands which should be run with root privileges and whether a password is needed or not. 

just copying will not do the job. You have to take care the program is decoupled from stdin and stdout. Therefor all output has to be printed to a logfile. You also have to background the program, which should be done by the daemon function. 

normally the traffic will go back through the lb. this is necessary with sticky sessions for example. when you do not want the traffic to go back through the server, you have to send a redirect. but this way, the client will direct the next requests directly to one node and will not know about the other node when this one goes down. so it is definitely your interest that the traffic goes through the lb all the time. 

just put the redirect rule in the vhost config of your deploy domain. this way it will only be active for this host. 

you have to add the self-signed certificate as trusted to your client certificate store. This way the client will accept it and establish a connection. 

I assume your first describes linux box has a timeperiod set to wait before doing the actual reboot. To get to now more about the shutdown/reboot process in your linux distro, take a look at . It will explain, where the configuration is placed. In my distribution, SUSE, these are some files which are part of the shutdown process: 

just replace db1 with the database name you are searching for. when you take your example from above, this script will give you: 

Don't know whether this is present in the service context, but you can test it. Define a timeperiod "non-working hours" and take ISVALIDTIME as a parameter to your eventhandler script. When the script is called during working hours, let it do nothing and exit. When this doesn't work, you could just verify the time of day in the eventhandler script. 

you were heading in the right direction. in tomcat 5.5 connectionTimeout is the same as keepalivetimeout in tomcat 6.0. but you should change the value from 2 to 2000, as the tomcat value is in milliseconds and the apache value is in seconds. you are right as you don't see the timeout value in the http headers send by your tomcat. but the timeout will take action on the server side. i asume it is not send as the tomcat connector is HTTP/1.1 and the is HTTP/1.0. as the tomcat documentation tells, you should synchronize the timeout values on both sides, loadbalancer and tomcat in your case. 

Take a look at the tomcat logs. They are placed in the directory in the tomcat install folder (e.g. ). this might give you some hints about whats going on. 

one possibility is using openldap. here is a how-to to get you started. the how-to is a bit outdated, but it should get you started. perhaps your distro offers some of the needed packages, this would make it much easier. another option is ad. when you already have a windows network with a working ad, you can add your linux servers to the ad. you could use unix services for windows or pam or ... some distributions already have bundles to connect to ad, for example likewise in fedora/rhel. 

when you have different clients you can put multiple first nodes in your openldap, but when the different clients are only different organisational units you should create them as subnodes of one organisation. do you have an example what types of first level nodes you want to create? perhaps this way you get some more precise answers. 

a lot people are trying to adopt agile software development principles to system administration. and there has been more written about than i can tell you here. i will mention some resources later on, but first some hints: 

this will work , but works from the back to the front. here is a good how-to for chopping strings in bash. 

how do you document changes to your configuration which were made during an incident? In ITIL incident- and configuration-management are two different processes but they are related at this point. My ideas are: 

is your java application using log4j? perhaps the app has a rollingfileappender configured, so the logrotation came from the app itself not from your logrotate configuration. 

you could monitor your application via jmx from the outside. when you know some metrics which indicate an upcoming OutOfMemory, you could trigger a jmap run before the exception is thrown. 

you can set the variable in . but this is the timezone for all crontabs. another alternative is using fcron. the documentation shows an option to set the timezone in each crontab. 

There is an extra module for doing this: mod_dumpio. Here you'll find an short article how to use it. 

to get the ssh output to your logfile, you have to redirect to . you can do this by appending after your bash script. it should look like this: 

For monitoring and deployments on a single tomcat, psi-probe is a possible solution. It is a fork of Lambda Probe. For monitoring the hole infrastructure I agree with the Nagios answer. An alternative to jVisualVM is jconsole, also part of the JDK. For managing the hole infrastructure, I think there is no way around some custom script, whether they are written in bash, perl, ant, puppet manifest or whatever. But you can hide their complexity behind such tools like already mentioned hudson, puppet, cfengine, chef, ... 

take a look at this section of the awstats faq, this is the solution to your problem. there are also some examples. 

perhaps your default policy of iptables is allow for all input traffic. this would allow a connection from any ip. change this policy to drop: 

i don't know the check point vpn, but in f-secure it was possible to configure the vpn profile to only route some defined networks/subnets over the vpn.