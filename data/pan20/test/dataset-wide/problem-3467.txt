As a consequence, if it is true that $R$ is truly known with perfect certainty by all actors except the economist measuring the behavior of $x_{t+1}$, then the economist has no test that can ever be performed validly to determine whether the hypothesis is either true or false because the statistic is the mean of a distribution that notoriously has no mean. You cannot accept or reject any null hypothesis because there is no power to test the hypothesis in the first place, regardless of sample size including having an infinite sample. In the Harris paper above, the equation does have a Bayesian solution, but that rejects the rational expectations model as no Bayesian solution with a proper prior is unbiased. R must be greater than one as who would invest money if they believed the future would be worse tomorrow so all models with capital in it must lack a mean and a variance. Rational expectations should hold when one is playing roulette or parimutuel horse racing. Indeed, there is literature showing this to be the case that is quite credible. Those models have finite variance so they behave well. It isn't that people are biased or anything like that. Models where $R$ is greater than one have no solution. The White paper above is effectively a non-existence proof. So, for macroeconomics, the best refutation is that there is a non-existence proof for the expectations. 

As with all the others, I would agree "no." I am only treating the first sentence as the axiom as the "corollary" implied in the second sentence is actually less clear. As the axiom I am taking it to be 

You are missing the key element, transfer fees. If the transfer fees are large enough, then the marginal cost to move between exchanges just needs to stay inside the gross bid-ask spread. It can probably move outside that spread because there is an additional risk for one who moves between exchanges. Not all exchanges move back and forth between real world bank accounts equally well, nor do all have equal trading depth. Additionally, the time value of money between the exchanges is probably sufficient to eat up any arbitrage. It may be worth giving up arbitrage profits if they are traded for the time value of money profits. If the arbitrage is small and the cost is great, you can carry a wider bid-ask spread and get away with it as a market maker. It sounds like you need decision theory and not game theory unless you are considering the behaviors of the other actors as part of the game. You should start with de Finetti's Coherence Principle. It is a weaker principle than the "no arbitrage" principle. In its weakest form, it only states that odds are coherent if a bookie cannot take a sure loss in all states of nature through the clever combination of bets either by a single or by multiple actors. What makes it weaker is that it never says that no arbitrage opportunity exists, but it would imply that if it did exist, then the market makers would capture all or nearly all the profits. I would be surprised to find that the exchanges did not profit greatly from the arbitrage opportunities on each other's exchanges. They are perfectly placed and have deep inventory that they could borrow from their customers. They probably allow some to go by to get speculators into their markets. They are profit maximizers. 

So, let me start with your second question. No you cannot multiply by 365. You could approximate it by $$\log(\text{Annual Return})=365*\log(\text{Daily Return}),$$ but for what you are doing, it does not make sense to do so. You are correct in your annualized rate of return. It is 2069063%. It should be obvious as to why you would not want to do this. There are two solutions. The first is to convert annual rates, such as the bond rate, from an annual format to a daily format. So make your risk-free rate: $$\text{Daily risk-free rate}=1.065^{\frac{1}{365}}-1=0.0001725485.$$ The second is to search through the dates of your returns and find returns that are 365 days apart, so return would be $$r=\frac{p_{366}}{p_1}.$$ By annualizing daily returns, you are insanely increasing the variability, but it is artificial so it isn't a true increase, it is an artifact of the method you are choosing. 

You never consider either $r^2$ or significance in choosing a model. $r^2$ is non-decreasing in the number of parameters. You can improve $r^2$ quite often by adding a set of unrelated variables, such as the rainfall in Quebec to estimates of the price of diamonds in Singapore. Significance assumes you believe the null is true. If you have multiple models, then you obviously do not believe one to be true. When using Frequentist methods, model selection should be done with something similar to the Akaike Information Criterion or the Bayesian Information Criterion. Both are summary point estimates of a stylized Bayesian posterior density over the model space. Only use one, don't use both. There are also a few other similar methods. The information criterion are essentially Bayesian solutions with slightly different prior mass functions. You should write software to go through each combination of variable, excluding variables that are collinear, and run either the AIC or the BIC. The AIC and the BIC measure how close a model is to the data generating function. This is not like significance an doesn't imply significance. 

There are four categories of approximations used in economics. Unfortunately, none of them are economic approximations, but rather mathematical or statistical approximations. As such, they do not appear in the economic literature, so much so as in the mathematical literature. You are not seeing in the economic literature because students should be exposed to the content by mathematicians. The difficulty comes from the type of math used in economics. There are two ways you could look at calculus. One is that it is the study of change. The other is that it is the study of approximations. All of calculus is a set of approximations, though if you will mentally agree that $\lim_{n\to\infty}\frac{1}{n}=0$ then you can get "exact" answers. Imagine, however, that the smallest unit is a penny, then you cannot go to zero and now you have an approximation if a penny is the smallest possible size. The question economists have to answer within the profession is whether or not the difference has a large enough impact to matter. It is rare in economics for anything to be anything except an approximation. There are some categories though to these approximations. The first category can be thought of as either equilibrium approximations or local approximations. Mathematicians would use the phrase "local approximations." The main one of these is the Taylor expansion, though there are others. In classes, I have to be very careful when I teach undergraduates because they often substitute their own mental approximations for the actual ones in work. While it is wonderful that they are trying to work problems through from first principles, its usually the ones who didn't study the answer in the book and who are panicking. The second one can be thought of as functional approximations. The actual functions in the real world are discrete and usually created by the separate functions of many different firms. While there are certain mathematical properties created by the topology of the problem. We know that certain types of problems are forced to have a certain general form, but the specific form will be unique to the real world problem. As an example, a taxi company needs one driver per cab. If you add cabs but no drivers then no revenue gain is created. Likewise, if you add cab drivers but no cabs, you get not revenue gains. Indeed, in both cases you can take losses from paying the cabies and the costs for the cabs such as insurance. So the production function, which is a Leontief production function is assumed to be present almost all the time, ignoring cabbies who quit, or wrecked cabs resulting in a temporary mismatch between workers and cabbies. There is also the issue that utility functions are ordinal and any function that preserves preference ordering is equally valid. This is not an approximation so much as an arbitrary choice for convenience. The third category could be thought of as model approximations. For example, using game theory, is an arbitrary choice as a way to model something. The use of a general equilibrium method versus a model that is sub-game perfect will create different approximations in the end. Of course the goal is for empiricism to judge between the modeling methods, but for your purposes they should both be thought of as approximations. Finally, all of statistics is a form of approximation. The danger is to believe statistical models and all economic models are statistical models. They can be excellent or poor. Statistical significance is not enough, it needs economic significance, that is "what is the effect size." Likewise, how sensitive is the statistical model to the data and to reality. You can easily create a statistical model that is not at all robust. That will make your approximations very fragile. There isn't a list. The most common are Taylor expansions, finding the slope, $\hat{\beta}$ in models as an approximation of the derivative, and production functions. The best way to tell you are dealing with an approximation will always be seeing $\Delta$ in a problem. If you see $\Delta$ then you are dealing with an approximation of $\mathrm{d}$. Although $\mathrm{d}x$ is really an approximation of $\Delta{x}$, economics always solving inverse problems so in economic practice $\Delta{x}$ is an approximation of $\mathrm{d}x$. 

All price changes in the stock or commodities markets impact the money supply to the extent it results in a change in margin credit. Do note that the amount of credit itself must change and not the price alone. A crash only has an impact if it results in net changes in the amount of credit. This is a second order effect of a crash and not a first order effect. It also usually has no material impact on the money supply. Although a crash may have a large impact on notional concepts of wealth, the effect on the supply of money itself is usually neutralized by the banking system. It is normal, even in the 1929 crash, to see bankers announce they will keep open the brokerages' lines of credit and stand ready to purchase safe assets such as Treasuries in unlimited quantities from these brokers to hold the money supply roughly constant. If brokers sold enough treasuries, it could even expand the money supply. The size of the money stock (M2) is 13.4 trillion dollars. The total size of all margin credit from all sources is 554 billion dollars. Although this is four percent of the money supply, which is a lot, you are also forgetting that in a crash margin purchases will begin happening too. Just as brokerages will be issuing margin calls, other investors smelling blood in the water will be purchasing on margin, though this will be a smaller effect. The Fed normally drops the interest rate to expand the money supply after a crash to compensate for the loss of money created by the reduction in margin credit. Remember that 554 billion would not be wiped out in a crash, it would just be reduced, at least initially, as brokers required repayment. 

I think there is a missing concept in your question. If you are discussing simple gambles and not things like the stock market, then the distributions that are involved usually have sufficient statistics. A statistic is sufficient for a parameter if the point estimate could be substituted for the data itself with no loss in information. A statistic, $t$, is said to be sufficient for $\theta$ if $\Pr(\mathbf{x}|\theta)=\Pr(t|\theta)$, where $\mathbf{x}$ is a vector of data. What this implies is that the implied distribution from an observed set of data has a perfect substitute in the vector of point statistics when sufficiency holds. They are indistinguishable concepts mathematically. If you are talking about the stock market, then those distributions lack both moments and sufficient statistics and people need to use the distribution or fully process the uncertainty of the parameters using Bayesian methods as Frequentist solutions do not exist that are admissible in the general case. The reason is relatively simple. Consider a stock traded on the NYSE. It is sold in a double auction with many potential buyers and many potential sellers. Since the stock is sold in a double auction it follows that the winner's curse does not obtain. Since the winner's curse does not obtain, it follows that the rational behavior is to bid your expectation regarding the price. Since there are many potential buyers and sellers, the limiting distribution of those prices is the normal distribution from the central limit theorem. If we assume we are in equilibrium, then the purchase price and the selling price are both normally distributed ignoring bankruptcies, mergers and liquidity costs. Those do not change the general principle here, but do radically alter the mixture distribution. Nonetheless, if we center our graph around $(p_t^*,p_{t+1}^*)$ then we could also think of it as being $(0,0)$ in the error space. The return can be operationally defined as $$r_t=\frac{p_{t+1}}{p_t}-1,$$ so it follows that returns are the ratio of two normal distributions. In error space, the distribution of two normals centered around zero is the Cauchy distribution. If you then translate the distribution back to price space and truncate due to the limit of liability your distribution of returns must be $$\left[\frac{\pi}{2}+\tan^{-1}\left(\frac{\mu}{\sigma}\right)\right]^{-1}\frac{\sigma}{\sigma^2+(r_t-\mu)^2}.$$ Although you could manually verify the absence of statistics by the Neyman Fisher Factorization Theorem, it is well known that the statistics from this distribution are not sufficient. This can also be seen from the Pitman–Koopman–Darmois theorem which basically states that only distributions in the exponential family of distributions, such as the normal, have sufficient statistics. Joint sufficiency does exist for purposes of inference, but not for projective purposes which are what matter here. As such, any point estimator not constructed from the Bayesian posterior predictive density will lose information, this is amplified by the truncation. The Bayesian posterior predictive distribution can be used because it is defined as $\Pr(\tilde{x}|\mathbf{x})$. Notice that there are no parameters at all in that probability statement. The problem with this distribution is that neither skew nor kurtosis are even defined for it. Consequently, higher moment discussions with stocks are deeply flawed as the above distribution has no moments at all. Old articles such as 

I am a financial economist, so I will attempt to answer what I think you are asking. A market order isn't one concept, it's many. Each place that accepts trades operationalizes it differently. It also depends upon whether it is a small order or a large order. For a small order, the order result will clear at the bid if it is a sell order at the bid and at the ask if it is a buy order. Only in that case will the outcome be the same everywhere. For a large order, it depends upon where the security trades at and it can depend upon the internal policies of which broker-dealer you use. For example, if you place a large market order on a stock that trades on the NYSE then what you are actually doing is instructing your broker to go to the floor of the exchange, to a place called the "post," and continue to bid until you are the high bidder, if buying or the low bidder if selling, until your order is filled. On the other hand, a large market order on the NASDAQ will be filled up to the stated size limit. After that, it depends upon a number of things. For example, if there are multiple market makers then decisions to fill or not fill is made by each maker depending on orders they have, their inventory needs and what risks they will take and for what fee structure. Because a market order essentially says "I will pay any price," the makers have liberal pricing powers on large market orders. Also, the processing broker can sometimes cross the order with other orders they have pending from their own customers and so it never really makes it to the market itself, other than for record keeping purposes. Outside of this, there are order crossing services in what have been nicknamed "dark pools," which are available to the largest financial institutions but not to ordinary traders. Different rules are in use for things other than stocks and for stocks traded in non-US exchanges. To provide an example, imagine you own $\$1,000$ and also own 100 shares of ABC which is currently trading at 9.90 bid and 10.10 ask. It trades on the NYSE. A decision to sell 100 shares at market would increase cash to $\$1,990$ and reduce shares to zero. Since you may or may not be permitted to short sell the stock depending on your contract with your broker and the availability of shares to borrow, a decision to short sell 100 more shares of ABC would increase cash by the new bid, which for our purposes will be 9.85 per share, for $985 and add one hundred shares on the short side in the margin account. The cash will also be segregated and held as collateral on the long side of the margin account. A large market order for a thinly traded stock could take a month or more to fill. For a "pink sheets" stock it could take years. For a heavily traded stock, it could take less than a second. It is important to note that the recording of a trade for customer accounting purposes may not match market records. Consider a person who sold 100 shares of ABC for 9.65 per share. They record an increase in cash of $965 and a decrease in shares by one hundred shares. The "tape" may not record it this way if it is part of a large buy order. In that case, if the weighted average price was 10.10 then the market would see one large order for 10.10 per share, though it may also be the case that nobody actually received that amount of money. It is also the case that the time of order completion will not match.