Deleting data isn't expected to change the size of the table segment. It will create free space in blocks that are part of the table segment and it will probably create at least a few empty blocks. That free space will be made available for subsequent operations on the table (assuming you're doing conventional-path inserts) as well as operations that cause the size of rows to increase. If you want to actually shrink the size of the table segment (generally not a good idea if you're just going to insert more data into this table in the future), depending on the Oracle version, the type of tablespace, and whether you've enabled row movement you can probably shrink the segment 

You could also define a different collection for each column (you could probably define a much smaller number of collection types) 

As with any other piece of software, it's a question of features vs. cost. Enterprise class commercial databases (Oracle, DB2, and SQL Server primarily) generally have a variety of technical and non-technical features that are very helpful for large companies. Focusing just on the non-technical, commercial databases will generally have a much more mature support organization (both from the standpoint of dealing with bugs and support issues as well as being able to call in pre- and post-sales folks for demos or to discuss architecture options). There will generally be more people in a given market that are experienced with the various commercial options. And companies generally already have a fair amount of expertise in-house. Then, of course, there are the technical features but there you have to get into much more detail to figure out which features of which databases are going to be important to any particular project or organization. The market for database engines in a startup (particularly a technology startup) is generally rather different than the market for database engines in an established company. A startup is generally going to prefer the cheaper option even if it requires more development time up front both because cash is precious to a startup and because developer time is generally less so. A technology startup is also likely to want to invest a lot of time customizing their particular technology stack because they have a vision where their technology stack provides some sort of competitive advantage. And remember, for every startup you see that has managed to scale up like Facebook or Twitter, there are probably thousands that failed (not always because of their technology choices, of course). While it's quite possible that something like Facebook can write quite secure and scalable PHP code on top of a database engine that provides eventual consistency (that is, everyone will eventually see your latest status update but it may take some time), that doesn't necessarily imply that it's a great idea for the next company to mimic that infrastructure. You're not looking at the hundreds of sites that failed because the PHP developers didn't do proper separation of concerns and created brittle code that couldn't adapt quickly enough when the business needed to pivot to a new business model. You're not seeing the extra effort that would be required to bring a company full of developers up to speed on a new database engine or the headaches that most companies would have trying to go out and hire a Cassandra expert if they wanted to ramp up development. And it doesn't imply that the technical assumptions that underly one company's technology choices apply to another company-- your bank ought to care a heck of a lot more that everyone is always seeing a transactionally consistent view of the data than Twitter cares about whether you can see every single tweet someone has made. 

If you actually have different databases (and it sounds like you do), you can't grant permissions for users in one database to access data in another database. You'd need to create a database link in the EmployeeDB database that connects to the ContractorDB database. That database link can either use a fixed username and password in the ContractorDB database (i.e. create an EmployeeUserRemote user in ContractorDB with a password that doesn't change that can be hard-coded into the database link definition) or that database link can be a database link in which case the username and password would need to match in the two databases for every user that wants to use the database link. Queries that use the database link log in to the remote database (ContractorDB) as the specified user and have the privileges of that user in the remote database. 

If you have enough child tables, you could potentially write a script that dynamically generates that DDL. Once that's done, you can drop the old table and let the applications start modifying the data again. 

Personally, I've never come across a case where there was any benefit to ditching the "normal" PL/SQL syntax in favor of using in a trigger. This assumes that you can call the procedure successfully in general. 

There was never a rule that bitmap indexes were only useful on columns that had relatively few distinct values. That was a myth that derived from the fact that bitmap indexes aren't appropriate for columns that are unique or mostly unique and that a lot of the columns that you would want to put bitmap indexes on happen to have relatively few distinct values. Richard Foote (who probably knows more about indexes in Oracle than any other person on the planet) has a nice article on bitmap indexes with many distinct values that walks through why this is perfectly reasonable and appropriate in much more detail. A followup article comparing bitmap and b-tree indexes on columns with many distinct values is also well worth reading. 

Assuming that you allow multiple users in your application, will guarantee that you will get duplicate values. Therefore, it is a bad practice. It's also less efficient but that is a lesser concern. If you have two sessions, session 1 gets the max() + 1, and session 2 tries to get the max() + 1 before session 1 commits (or rolls back), session 2 will get the same value that was given to session 1. Using a sequence will eliminate the possibility of duplicates. Use a sequence. 

In general, no. A tnsnames.ora change shouldn't require a reboot but some applications will read and parse the tnsnames.ora at startup to be able to present a drop-down list of servers to the user, for example, and will cache whatever was read when the application started up rather than re-reading the file. Depending on the situation, it might be easiest to reboot a Windows client rather than figuring out how to kill and restart any applications that might have the data cached. A sqlnet.ora change might benefit from a reboot for the sake of consistency though it is not required. If you are doing something like enabling dead connection detection on a server by setting , for example, it probably makes sense to reboot the server to make sure that the setting applies to all connections rather than just new connections-- if you're trying to debug why a particular dead connection is still hanging around, knowing that the server was restarted and that you're not looking at some artifact of a connection that was opened prior to the setting being made would generally be helpful. 

Generally, though, I'd question the problem you're trying to solve. I can't think of many times that it would make sense to drop a user that has active sessions at the time you decide to issue the drop. In the vast majority of cases, the presence of active sessions strongly implies that the account should not be dropped. 

Additionally, you'll want to make sure that your initialization parameter is set to something greater than 0 to ensure that jobs will actually run. 

which your original query could use. Of course, in either case, whether Oracle actually uses the index will depend on how selective it expects the predicate to be. Since you are returning 4 years worth of data, it seems unlikely that an index would be beneficial even if it was eligible to be used. Unless your table has hundreds of years worth of data in it, a table scan (or a full scan of one or more partitions if the table is partitioned) would seem like a more appropriate query plan. 

Oracle's SQL Developer is a free Java-based GUI that works with any database that you can use to connect to a variety of different databases using JDBC. I use it regularly to connect to Oracle and SQL Server databases. It has Sybase and Access support out of the box and there are MySQL and RDB extensions that I'm aware of. I'm not aware of a SQLite extension though it shouldn't be too hard to put one together. 

You would need to use analytic functions to get that result. I'm not completely sure that I understand exactly what results you're looking for, but something like this would appear to be what you're looking for. 

will interpret as the column in the table, not your local variable . One of the reasons that PL/SQL developers generally add prefixes to local variables (i.e. rather than ) is to avoid inadvertently using a name that is also used by a column in a table because these sorts of scope resolution problems are notoriously difficult to debug. 

In the simplest case, when you restore a database, you tell RMAN to recover it to the most recent point in time by applying all the (archived and online) that is available. That will re-apply any statements that were executed during normal database operations including things like dropping tables and deleting data. If you want to restore a database to a point in time before something bad happened, you'd need to do a point-in-time recovery with something like 

First, if you are creating a procedure in a package, the package name will need to be included when you call the procedure. 

If I try to guess what you really want based on the SQL statement that you posted, my guess is that you want all the file names where there exists a row with a status of 'started' and a status of 'completed' (I am guessing that the 'Started' from values 1 and 3 are erroneously capitalized) then you would want 

Realistically, you're often better off creating an actual function that you use in both your index definition and in your query so that you're not coding the statement in multiple locations. 

It's realistically a bug in whatever version of 10g you're using that would appear to be fixed in whatever version of 11g you're seeing an error on both statements. I don't have a 10g database in front of me to test this. I would wager, through, that you'd get an error in both cases if you used the old-style join syntax rather than the SQL 99 syntax. That would imply that the problem is that Oracle wasn't translating from SQL 99 syntax to old-style syntax correctly and was missing the fact that the column was ambiguously defined. 

Depending on exactly what you need (and why you're running the query), you may want to truncate the to the hour or minute so that you're always comparing against data from the top of the hour 

Normally, listing out the columns you want to select, particularly doing it once when you are defining a view, isn't a big deal and isn't something you should seek to avoid. It's just part of coding. Normally, explicitly listing the column names is the appropriate way to define the view. Be aware that, unlike with a standalone SQL query, Oracle automatically expands the when you define the view. So even if you define the view with an 

Why are you doing this in PL/SQL in the first place? The most efficient way to do anything in an Oracle database should be to do it in SQL 

Not to the current session, no. You can use the package to instrument the code so that tools running in other sessions can query and see how far along the process is. This is the same tool that Oracle internally writes to during long-running operations so lots of front-ends like Enterprise Manager and Toad will already read from it. Alternately, you could use autonomous transactions to write to a status table as your code runs and read from that status table in a different session. I can't see why you'd want to, but you could theoretically also use the package to send alerts to other sessions. 

In general, it would mean that the instance that was servicing the request failed and is no longer running. In a RAC cluster, you run one instance on each node of the cluster. I wouldn't make sense to move an instance from one node to another in the cluster-- that would imply that one node would be running two instances of the same database. Depending on your environment, the DBA might choose to add a new node (and a new instance) to the RAC cluster at least temporarily in response to the failure of one of the instances. 

I'd rather have the environment be the suffix assuming I had to pick one convention. Otherwise, it's too difficult to navigate through lists of login names that all use the same first five characters. Of course, if given my choice, I would prefer to use neither the prefix nor the suffix. I'd rather keep the same username in all environments and vary my password and TNS alias to identify which system I'm connecting to. 

You cannot create a foreign key that references an Oracle data dictionary table. So assuming the "users on that database" are Oracle database users that exist in , you can't reference in your constraint. You can, however, create your own copy of the table and create a job (using or or some external scheduler) that will periodically copy the data from to your copy of the table. You can then create a primary key constraint on the column in your copy and create a foreign key constraint that references your copy of the table. You may need to work with the DBA team to ensure that your copy of the table gets refreshed as part of any process that creates new database users depending on how soon after a user is created you would expect rows to be created in the child table. 

The only real downside is that there are a number of configuration options that you can put in a TNS alias that you can't use when you use the easy connect syntax. Easy connect is designed to simplify the syntax for simple connections. In order to do that, though, it loses the ability to create more complex TNS aliases that do things like load balancing/ failover or that request a dedicated server session. If you don't need any of that, there is no downside to using the simpler syntax. As soon as an organization has one use case that uses something that the easy connect syntax doesn't support, though, it either has to support a hybrid configuration where some things use easy connect and others use TNS aliases or it has to use TNS aliases everywhere. Supporting hybrid configurations or changing how connections are handled can be somewhat painful depending on your perspective. It's relatively easy for the Oracle client to handle a bunch of different naming conventions. It's much more difficult for an organization to support. It's relatively easy for organizations to come up with ways of distributing a single, shared tnsnames.ora file to everyone or to stand up a central LDAP server to host the organization's TNS information. If some applications use easy connect syntax and some organizations use TNS aliases, that can be problematic when an organization wants to do things like consolidate databases or move databases from one host to another. If an organization wants to add load balancing support, it can be problematic when some applications fail over automatically and some don't. Someone has to track which applications are which, admins have to treat applications differently rather than being able to count on organization conventions that are adhered to across the board. None of this is impossible, of course. But consistency across the environment makes DBAs and admins much more productive and allows teams to support many more applications/ databases/ whatever with any given level of staffing.