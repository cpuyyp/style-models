Since you are using states, provinces, and boroughs in your design, and not prefectures, for example, I'm assuming that you are working in a North American context. If that's true, then you have well established postal authorities (USPS, CPC) with very well regulated postal data and readily available address data quality tools. Even if you are working outside of US/Canada, there are probably data quality tools that will do what you need. With validation and standardization of your address data, you can make sure that you are able to meet your first goal. Using ZIP+4 in the US and Postal Code in many other countries, you can get everything you need for your second goal. A lot of people are really tempted to break addresses down into granular fields. This is a reaction to how bad address data typically is when all you have is "address_line_1, address_line_2,...". However, breaking out lousy, unvalidated city names into its own field only mean you've got a smaller pile of garbage instead of a larger pile. The only way to solve this is to use an address data quality tool to validate and standardize your addresses. If you attempt to normalize your address data you end up with a big pile of many-to-many associations. This is because addresses in real life don't fit into the neat hierarchies that you would see in a textbook. Unless you have some really specialized need for addresses, just keep your tables simple (a few address lines, with maybe the postal code broken out) and get a good address data quality tool to scrub the data on the way in. 

You could really use either option: one translation table with a partitioning attribute, or a one translation table for each table requiring localization. The pros and cons of either approach are not likely to be related to the criteria you ask about: space or query performance. These aren't necessarily the most important criteria for you to be concerned about either. Unless you have really substantial scalability requirements then you should focus on code maintainability as a differentiator rather than disk space or query performance. Another factor to consider is how much time you want to spend explaining your choices to other developers. If you use a single table with a partitioning attribute, then you are opting for something very much like Entity Attribute Value (EAV) which is widely frowned upon for many mostly good reasons. On the other hand, having a translation table per translatable object lets you tailor your column sizes to the actual data rather than picking the largest common denominator and also use tools like declarative referential integrity which are generally seen as best practices. 

A system scope / boundary diagram is sometimes also called a level zero data flow diagram or a context diagram See here for the Wiki on DFDs and here for System Context Diagrams. Whereas an ERD is a model of data at rest, DFDs are a model of data "moving" through processes. Before RDBMS was the predominant data storage mechanism, data was kept in files that were accessed sequentially and was processed a record at a time. Each time the data was processed it was written into a new file. In this way, you could think of data "flowing" from one place to another, transforming as it went. Each data store was shown - often as a rectangle. Processes were shown as circles ("bubbles") and flows were shown as lines or arrows. The level zero DFD shows the whole system as a single bubble. Outside of the bubble are external actors (e.g. users of various types) other systems, and any other data sources and sinks that you might be concerned with. The idea of a DFD is to incrementally decompose your system into smaller and smaller processes, until the whole system is described. The level 0/context diagram is the starting point and you drill into each bubble as deeply as necessary to develop a complete system description. In modern systems, especially where data is stored in RDBMS, the data flow paradigm is not always an especially good fit. However, the context diagram, or level 0 DFD is still useful for describing the scope or boundary of the system in terms of data sources and sinks. 

The question you should ask yourself is whether or not a website click is really the same as an ad click. Do they have exactly the same predicates? (other than what type of parent they have) Similarly, when you access click data, will both types often be considered together or are they always treated separately? The reason that this is important is that you want to draw a distinction between a true subtyping situation and simply having two distinct tables with similar schemas. Someone with a programming background might be tempted to say: "These two things look alike, so they should be like implementations of an Interface." This leads you to a polymorphic design. However, the considerations for entity subtyping are not quite the same as the considerations for code re-use. If there is no reason to comingle both types of clicks in a single query, then there isn't necessarily a good reason to comingle both types of clicks in a supertype table. There may be a trade-off to be made between a bit of code reuse and keeping your data organized for convenient and logical retrieval. 

Would it not solve your problem to redefine the table such that it contains either the grade of a course or the level of the course (which ever applies)? The rows in could contain values such as: 

You are much better off using the EAV (entity-attribute-value) approach for a product feature catalog. The approach is transportable to any SQL platform as it doesn't rely on a particular feature of Postgresql (JSON columns). All of the usual arguments against EAV don't apply to a product feature catalog, since product feature values are generally only regurgitated into a list or worst case into a comparison table. Using a JSON column type takes your ability to enforce any data constraints out of the database and forces it into your application logic. Also, using one attributes table for every category has the following disadvantages: 

In a datamart you need to denormalize time variant attributes to your fact table. Your transactional source database will have the flyer's club level on the flyer table, or possibly in a dated history table related to flyer as suggested by JNK. In your datamart, you need to apply the current club level of each particular flyer to the fact record that brings together flyer, flight, date, (etc) In your case, club is a time variant property of flyer, but the fact you are interested in is the combination of a flyer and a flight. Therefore you need to record the FlyerClub on the flight transaction (fact table). This will work as long as you don't let flyers change clubs in mid-flight. 

One option would be to replace your n identical SQL Server databases with writable views against your data warehouse. In this way, you would keep all of your data in one place, but you could use views to filter the data such that each subset is viewable as a distinct entity. Since you really only have one copy of the data, it won't matter whether the insert/update/delete is done to the filtered view or to the unfiltered table. Both sides will see it at the same time and there will never be a synchronization problem. Note that in order for a view to be writable, certain conditions must be enforced. See here. 

A general rule of thumb is that you will have a table for each type of thing that you are recording information about. Each instance of a thing will be a record in a table. It would probably* be a terrible idea to have one table per user. Have one table with and one table with . In the table you will have a foreign key which is a column containing a that points at the record for the owner of the thing. Relational databases are designed specifically to make this kind of structure easy and fast to work with. You shouldn't* ever need to worry about how many users there are or how many things they have. * NOTE: Your requirements are very sparse. These are rules of thumb. 

First off, don't worry about performance until you're sure you have a performance problem, especially for a tiny number of rows. Second, whether you use a unique index that is primary key or a unique index that isn't the primary key to enforce the uniqueness of the combination of your four columns the performance will be the same with respect to that index. Whether to make that unique index the primary key should depend on what else is going on in your schema. If you have another table (or tables) that reference this table, then your compound primary key is going to get propagated to other tables as a foreign key. If that is going to happen, they you have more to think about. If you don't need to worry about propagating a compound foreign key, then you should probably just make your compound unique index the primary key of your table. That would be the least effort which achieves your goal of enforcing your business rule. 

If you still have their attention by this point, you could try to show them the power of binary searching. For this you might be better finding a YouTube clip of the game show "Price is Right" and their "high/low game" which smart players play using binary searching. 

There's no single right answer for any performance question. The answer is always that query optimizers are very smart, to a point. So the most efficient design or query will depend on the number of records, what kind of indexes you have, how selective they are, and many, many more factors. If you want to avoid keeping both bounds of your ranges, you can search using TOP 1 and order by lower_bound but I wouldn't expect this to be efficient necessarily. I'd suggest profiling it this way to see if the performance is acceptable. Having an index on will probably help, depending on the number of records in your table. 

If it fits within the rules of normalization, then 1:1 relationships can be normalized (by definition!) - In other words, there is nothing about 1:1 relationships that make it impossible for them to obey the normal forms. To answer your question about the practicality of 1:1 relationships, there are times when this is a perfectly useful construct, such as when you have subtypes with distinct predicates (columns). The reasons you would use 1:1 relationships depend on your point of view. DBAs tend to think of everything as being a performance decision. Data modelers and programmers tend to think of these decisions as being design or model oriented. In fact, there is a lot of overlap between these points of view. It depends on what your perspectives and priorities are. Here are some examples of motivations for 1:1 relationships: 

This lets you keep the initial origin and the final destination in your trip table and it lets you add zero or more waypoints along the way without storing any redundant data (i.e. the schema is in third normal form). 

This lets you maintain the phone types master list in one place while enforcing the list of allowable phone types for each of the different types of legal entity that you support. 

If you can tell us more about how you plan to use the events, from the perspective of how the information is meant to be helpful, I might be able to give you more concrete advice.