"Phonotactics" should probably be divided into syllable structure (and contact), plus syllable-count, since you would get different results if a language was Hawaiian with only 2 and 3 syllable words, versus Hawaiian with 2-12 syllable words. Looking at "words" is also somewhat problematic, since words may have a range of inflections, and so-called phonotactic restrictions might actually be broken by inflectional endings (as is the case with English monoconsonantal inflections). It would probably be more productive to look at phoneme combinatorics of stems rather than words, though "stem" is not a totally trouble-free concept. This hasn't been done systematically (so it isn't a named sub-field), but it might yield interesting results. There certainly has been a history of making claims about gaps in the lexicon, which rise to the level of being a possibly publishable result in case someone discovers an interesting pattern. The main problem is locating comprehensive electronic dictionaries with interpretable phonetic data in a decent range of languages. The runner-up impediment is lack of knowledge of combinatoric possibilities in an interesting range of languages. Theoretically, you could do this right now, using the CMU English dictionary, as long as you can compute all of the possible 1-syllable, 2-syllable up to ??12-syllable words of English, and see how many and what type are actually attested in English. I don't know of any large-inventory (Taa) or small-inventory (Austronesian) languages for which there are comprehensive computer-friendly dictionaries. 

If you have decided that phonology is all about the mind and phonetics is all about the body, then all you need is a drawing of a person, dichotomized into the mind versus the body, and you can label the parts. Good luck with that. However, I disagree with your initial premise (though I do recognize that that is the standard undergrad edu sound bite). In fact, phonology is about mental manipulation of sound qua symbol, and phonetics is about how those symbols are realized / perceived. Before you get to actual neural impulses and movement of air and muscles, there is a lot of pre-physical planning. Some of this is quasi-scalar, so for example "voicing" during obstruent stops is not uniform across languages (and I mean even excluding situations like English where [g] is for some people voiceless unaspirated), voicing can be strong (higher amplitude, less prone to decay) vs. weak. The path of coarticulation from vowel to consonant is not uniform across languages (e.g. Marshallese vs. English vs. Turkish). The real question is, why would you want to explain this difference? The simplest solution is to say that it's very complicated and controversial, but here are a few approaches (and then explain some of them). Or, if there is a real point that motivates you (such as that phonology does not deal in continuous functions or numeric values and that is has rules / operations of a particular type), then define the difference that way. The graphic should be trivial and really unnecessary, once the concept is clear. 

I'm interpreting your question literally in my answer, as I think you intended it: "Which actually reconstructable proto-languages languages are believed to have been spoken around 7500 BCE?". PIE might have been spoken around that time (so Hittite was not spoken, nor Germanic) – this could actually push PIE back too far, see below for discussion of dating. Bantu was not spoken then – that's a later development. Austronesian doesn't date back that far either (it's put more at the 3500 BCE). AFAIK, there is no reconstructed proto-language from which Austronesian descended, where we can say "at 7500 BCE the language that was spoken is Proto-Austronic", so Austronesian drops out of the count. Austro-Asiatic is also thought to reconstruct to much later than that period, so scratch that family. You should note that I'm not mentioning "Austric" which could be an ancestor of Austro-Asiatic and Austronesian, because Austric isn't a reconstructed language (which might have a time depth), it's a conjecture that certain proto-languages might be related. For pre-Bantu, we would probably conclude that Niger-Congo was spoken around that time (whence Bantu and related languages). The difference between Bantu and Austronesian is that for Austronesian, we can only say "Something, which must have led to Austronesian, was spoken", but for Bantu we have to make a choice as to which specific level of reconstruction we are talking about. Likewise, Semitic probably reconstructs to 3500 BCE so that would not be a contender, but maybe Afro-Asiatic (but it may be that Afro-Asiatic actually is much older: yet we don't have a descendant branch in Afro-Asiatic that confidently reconstructs to 7500 BCE, in case Afro-Asiatic is too old). Because of lack of time-depth in reconstructions, that lets out much of the New World, though Dené–Yeniseian could qualify. (A caveat: Dené–Yeniseian is not actually a reconstructed language, it's a family some parts of which are kind of reconstructable). We can probably add Uralic to the list. However, given the imprecision in dating reconstructed languages, it might be that Uralic and Austronesian have comparable time-depths (so Uralic should not be included?). Pama-Nyungan might reconstruct to that time depth. Again, many languages will fall out of the count simply because there is no reconstructed language (viz. there are many language isolates) – Basque, Burushaski, Etruscan, Korean, Sumerian. I may have mentioned the entire set of proto-languages spoken at the target date, as long as you don't mean 7500 BCE plus or minus 4000. Clearly very many languages were spoken in 7500 BCE, but they mostly do not correspond to actually reconstructed languages. One cannot look at structural changes from a proto-language and compute a time-depth based on a theory that languages change at a constant rate. Instead, you really need to look at archaeological and word-appearance facts (where a particular word referring to a significant cultural item, such as plant names or technology, correlates with language subgrouping, and this can be pinned to archaeological events like the emergence of Urewe ware maybe around 500 BCE). It may be that Indo-European archaeology is in a more advanced or, simply, lucky state compared to Niger-Congo. In fact, looking for "7500 BCE" is probably not reasonable, instead you should look for a range spanning at least 3 millenia, definitely not centered around 7500 BCE, since that is at the far limits of reconstructive technology. 

I don't know what a frameworkless account of anything would be, but this paper is not meat grinder-theoretic and is intelligible on its own. 

Excluding emphatic readings like "it's NOT funny", there isn't anything resembling an accent on "not". Because of the vowel quality, the vowel of "not" may seem more prominent than the one in "it's", but change the first word to "that's" and I don't see any difference at all. EDIT: I've observed that judgments on secondary stress are often messed up when vowel qualities differ (the greater duration of [ɑ] in "it's not funny" may be confused with secondary stress). In "It's funny", the duration difference in the syllables is greatest so judgments that "It's" is unstressed are strong. In "It's not funny", "not" is longer and it's shorter than "It's", which can encourage one to think that "not" is somewhat stressed. By lengthening the first syllable (changing "it's" to "that's"), you reduce the duration difference of the first two syllables, which should convince you that there's no stress on "not" and that any feeling that there is, is a consequence of segmental side-effects. 

The notion "degree of X" really requires a three-way distinction to be valid, as in degrees of length (Estonian, Saami, Dinka), nasalization (Palantla Chinantec) or breathiness (Bor Dinka). If there are only two systematic values, we say "it is" or "it isn't", and we don't have to say "how much" (degree). In the present case, it is likely that you are using one language (Bengali) to set a standard of comparison for another language. Korean has been subject to a half century of acoustic analysis, and it is clear that there has been recent language change where /p/ et al. in Seoul Korean have become "more aspirated", over time and compared to other dialects. It is now to the point that the unaspirated stops of Seoul Korean are comparable in voice onset time to the contrastively aspirated stops of languages like Apache and Khonoma Angami (Cho & Ladefoged). Some languages have extreme degrees of aspiration, for example Navaho [k] at 45 msc vs [kʰ] at 154 msc; 28 and 128 msc respectively for the same pair in Tlingit. From that perspective, p in Korean is "somewhat aspirated" and ph is "more aspirated, compared to Bengali or other Indic languages where the phonologically unaspirated stops don't have a very long VOT at all. What makes Korean so special is that it has a third kind of consonant, transcribed typically as pp, which is termed 'fortis; tense; glottal; geminate' – all of these seem to be appropriate descriptions of the facts. These 'tense' consonants have an even lower VOT, which is the element missing for claiming that there are "degrees of aspiration:. However, the 'tense' consonants also have some kind of glottalization associated with them, at least for older speakers. It is possible that some time in the future, the p, pp, ph contrast will develop into a three-way difference in aspiration alone, but at present, there are enough other differences associated with these consonants that we can legitimately treat the system as having two orthogonal two-way differences (aspiration versus glottalization). In one way of looking at it, English could provide another example of three-way aspiration differences (though not contrasts). It is well-known that aspiration of voiceless stops is governed by a rule where they are aspirated in foot-initial position and not aspirated elsewhere. This is often treated as a phonological (categorial) allophonic process. However, it has also been observed that for many speakers, the contrast between "g" and "k" does not involve voicing, it involves aspiration ("g" is really unaspirated [k] and "k" is aspirated [kʰ]). Putting these two facts together, one can say that phonetically we have three aspirations states: unaspirated ("g"), lightly aspirated ("k" not in foot-initial position), and more-aspirated ("k" foot-initially as in "cap") There are plenty of ways to reduce the significance of these aspiration differences. One can maintain that the "k" / "g" contrast is indeed one of voicing and the failure of vocal fold vibration in phonetic outputs is the result of something about phonetic implementation; or one can say that that contrast is indeed phonologically one of aspiration, but phonetic implementation amplifies the degree of aspiration of foot-initial aspirated stops. The situation with English is different from that of Korean in that English only has two kinds of underlying consonants as opposed to three in Korean, and "what is underlying" is one definition of "phonemic". But the other definition of "phonemic" refers only to phonetic outputs and the ability to reduce two phonetic categories to one by rule, referring only to available phonetic properties and not abstract phonological ones. Because of resyllabifications (an abstract phonological process), you can actually get all three stop types in comparable contexts: "a Benny" (unaspirated), "up any" (lightly aspirated), "a penny" (most aspirated). This is typically dealt with by referring to juncture / word boundary, but junctures or boundaries are not phonetic events, they are abstract phonological units. Since I don't subscribe to the strict surface-oriented view of the concept "phoneme", I don't know whether advocates of that theory would then hold that English also has "degrees of aspiration". This handout and paper by Vaux gives an overview of relevant facts and theoretical interpretations of the matter. 

Merge does not directly produce surface utterances. Fixing linear order can be treated as phonological (a terrible misinterpretation of phonology in my opinion), and there is also Move. 

Collection of data is one area of linguistics, though there is currently a nomenclature problem that we don't know what to call it. I'm referring to field-working, where one gathers data from speakers of an un(der)-described language and presents it to the world. The first step in dealing with "curious" features of language is knowing and showing that they exist in a specific language. Finding such rarities depends in part on good luck (there's nothing that you can do about it if the language is totally ordinary), and in part on the analyst's recognition that a particular fact of the language is unique in the world's languages. Because finding such nuggets is a low-frequency event, it (rarity-finding) is not generally a practical specialization. Typology seeks to determine the range of possibilities in languages, and in so doing cares about what things are common versus uncommon (for some practitioners, it also includes the development of a set of "types", and may seek functional explanations for observed correlations and frequencies). Typology depends on the research product of field workers, indeed many typologists are also field-workers. However, typology's interest is not limited to just those things that you could call "unusual". The organization of knowledge of features across languages is what we could call an "emerging" discipline. One main impediment to systematically studying unusual features (i.e. features with surprisingly low occurrence in human language) is the non-systematic form of the available data – there is no structured database of all of the facts known by field-workers. I cannot call up a complete or even particularly good list of languages that have inflectionally-conditioned tone changes on verbs, so if I encounter a language that distinguishes two verb tenses solely by tone pattern, there's no central repository where I can see if that is common or rare. Instead, I would have to rely on my prior experience, or email a colleague (who may also not know). Accordingly, I do not know if there is any language where geminate consonants block vowel harmony, other than the one that I know of. This area is "emerging" in that more and more data is becoming available online (thus potentially part of a systematic study), and methods are developing for encoding analysis. I am skeptical that anything short of a sentient data-crunching program would answer the question of geminate-blockage, unless it goes on a checklist of questions to ask (similar to the Lingua descriptive questionnaire). Most subdisciplines of generative grammar have an implicit typological component, given the premise of that theory that there are limits to how languages can differ, thus you have to know what is possible versus impossible, and in particular you would need to be able to distinguish "impossible" from "rare". In principle, syntax, phonology, morphology, semantics and phonetics could be concerned with rarities (in a certain component of grammar), as a tool to reaching their ultimate research goals. 

I would not say that they have to precede a noun, since that implies that *"The should give you money", because "the" precedes a noun. Articles have to be followed by something in the NP, and what immediately follows could be all sorts of things ("the old cow", "the three dogs", "the over the hill gang"). Articles are not strictly only found before another word in the NP, though the contexts where they aren't is quite restricted. You can say "I said 'You can have these cookies, not the. Leave those ones alone". I'm sure some people will feel that these are unnatural, and they are clearly uncommon, but they are possible. In contrast, if you want to contrast bound-morpheme selection, you can't say "I didn't say [s]", or anything like it, meaning "I didn't say cats (I said cat)". Under the assumption that a bound morpheme is definitionally "a morpheme that is always a subpart of a word", then setting aside isolation meta-linguistic uses of articles, they still are not part of the following word – unless you definitionally declare that they are. It is plausible to say that they are clitics, and there is a theory of clitics that they are syntactically separate words which are morphologically or phonologically part of some other word. See work by Phillip Miller from the 90s on the notion of "edge inflection", and his treatment of genitive 's in English. So one theory would simply be that articles have a specific left edge location in the NP so that they can't be preceded by anything (at the NP level), and the rest of the NP has to contain something. Or, articles could be the realization of an abstract feature that is assigned to the left edge of the NP and is realized as a prefix morpheme on whatever is "really" the leftmost word in the NP. Under the latter construal, you could say that articles are bound morphemes. IMO it depends on your theory of clitics. Also, one never really knows how many NPs an NP contains. There are quantifying expressions that come before articles, like "some of the people", "three of the cars", "all of the dog" and the latter can be reduced to "all the dogs". How you dispose of such cases is rather theory-dependent. They could be NP inside NP, or they could be special things that come at the left of the nounish-phrase. 

You almost certainly do not want to compute the formants yourself (complex programming, way too long to describe here). If you can see a waveform in Praat, having selected a sound object and you clicked View & Edit, there should be 1 or two windows with waveform on the top, and another window at the bottow, which should display a spectrogram (if, clicking "Spectrum", you see a check on "Show Spectrogram"). In that same window, it will display pitch, amplitude and formants (look under "Formant"). If you want something besides a picture (i.e. numbers), if you select a part that you want analyzed, under Formant, you can select "formant listing" which will give you a series of formant calculations.