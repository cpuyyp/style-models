There is also the newer conlang Ithkuil. As with Lojban it has the problem that it forces the human to change behavior by being much more precise and specific than normal. No ambiguity comes at a cost to the speaker, being vague has its own rewards. It might be that learning languages like Lojban and Ithkuil are easier for programmers since they are already used to an absurd level of precision when coding. Because of this I don't see how using a programming language that was also a human language would gain us anything: No matter the syntax of the programming language, in order for the computer to be able to do something predictable with it, the human coder would still have to be a pedantic perfectionist crossing all t's and dotting all i's. Humans do fuzzy matching and inference very well, computers not so much. 

I must agree with Steven Xu, "primary difference" is the wrong question to ask. In addition to Steven's list, dialects also differ in: 

For a subset of words there's World Loanword Database and The Intercontinental Dictionary Series based on the book "A Dictionary of Selected Synonyms in the Principal Indo-European Languages", Carl Darling Buck, 1949. 

It's not science. It wastes manpower, time and energy that should have been used to rescue an already existing rare natural language or do field work. 

I graduated as a computational linguist. The cognitive sciences really weren't alluded to much at all at my uni. so can't help with the compare and contrast. So, what a computational linguist does: Not being exhaustive here... computational linguists work with corpora, work to make corpora, design tools for corpora, like POS-taggers, word sense disambiguation; and design natural language parsers, work towards machine translation and text categorization eg. "is this spam or not", and I'd say the first word net sorts under CL. I have a feeling computational linguistics (CL) might be to linguistics what AI is to computer science, developing methods, which when they are commonly in use, are no longer considered to be part of CL/AI. Which is okay, since CL/AI is already working on the next big thing. So, basically whatever is bleeding edge language data crunching at the moment. I'm assuming the reason your advisor wants you to do some CL is because you'll need to know the tools, corpora and methods. Then there's getting a better understanding of what's easy and not easy for a computer vs. what's easy and not easy for humans, which should be relevant to cog. ling. indeed, and finally, it's an obvious suggestion since CL is basically going all CS on poor, unsuspecting language data: it's fun! :) 

Please see the comment I left on the question about intelligence vs. IQ. I am assuming the OP meant "intelligence" and was simply referring to IQ as a convenient measure. Language alone is not an indicator of intelligence. Most introductory Linguistics courses will at least briefly cover Williams Syndrome, a form of developmental delay that characteristically presents with exceptional language skills. While it might not make sense that something that causes exceptional language skills could be classified as a developmental delay, you need to remember that when people talk about "intelligence", they are often talking about a wide range of independent skills (math, induction, memory, expertise on a specific topic, etc.). However, you did not ask about the link between language and intelligence, you asked if one's native language can affect their intelligence. I couldn't find any research on that topic but I did find this paper which found that bilingualism has no noticeable effect on intelligence. Another problem is that it would be almost impossible to attribute any differences to language alone. There are too many variables between different language speaking areas such as culture, education, and health care. I have found this paper which found evidence that quality of education can noticeably affect intelligence test results. In summary, although I could not find any academic sources explicitly addressing your concern, I find it highly unlikely that one's native language affect intelligence levels and any differences that may exist cannot be reliably attributed to language as other factors have greater effect. 

If personal anecdotes are ok, and "thinking in a language" includes internal conversations like "what do I want for dinner today" (introspective people have a never-ending conversation/discussion going in their heads), then I would say I am capable of thinking in both my mother tongue and English, and that I almost manage to think in German when I've been there long enough. What language I think in depends om my surroundings or what I have been reading recently. Since I'm writing this answer in English, I am currently thinking in English. I dream in both my mother tongue and English and sometimes in halting German. I often can't remember the language of some article recently read, I only remember the content. I would go so far as saying that you aren't really fluent in a language until you can think in it, mentally switch to it completely. If you need to translate in your head you are not fluent. For me, this can make translation quite hard as I know what a word means in one language but lack that word in a different language and so have to resort to circumlocutions and a lot of hemming and hawing and halting explanations :) 

Less intelligible to whom? I think what you are trying to mean with "accent" here is the form of "accent" that can mean either of three things: 1) a form of speech that differs from some standard, prescriptively more correct form of speech of the same language 2) a form of speech that differs from your own, but is still the same language 3) a foreign accent, where the speaker has learned the language as an adult and cannot pass for a native. Let's ignore 3). The first generally only holds for modern countries with a strong capital city, a high degree of centralization and/or mass media. Preferably all three. The second definition first occurred the moment the very first group of speech-using humans became two groups of speech-using humans, + about a generation of passed time. Accent sorts under sociolinguistics for a reason. The intelligibility is, among other things, dependent on the status difference between speaker and listener - if, say, the listener considers him/herself to be of higher status, there is less of a penalty if the listener cannot understand the speaker. A low status person on the other hand might depend on being able to understand a high status speaker. Rural people are often considered lower status by people living in cities, ergo, rural people are unintelligible. Power games. Moving away from the sociolinguistic side, read up on dialects as to why area-dependent accents differ. (There are also solely status-based accents, aka. sociolects, for instance those acquired through being stuck in a boarding school.) Dialects are accents turned up to 11, and is why one language can split into two or more languages given enough time. 

Synonymy means two words have the same meaning. An example would be words that are in the same WordNet synset. It should be noted that WordNet is word-sense disambiguated, meaning words with multiple meanings are represented as completely separate entities. Similarity measures the extent to which two words share meaning. For example, path similarity, Wu-Palmer similarity, and Leacock-Chodorow similarity all compute the similarity between WordNet synsets using the paths between them in WordNet's is-a heirarchy. If two words are in the same synset (i.e. they are synonyms), their similarity is generally 1. You can play around with these similarity scorers using NLTK's WordNet implementation. Another common type of similarity is word embedding (Word2Vec, GloVe, etc.) similarity. This is generally computed as the cosine similarity between each word's vectors. In generally, word embeddings follow the distributional hypothesis that similar words occur in similar contexts. This means what you're actually measuring is subsitutability. For example, any sentence where you use the word "dog" (as in the animal), you can substitute it with the word "puppy". Thus "dog" and "puppy" are similar. Interestingly, this means that word pairs like "white" and "black" are also very similar according to word embeddings, even though humans generally consider them opposites. I personally consider relatedness and association to be the same thing. Relatedness encompasses similarity but is much broader. Similarity measures the degree to which two words are the same, e.g. "dog" and "canine" describe the same concept. However, "leash" and "dog" are very different. One is an animal while the other is a long, thin, strong piece of cord or fabric. However, "leash" and "dog" are very related because dogs often are walked on leashes. Examples of related measures are less common but still exist. Good places to start are Extended Lesk and Gloss Vectors. Both are implemented as part of the WordNet::Similarity Perl library. If I may self-promote, I'm actually developing a semantic relatedness measure based on word associations and I've seen some encouraging results on a humour recognition task. I've also developed an association strength prediction model. 

The differences might be visible in a spectrogram. You could do the experiment yourself: record yourself saying /sasas/ /zazaz/ both voiced and whispered and make spectrograms with for instance Praat, and compare. You'll need to increase the size of the y-axis though, since you're comparing fricatives. If you're comparing other consonants the standard setup should be fine. If you're not comfortable with reading spectrograms you could add the images to your question and let us have a look at it. A little quick experimentation of my own makes me think the whispered voiced fricatives are more tense than the unvoiced ones: less space between the articulators, and less air pressure. An interesting follow-up question to this though: is whispering language dependent? That is: is a whispered /z/ the same sound acoustically speaking in, say, French and English? 

If you memorize them while learning to pronounce them you're aided by muscle-memory. They are defined after how they are pronounced after all. The vowels are hard to explain but a tutor can show how to do them. That's how they've traditionally been taught, from mentor to student. Take [ɑ]: relax the lips, open the mouth just about as far as it goes, try to press the tongue down, do not let the jaw move forwards, vocalize. A mentor can tell you when your mouth is in the right position. Once you've hit the right spot enough times, you've reprogrammed your hardware, and might be moving the mouth into position even when reading or thinking about IPA, which is fine :) 

I read a paper called Loanword adaptation as first-language phological perception by Boersma and Hamann for a Phonology class project last semester. That introduced me to both the idea of a productive perception and the idea of a learning algorithm for OT. I think these two ideas (along with Optimality Theory) may help explain the phenomenon you are investigating. The assumption in a lot of literature is that perception is completely faithful, i.e. what people perceive is a 1:1 relation of the incoming soundwave. Productive perception says that this isn't true and that people's perceptions are based on their ingrained phonology. The Boersma/Hamann paper I linked earlier uses this idea to say that people intuitively transpose any foreign words into their own phonology during perception, explaining why loanwords are often pronounced differently from their source form (and indeed, from other native words with similar forms, thus either ruling out a production-only based explanation or requiring loanword-specific rules and constraints). This paper cites a 1997 paper by Boersma called How We Learn Variation, Optionality, and Probability. I have not read this paper in full, so my understanding mostly comes from the summary included in the Boersma/Hamann paper I included above. Basically, Boersma claims that the constraints (normally used for mapping SR to phonetic form) are bidirectional and that they can be applied during perception as well as production. These constraints are then ranked by probability. For example, consider a cue constraint . During production this constraint would be understood as "don't produce the phonologic form /e/ as the phonetic form [ɛ]"; during perception as "don't perceive [ɛ] as /e/" (or generically, "[ɛ] is not /e/"). In English representing /e/ as [ɛ] (as opposed to the more faithful [e]) usually does not result in any loss of meaning and thus can be freely violated. These frequent violations would then lead to become lowly ranked. Contrast this with French where representing /e/ as [ɛ] does usually lead to a loss of meaning and thus such a constraint would be more heavily used and therefore become higher ranked. Now to tie it in with your question. Presumably one hears their name very often. So a learning algorithm like the one Boersma proposes would rank the constraints in such a way that one can readily distinguish their name during perception, even if the incoming phonetic form is damaged or has heavy interference (such as in a noisy room). It would also explain why people sometimes hallucinate hearing their name (or other common words) and rarely hallucinate uncommon or arcane words. Obviously, this is just one possibility but I hope I at least provided some interesting links. As for what it's called, perhaps selective hearing? 

See the figure. Line 1 is stress timed (secondary stress ignored) and line 2 is syllable timed. IIRC, the time between each dot is the same. 

There are still rules though. In the second parataxis-example (2a), the latter sentence is generally assumed to happen simultaneously or after the first sentence. What happens first goes first, and a cause would be mentioned before the effect etc. The Wikipedia-page on parataxis isn't very useful as it concentrates too much on parataxis in English for poetic effect, but now you know one more term to look for in reference grammars at least! 

Prepositions and subordinators (aka. subjunctions) are readily borrowed into other languages provided that they lack them, and such borrowing sometimes also pulls in the necessary syntactic structures. Several Southern American languages have borrowed subordinators from Spanish, and the idea of an explicit subordinating marker with them. Hm, I read this recently, from Cristofaro (2003) "Subordination" possibly? 

The book to go to is "Phonetic Symbol Guide" by G. K. Pullum and W. A. Ladusaw. Page xxii is a summary of the principles the IPA (association) uses to select the symbols for the IPA (alphabet) and if you wonder about a specific symbol you can look it up in the same book. Most of the symbols have their own separate histories, for instance one of the principles is that letters taken from the latin alphabet is to stand for the most common (as in number of languages, not number of speakers) sound for that letter. However the {r} is used for the alveolar trill, which isn't really that common. 

I recently took a phonology course where Mora Theory was breifly mentioned but we didn't go too indepth so your question piqued my interest. I did some light googling and I found this set of lecture notes which I think does a pretty good job explaining it: 

To me (native Canadian-English speaker), these three sentences show different levels of confidence in the opinion "it's a bad idea". Without any "think" construct, (1) implies absolute certainty. The stative construct (2) implies a high level of certainty but leaves room for doubt (for illustration, let's say 80-95% confidence). Finally, the progressive construct (3) allows room for much more doubt (maybe 55-70% confidence). This is reflected below. 

While I have been unable to find any excerpts from the book in question, "A Fundamental Algorithm for Dependency Parsing" by M. Covington references "Computational Complexity And Natural Language" by Barton, Berwick, and Ristad, saying: 

Korean was a tonal langauge until the 16th Century. In fact, even today the Gyeongsang dialect still uses tones. From my ancedotal experience, remanents of tone are still visible in the "standard" Seoul dialect as well (mostly related to length of articulation), but they aren't ubiquitous among speakers and are not taught to Korean Second Langauge learners. 

The only way I can think of to treat an adjective as a noun is using (literally a noun modifier + thing ). Verbs also follow the same pattern: 

I think you might be interested in the paper "Question Identification on Twitter" by Li, Si, Lyu, King, and Chan. This should give you a background on question identification in general as well as an overview of the machine learning techniques typically used for these types of problems. I suspect with this knowledge you should be able to come up with a modified approach that does what you need. My one caveat is that this type of approach relies on hand-annotated data which is expensive to produce both in terms of time and paying annotators. Unfortunately, the authors of this paper are highly unlikely to share their data with you as the Twitter API license agreement forbids the resyndication of Twitter content. As I said, most modern NLP techniques require large inputs of annotated data and it may not be feasible for you to collect this data yourself.