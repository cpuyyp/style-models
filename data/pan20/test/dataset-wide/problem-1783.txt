Sometimes the other uptime command display a time that is not correct so I use this command to display the 

I use terminal-server Windows Server 2012 when a user inside the terminal session deletes a file on a shared drive (R: here) and gets the error bellow: 

I have started with array, but the old HDDs failed one by one and when the spare parts came they where bigger disk inside and I recently ended up with 3 HDDs of . 

you are not vulnerable. The other part of ShellShock check is the CVE-2014-7169 vulnerability check ensures that the system is protected from the file creation issue. To test if your version of Bash is vulnerable to CVE-2014-7169, run the following command: 

Amazon's marketing materials claim that the m4.16xlarge node has 64 vCPU. When I look at on the system, however, I get the following information: 

What are the network protocols that can be used to measure the system clock (time) of a remote server? So far I have: 

RHEL 7 includes in the official repository. However, this module does not install mod_php. I have tried all of the approaches for installing rh-php70-* and none of them installs and registers the appropriate php module. How does one do it? 

We would like to have a network backup system with a user that can read any file on our servers but not write any file. Is there any way to do this under Linux (and specifically Fedora)? We would rather not have a remote that can erase any file... 

ShellShock is practically a conjunction of more than one vulnerabilities of bash, and at this moment there is also malaware that exploits this vulnerability, so ShellShock can be an issue that is still open, there is a thread with updates from RedHat about this issues. Redhat recommeds the following: Run command: 

Is this general practice and PTR records can be added only by the ISP companies that manage the IPs, or is just the practice for this DNS provider? 

If there is not a gateway on the traced path, only Layer 2 devices, the IP will not be reported. But Cisco has a utility that works on Layer 2. But this utility is dependent on CDP protocol which : 

An Amazon Machine Image contains an EBS volume. Is there a way to get the EBS volume out of the AMI without booting the AMI? 

So this makes no sense to me. This means that I have 64 CPUs with 16 cores each, or a total of 1024 cores. However, online documentation for the Intel E5-2686 v6 claims that it has 36 cores and hyperthreading, for 72 virtual cores. What's going on? How many cores are there? 

It appears that Hive, Impala, Pig, and others all provide SQL or SQL-like access to data stored on Hadoop clusters. They all seem to have support for HDFS, S3, and other forms. So why are there so many different ways for accessing Hadoop information by SQL, how are they different, and how does their performance compare? Do we have so many different versions because all of the projects were started at the same time for more or less the same reason? If so, is there an advantage to knowing more than one of them? I have found several articles that attempt to explain the differences (e.g. 10 ways to query hadoop with SQL and Selecting the right SQL on Hadoop, but mostly they just list features. 

I am using SyncBack to mirror two shared folders on two separate shared folders(SMB) on two servers in the same trust domain. I checked the option to copy also the folder rights: 

Then a series of multiple command are passed using switch to strips all the trailin leaves only what is between and replace each with the rest two command add a few spaces and a The result is something like: 

Directories need x bit set (for directory that bit is seen as search bit) to open. So I use tree so I can get only the folder set and avoid the nightmare of having all the files set as executables ( the option for tree is ): 

I have a Centos VM running with SELinux enabled. I wish to have sshd listen to another port --- says, 993. I've modified the sshd_config file to listen to another port, but SELinux is getting in the way. I don't want to disable SELinux. How do I tell SELinux that it's okay for sshd to be reading TCP connections on port 993? The correct command to use is but I cannot use that command because port 993 is already in use in another policy: 

RAID5 and RAID6 can detect and usually correct bit corruption if you verify parity of the entire drive. This is called "scrubbing" or "parity checking" and typically takes 24-48 hours on most production RAID system. During that time performance may be significantly degraded. (Some systems allow the operator to prioritize scrubbing over read/write access or below it.) RAID6 has a higher chance of correcting it, because it can correct it if you have two drive failures, whereas RAID5 can only handle 1 drive failure, and drive failures are more likely when you are scrubbing because of the increased activity. 

I am using terminal-service server with Windows Server 2012 and we have two terminal servers share trough NLB, users connect to a terminal in the domain with Remote Desktop Connection (see picture below). I need to map a network drive for the users in terminal to a share on another domain ( which has the domain controller Windows Server 2003). I try creating a GPO in through (domain controller for with Windows Server 2012) to map the network drive. and are physically connected in the same network. How to map a network drive to a share in the other domain () on terminals in the main domain ()? 

You can also make a shell script see details here. Explanation for the above command: list all directories, sub-directories, Explanation list all the file and directories recursively ex: 

We are using Lustre in a cluster with approximately 200TB of storage, 12 Object Storage Targets (that connect to a DDN storage system using QDR Infiniband), and roughly 160 quad and 8-core compute notes. Most of the users of this system have no problems at all, but my tasks are I/O intensive. When I run an array job that has 250-500 processes that are simultaneously pounding the file system typically between 10 and 20 of my processes will fail. The log files indicate that the load on the OSTs are going over 2 and that the Lustre client is returning either bad data or failed function calls. Currently the only way we have of resolving my problem is to run fewer simultaneous jobs. This is unsatisfactory, because there is no way to know in advance if my workload will be CPU-heavy or I/O heavy. Besides, just turning down the load isn't the way to run a supercomptuer: we would like it to run slower when running under load, not produce incorrect answers. I'd like to know how to configure Lustre so that clients block when the load on the OSTs goes too high, rather than having the clients get bad data. How do I configure Lustre to make the clients block? 

Is there a way for system administrators to better track and debug what happens with files stuck in the print spooler ? 

I am not getting an IP in the subnet where the DHCP is working and should serve an IP, I am getting instead only an IP in the subnet (link local address) that is probably given by Windows which runs on the laptop I am using to test this, which makes me think it might be a problem of the network firewall blocking the DHCP Discovery service. And if that is caused by the firewall, which ports I should be opening for DHCP Discovery to work? 

I have a print server running Windows Server 2012 which is shared in a domain and is used by terminal services clients. The domain network spans between countries using tunnels and in some situations the print server and the printers are not in the same physical network and in this situations when the network is busy big documents fail to print. If I check the Print Queue Window I don't get very much data on what happens the only available fields displayed are Sometimes Size is empty and Status doesn't give too much information. The only solution remaining is to select from that same Print Queue window and when some print job is stuck that usually doesn't work.