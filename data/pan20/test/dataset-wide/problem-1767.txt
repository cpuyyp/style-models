Here's a couple of links explaining what you'll want to be doing. Take a look through the rest of the documentation on the official site if you have any more issues. 10 minute tutorial. Part 5 shows what you need to be doing. Parsing old logfiles with correct timestamps. Might be useful as you get into the swing of things. 

Test-NetConnection will always return "False" for a localhost port. Use another tool (like canyouseeme.org) if you're checking to see if a port is open from a remote address. Your Powershell looks fine, and you should see the rule if you look in the Windows Firewall rules GUI, but you can't test it the way you are attempting locally. 

Rather than beat yourself up trying to troubleshoot the sometimes tempermental web platform installer, you can always just install directly from the Microsoft provided ISO instead: here. Also, you might check that you're on Windows 7 SP1, as that's listed as the minimum requirement for this version, NOT standard Windows 7. 

There is DEFINITELY going to be some consequences of doing this. The primary one is going to be IO read/writes. Beyond that, it's just a very scary way of dealing with that type of data(at that scale). 

Why not run that same command again with the proper permissions then apply your desired permissions to sites/default? 

I have approximately 30 users on a box. Those users are in overlapping groups(about 6-10 groups). I need them to be able to land in a specific folder based on their group assignment when they FTP in. I.e., group1 -> /tmp/site1 group2 -> /tmp/site2 Is this at all possible with VSFTP on a SuSE box? Using SFTP isn't an option unfortunately. Thanks! EDIT: And in the event of a user being in several groups, just dumping them to the highest-level folder necessary to view the various folders they have access to. 

We have a bunch of existing servers in EC2. Future servers are created with Cloudformation with Cloudwatch integration. However, I need to setup Cloudwatch for servers that weren't created with Cloudformation. I have been asked to create a Cloudwatch Cloudformation stack. Is it possible to just create alarms in Cloudformation? If so, how do I specify which servers to monitor? Thanks! 

I'm running a basic Powershell script on a remote server, and while it's running, it ignores keyboard input completely. This is intended. When, however, you click anywhere in the Powershell window, it stops input until you clear the pause with a keypress. Is there some way I can force Powershell to ignore mouse input altogether while the script is running? I want this to be as difficult to close as possible, ie. you need to actually click X to close the window, or kill the process, or the like. Running in a Command Prompt window instead is acceptable, but I'd rather run it in the Powershell GUI instead if possible. 

I'm trying to find out how to measure the total bytes written (or a percentage of maximum expected, either is fine) for a few RAID arrays behind LSI controllers. The controllers are all LSI MegaRAID SAS 9271-8i controllers. I've tried using MegaRAID Storage Manager and MegaCLI, but neither seems to show the information that I need. I've found a couple solutions online, but they only seem to be for Linux, where you can patch the kernel or use smartctl in unconventional ways. That won't work for me on Windows. I'd really like to avoid pulling the drives out, putting them in another machine, testing with SMART, and then putting them back. Would be a real pain in the neck. If it's important, each controller has two virtual drive groups of 4 disks each, in RAID10, with SAS SSDs forming the groups. 

Check out webmin It should allow you to do a lot of this. There are some great guides on configuring it. 

I wouldn't recommend it. There ARE still methods of cracking these quite easily. I personally recommend a Truecrypt volume that contains a Keepass database. It servers me well and is extremely portable. And I'm using it in an environment with thousands of passwords. EDIT: And Keepass is already well laid out for password management. With a nice GUI(i.e., easy to see what password is which type) and built-in password generators...can't go wrong. 

You didn't specify your ssh key by the looks of it so it's assuming you want to use a password. Most AMIs are configured to NOT allow ssh access for root with a password. Fix your parameters to ensure the key is being specified. 

We're presently using googledocs(word processing, spreadsheet and email) for everything. We'd like to move away from remotely hosted software. We have a large internal infrastructure to support this sort of thing so technical resources won't be an issue. Are there any decent alternatives to googledocs that meet the following requirements? Open-source Local Hosted option Collaboration/multi-user support Word processing Spreadsheet support Privacy features Importing/Exporting Minimal installation footprint I've been leaning towards a collaboration suite, however, I was also wondering if there is some way to collaborate within Openoffice? Thanks! 

And so on. This is annoying, as our code needs to be maintained across multiple folders, and people often forget to do so; that's an issue that can be fixed, but I'd rather solve it in the solution rather than the process. The code in all the folders is identical, only the web.config files are different. The reason we have our sites split this way despite the identical .NET code is that we need different connection strings to different servers. This way, whichever site hosts the binding gets the traffic. This is how we handle load balancing, which is not the prettiest way, but it gets the job done. I'd rather MULTIPLE sites (with one or multiple app pools, doesn't matter) which work the same way, but function with ONE shared code folder. What I need is a solution to handle the connection strings based on the site in IIS. I didn't have much luck looking into virtual directories, which seemed straightforward but I don't think is possible, as it needs a web.config to even know WHERE to look for virtual directories. I'm sure I'm missing something simple here, but I need a little help to get to this: 

I just spun up two EC2 instances and got an elastic IP for each. I can't seem to get it to connect. Here is the bulk of my config: 

The cluster gives you a little more power and some redundancy(if it's feasible for your software). The big 'workstation' has the advantage of being simpler to deploy and won't be bottlenecked by your switch. I can't say for certain if the switch will bottleneck since it will depend on your transfer sizes, etc. 

I've seen this done before. It's only as insecure as your network/destination servers make it. Only you know that. Are you transmitting these over a secure network? If so, you SHOULD be fine. But we can't possibly guarantee that. Why not write a simple ssh script to distribute them? That's what I would recommend. Or write a script to download the cert from a central server and distribute the script via puppet. Just an idea. EDIT: Since there is some confusion. I'm NOT saying Puppet/SSH are anymore secure. But if you're worried about unauthorized access, ensure everything is secure. This is most easily done with a custom SSH script YOU distribute. 

Time+ represents CPU time, or more specifically, "Cumulative CPU time which the process and children of the process have used". 

Depends on what you're comfortable doing; when starting to "professionalize" our office network for a similarly sized business, I set up a pfSense Firewall behind our modem, and assigned it routing tasks for the office. All you need is a machine to dedicate with a few NICs and you're on your way. It's pretty well documented online, and I haven't had an issue finding support when I've needed it. Once you have that installed, you can use the OpenVPN package to easily route the traffic you want to your network from anywhere in the world. The only steps besides setting this up on pfSense would be allowing traffic through your ISP's modem, which shouldn't be too difficult either. I'd say give it a shot! It's been rock-solid for us, and we run it in production now for our datacenter rack as well. It also never hurts to have a little more experience under your belt, and you could look to it as a cheap learning experience. The added benefit is when you want to do this, you can set up a VPN tunnel to your hosting provider, and then you'll have full access (on the terms you decide through firewall rules, etc) to your infrastructure more easily in the office. 

For the hosting side, I'd recommend Linode or a micro EC2 instance(free tier). Wordpress can handle multiple sites with some plugins. That's probably the easiest one to setup. It's not ideal for static content, but it'll do the trick. 

This is very doable and a great idea if you have the server hardware but not the routing stuff and don't want to make the full switch to AWS. Check out this guide. Just tweak some of it to your needs/what you already have. 

I've setup IPSEC tunnels between 3 management VPCs in 3 distinct AWS regions. Each of those regions has additional VPCs (dev/prod) that are peered to the management VPCs. It's setup in a hub/spoke like this: 

I'm assuming you mean block access for incoming users and not for YOUR users connecting to the site(external)? If the former: Look into apache mod_rewrite. That'll do what you want. If you're not using apache, you may need to consult with someone else. If the latter: Look into setting up a proxy server. It'll depend on what OS you're using. 

The flag will overwrite files. It is 'include same files', and should accomplish your task. I would also use to set the retry wait time to 1 second, rather than the default 30. Your delay might be from locking on your target side; have you checked that out? I don't know of a way to copy without checking, but that should get you where you need to be. Of course, you could also use another line of robocopy to just delete all the files from the target directory BEFORE running the mirror. That would certainly work as well. 

Running on LSi 9260-16i, with 16 SAS drives (Seagate Savvio 15k.2). One drive is getting media read errors, so we're going to RMA it out. Which of these is the least performance destroying option in the meantime until the new drive gets here in a day or two? 

We've been using an EC2 micro instance for a little remote monitoring for the past few weeks, and it's bothered me badly how the system time is never accurate; no matter how much fighting I did with Windows time syncing, it was never working properly. Didn't find an answer on serverfault that explained how to fix it, but found a good one finally on Amazon's forums, and wanted to share it here for anyone else looking. Scenario: Windows 2003 Server on EC2 Micro Situation: Time always wants to sync to UTC, no matter what is configured in Windows. 

You've really over complicated your configs. First of all. Look into $URL$ Then, trim down those files to the bare minimum. Make sure to activate the sites and ensure the directories exist. That should do the trick. Also, don't forget to reload apache after all of that. 

I'd like to simulate 1000 concurrent downloads of a single file from Cloudfront. I figured I'd setup ~10-20 xlarge EC2 instances for this. Is there an obvious way I'm missing to trigger this at the same time and get the average download time while ensuring the instances aren't the bottle-neck. We REALLY need to know how much outbound bandwidth we can sustain from Cloudfront. Thanks! 

I'd 'personally' recommend Debian i386. Its relatively simple to setup and has a great track record. Ubuntu is nice, but not ideal if you don't know how to tweak out some of the cruff that comes pre-installed. Once again, this is just my personal opinion. Edit: If you're willing to spend any money, you may want to check out a Micro EC2 instance from Amazon(aws.amazon.com) or a 512 Linode from linode.com. I've used both for dev work and they're both cheap options.