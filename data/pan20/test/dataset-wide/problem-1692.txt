MyUSBonly from AC Element Company and StopUSB from EverStrike will also work. Both of them ask for a password if you want to mount a new, unknown USB device. To be honest, both could be better designed, but at least they do the job. 

Aha! Found it. Searched for "msrvc" in the registry, and it pointed to "C:\Program Files (x86)\ssrvc\msrvc.exe:*:Enabled:USB aware Client". I ended up installing (then uninstalling) "USB Lock" from Advanced Systems International a while ago. This program was so incredibly badly designed that its still monkeying with my home PC, 6 months later. I had to reboot into safe mode then delete the rogue files that it was infesting my system with. Update: Ended up reinstalling Windows to get rid of that buggy app. My advice? Don't touch it, unless you're into that suffering thing. 

I've run into a brick wall trying to install MySQL v5.5 on my machine. My PC is Windows 7 x64, Enterprise edition. MySQL installs fine, but when I run the "MySQL Instance Configuration Wizard", it pauses forever on the step "Start Service" (I can let it run for 30 minutes with no response). If I go into services, I see that the "MySQL" service hasn't started, and if I try to start it, it says "Windows could not start MySQL Service on Local Computer. Error 1067: The process terminated unexpectedly." I've tried the following: 

I'm using nginx as a reverse proxy with . The back-end is setting response headers which makes nginx serve responses from cache when possible. However I would like to allow clients to bypass the cache by setting a request header . This way users can get a fresh copy by hitting in browser. By default, nginx seems to ignore the request header. How can I configure nginx to fetch a fresh copy from the back-end and update the cache whenever a client requests a resource with ? 

I am trying to wrap up one of our projects in Ubuntu/CentOS packages. The project is a typical web framework which requires to deploy a tomcat war file, create some directories and run a mysql script that creates a new user + database with some tables, etc. Now I am not sure how to package up the mysql script. It should work either way if mysql has not yet been installed on the machine, or when the user already has a working mysql installed. The thing I am uncertain about is how to login in mysql with root so that I can add a new user and table. Should I prompt the user for his mysql-root password during installation? This might add confusion. Or can I just bypass authentication? Assuming the process is executed as sudo either way, I might be able to do a mysqld_safe --skip-grant-tables &, and login with mysql-root without any password. What is common practice when it comes to running a mysql script during the installation of a package? 

Mount the drive as per normal. Right click on the drive, and turn on file sharing. Alter the permissions for the share to remove "write" permission. Leave "read" permission enabled. 

Is there any service which runs in the background, and does the job of prime95 (i.e. it tests server CPU/RAM for integrity now and again)? Regards, Shane. p.s. The reason I ask: we recently had some bad RAM on our server, which ended up slowly corrupting all of our business files. A file would be copied into memory, it would be corrupted in RAM, and copied back onto the RAID 1 hard drive in a corrupted state. 

I'm copying 600GB of data from external hard drive A to external hard drive B. Windows Server 2008 R2 has notified me that 100 files have filenames that are too long (i.e. >255 chars). Is there a utility that can allow me to search for these filenames, and manually shorten them? Shane. 

We ended up setting up port forwarding on the router for the specific services we needed, and Hamachi 2 while we got the port forwarding working. Hamachi 2 is slow (seems to be limited to 8kbyte/sec), but it always works (unlike Hamachi 1, the subject of the original question). 

This basically replaces the upstream name with the $host and removes the port. This worked in my case because I am hosting nginx on the default ports for HTTP/HTTPS. If nginx is running on a non-default port, something like this is needed: 

For whatever reason I managed to mess up Apache quite bad (Ubuntu 12.04). The output below complains about configtest failing and a segmentation fault. However, I can't find anything in . How can I debug this "configtest"? 

in inside of the in either 000-default or default-ssl, it works perfect. However instead I would rather use the Ubuntu convention of putting new sites in a seperate file inside /etc/apache2/sites-enabled/. I have other sites inside of this path using directives. However, when I put the above line in a file in this path, it does not work. I just get a 404 when trying to access the site. Also, when I instead use something like 

I assume his wordpress site runs on some shared hosting service linux box. So now I'm curious, which versions of Debian, Ubuntu, RHEL, Fedora, FreeBSD, etc, include the StartCom root certificate? 

Here's how I fixed the problem: Step 1: By default, its impossible to find out the reason why the service is failing to start. So, tell it to start in non-service mode, and pipe errors to the console: 

Now, do all of your access through the UNC file share, e.g. \computer\mydrive - RO\ I use this trick to provide an absolute guarantee that my file backup program won't inadvertently stomp on the data that its meant to be backing up. 

You can also use the "Reliability and Performance Monitor" that is available under Windows Server 2008. As you can see below, it automatically keeps a record of the reliability of the server, and assigns it a "reliability score" out of 10. This score starts at 10, and drops if the server experiences any crashes or unexpected shutdowns. It even keeps a record of which programs were installed, and when, so you can diagnose if an installed program seemed to cause more faults. You can also set it up to continuously log the CPU usage of programs, to see which program is causing the 100% CPU utilization. 

If you choose the file structure option, one thing you can do to improve disk I/O performance at least to some degree is to mount the partition with noatime+nodiratime unless you must have them. They are not really important at all so I recommend doing that. Maybe you can also use a solid-state drive. 

Postfix. It's very reliable, still developed and used by many mail servers. Most Linux platforms support it. Works fine with php and mysql. If not postfix, then check qmail: same same but different. 

I am setting up a single Debian server serving Magento with Mysql. High load is expected (millions of hits per day). I've been reading various people's comments and found so many different setups but I am not sure what to do if I want to. Load balancing between several servers would of course be nice but I have only one server. So how about this? Varnish in front of nginx and apache, where nginx does only static data and apache dynamic. Or would be a better idea to put varnish in front of apache only? So nginx is in front and having static data directly served by nginx and dynamic apache pages via varnish? 

Google "undelete for Linux", and you'll find something such as $URL$ You can use this to undelete any documents that you want to recover. Take a step to avoid this in the future. What you want is a method to make a copy of the partition, something like Acronis TrueImage for Linux. If you run Acronis TrueImage for Linux, it copies a complete clone of the hard drive to a file (which you can safe offline for safety), so you can always restore from the bare metal if anything goes wrong in the future. 

We have a server with 16GB of RAM. The memory manager is very aggressive in trimming back the Working Set for a particular process, which is resulting in a lot of soft page faults which is slowing the process down and introducing latency. Are there any settings we can change in the registry to force the Dynamic Memory Manager to be less aggressive with trimming the Working Set? We wouldn't mind if the server devoted 80% of its RAM to the Working Set of the processes (its only using 2GB at the moment). 

The repositories are active and even my EPEL packages are being updated, but it seems that the person who created this instance somehow fixed it at 5.0. However, I really need to update to the latest 5.x. How can I get yum upgrade to work again? 

I had a thorough look at the machine that is running on 192.168.1.111, but I am quite sure there is no malicious software on there. What could be triggering this alarm? The only thing I can think of is utorrent local peer discovery, although I that is not exactly a "port scan". 

Mostly depends on which Apache modules you want to use. I think worker is generally the default choice, but some (older) modules require forking and depend on prefork. If you have no preferences, I recommend you go with the preferred dependency from your OS distribution. Ubuntu for example will by default install mpm-worker when you install Apache2. 

I manage an application that contains a filestore in which all the files are stored with the filenames equal to their md5 sums. All files are stored in one directory. Currently there are thousands, but soon their should be millions of files on the server. The current server is running Ubuntu 11.10 on an ext4 filesystem. Someone told me that it is not wise to put many files in a directory, as this will create significant increase in lookup time and reliability (he had a story about max files a single dir could point to, resulting in a big linked list). Instead he suggested to create sub directories with e.g. substrings of the filename. However, this will make some things in my application much more cumbersome. Is this still true, or do modern filesystems (e.g. ext4) have more efficient ways to deal with this and naturally scale? Wikipedia has some details on filesystems, but it doesn't really say anything about max files per directory, or lookup times. 

I was trying to create a mirrored drive in Disk Management, but I kept on getting this error: "All disks holding extents for a given volume must have the same sector size, and the sector size must be valid." 

If I type "netstat", I can see a list of IP addresses that my PC is connected to. If I start with an IP address, how do I find the process that has opened the connection to said IP address? 

Update 2010-12-20 Tried MySQL v5.1, it didn't work either. Its amazing - if you type "mysqld /?", or "mysqld -help", it doesn't give you any help. And, if you try to restart the service manually, it doesn't display any error messages. Could it be any more unhelpful? Update 2010-12-21 Installed MySQL 6.0 alpha, and it worked. However, I'd rather not use an alpha release, given that the "stable" release is anything but :( Update 2010-12-21 Found $URL$ dealing with troubleshooting under Windows. Discovered that you can generate an error log if the service doesn't start - see here: $URL$ Update 2010-12-21 Aha! A clue. To actually see the error, add "--console":