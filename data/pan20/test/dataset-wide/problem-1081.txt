a) Pretty much anything takes a schema stability lock. You don't want something else changing the structure of the table while you are updating your statistics. According to this, update statistics takes schema stability and modification locks. b) If something tries to change the table's structure, it will be blocked. IIRC, update stats does dirty reads, so it shouldn't block connections that are merely reading or writing. c) If you use FULLSCAN, it will read the entire table because that is what you told it to do. I don't see how that can be seen as anything but 'causing heavy i/o'. Normally the default of 'sampling' works well enough, but I have seen it cause problems with data with non-homogenous distributions. Often, it's also easier to just reindex the whole table (especially if you can do it online) because reindexing is parallelizable where as update statistics isn't. (AFAIK, MS did not fix that in sql 2008.) 

As an aside, note that your table is literally named "IR.TimesheetHoursProjectTask" and not "TimesheetHoursProjectTask" under the "IR" schema. To specify the later, using the [] notation, it would be [IR].[TimesheetHoursProjectTask]. 

400 million names is a lot. Am I in there? ;-) My gut level feeling says that using substring isn't going to be terribly much slower than coding up something via the CLR. I'm a SQL guy, I've done a fair amount of simple parsing in the past (2000 or 2005), and I was involved in what was going to be a very complicated parsing scheme (addresses, world-wide) written in c and called via an xproc until we found that out prototype "native" code wasn't any faster than the same thing written with tsql functions. If you want to use a language other than tsql, I'd suggest writting a CLR in c# or vb.net. For simple things, it's not hard to code in CLR. I went from newb to having a couple of working directory and file utilities in less than one morning. There are plenty of examples of simple clr procedures on the net. and you wont have to learn anything (or install visual studio) to write it in tsql No matter what, you will have to go through the table at least once. If you export, then parse and then put back what is not a small amount of data, that's a lot of time. Can you guarantee that your source isn't going to change in the mean time? Here's the thing that always seems to sneak up on everyone: What happens with the parsed data? Where does it wind up? Do you intend to update the row, perhaps there are lastname and firstname columns that you don't show in your example? If you do, and those columns are currently null or have zero length stings in them, you may find that the update statement performance is very bad because sql may have to split pages to store the lastname. In other words, your performance problem isn't parsing, it is storing the parsed data. Often, this is worse than inserting the data into another table. Also, all of those page splits will fragment your table and cause query performance to drop, which may enrage your dba because s/he will have to run a defrag procedure on the (large) table. Here's one last thought: Do you really need to store the parsed data? Can you get away with a computed column that calculates the last name on the fly? Those are indexable, with certain conditions, if you need that. Another approach would be a view that exposes the columns of the table as well as your "parsed lastname column". 

For a point in time recovery, you need the series of transaction log backup files that are usually generated by regular, scheduled transation log backups in addition to the full database backup that you possess. Transaction log backups are often denoted by .bak or by .trn extensions. If you do not have those transaction log backup files, you can't do what you want. Roughly speaking, you would restore to a point in time by: 1. Restoring the full backup file using the NORECOVERY option. 2. Restore all of the relevant transaction log backups using the STOPAT option. 3. Lastly, bring the database out of recovery. 

Log shipping might be "crude" (I prefer to call it straightforward), but replication is complex and, in my experience, brittle. You don't want to add complexity if you can avoid it. You don't want to have to go through some sort of heroics to fix a broken replication setup. Managing the replication configuration of all of those database would be a bear of a problem. You can't use the forms of replication that would alter database tables, as I suspect testing those changes on 500 databases is a very large project, if not a career, in and of itself. You can start to have performance problems on the subscriber side that require you to start looking at fillfactor, maintenance plans and other, similar complications. What happens if you need to flip over from your primary servers to your "local" secondary servers? Are you going to re-do all of the replication configuration? That's likely to be a big task. If you automate it, that is time spent creating and testing the automation, and then it needs to be looked at when you change those 500 databases. Recall that all we had for DR was log shipping up until SQL Server 2005. It's not glamorous, but it works. TL;DR- Simpler is better. Use log shipping. 

I suspect that there is some confusion about terminology here. Maintenance plans do not delete transaction logs. Maintenance plans delete transaction log backups. Transaction log files are required for the operation of a SQL Server database. Every SQL Server database has at least one transaction log file. By convention, these files are given a "LDF" extension. SQL Server log files do not behave like Oracle's redo logs. You can't just copy a log file and/or delete it. In your configuration (FULL), when the maintenance plan runs it should be configured to execute a transaction log backup command. This (more or less) makes a copy of the information in the transaction log file and places that information in a separate file called a transaction log backup. The transaction log backup command then marks the relevent space in transaction log file as free for use (again) by SQL Server. If this "marking of space" does not occur, the transaction log will eventually fill up. (Things work differently if the database is running in SIMPLE mode, but you aren't doing that.) With the FULL model, if the transaction log backup commands do not execute "often enough", the transaction log file can fill up. When that happens, depending on your exact configuration, the transaction log will either be automatically grown or updates to the database will start to fail. "often enough" is determined by how much space you have available, how big your files are, the rate of change of data in the database, the exact nature of how you do updates, your point-in-time recovery requirements, etc. Note that you can schedule transaction log backups in a highly flexible way: once a day, once an hour or once a minute or once every few minutes. You can even run transaction log backups more (or less) frequently during busy periods. The next time the maintenance plan runs, it will do the same thing as it did the last time but it will vary the name of the transaction log backup file (using a timestamp) so the earlier file is not overwritten. The maintenance plan runs the transaction log backup command based on the schedule that you configure. As the minutes and hours and days pass, an ever-growing set of transaction log backup files is created. At some point, those files need to be trimmed back by deleting them or moving them to somewhere else. If you are running some sort of log-shipping scheme, it's possible that you would be moving the transaction log backup files to some other safe location. The key concepts here are that the data is copied from the transaction log file to a separate transaction log backup file. These transaction log backup files should be stored for an appropriate period of time. A maintenance plan can be configured to automatically delete those files, you you can delete them manually or by some other scheduled method. Generally, most sites keep things as simple as possible and that "appropriate period of time" would be until your next full backup. For example, if you are doing a full backup every Sunday, you will need to keep at least seven days of transaction log backups. (I'd keep eight days, I'd like a little bit of overlap just to be safe). If you only keep three days of transaction log backup files, you will not be able to fully restore your data once you get past Wednesday. In the simplest FULL strategy, in order to restore a database you need the last full database backup and all of the transaction log backups from the time the full database backup was taken until the time that you need to restore to. If you are taking full database backups on Sunday with a three-day lifetime on transaction log backup files, when it gets to be Friday you will be boned since the maintenance plan has deleted the transaction log backup files from Monday and Tuesday. You will only be able to restore the database using whatever was in the last full backup. I have seen people fall into that trap. I'll close by saying that any person who is new to the way that SQL Server backup works should experiment/test restoring their database onto a spare machine, just to be sure that everything works the way you expect it to. You don't test a backup, you test a restore. 

I would not trust rsync to copy the ldf and mdf files for user or system databases, nor would I trust anything I hacked together (VSS or otherwise) in a production environment. SQL Server is very fussy about when (and if) things get written to the ldf and the mdf files. Software (rsync) that isn't designed with that in mind might not get it right, especially if it doesn't understand that the ldf files and mdf files need to be treated as an interrelated system of files. If that software doesn't get things right, nothing might be noticed until you failover, try to go live and have your databases flagged as suspect due to what SQL Server sees as data corruption. Even worse, "getting it right" might be dependant on how much load is on the system and you might not find the problem in a lightly-loaded test environment. I have seen enough examples of people who thought that they were replicating their files but were not. They were left with corrupt files on the recovery site and with inaccessible backup files at the primary site. So, that meant no database for them. In short, you are making an appointment for trouble. If you had some sort of block-level replication technology like EMC's SRDF or were looking at shared-nothing clusters, that might be different. Those technologies interface with SQL Server and the clustering services that Windows provides in a way that your writes will be safe and your files will/should be consistent. If my only disaster recovery option was a remote site that was normally down, I'd use log shipping and make sure that I had all of the pieces to restore the database(s) on the remote site. If you can't make the built-in log shipping do that, writing your own isn't that hard. I've probably written three or four (simplistic) log shipping systems from scratch in the last 14 years. At a minimum, the key things you need are: A full backup needs to be taken and copied over to the remote site. Your tlog backups need to be taken and copied over to the remote site. You need an automated method to restore that full backup and any relevant tlog backups. Ideally, this should be simple enough for someone else to do and/or simple enough for you to figure out at 3AM when your primary server fails and you are half asleep. When you have an event, bringing up the other server will take longer because you would have so much manual stuff to do. That means that this isn't as good as simply implementing regular log shipping. You will need to test this periodically, as well. (Of course, you need to worry about other things too, like jobs, packages, login and user synchronization, changing DSNs on the web servers during a failure, etc. If you have a large environment and a serious disaster, like the loss of your primary data center, you will be trying to do this when IIS guys, file server guys, network guys and whatever else guys are trying to bring their stuff up too.) If it were me, I would be agitating for a warm standby server at the remote site. That would make (standard, out-of-the box) log shipping easier and database mirroring possible. It sounds like you have tried that.