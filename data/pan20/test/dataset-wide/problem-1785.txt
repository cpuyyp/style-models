htop as far as I know has linuxulator as a dependency. What it can't find here is something within your kernel source. Check if there's anything in /usr/src. If not you should check out the source collection, which belongs there, using csup. 

What are you trying to achieve with Gluster on the VM Host? Gluster is a FUSE Filesystem. That means it runs on top of whatever other filesystem and only proives Network functionality. If HA Disk replication with VMs on top is what you want, then Gluster would be the wrong choice. There are things like drbd for that. If however you need a shared filesystem for VMs to mount it may be a good option depending on what exactly you're trying to do. 

They changed the kstats. Oracle Solaris 11 has removed the following statistics from zfs:0:arcstats: 

On VMs your "Disk" is usually not a disk but a virtual device, which is either a file, an iSCSI Volume, an LVM Volume or something like that. I'd also consider this to be the case for you, as you're saying its on a SAN. That means that there is some machine(s) with numerous disks with Storage Volumes on top of them which they serve out to other machines via a block based network protocol, which they then mount as if they were local and those may serve out again to yet some other machines via a standard network transfer protocol like NFS, SMB or whatever. This means, that your disk is not a disk, but more of a file residing on a storage volume on the network, which itself resides on a bunch of disks. This also means, that disk monitoring is something that is being taken care of somewhere else and also, that you'd probably only get bogus data (if you'd get any at all) from any sort of disk monitoring. You can do file system checks though, of course, as something could still happen there due to a bug or some other not forseeable reason. 

I have a setup with 2 machines with Pacemaker and Corosync installed. There I have a PostgreSQL Master/Slave Set running. The Master has a resource Group with a Virtual IP and two additional services that are supposed to run on the master. In case of a failover triggered through killing the database master, this migrates all the services in the group to the other node, which is exactly what I expect and want it to do. The additional services however will just be marked as failed when I kill them and thats it. Since I only want a migration to happen when the database fails, that is actually fine. However I want Pacemaker to restart these services in case they fail, and not just mark them as failed. My expectation was, that it will do exactly that when I add the on-fail=restart flag to the monitor op of these services, but this is not the case. The group looks like this: 

There is nothing, that is not working with a motherboard as the one you list. Even Sandy Bridge Integrated Graphics are more or less working by now, which has been an issue for a while. Since motherboards generally just provide generic I/O stuff there are usually no specific drivers required for their basic functionality is supported by generic drivers. Additional SATA Controllers sometimes tend to be rather esoteric and may require additional drivers. For (normal onboard) Sound devices and Networking on the other hand the available chipset choices on the market have become so small by now that you'd probably have trouble finding an unsupported device. 

As far as I know Hamachi needs a Kernel module. A module that is precompiled 32bit or which can't be compiled on your 64bit arch won't work. This is probably why it just exits without giving any message, and frankly won't be solvable unless there is a 64bit package of Hamachi1. 

Why don't you ask within the University then. Someone within must have assigned that IP and will know, who or what it belongs to. From what you say you're on the University Network and this traffic does, too. So there are two options: 1. A student trolling - unlikely, they usually have IDS and stuff nowadays and would've probably already shut this down 2. Its some check the university does. In either case. ASK! Blocking may be a very bad idea, because if it is something legitimate they run for security or whatever and you block it - they will probably do the same with you. 

Right now your only option is restoring from backup as has previously been stated. I'd highly recommend using snapshots and integrating those into your backup workflow. First of all you will only be backing up changes and new files once you've done a full backup and go with snapshots from there and it may also increase your flexibility depending on how you do backups now. Considering you're on a 12 hour backup schedule you could do hourly ZFS Snapshots in between and keep those for a few days. You could probably even integrate the snapshots themselves into your backup mechanism using send/receive. Its a great feature for incremental backups. 

from lom, because that output doesn't look very good (come on, it didn't find an EEPROM and I2C failed). If theres anything interesting in 

Attackers also often use spoofed adresses. Probably what you are looking for is sockstat. There also is the accf Kernel module, which might help you depending on what your applications are. I'd also recommend using pf. You could build something along the lines of: 

The obvious advantage for swap is that when your machine crashes you can still create a crash dump. Correct me if I'm wrong, but afaik this isn't possible without swap. This isn't VMWare specific of course but rather applies anywhere. I felt like it might be important to point out. 

GlusterFS would seem like the ideal solution to me. To the guy who claims that Gluster takes lots of effort to set up I've got to say that he's probably never tried. As of Gluster 3.2 the configuration utilities are pretty awesome and it takes 2 or 3 commands to get a gluster volume up and sharing on the network. Mounting gluster volumes is equally simple. On the plus side it also gives you a lot more flexibility than NFS. It does striping, relication, georeplication, is of course POSIX compliant and so on. There is an extension called HekaFS, which also adds SSL and more advanced Authentification mechanisms, which is probably interesting for cloud computing. Also it scales! It is F/OSS and is being developed by RedHat who've recently purchased Gluster. 

There also is CloudSigma in Switzerland which might be a good choice if you're concerned about US Internet legislature and privacy related stuff. 

You probably just need to set up Avahi/mDNS Resolution. It does kind of the same as LLMNR on Windows. 

is part of the default smb.conf. Well then. Is root a valid smbuser, yet? Then try explicitly allowing root : 

I'm currently planning a more fault tolerant setup for my organizations Mail setup. We currently have two machines available in different datacenters, that we're planning to use for this, one being a physical host, the other being an lxc "VM". The plan is to make the Physical host our primary Mailserver (lets call it server1) and the VM (lets call that server2) the secondary via DNS, so that when server1 goes down, server2 could take over until we've got the other one up and running again. As with lxc I can't do fancy stuff like using DRBD or even GlusterFS due to permission issues, my choices for replicating data are rather limited. As I personally like dbmail I've thought about using that with Postgres behind it to take care of storage. So basically I'm thinking of having dbmail with its own postgres running on server1, as well as another dbmail and another postgres on server2. The question is: What options are there to keep those two postgres servers in sync without fancy stuff like drbd or a distributed filesystem, so that, when server1 goes down, server2 just continues where server1 stopped. 

The Management of Memory in this regard is rather bad on Linux. It will take the first 4G of memory and split them 3/1. 1GB being LowMem. With 32GB of memory on the system it will already need a substantial part of this 1GB for addressing purposes. During the 2.4 days there was a discussion about putting some effort into this to make this limit configurable, or to integrate the 4G/4G patch, none of which has happened though, as Linus didn't see any need for this, and things were already ugly as they were, not to mention, that 4G/4G is not pretty either. There still is a 4g patch around for 2.6, but it got written for 2.6.6 originally, which is very much outdated today. By 2.6.7 it was pretty clear that it would never be merged, its performance overhead was gigantic anyway, so the decision was made, that the VM System was good enough as it is. So on 32bit there is probably no way around this issue, as the memory system is simply not meant to scale to such amounts of memory. On 64bit on the other hand addressing has changed considerably, so you won't find this issue, there. 

That'll give you more information. Have you configured a logging facility on the client yet, to tell syslog what to log? To find out more about it look at the aforementioned Handbook and the syslog Manpage 

There is no way to get FreeNAS to do a suspend, because AFAIK they didn't compile the necessary ACPI stuff into the system. Actually I think I've been wrong here! I just found acpi feature request and trac commit log. It sounds a little vague though, so you may want to ask at FreeNAS Dev Mailing list 

I'd actually recommend Gluster. It is OpenSource, well documented and RedHat recently purchased it. It has a relatively good performance, and since it is a RedHat project now, it is well supported on CentOS. There also is a project called HekaFS, which has the goal of extending glusters Authentication and Security capabilites by adding SSL and other goodies. It is quite simple and very well designed. It comes with pretty good management utilities. 

Or chech the paramters you're starting with already and see if one of them causes it to omit the config. 

Might be you start mysql with --no-defaults or (-no-defaults), which causes it to not load a config at all. Theres also --defaults-file (or -default-file, not sure), which causes it to load a specific config. Also check if that is to a different config than the one you edit. You can try starting with: 

You may be running into an old ZFS import bug here that causes forced imports to only be possible when doing it by id. See an explanation in the comments to this post. If this resolves the problem I'm glad. If it does not you should check your logs for zfs complaining about checksum mismatches. If this is the case you'll probably want to see what zdb -l /dev/dsk/ad* tells you about your disks as it would mean corrupted metadata. 

You can just log on manually using telnet on Port 25. Then you say and try next up At that point it should tell you that it rejected the address because it denied access if it is not an open relay. If it is open it will do what you just told it - send out an email. 

The problem you are facing is that SPARC is Big Endian, while x86 is Little Endian. Back in the days that meant, that Little- and BiG Endian systems save stuff in opposite byte order to disk. With modern filesystems like ZFS that doesn't matter anymore, but what you got is probably UFS (just out of my head). So basically you need another SPARC machine to read that.