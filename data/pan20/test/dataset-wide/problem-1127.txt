I'm working with a colleague who's proposed to split our 1 instance-database into about 7 databases (divided by data domain) for development and 7 identical databases for production. I get the test-production duality logic, but in what case or what advantages are there to splitting our 1, relatively straightforward database into 7 databases? Our data warehouse is only consumed/ used by ONE business intelligence application, period. I am concerned about this direction so hopefully you can discuss the general reasons proposed for this split, and I can give you a run down of the current properties of the database. 1 database Data Warehouse: 352 GB total, 203 tables, 170 views Proposed split: 

We're using a business intelligence system and need to load a list of dates so we can flag them as 'last week' or 'last 12 months' or some dynamic value. I'm wondering what's the simplest way to virtually generate a table that simply lists dates in one column, literally every date from '2014-01-01' to the current date (the other columns I can use a formula from there). Actually even appending future dates on for a year would probably be useful as well. Now yes I can grab distinct dates from another random fact table that has thousands of entries, but that seems sloppy and is creating a dependency where there really shouldn't be one. 

The process works just fine on Machine 1. It's suddenly failing on Machine 2. I'm inclined to believe that the server settings are the issue. After talking to our DBA, I was informed that the suspected difference is that Machine 1 has service pack 1 installed for windows and Machine 2 has service pack 2 installed for windows. Apparently the latter may be blocking connections after a certain point to prevent a DDOS attack --- or enough open connections may cause the firewall to jump in. Here are my thoughts. The connections are closing just fine. I know this because (for testing) - I used a SQL DB on HostGator which limits me to 24 simultaneous connections (and would let me know if I exceeded it). But maybe merely opening connections repeatedly, or sessions (?) -- is triggering some kind of security/ rate-limiting on this server. My question is --- how do I proceed from here? The way I see it, I have two options: 

I'm running an ETL process and streaming data into a MySQL table. Now it is being written over a web connection (fairly fast one) -- so that can be a bottleneck. Anyway, it's a basic insert/ update function. It's a list of IDs as the primary key/ index .... and then a few attributes. If a new ID is found, insert, otherwise, update ... you get the idea. Currently doing an "update, else insert" function based on the ID (indexed) is taking 13 rows/ second (which seems pretty abysmal, right?). This is comparing 1000 rows to a database of 250k records, for context. When doing a "pure" insert everything approach, for comparison, already speeds up the process to 26 rows/ second. The thing with the pure "insert" approach is that I can have 20 parallel connections "inserting" at once ... (20 is max allowed by web host) ... whereas any "update" function cannot have any parallels running. Thus 26 x 20 = 520 r/s. Quite greater than 13 r/s, especially if I can rig something up that allows even more data pushed through in parallel. My question is ... given the massive benefit of inserting vs. updating, is there a way to duplicate the 'update' functionality (I only want the most recent insert of a given ID to survive) .... by doing a massive insert, then running a delete function after the fact, that deletes duplicate IDs that aren't the 'newest' ? Is this something easy to implement, or something that comes up often? What else I can do to ensure this update process is faster? I know getting rid of the 'web connection' between the ETL tool and DB is a start, but what else? This seems like it would be a fairly common problem. Ultimately there are 20 columns, max of probably varchar(50) ... should I be getting a lot more than 13 rows processed/ second? 

I'm not the expert on indexes really - I'm trying to work my way through a few materials now. Anyway, let's say I have a table with 7 million records spanning 3 years. However the most common query will pull say the last 3 months -- using an integer field (unix epoch time). This 3 month query usually takes about 18 seconds --- but many other queries may be built atop it, so I was trying to speed it up. I created a clustered index on the unix_epoch_time int field that would eventually be in a 'where' statement filter. However, this doesn't appear to have sped up the query time at all. Does this mean this clustered index is a spurious waste? Or accounts for something like 1% of the query time? I thought it would be much faster. 

I'm very green when it comes to the world of database permissions management in SQL Sever. Let's keep an example simple. Say account 'admin' is the owner of schemas A, B, and C. There another account 'minion' that you want to have full rights (update/delete/insert/select/alter) on any object (table/view) created under schemas A, B, and C. Is this possible? Or do you have to execute a grant statement each and every time you add a table/ view under these schemas? (seems a bit silly to me). 

My thoughts: .... This seems completely ridiculous to me, but maybe I'm wrong. We've already organized our data warehouse by 13 'source system' schemas. 

I'm using survey monkey's API to dump survey data into a SQL data warehouse. So I'm dumping response data into a table 'resonses' thusly: 

So this would give me the data I need. HOWEVER, now I have a problem --- say I have 500 employees and the REGION data is updated a month in arrears. So right now, February 2016, we only have region data from January 2016. Would I would LIKE to do, is ... for the current month (February 2016) ... if the data in the "helper table" for region is missing for the month, take the last month found (which may be January, but sometimes even December or November potentially). I'm not sure what to do here. Create some kind of view? Do I restructure the 'helper table' so it's more of an inequality statement? EDIT: I think this is a slowly changing dimensions problem. Hmm I probably have to reorganize the dimension table. 

-- Isn't this problem also completely irrelevant to having multiple databases? It's my understanding that deadlocks occur at the table level (actually usually even just the row level, but eh). Even then, all our data inserts happen at midnight, all our selects downstream to the BI happen at 2 am. Having two processes update the same table is irrelevant to multiple databases, is it not (deadlock would happen either way)? Also, I personally have seen no evidence of table deadlocks occurring during normal operations. 

etc. etc. So in order to aggregate by region, I would need a helper dimension table as follows (Remember, region can change over time) (table: employee_region) 

I'm not a general IT expert. Sometimes with SQL queries ... they will take 10 seconds about 95% of the time. Every so often, the exact same query will take 5-10 minutes. As will others. The constant is the host computer and current time --- not the query itself. I'm just wondering if there's any way for me to test or verify what the problem is that is slowing down the query (and that it is, indeed, slowing the query). Would I remote into the host machine and check the memory usage/ processes? The host machine is also in a foreign country --- USA to Germany. Again, 95% of the time, this doesn't matter. I'm wondering if there are temporary 'network spike' issues. I want to know so I can actually complain to IT there ... but I want to pinpoint exactly what's causing the query issues. Maybe this is out of scope of this board ... I know a showplan wouldn't really indicate anything ... or would it? Would network or memory issues slow all steps the same duration? Is there an easier way to troubleshoot this? 

I'm doing some data analysis and want to find an easy way to examine all the members of each "group" in a group by function. Like, 3 agents may be involved in an order. I want to quickly examine the three agents that were 'grouped' in this order for various reasons. Usually, I would use group_concat for this (easy way to see all grouped strings). However replicating that using a 'group by' appears difficult and unwieldy for now in SQL sever. Right now, rough-and-dirty, I would max(username) and min(username) to quickly find 2 (and 90% of orders probably have 2 or less people. Is there a way to do mid(username) or 2nd-highest(username), or percentile(50th, username)? That would be a great, quick way to find this relevant data. For some reason, the previous answers I've seen describing group_concat on SQL Server do not sound straightforward to me. Sample data for instance: 

One database is harder to document and draw schema diagrams for than 7 databases (we will have views that will span multiple database). 

I'm not an expert on database performance. A colleague has suggested splitting a BI data warehouse database into several databases based on functional business unit - I believe still on one machine though I'm not sure - for performance reasons - there are many processes, procedures, logging, and monitoring happening. This has nothing to do with security or permissions - in fact those will be made more complicated potentially. Again I can't say I'm the expert but for some reason, it doesn't seem logical to me. DB in total is about 500 GB and we have about 200 tables. Naturally it seems like most solutions from my cursory research recommend improving the disc drive speed (like SSD) or RAM (currently an anemic 4 GB) for performance reasons. Or possibly keeping one logical database, but fragmenting the schemas into 2-3 different machines. But I don't know. Is there a significant use case or performance reason to split one database into 2 logical databases - other than user permissions/ security which aren't a factor here. There will be many views that span across these 3-4 databases. Do read operations, normalization, tuning options, or database/table locks come into play here? My instincts tell me no - those are entirely separate issues, but I can certainly be wrong. I'm just concerned that we are going to trade minimal, if not zero, performance improvements for a whole lot of added complexity and maintenance. I obviously am coming in here already leaning/ biased at one side, but wondering your thoughts. 

However the group_concat seems very confusing to use with a group by statement - or simulating group_concat with SQL Server. So instead, how bout this. select max(employee), min(employee) 

Does it make a difference whether that table has 1 field, 2 fields, or 100 fields, in actual query time? I mean all fields other than "order_value." Right now I'm pushing data to a data warehouse. Sometimes I dump fields into the table that "may be used in the future, someday" - but they aren't being queried right now, by anything. Would these 'extraneous' fields affect select statements that do no include them, directly or indirectly (no * I mean)? 

I'm running an ETL process that is writing about 2 million rows to a SQL Server database. I'm trying to optimize the time it takes for pure inserts (I guess updates is a different story). I'm wondering what the biggest bottleneck is, or best way to reduce time, for basic inserts into a SQL database. I mean, the first thing, probably, is the size of the data, right? Both the number of rows, the number of columns, and the data size in each column. Some of these may not be able to be minimized, the KB/ footprint of each row is one thing that can be potentially optimized, right? What else can be optimized or is the largest factor? Is it the transmission medium? I mean, how much magnitude of difference is there between writing to a database that's on the same computer, vs writing across a web connection (that is robust, fast, and has a ping of 1 ms?). Finally --- why is it that multiple parallel connections to the database seem to speed up the process to a point? I mean, when I have 20 connections making inserts round-robin style, it's about 6-7x faster than one connection writing ALL the data. I'm curious why this is. Right now I have 2.2 million rows totaling 2.7 GB. That's 1.23 kb per row. Right now inserting 1000 rows at at time (1.23 MB) using 14 connections takes 6.7 seconds. That's a snail-paced 10.66 rows per second. Even assuming 1 connection would be just as fast (it isn't) that's 150 rows/ second at best, which is not exactly "fast" either. I'm writing over a super-fast, robust web connection b/c we can't have the ETL process on the same space as the data warehouse. So .. how can I optimize for speed here? The reason for 1000 rows at a time is because the data comes from pages of 1000 - but optimizing the parsing is a separate issue for now. I do have one primary index I believe, but nothing too write-expensive. Right now I've simply been doing Monte Carlo like testing (try it and see what works) but I need something more focused. 

Just wondering - I made a trivial change to a view. No additional tables were added, although I guess the logic was changed a bit and a subquery was added --- but again, no additional base tables. Suddenly, it seems the permissions to other users on the view have changed/ become restricted. What's going on here? 

I'm wondering how I can join the 5 answer columns above into meaningful text (or at least the integer) without doing 5 joins. I mean say I have the table 'responses' and the table 'answer_key'. My initial thought would be go through the motion: 

I'm talking about SQL Server in particular. I suppose a user tried it and got an error (you don't have select permissions on hidden underlying view B not explicitly mentioned in query). The main view A joins a table and another view (view B) together, and they don't have permission for view B. I'm not in charge of permissions for the database, but I'm curious how it works. Do you need to grant permissions to every object built underneath the views? Or just the view itself? Or either one? I thought one of the purposes of views was a snapshot of underlying objects that you DON'T want to give full permissions for. That's what is unusual to me.