Discussions: That said, there is the generated regressor problem in the regression above, that is, the some of RHS variables ( and ) are generated using the OLS regression results. This usually causes trouble. However some tests can be done even if the regressors are generated. I guess this is one, but I haven't checked it formally. Further discussions: The BG test is an LM test, while does a Wald test. The difference should be minor. Simulation results: I did simulations. The ordinary BG test seems to fail. The robustified version seems working. 

Weak instruments combined with slight instrumental endogeneity can lead to a larger bias than OLS. As Nox's answer shows, the probability limit of the IV estimator is $\beta_1 + cov(z,u)/cov(z,x)$. When $cov(z,u) \ne 0$ though small, if $cov(z,x)$ is small, then the bias can be large. See Bound, Jaeger and Baker's (1995, JASA) remark following equation (7) on page 444. $URL$ "It is clear from Equation (7) that a weak correlation between the potentially endogenous variable, $x$, and the instruments, $z_1$, will exacerbate any problems associated with the correlation between the instrument and the error, $\varepsilon$. If the correlation between the instrument and the endogenous explanatory variable is weak, then even a small correlation between the instrument and the error can produce a larger inconsistency in the IV estimate of $\beta$ than in the OLS estimate." Without instrumental endogeneity, I don't think the IV estimator's bias (of the limit distribution, there may be no probability limit) is larger than OLS's inconsistency. Another thing to consider is that the variance of the IV estimator using very weak instruments can be large even with a very large $n$, and thus you may have an IV estimate more nonsense than OLS for a data set just by chance. 

Watch out!!! This problem is very subtle. The unobserved term $\epsilon$ cannot be the regression error term if $\epsilon$ is uncorrelated with $Y$. In the model $Y=\beta_0 + \beta_1 X + U$, the error term $U$ is a part of $Y$, and thus $U$ is 100% correlated with $Y$. In your case, $\epsilon$ is uncorrelated with $Y$, so it is not the error term unless $\epsilon = 0$. [Edited from here] The issue of endogeneity has nothing to do with whether $X$ and $Y$ are correlated. If $Y = \beta_0 + \beta_1 X + U$, $\beta_1 = 0$ and $X$ and $U$ are correlated, then $X$ is correlated with $Y$. On the other hand, if $\beta_1 \ne 0$ and $X$ and $U$ are "exactly" correlated, then it is possible that $X$ and $Y$ are uncorrelated. The question only says that $X$ and $Y$ are uncorrelated. (It also says that $Y$ and $\epsilon$ are uncorrelated, but this information is irrelevant because it does not explain what $\epsilon$ means other than that it is unobserved.) So, endogeneity matters, not because $X$ and $\epsilon$ are correlated or whatever but because it says nothing about how $X$ and the error term are correlated. If $\epsilon$ is the error term, then it is implied that $\epsilon = 0$ so we have a perfect fit and no regressor-error correlation. 

The equation-wise OLS estimators are consistent under the further normalization that $E(u_{i1})=0$ and $E(u_{i2})=0$, which are naturally assumed to be satisfied. For GLS, I don't know what you mean by GLS, but if you mean the SUR estimator, then I see no reasons why inconsistent. The fact that $E(x_i' u_{i1}) \ne 0$ is irrelevant because $x_i$ is not a regressor in the first equation. 

I am a bit confused because you said there are two treatments. I presume that one "treatment" group is the control group and the other is the treatment group. (I don't think you have a separate control group.) I am also confused because you said the treatment is identical until round 3 but then the treatment begins in round 2. I presume that there is no treatment in rounds 1 and 2, and the treatment group is treated differently thereafter in rounds 3, 4 and 5. Your $Roundn$ dummy variables for $n=2,\ldots 5$ make sure that you handle the "round effects" (for the control group) properly in a nonparametric way. This looks fine to me. If you include only the $After$ dummy, it means that there is no trend within each of the "before" and "after" periods. You would not want that. A single $round$ variable means that there is a linear trend in $Effort$ in the control group. You could try that, but I would wonder where the linearity belief comes from. Also, you lose only 3 more degrees of freedom by including the round dummies comparing to the linear trend model. That's not a big deal unless you have a really small sample. I would be happy with the full round dummies. Your model assumes that the treatment effects (measured by diff-in-diff) are identical in all rounds 3, 4 and 5 (because you have only one interaction term). If you believe it is true, that's fine. If you believe otherwise, you can include three interaction terms $Treatment * Round3$, $Treatment * Round4$ and $Treatment * Round5$ instead of the single interaction term. If you want, you can test if those treatment effects are identical across rounds. 

In Alecos Papadopoulos's answer, both the conditional mean and the conditional median are linear in $X$. In the following example, the conditional mean is linear in $X$ while the conditional median is quadratic in $X$. This example is a constructed toy but it touches the heart of the issue. Example: Suppose that $y = \beta_0 + \beta_1 x + u$ and $u = x^2 (w - 1)$, where $w \sim \chi_1^2$ and $x$ and $w$ are mutually independent. Then, as the R command gives, $med(w) \doteq 0.455$ so that $med(u|x) = x^2 (-0.545)$, while $E(u|x) = 0$. We thus have \begin{equation} E(y|x) = \beta_0 + \beta_1 x, ~~\text{ while }~~ med(y|x) = \beta_0 + \beta_1 x - 0.545 x^2. \end{equation} Note: Asymmetry of the error distribution and the dependence of $u$ on $x$ are both important. If the error distribution is symmetric, then the conditional mean function and the conditional median function coincide. Also, if the distribution of $u$ is independent of $x$, then $med(u|x)$ is nonzero but constant and thus $med(y|x)$ is again linear in $x$. 

Interpretation: Model 1: Within the same unit, $y_{it}-y_{is}$ is expected to be $(x_{it}-x_{is})\beta_1$. That is, within a unit, between two periods with different $x$, $y$ is expected to differ by the difference in $x$ times $\beta_1$. Model 2: In the same period, $y_{it}-y_{jt}$ is expected to be $(x_{it}-x_{jt})\beta_2$. That is, in a period, between two units with different $x$, $y$ is expected to differ by the difference in $x$ times $\beta_2$. Model 3: Within unit $i$, $y_{it}-y_{is}$ is expected to be $(x_{it}-x_{is}) \beta_3 + (\gamma_t - \gamma_s)$, and within unit $j$, $y_{jt}-y_{js}$ is expected to be $(x_{jt}-x_{js})\beta_3 + (\gamma_t - \gamma_s)$. So if $x_{it}-x_{is} = x_{jt}-x_{js}$, then the expected differences in $y$ are the same for both units, and $(y_{it}-y_{is})-(y_{jt}-y_{js})$ is expected to be $[ (x_{it}-x_{is}) - (x_{jt}-x_{js})] \beta_3$. How can I interpret this math verbosely? Let me try, though I'm not sure I will succeed. Suppose that unit $i$'s $x$ differs by 1 across two periods, so does unit $j$'s $x$ across the same two periods. Then those two units' $y$ values are expected to differ across the two periods by the same amount. If the two units have different differences in $x$ over the two periods, then the expected differences in $y$ differ by $\beta_3$ times the difference in the differences in $x$. Yes, this is like DID (difference-in-differences). 

R-squareds and p-values give different information. For example, suppose that you regress birth weight on mother's smoking. The p-value tells us whether the association is indeed nonzero (in the statistical sense), and the R-squared tells us how much of variation in birth weight is explained by mother's smoking. p-value > .15 and R-squared 40% would mean that (1) we can't say that the association is nonzero (2) but the explanatory power for the given sample is quite large. The two neither reinforce each other nor are contradictory. 

Perhaps you are having trouble because the meaning of "nonrandom assignment" is not specific enough. Just saying "random" is not good enough. You need to ask "random (or nonrandom) in what sense". If the assignment is random in the sense that it is independent of any factors whatsoever, i.e., of all the factors that affect $y$, then it is also independent of the regression error (with zero unconditional mean), and ZCM holds (because independence implies ZCM). In other cases (nonrandom assignment), whether ZCM holds depends on what kind of factors the assignment depends on. If the assignment is nonrandom in the sense that it depends on other explanatory variables (such as $age$), ZCM still holds. If the assignment is nonrandom in the sense that it depends on the error term (the factors that affect $y$ other than the included control variables), then ZCM is likely violated. 

Let $\hat\alpha^i = \left[\sum_t (x_{it}-\bar{x}_i)^2 \right]^{-1} \sum_t (x_{it}-\bar{x}_i) (y_{it}-\bar{y}_i)$, estimator from the individual OLS regression. Let $\hat\beta$ be the FE estimator using the panel data. Then, math gives the identity $$ \hat\beta = \sum_{i=1}^N w_i \hat\alpha^i,\quad w_i = \frac{\sum_t (x_{it}-\bar{x}_i)^2}{\sum_{j=1}^N \sum_t (x_{jt}-\bar{x}_j)^2}. $$ This confirms ChinG's answer. (Note that the mean-group estimator is $N^{-1} \sum_{i=1}^N \hat\alpha^i$, which is different from the FE estimator.) 

I would construct and maintain only the full (unbalanced) data set with many variables with many missing values. Why? We can always extract balanced panel data sets from the full one; we can't go the other way around. When a balanced set is needed (in order to reduce the file size or send it to others), I would use Stata or R and never do it manually. To err is human. 

If you want to stick to OLS, your suggestion (clustering) seems fine. If you want to pursue efficiency, you may want to use the random effects FGLS () estimation. If you believe that all body temperatures are identical on average, it is alright to use any of them. But if they are heterogenous (due to genes or whatever), none is satisfactory. I would rather think more about what "average body temperature" means. Let us take an example. When your population is three people (101, 102, and 103) and your sample is 

Your approach seems no good. Let us make it really simple: You have only the dependent variable and you want to compare the averages across countries. You keep the firms whose $y$ values are inside the range. By doing this, you are (sort of) equating $y$ values, which you aimed to compare. This is strange. In fact, this leads to endogenous sample-selection. It seems, however, acceptable to keep only comparable firms (rather than firm-year) based on the exogenous variables. 

As jmbejara says, better to be more specific, but let me guess. You have a linear model $y = \beta_0 + \beta_1 x_1 + \cdots + \beta_{10} x_{10} + u$, where $x_1, x_2, \ldots, x_{10}$ are endogenous. Then, yes, you need at least ten instruments. Can you use the same dependent variables as instruments for the regression? No, you can't. Instruments should be exogenous (uncorrelated with $u$) and relevant (strongly correlated with the endogenous regressors). But if some of the regressors are (believed to be) exogenous, then you use those exogenous ones as instruments. But you still need extra instruments for endogenous regressors. The rule is that you need at least $k$ extra instruments if there are $k$ endogenous regressors. Finding good instruments (exogenous and relevant) is usually difficult. I suspect it would be really hard to find 10 instruments for a regression unless you have good economic justification. That said, again it would be better to see your specific problem. 

Let $\hat{\pi}_0$ and $\hat{\pi}_1$ be the first stage coefficients. That is, $\hat{D} = \hat\pi_0 + \hat\pi_1 Z$. When $Z_i D_i = Z_i$ (if $Z_i=1$ then $D_i=1$), we can show that $\hat\pi_0 + \hat\pi_1 = 1$ always and $\hat\pi_0 = (\bar{D}-\bar{Z})/(1-\bar{Z})$. (Proof is fun.) Unless $\bar{D} \simeq 1$, $\hat\pi_0$ is far from unity, which means $\hat\pi_1$ is far from zero. Not a surprise that the p-value is very small. Another argument: Because $ZD=Z$, we have $Z(1-D)=0$. Thus, $Z$ and $1-D$ are strongly correlated unless $E(Z)=0$ or $E(D)=1$. By the way, try to see an interesting result. Whether you need more variability is another thing. The question is whether it is possible that $Z$ is exogenous and $D$ is endogenous. Need to think more. EDIT: Thought about whether it is possible that $Z$ is exogenous, $D$ is endogenous, and $ZD=Z$ at the same time. It looks possible. Let $u$ be the regression error term. We can let $D=Z+(1-Z)\xi$, where $\xi$ is a Bernoulli random variable, so that $ZD=Z$. Let $E(u|Z)=0$ as we wish. Then $E(Du)=E(u|Z=1)P(Z=1) + E(\xi u|Z=0) P(Z=0) = E(\xi u|Z=0) P(Z=0)$, which can be nonzero. It is possible that $Z$ is uncorrelated with $u$, $D$ is correlated with $u$, and $ZD=Z$, I guess. So my answer is: No worries. EDIT: But "$Z=1 \Rightarrow D=1$" means that $D$ is exogenous if $Z=1$, and is endogenous if $Z=0$. Whether this is OK depends on the context. 

(i) I think your idea makes sense. Under the null, $[X,Z]$ is orthogonal to $\varepsilon$. Under the alternative, $X$ is correlated with $\varepsilon$. (ii) Your statement that it's "basically a test of whether the OLS residual are orthogonal to $Z$" is exactly what I think. (iii) Your thought about the power depending on the relevance of $Z$ also makes sense. If $Z$ is irrelevant, then the moment restrictions $E[Z_i' (y_i - \alpha - X_i \beta)] = 0$ hold for any $\beta$ (due to the uncorrelatedness of $Z_i$ and $X_i$) so the OLS residuals should appear to be uncorrelated with the instruments although the OLS estimator is inconsistent. I think this test is closely related with standard methods explained below, although I have not done any derivation in this regard. (iv) As you would be aware, there are textbook methods to test endogeneity. Stata implements some. See . You will see there. There, for the 2SLS estimation, OLS and 2SLS can be compared; or $y$ is regressed on $[X,\hat{\eta}]$, after which the statistical significance of $\hat\eta$ is tested. 

When $\beta_1 \Delta x = 0.01 \times 1 = 0.01$, the average growth rate is not 1% but 175%. But $\Delta E(\log y)$ is approximately 0.01. Note 1: The above discussion is for the log-level model. Little is changed for the log-log model. Note 2: If you define the growth rate as the log-difference, the expected growth rate equals the expected log-difference = $\beta \Delta x$. 

$y=\beta_0 + \beta_1 x + u$ if no $y$ is close to 0 or 1. A log model $\ln y = \beta_0 + \beta_1 x + u$, which is helpful when some $y$ values are close to zero but all are far from one. Note that interpretation changes. Transform the dependent variable to $-\ln (1-y)$. This is helpful if $y$ are all far from zero but some close to one. But this looks a bit unnatural to me, and I would consider the next logistic model instead. The logistic model $\ln \frac{y}{1-y} = \beta_0 + \beta_1 x +u$. This usually helps if there are many $y\simeq 0$ and $y\simeq 1$. Interpretation is done in terms of logits.