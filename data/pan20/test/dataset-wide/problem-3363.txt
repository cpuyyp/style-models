Sometimes technical descriptions are made with technical vocabulary neologisms in order to capture complicated and nuanced concepts which might be ungainly to convey correctly with simpler non-technical terms. So it may be difficult to come up with a description of a technical concept using only non-technical terms. A rule in a language is context-free if the rule (affecting some words together) does not use text surrounding those words in order to apply correctly. For example, suppose you want to make a comparative out of an adjective. Just add '-er': hot -> hotter, tall -> taller. For longer words, you use 'more' first: 'more consistent', 'more independent'. And then for a few words, like 'good' you have a rule exception: 'better'. For all of these, it doesn't matter what the word is, you don't need to know anything else in the sentence to form that comparative: in 'he is good' and 'he is better', the comparative doesn't depend on anything else in that sentence outside of what happens to the adjective. In contrast, one might say that conjugation of verbs is context-sensitive (the alternative to context-free). For example, in the present simple in English, it is 'I say' and 'he says': to get the right ending on the verb, you need to know the context (the pronoun that comes before). The pronoun itself is not part of the thing that changed, but its presence changed the following word. 

This isn't saying that linguistic theory is necessarily useless, just that... maybe... those trained in language theory aren't as good at implementation as maybe others? Anyway, I suspect that once a lot of accuracy has been squeezed out of tweaks of the Deep Learning methods mentioned above, there will have to be some use of explicit knowledge of language specifics to get things 'human'. For example, current statistical machine translation has trouble with presumably simple things like getting gender right, which is hardly deep linguistic theory Douglas Hofstadter has a recent Atlantic article about the shortcomings of purely statistical methods. His simple example about gender translation is: 

There are qualitative and quantitative measures for 'distance'. Qualitatively, many languages are easier to compare simply by know something about their family tree, which is implicitly recognizable (if one is lucky enough to know so many languages so well), by comparing vocabulary lists, syntax rules/systems, and phonology. The more common elements the closer they are. Mandarin and Cantonese are not mutually intelligible, but both have rich tonal systems, similar isolating grammars, and fairly simple phonological rules to map cognate words. Likewise French and Italian have a set of comparisons (similar conjugation/gender rules) and fairly regular rules mapping cognates. And the two sets of differences, those between Mandarin/Cantonese, and those between French/Spanish are of the same scale. The difference between French and German is so much larger than between Mandarin/Cantonese that one would be hard pressed to say they are of similar distance. Quantitatively, one can define distance mathematically, by collecting a set of quantitative features in each language, and then defining a distance function that calculates an exact positive real number out of a combination of all the differences of these features. Then one can do a simple numerical comparison, the distance between X and Y and the distance between Z and W. One can go further and use a clustering algorithm to create a formal family tree of all languages of concern. Presumably, the utility of a distance function would be to predict the difficulty in language learning or translation (machine or human); the further the distance, the more changes would need to be navigated. At some point, a distance will be meaningless; the difference between French and Swahili is just too much to settle qualitatively or quantitatively to then bother comparing with some other distance. 

Categorical means yes vs no or one of small set of things like red/green/blue. Phonemic features of consonants are usually like this, place of articulation, stop vs fricative, voiced vs unvoiced. Continuous means on a smooth scale like frequency or a large approximate set of things (like time in milliseconds). Perception works through sensor devices, special nerve cells like rods and cones in the eye or hair cells in the cochlea. The action of these sensors is limited to firing once past a threshold and then taking some (very short time) to reset so that it can fire again. Also there are many cells of the same kind so that they may not fire all at the same time but if exposed to an extended source of one particular kind of energy can seem to, as a set, fire continuously. A single firing is a sign of a category, the number of firings (by multiple cells, or by a single cell over time) approximates continuity. Perception of timing is not done in these primary sense cells, but further down the neural pathway. However the same principles apply to these dependent cells. So despite the inherently categorical (binary) nature of a single cell, a bunch combined together or analyzed over time produce a continuous measure. Voice Onset Time, in terms of production, is the time from the start of some articulation and the start of vibration of vocal cords at some generic frequency. Timing is a continuous parameter, there's no threshold. But consonant phonemes in human language are perceived categorically, meaning there is some VOT value that acts as a threshold, that is there is no slow statistical increase in probability of a positive voicing feature, rather a sudden jump from always considered yes to always considered no. When the VOT is less than this threshold then the voicing feature is almost always considered on, when greater then off, and only in the smallest neighborhood of the threshold is it questionably on or off. 

Words and concepts don't necessarily correspond for social reasons. The word 'etymology' is about individual word histories, which is definitely a part of linguistics. But that doesn't mean there is a sub-section of academic linguistics departments with faculty members who primarily call themselves 'etymologists'. Dictionary makers might have specialists in etymology (which would require all sorts of linguistics and particular language training). The people who 'do' etymologies might have more training in multiple specific languages. It doesn't seem likely that they would be a member of a Linguistics department's faculty (at least not in the US). Philology might be a label for a faculty that culturally etymology falls under elsewhere. 

Clarity of a convincing argument or in transfer of information has been a major desired goal in Western philosophy forever, and departures from it have been seen primarily as negative. The use of amphiboly, deliberate or not, is usually seen as a fallacy or error. So I feel that any intended use of vagueness would be considered misleading and manipulative, and so not likely in a positive light. The above references don't do their analyses from an obviously tendentious, negative direction, so they may suffice for you, even though they don't attempt to make something explicitly positive about vagueness. 

Thought is not necessarily linguistic. It often feels that way because communicating our thoughts is usually linguistic, and self-reflection is an attempt at making whatever is floating around in our heads communicable. The fact that you are reading this now means you're in the middle of communicating, which biases you to think that your thoughts are words. Most of our thoughts are things like to make dinner tonight, I'll have to start boiling the potatoes early and at the last minute do I sautee the greens and garlic, oh I'll need to ask someone to bring home lemons, which reminds me of that trip to Florida last year. I'm using words to express this, but the thoughts were all a sequence of nebulous ideas. Not that thoughts involved in playing chess or rock paper scissors are generic thoughts, but you're not literally speaking in your head "If they do this then I'll do that, but then I should do this now so that...". That all said, yes, there does seem to be a 'narrator' to the 'blooming buzzing confusion' of senses, desires, and expectations we're thinking. What does that narrator speak? Well, frankly, you should tell us. We don't know what your dream is unless you show us. We don't know your inner monolog unless you tell us. Language is arbitrary, meaning we can lie, so there is some lack of total trust, but there are generalities. In the shortest way to say it possible, your first language will most likely be your inner narrator your whole life (this is obvious) but if your second language comes in early enough and is what you use 99% of the time, the second one may become the narrator. It all depends on how soon and how much. And similarly for more than two languages. Many people get along fluently their whole lives in a second language while fully being conscious of a process of mentally translating (without ever taking the 'conscious' leap of thinking directly in the other language). But a 'ruling' language that controls the others is a thin metaphor. One may have one internal 'pre-verbalized' practice language or more than one. Different people might do things differently: one may have an internal narrator that is always one language whatever is being spoken, others may have language for talking with their parents and another for business or school. That said, the external evidence is that things like numbers and expletives tend to be from the first language (or rather they're the last to be replaced by a second language). So you'll have to wait to your own later stages of dementia to have others confirm what you may have claimed all a long whether you've entirely converted your inner monolog to a newer language. 

This is fascinating question about learning word semantics. Word meanings, the connection between the utterance and the mental concept, are related in many ways. A lot of this learning comes from the pragmatic side. One of these is methods is learning from negative evidence (you didn't say this other word but instead this word, so you maybe mean something different). Nothing is definite or immediate; little things happen one by one and need reinforcement. Even if it seems as though an immediate inference is made, that's is probably after the accumulation of many small instances and the last one clinches it. So your explanation does seem reasonable. 

I can't seem to find a language where there is an article that comes after a noun -and- is obviously separable. (as an aside, it is rare that an article comes before the noun and is not separable). 

For a and c it is complex and you are a very special case. For b, it's just about difficulties in learning an additional language as an adult. Yes, you may very well have more than one L1. You state that you have no accent in English (most likely no accent because of when you started immersion, but have others confirmed that?). Do you have an no accent in Hebrew or Russian? Do you make grammar mistakes like natives in those? Then those are probably both L1's also. Whether you're labeled L1 for Hebrew or Russian or English just says what kind troubles you will have learning more of the language. If you're L1 then there's no accent, basic grammar is no problem, elementary vocab too, and you will learn more complicated grammar quickly. L2 you may never get a perfect accent, and native-like grammar (with mistakes like a native) will require lots of practice (but can be achieved). 

In lexical semantics, a lot of meaning in individual words, the concept behind the utterance, is captured in ontological relations: is-like for synonymy, is-a for a hypernym hierarchy. But this doesn't capture a lot of the incidental nuances. Simply relating one word to another doesn't include all the non-word connotations; not every concept has a single word to label it. In analogy with phonology, the area of generative semantics sometimes uses semantic features, often binary, to label lexical items. For example, one might label 'bucket' and 'pail' as follows: - 'bucket' - [+ receptacle] [+ wide open top] [+ big] [+ (opt) metal] - 'pail' - [+ receptacle] [+ wide open top] [- big] [+ (opt) milk] [+ (opt) toy] (and one can see how a hypernym relation can be extracted from analyzing the containment of features) Most dictionaries attempt some connections, usually synonyms and antonyms. But I haven't seen anything that is binary, except maybe if the word is very distinctive, a better dictionary might label the subculture it is specific too ('chemistry', 'vulgar', 'archaic'). Are there any published dictionaries, print or online, that attempt to do a binary feature analysis beyond toy examples? Assuming none, is this method simply an old academic trend that dies out long ago out of fashion or out of impracticality? 

which only gets the French agreement right on the last pair. In the other direction though is this article about how current methods are pushing up against the limits of what they can do without linguistics. At first it shows how little linguistics is actually used, and then goes on to claim that linguistic models may be added to translators soon. This is all related to the rule based vs statistical controversy in AI.