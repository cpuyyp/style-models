This is almost certainly an issue with the formatting of your public keys in the authorized_keys file. Check that there are no line breaks in the public keys (the key should all be on one line), and that there are no omitted characters. 

Your problem seems to be that you have have given the third party a copy of the private key that is used to access the server. You could just disable access for that private key, but it would seem that other people are also using it, so your Network Administrator doesn't want to do that. You really should have asked the 3rd party to generate their own key pair and provide you with the public key. You could then have permitted that for a short period of time and removed it, which would not have impacted on anyone else. There isn't much you can do. One option is to restrict access to the server by using the limit features of the authorized_keys files. If you know the ip addresses from which access is allowed, you can configure OpenSSH to only allow access from those addresses, even if the correct key is provided. eg $URL$ You could also experiment with the "Match" options in sshd_config to prevent access from users who match certain criteria. Bottom line: never share private keys. Every user should be using their own key pair. 

If your server doesn't support SNI, or you're concerned about browsers that don't, you'll need a Cert per domain and you'll need an ip address per domain. Prior to SNI, when a client made a https connection, the server had to accept it and decrypt it before it knew that http host the client wanted to connect to. In that instance, if the server was serving multiple http hosts on the same ip address, the server would always assume that the client wanted to connect to the first ordered host in the configuration. If the certificate served in response didn't match the host requested by the client, the browser would generate an error. Hence, you had to run different domains on different ip addresses. Where SNI is enabled, the server presents all available certs to the client request, and the client can match its host request to the correct cert if its exists, meaning a browser error is only generated if none of the certs match. This allows you have host multiple https hosts on the same ip address. You will still however need an https cert per domain (free or commercial). 

DNS in a VPC works as follows: When an instance starts, it registers with the DNS forwarder in the subnet. Thereafter, when any other instance in the VPC queries any hostname for that instance, it will return the private ip. When the instance is stopped, the record with the forwarder will expire. Thereafter, when any other instance in the VPC queries any hostname for that instance, it will return the private ip. This is because the local DNS forwarder doesn't have a record and forwards the request to the primary DNS servers for Amazon. 

If this relates to the execution of jobs based on a cron schedule (ie Build Periodically), you can set you Time Zone in the cron schedule on a per job basis: 

Apache httpd sends request to worker Worker doesn't respond, or responds with HTTP status which triggers failover,and puts member into ERR state Apache httpd starts retry timer (default 60 secs) and doesn't send any more requests until retry timer expires When retry timer expires, go back to Step 1 in sequence 

Which does install and set Ruby 2.3.1 as the default, the "bundle install" step just times out when the Docker image is building. When I open a shell to the container, and reset Ruby to 2.2.5, bundle install works. Is it possible to use this image with Ruby 2.3.1 instead of Ruby 2.2.5? 

OK. I can explain this. Apache mod_proxy_balancer doesn't have its own independent healthcheck mechanism. The state of Balancer Members (workers) is determined based on outcome of actual forwarded users requests. Sequence is as follows: 

I figured this out. Puppet runs as root. That means that for vcsrepo using mercurial, the root user needs to trust the user who owns the .hgrc file in the repo being updated. To establish this trust, you add 

My colleague and I use Ansible to manage a group of servers. We use the .ansible.cfg file in our local home directories to setup our local environments and keep our playbooks in a git repo. When authenticating to servers, I use user1, and he uses user2. 95% of of our servers have these accounts, but historically reasons, a few servers only have a "user" account. We're using host_vars to set the remote_user variable for the minority of servers in question. However, in our playbooks, we generally user "all" to stipulate what servers we want to hit, and use the --limit parameter on the command line to specify exactly which servers should get the update. Our server farm is a legacy of mis-mash poorly engineered servers that have to be kept online until they are retired in a few years, and we've found that this approach best suits our needs. Our issue is that our remote_user parameter is set in our .ansible.cfg file, where it is exposed as environment variable rather than a script variable. That means if our task contains: 

I am implementing a reverse http proxy: proxy.example.com This will forward requests to servers based on URI: proxy.example.com/server1 -> server1.example.com When a user requests proxy.example.com/server1, server1 sends a programatically generated (Ruby Devise Gem) 302 response with the following "Location" value: proxy.example.com/users/sign_in I need this to be: proxy.example.com/server1/users/sign_in I have implemented the following config in Apache: 

I've inherited responsibility for a 400GB MySQL database that is hosted in Amazon RDS. The DB serves an application that relies intensively on DB operations, so version releases frequently involve running ALTER statements on very large tables. These updates take over 24 hours to run. Typically, we make a snapshot of the production RDS instance, restore it to a temporary instance, run the migration on that and then rename it so that the apps start connecting to it as the Production DB. The obvious difficulty in this is that during the 24+ hour upgrade, we have to keep both DBs in sync, which means data has to be loaded to the application twice, which is a major headache. I'm convinced there is a better way to do this (albeit I'm aware of the limitations of RDS), but I can't see find a solution. The DB can't be live while it is being altered, but the alteration takes forever. Anyone have similarly experience, or should I just learn to live with it? 

I am a long time AWS user but need to trial something on MS Azure. I have created a Free Trial subscription. I then created a Namespace in the Default Directory, and added the user to that. I want this user to be able to use Azure services as a Global Administrator. When I login as that user, I get a splash that says the user has no active subscriptions. Is this something to do with the Free Trial (ie it only allows one user)? I can see that "Rights Management" is disabled in the Default Directory, and when I try to enable it, I am told "Cannot detect Rights Management (RMS) support for Default Directory. To use RMS with this tenant, you must have a subscription that supports RMS." 

Turns out this is pretty straightforward (it not very obvious in the documentation). Rather than use source_profile, you use credential_source: 

My retry value is 60 secs (default). The reason I am seeing multiple log entries is that my Apache httpd Balancer is configured with multiple balancers, each with its own independent retry timer. As such, depending on application activity, the retry timers are being reset arbitrarily, and being tested arbitrarily, which explains the non-uniform distribution of worker status updates in the log. 

I don't think Skype is a good solution in a corporate environment with anything more than 20 users. Large corporations will use Port Address Translation at their Internet gateway, which Skype hates, because it can't easily establish P2P UDP connections. Instead, it routes traffic via other Skype computers (outside the corporate environment) called relay nodes, which can establish P2P UDP connections. However, to limit the impact on those nodes, Skype throttles bandwidth usage, so the quality of your call will suffer regardless of the amount of bandwidth that is available to your network. A solution to this would be to force all your Skype traffic via a SOCKS or HTTPS proxy that have a dedicated native NAT translation (ie it doesn't use PAT), but here's the thing: Skype will only use the proxy if there is no other route to the Internet, so even if you configure your Skype client to use a Proxy, it will ignore it! (WAT?) There are ways and means around this by editing registry files and distributing XML files to Mac OS etc, but in a large organisation, this isn't practical. More here: $URL$ 

I have a Rails app that works fine in ElasticBeanstalk. I am now trying to run in in the Docker Phusion Passenger Container. My Dockerfile is pretty simple: 

You can't use backup_add_prefix by itself (the docs suggest you can). You can only use this parameter in conjunction with backup_to_bucket Make sure the IAM account/role you are using to interface with S3 has Write permissions for the buckets you are using (other Logstash can't delete/rename objects). 

In addition to this, you also need to set the COMPOSE_HTTP_TIMEOUT environment variable before you run docker-compose, as otherwise, docker-compose will time out before elasticsearch can start. You should set the value to something greater than you set for ES_CONNECT_RETRY eg 

I've discovered after much suffering that the best way to run elasticsearch on a single server is to change the default setting of: 

The logstash documentation indicates that you can collapse the multiple indented lines in a Java stacktrace log entry into a single event using the multiline codec: $URL$ 

To see what the remaining TTL on the NS record was, but now I understand that this NS record is the NS record for subdomains in the zone, and not the NS record that emanates from the root servers, which is the one that ultimately determines to which name server the query will be sent. I tested this by setting up a test record in the zone in each of the providers: 

as per: $URL$ "This directive lets Apache adjust the URL in the Location, Content-Location and URI headers on HTTP redirect responses. This is essential when Apache is used as a reverse proxy (or gateway) to avoid bypassing the reverse proxy because of HTTP redirects on the backend servers which stay behind the reverse proxy." But the Location header that is being returned by server1 is still: proxy.example.com/users/sign_in Is there something wrong with my config? thx 

This is based on logstash finding an indent at the start of the line and combining that with the previous line. However, the logstash documentation is the only place where I can find a reference to this. The general user community seems to be using elaborate grok filters to achieve the same effect. I've tried the basic indentation pattern provided by logstash, but it doesn't work. Has anyone else managed to get this working by matching the indentation pattern?