Every such decision problem is computable, even in the harder version of the problem, assuming that the transition probabilities are, say, fixed rational numbers. A deterministic algorithm can calculate the probability distribution on the set of states of this stochastic TM after each $t$ time steps, and then step through $t$ until the probability of halt at either yes or no exceeds $1/2$. 

A more efficient version of Doug's lower bound: We get the same probability $P(n)$ if we assume that the longest edge has length 1. In general the longest edge has some length $\alpha$, the other edges are then independent and uniform in $[0,\alpha]$, and then you can rescale the other edges by $1/\alpha$. In fact, without loss of generality, only 1 edge has length $\alpha$ and you can prespecify which one. So suppose the edge $\{1,2\}$ has length 1. Then there are $n-2$ triangles that contain that edge, and with independent probability $1/2$ each one is good. These $n-2$ tests are also independent of whether the remaining graph on $n-2$ vertices is good, and that probability is $P(n-2)$. In this calculation, we're ignoring the triangles that only uses one vertex from $\{1,2\}$. So we obtain the recurrence $$P(n) \le 2^{2-n} P(n-2).$$ Thus $$\frac{-n(n-1)}{2} \le \log_2 P(n) \le -\lfloor \frac{n}{2} \rfloor \lfloor \frac{n-1}2 \rfloor,$$ using also Doug's elementary bound that the graph is good if all edges lengths are at least $1/2$. 

It's clearly an elementary oversight that, on the other hand, doesn't matter for the real development of the material. Yes, the chain $c$ ought to be at least $C^1$ for the pullback to be continuous and integrable, and you might as well make it $C^\infty$. You need a smooth approximation lemma that singular (co)homology with smooth chains is the same as singular (co)homology with continuous chains. It is in the spirit of simplicial approximation, which is used to establish isomorphism between combinatorial simplicial (co)homology and singular simplicial (co)homology. You can also use piecewise-smooth chains as a mutual generalization of PL chains and smooth chains. To see that you have to restrict the chains somehow, you can let $\omega$ be the area form in the plane, and you can use the fact that there is a topological circle in the plane with non-zero area. You can fill in the circle with a 2-chain $c$, and there is no reasonable way to define the pairing $\langle c,\omega \rangle$. This pathology is impossible if both $\omega$ and $c$ are closed, but that is sidestepping the issue, because you can only prove that by setting up enough machinery with smooth restrictions. 

VA asked for more details about the indecomposable modules, which is a fair request because I was quite cryptic about the relation. In particular, I use non-standard indexing in my own thinking about this. Unfortunately, I'm not sure that I can convert to standard indexing without making a mistake, so I won't convert. But I did change one thing above: I fixed the filtration degrees so that they are correct. Suppose that $C = (C_n)$ is a complex of filtered vector spaces. Say that the filtration is increasing and indexed by $\mathbb{Z}_\ge 0$. The complex has two degrees: The chain degree $n$ and the filtration degree $k$. Suppose that $\partial$ is a differential with chain degree $-1$ and filtration degree $0$. (This is where the numbering begins to be non-standard, although it makes sense from the point of view of quiver representations.) Then the theorem is that over a field, the two kinds of indecomposable complexes are those listed above, where the term $k^{(i)}$ has chain degree $n$, and the term $k^{(j)}$ (if present) has chain degree $n-1$. To be precise about what $k^{(i)}$ means, it is a filtration of the field $k$ in which the degree $j$ subspace is $0$ when $j < i$ and $k$ when $j \ge i$. The page $E^0$ is the associated graded complex. The page $E^r$ has a differential of degree $(-1,-r)$. When $r = i-j$, the differential $\partial^r$ of the first type of indecomposable complex connects the "tip" of $k^{(i)}$ to the "tip" of $k^{(j)}$ and kills them both on the next page. The other kind of indecomposable complex has a vanishing differential, so the induced differential on every page also vanishes. 

Anton asks for more problems in topology and algebraic geometry. One issue is that the concept of a "trick" is treated differently in these two areas than in differential geometry. In topology, not quite as many ideas are called "tricks"; they are sometimes named after people and co-opted as material, e.g., the Alexander trick and the Whitney trick. In algebraic geometry, tricks are sometimes regarded as suspect; they are sometimes taken as a reason to reorganize definitions to either again co-opt the trick or avoid it outright. Still, a problem based on the Alexander trick could be at a good level for this problem list. Problem: Prove that space of tame knots, meaning piecewise-linear embeddings $f:S^1 \to \mathbb{R}^3$, is connected in the $C^0$ topology on functions $f$. 

The question seems to be groping for a fancy, specific answer when, in my view, the most important connection is relatively basic and general. In mathematics, as you say, you have symmetric monoidal categories. A word in a symmetric monoidal category has a set of inputs and a set of outputs and a certain type of labelled graph in between them. Symmetric monoidal categories are used in many situations. They were inspired by multilinear algebra with tensors; I gather that they are also important in linear logic. In theoretical computer science, there is the notion of a circuit, and the closely related notion of a straight-line program. A circuit is also a labelled acyclic graph with inputs and outputs. Computer scientists began with boolean circuits, but these days there are a lot of study of arithmetic circuits over any ring, as well as quantum circuits composed of unitary gates. A precursor to quantum circuits is the clever definition of reversible and conservative circuits of Toffoli and Fredkin. In the paper cited, they have in mind a dynamical interpretation of graphs that don't have to be acyclic. However, the case of what they did that has had the most influence is finite, acyclic graphs. In this case, their essential insight is that you can have a perfectly good model of boolean circuits based on permutations of $\{0,1\}^n$ rather than functions from $\{0,1\}^a$ to $\{0,1\}^b$. So the point is that the math concept of monoidal categories, which are broadly important, is basically the same as the CS concept of circuits, which are also broadly important. Every monoidal category, maybe together with some distinguished generators, gives you a new type of circuit, so that you can then ask circuit complexity questions. A particular monoidal category may or may not yield interesting circuit questions, and two different monoidal categories might yield equivalent circuit questions. Nonetheless, there are many interesting different cases. Actually, not-necessarily-acyclic graphs can also be interpreted as words in symmetric monoidal categories that are also "closed" or "pivotal". There are several ways to interpret tensor words of this type as circuits; one of these ways (for some types of words) is as dynamical circuits. Linear logic and conservative logic are both called "logic", and they both use monoidal categories. (Also, as Peter Shor mentioned, linear logic is partly inspired by quantum probability, while conservative logic is used in quantum computation.) Other than that, they don't look particularly related to me. 

The more general problem, as Ben Green says, is to cover every $t$-tuple by subsets of size $k$ in a set of size $v$. (These are more standard variables.) This is called a covering design. (It's not the same as a $(v,k,t)-\lambda$-design, because in those each tuple is covered the same number of times, rather than at least once.) Although there is no theorem to this effect, finding covering designs looks like an open-ended problem that will never be completely solved, even though there are many good ideas and it can be solved in some cases. In this respect it is like the problem of finding sphere packings, error-correcting codes, graphs with various properties, etc. Non-rigorously the La Jolla covering repository shows you that it looks that way, because many of the competitive covering designs, even for $t=2$, were found by methods such as simulated annealing and even "private tools". As Ben also mentions, if $k$ and $t$ are fixed, then Rodl showed that the covering density converges to 1 as $v \to \infty$. In this limit covering designs are equivalent to packing designs up to an $o(1)$ fraction of slop. Rodl's technique is the "Rodl nibble", in which small clusters of blocks are added incrementally. However, it was discovered (see [arXiv:math/9511224]) that you might as well let a Rodl nibble be just a single block. This simplifies Rodl's construction to the random greedy algorithm. There is also a non-rigorous model that correctly predicts the rate at which the random greedy algorithm converges to density 1. There has been a lot of interesting work to close the gap between rigorous bounds on the random greedy algorithm, and the non-rigorous model of the algorithm. If $k$ grows along with $v$, then there is no known asymptotic $1+o(1)$ bound even for $t=2$. (Unless $k$ grows so slowly that the arguments for $k$ fixed still work.) The volume lower bound, just the fact that the covering density is at least 1, can be improved to the Schonheim bound. (If $k$ is fixed then the Schonheim bound is the same as the volume bound up to $1+o(1)$.) It's easy to believe that the Schonheim bound is close to the truth for a wide range of parameters, but no such thing is proven except for the smallest values of $k$. For instance, when $k=3$, the Schonheim bound is always sharp. 

The shortest and most amazing proof (in my opinion) is by Steiner symmetrization around half of a great circle. Given $A$, and given a half great circle $\gamma$, rotate the sphere so that $\gamma$ is a meridian arc. Then for each latitude sphere $H$, you can replace $A \cap H$ by the spherical cap in $H$ centered at $H \cap \gamma$. Let $A'$ be the result. Then it is not hard to show that $|A'_s| \le |A_s|$ for all $s > 0$; in fact even each $|A'_s \cap H| \le |A_s \cap H|$. And you can show that you can pick a sequence of half great circles such that $A$ converges to $B$ under symmetrization, and that some of the inequalities are strict unless $A$ is congruent to $B$. Of course this is just an outline, but it is an accurate summary (I hope) of the Steiner symmetrization argument. It also works in Euclidean or hyperbolic space using a line rather than half of a line. 

Every smooth manifold has a smooth triangulation, which yields a pseudofunctor from the category of smooth manifolds to the category of PL manifolds. (There is no actual functor; that would be crazy.) If two smooth manifolds are PL isomorphic, then the answer is yes. You can start with the PL isomorphism, and then build a homeomorphism that follows it and that has the property that all derivatives vanish in all directions perpendicular to every simplex. You can build the homeomorphism by induction from the $k$-skeleton to the $(k+1)$-skeleton using bump functions. The PL Poincaré conjecture is true in dimensions other than 4, so all exotic spheres in the same dimension $n \ge 5$ are PL homeomorphic. (High-dimensional examples of exotic spheres start in dimension 7, it was calculated.) In dimension 4, by contrast, every PL manifold has a unique smooth structure, and it is not known whether there are any exotic spheres. On the other hand, if the manifolds are homeomorphic but not even PL homeomorphic, then I don't know. It is known that every manifold of dimension $n \ge 5$ has a unique Lipschitz structure, but I do not know a Lipschitz version of the above argument. On the positive side, passing from smooth to Lipschitz is an actual functor, so the answer to a modified question, is there a Lipschitz-smooth homeomorphism, is yes, and you can even make it bi-Lipschitz. 

I decided to look at the problem this way: Among all $n^{\pi(n)}$ homomorphisms from $P_n$ to $\mathbb{Z}/n$, can we heuristically estimate the number that are a bijection when restricted to $\{1,2,\ldots,n\}$? Let's say that the restriction of such a homomorphism is not particularly more likely or less likely to be a bijection than a random function. The latter probability is $n!/n^n$, so we can expect roughly $n!\cdot n^{\pi(n)-n}$ solutions. We can now take a logarithm and apply Stirling's approximation and a sufficiently careful version of the prime number theorem, $\pi(n) \approx \text{Li}(n)$. The answer is that there is plenty of entropy to have solutions; the predicted log of the number of solutions is roughly $n/(\ln n)$. This heuristic can be checked for small $n$. If the heuristic is reliable, then there should be solutions for all $n$. It would be nice to have an effective construction of such a homomorphism for all $n$, but my impression is that the other answers so far don't find one. Note the last remark in Victor's answer: "I have a truly marvelous proof of this proposition, but the margins of MO are too thin to contain it."