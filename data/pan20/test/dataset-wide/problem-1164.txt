Recently, we've done some optimizations to the functions and now the BI Manager has tasked me with a request to compare the result of the 2 functions as below making sure the row counts match, along with the exact values of each field. In the same request he is expecting this to be a while loop that will loop all the YTD ranges dating back 20 years. (not sure if this is really practical in order to test the validity). However, the while loop is not a mandate for the same. Is there a way to compare the result of the 2 functions without hard coding the ranges and automating the same like for e.g declaring a variable and then based on the date range calculating the next date range etc without the user intervention. Last but not the least a script that would validate there are no differences in the results of both the functions which means that the optimization did work. I am thinking of using except clause but I am not sure if it would work with functions. Also, if that won't work what else would be a possible way of doing it. I would really appreciate for all the help. It is a big impact for the company if I could complete this task. However, I am not even sure if this is something that should be even handled by a DBA. Functions Original Function: report.fnReportDealCore_original(@StartDate, @EndDate, -1) New Optimized Function: report.fnReportDealCore(@StartDate, @EndDate, -1) Example of Run Dates dating back 20 years 

I have a SQL job that runs an SSIS package and it basically pulls records from staging table and dumps it into the master table. It used to take around 8 hrs. to run but in these last 10 days or so the run duration has increased and I would like to know if there is a way I can find out where the job is spending more time and what it is waiting on either running a profiler trace or running a tsql script inside a job to capture anything useful. It runs every day except Sat at 11p est. 

My manager has asked me to work upon a request. I am not sure if it is reasonable or weird but he wants me to setup an alert based on the "waiting task" that we see in the activity monitor. Basically, if the value in the "waiting task" goes above 10 then we need to be alerted upon. I am not sure as to how I would replicate the same information that we see in Activity monitor as this seems to be live data. Closest DMV and the column I could think of is "waiting_tasks_count" in sys.dm_os_wait_stats. However, this does not seem to be in line with what he is looking for and the column consists of cumulative values over a period of time since last time SQL was restarted. Can someone shed some light on where I can find a query or a counter that I can alert upon? I have thought about a query at this time but I think I can't calculate the delta correctly it always shows me 1. Can someone help me out exactly where I am going wrong here. 

Not being familiar with codeigniter, I am doing this first in SQL, and then make a guess on how to translate it back into codeigniter: I translated your query to SQL first" 

I went through the PREPARE and EXECUTE, because LIMIT doesn't work directly with a variable, this is a workaround. Now put it back into codeigniter: function reports_median() { 

I have been asked for recommendation on the optimal RDBMS for a large amount of data. The largest table would contain 2TB of data. My own largest table is only 400GB (mysql, performs very well). The individual rows are going to be short, no blobs/etc, just a lookup table. Is PostgreSQL nowadays performing better than MySQL? Can I ask DBAs with tables in this order of magnitude for their experience? Oracle might be considered as well. The available hardware is probably a standard linux box with about 64G ram. 

From the MySQL documentation: The NULL value means “no data.” NULL can be written in any lettercase. A synonym is \N (case sensitive). $URL$ 

Perhaps naming the columns "hired" and "fired" instead "start_date","end_date" would be more accurate, because a person in vacation hasn't been fired, and indeed is still employed. Same with "firereason", which is misleading, a simple "reason" would be more accurate. 

Dividing by 365 is slightly incorrect, because it doesn't take the leap years into account. Another problem with this answer is, that overlapping hire/fire periods are ignored. For example, if an employee was employed at 50% time from Aug to Dec, and additionally at 30% time just in Dec, the query would fail - but you don't take percentage into account anyway, so I assumed that the employment times don't overlap. This has been confirmed by the original poster. For more date functions, review the documentation: $URL$ The different date/time functions will also help you answer your other SQL question. 

Why do you want to partition? Is you performance not good, or do you have other reasons? I don't see an index for the lastupdatetime column, and you wrote that you query for it a lot. An alternative to partitioning could be creating tables for each year, and use merge tables to access the subsets you need. Would have just left a comment, alas, not enough reputation. 

$URL$ With 10,000, you are at the maximum number of connections, which is probably your problem, as the allowed memory usage for an individual connection then gets multiplied by 10,000. You can decrease the number of connections needed by optimizing your queries. Frequently, query optimization ( run explain, make sure you have properly indexed your tables) solve issues with overload. 

The other answer and the commenters explained the reason for the having behavior. I was wondering what you actually wanted to do, is it this: 

As you see from the explain output, mysql does a full table scan on table t4 (rows: 2374015, keys: NULL). Adding an index just for networkAccessMode will probably help - depending on the value diversity in networkAccessMode. How does the explain change when you don't specify the indices explicitly? 

1/1/1997 – 1/31/2997 2/1/1997 – 2/28/1997 3/1/1997 – 3/31/1997 …. 1/1/1998 – 1/31/1998 2/1/1988 – 2/28/1998 … Through 9/30/2017 

One of my client is having issues with one of the jobs failing every weekend to perform index maintenance on a couple of databases. It is a SQL job which uses maintenance plans underneath. Okay, so the maintenance plans consist of check database integrity task, followed by 3 t-sql task in sequence which runs the same script but for different databases (code provided below) and finally runs a reorg task and then update of statistics. I know this seems a bit weird the way they are running things at this time but I will change it moving forward. Presently, I am stuck with this job completing the maintenance check but failing to execute the t-sql task with the following error. Executing the query "USE XMain; declare @frag_Temp as Table ( ..." failed with the following error: "Incorrect syntax near '-'. Changed database context to 'XMain'.". Possible failure reasons: Problems with the query, "ResultSet" property not set correctly, parameters not set correctly, or connection not established correctly. 

I've been working on this query handed to me by a developer who wanted to reduce the query execution time even further. Currently, it takes almost around 40 secs and outputs 262K records. Based on the actual execution the query cost is more towards the clustered index scan on "CacheClients". However, I am not sure if adding a non clustered covering index would benefit it. Also, I see a table scan at the extreme bottom right of the plan and as per the developer they were testing with an index on that table but they weren't able to figure out an appropriate one that can fit the criteria which is basically they would join on the id fields and run queries based on date period. 

Based on the discussions that we had internally we initially decided to move with a SQL FCI for our non critical databases so that we would have hardware level redundancy. However, now the issue arises where in the project manager says that one of their azure VM's are in main campus and the other VM is in the DR site...so shared storage is not possible. Basically, as per him it would defeat the purpose of having them in separate locations on separate hardware..so he is overruling SQL FCI as an option. Now, the requirement is to have 2 SQL Servers with their own storage that we can fail over in an emergency or for maintenance. My question at this point is why we can't have a shared storage between 2 different sites? Also is it feasible and recommended to have more than 30+ databases into AlwaysOn in case if we want to explore that route. Is there any other option if not that. 

All, I would like to know what path should I chose in order to achieve MCSE for SQL 2016. I already completed MCSA certification for SQL 2012/2014 and would like to upgrade it to MCSE SQL 2016/2017. Kindly, let me know what is the best approach for this. Is there any direct path or do I need to take n number of exams to get that certification. Also, would like to know what resources or links you'll can share that can help me possibly get through to that certification...most likely in first attempt. I only would choose one exam but it should be more focused towards honing my existing DBA skills. $URL$ Thanks