Instead of looking at temporal statements as playing out in a time-indexed classical logic, it is more realistic to look at temporal logic as a variety of (time-indexed) modal logic. I disagree that your statement most naturally breaks down to "If it is tomorrow, it is raining." There is no world in which such statements have any purpose. What is missing is not logical segmentation of this sort. What is missing is the mood or modality in which you are putting forward the otherwise meaningless statement. One simply cannot meaningfully say "Tomorrow, it will rain", where the will is an expression only of the future tense of 'is'. You cannot ever know this fact, so it is not a realistic approach to the use of language. Instead, here, 'will' is a modal verb, the future tense, not directly of 'is', but of 'must', the complement of 'can', indicating your prediction or belief, and not an indicative connector. You mean that for some refinement of 'can', it cannot fail to rain. But in that translation, 'some refinement' is very important. "If the laws of physics are correct, it cannot fail to rain tomorrow" is very far from "If the wind does not shift unexpectedly, it cannot fail to rain tomorrow." or "If the way my knee aches right now is just the right way that faithfully represents a given barometric trend, and I have correctly assessed the degree of the pain, it cannot fail to rain tomorrow." Or even "(Without external basis) I believe that it will rain tomorrow." There is an intermediate position between modal and ordinary logic that considers all modal propositions vacuously true. But only until enough context is supplied. One has enough context once the premises are supplemented enough to express the mode asserted at least well enough to give a notion of probability to the statement. For some notion of 'can' -- in which you personally simply cannot every be wrong -- it is surely true that whatever you say, including 'It will rain tomorrow' is true. That world accords with the non-modal world where the statement is vacuously true. But if you live in that world you are megalomaniacally psychotic. Instead, all of us are supposed to guess by context the particular refinement of the meaning of 'can' involved in making sense of your statement. That refinement is made up of a bunch of premises, the likelihood of each of which we can assess. 

You can claim the sleeping are in some way conscious, but in a vegetative body, one which has no alpha waves, and which we would therefore declare brain dead, all of the machinery you allude to is still active and generally not compromised. It also has all that history. So no part of this explains consciousness. If I return the body to a functioning state, consciousness may or may not return, and that consciousness may or may not see itself as a continuation of the previous consciousness. Since consciousness can cease or resume for reasons unrelated to the ongoing life processes, looking back across the generations of life that culminated in this one does not reduce out whatever other factors are needed for consciousness, so that lead is a red herring. The direction we need to look to find whatever information is necessary for consciousness must include spreading outward in the present as well as, or instead of, back in evolutionary and developmental time. I would vote for 'instead of', on the basis of a common science-fiction thought-experiment. If I constructed a functioning body identical to my own, I may or may not be able to capture everything that maintains my consciousness in the duplication process. But if the copy is exact enough, it is likely we can do something to that physiologically duplicate body to provide what it needs to become conscious in some limited sense. So I would argue that no biological history is truly necessary to produce one consciousness from another, if we can imitate enough aspects of the present state. Though, as the brain-death example shows, those aspects of the present state must involve something more than my ongoing life processes or their biological sources. 

For any subjective idealist with a God, wouldn't He have to be such? There is no point in perceiving the results of your own being and intention. So God most likely has no use for senses and would have none. There is also no point in an ideal being having a brain, per se, but if the brain is just where thinking happens, and thinking happens, we could see Him has having such, at least metaphorically. Since this is a majority of idealists historically, and most such idealists then go on to imagine thoughts in the mind of God, your answer would have to be yes, almost always. 

It seems that once you have a notion of dualism and any other-worldly leaning, you automatically get theologies like this within any religion. The subtlety necessary to make sense of these two things together is not available to a lot of people. So to the extent that theologies are philosophies, there are clear answers. In early Christianity, we had Gnosticism, which painted our reality as the construct of a lesser God in defiance of the genuine Divine order. Some forms of neo-Platonism sometimes saw 'participation' in material reality as something that introduces only error and therefore sin, and so is ultimately entirely evil. Gnostic Christianity and Hermetic Neo-Platonism remain in extant forms of Satanism. But this notion is easier to find in older traditions with more thoroughgoing unifying philosophical roots. The most severe forms of Buddhism can go this far: life is suffering, and the best parts of reality are the parts that render it possible to escape from existence. This is arguably harsher than the original intent, but there are certainly real sects of Buddhism that take that form. Likewise, the harsher versions of Hindu notions of maya sometimes paint it as a form of deception which is basically bad and must be conquered. I think that non-theological philosophers who might lean in this direction would not use the concept of evil, because it comes with too much religious baggage. But many tend toward seeing pointlessness or baselessness instead, and clearly mean much the same thing. In that case, despairing nihilists like Schopenhauer or Adorno qualify. 

I would choose a different, non-"exact" science to compare moral axiom sets to -- in fact any one of them. Moralities do not just have to make us feel good, they also have to work. A society in which all of the prevailing moralities did not support social order consistent with human psychology would collapse, and the moral theories would have failed. Math can't fail, it just turns the page and creates a new branch, and the less helpful branches get forgotten and no longer taught. So I would suggest that moral theories are exactly as arbitrary as theories in the (observation-based, non-mathematical) sciences. The basic forms of scientific theories are not set by nature, they are contrived by a social process. They are a consequence of the directions of past decisions, the order in which those decisions got solidified, and the ways in which facts have been rolled into the theory or left out as special cases (q.v. Kuhn). But they are rigidly bound by the requirements that they do, in fact, help us cope with their intended domains. Once they stop doing so, or even stop improving fast enough, we replace them with better ones. Virtue ethics, for instance, given our more cosmopolitan exposure to cultures with different sets of virtues, can only be seen as a meta-theory that does not specify any clear morality. It has lost its claim to be a workable, whole ethical theory because choosing a given set of virtues will unfairly besmirch the actions of obviously good and important historical figures without whom the world would be a worse place. It is a good rule of thumb that needs a real ethics to support it. It can't work anymore, at least on its own. That puts it in a place like Newtonian physics. It works for most of us, but we know just a little bit too much to consider it true at its root. If you choose it as your dominant basic theory, you need adjunct theories for cases of astronomy and particles. 

Actually, risk-aversity is very basic to older philosophies and there is a strong trend against it as those philosophies mature. So it has vast and broad support, but is resisted more and more as we move forward. It is ambiguous whether there are any arguments in its favor that do not then have more powerful contradictions later. Epicureanism favors the lack of fear resulting from removing risk as a separate, special category of good -- the katastemic or static pleasure. Economics and Utilitarianism, Epicurus' more modern form, tends to reject affording absolute good to the sustainable state, and seeks to characterize it in terms of some dynamic tension between risk and reward. The Stoics strongly valued avoiding premature closure, and taking a risk is the most common stereotype for allowing premature closure, as they reached their epitome in Skepticism, they took avoidance of attachment to an outcome as a principle value. Their revival in modern skepticism that leads us through Nietzsche, strongly attacks cowardice and conformity as the ultimately greater risk, putting risk avoidance into a state of complete paradox. Schools like Cynicism and those attending to Heraclitus, who promote risk as a natural part of every aspect of life, which is to be embraced, rather than controlled fare very badly historically, but will not die. They often are resurrected and reshaped in positive directions in the thinking of those who move us forward later. Dialecticism takes this notion of basic instability, and gives it a reason, but encourages sustaining it, if not pushing it toward less stability. Modern moves in philosophy toward a 'therapeutic' direction may be taken as a return to pursuit of stability and limitation of risk. But it is not a strong thread. Also people motivated by the destructiveness of the Western lifestyle are preaching sustainability once more, and speaking in terms of risk-aversity in establishing empathy for the future. So this trend away from safety as a value in itself is not absolute, but it remains strong. 

Yes. The point of intersection of two lines that cross divides both of the lines that intersect there. So deducing anything, however abstruse, from this 'impossibility' is not possible. I cannot make any sense whatsoever out of the rest of the question, but it must not be valid logic because it proceeds from a false premise. 

1) I see no self-reference. 2) Why use negative forms? One could obey both by simply never acting. 3) Sometimes the way someone wants to be treated is not in their best interest long-term, so with the second form it is no more complete than the original. All told, the Golden Rule is helpful, but it is just not enough. Kant tries to complete it, in his Categorical Imperative, by including not just your and their perspective, but all potential perspectives and finding the balance between them. This gives it a sort of completeness -- but it renders the decision intractable in many situations. When it all comes down to it, you need an outside standard of reference: outside you, and those affected, and the future versions of both of you, and any observers empathically involved, and... and... and... Being good cannot be reduced to a complex version of being nice. 

So people are already doing meta-analysis, meta-ethics, meta-semiotics, meta-psychology... and they don't seem to feel that takes them out of their own branch of philosophy, much less into some separate place named meta-philosophy. It is awkward to snip out just the recursive parts of a given vertical, so we would have a hard time populating meta-philosophy that way. The result would not be a discipline, just a filter. What is left would we whole philosophies, in the systematic sense of Aristotle, Kant, Spinoza etc. that are themselves recursive, and include other philosophies in their model of the world in an explicit way. That would be an insane demand, as it would require an ethics that applies to semiotics as well as ontology, a psychology that applies epistemology and ethics... Labeling something does not make it happen, and choosing for your perspective to have a specific intention does not make it work. This is a term destined for the dustbin, for while it identifies a phenomenon, as a field of inquiry the result is either artificial, disconnected and disjointed; or it is empty. 

'Monadologies' like that of Leibniz, or the simplified form of Whitehead's Process and Reality, and virtual pantheism like Berkeley's model of God, reduce the physical to the mental rather than the other way around. The reason is that they seek to allow for the rules themselves to come into being for a reason, and not to be arbitrarily dictated by some pre-existing substance. If one is a physicalist, then one always seems to seek a more basic physics to explain why the observed laws of nature are as they are. We reduce Newtonian dynamics to quantum dynamics and quantum dynamics to String theory, and derive the properties of a string from the quantum foam and the nature that gives space, ... But in each case, there are arbitrary rules left, which appear to come from nowhere. This flips the question on its head: Can one ultimately be a materialist if one is really a hard-core reductivist? Or does that lead to an infinite explanatory regress? In contrast, if one is a pure-enough idealist, one can blame the current state of the laws of nature upon a decision made by some mind, or negotiated between some set of minds, and imagine how they might have been different, or might become so. We do not seem to object to arbitrariness if we can put a face on it, especially if the arbitrary decisions are not final. The evident stability of the current set of rules is then blamed upon the wish for the minds not to alienate one another, or traced to some hierarchy of minds or convergence toward agreement that produces a deciding or harmonizing force. 

By a Lockean notion of the social contract, you have tacitly agreed to obey laws that support that contract by remaining in your society and accepting the protection of its principal players and its institutions. Conservatives are not really being hyperbolic when they claim people have died so you can have your nationality. If you want out of that contract, that can be arranged. But transferring nationalities is difficult unless you afford exceptional value, and the cost of not having some social identity is extremely high. By not taking that risk, you are accepting an obligation. So breaking the law is lying, at least as long as the laws themselves are not contrary to the deeper tenets of the social contract. It does not matter whether anyone is looking, misleading people is immoral unless there is a very good reason. To water down Kant a bit, being disingenuous as a matter of course degrades our ability to communicate the difference between what is important and what is not. So yes, to the degree that honesty has any moral content it is inherently moral to obey the law. -- EDIT -- Long response to comment: There is always an element of deception at the point of decision to break the law because you have previously implicitly agreed to the social contract. One can lie for good reason. And admitting you are doing something now that renders past promises into lies is not the same has never having lied, though it can come close. Especially so if you are only doing so because you feel there is a good reason. I do not intend the answer to depend jointly on Lock and Kant. Locke's approach translates lawbreaking into a form lying. And most moral codes have some injunction against serious lies. You can choose any one of those. So this is a case of reducing a new problem to an existing solution. I have just given my own watered-down, evolutionary pseudo-Kantianism as an example. Social contracts also go beyond law. By citing Locke, I am speaking from a strong Western European bias here, set in the explicit Enlightenment philosophy on which we have created constitutional democracies, where we expect the laws to be continually edited to represent the contract as it stands, so that obeying the law really means obeying the law. The "Russian" version of the social contract, for instance, contains a corrective translation to dilute corruption. In a culture where a given group consistently and unfairly uses the law to their own advantage, one can see law as a false expression of the social contract. This can go so far that all laws are 'honored' in a different way from actually being obeyed. (Even in the US, we have a sort of "Old New York/New Chicago" thread, in some ways similar to the "Russian" one. Our current President holds it. He has openly stated that actually paying the taxes one legally owes, without attempting to cheat, is stupid. And it does not seem to be one of his nonce-lies, but something he has always believed. He is working from an approach that says people put stuff in the statutory law that they don't really mean, and we are meant to read past it as a matter of course.) So there is still a problem with the definition of 'obeying' the law. One may not obey it outright, but as interpreted through a prism of tradition, where not all players come out of the same tradition.