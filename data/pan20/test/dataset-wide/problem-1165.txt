I'm restoring to an alternate server in a DR drill and have restored master but can't restart the instance, even with /f and /m switches. The log is indicating tempdb can't be created, probably looking for the paths from the source instance. How can I determine which path it's trying to create tempdb's DB files in? I was able to restore the master backup under an alternate name on a different instance so I can browse the system tables. Is there a spot where I can look for them? I have to go with the assumption that access to the source server is lost so I can only rely on the backups to determine the correct configuration. 

In the process of removing an usused disk resource from cluster, I would follow these steps: 1) remove dependencies on the disks by the SQL Server resource, 2) take the disk resource offline, 3) delete the disk resource from the SQL resource group, 4) delete the disk resource from the Available storage pool. I was proceeding to do this on a two node cluster with two SQL 2008 R2 named instances (running Win2K8R2 SP1 build 6701, 64-bit). On deleting a resource in step 3 above, SQL would go offline, all the disk resources would go back to the Available pool, and the SQL resources (Net Name, SQL Server, SQL Server Agent) would disappear. SQL services do restart OK after via the services console as all the disks are still on the same node. I ran from the command prompt and I see the SQL service resources are still there, in Available Storage, and offline. To bring the resources back to the appropriate group, can I move them via commands? 

By having your database is no archive log mode and doing a daily warm backup, you are saying that is acceptable to lose a day's worth of data. If that is the case then fine, you don't need archive log mode. Doing a data pump export will give you a logical backup of your database at a point in time. The next question that you should ask yourself is that if it takes 11 hours to export your data, how long will the import take. If you are in a recovery scenario and you need to import your data the process could take awhile. You would probably be better just doing RMAN warm backups. Having said that, there are two thoughts that I have about your current issue. Is there a process that is running during the export that is changing and possibly locking data. If so your export won't be consistent and won't have all of the data that was changed for that day. For example if you are running an ETL process during the export you might get some of the data from the ETL, but not all of the data. The second thought is, if you are using enterprise edition, are you using more than one channel? You should be using multiple channels and compression. But I would guess that there is a locking issue that is causing most of your issue. 

Can you confirm with a Profiler trace which credentials are attempting to execute sp_start_job? I'm curious if it's the credentials of the client using the browser or the underlying application pool. If the account in question has sysadmin, it should already have the permissions to execute jobs. 

In the scenario of adding a column to a VLDB-class table, it may be worth exploring creating a new table with the new structure and moving records from the old table to the new table in small ranges. It'll keep the individual transaction size small so the high-water mark for the Tlog in Simple recovery would be relatively low. You can't avoid ACID requirements altogether but if you can batch your upgrade as smaller steps instead of one single transaction, you may be able to work around the disk space constraint. 

Do both servers have similar hardware and OS? Is there enough storage space for the 11g database? Have you thought about using data pump instead? With data pump and enterprise edition you can export and import in parallel as well as compress. With 10g you can compress the meta data but not the data. With 11g you can compress the data and meta data. How does the SGA compare between the databases? Are you using AMM on 11g, which isn't available on 10g? 

I recent upgraded my Ubuntu 14.04 server to Ubuntu 16.04. I managed to get mySQL upgraded to 5.7. I don't currently have anything in the mySQL instance. Since I have been an Oracle DBA for almost 20 years, I want to configure mySQL with some of the same features. I want to turn on logging for all transactions, and setup hot backups, hopefully using open source software. I'm also not sure which database engine to use. I have been looking through the documentation, and I am just not finding the steps for setting up a mySQL instance to work they way I think it should work. I know I can do a logical backup, but that doesn't allow me to do point in time recovery. I know I can use all of the default settings, but if I am able to market my skills as a mySQL DBA, I want to use as many of the advanced features as possible. Thanks. 

What I learned on my own. Basically from each job I was hired for, I learned how each shop operated, listened to business requirements and learned on my own how to complete the assignments. A lot of reading the manual (now Googling) and trial-and-error is involved but it's ultimately how we all learn. What I learned from certifications. One time I picked a textbook for my favorite programming language and discovered that I was only using a fraction of the techniques described on the job. I wanted to learn as many aspects as I could. In the back of the book they had mentioned the certification program for my language (Microsoft Certified Professional in this case). It's a structured way to learn all the major features of the platform you're focusing on for your employment. There's a lot of studying and practice involved (My MCSD took four tests and my MCDBA and MCITP:DBA were comparable). You'll learn about features you might not end up using on a particular job site, but being aware of their existence helps shape you decisions on how to tackle a business requirement. What I learned from the community. This by far, is the most valuable. You learn firsthand that you're not alone in your profession. You also learn what your peers have learned on the job and through their own research, and it's great to share what you've learned. You'll find your mentors here as well. There's a large of community of masters, MVPs and fellow DBAs who share their knowledge in-person, in print and online. Search for them in Google, attend a local user group, have your employee fund a trip to your favorite products' annual convention. I've seen at SQL PASS twice and the knowledge shared there is amazing. 

You need to understand the fundamental difference between storage space and memory. Your table space may run out of free segments, and thus the table can't grow at a certain point. But not all of the table will be in memory at any given point. Here is an overly complex SQL script that will show all of your table spaces and data files and how much space is being used in each data file. You may need to allow the data file for the table space that you are using to grow beyond what it currently is set to. Hence memory is not the issue, free space in a table space is the issue. 

I wrote a backup script which inserted a row into a backup table at the beginning of a backup returning a sequence number. I then had the following in my backup script and when the backup finished I updated my backup table with the end time and success or fail. Since my backup table had the server name and backup time, I could write a report joining my backup table with the rsr table to get a detailed backup report send to my email every day letting me know what backups happened the night before, as well as a section for each database and its last backup and finally a section on databases that have not had a full backup in over a week. It helped me to manage the databases since we had over 100 databases to backup. You can certainly use the command_id to gather all of the parts of any backup in a similar fashion. FYI if you backup a pre 10g database the rsr table is not used, therefore this won't work. 

During debugging, the OLEDB connection was created from scratch using the SQL Server Data Tools GUI. The notable difference in the connection string was the the addition of a space to have parameter now read . Connections to the secondary are now successful. It appears that the OLEDB driver accepts both variations but only the one with the space is implemented properly with SSIS packages. Is this an anomaly with SSIS or with the SQL Server Native client/OLEDB driver? 

A data warehouse ETL process is querying a read-only secondary in an availability group. The ETL process queries a single table incrementally using datetime range criteria of a minute and read committed isolation level. At the time of execution, 5 records that meet the criteria are committed on the primary, but another 3, with slightly earlier timestamps than the first 5 (but within the criteria range) are still in open transactions. Does the nature of availability groups require all transactions to be applied in LSN order (delaying the visibility of all 8 records until all are committed) or do the delayed 3 records get later LSNs and are applied as soon as they are committed, potentially after the ETL process has adjusted its date criteria? 

You probably need to drop the tablespace, not just the datafile. alter tablespace my_ts offline; drop tablespace my_ts INCLUDING CONTENTS CASCADE CONSTRAINTS; 

Currently we have two Model Mart repositories, 7.2 and 9.5. We only migrated the models that are currently being used, but we need to keep the old models available in case they are needed. Is there a tool to do the migration from Oracle 11g to SQL Server 2016? Has someone done a manual migration where the instance, database and schema are created in SQL Server, the constraints are dropped and the data is moved over followed by creating the constraints. 

When in doubt ask Tom... backup up and recovery generating extra redo Which is good reason to use RMAN for backups rather than scripted hot backups using begin backup/end backup. 

If you can define the T-SQL query to filter the rows to a specific date range, create a view and choose the view object within Excel instead of the source table. 

I'm the primary DBA for a 100-user data warehouse with approximately a dozen developers and analysts regularly contributing additions to the data model and codebase (mostly stored procedure and fact table additions). I'm following a relatively traditional deployment process focusing on regularly-scheduled code reviews and deployment windows with change control tickets twice a week with me an a colleague DBA handling the deployments. I've used SQL Audit, default trace data and RedGate DLM Dashboard to keep tabs on all the schema changes. The tempo has increased steadily in the last two years and some of the lead analysts would like to go towards a DevOps deployment method using automation. The director and app owner is the most accomplished developer creating assemblies for the advanced ETL, looking to perform deployments independent of me and my colleague, asking for sysadmin privs when we're backlogged or short-handed during vacation. I've declined requests to share sysadmin privileges with the director because of the risk of setting a precedent for other developers, the liability if access is opened too broadly in error and the risk of a deployment error which will get me paged at 3am when I'm off-call. On the flip side, I'm aware of the business pressure on the developers to get new content to market faster, even if it's not properly tuned. Finally, the political and departmental element of reporting to the director is present; it's easier to say 'No' if you're a DBA in a separate unit, but harder if the director signs your paycheck. I've told my director in the past that in the event of a breach or system failure, all the sysadmins need to be at the IT Security board of inquiry to explain what happened, which has given him pause in the past, but the business risk of delaying content is getting stronger. What options can I offer to the director that go short of sysadmin rights that give him more latitude to deploy but have the proper level of auditing in depth? I'm familiar with the various server and database fixed roles, but I'm looking at departmental protocol options as well. 

Try "export LD_LIBRARY_PATH=$ORACLE_BASE/product/10.2.0/db_1/lib" at your command prompt and then try sqlplus again. If that works you need to make sure that the variable is set for the environment everytime you log in. 

When you install Oracle there are a set of registry entries under hkey_local_machine, software, oracle. If you don't have the registry, and don't want the Oracle home, then you can just delete the directories. There is an Oracle directory under program files, which stores your Oracle Inventory, and your Oracle Home where ever you put that. 

Then you just need to write the SQL that generates SQL, if you need to connect as a different user you can include a line like: SELECT 'CONNECT user1/cdjkfghljsdg@sidname' text from dual; But you need to figure out how to write the SQL, since you have not given many details. You will spool to a file, then turn on feedback, set pagesize 50 or so, probably set timing on etc, then run the output of the spool file. You may want to spool to a log file during the run phase. There are alot of examples, you just need to query for an example that matches what you want to do. 

I investigated an instance that experienced an unexpected restart and came across the usual service control event but no user login associated with it. Is it still possible that an authorized user initiated the restart or is this due to Windows issuing the command? If an authorized user, outside of the SQL error logs and event logs, where could I go about attempting to identify the user? 

It'll involve 1) removing the user from any database fixed roles like (only will remain), 2) creating a custom role in the DB, 3) granting the new role specific permissions on the one table, and 4) adding the user as a member of the new role. You'll also want to make sure has no grants to user objects as all users in a DB are a part of the role.