It doesn't look like Team Foundation Server 2008 supports application-level load balancing. Apparently 2010 does, though? 

I would segment the network so as to afford the wireless devices access to the internet (which is what most people are looking for), and deny any access to the internal network. Sometimes it is okay to say no. 

1 - probably, though not recommended 2 - odd caching behavior, insecure, no support anywhere 3 - no idea There is essentially a ready-made solution for this in memcached: $URL$ 

OK, here's why this isn't going to work. What you're attempting to do is convince ntbackup to stream backup data to standard out instead of to a file. The problem is that ntbackup is not written to be able to do this. ntbackup must be given the filename to output the data to, and unfortunately in Windows, there is no equivalent to /dev/stdin or /dev/stdout. In order for this to work, you would need to convince ntbackup to print the raw backup data to the screen. There is no documentation that I am aware of (or have found through searching) that even hints that this is remotely possible. It just wasn't written to do this. Sorry. 

What you want to do is alter your clients to check authentication against two sources. In other words, keep your existing database while you are transitioning to Active Directory. Have the clients check the AD account and if that doesn't auth, have them check the legacy auth source. Any damage you do to make AD accept multiple usernames/passwords will be something you deal with for a long time. Just run parallel authentication sources until you're all transitioned. 

hrm. My kneejerk reaction is that it sounds like a bad idea, but I can't think of any technical reason that it wouldn't work. So you're doing this because machine3 is unable to talk to machine1? 

I haven't used any of these, but there are a (surprisingly) large number of packages available for this on freshmeat: $URL$ 

and update with the results? My initial guess is that PHP isn't installed. Assuming you don't see something like: 

Then place your archive logs (or Sunday's, if you've got them segregated by day) where they belong and do 

When you say load average, you mean the three numbers on the top line on the right side? Your screen looks something like this?: 

OK, this is why users should be disabled, rather than removed. There are built-in options to passwd for that sort of thing (and settings exist in every centralized-user-management software that I know of for it). But you're not here to get yelled at. Do the user's files still exist, or were those removed with the account? Screen uses the lock files to reconnect, and if they're gone, I'm honestly not sure how to reclaim the sessions. Try Keiran's suggestion of recreating the UID if possible. Sometimes it isn't feasible, especially in a centralized authentication where the IDs are hashed from the other user information, but if you can, that would be swell. Assuming you can't, you need to find a way to take over the terminal. I'm going to guess that TTYSnoop isn't configured on your system already. In that case, it's going to get hairy. Assuming that your screen sessions issue you a pts/# terminal like mine do, there may (MAY) be a chance. pt stands for pseudoterminal, and it's sort of client/server based. There are the pt slaves, which is the terminal that you type with, and there is the ptmaster, that receives the input. There are many slaves (/dev/pts/*) to one pt master (/dev/ptmx). From the man page to ptmx: 

I suggest this might be cheating (since I gave you the answer offline after you asked this), but I'm claiming my points anyway ;-) Check $URL$ 

Right now, this instance is actively recovering archive logs, but is not "live". I'm still looking for a way to tell if it isn't mounted as a standby database. 

It's a tradeoff. I've heard that big log files are more efficient, but we don't use them because A) the bigger the loss of a file, the bigger the loss of your data and B) our bandwidth is crap This leads to us spooling files for a few hours then transferring and running updates on the various standby machines. We're actually using Oracle8i still, and because the database was designed a long time ago by someone who wasn't a DBA, I still have to manually create new datafiles and control files. /sigh 

If you look at the contents of nagios.cfg, it will almost certainly include references to windows.cfg and commands.cfg. When you run the config verification that you mentioned, it traverses through all of the files referenced by nagios.cfg as well, so you are checking the changes you made as well. 

There is some existing documentation on a piece of software called MirrorManager, but I have never used it. It sounds like it should works fine, and maybe someone else can speak to that. There's also a useful page in the Fedora documentation wiki with a section on How can someone make a private mirror. 

I'm answering this, and I'm making it a community wiki, since I am copying and pasting from an existing document. For the record, I use Amanda Enterprise as my backup solution, and I don't use the tape encryption that it provides, for the very reasons that you mention. I was researching tape encryption, and I came across a great whitepaper from HP talking about LTO-4 encryption, and included are many possibilities for key management. Here's a basic rundown of the available options that are presented: 

Oh, I hate myself for saying this, but have you looked at daemontools by DJB? ( $URL$ ) They do what you want. But why do you want to do it like that? Are you having some sort of problem where your daemons die? Can't we fix the problem, rather than continually respawn them? 

Evan's comment is right on, except that I would urge you to consider enabling the "redire-gateway" and configuring the server to accept all internet-bound traffic, at least if you do any content filtering. If you don't, your laptops become (even more of) a vulnerability to your network. Split-tunnel VPN is generally considered insecure since it essentially offers attackers who compromise the laptop a short-circuit to the juicy center of your network. 

I am also having this problem. The cause is that upon installing 'rabbitmq-server' package on Ubuntu, an instance of rabbitmq starts. This is by design. Sadly. As for a solution, I haven't found one yet. Edit I don't know if this is the case for you, but in my case, I was changing the node name in puppet from 'rabbit' to something else. The brief explanation is that, as I mentioned, installing rabbitmq-server causes the rabbitmq-server service to run. By default, it uses a node name of 'rabbit'. In my case, puppet came along, configured rabbitmq, and then, before trying to start the service, ran '/etc/init.d/rabbitmq-service status', to check and see if it was already running. In an ideal world, the answer would have been "yes", since it was, in fact, running, but in this case, the '/etc/init.d/rabbitmq-service' script uses the configured node name to check and see if the instance is running - and when puppet changed the node name in /etc/rabbitmq/rabbitmq-env.conf, that completely broke the service script's ability to determine if it was running, so of course, the script returns 0, because it can't find a running instance. Puppet then tries to start the new instance with the new node name, but that fails because only one service can own a port at a time, and the running instance had it. To fix it, I set the RABBITMQ_NODENAME back to 'rabbit', and everything works. I wrote about it here: $URL$ 

Here's what I do: My primary database has transaction log shipping turned on. The logs are written to /db/archive. Every hour, a cron job runs as the oracle user. This cronjob does the following things: moves the contents of /db/archive/ to /db_archive/YYYYMMDD/ (using the following script (that I didn't write, and so don't hold me accountable for ugliness)) 

I've got a Juniper Netscreen SSG-5 that occasionally gets a high session count. I've got 4096 licensed sessions, and there are times I see 3000+ for a small office (a dozen or so people). This is higher than I would like, and it makes me a bit more than curious about the sessions that are open. Ideally, I'd throw the output of the "get session" command into a text file, but that's not something that ScreenOS has support for. At least, that I know of. Please let me know if that's the case. Barring the ability to save the output to a file, if I could somehow obtain the session list via SNMP, I'd be content to wr/m-angle it that way, but all I've managed to find is the number of active sessions (enterprises.3224.16.3.2.0). I could write a script using 'nc' to connect to the netscreen and hit enter repeatedly to page through the several thousand lines, but that seems less fun than doing it the "right way", if such a way exists.