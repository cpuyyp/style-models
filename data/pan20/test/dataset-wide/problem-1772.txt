So I'm trying to set up OWA, but I don't want my users to have to type in I've got the http redirect setup to redirect all requests to /owa so that narrows it down to I don't think my users are smart enough to type in that 's' at the end of http every time they want to access their mail. I want it to redirect to the full url with the 'https' and all I've read this which tells me that I should be able to use http redirect to redirect all traffic to if I just type in the absolute path to the page. When I do that, I get an error saying that the page isn't redirecting properly. So I do a URL rewrite. I download the URL rewrite module for IIS and follow the instructions listed here. The page redirects perfectly! Then I'll sign into the forms-based authentication and the same error as before pops up - the page is not redirecting correctly. Can someone give me a solution that works for them? 

I want to see a list of the model numbers of all the monitors attached to the computers in my firm. Is there an SCCM Query that will do this? 

I have network drives mapped to my users. If they lose network connection for any reason, it pulls up the client side cached version of it and lets the user work in that. Now some duplicate files are popping up and people are seeing inconsistent versions of things. I want to make it so that when a user loses network connection, they can't access the files. Period. EDIT I forgot to mention that my file servers are Linux-based samba shares 

So it is not exactly broken. Com port is correct because otherwise it would not respond at all. There is a configuration utility with the pole but it is either the wrong version (it says it's for VFD660_460 but the documentation at $URL$ says that VFD860 uses software which installs itself to Start menu > Programs > VFD-660_460) or there is some other problem because whatever choice I make there, after clicking Accept I always get Device Time Out error (even though I see another garbage character appearing onto the display the second I click Accept). How to fix that? 

I have googled high and low but there doesn't seem to be any example doing raid10 with megaraid (only the syntax). Can anyone explain what is wrong? 

This will mount the CD disc, make it the only package repository and install a kernel that has support for more than just a handful of vmware/xen/etc virtual devices. After you get it working, restore /etc/apt/sources.list file from the backup and update aptitude again. 

I have a 2.7 TB virtual disk (LSI MegaRAID controller with ten 600GB SAS drives configured in RAID10) under Linux. I am sharing this disk to a remote ESX host via ISCSI. Unfortunately ESX will only make a 740GB VMFS partition if you present it LUN greater than 2TB. I could make a 6 disk RAID10 (which would be smaller than 2TB) but I really don't want to lose spindles (IOPS). Is there a way to split this big RAID10 virtual disk up (for ESX) in Linux? 

When I run this script from the command line, it works as expected every time. Meaning, if I take out the call, the binds fail. If I leave it in, the binds work. However, when accessing the script from a web server, it's a different story. The web server is using PHP-FPM and is configured such that each spawned PHP-FPM process lasts for 500 requests. If I hit this script via the web server, and I get a brand new PHP-FPM process that just spawned up, it works fine. If I change the script to take out put putenv() call, IT STILL WORKS for requests served from that same PHP-FPM process, which makes no sense. Only when that PHP-FPM process dies (after 500 requests), does it's replacement process fail every bind attempt as expected. To summarize: 

Those cookbook paths all exist and the cookbooks have indeed been copied there. What am I doing wrong? 

I've been following several guides and read thru documentation trying to get to provision a VM for me. Knife solo will work with Berkshelf (used to manage a library of chef cookbooks), so I'm using that as well. I can successfully a VM using But when I go to then run , I get an error as soon as chef-solo tries to compile the cookbooks: 

If there are any XHProf or PHP veterans that can confirm if the increasing PHP's had any impact on the time reported in from XHProf? What underlying caching mechanisms should I be aware of in Linux that could have an impact? Same as above, but for NFS? 

I have an Openbravo POS workstation with a customer display pole. The pole is VFD-860. When I click on the products on the Sales page of Openbravo, the display responds but displays garbage: 

Is it possible to use .htaccess file to rewrite this: $URL$ into this: $URL$ If it is then how? EDIT: The goal is to avoid any updates to .htaccess when a new customer is added EDIT2: I see that my question has been too vague. I have a CMS that is being used for lots of different customers (www.foo.com, www.bar.com, www.somerandomname.com). Each customer has it's unique files (design, uploads) in a directory /data/www/cms/customers/XXXXXX/ where XXXXXX is the name of the customer. Apache config file has default DocumentRoot set on /data/www/cms/. What I'm trying to do is to have the URL $URL$ to return a file from the given customers data directory, i.e. /data/www/cms/app/customers/foo.com/assets/css/bg.png. 

Two way Intel E5504 @ 2GHz, 24GB RAM, 12x32GB Intel X25-E SSDs in RAID10. Intel Core2 6400 @ 2.12GHz, 3GB RAM, simple 80GB SATA drive. 

The "Why?" part: According to $URL$ the maximum LUN size for VMFS is 2TB. According to $URL$ what ESX will do if the LUN size is bigger is it makes a 740GB partition and the rest is inaccessible. The "How to fix it" part: Present a smaller than 2TB LUN to ESX. That can be done by creating partitions on the disk (/dev/sdb1 /dev/sdb2) and exporting those partitions as separate LUNs to ESX. 

It keeps telling me that ‘The specified network password is not correct’Is there a step that I'm missing? 

I have network enabled printers and I'm wondering what the best way is to deploy 'em I can deploy them using GPOs that map the TCP port with the printer's IP address (how I'm doing it now) I could deploy them using Print and Document Services and AD I could deploy them using the "Deploy Printers" in GPO What are the benefits and drawbacks of each and what are you using? 

I have seen a consistent spike in traffic over my network since Monday morning and I don't know where it's coming from! I don't have netflow routers (like I would like), I have IPCop firewalls. Is there any way that's built in to Linux that I can see where the packets are coming from/to? Like a built in packet capture? If there's not, how do I go about finding where this traffic's coming from? 

We want to use Lync Attendant for our front desk but I'm confused on how to set this up. We usually have one receptionist that's up there, but when she goes to lunch, there can be any number of different people covering the front desk. I'm wondering if I should set up one account called 'receptionist' and have anyone covering the front desk log into that account. The drawback of that is that the people can't work on their things when they're up at the front desk. I can each have them log into the computer front desk computer. That way, they can work on their own things AND answer calls. The problem is that they'll have to log into the phone up there too. That sounds like a pain. So I wanted to see how anyone else solved this problem. What are you guys doing for your front desk? 

The call to turn off SSL cert validation always works as expected when executing the script from the command line. When executing from the web server, the FIRST request served by a PHP-FPM process will set the environment variable, but if I modify the script to unset it or not set it at all, it has no effect. My debug output which prints the value of the environment value shows that the values are set and unset as expected, but are somehow not having an effect on LDAP. 

The Problem I'm attempting to diagnose a performance issue on a Drupal website I'm developing. In the morning, when the site has seen no traffic (not even a cron run) for over 8 hours, the homepage takes around 3.5 seconds to load. Reloading the page takes an expected 250ms. This is a development web server, with a fairly old version of PHP installed (5.3.3). All of the files are statically mounted over NFS (which I believe is the root cause, more on that below). To help diagnose, I installed XHProf on this development server, and enabled a Drupal module that profiles page loads and displays the profiling data in a nicely sortable table. For those unfamiliar with XHProf, it provides data on every function called and things like the total time spent, memory usage, and calls for that function. My Findings On the initial "slow" hit, the PHP function took 1400ms from 82 calls, accounting for roughly 43% of the total execution time. On the subsequent page load, the same function was again called 82 times, but this time it was dramatically reduced to just 3ms accounting for just 1% of the total execution time. I additionally looked at the file that took the longest for PHP to load into memory (which is what I believe the prefix means on a function name). This PHP template file took a whopping 42ms to load the first time around, and only 3ms on the subsequent reload! What I suspect It's clear to me there is some sort of caching going on somewhere - I just don't know where yet. The PHP docs for file_exists mention that the output of this function is cached. I then found I can control the size of this cache and it should probably be increased from the default 16k to something more appropriate for Drupal (which loads tons of relative files). However, while I think that would reduce the amount of time spent in , I'm not sure that will affect the time PHP takes actually loading the file (the I mentioned earlier) and increasing this value seems to just be hiding underlying performance issues with the file system. Questions 

I want to be able to see a nice picture of nested groups in AD. Are there any products that do that for me? 

I want to install a Windows 7 image on 40-something computers without touching 'em I have a WDS server, an image captured, and an answer file setup. I go to create a multicast transmission out of the image, but I can't see it on my client computer. I think it may be because my router doesn't support PIM (it's an IPCop firewall) but I'm not sure it would need to if I'm just multicasting to client computers within the network that the router. So my question: if I want to do this, will I need to have a router/firewall that supports multicasting in order to multicast to client computers on my internal network? 

I have an AD domain and I don't want my users to have admin privileges on their local computer. I have WSUS for Windows updates and stuff but what about Firefox, Flash and other third party program updates? Isn't there a better solution than to go to everyone's computer and give them my username/password so that the update will actually go through? 

Has anyone had to deal with a corrupt file system on a storage gateway volume? One of my volumes is now telling me that it's corrupted or unreadable. I've tried running a chkdsk /r on it and it took days (10TB volume). Once completed I got the same error message. I didn't have snapshotting scheduled so I don't have a previous version of these files. I'm currently working with AWS support and they're having me run chkdsk a couple of different ways. Has anyone had to deal with this before? PS: BTW, don't run a chkdsk on a storage gateway volume, it screws up your cache and runs very slowly 

I need to configure Postfix to send for a specific domain. I have tried to google for returning custom error codes for a domain with no luck so far. 

Both machines run Windows Server 2008 R2 now and have 10Gbit Supermicro AOC-STGN-i2S (actually they are Intel 82599 bearing Supermicro logo) in PCIe x4 slots- with a SFP+ direct attached twin axial cable between them. The second server is for testing only. First I installed ESXi on the 2nd and used the 1st as a datastore. I noticed that according to CrystalDiskMark, a VM on ESX only got 325 MB/s seq transfer rate (tried with both NFS and ISCSI). I ran the same test on the first server locally and got ~1000 MB/s. I wondered if the network link really kills 2/3 of speed, so I replaced 2nd's hard drive and installed Windows Server 2008 R2 and tried Jperf and NTTtcp. Jperf showed 400 MB/s and NTttcp showed 4300-4600Mbit/s. Windows Task Manager showed some 600 000 000 bytes per interval which translates to 4.47 Gigabits. I verified that both ends had full duplex and tried toggling jumbo frames on and off both ends but the difference was only 580 000 000 vs 600 000 000 bytes per interval. Why the throughput I'm seeing is only about half the theoretical maximum of 10 gigabits? ADDENDUM NTTtcp command lines: 

But the last step never completes (Executing is displayed forever). When Management Studio is forcibly closed and restarted the new linked server is there but only contains Catalogues subitem. If we try to expand it, Management Studio goes into loop yet again. 

I have a Catalyst 2900 that's sending out an STP packet about once ever 1-2 seconds. Is that excessive? 

A search for that error led me here but none of the solutions seem to be of any help (or maybe I'm not using the solutions correctly?) Has anyone remedied this error before? Can someone point me in the correct direction? 

Found out that it was what everyone was suggesting. When I made the Staff distribution list from a standard list to a dynamic list, something got messed up. I went into OWA, brought up the nickname cache (little pop-up menu when typing in the address in a new mail) and hit delete on the name, it removed it from the nickname cache. When they tried to send again, it sent to the correct list. Thanks for everyone's help! 

I'm redirecting appdata, my docs and desktop folders to a server using a GPO based on what facility they're usually at. This brings up a problem for users who bounce between multiple locations. Firefox profiles are stored in appdata so it takes forever to go to another facility to get the Firefox profile so that it can start. And after two to three minutes of waiting for Firefox to load, it runs so slowly that it's almost unusable I want to remedy this with DFS but I don't want to have a hellish transition period. Right now the GPO for facility1 is set to redirect the folders to and the GPO for facility2 is set to redirect the folders to . I know I'll need to change these settings to the DFS namespace that I create. I'll need to add and to the DFS namespace and change the GPO redirection settings to redirect appdata to . Writing it down here makes it seem really easy though; will that be all I have to do? Will the GPO setting direct to the UserName folders in the DFS namespace correctly without me having to move anything? Will the users see any hiccups at all? Or will it just magically replicate? I'd appreciate any feedback and tips from anyone who's gone through this (non-dfs folder redirection to dfs folder redirection) transition before.