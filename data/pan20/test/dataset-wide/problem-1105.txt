As you already wrote in your question that you fiddled with registry and in this process you made registry inconsistent so SQL Server is not able to proceed with installation of Setup files its failing at setup support file installation like below 

You said you have only 4 G for SQL Server have you set max server memory for SQL Server ? Is windows server 2003 patched to latest Service pack. Make sure it is there was bug in windows server which trimmed SQL Server memory excessively. Edit: 

I would suggest you follow what the recommended. You can change the option to . Checksum might be able to detect more problem that torn_page. Torn page allows you to detect whether page was successfully written to disk or not. It would not check what is inconsistency inside the page. While checksum performs more thorough checks. Having said all this you must know . What Paul said, quoting from This SQLServercentral link 

I hope this will resolve your issue. After installation completes change Service account of SQL Server to some low privileged domain account you can use this link for configuring service account for SQL Server 

No buffer pool and VAS are not the same thing. In SQL Server direct physical memory access is not allowed any memory request which comes is first mapped to process VAS and then if SQL Server find memory free it would map this VAS address to physical memory and then memory becomes committed. Bufferpool is physical memory VAS is virtual memory. VAS VAS is Total amount of Virtual address space visible to process. Total VAS in windows OS is divided into VAS for OS process and VAS for application. SQL Server runs as an application. For 64 bit system total VAS is 16TB. So now both SQL sever and Windows process gets VAS of 8TB. Whenever a new process wants to read data or write data into memory it would reference memory in its VAS region so the new process would see VAS of 8TB and then it would be mapped to physical memory. This physical meory would be your buffer pool Buffer pool A buffer is an 8 KB page in memory, the same size as a data or index page you can consider buffer as a frame which holds data and index pages when they are brought from disk to memory. SQL Server buffer manager manages the task of reading data pages into buffer pool and also writing it to disk. It is a reserved memory store for SQL Server and by default if you do not set value for it it will take as much memory as possible. Buffer pool only allocates memory to requests which requires less than 8 KB pages this feature changed in SQl Server 2012 where buffer pool has no meaning its just consumer. It is easy to allocate small contiguous amount of memory than large contiguous amount( Large contiguous amount might not be free or present to allocate). 32 Bit system In 32 bit system SQL Server has VAS of 2G. If system is WOW it would have VAS of 4 G. So this becomes limitation for process which requires huge amount of continuous memory and such memory is not satisfied from buffer pool but outside buffer pool. 

The execution plan (PasteThePlan) is vastly different with this small change. It is tossing a warning that there is within the predicate. It was my assumption that would implicitly satisfy this argument as it does with . However, given that the operator returns each row from the initial input when there is a matching row in the second input, and because the warning exists, query optimizer assumes that each row is a matching row. Therefore all subsequent batch operations are ran against all 2048 rows in the table. What can I look into to better interpret what the execution plan is describing so that I can understand how to properly resolve the issue? Or, alternatively, am I simply missing the purpose of the operator when filtering result sets based on a static input? 

I have a question about whether or not it is necessary to back the tail log up when performing a restore of a database in Full Recovery mode. I've sprawled for a good while now, and have gotten conflicting reports. Consider this scenario in SQL Server 2012: 

Has anyone experienced behavior such as this, or know what could be causing index statistics to go awry so quickly across so many tables? Any recommendations for diagnosing next time this occurs? Manually running the stored procedures and diagnosing the execution plans have proved rather unhelpful as the statistics its reading (Expected/Actual Rows) are the same. 

RAID 1 is less suitable for any of Data file, log file and tempdb file. Although RAID 1 provides redundancy by mirroring and also provides bit of a fault tolerance it is kind of outdated with arrival of RAID 5 and RAID10(or RAID1+0). Best is to keep data, log and tempdb on differnt physical drives which are configured either in RAID 5 or RAID10. RAID 10 is more preffered but is costly as compared to other RAID technologies. If you have restriction with RAID 1 you can use it but then make sure you separate data,log and tempdb files on different physical drive to avoid I/O contention. Above suggesstion were given because you asked about seggregating files. In many environment because I/O load is relatively less data and log files are kept on same drive, even tempdb is placed on same drive where other files are present. According to surrounding/env/load we can seggregate files to get optimum performance. 

You are having SQL Server 2008 and you are worried why version is showing instead of and this is because of bug for which connect Item was raised and confirmed by MS employee that this is bug. From incomplete knowledge I have this is happening because SQL Server is reading some random registry values while scanning the registry for installed SQL server versions which it should not. I believe this should be fixed in Future service pack releases. Since you are on SP2 + some CU I suggest you apply SQL Server 2008 Sp4 To get information about SQL Server version simply run 

Nothing with the server setup has changed since the migration. We are running two servers, a PRIMARY and REPLICA within an AlwaysOn Availability Group. The VMs are housed on Azure and have 8 logical processors, 28GB of memory, and all SSD drives. The data file layout looks like: 

This returns: My clause clearly brings back only the rows that are explicitly , so it is not an issue of being an empty string and evaluating to . 

The data it stores is enumerated based on a JavaScript method that uses information from their browser (I don't know much more than that, but could find out if needed): 

We have been experiencing an odd behavior in our application where various modules will begin to timeout in SQL Server 2012. Each time we stumble across this issue, we find that the statistics require update and that running fixes the issue. After updating index statistics, the timeouts go away. However, the frightening issue is the frequency in which we have been experiencing the need to update the statistics, and the fact that nearly all of the statistics are showing a need to be updated across all of our tables related to order processing. Due to the frequency, we have setup a job to run prior to business hours at 7:30 AM. This seemed to have calmed the issue while we continued to investigate until it occurred again today. Not 10 minutes after business opening, they were receiving timeouts. I immediately ran and the timeouts disappeared and application function returned to normal. The order volume since business opening was low (less than 20 rows added to the primary order table), yet the statistics became so bad between 7:30 AM and 8:10 AM that we began experiencing time outs. A couple of notes: 

First please note logins and users are both different thing. You need a valid login for a user to make changes into database. User is anyone authorized to perform ceratin actions on database while they use to Log into the SQL Server. 

This is why the memory utilization in task manager shows more than 1 G. So 1 GB limitation is only for database cache other caches like procedure,plan cache can still add to SQL Server memory utilization. Ideally you should use DMV sys.dm_os_process_memory for looking at memory utilized by SQL Server instance, I always advise users not to use Task Manager. Regarding OOM error are you getting this error when running query in SSMS, this can also be limitation from SSMS side which gives OOM error when user forces it to return large result set. Instead you can set SQL Server to post result in file and save it on drive to check if you still get the OOM error. Can you also post information from about this OOM error 

First you did a bit of mistake you are actually installing SQL Server 2008 R2 but in your question you have clearly in Bold written SQL Server 2012. Since you provided me 2008 r2 setup logs I would assume you did typo and you are installing SQL Server 2008 R2 so Please correct the question I found below in SystemconfigurationCheck_report.htm 

We have the following (unfortunate) scenario: we have a database where on-the-fly changes are being made by developers as quick remedies to dictionary tables (order types, document types, fee distributions, and so forth). This wouldn't be an issue if these particular developers didn't forget to check their changes into TFS, but alas, we face it. This causes an issue where the database project(s) in TFS do not properly reflect what is in the database. Essentially, I want receive an e-mail or other type of notification (table insert with necessary information) when a change is made to 15-20 tables, what the change was, and who committed the transaction. What is the best method to monitor this type of behavior? We are willing to invest in tools that allow for this ability. I've checked out RedGate SQL Monitor, but it doesn't seem to have this feature set (or I am missing it). I've also seen usage of triggers, but we have a large amount of tables that would require monitoring and I don't know the performance degradation that might incur and if we can collect the appropriate data. I do like the trigger approach where it would insert the information into a table ([dbo].[TABLE_UPDATES]) because it would allow us to build an SSRS report to run against and pull data as needed, but I'm not sure I 100% trust this as a solution. We currently use RedGate DLM Dashboard for monitoring schema changes, and it has proven quite invaluable to receive those updates. We just want to expand it to the data itself. Disclaimer: I know - terrible practice by developers to apply things directly to a database without checking in their changes, but we are currently having to deal with this hurdle and reign these individuals from wildly shooting at the hip. We are wanting a long-term solution for accountability purposes as our database and company grows. Thanks in advance. 

You are correct the last backup time is showing while last copied and restored are much greater and . So I believe somehow the catalog is not getting updated. There can be two reasons: 

I got similar error and posted a connect item. I did this because this happened on my PC and I was the only admin on the machine. Unfortunately MS team was Below was reply given by MS team 

SQLServer:memory Manager--Target Server Memory: This is amount of memory SQL Server is trying to acquire. SQLServer:memory Manager--Total Server memory This is current memory SQL Server has acquired. ( Ideally Target value should be less than or equal to Total) Page reads/sec – Number of physical database page reads that are issued per second. This statistic displays the total number of physical page reads across all databases. Because physical I/O is expensive, you may be able to minimize the cost, either by using a larger data cache, intelligent indexes, and more efficient queries, or by changing the database design Free Pages – Total number of pages on all free lists (free lists track all of the pages in the buffer pool that are not currently allocate to a data page, and are therefore available for usage immediately). Undoubtedly this value should be high Page Life Expectancy – Number of seconds a page will stay in the buffer pool without references> if you have NUMA system analyze PLE for each node as mentioned in this article Free List Stalls/sec – Number of requests per second that had to wait for a free page. Ideally stalls should be as zero or as minimum as possible SQLServer:Memory Manager--Memory Grants Pending: If you see memory grants pending in buffer pool your server is facing SQL Server memory crunch and increasing memory would be a good idea. For memory grants please read this article: If you see a non zero value of memory grant pending with Low PLE and High free List stalls you definitely have a memory pressure and should consider providing more RAM. 

I am attempting to update a query that utilizes the operator within a clause predicate with to compare potential performance improvements and better understand what is happening behind the scenes when the two are interchanged. It is my understanding that in practice, the query optimizer treats and the same way whenever it can. I'm noticing that when the query is ran with the operator, it returns the desired result set. However, when I replace it with the equivalent, it pulls in all values from the primary table I want to filter. It is ignoring the provided input values passed to and returning all possible distinct values. The use case is relatively straightforward: the query accepts a pipe delimited string that can contain up to 4 values, e.g. or . From here I the input into a table variable to determine the corresponding internal for the . The result set returned is then filtered to exclude the that were not passed. To illustrate, here is a similar table definition: 

We are designing a multi-schema, multi-tenant database for SQL Server 2016 that will service a basic CRUD application which will see small to medium transactional throughput and comprise of 15-20 tables. For the sake of data isolation and security, we are exploring the utilization of a composite primary key that has and . These two columns will repeat throughout tables in a hierarchical manner, enforcing referential integrity with appropriate foreign keys, and allow us to enforce deeper row-level security. In most scenarios that I have read, the usage of a GUID for seems to be controversial due to performance implications, especially with regards to space. However, we believe that for the management of physical file partitioning, resource pool delegation, replication, portability, and general referential integrity, that a pairing of an integer and GUID will allow for better isolation and security within the multi-tenant database. We are bound by a range of security provisions, so this further leads us down the path of using this composite key type. In thinking of children composite keys, I know the order of the columns specified matters. However, I cannot seem to come to a consensus if we should feed the the same composite key structure down the table chain or if it is better to segregate it in a more traditional manner. For instance, given the two table structures: