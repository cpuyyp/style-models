There was a revolution in mathematics because of Descartes, back in the 1600's. He invented the method of using the mapping from geometric objects to coordinates and algebraic equations, where a quicker rote symbolic proof was possible in comparison to many synthetic proofs (like those in Euclid). What's great about mathematics is that we learn it all in grade school now. It is almost unconscious and seemingly lame now to even discuss it as a revolution; it's the water we swim through now. I think that it is expressed Fresnel's attitude that in La Géométrie, what we get out of Descartes is not the method at all, but simply some handful of clever results (those applications of the method). The distinction that Fresnel is trying to draw is not clear. I take it that he means that a method is a clever general way of doing a great many things with little thought versus an idea which is a clever specific idea for a very specific difficult problem (that probably doesn't generalize). If that is the appropriate interpretation of Fresnel's statement, then I disagree with Fresnel; the examples in La Geometrie, may or may not be specific instances of the application of the genera method (I vaguely remember those instances only tangentially being an application of analytic geometry, that is they each had their own idiosyncratic problems and solutions). But even where the distinction works, I think the 'method' is the revolutionary thing rather than the specific solutions to the specfic problems in La Géométrie. 

You have two questions here: one about the uniqueness of set theory, and another about foundations of mathematics. 

The axiom of reflexivity, specifying that equality is reflexive, simply helps define one aspect of the '=' symbol. This helps make a connection between well-formed formal strings that use that symbol and our personal conception of what equality should act like. It only seems boring and tautological because the concept is obvious, but the axiom is needed to operate on the symbols usefully. The axiom contains quite a bit of information ('usefulness' might be more appropriate) because without it, all sorts of theorems about natural numbers could not be proved. I don't know what you could mean by 'empty of information in their own frame of reference'. You're using words that don't have any technical relevance to axioms and proof theory. If you're using them non-technically, you'd really have to explain what you mean by 'information' and 'frame of reference' as they refer to the mathematical system. But does 'mathematics then provide truth only within an ontologically relational framework'? Whatever you think about those words together, a yes or no answer to it will only justify some hidden personal definitions to make the answer right. But to jump from the axiom that equality is reflexive and that it feels tautological all the way to a statement about all of mathematics and reality (and ontology) is just perverse. The axiom is included to make proofs work (if you do some examples you'll see where the axiom comes in useful). If you want to draw the earth-shattering consequence that there is no absolute truth, the seeming tautology of reflexivity in arithmetic is not the place. (on the other hand, truth is undefinable inside arithmetic, but that's not the truth you're thinking of) To help with this, is there anything special about reflexivity axiom? Aren't symmetricity and transitivity also tautological? What about the other axioms? Axioms are often very boring because they should be intuitively non-questionable (in order to trust that the proof system is proving things that fit with your intuition). 

I've always taken this very informal almost joking statement to mean that one has a general rule in mind, one sees an exception to the rule, but the circumstances of the exception are so rare that it implies that rule probably is pretty accurate in most circumstances. If forced to formalize and pedanticize this I would say it's a Bayesian confirmation by improbable data. 

The Loeb Classical Library has among others the complete works of Seneca one Page the original the facing page an English translation. 

Left and right hands are almost identical but are not exactly the same. As Wittgenstein pointed out in 6.3111 of TLP, there is a transformation from one to the other. But that doesn't answer the practical question: Is there a self-contained dictionary-like definition of right and left? 

From the SEP article you link to, there are many justifications for dialetheism (but also many objections). But to answer your direct questions: 

I don't think there's a strict technical answer. Because there are many possible -equivalent- systems (axiomatizations plus rules of inference), equivalent in the sense that they all prove exactly the same propositions, the choice among them boils down to other properties, some technical and some simply style. From the automated deduction direction, anecdotally, it seems that the more rules of inference you have (and fewer axioms), the longer it takes to find a proof. As to style, the three rules you gave are understandable, meaningful, and fairly short. I bet there is a single axiom, but then I expect it would be long, unfathomable, and hard to remember. With a single rule of inference (like modus ponens) you need a few more axioms, but then the longer list, even though with simple axioms, is itself too long to remember. One could define 'simplest' as fewest axioms, or fewest rules of inference, r even shortest number of symbols to represent (axioms/rules of inference together), but the resulting system may not be psychologically satisfying. Short answer, it boils down to human preference. 

Geometry was axiomatized 2500 years ago. It was only towards the end of the 19thc that arithmetic was. Numbers are just so 'obvious' that it is hard to think about them. Usually when you are introduced to complex numbers in school, they are quite obscure. Negative numbers at one point in history (of mathematical thought in Europe) were not considered numbers. Before that, there was controversy about zero, and even about 'one' being a number. One can question all these things (and it is a good thing to question them) but at some point the questioning stops because you realize what you can and can't do. No, I can't hold '2+3i' apples, but that's OK, the formal rules that apply to such numbers don't apply to the situations of holding apples. I can see 5 apples, but not -5 of them, but that's OK, '-5' is not something to be seen (well, actually, if you see them in someone else's hands you might be considered to be seeing '-5 apples'). But do you really see '-5' by itself or even '5' by itself. I don't think so. Existence of numbers is not like the existence of real word objects. Anyway, 5, -5, 3+2i don't actually exist 'out there', but we can use them when talking about 'out there' objects. 

One way to evaluate a new different logic is to see what new things you can prove in it and what old things you can't prove; and also whether you want these new changes or not. That is, start with the restriction (e.g. relevance logic only allows proofs where propositions have antecedents that appear in the consequents (i.e. hypotheses have relevance to inferences)) and then see if you get something you don't like (like p -> (q -> p)). Another way to decide is how -long- your proofs are; removing rules of inference or restricting their action can make some things not provable, but sometimes it leaves them provable but just with much longer proofs (see for example cut-elimination) 

is a theorem of propositional logic (where F is false), so that anything that evaluates always to false will also be a theorem. A contradiction is presence of two hypotheses that have opposite truth value. A proposition is not itself a possible contradiction, it's just the collection of all its valuations. Is 

Now for the other direction. Yes, you're right, philosophy really is pretty disconnected from the 'real' word, with its synthetic a priori and categorical imperative. Really, it is the height of ivory tower ethereality. Only academics read their publications. The great majority of words written is commentary on commentary, and the few original works are about made up problems. (this may sound sarcastic but it is as sincere as the first part). Now rather than try to weigh these two sides, I'll deflect...consider other faculties of the modern university, say those mentioned before, literature and history. I think it is obvious that literature has less impact than philosophy. As to history, I feel the need to justify the same claim. How can study of the American Revolution possibly have an impact actions taken today? All you really need to study is the newspaper -now-; the news -now- is what makes an impact on next week. It would be boringly self-supporting here to say that philosophy matters. And it's too easy a target to say philosophy is irrelevant; there're no facts there. Let's be philosophical and empty (they're almost the same!) and say the truth lies somewhere between the two. 

Logic is just a way to formalize thought. If you have two logics that contradict each other, it doesn't mean that all logic is a sham and buildings and bridges are going to fall apart, it means that you possibly made a mistake in operation of one of your logics, or it may mean that the two formalizations capture different kinds of ideas, or it may mean that a statement is true in both but provable in one but not the other. 

This particular philosophical question is one that has very much devolved to the sciences, specifically to the biologists, psychologists, anthropologists, and linguists. Where does human knowledge come from, is it learned from experience or is it inherited from ancestors? The philosophical tradition has laid out the definitions and issues, but the answers have all come from experimentation and observation. And there is no one answer: there are contexts where one is the obvious answer, some where the other is obvious, some where it is altogether mixed and some where it depends on much finer distinctions. The question now is not which one is the answer but which is more important under very particular circumstances. We tend to think as obvious that biological form is inviolate and behavior is infinitely malleable. But experimentation has show that biological development can be altered by environment and that behavior can be inherited (and can be hard to change). We all -know- that 5+7=12, and it always was and always will be, but then really at some point in our lives we had to learn it (and frankly sometimes there is doubt depending on your numeracy). Experiment/observation methods include: comparisons of identical twins, study of infant behavior, study of behavioral dysfunction from lesion/trauma, universal characteristics of languages. The meta-'of course' is that some of these experimental questions get answered definitively, but some create more questions, which parts of a phenomenon are determined by nature and which by nurture.