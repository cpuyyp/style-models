The root of your problem is not the maths, it is choosing the correct units. The relevant equation is $$L = 4\pi d^2 b,$$ Where $b$ is the power per unit area received at Earth from Vega, and $4\pi d^2$ is the surface area of an enormous sphere constructed around Vega, with radius equal to the distance to the earth and which would catch all the light emitted. To get a luminosity in Watts, we need to express $b$ in Watts per square metre and the area in square metres. It helps to know that $d =19.26 \times 3.08\times 10^{16}$ m. The value of $b$ you have used is the apparent visual magnitude of Vega and I'm afraid that is not the power per unit area received at the Earth. A bit of reverse engineering tells me that actually $b = 3.5\times 10^{-9}$ W/m$^2$. To obtain a final answer in terms of a multiple of solar luminosities, you divide your result by the solar luminosity in Watts, which is $L_{\odot}= 3.83\times 10^{26}$ W. However, I think you want to be able to calculate the stellar luminosity from the distance and apparent magnitude. In which case, I refer you to James K's answer, noting that the bolometric correction depends on what type of star you are looking at. 

All type II supernovae are associated with the core collapse of a massive star and are thought to produce some sort of compact star. Supernovae should also always produce an expanding supernova remnant. Almost all massive stars are part of multiple systems. These three facts might lead you to believe that most, if not all, supernova remnants should contain an observable compact star in a multiple system. There are two classes of reason why this is not the case - observational issues and physical mechanisms. Observationally, we do not see compact multiple systems unless one of the stars is a pulsar, which requires a correct orientation to see it, or if the compact star is accreting mass from its companion. The latter requires a close binary system that thas survived the supernova. Also note that isolated black holes may exist in supernovae remnants but would be unobservable. Physically, the supernova explosion may disrupt the binary system. Pulsars are typically travelling at hundreds of km/s thanks to the "kick" they received from their supernova. Supernova remnants are not visible for long ($\sim$ a million years or less), so pulsars that have been kicked out of a binary can still be found within the supernova remnant. The short-lived nature of supernovae remnants also biases against finding compact, X-ray accreting binaries. Typically, the compact star's companion must evolve to either fill its Roche lobe (for a low-mass X-ray binary) or to develop high mass loss rates (for a high-mass X-ray binary). This would not normally happen within the short life of the visible supernova remnant. So examples of observable compact binary systems in supernova remnants are rare. But there are at least two examples known in our Galaxy and several in others. The (at least) two in our Galaxy are Circinus X-1 and SS433. 

They are (very close to) "standard candles". The physics of the supernova detonation, thought to be when a white dwarf accretes matter and exceeds the Chandrasekhar limit, is very "standardised". The bomb goes off in exactly the same way with the same amount of identical explosive. That means to a good approximation, measuring the apparent brightness of a type Ia supernova and comparing it with nearby examples means that the distances to these events can be accurately estimated. They are really luminous and last 2-3 weeks. This means that they can be seen at enormous distances, they almost outshine the galaxies that they are in, and they last long enough for astronomers to discover them in automated surveys and still have time to follow them up and measure their light curves and estimate their peak brightness. 

The answer is pair production. Once photon energies exceed 1.02 MeV it is possible to spontaneously create an electron-positron pair in the presence of an atomic nucleus (to conserve momentum). In general for high energy photon interactions with matter you need to consider the photoelectric effect, Compton scattering and pair production. The former is more important at lower energies, the latter at higher energies. The picture below shows the mass attenuation coefficients for Aluminium and Iron as a function of energy, with the contributions of these three processed identified. 

An astronaut on the moon could only be seen by reflecting the Sun's light towards Earth. Stars on the other hand emit their own light. To first order, the amount of flux incident upon the Moon from the Sun is the same as that at the Earth - about 1.4 kW/m$^{2}$. Let us assume that an astronaut is perfectly reflective and that the relevant reflective area that we can see from the Earth is 1 m$^2$. NB: If the astronaut is not lit up by the Sun, then there is obviously no way that they can be seen. Treating the astronaut on the Moon as an isotropic point source emitter of reflected light, we have a light source of power 1.4 kW at a distance of 400,000 km. The flux at the Earth is therefore $7\times10^{-16}$ W m$^{-2}$. How does that compare with starlight? Well, the total luminosity of the Sun is $3.8\times10^{26}$ W. It has an absolute magnitude of 4.8. This means that if we put the Sun at a distance of about 20 pc, it would be about as faint as the faintest naked eye star in the sky. The flux received at the Earth from such a star would be $8\times 10^{-11}$ W m$^{-2}$ and thus 100,000 times brighter than the astronaut. No need to worry about the resolution of the eye, since both the star and the astronaut (at the distance of the moon) are unresolved points. Also no need to go into the problems of contrast against the moon's bright surface (which you would need to consider if an astronaut's reflective area was 100,000 times bigger), the reflected light from the astronaut is just too faint to be seen at that distance. 

In theory, a charged and rotating black hole can generate its own magnetic field. The magnetic (and electric) field can exist and can be measured outside the event horizon of the black hole. I completely agree with both existing answers that magnetic field does not "escape" from black holes, however I would argue that it is extremely unlikely that any real astrophysical black hole generates a significant magnetic field. The simple reason for this is that is is extremely difficult to see how any realistic physical process would deposit material with a net charge inside the black hole. i.e. Most astrophysical black holes are expected to be uncharged and have no magnetic field. (Though there are at least a couple of astronomers who think otherwise - see $URL$ ). The magnetic fields you are thinking of, and which are referred to in the link that you provide, are fields that are generated within the accretion disk of material that is spiralling in towards the event horizon. i.e. They are generated outside the black hole, and are completely unrelated to the magnetic field that you show for a planet like Jupiter, where the field is generated by processes inside the planet. 

The nebula colours are mostly due to forbidden line emission from oxygen (green) and nitrogen (red) and from the balmer series of hydrogen and occasionally, ionised helium. The key point is that this is usually optically thin emission, meaning that what we see is proportional to the number of excited atoms/ions in the nebula in the line of sight. This is important for your question, because in such cases, the surface brightness is independent of distance. The luminosity increases as one over distance squared, but the area occupied on the sky also increases by the same factor. From inside the nebula the surface brightness would be even lower. 

I think this question is too broad, but I'll take a stab at it. The Russell-Vogt (or sometimes Vogt-Russell) theorem is that the position of a star in the HR diagram is determined by its mass and composition. The luminosity is determined mostly by its central temperature and composition. In turn, the central temperature depends on mass and radius and the radius depends on the luninosity and effective temperature. Thus the question you ask fills textbooks. But to first order. The time dependence of luminosity is set by the time dependence of mass - ie mass loss (or gain) and the rate of change of composition, particularly in the nuclear burning regions of a star. In a star like the Sun, mass loss is relatively unimportant, so it is the rate at which hydrogen is turned into helium in the core that sets the timescale for luminosity evolution. Other processes that alter the core composition like mixing (due to convection, rotational mixing or diffusion) are thought to be second order effects. Restricting myself to the main sequence as an example. The luminosity of the Sun is set by the core temperature. The the nuclear reactions become faster as the mean particle weight increases and the core contracts and the temperature rises to maintain hydrostatic equilibrium. The rate at which this occurs is given by the luminosity of the star divided by the amount of H available (proportional to the mass of the star). Overall chemical composition drives this as a second order effect. A lower metallicity star is less opaque to radiation. The radiation escapes more efficiently and the star is smaller and hotter at the surface for the same luminosity. Smaller stars are hotter in the middle for the same mass and thus more luminous. Thus the luminosity evolution of low metallicity stars is faster than those of higher metallicity. See for example Figs 1 and 2 of Bazan & Mathews (1990). 

Let us suppose that the pulsar is spinning down at a uniform rate. So it has a period $P$ and a rate of change of period $dP/dt$ that is positive and constant (in practice there are also second, third, fourth etc. derivatives to worry about, but this doesn't change the principle of my answer). Now let's assume you can measure the period very accurately - say you look at the pulsar today and measure its radio signals for a few hours, do a Fourier transform of the signal and get a nice big peak with a period of 0.1 seconds (for example). With that period, you can "fold" the data to create an average pulse profile. This pulse profile can then be cross-correlated with subsequent measurements of the pulse to determine an offset between the predicted time of "phase zero" in the profile, calculated using the 0.1 s period, and the actual time of phase zero. This is often called an "O-C" curve or a residuals curve. If you have the correct period and $dP/dt=0$, then the residuals will scatter randomly around zero with no trend as you perform later and later observations (see plot (a) from Lorimer & Kramer 2005, The Handbook of Pulsar Astronomy). If the initial period was in error, then the residuals would immediately begin to depart from zero on a linear trend. If however, you have the period correct, but $dP/dt$ is positive, then the residuals curve will be in the form of a parabola (see plot (b)). If you have second, third etc. derivatives in the period, then this will affect the shape of the residuals curve correspondingly. The residuals curve is modelled to estimate the size of the derivatives of $P$. The reason that $dP/dt$ can be measured so precisely is that pulsars spin fast and have repeatable pulse shapes, so changes in the phase of the pulse quickly become apparent and can be tracked over many years. 

Stars that have a mass lower than about $0.5 M_{\odot}$ will not ignite helium in their cores, in an analogous fashion to the way that stars with $M<8M_{\odot}$ have insufficiently massive cores that never reach high enough temperatures to ignite carbon. The cause in both cases is the onset of electron degeneracy pressure, which is independent of temperature and allows the core to cool at constant pressure and radius. [A normal gas would contract and become hotter as it loses energy!] The end result for a $0.5M_{\odot}$ star will be a helium white dwarf with a mass (depending on uncertain details of the mass-loss process) of around $0.2M_{\odot}$. Such things do exist in nature now, but only because they have undergone some kind of mass transfer event in a binary system that has accelerated their evolution. The collapse to a degenerate state would be inevitable even for the lowest mass stars (which would of course then be very low-mass white dwarfs). As an inert core contracts it loses heat and cools - a higher density and lower temperate eventually lead to degenerate conditions that allow the core to cool without losing pressure. The lowest mass stars ($<0.3 M_{\odot}$) do get there via a slightly different route - they are fully convective, so the "core" doesn't exist really, it is always mixed with the envelope. They do not develop into red giants and thus I guess will suffer much less mass loss. The remnant would be a white dwarf in either case and is fundamentally different from a brown dwarf both in terms of size and structure, because it would be made of helium rather than (mostly) hydrogen. This should have an effect in two ways. For the same mass, the brown dwarf should end up bigger because the number of mass units per electron is smaller (1 vs 2) and also because the effects of a finite temperature are larger in material with fewer mass units per particle - i.e. its outer, non-degenerate layer would be more "puffed up". NB: The brown dwarfs we see today are Jupiter-sized, but are still cooling. They will get a bit smaller and more degenerate. A simple size calculation could use the approximation of an ideal, cold, degenerate gas. A bit of simple physics using the virial theorem gives you $$ \left(\frac{R}{R_{\odot}}\right) \simeq 0.013\left(\frac{\mu_e}{2}\right)^{-5/3} \left(\frac{M}{M_{\odot}}\right)^{-1/3},$$ where $\mu_e$ is the number of atomic mass units per electron. Putting in appropriate numbers I get $0.32\ R_{Jup}$ for a $0.07M_{\odot}$ Helium white dwarf versus $1.01\ R_{Jup}$ for a $0.07M_{\odot}$ completely degenerate Hydrogen brown dwarf (in practice it would be a bit smaller because it isn't all hydrogen). However, it would be interesting to see some realistic calculations of what happens to a $0.07M_{\odot}$ brown dwarf versus a $0.08M_{\odot}$ star in a trillion years or so. I will update the answer if I come across such a study. EDIT: I knew I'd seen something on this. Check out Laughlin et al. (1997), which studies the long-term evolution of very low-mass stars. Low-mass stars do not pass through a red giant phase, remain fully convective and can thus convert almost all their hydrogen into helium over the course of $10^{13}$ years and end up cooling as degenerate He white dwarfs. 

The question is compromised by saying that you allow arbitrarily perfect measurements. If we have a bolometer that can measure the amount of flux from a star, at a distance that is known to arbitrary accuracy, with arbitrarily good spatial resolution, then what we do is measure the bolometric luminosity from a 1 m$^2$ area at the centre of the stellar disk. This flux is $\sigma T_{eff}^4$. Now of course, stars do not have homogeneous atmospheres (spots, granulation, meridional flows, non-sphericity due to rotation...), so the result you would get would depend on exactly what 1 m$^2$ bit of atmosphere you were looking at. So, with my arbitrarily accurate instruments I would have to measure the luminosity from every 1 m$^2$ patch over the entire surface of the star. Each one would give me another estimate of $T_{eff}^4$; each would be somewhat different. That would be difficult, but the form of your question allows me to ignore those problems. At this level of precision, the utility of a single $T_{eff}$ for the whole star is questionable, but if you wanted one then it would be the flux-weighted mean of all the above measurements, and as far as I can see one can instantaneously determine it to whatever accuracy you desire. Of course it will then vary if you have a variable star, and it will vary from point to point with time due to granulation; so the accuracy of the $T_{eff}$ could depend on how quickly and by how much it varies compared with how long it takes you to do your arbitrarily accurate measurements. I think to get a better answer, you do need to specify some realistic observational constraints - such as (a) you cannot resolve the star at all, or (b) that you can resolve it, but observations can only take place from an earth-bound observatory (thus not allowing you to take flux measurements from the whole surface at once). One thing occurs, is that in unresolved observations, even with an absolutely accurately measured luminosity (assuming isotropic radiation) there is still the issue of what radius to use. The radius at which the radiation escapes the star (at optical depth $\sim 2/3$) is ill-defined and wavelength dependent. An error bar of maybe tens of km is appropriate here, since atmospheres are 100-200 km "thick". For a solar-type star this would limit $T_{eff}$ accuracy to $\sim 0.1 K$ !