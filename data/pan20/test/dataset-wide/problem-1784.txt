With most Cisco switches, you can apply ACLs on switch ports without going any routing trouble. This works with L2 switchs such as the Catalyst 2960G. 

You should consider using a standard Linux box with the appropriate number of gigabit NICs. You can use Pfsense as the OS, it will do everything you want through a website interface. 

I should also mention that XenServer includes everything that you need to install on top of bare metal. With Xen.org you need to install it yourself which can be very challenging/fun/interesting (pick whichever one fits the context). 

One not very smart way to handle this problem is to add a line in your initd startup script that deletes the file in question at startup. Make sure you put it somewhere before amavisd starts. 

Well I don't think there actually is a limited number of clients... Just remember how Wifi works. The speed of the access point is spread across all nodes. If you have 54Mbit/s and 100 clients, each one gets 0.54Mbit/s. It would be fine to have such bandwidth of course but the reality is not so pretty. The speed used for all the nodes on the access point is the speed that the worst node supports. To make this a little clearer, if you have a node that is far away and can only use 1Mbit/s then 1Mbit/s will be the bandwidth spread amongst all the nodes in your network. Now you spread 1Mbit/s over 100 clients and that gets you 1kbit/s, worst then 56k ! 

Your question as it is asked currently is kind of like asking "is a door secure ?". There is no real answer to that. If it's unlocked, it's useless. If it's a crappy lock, it's 90% useless and so on. If the database server is in a VLAN on it's own, it's pretty much useless as it cannot communicate with anyone else. If it is in a routed VLAN then it's somewhat more useful. Just placing a server in a separate VLAN gives you no extra security. Placing a server in a separate VLAN with IP/L4 filtering starts giving you a litte security. VLANs are just one tool amongst many that must cooperate to obtain the desired level of security. Don't forget that in matters of security, the point of failure is almost always the weakest link. This means that if your server is badly configured, no amount of network security will secure it. If your network currently does not have VLAN, I would suggest to put a software firewall on your DB server and secure the OS/SQL services. 

When you log in from a TACACS account, you have to type your own password twice (once at connection and once at enable). I imagine this works as well for RADIUS accounts. 

One way to do this is just to allow a certain set of IPs to access the repository. This works very well for LAN and VPN. Simple and efficient. 

If al else fails, you can always compile from source. I've done it before on CentOS and it actually works quite smoothly. 

I have actually had to solve such a problem recently. We have 8Mbit/s for 150 PCs. The problem was not so much regular bandwidth use but people who would download big ISO files and kill the bandwidth for everyone else. We handled this by inserting a caching web proxy (Squid on Pfsense) that allows for 2 bandwith limiting parameters. First one is max global bandwidth which is the speed of the line. Second is maximum bandwidth for one host. This is where this gets interesting. We supposed no more then 3 people would try to eat up all the bandwidth aht once. Therefore we set the second parameter at 1/4th (2Mbit/s) of the first parameter. This kept large downloads from killing the internet for everyone yet allowed for decent download speeds. All our problems then vanished. You don't want to set the max banwidth per host too low because it'll also be your maximum download speed. 

Have you considered creating a Google Apps account for your domain then creating 25 email adresses under that domain ? 

My rule of thumb is : Go for 64-bit. It doesn't hurt. Every processor alive today supports it (except the Intel Atom). 32-bit will diseappear one day, 64-bit still has a long life ahead of it. If it's a server app, go 64-bit and maintain some sort of retrocompatibility. 

It may also be possible that your backend web servers are refusing too many connections from a single IP. With HAProxy (without tproxy), requests are seen coming from the load balancing node. This could create such problems. 

Unplugging the computer would solve that problem. There also maybe a cable running from your network card to your motherboard. If you remove it, it may disable WOL. 

If I were you, I would look at the SASL logs (if that's what you're using) and see which users are being logged on at the same time. If you don't have SASL logs, you can always look at MySQL/LDAP search logs around that time. Considering he is logging in as null, I would check my database to make sure I don't have malformed entries or something like that. Base64 encoding is the norm for SMTP authentication, so that's normal. There is MD5 digest but that won't help if he has "valid" credentials. A temporary solution would be to block either his IP or the entire IP block. 

Depending on the type of virtualization, 5% overhead is pretty much best case scenario. With full paravirtualization, you can achieve such overhead on IO-light workloads quite easily. With hardware-assisted virtualization (technology used by VMWare), it is possible to achieve such a low overhead on IO-light workloads on an hypervisor with few VM. With full virtualization (no CPU extensions), 5% overhead is pretty much a dream. Keep in mind this can depend on very many factors. Virtualization tends to add a significant amount of latency between disks and the guest OS. This will increase IO wait and therefore load averages while keeping CPU usage rather low. If your storage is on the lower side of the IOPS scale, this will have a very big impact. If you are using network storage, this will almost always add latency due to having to access a network for each IO instead of just accessing an internal bus. Virtualization can also add extra network latency if you use special network configuration modules such as virtual switches but this usually is not very significant. Virtualization tends to add many extra interrupts which are required to switch from a VM to another. Depending on the scheduler of the hypervisor, this can be significant. There isn't much you can do about this since it is just due to the nature of virtualization. But it is something to keep in mind as a justification to lower performance. Due to the single-threaded nature of your application, having more cores will yield no significant performance improvement. Both CPUs have similar frequencies but you will notice the X5650 has a slower frequency without "Turbo Boost". You may want to check that feature is compatible/enabled with your setup. 33% overhead on IO intensive workload is, I find, not so bad. Try separating the storage for your two VMs and see if it helps. 

You have OBM from Linagora. It's main objective is to replace Exchange. Most of the web site is in French but you can probably contact them and get info. 

If you want to go the open source route, you have Convirt and Enomalism If you go the Citrix way, you have the XenCenter which looks very much like VMWare infrastructure. If you need entreprise level features, this is what you really should be using. 

I had to solve such as problem a while back and from what I researched such a setup would not be possible. What I had to do was have the MX servers forward all the email to the amavis server using plain SMTP. This allowed for greater flexibility and a standard configuration. Having an extra mail daemon was worth it because it potentially allowed us to configure what mail would bypass or not the amavis server. This also allowed the email to be forwarded quickly from the front servers so that they could handle the next emails without having to wait the spam processing. 

I could go on for a while. I have switched my personal machines from Debian to OpenSolaris and am not looking back. 

I don't think you're not seeing the entire picture here. You are wanting to connect a file server at 10Gbps speed, which may sound like a sexy idea. The thing you are not seeing is the ability of that server to generate that amount of traffic reading from disks. Getting 1GBps from a file server is, today, a very good achievement. 10Gbps will not only be expensive as you have realized yourself but at minimum 90% useless. Your best option is to start putting in some blazing fast disks in your file servers if it needs to provide such great amounts of IOs. I strongly believe the "affordable" (notice the quotes) path to this is SSD drives in fast RAID configurations (that is RAID10). As for networking, a 4x1Gbps agregate will do the trick fine and you can even add more later. Watch out for the fact that internal buses (read PCI*) are not always capable of handling multi-gigabit speeds. This is especially true if you are not using server-grade motherboards. I believe this is your only "affordable" option here. Infiniband cards are not horribly expensive. I believe you can find some for ~150$ but the switch will be very expensive.