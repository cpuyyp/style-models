However this would be too tedious since I have thousands of records (above tables are just a simplified example). Is there a way in Access to relate these tables directly without having to worry about the ID's? I know I used to be able to do this in ArcGIS using the "Relate Tables" feature so hopefully theres a way to do it in Access? 

I am trying to establish a one to many relationship between the tables based on Field1. However Field1 is not unique in either table. This is conceptually possible, however, because each record in Field1 does have a corresponding record in Field2. I could create an intermediate table containing just the ID's as such 

AE Career Fair - Struc contains a list of companies where the company name is distinct. Office Locations - Int is a junction table containing in one column all the companies occuring in AE Career Fair - Struc, but duplicated based on the countries in which they occur. Countries contains a list of all the countries in the world. I was able to run a query for companies occuring in one particular country of interest. I would like to take this a step further and query this relationship and determine which companies are in more than one country. I cant really think of a way to do this based on how I currently have it set up. Could anyone provide indications as to how this would be done? Thanks. 

But neither is giving me the desired result. Is it possible to extract these results using just one query? Also why did the previous two not give me the intended results? 

I have a simple many to many relationship set up on access with "List", "DayJunction" (Junction Table) and "Days". I am trying to query for records in "List" that have no related records in "Days". 

I have a table in MS Access containing a list of about 500 companies and some information about them. I am trying to create another table that contains information about the office locations of these companies. I am think to populate this table with the following fields: Country, Company 1, Company 2.... all the way to Company 500. The Country field will contain all the countries in the world. The other 500 fields will contain True/False values for whether the company has an office in the country in question. Needless to say, this will be a huge table containing true/false values, furthermore it will be difficult to populate and enter data accurately for this table. Does anyone know of a more "correct" and data efficient way to store this information and query it easily? The objective is to be able to summon a list of all the countries where each company has an office location easily and efficiently. Or to start from a country and come up with a list of all companies present in that country. 

** The constraint UNQ_IntegerSettings_SettingID_PreviousFinishedAt ensures exactly that. The first interval does not have a previous one, which means that PreviousFinishedAt IS NULL. The UNIQUE constraint guarantees that there can be only one such row per setting. See for yourself: 

I would use the Profiler to monitor connections and determine if a query completed - it can give you a complete information about what connects and what runs. However, could you solve your problem using a more mainstream approach, such as starting a new thread, opening a normal SqlConnection in it, and executing a SqlCommand? 

In such cases I am storing version numbers, and using referential integrity to make sure version numbers begin with 1 and have no gaps. Because I use constraints, I am sure my solution works in high concurrency situations. I have described this approach in this article 

I regularly do such refactorings, without shutting the system down, and without stopping modifications against the table being refactored. I am moving data in small batches. Copied from my blog: Refactoring large live OLTP tables without downtime Refactoring tables does not have to be such a big deal. We do not have to shut modifications down, migrate all data into new structure, deploy modified modules, and do it all at once, while the system is down. Doing all migration at once may be risky, and if we make a mistake, there is no easy rollback to the old version. This is why whenever we need to refactor a large table, we are using an alternative, low-risk, no-downtime, incremental approach. The method I am going to describe has been used in practice several times, and we did not have any problems with it. All our transitions from old table structure to the new one were smooth and incremental. More to the point, we were able to set aside the migration at any time and switch to some other more important task, or leave for the day, or enjoy the weekend, all while the database was fully functional. Typical refactoring scenario I am totally making this up, I have never worked on bug tracking systems. Suppose that we are storing tickets in a table dbo.Tickets, which, among other columns, has a column named AssignedTo. As such, each ticket can be assigned to only one person at any time. However, the next version of our system will allow to assign a ticket to more than one person. Our plan is to create another table, dbo.TicketAssignments(TicketID, AssignedTo), migrate existing data to it, and change appr. 50 stored procedures affected by this change. To minimize risks, we are going to finish all database changes at least a few days before the new version of our system is released. This means that the current version of our system is going to run against our refactored database exactly as it did against the old one. BTW, the table dbo.Tickets is quite large, and is heavily used all the time. Before the migration. Prerequisites. We shall need some extra disk space, approximately as much as the old table uses up. Besides, we need a server that is not struggling with its current workload, so that it can withstand some additional work for the duration of our migration. Also we shall need a good test coverage on all modules using the table, including unit tests, stress tests, and performance baselines. Typically in our system we already have solid test coverage. One more thing: because table structure is going to change, inserts directly into the table are not going to work any more. As a result, all test data should be populated via stored procedures that continue to work against the new table structure. For example, we might use a stored procedure dbo.SaveTicket that has a parameter @AssingedTo to populate test data for our unit tests. Creating new tables, changing modifications. As our first step, we create two new empty tables: dbo.TicketsV2, which has all the same columns as dbo.Tickets, except it does not have AssignedTo column; dbo.TicketAssignments(TicketID, AssignedTo) We also change all the procedures which modify dbo.Tickets, so that they write to both old and new tables. This biggest risk in this step is introducing concurrency-related problems. We need to stress test our modifications well. If we have any problems, however, we can just run a rollback script, changing all the modifications back to their original version. Such rollback takes just a split second. In fact, we never actually had any problems at this step, because our stress testing harness is quite solid. Of course, our modifications get slower, but we have made sure that our hardware can handle it. We had not had actual problems with slow modifications either. Migrating existing data At this stage all the reports are running off the old table. As all new changes get saved into both old and new tables, we are also moving over all existing data to the new structure. We want this migration to be non-intrusive, so we typically just run one WHILE loop, moving over like 1K-10K rows at a time, so that our migration does not hinder OLTP activity and reports. Modifying the reports. While the data is migrating, we can take our time changing our stored procedures to read from new tables. Because we have good unit testing coverage, we can refactor procedures with confidence - if we break something, we shall know it right away. Because not all data has been migrated yet, we do not deploy the modified procedures. Verifying that migration completed. To verify that all data migrated correctly, we need to write a SQL query. To my best knowledge, there is no GUI tool that can efficiently compare large tables. Writing a query, however, is not that difficult. This query is going to use a lot of resources. We need to be careful not to bring the server to its knees while the query is running. There are several ways to accomplish that. Deploying modified reports. Once we have complete and correct data in the new tables, we can start deploying new procedures. We do not have to deploy them all at once - we can deploy them five or ten modules at a time, even if some other procedures still access the old table. There is one more problem our team need to be very careful with - our test server is not exactly identical to our production one. As such, we encounter a risk that our procedure runs fast in test environment, but is slow in production. This is why we first deploy our changed procedures into a different schema, which (schema) is not exposed to our user. For example, instead of altering procedure dbo.GetTicketsForDay, we create a new procedure Internal.GetTicketsForDay. Only developers have privileges on Internal schema, so users cannot execute it yet. Once we have executed Internal.GetTicketsForDay in production environment and are happy with performance, we can deploy it as dbo.GetTicketsForDay. Our only risk at this stage is that we can deploy poorly performing procedures. Our rollback strategy is simple - we just roll back to the original stored procedures that read from the old table. Finalizing the migration. Once all the reports access the new table, we can change our modifications, so that they no longer write to the old table. The old table can be archived out and dropped, reclaiming the additional storage which we needed for the migration. Conclusion As we have seen, we can refactor an OLTP table without downtime and with low risks, even if it is big and data migration takes a lot of time. One more thing: a common reaction to such Agile war stories is a recommendation to "do it right the first time". It is so common that I would like to address it right now. Of course, at the time when the previous version of bug tracking system, it was quite obvious that eventually we might need to add the ability to assign a ticket to more than one person. Of course, if we had the table dbo.TicketAssignments from the very beginning, we would not have to go through this complex refactoring. In other words, we should "do it right the first time", should we not? In general, "doing it right the first time", developing a flexible database structure that we should not have to change later, makes a lot of practical sense, but not under all circumstances. More specifically, when the previous version of bug tracking system was being designed and developed, there were hundreds of brilliant ideas, lots of great features that might be useful later on. Implementing all these hundreds of brilliant ideas would take years, and a much bigger team. It was just not possible. More to the point, it was not what our customers wanted - they wanted us to take care of their most important problems first, and quickly. They did not want to wait until we could provide more features. Instead of trying to add as many features as possible and as a result delivering a half baked low quality product, the team concentrated on developing only must-have features, on performance, and on very high quality. As a result, the product had a very limited set of features, but it delivered everything it promised, and it was simple, very fast, and rock solid. As such, it was a success. It would be quite easy to provide examples of over-engineered projects that tried to "do it right the first time" and miserably failed, but this is beyond the scope of this post... 

Lists the data you want for open transactions that have current locks. There is a whole bunch more information you can get out of it, but this is close to what you get in SYBASE. I've excluded non-DML commands and database locks (of which you'll get one per connection). 

A really solid grasp of the fundamentals is the best starting point for advanced and basic SQL. You can download Itzik Ben-Gan's Logical Query processing poster here $URL$ The poster helps you think about and write your SQL in a more logical and consistent manner. Also, probably quicker than most training courses that might take a week to get you up to the same level. That is probably the single best aid for moving quickly and smoothly from basic SQL knowledge to doing the more advanced topics. Itzik Ben-Gan's books are pretty good too, as well as the classic Joe Celko "puzzlers". But, almost everything involving "SQL development" revolves around that one poster - I have an A3 colour copy that goes on my wall. I also use it as basis for all SQL training. 

There hadn't been any transaction log backups, to allow the log to be re-used. So it has to get all of that empty log to get to the parts where there may be some transactions it needs to recover. So assuming you're prepared to suffer some (possibly quite a lot of) data loss, then you could recover it without the log. This is not recomended. Here's a link to the broad topic of SQL Server database recover, $URL$ You are not alone: $URL$ Finally, to a few commands that may help in this situation, again not recomended (you will almost certainly suffer data loss) $URL$ 

Not worth it for the reasons you gave, especially considering that you're not going to use it for production purposes. Reporting Services is a great platform for reporting, but I'm sure there are Open Source tools and frameworks to help you with that for PHP / MySQL. Ditto Integration Services, especially in 2012 (much better IDE for developing packages) it's a great tool for ETL, data staging and other import / export work, but probably overkill for you. I'd definitely stick with the open source for now - it's always going to be cheaper for licencing ;-). Especially as the cost model for SQL Server 2012 shifted to a different per core model. We had to disable cores on recent deployments to bring down licencing costs, as we'd bought hardware before-hand (on assumptions of the same cost model as before). Funnily, enough that's how I started out on SQL Server - with a "we must port this to SQL Server, because Oracle licencing just got too expensive" project. I look after about 16TB of high-transaction throughput SQL Server production databases for international clients. So definitely not anti-SQL Server and make plenty of use of all those tools you mention. 

Every time I run integration tests, I build my database from scratch off scripts stored in git. In the past, whenever slowness would become a problem, we would run the server locally on developer's workstation, with the database on RAM Disk - that should be fast enough. We did that many times for our applications powered by SQL Server I have not yet done that for PostgreSql, but I am very new to it. 

I would redesign the applications so that rows are saved in batches. Typically batches of 500-1000 rows work well for me, but you need to run your own tests. Before saving a batch, I would serialize my batch updates using sp_getapplock. Surely this serialization slows modifications down a little bit, but saving in batches more than compensates for it, so overall this works much faster thgan saving rows one by one. Also I would run my selects with SNAPSHOT isolation level so that they are npot blocked by modifications. That done, you may never have any deadlocks at all - we do not have deadlocks in our mixed load system, and so can you. Good luck! 

We have used this approach many times to change large live production tables without downtime, with no issues at all. 

It is kind of possible: you can invoke a scalar UDF from you CHECK constraint, and it kind of can detect cycles of any length. Unfortunately, this approach is extremely slow and unreliable: you can have false positives and false negatives. Instead, I would use materialized path. Another way to avoid cycles is to have a CHECK(ID > ParentID), which is probably not very feasible either. Yet another way to avoid cycles is to add two more columns, LevelInHierarchy and ParentLevelInHierarchy, have (ParentID, ParentLevelInHierarchy) refer to (ID, LevelInHierarchy), and have a CHECK(LevelInHierarchy > ParentLevelInHierarchy). 

For performance reasons, in most cases we must store current balance - otherwise calculating it on the fly may eventually become prohibitively slow. We do store precalculated running totals in our system. To guarantee that numbers are always correct, we use constraints. The following solution has been copied from my blog. It describes an inventory, which is essentially the same problem: Calculating running totals is notoriously slow, whether you do it with a cursor or with a triangular join. It is very tempting to denormalize, to store running totals in a column, especially if you select it frequently. However, as usual when you denormalize, you need to guarantee the integrity of your denormalized data. Fortunately, you can guarantee the integrity of running totals with constraints – as long as all your constraints are trusted, all your running totals are correct. Also this way you can easily ensure that the current balance (running totals) is never negative - enforcing by other methods can also be very slow. The following script demonstrates the technique.