This will set the hostid in a manner for which gethostid will return the same value as the donating box as the migrating box. 

My first suggestion is ask yourself why you need to encrypt the SSL certificate for, and what losses this produces on your operation if service is down for, say - 2 hours until it is discovered. What are the potential losses to you if the certificate is breached, and how much time would it take to rectify? If it costs more than your SSL certificate itself and the time to recover in the event of a breach is too high, then perhaps it is right to keep it encrypted. To be blunt, you cant have your cake and eat it. If you want to encrypt the SSL key, then you do so at the cost of non-graceful restarts and delays to your operation. If you encrypt it but provide a way to automate decryption, its probably just as worthless, or potentially more dangerous than having the key decrypted in the first place. If you make it your VPS providers responsibility, then your just shifting the problem onto them and it will likely either cost you a lot to give you the guarantees you want, or you'll have to accept that there will be times where the VPS provider doesnt meet your expectations when they reboot the VPS. My suggestion, is to make reboots timely and with forewarning, I dont think it would be unreasonable to ask your VPS provider for this. That way there shouldn't be an incident you cannot at least put under some control. *The problem of using SSLPassPhraseDialog is a similar, more common and problematic mistake people do with cron jobs. That is, deploy cronjob to run as root, then make the files ownership of whats run a non-root (say FTP) user who can potentially modify the application to escalate privileges. If you write a program to spit out the passphrase, make steps to ensure that the file is not readily readable nor is the file modifyable with anything but root (this includes making sure its parent directory its kept in is not owned by someone else). 

From there you can use inotify to watch for the file to be created/deleted to know the outcome. To test, set the threshold to something you know breaches it now and then touch a file on the affected filesystem. You should get your monitor file created. Obviously its probably best to put the monitor file somewhere not on the same filesystem (/dev/shm is a good place). 

This will manipulate the zone file in such a way that it will add the correct serial number and append the right nameserver. You need to call the script for every zone file. You can use or a shell loop of a glob to do this. This relies on the format of the serial number being such that it contains "; Serial Number" on the line. It also expects the domain name to be declared as the first word on the first line. Finally it expects the nameserver declared in "$LASTNS" To be there. You can change the value before running the script though. Oh and running this script against a zonefile already manipulated by the script will break the zonefile. There is a simple test to fix this but will leave you to work that out. 

Which tells a program to first interrogate and then interrogate DNS if unsuccessful. Since hosts does exclusively DNS lookups it does not peek into to do the lookup. 

This will ammend the SELinux policy to be aware that haproxy is is listening on these ports. The strange label "commplex_port_t" is the type definition of port 5000 which (I assume) haproxy defaults to if you dont choose a port. In port 5000 claims is registered to the service, hence the name of the label being out of place. 

Rule 1 of writing secure code: You must sanitize your user input. At the very least pass into you mkdir clause to prevent manipulating switches. Currently it could be used to create directories in arbitrary locations with arbitrary permissions. In of itself, it probably wont lead to a breach but you could pass stuff like: 

This is sometimes an indication the host is compromised or is running a misbehaving process. I say compromised because lots of malware forks into the background and ignores signals. However they do not normally close open file descriptors that were present at the fork. This leads to a situation where the socket that was held open by apache was inherited by the malware which ignores signals to finish. This causes the 'unable to bind to address' issue. Try issuing the command and see if any processes show up that are not apache. 

By this I mean, does virtual user A have the same access requirements as virtual user B? If the answer is yes, you're requirement is to offer unique authentication over multiple accounts yet all users have the same authorization relationship. If that is the case, what you are doing works. If the answer is no this means you want to offer unique authentication and unique authorization for said accounts. In this form, the design is fundamentally flawed, because it is impossible for the system-level authorization to identify one virtual user over another. 

Veth pairs aren't used this way. A veth pair is literally a device pipe, one end of the pipes packets come out the other end. The simplest synonym I can offer is imagining one half of the pair is a ethernet device, whilst the other end is a switch port the device is plugged into. You shouldn't treat the pair as being two separate devices, but one device which has two 'ends' to push/pull from. 

The answer to your question is you are using more memory than 72%. You can calculate it via the OOM Report. The bit that tells you what the state of the memory was is this bit: 

So for option 1, you might find you avoid all the semantics as the block allocation is done already (and quickly) because it was pre-allocated via and metadata is setup prior to the write. It would be useful to test if this really is the case. I have some confidence though it will improve performance. For option 2, you can use ionice a bit more. Deadline is demonstrably faster than CFQ although CFQ attempts to organize IO per-process such that you find it gives you better share of the IO through each process. I read somewhere (cant find a source now) that dirty_background_ratio will block writes against the individual committing process (effectively making the big process slower) to prevent one process starving all the others. Given how little information I can find now on that behaviour, I have less confidence this will work. Oh: I should point out that relies on extents and you'll need to use ext4. 

This is the absolute worst case scenario. So, broadly speaking -- whats the impact? There is no negligable impact for 97% of your served requests. Of the 3% that are affected, expect a higher delay in those being served up -- probably 500 millisecond. BUT of those 3% of unlucky requests around 2% of them anyway would have been slow because they wanted something out of the 6500Mb cache that was never in pagecache anyway! They however do suffer from the high disk utilization now. So, in summary with my contrived and assuming example you'll see in broad terms about a 3% loss in efficiency. (100% efficient would be all objects for every request are served out of memory). In 'normal' running in this contrived example performance would be about 98% efficient. Not bad for a cache that doesn't fit into memory! 

Low-impact consistent MySQL point in time backups with little effort on your part. Much of what LVM can do though is going to be being superceded by btrfs which has in-built volume management anyway, but as it stands now I think the possibilities you get with LVM make it a no-brainer for enabling by default. The downside to this is it can be a little more trickier to rescue from, but its one of those things you figure out once and remember. More importantly though you lower consistency of your data. Not all LVM implementations support filesystem barriers for example and that might be worth something to you depending on what your doing with your system. 

I think you will need to create multiple route tables. One is your default meant for normal traffic, the next one is your special SSH table which contains route entries only for the ISP connection you want to use. Next, setup iptables to mark packets which are from your IP and listening SSH port. Finally, you can setup ip rule entries to route based off of the firewall mark. Theres lots of info (but you might have already seen it seeing as your this far) in the LARTC documentation: $URL$ So, as an example: 

It is a semtimedop system call. A semaphore is a value which which is used to maintain synchronization between processes. In your example "semtimedop" means apache is waiting for the semaphore to become zero with a wait occurring until the value does become zero or a timeout occurs and thus the process is blocking. You have not mentioned whether or not this is a problem for you so theres not much more than that I can add. 

The -F defines the filters and the -S defines the syscalls, the more filters the less intensive it is on the kernel to track it. So in this case I filter on the user (apache), the vhosts directory and arch. Arch becomes important b64 being 64 bit b32 for 32 bit. You can set these up long-term by putting the rules in /etc/audit.rules. 

They might be your groups now but not for the shell you are using. You can see what supplementary groups you are in in this case by running 

Moving Between Users in DAC (Unix) and MAC/TE (SELinux) Lets define some terms for this to make sense. 

Alter this value to be something way lower. Perhaps would be a more reasonable number. You also shows some 'orphaned tcp socket' errors. You may also wish to reduce the values for and which will permit you handle more sockets (at the cost of latency). So, to answer your question why are you killing processes with so much free swap - the answer to that is the setting is ridiculously high for the amount of actual memory you have in each zone. Reduce this value from 32MiB to 1MiB. 

and have different meanings for the two different situations. in the client context makes all writes to the file be committed to the server. causes all writes to the file to not be transmitted to the server immediately, usually only when the file is closed. So another host opening the same file is not going to see the changes made by the first host. Note that NFS offers "close to open" consistency meaning other clients cannot anyway assume that data in a file open by others is consistent (or frankly, that it is consistent at all if you do not use to add some form of synchronization between clients). in the server context (the default) causes the server to only reply to say the data was written when the storage backend actually informs it the data was written. in the server context gets the server to merely respond as if the file was written on the server irrespective of if it actually has written it. This is a lot faster but also very dangerous as data may have a problem being committed! In most cases, on the client side and on the server side. This offers a pretty consistent behaviour with regards to how NFS is supposed to work.