There's a 'block policy inheritence' setting sitting between the parent and child DCs (it's set on the AD object for the child domain). You can untick it and policies will start cascading down. To prevent blocking of a GPO, select 'enforce' on it. 

31 days doesn't seem unreasonable, but it entirely depends on what's being logged and what the potential usage of the logs would be. The size of the event log depends entirely on how many events have been recorded. You can fix the event log retention at 31 days but you will need to consider: 

I realize these options won't provide exactly 8Gb RAM, but that amount appears to be an inefficient one for this processor type. Multiples of 3 seem to be the most appropriate. 

The EFS built into any Windows OS XP, 2003 or higher will allow multiple-user EFS. $URL$ No need to involve any additional software. 

You should be able to strap the disks together using this method, too, and present it all as large aggregate LUNs. 

The system will take a variable number of SAS cards/shelves. Each MSA70 can be daisy chained with a second one, and each card has 2 SAS ports. So 2 cards in the server = 8 shelves with a max capacity of 60Tb RAW, and you can scale all the way down to a single card with an MSA bundle deal that'll yeild the 4Tb you need. 

My switching knowledge is sketchy, but: You can't send more than one untagged VLAN to a port. As soon as you start sending more than one VLAN you HAVE to use tagging. Also I'm not sure there's such a concept as 'no PVID setup', I think that the PVID is already set on all ports, but you can change the value. 

Do your pings traverse a gateway to reach your VM? The setting that should fix this for you is in control panel -> network -> vpn connection -> tcp/ip settings -> Advanced Look for 'use default gateway on remote network' and un-check it. You'll probably get stuck in the middle on this - on one hand, you want some traffic to route via the VPNs default gateway. On the other hand, you want some traffic to route via your own network. If the tick-box doesn't solve the issue entirely for you, you'll need to see what IP ranges are needed in which networks, and start adding static routes as per this guide: $URL$ 

I suspect that you're using consumer-grade drives for your storage? If so, these have onboard error recovery systems which will stall the volume while error recovery is being attempted. When this occurs, all storage serving can be delayed for a considerable period of time (10+ seconds). In enterprise-grade drives, this 'feature' is disabled or never included, on the assumption that error recovery will be handled at the RAID-array level (RAID's implicitly assumed for enterprise deployments). For example, Western Digital refers to this feature (or removal of a feature!) as TLER - Time-Limited Error Recovery. In practical terms, it means a drive with TLER enabled won't stall out for an extended period of time to perform sector recovery/remapping/whatever. So if you're running consumer drives, there's a fair chance you've hit an error on one of your disks, and it's repeatedly stalling out while it attempts to recover. Solutions for this may be a bit tricky - I don't know if any third-party disk error scanners will support VMFS, and wouldn't risk pulling the disks and scanning them with ANYTHING, unless completely sure it won't trash the volume. 

As I understand it, you can't pull NetBIOS out if you're still running Exchange 2003, but 2007 upwards will operate without NetBIOS. 

Platespin Protect offers this functionality. It takes live copies of production machines, and is able to spin up a DR replica of the 'workload' on-demand. Replication runs in the background and can replicate across a WAN. It's source-agnostic, and supported targets include ESX, Hyper-V and Xen. Time to bring a DR copy online is about 5 minutes. It boots the target VM, installs relevant device drivers into the OS, then brings the box up with networking settings that you specify. You still have to have some means of your end user accessing this DR replica, however. The key issue you have here is your criteria that the replacement be brought online without any delay, while also being an identical copy of a highly customized machine. What this implies is having some kind of 1:1 permanent sync between the machines. This is clustering. You want to cluster desktops. My gut feeling is that providing a VMWare VDI environment for your users will be the best solution for you. This makes the local workstation irrelevant as long as you can get it to connect to the VMWare infrastructure. They'll also be able to run their personal environment customized in whatever manner they like, and on the back-end you can provide proper redundant power, cooling, and solid VCB-based backups. 

This depends on how your storage is accessed by the virtual hosts. Is it local storage in the hosts drive bays, or remote SAN storage over iSCSI/Fibre Channel? What are the IO speeds you are seeing when benchmarking the physical and virtual instances of these servers? What are the hardware specs and configuration (drives, controllers, cache, RAID levels, SAS, SCSI) of your previous physical servers, and your new vmware host servers? There are some overheads to disk throughput on VMs, but not inherently big. It's much more likely to be a shift in how the storage is being made available, contention from multiple VMs accessing the same storage, or a change/problem in the underlying disk storage hardware (e.g. lack of write cache on the vmware hosts). There's a whole bunch of different ways to improve your IO performance on vmware, but it'll depend on having more info about your current setup. 

First off the usual disclaimer: VMWare is a company, not a product. Name the product that you're referring to, because they vary in their operation quite drastically. The stats within the guest operating system are only what's visible to that OS. It has no visibility of the load that other VMs on the same host may be generating, so you're only seeing your slice of the pie. That said, in an uncontended (low-load) environment, what you see is probably quite close to what's available. But if overall host load or individual VM load is high you're almost certainly not seeing the complete picture from within the VM. The CPU ID that's reported is usually the actual CPU of the host machine, unless CPU masking is being used (not that likely). You can't tell how many sockets or cores are available on the host via the guest, however. 

30 VMs served from just 2 spindles (disks) will probably suffer an IO bottleneck, even if those VMs aren't particularly IO intensive (either random or sequential). You're looking at 30 separate concurrent read requests occurring across widely separated areas of the disks. Lots and lots of time wasted seeking between places. I'd recommend setting up a second drive array if the option is easily available to you (spare drive slots or a spare external housing), and migrating your VMs across to it. 4-6 disks min. Another improvement would be a larger read/write cache, if you're only running on a 128 or 256 chip. Another place to check is the vCPU allocations as Zypher mentioned - assigning too many vCPUs to each VM is (counter-intuitively) likely to slow all the VMs down (each VM has to wait for a free core for every single one of its vCPUs before it can get CPU time, so a 4vCPU VM may get less cycles than a 2vCPU VM) Edit: thinking about it a bit more, there are also some locking problems you might come across by having so many VMs on a single LUN. You can encounter per-datastore locks during various VM operations, possibly power-on/suspends etc. That'll start to stack up quite quickly so slow boot-ups etc may be caused by this. You can get around this by setting up separate datastores within the same amount of drive space (resize the current partition to half, then create a new partition in the blank space. Spread VMs evenly between the datastores). About 15 Vms per datastore is a good maximum. 

Download the updates on one PC (but don't install them yet) Stop the Windows Updates service on all the PCs Go into C:\Windows\SoftwareDistribution\Download on the first PC, and copy all the contents to a USB stick Copy those contents into the same folder on the other PCs Start the Windows Updates service again 

It may be fruitful to investigate why the program needs admin priveledges. If it comes down to things like file or registry permissions, then you may be able to get the program operating under non-admin credentials by adjusting permissions to grant the user access. 

That said, it really depends on the organisation, and the kind of DBAs you have. Plus, you can grant a DBA administrative access over the SQL instance, without also granting administrative access to the OS. This is preferred if you don't/can't trust the DBA to maintain the OS, and you'll have to shoulder rebooting/maintenance responsibilities yourself. You've asked a loaded question though, since MS doesn't really have a fixed position on this issue. You might find some technet discussions on the issue but I can't see them issuing a white paper. The closest I can get you is the security best-practices doco: $URL$ 

Naveen - Is it the application that you have written, which is being shut down by DEP? If so, have you recently updated the application in any way, or were there no changes to it at the time this problem began? If it's a change in the code that you have made, that started all of this off, then you need to read up on what DEP is and how it works, and take the necessary steps to fix your code. I wouldn't recommend disabling DEP to fix this issue any more than I would recommend disabling your firewalls because you're having problems with a port connection. 

I'm already familiar with a partial solution - MOXA make an IP-connected serial server that can connect to a bunch of modems. This works in exactly the manner we need, but it doesn't go as far as integrating the modems - So we still have a bunch of them sat on rack shelves. Can anyone point me in the right direction on this? Edit: I'm thinking I can probably get closest to what I'm after by stacking one of these with one of these. I was hoping there would be an integrated, single-unit solution but maybe not. 

First things first - Is the test file comparable in size to the real database? If not, seek times may be the differentiating factor here. If you're not already using the vmware paravirtual scsi adapter for the log volume, I'd recommend giving it a whirl. It's not a magic fix, but certain workloads benefit from it. Usually, it adds a little latency but allows for higher overall throughput when dealing with a high number of writes. If it's feasible, and I realise it's a big hassle, you may want to look into hooking up a physical instance of this same server directly into the SAN and running the same test, taking ESX out of the equation. Since you're reading the IOPS figures from two seperate locations and products, I would also consider that a possible factor. That said, I wouldn't expect the figures to be off by 3x. 

You can disable RDNS lookups but you'll be reducing security somewhat. Check your system time is accurate on all the servers involved. Check the email that is arriving hasn't already been run through spamassassin before it reaches your spamassassin server, as that triggers the ENC_HEADER rule. 

I've encountered numerous problems with timekeeping in VMs, and across several different virtualization platforms, I've also had problems with the native Host-VM time sync tools. My advice is to have an authoritative source of time that is based on a physical piece of hardware (in this case, your KVM host server) and sync your VMs using the standard OS time sync methods (in your case, ntpd) back to that server. Often a shorter resync interval is necessary if the vm guests are drifting excessively. In short, I think you're on the right track with ntpd. As an example on VMWare, we found that the VMWare Tools time sync will only correct time if it drifts backwards on the VMs. It wouldn't correct the time if a VM got ahead of its host. 

Any one of the factors above (and probably a whole bunch more) will impact the reliability of a real network. So you can see that a holistic view must be employed to properly gauge a real network's reliability. Applying computer science directly to IT scenarios tends to be problematic because real-world factors are rarely considered in detail Here's a few real-world examples I've encountered where the reliability of a network couldn't be measured by k-connectivity: 

This error is a bit misleading, it (usually) refers to a problem with the virtual devices you've assigned to the machine. When you run through the converter wizard, how far are you getting before it fails? I suspect you've got all the way through the wizard, can inspect the source, can browse the target, and are back at the job screen when it tanks? Try disabling any kind of customization of the VM, in the wizard, on the last page. Check the devices you have assigned to the virtual machine, and remove everything but the bare min. Check that your defined VLAN networks are available on both ESXi machines and are labelled the same. Are you using the latest version of Converter? (4.0.1) 

Backup Exec 2010 has just dropped, and I'm about to implement a new BEWS infrastructure, complete with CALs and new central servers. When I specced this up last year, I ignored 2010 and focused on Backup Exec 12.5, since it's a mature product. In previous experience, major released of BE had numerous technical issues and seemed to improve significantly at the first service pack. However, our refresh cycle on the backup infrastructure is slow, the main driver usually being lack of support for some new server type (in this case, ESX has driven our current upgrade need). With this in mind, I'm wondering if Backup Exec 2010 should be my first choice, as it'll last longer under current support than 12.5, which will approach EOL soon. Has anyone got any perspective they could add to this? Right now, I'm leaning towards biting the bullet and going with 2010. 

A virtual CPU equates to 1 physical core, but when your VM attempts to process something, it can potentially run on any of the cores that happen to be available at that moment. The scheduler handles this, and the VM is not aware of it. You can assign multiple vCPUs to a VM which allows it to run concurrently across several cores. Cores are shared between all VMs as needed, so you could have a 4-core system, and 10 VMs running on it with 2 vCPUs assigned to each. VMs share all the cores in your system quite efficiently as determined by the scheduler. This is one of the main benefits of virtualization - making the most use of under-subscribed resources to power multiple OS instances. If your VMs are so busy that they have to contend for CPU time, the outcome is that VMs may have to wait for CPU time. Again, this is transparent to the VM and handled by the scheduler. I'm not familiar with KVM but all of the above is generic behavior for most virtualization systems. 

Standardize the OS. 3 flavors of windows workstation? Get everyone up to Windows 7. If you're not on a Microsoft agreement of some kind, look into it. For a 150-person company you can probably justify the cost in terms of how much labor cost you'd be saving over a year. Standardize the hardware. Make sure all workstation/laptop purchases from now on are Dells. Get all the workstations on a domain (if they're not already) and use Group Policy as much as possible to automate the post-OS-install configuration that you perform (proxy settings etc) Use folder redirection to point My Documents to the file server. Get all the users documents hosted on the network. Enable offline files with a GPO, and ensure it's business policy to save all documents into the network shares, with a clear understanding that any documents lingering on a workstation are not protected in any way. Get WSUS up and running and forget about manually patching the OS whenever you reformat. Get your enterprise AV solution's automated discovery/install process working and hook it into your domain so AV installs itself. Get line-of-business apps (and apps that are hard to configure) running on a Terminal Server, and make it available to the staff that need it. Give access to remote/home workers over VPN. Bask in never needing to futz around registering DLLs etc on workstations ever again.