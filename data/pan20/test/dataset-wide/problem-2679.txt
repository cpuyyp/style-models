In terms of non-statistical fallacies, the base-rate fallacy is analogous to generalization from an anecdote, and the confusion of the inverse is analogous to affirming the consequent. In the base rate fallacy, you focus entirely upon the desired result about which you have the best information, and you ignore all other cases, although they are more numerous or more likely, and end up outweighing the target case by sheer bulk. If you stay home because you live in a terrible neighborhood, where you have significant odds of being attacked by a stranger on the street, in any given meeting and are only a hundredth as likely to be attacked by an individual you have invited into your home, in any given meeting, but you meet the people you invite into your home several hundreds of times as often because you don't go out, you have committed a base-rate fallacy. You have focused on the significant case, without looking at the context. You are not necessarily any safer at home in this case. If you feel significantly safer avoiding men because a vast majority of murderers are men, but only a tiny number of men are murderers, and those men are unlikely to be among the men you are avoiding, you have committed the fallacy of confusion with the inverse. Your relative comfort is not proportionate to the change in risk, which is very, very small because you have reasoned from the wrong direction of the implication. The two are indirectly linked because they both indicate how bad humans are at grasping what can happen unexpectedly to a Bayesian ratio. The base rate fallacy assumes the odds of A (I get attacked) must be proportional to the odds of A|B (I get attacked because I go outside), ignoring to the odds of B (I go outside). But when B is not common, it is unwise to base your estimate of A on what happens to it when B is true. The confusion with the inverse assumes that the odds of A|B (men murder) must be proportional to the odds of B|A (murders are men). But these two are just not really related in any important way. 

To me, this question comes down to whether it is better to communicate something that is at the edge of one's ability to communicate, or to always stay firmly in the territory where one is certain. Nietzsche in particular considers it pointless to simply say the things one can be reasonably certain of. Others have already said those things better, and they are boring to him. They cannot be put 'presto', as he puts it in one place: they do not continually surprise you enough to sustain a high tempo and a bright tone -- they end up pushing instead of pulling insight. Instead, in many of his books, in particular The Gay Science, he probes the boundaries of things that appear obvious, hoping to communicate more than one might by simply stating the part one can consistently convey clearly. The result is that what is said is often captured as a matter of taste, or in objection to other positions that are commonly held. And this leads to people seeing him as basically a critic of everyone else, who just won't be clear about what he means. Instead, you have to sit with the impressions, and take in the work of philosophy as one would take in a work of art. Other names who are 'big for being deep' produce the same issue. It is difficult to talk about something as normal as time and existence (e.g. Heidegger/Hegel), meaning and reaction (e.g. Lacan), truly basic moral necessities (e.g. Kant), the kind of psychological furniture that people use automatically (e.g. Kierkegaard), the essential features of perception (e.g. Husserl) etc. without getting trapped in models that intervene and distort our interpretation. One has to process them very thoroughly, or subject oneself to them very completely and put forward the subtle effects that others have missed. That involves stating edge cases that threaten to be hidden by the simplifications of the everyday model. The drawback is that you may, in fact only be conveying impressions that are special to yourself, or that are omitted from the common model exactly because they are utterly pointless (to be a bit catty, e.g. Adorno). 

we really can only talk about the concept of God, and never productively argue either side of the existence debate. I think the appropriate approach to theology on a philosophical level is the Jungian approach parallel to Mathematical Fictionalism. The fact that any divinity may or may not exist does not limit our ability to delve productively into our own and others' intuitions of those concepts. Whether or not there is some Platonic realm of mathematical objects, we are continually compelled to consider one. And we are free to both believe in it and disbelieve it, as long as we agree to suspend that largely irrelevant judgment when doing actual mathematics. Likewise, we do not have to prove or disprove God in order to support or criticize actual religious behavior and thought. 

By many accounts, it can't. In order to have a fully naturalist account of mathematics, you need a middle ground between these extremes. Kant theorized that space and time are not aspects of reality, but instead are formats that our perceptions are fitted into because of the nature of human intuition. It would then be impossible for them to be empirical, because they are necessary parts of the mind that underly our ability to accumulate data. They are, in his terminology, synthetic, a priori facts. But that does not mean they are a part of some additional reality, as suggested by Platonism. They are instead patterns that we as a species impose on our interpretations of our environment, and that we share with one another and, indirectly, with all other related species. (Neo-)Intuitionists generalize this to the whole of mathematics and logic, making the range of structures expressed by mathematical and logical objects part of rational psychology, rather than of ontology. Mathematical facts do not describe what is true of anything or even what is truly necessary in the world, but they do dictate what it necessary for humanity to truly grasp any aspect of the outside world and give it a shared expression. Jungian psychology spreads this notion of shared, underlying structure beyond mathematics and into other realms like the patterns that underly recurrent religious sentiments and story structures, as the genetic component of our collective unconscious experience. Things like the notion of the repeatability of experience, or our ideals of safety and equality, and their competing notions of the coincidental or miraculous, or the notion of heroism and earned privilege then take on the same nature as mathematical idealizations. This makes mathematics less singular of an endeavor, even if none of the other related tropes are as deeply embedded and thus our reasoning about them can never be equally clear or perfect. And it emphasizes that all human abstractions require a shared underlayment, partially inborn and partially negotiated by elaboration of conventions, in order to retain their stability. 

The perspective you are pointing at is emergentism. From that perspective, the problem is that the physical point of view simply lacks adequate context to express the notion of identity and will, not that it actually conflicts with the idea. As you point out, physics proper does not handle notions like pressure very well. It has even more trouble with notions like acidity. Without the context of the 'bond' to identify which hydrogens are 'free' which can only be observed from a more complex POV than the laws of physics capture well, acidity is not a thing. That does not in any way make us doubt that chemistry has reasonably defined acids. But somehow, many of us are quite willing to look at the physical sciences together and assume that they should either correctly identify identity and will, or those things do not exist. From a higher-level psychological point of view, it is obvious that identity and will exist, and philosophical takes on rational psychology and political theory do think about that. From a point of view in one of those frameworks, say psychoanalysis, or Existentialism, you can reasonably say that if you have one of 'identity' and 'will', you get the other. For instance, for Kleinian psychoanalysis will is what allows an identity to detect that it is different from its source, and a maturing will establishes and clarifies its identity in order to defend itself from merger back into that source. From Sartre's perspective, 'will' is the illusion of freedom which we must maintain in order to have responsibility, and we insist upon having responsibility as a way of maintaining our identity. But if these are properly emergent phenomena, the way chemistry is an emergent framing of physics (which 'supervenes on it', as such people say), then 'will' may still be determined (as the idea it is a necessary illusion points out). More complexity does not necessarily reduce the ultimate nature of cause, it may just hide deterministic cause in the cover of mathematical chaos. If there is not already indeterminacy at the lower level, it should not arise at the upper level. There are all kinds of arguments that there is no 'free will' in randomness, and randomness and determinism together never result in choice. But, to my mind, they are pasting the ideas together wrong. One can easily say there is 'freedom' in randomness, but there is no 'will'. But then there is no reason 'will' cannot result from an emergence from physics and get its 'free' nature from the underlying randomness. As a parallel example, one can say there is no 'intention' in genes because there is no 'consciousness'. But that does not mean there cannot be 'intention' in a being derived from those genes. Consciousness arises as an emergent aspect of survival, as a way of keeping better track of one's environment. Then, added to the goal of survival itself, this emergence easily explains how 'intention' is still possible. 

I would claim that yes, those things are art, just as reasoning things out in a logical way with oneself is, in fact, an application of logical argument. It may be the ultimate aim of logic to allow each person to make his own decisions correctly, and not to resolve disagreements between individuals. In the spirit of Sartre, you are always the primary witness to yourself. All other witnesses are in some sense superfluous, except for the mirror they give you. But that mirror is a truly magical and necessarily transformative implement. I think people like Crowley (see "Liber Aleph Nought") and Starhawk (see "Dreaming the Dark" or "Truth or Dare") depend highly upon Nietzsche's notion that art is not done for a witness when they elaborate personal or small-group rituals to delve into self-expression and elaborate one's personality and confront one's cultural and personal bases and biases. Modern Witchcraft and Ritual Satanism (along with the whole range of Neo-Pagan, Thelematic, and other Ceremonial Magicks) are, it seems to me, very much about "Making Art of the Self", "The Creator in all of us" and "Investigation of Personal Power for its own sake." Concepts that pedestrianize the central messages of Nietzsche to good effect. Nietzsche often acknowledges that at his best he writes like a religionist and not a philosopher, and his true followers may be religionists of the 'New/Old Religions'. It is also obvious that having a witness makes art a much riskier endeavor, and that that risk elevates the effect on the artist. In the spirit of challenging the will and taking power in steps as large as is wise, therefore, having witnesses is not a negative thing, only dependence upon them. There is power in a proud performance that is absent from narcissism, and personal art is much more powerful when exposed even in dyads and small groups. 

We might start from some concrete examples of his assertion. I know where my keys are, and I am wrong but not lying. I am confused. Or I know dinosaurs aren't birds, and I am wrong, but not lying. I was taught biology 30 years ago, and I am misled. To some degree, in each case, I have done the work of justification and succeeded at establishing the truth of the proposition, but then the world changed. So the issue is not truth, or justification, it is time and mutability. Beyond the uncertainty the quote points directly at, having these additional dimensions makes it difficult to classify knowledge as a mental state. You could go well out of your way and identify it as a component of a composite mental state that transforms across time. But then what isn't? The notion is dauntingly broad. And you would need a separate component for every nuance of every item of information that you separately know, which takes us in unhelpful directions. 

God's, and only God's. By the grammar he is close to what he means -- 'what is meant': not what you understand, but what is there to be understood if only you could actually understand it. We never receive the thoughts of others perfectly. What they mean is not what we get out of it. And that gap is analogous to the more fundamental gap Kant wished to insinuate between "God's" intended thoughts and the way we, in our imperfection, actually understand them. 

It is hard to back an institution into a corner and make it defend itself on a schedule set by you as its interlocutor. This inability to address the real topic because the bandwidth is taken up defending oneself personally is the essence of ad hominem argument, in my book. It is hard to pull this off with an institution, unless you are another institution. The PR branch can always sit back and think before responding. Even the 24-hour news cycle has hours of down time. The press release can always be a a little longer, and address both concerns. We can rely on the media to filter the issue from the fog, and those really affected will eventually write off the media that focus on the fog. So, on the slower timescale where group decisions are made, I think it is basically impossible to conduct a real ad hominem attack of an institution without simply alienating all listeners who genuinely care about the issue at hand. As noted above appeal to reputation is a genetic (source) fallacy, not an ad-hominem one. But the two are linked, in my mind. The source fallacy is the mirror image of the ad-hominem fallacy wherein ineffective ad-hominem attacks nevertheless accrue to the attacked and can be leveraged against their future credibility. In my experience, it is exceptionally easy to apply genetic fallacies to institutions because people resent the political power that comes from institutional boundaries and ineffective past ad-hominem attacks remain more memorable than the actual outcome of the controversy. So what would otherwise be ad-hominem attacks are often leveled ineffectively at institutions with the ultimate effect of losing the battle to win the war. This is generally not labeled 'ad hominem', but instead 'poisoning the well'.