Here is some general discussion of the level-1 weight of Boolean functions -- $URL$ -- including the theorems that: 

Noga Alon published half a dozen papers under the name "A. Nilli". Mathscinet links directly from this pseudonym to Noga's publications. 

This 74-page paper in Journal of Machine Learning Research (by Huang, Guestrin, and Guibas) -- $URL$ -- is an amazingly useful and undergrad-friendly intro to the representation theory of the symmetric group (in a very surprising venue). Also, the fact that its applications are to machine learning and statistics looks very appropriate for OP's question. 

What are sufficient conditions on $f, g : \mathbb{R}^n \to \mathbb{R}$ such that $\{x : f(x) \geq t \} \cup \{ x : g(x) \geq u\}$ has piecewise-smooth boundary? Some remarks: 

The sharpest multidimensional Berry--Esseen Theorem I know is due to Bentkus and appears in the paper "A Lyapunov type bound in ${\mathbb R}^d$". $URL$ It does not use the characteristic function, though. 

The space $S(\mathbb{A}_{\mathbb{Q}})$ of Schwartz-Bruhat functions on the adele ring $\mathbb{A}_{\mathbb{Q}}$ is the space of all finite linear combinations of elementary functions as above. Note that number theory references often do not tell what the topology is on this space. It turns out that as a topological vector space $S(\mathbb{A}_{\mathbb{Q}})$ is isomorphic to the space $\mathcal{D}(\mathbb{R})$ of smooth compactly supported test functions on $\mathbb{R}$. 

1) This is the Chow variety of degree $n$ zero cycles in $\mathbb{P}^{d-1}$. 2) Yes, this collection of polynomials can be bundled together into the Brill form or covariant. 3) Rather explicit descriptions of the Brill equations can be found in the book by Gelfand, Kapranov and Zelevinsky on resultants. There is also a paper by Rota and Stein. But first check out Emmanuel Briand's page and in particular the articles "Covariants decomposing on totally decomposable forms" and "Brill's equations for the subvariety of factorizable forms" and if you read French (or German) the translation of the original article by Gordan (respectively the article itself). As an aside, analogues of the Brill equations for the variety of forms which are powers of forms of degree dividing $n$ have been given recently in my paper with Chipalkatti "On Hilbert covariants". 

If f is boolean-valued and all of its degree-1 coefficients are small then the weight at level 1 is not more than $2/\pi$; If f is boolean-valued and has very small variance $\alpha$ then the weight at level 1 is at most $2 \alpha \log_2(1/\alpha)$. ["Chang's Lemma", or "Talagrand's Lemma"] 

Cambridge University Press is perfectly willing to publish books which are also freely available on the web (at least in 99%-complete draft form). I'm not sure how far they'd go in terms of the most liberal Creative Commons license, but here are a couple of examples of it occurring: $URL$ -- scroll down to find the link to "The AGT Book". $URL$ I think there are additional other examples from Cambridge; those are just two I knew off the top of my head. Edited to add: Sorry, I didn't notice that David Speyer already pointed out Cambridge University Press in the context of Allen Hatcher's Algebraic Topology book. 

Before interpreting them in more advanced language like "string diagrams" or "monoidal closed categories" it might be good to stress that Feynman diagrams are very elementary combinatorial objects which encode contractions of tensors. By tensor I mean an array of numbers like $A=(A_{a,b,c})$ with indices $a,b,c$ running over some finite sets which need to be specified. If you have such objects say $B_{abcd}$ and $C_{ab}$ you can construct new ones like $$ T_{a,b,c,d}:=\sum_{e,f,\ldots,l} C_{ae}C_{bg} B_{eghf} C_{fi}C_{hk}B_{iklj}C_{jc}C_{ld} $$ Obviously one can produce tons of similar examples of increasing complexity and it is desirable to have a way of encoding precisely such constructions. A natural way of doing that is basically to use pictures or graphs which is what Feynman diagrams are. Linear algebra "done wrong", i.e., matrix algebra is the particular case where tensors have one (vectors) or two indices (matrices) only. Although, the $n$-dimensional determinant introduces a higher (Levi-Civita) tensor $\epsilon_{i_1\ldots i_n}$ given by the sign of the permutation $i_1\ldots i_n$ (and zero if indices are repeated). A significant part of functional analysis is about studying what happens when discrete summation indices $a,b,\ldots$ become continuous variables that are integrated over. Then, matrices $C_{a,b}$ become kernels $C(x,y)$ which one can make sense of, e.g, as distributions, by invoking the Schwartz Kernel Theorem. Feynman diagrams, as they are used in quantum field theory, typically correspond to this infinite-dimensional generalization. For instance, the diagram for the expression $T_{abcd}$ above becomes the main second order contribution to the four-point function of the $\phi^4$ model in $d$ dimensions if one decides that the tensor $C_{ab}$ now becomes the kernel $C(x,y)$ of the operator $-\Delta+m^2 I$ in $\mathbb{R}^d$ and one lets the tensor $B_{abcd}$ become the kernel $B(x,y,z,u)$ of the distribution in $S'(\mathbb{R}^{4d})$ given by the action $$ f\longmapsto\ \int_{\mathbb{R}^d} f(x,x,x,x)\ d^dx $$ on test functions $f\in S(\mathbb{R}^{4d})$. As for why this should have to do with the Laplace/stationary phase method, the reason is because Gaussian integration is an "algebraic" operation. Namely, it can be expressed as a differential operator (albeit of infinite order). For example if $\mu$ is the centered Gaussian measure on $\mathbb{R}^n$ with covariance $C_{a,b}$, then for any polynomial $P\in \mathbb{R}[x_1,\ldots,x_n]$, $$ \int_{\mathbb{R}^n} P(x)\ d\mu(x)=\left.\exp\left(\frac{1}{2}\sum_{a,b=1}^n C_{a,b} \frac{\partial^2}{\partial x_a\partial x_b}\right)\ P(x)\ \right|_{x=0}\ . $$ Note that Haar integration on $SU(n)$ can also be expressed as an infinite order differential operator (see my two answers to How to constructively/combinatorially prove Schur-Weyl duality? ). So Feynman diagrams also appear in invariant theory/representation theory (see my answer to Who invented diagrammatic algebra? for some examples and pictures by Kempe in the case of the invariants of the binary quintic that are given explicitly in nondiagrammatic fashion in my answer to Explicit formulas for invariants of binary quintic forms ). 

I too was looking into this recently and found good citations difficult to find. Here are two I found which I can recommend: .1. In the book "Special Functions and their Applications" by Lebedev, he gives the following clear theorem statement (with a moderately technical but not too bad-looking proof): "Assume $f \in L^2(\gamma)$ (i.e., is square-integrable w.r.t. the standard Gaussian measure) and is piecewise-$\mathcal{C}^1$ on every finite interval $[-a,a]$. Then the Hermite expansion of $f$ converges pointwise at every point of continuity of $f$ (and further, converges to $(f(x+)-f(x-))/2$ at any jump discontinuity $x$). It would be nice to also know that one has uniform convergence on any interval $[-a,a]$, but I didn't immediately see how to read that out. .2. The book "Gaussian measures" by Bogachev gives pretty careful statements about the domains under which formal operations (e.g., differentiation of Hermite expansions) hold. 

Jensen's inequality gives a lower bound, but it might be too trivial for your needs. For comparing your expectation with the independent case, one can use the method of cluster expansions in statistical mechanics. Your model seems more like an Ising model than a field of Bernoulli variables. 

The answer by JM Landsberg in the link from aginensky's comment precisely says that the $r+1$ lower bound on the degree of generators is true. See top of page 2 in the article "Prolongations and computational algebra" by Sidman and Sullivant where they point to the article "On the ideal of an embedded join" by Ulrich and Simis. 

In other words, this realizes a linearization in the unstable direction only. I would like to know about similar/related theorems, follow-ups, improvements, etc. that exist in the literature. Using keyword searches etc. has been quite disappointing and I can definitely use the help of people with expertise in the area. For instance, I did not know the above paper contained such a theorem until a chance discussion with one of the authors. 

The story with sharp thresholds for random constraint satisfaction problems somewhat fits into this picture. In the random k-SAT problem with $n$ Boolean variables, one includes each of the $2^k \binom{n}{k}$ potential clauses independently with probability $p$, where $p$ is chosen so that the expected clause density (number of clauses over number of variables) is some fixed constant $\alpha$. It is very widely believed that for each $k$ there is a critical density $\alpha_k$ such that for all $\epsilon > 0$ the following holds: 

One example: A 3-uniform hypergraph is the natural way to model the variable/clause structure of a 3-Sat instance. Since 3-Sat is one of the most important algorithmic problems in computational complexity theory, hypergraphs play an important role there. For just one of many possible examples, take a look at the following paper of Feige, Kim, and Ofek: $URL$ 

The Alon-Tarsi Conjecture: A latin square of order $n$ is a filling of an $n\times n$ matrix with the numbers $1, 2,\ldots,n$ such that each row or column gives a permutation of $1,2,\ldots,n$. Take the product of the signs of these $2n$ permutations and call it the sign of the latin square. Let $EVEN(n)$ be the number of latin squares with sign $+1$ and let $ODD(n)$ be the number of latin squares with sign $-1$. The conjecture says: If $n$ is even then $EVEN(n)\neq ODD(n)$. The original reference is: N. Alon and M. Tarsi, Colorings and orientations of graphs, Combinatorica 12 (1992), 125-134. See also this preprint by Landsberg and Kumar for a recent update. 

Let $X=(X_{ij})_{1\le i,j\le n}$ be a generic $n\times n$ matrix and $F_1(X)={\rm det}(X)$ the degree $n$ homogeneous polynomial given by the determinant. Let $$ F_2(X)=(X_{nn})^{n-m}\times {\rm perm}\left[(X_{ij})_{1\le i,j\le m}\right] $$ which takes the permanent of an $m\times m$ submatrix and multiplies by one's favorite linear form in order to make another homogeneous polynomial of degree $n$ (one could also use the entry $X_{11}$ instead of $X_{nn}$). This modification is called padding. Then define the number $$ c(m)=\min\{\ n\ |\ n\ge m\ \ {\rm and}\ \ \overline{G\cdot F_2}\subset \overline{G\cdot F_1}\ \} $$ where $G$ is $GL(n^2)$ acting on the affine space of dimension $n^2$ where $X$ lives and $\overline{G\cdot F_i}$ are Zariski closures of orbits. The big conjecture in the area or Valiant's Hypothesis (a complex version of ${\rm P}\neq{\rm NP}$) is that $c(m)$ grows faster than any polynomial in $m$. Now if $\overline{G\cdot F_2}\subset \overline{G\cdot F_1}$, then one has a surjective $G$-equivariant map $$ \mathbb{C}[\overline{G\cdot F_1}]_d\longrightarrow \mathbb{C}[\overline{G\cdot F_2}]_d $$ between degree $d$ parts of the coordinate rings of these orbit closures. So the game is to try to show that this does not happen, for $n$ insufficiently large relative to $m$, by proving the existence of a multiplicity obstruction, i.e., an irreducible representation $\lambda$ for which multiplicities satisfy $$ {\rm mult}_{\lambda}(\mathbb{C}[\overline{G\cdot F_1}]_d)<{\rm mult}_{\lambda}(\mathbb{C}[\overline{G\cdot F_2}]_d) $$ or at the level of ideals $$ {\rm mult}_{\lambda}(I[\overline{G\cdot F_1}]_d)>{\rm mult}_{\lambda}(I[\overline{G\cdot F_2}]_d)\ . $$ An optimistic approach is to try to show there exist occurrence obstructions, i.e., $\lambda$'s such that ${\rm mult}_{\lambda}(\mathbb{C}[\overline{G\cdot F_1}]_d)=0$ and ${\rm mult}_{\lambda}(\mathbb{C}[\overline{G\cdot F_2}]_d)>0$. This hope has been squashed in the work of Bürgisser, Ikenmeyer and Panova mentioned by Timothy. However, the possibility of multiplicity obstructions is still open. I think the approach by Mulmuley is to try prove the existence of such multiplicity obstructions by leveraging all the tools available from representation theory for the computation of these multiplicities. Personally, I have never been a fan of this approach. Having studied 19th century invariant theory in some depth, it seems more natural to me to approach the orbit separation problem using the explicit tools from that era. This article by Grochow seems to also point in a similar direction (I suspect the third article mentioned by Joseph is in the same vein). In classical language (see Turnbull or Littlewood), one has to explicitly construct a mixed concomitant which vanishes on $F_1$ but not on $F_2$. One also has to do this infinitely often (in $m$) in order to establish the superpolynomial growth property. Such a concomitant is the same as a specific $G$-equivariant map from your favorite model for the irreducible representation $\lambda$ to the polynomial algebra in the $n^2$ variables $X$ (Grochow calls that a separating module). Invariant theorists from the 19th century had two methods for generating such objects: elimination theory and diagrammatic algebra. A very baby example where $F_1$ and $F_2$ are binary quartic forms under the action of $G=SL(2)$ (see this MO question) is say $$ F_1(x,y)=x^4+8x^3y+24x^2y^2+32xy^3+16y^4 $$ and $$ F_2(x,y)=16x^4-24x^3y+12x^2y^2-2xy^3\ . $$ A separating concomitant (here in fact a covariant) is the Hessian of a generic binary quartic $F$ $$ H(F)(x,y)=\frac{\partial^2 F}{\partial x^2}\frac{\partial^2 F}{\partial y^2}-\left( \frac{\partial^2 F}{\partial x\partial y} \right)^2\ . $$ It vanishes (identically in $x,y$) for $F=F_1$ but not for $F=F_2$. In this case, the Hessian can be seen as an equivariant map form the irreducible given by the second symmetric power (of the fundamental two-dimensional representation) into the coordinate ring for the affine space of binary quartics. So a possible superoptimistic "plan" for GCT involves the following sequence of steps. 1) Find a way to generate tons of concomitants. 2) Identify some explicit candidates for the vanishing on $F_1$ and prove that property. 3) Show they don't vanish on $F_2$. Step 1) is in principle solved by the First Fundamental Theorem for $GL(n^2)$ but there is a mismatch: the determinant is a natural object in the invariant theory for $GL(n)\times GL(n)$ (acting on rows and columns) rather than $GL(n^2)$. One could try to repair the mismatch by expressing the basic building block for the invariant theory of $GL(n^2)$ in terms of the one for $GL(n)\times GL(n)$ (see this MO question for a similar reduction problem from $SL(n(n+1)/2)$ to $SL(n)$). Guessing the right candidates for Step 2) looks hard to me. Knowing beforehand that some multiplicities ${\rm mult}_{\lambda}(I[\overline{G\cdot F_1}]_d)$ are nonzero would definitely help. Although, one could procrastinate and defer the proof of nonidentical vanishing of the concomitant to Step 3) which should show more than that anyway. If one has such right candidates, showing they vanish on $F_1$ may be easy by arguments one could call Pauli's exclusion principle (contracting symmetrizations with antisymmetrizations), high chromatic number property, or simply `lack of space'. However, I think the most difficult part is Step 3). For example, in my paper "16,051 formulas for Ottaviani's invariant of cubic threefolds" with Ikenmeyer and Royle, the guessing was done by computer search, but with the right candidate in hand, the vanishing on $F_1$ was relatively easy to explain (it's a rather pretty example of chromatic number due to the global properties of the graph rather than some big clique). The analogue of Step 3) in our article was done by brute force computer calculation and we still don't have a clue for why it is true. The paradigmatic problem related to Step 3) is the Alon-Tarsi conjecture (see this MO question and this one too). In my opinion, one needs to make progress on that kind of question (the Four Color Theorem is of this type too, via a reduction due to Kauffman and Bar-Natan) before Valiant's Conjecture. Since the question is about breakthroughs in GCT. I think this article by Landsberg and Ressayre also deserves some attention since it suggests that a reasonable guess for the exact value of $c(m)$ is $$ \left(\begin{array}{c}2m\\ m \end{array}\right)-1\ . $$ Note that a proof of concept for the explicit "Step 1),2),3) approach", on a much simpler problem, was given by Bürgisser and Ikenmeyer in this article. Finally, for more information on GCT, I highly recommend the review "Geometric complexity theory: an introduction for geometers" by Landsberg. PS: I should add that my pessimism is specific to the Valiant Hypothesis which is the `Riemann Hypothesis' in the field. Of course, one should not throw the baby with the bath water and denigrate GCT because it so far failed to prove this conjecture. There are plenty of more approachable problems in this area where progress has been made and more progress is expected. See in particular the above-mentioned article by Grochow and review by Landsberg. 

If you're not concerned with constant factors on the covering radius (and it looks like you're not), then you should be able to get the right answer on volume arguments alone. On one hand, in order to cover all of $A$ with "balls" of radius $\epsilon$, you need to take at least $|A|/|B(\epsilon)|$ points, where $B(r)$ denotes the number of points in $A$ within Hamming distance $r$ of a fixed point in $A$. On the other hand, suppose you greedily choose balls of radius $\epsilon/2$, so long as these balls are completely disjoint. Certainly you will not choose more than $|A|/|B(\epsilon/2)|$ balls. But having done so, if you double the radius of each ball, you will have covered all of $A$ (by the maximality of the set of balls you started with). So you see you get the right answer up to not worrying about a factor of $2$ on the radius. For more details, I think the keywords to search for are "covering codes" as well as "Johnson scheme" (the latter being the coding theory terminology for considering all binary strings of a fixed Hamming weight). 

Another systematic study of multisymmetric polynomials is in the thesis of Emmanuel Briand available in .ps format at: $URL$ see also his other publications at: $URL$ Although not well-known the study of multisymmetric functions is very old and related to the search for explicit formulas for multidimensional resultants. Among the classics who worked on this: Poisson, Schlafli, Brill, Gordan, Junker. A good account of some of this old work is in the book by Faa di Bruno "Theorie general de l'elimination", p. 109--114 and 129--131. Another little-known reference is the beginning of Lecture Notes in Math no. 896 by Bernard Angeniol "Familles de Cycles Algebriques - Schema de Chow". 

In general, you need the covariance to be smooth, especially near the diagonal. For some particular covariances there are very quick ways to prove what you want: for example if you can rewrite the field as a convolution of a random distribution with a mollifier. As for a general theorem, see Thm 2.2.3 in "Random Fields and Their Geometry" by R. J. adler and J. E. Taylor. This involves suitable bounds on a finite difference operator hitting both entries of the covariance.