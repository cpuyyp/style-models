In classical Boolean logic at least, logical connectives can be completely understood semantically through their actions as truth-functions of Boolean values. In other words, the truth table associated with a particular logical operator provides a complete description of that operator. The standard truth table for the material conditional → is: 

This is the classical argument from design. Though the concept of intelligent design is a fairly ancient religious inclination, Aquinas's Fifth Way appears to be the first attempt at a philosophically rigorous formulation. 

At this point you probably notice something peculiar, namely that this truth table is awfully familiar. It turns out your modified conditional would just be completely equivalent to standard logical conjunction. Another oddity to note about this modified "condition": just like conjunction, it's symmetric with respect to p and q! That means, it doesn't matter which one of p and q you call the antecedent and which one you call the consequent. It's this property which it makes virtually useless as an operator whose intended purpose is to capture our intuitive notion of a conclusion predicated on a conclusion. As example to illustrate how important order is in changing the meaning of a statement, consider the Zeroth Law of Biology. The Zeroth Law states that "If you have had kids, then your parents have had kids." The converse statement, however, is obviously empirically false, so order matters here. Hence, we'd naturally disregard any alternative kind of conditional operator that fails to distinguish order. 

I also would disagree with Kuhn on whether many of his supposed examples of "paradigm shifts" would actually qualify as such. However, I think there are better and clear examples of paradigm shifts in the history of science, which if nothing else serve as proof of concept. In fact, I was reminded of what is probably the best example by the questioner's accidental word choice here: 

Conventionalism doesn't refer, at least in itself, to practical bearings. It is the view according to which there is no valid certain knowledge, no informational content about the world but only inter-subjective conventions. So it is essentially a claim regarding the nature of what we call knowledge; it is an epistemological stance. Pragmatism, on the other hand, is an all embracing approach for philosophical conduct. To relate to Conventionalism, one might inquire into the epistemological conclusions of Pragmatism: 

in an attempt to win the debate over the current state of affairs, as there is simply no need to give in to such historico-cultural matters in this context of knowledge. Claims such as The Incommensurability of Scientific Theories, whether formulated by Fayerabend or as it was plagiarized from Polanyi by Kuhn has long been given strong rebuttal themselves -- see Davidson in here for example. 

I think the honest answer is best viewed through the teachings of Karl Popper, notably the Falsifiability Criterion, according to which anything scientific has to supply a self refuting empirical criteria. That is, in the absence of an empirical way to test the hypothesis -- upon failure of which the theory is refuted -- the theory in question is hereby not falsifiable, hence not scientific. As presented, this could be an ideal behind scientific making. As such, it may be perceived as a defining characteristic to which science always aspires, doesn't always achieve. As Creationism assumes the existence of god, it is not falsifiable (There isn't, by any definition, an empirical test upon failure of which we deny the existence of god) and as such it is being demarcated from the natural sciences. I disagree that 

Actually, it's the opposite. Here's a simplistic overview of why: Suppose you do a finite amount of mathematics (prove a finite number of theorems from a finite axiom set). But by the incompleteness theorems, there are some theorems that simply cannot be proved. Thus, no finite amount of mathematics suffices to encompass all of mathematics. It's instructive to look at a specific case study of what situations like these actually look like in the course mathematical research. The first and still most famous case is the ancient question of whether or not Euclid's parallel postulate (PP) can be derived as a theorem from the first four axioms (the postulate states that There is at most one line that can be drawn parallel to another given one through an external point). One way geometer's posed this problem was to attempt to re-prove all the old Euclidean results from scratch without invoking the axiom of parallels, and see how far they could get. This system of geometry, Euclidean minus parallels, is called Neutral Geometry because it is neutral about whether or not the parallel axiom is true. The question is then, does Neutral Geometry end up being equivalent to Euclidean Geometry anyway? And of course, they failed time and again to prove PP within Neutral Geometry, and so they eventually started focusing their research on more indirect approaches, like considering what taking "PP is false" as an axiom would imply and presumably something absurd would result. One way to falsify PP is to say that we can find at least two parallel lines through a point, in contrast to at most one. Counter-intuitive results followed, but it this system was surprisingly proved to be consistent regardless, and it came to be called hyperbolic geometry. Whereas before geometry was a single unified system codified by Euclid, both complete and consistent, was shown to be but a stem of an ever branching family geometr*ies*. This is what really makes the implications Godel's incompleteness theorems so profound. It guarantees the openendedness of mathematics. In the beginning there was geometry and arithmetic, and then algebra and eventually calculus/analysis. The subject matter of mathematics could be understood in completely taxonomic terms: space, quantity, structure and change. But today we can see that mathematics exceeds any definition in terms of subject matter. On might say that mathematics is more of an art: the art of being creatively logical through the media of abstraction. 

Popper's Falsifiability criterion is a formal and logical one, and as such has nothing to do with the practical possibility or impossibility of testing a given hypothesis. It goes as follows: Given that H is a hypothesis / theory, if and only if H is falsifiable then there exists a finite set of observation sentences Γ such that ¬ H is a logical consequence of Γ. i.e., according to the criterion there must exist, in principle (as opposed to in practice), a set of observation sentences that by being true make the hypothesis being tested false. In order to be falsifiable, the Theory of Relativity needs only to provide such observation sentences, not the availability of real life execution method of the associated experiments. Back in 1916, Einstein provided a hypothesis which could, by definition, be tested -- by its own definition, that is. On the contrary, the Grue theory, by definition -- by its own definition -- cannot provide such testability in respect to itself, at least not at present. The Popperian reason behind the falsifiablity criterion, is that the attempts at refutation are what potentially corroborates a theory, so if no refutation is logically possible, no potential valid corroboration could be made, and so it isn't scientific. In other words, General Relativity is testable per Popper because there isn't any logical contradiction between testing it and all the rest of background assumptions. In contrast, present testing of the Grue theory contradicts the fact that time travel is logically impossible, which I would guess to be found in its background assumptions. The Popperian answer to the Grue theory in question would be, I presume, that it will be scientific once it'll be falsifiable in principle. If the Grue theory suggests also that the adequate time travel is logically possible then I guess it passes the criterion, but it does depend on that. 

Key word being IF the usual laws are available. In saying that, you just reaffirmed the truth of the very same p∨¬p that you presently are hypothesizing to be false. By the law of non-contradiction: ⊢ ¬(ϕ ∧ ¬ϕ) Hence, replacing ϕ with (p∨¬p), it follows that ⊢ ¬[(p ∨ ¬p) ∧ ¬(p ∨ ¬p)]. Option A) Let the whole darn thing explode by accepting that both LNC and its negation are both true. "Yes I absolutely accept the Law of Non-contradiction, and violations of it are impossible. Yes, violations of it also exist. And yes, I'm fine with that." Option B) Our original question was what if LEM wasn't true, but we leaped straight to investing what happens if it was false. But if LEM is not true, we don't have to take that to mean it must be false any more since the middle isn't being excluded. 

The thing about 'why' questions is that they presume an answer exists, whether or not an answer actually exists. Suppose I answered your question with "You should behave morally because such-and-such reason." That reason would then be an incentive to act morally. But we're operating under the premise that no such incentives exist. Contradiction. I also object that the very idea of actions without consequences might not even be coherent. An action is something done so as to accomplish a purpose. They are by definition characterized by the intentionality of agents. If there are no consequences, then by the same token there are no actions either. 

In your question you proposed a modified conditional (which we'll call ⇒ to distinguish it from the regular material conditional) where vacuously truths are replaced with vacuous falsehoods. The modified truth table that goes with this new operator is: