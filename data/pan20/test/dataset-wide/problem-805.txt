Use the Optical Spectra Query Form. You'll want to set PRIMTARGET to "GALAXY". Here's the list of possible parameters it can return $URL$ Note, there is a query limit of 500,000 rows, so you'll have to break up the query since you'll have more spectra than that. 

There is a Dantes Standardized Subjet Test (DSST) in astronomy. I have no idea if and how widely colleges give credit for these exams. Typically, you will start with physics and choose to specialize in astronomy. Therefore, most standardized exams are for physics which may have a handful of astronomy questions. 

Black holes are created because the core of the star becomes very dense, not just because the star is massive. Prior to the creation of the black hole, the core is able to create enough outward pressure to prevent the core from gravitationally collapsing to the density needing to create a black hole. 

A high density of stars definitely increases the chances of collisions, however, the high velocity of a halo star orbiting wouldn't increase its chances. Since the halo star is traveling very quickly, the halo star would only spend a small amount of time near the galactic core. Additionally, it would have less time to get deflected, gravitationally, by other stars in the core. 

HUDF suffers from cosmic variance. So while you could choose what you believe is a random and unbiased region of sky, your uncertainty will be larger than just the standard statistical sampling error. There would be additional uncertainty because of large scale structure since galaxies are not randomly distributed on the sky. 

Take another photograph of the same field that is less exposed. (Doesn't matter if you do this by shortening the exposure time, decreasing the ISO, etc.) This will give you an image with far fewer stars so you can easily pattern match your images. 

You're right, a redshift will give you a measure of a velocity, not an acceleration. I'm assuming when you say acceleration, you're referring to the accelerating expansion of the universe so you're not looking at the particular acceleration of an object (which you could get from examining the force of gravity imparted on that object). So in addition to a redshift, you need to find a way to independently measure the distance. To get out to an appreciable redshift (z~0.5), Type Ia supernova are used as "standard candles". They are considered standard because they're believed to be from white dwarfs that explode because they have reached their maximum possible mass, the Chandrasekhar limit, of 1.4 solar masses. Therefore, when they explode, they should all look about the same and any difference in brightness would be from how far they are away from us. So they measured the distance and the redshift of the galaxy the supernova originated from to determine the expansion of the universe appears to be accelerating over time. 

In the above, ACS is the Advanced Camera for Surveys and NICMOS is the Near-Infrared Camera and Multi-Object Spectrometer. That, I think, is the basic answer to your question, but there's a few interesting and related points I'd like to point out. First, the HUDF image was actually comprised of many individual images rather than a single 10-day exposure image (as the quote indicates). This would necessarily have to be the case, even if Hubble had the ability to stare at the HUDF region of the sky for 10 days without interruption. Cameras work by taking in light. The Charge-Coupled Device (CCD) which actually collects the light (and converts it to an electric charge) can only hold so much charge before it becomes saturated. At some point, you have to "stop recording" and read out the CCD which clears the accumulated charge. That way, you can resume again with a fresh slate. A 10-day continuous exposure would have easily saturated Hubble's CCD. So even if Hubble did spend 10 continuous days observing this patch of the sky, it would have been broken into many, many smaller exposures. This means, even if the Moon did get in the way during part of such a 10-day exposure, they could have just not made any observations during that time. The other related fact I wanted to point out is that Hubble has something known as the Continuous Viewing Zone (CVZ). This is a region of the sky which is never obscured by the Earth, Moon, Sun, etc. as Hubble orbits. As the name implies, it allows for continuous viewing, without interruption. The Hubble Deep Field (HDF) (the precursor to the HUDF) was observed in this region. However, there are reasons why you wouldn't want to observe in this region such as it being polluted by "Earthshine". 

I think the answer to this is that it doesn't have a name because such "ditches" don't occur for real meteor impacts. In movies and tv shows, these ditches are often shown as being caused by the meteor first hitting the ground at some angle, then sliding across the ground, digging out a ditch along the way, until they come to their final resting point where the crater is. When you stop to think about it, this scenario doesn't really make sense. These types of images (e.g., this one or this one) usually show a thin tail or ditch where the meteor first impacts which gets wider and deeper as the meteor scrapes on the ground, resulting in a final, large crater. But, the most energy a meteor imparts to the ground comes from the first instance it hits. If it were to scrape along the ground as portrayed in many movies, the crater would come from the point it hit the ground and the tail would be in the direction the meteor was traveling. The "ditch" or "tail" would get thinner and shallower as the meteor lost energy scraping across the ground. Generally the opposite is shown in movies. But all that is moot anyway because meteors don't scrape along the ground at impact. That would just violate conservation of momentum and energy. You really have one of two scenarios. 

Yes, minerals can be observed using spectroscopy on a telescope, typically from their thermal signature. We can determine what type of mineral based on the elements that make up the mineral as well as the type of crystal structure, which will give a different spectral signature 

We observe the temperature of the CMB as a ~2.7 K blackbody, but that's the redshifted version we observe. The CMB is also know as the "surface of last scattering" at the point of recombination when nuclei and electrons combined to form neutral atoms the universe went from opaque to transparent. This happens at a temperature of ~3000 K. From this we can estimate the redshift (z~1100) of the CMB which corresponds to an age given our cosmology. 

Schindler et al. 1999: Morphology of the Virgo Cluster: Gas versus Galaxies has details for $\beta$ model fits for Virgo and its subclusters. 

To estimate galaxy stellar mass with broadband photometry, you want to compare your observations to the spectral energy distribution (SED) of the galaxy. There are synthetic SED libraries which will generate SEDs for different types of galaxies with different masses. 

For each template you can figure out the expected luminosity in the I-band and find the best match. For this method, it's really best to have more than one point for comparison. Since the near IR will likely be a blackbody from your evolved stellar population, you might want to see if you can supplement your I-band data with 2MASS observations. 

To get an order of magnitude estimate you can just use the total mass $M$ and luminosity $L$ of the star and an assumption of your fusion process. Main sequence stars fuse Hydrogen in to Helium through the proton-proton chain, which converts 0.7% of mass into energy. So the estimated lifetime of the star would just be: $0.007\frac{Mc^2}{L}$ ($c$ is the speed of light). For example, the sun would have a lifetime ~$10^{11}$ years, but that's assuming that all Hydrogen is being converted to Helium via the proton-proton chain. Given the scaling from observations of the Sun, you can use the mass-luminosity relation to estimate lifetimes for other main sequence stars. 

I think that there isn't a strict answer to this question. However, I believe the answer is that there's a difference between the core of a hydrogen-burning star and the core of a protostar or star-forming, gas cloud. For a hydrogen-burning star, the core, as you say, is the region of the star where fusion is taking place. This is surrounded by the radiative and convective zones (which one comes first depends on the mass of the star). For true stars, the concept of the core is well defined. For things like protostars and star-forming gas clouds, the concept of the core is less well defined and more a nomenclature for a region of the object rather than a strict definition. I don't think it is correct to assume that because one is talking about the core of a protostar, that they are referring to the core as it is defined for a hydrogen-burning star. So what does it mean to be a core in a protostar or star-forming gas cloud? I will first point out that protostars can still be fusing. In the pre-main sequence stage, most stars will be fusing deuterium and we can consider the core to be the region where this is occurring. A protostar does not become a fully fledged star until it begins hydrogen burning, but deuterium fusion requires lower temperatures and pressures than the full proton-proton chain of hydrogen fusion (of which deuterium fusion is a part) and so can occur in protostars. We might also distinguish a core from outer regions based on a density or compositional change. If we plot up something like the density (or really any property that changes with radius), we are likely to see a drastic jump signifying some sort of boundary. Cores for star-forming regions may be defined by highly dense regions, high temperature regions, or regions "where the physics changes" because we've passed some critical threshold at some radius. These cores are less well defined and can't be likened to stellar cores very well. 

What this suggests is that the multiplicity of stars is highly variable, still somewhat unknown, and that it depends on the environment you look at (and I found a few papers discussing this very point). The Eggleton paper had the largest sample and so may be the most trustworthy in regards to a true average, but make sure to understand what their uncertainties are.