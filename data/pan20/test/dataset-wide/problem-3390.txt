Why do loanwords tend to be more polite, formal, technical, etc. than native words? I've noticed that native words in my language, especially those that refer to body waste and genitals, tend to be more informal, and even more vulgar than loanwords. The same goes with English: d__k and c_ck are vulgar, excrement is acceptable, feces is formal; lip is common and labia is technical. Why is that? 

Why were words for the four cardinal directions (east, west, north, south) in Romance languages borrowed from Old English? They could have used their own words derived from Latin because these words seem to belong to the basic vocabulary, and if anything, the Saxons wouldn't have been the dominant force over the French-speaking authorities (Normans/French). 

The sentence will be a CP in traditional GG: [CP [C have] [IP I seen the man]]. Note that constituent trees represent sentence topology rather than grammatical relations. In the example at hand, "have" and "seen" are coheads but they don't appear contiguously in the sentence. 

In (neo-)Davidsonian semantics, what you call activities is called events. Teaching/cooking in your example would be complex events. A typical example is "Brutus stabbed Caesar. Caesar died." vs. "Brutus killed Caesar". Brutus' killing Caesar is a complex event that comprises stabbing and dying (the former causes the latter). It's not an atomic (causative) predicate since there's a time spam between the two events. Google for "neo-Davidsonian semantics" for papers and slides that discuss events in more detail. 

Examples of words that literally mean "washbear" can be found here. This is apparently due to the fact that raccoons just love to wash things so much. But is it just a coincidence that many languages call them "washbears". They don't resemble bears much in my opinions; and the fact that many languages call them "washbears" but not "washdogs" or "washcats" (except for French which uses "washrat") makes me wonder if there's an ultimate etymology, and if there's a calque process going on across languages. So, why is it "bear" in so many languages? Is that a mere coincidence, or is it because of an ultimate common etymology? Is there calque across languages? If so, what is that ultimate source (from Latin or maybe German?)? 

Dependency parsing is constraint solving. I recommend you have a look at XDG, which is the only formally precise dependency grammar approach I'm aware of. 

The book you mention (Partee et al.) is the best one. If it's not an option for you, I'd recommend this one: Gamut 

The idea is that deep syntax structures should be as language independent as possible. In your sentence "will" is only a tense carrier and "to", though being a semantic preposition, functions as a case marker (illative). From a graph-theoretic perspective, to translate a surface tree into a deep dag is to remove all the nodes that represent function words from the graph. Thus at the level of deep syntax a sentence has the same structure in all languages unless there's a lexico-semantic mismatch (in which case the edge labels will be different). In grammars with feature structures, deep structures are constraint-based. In LFG, for example, c-structures are phrase structure trees but f-structures are isomorphic to (deep) dependency dags (if one substitutes thematic roles for grammatical functions). From a logical perspectives, deep syntax dags are "hierarchical logical forms". Lambda calculus could be used to incrementally assemble the logical form and there's a one-to-one relationship between logical forms and dags. Also worth mentioning is information structure which is sometimes added to dags (though it's not part of syntax sensu stricto). It may be expressed as an ordering on nodes (as in FGD). Note that to fully account for syntax one needs a LP component (such as topological trees) which means that surface dependency trees are uninformative since they can be reconstructed from structures at the other levels. They can be view as a blend of topological trees and deep syntax dags. Surface trees tell us little but are structurally closer to the phonetic form of the sentece. Update: A few definitions might be useful: 

How did the Latin past participle suffix -atus develop into modern French -é? Considering the two following examples: modern French état ("state; status") and été ("been"). Both derives ultimately from the Latin past participle status. But while the former, as a noun, retains a somewhat faithful representation of the root word, the latter deviates for some reason. Compare cognates in Italian, Portuguese and Spanish which have perfect parallels: Italian stato, Portuguese and Spanish estado are the single form of both the noun and past participle. On another note, considering the suffix -té of the unrelated noun été ("summer") which is on the other hand, regular, as it derives from the Latin suffix -tas. Basically, the most straightforward routes would be Latin -atus > French -at, -as > -é. Yet the past participle ending -é doesn't follow these routes. Why? 

You've correctly recognized that a predicate in syntax is not what's called predicate in formal logic. As has been mentioned above, appositives can be expressed as conjunctions in formal (first-order predicate) logic. In general, what's expressed by the logical predicate is the focus of a sentence whereas the argument is the topic. For example, "Melissa dormit" would be sleep(Melissa). But if you have "Dormit Melissa", you can't have Melissa(dormit), which is nonsense; formal logicians use lambda calculus to make Melissa a predicate: λf.f(Melissa). (I used a Latin example because it has free word order in order for both syntax trees to be identical.) Another possibility would be to use Davidsonian semantics which is very flexible and doesn't make you use lambda calculus. 

In modern times, there are countless ways of learning foreign languages, for example, through the Internet. But those languages are mostly well studied and understood, and a guideline on how to learn it has already been established. When I learn English, I work with dictionaries, textbooks, audio CD, etc. that are available everywhere. I wonder how the Portuguese missionaries learned Japanese without knowing anything about it beforehand; they didn't know Japanese, and they didn't have such things as textbooks or dictionaries to learn Japanese with; if anything they would have been the first people to write such textbooks and dictionaries; how did they take in totally strange words, especially those abstract concepts such as "intelligence" or "virtues"? In other words, how is an completely strange language is first learned and studied? 

-asъ changed to -axъ in this context, it’s a pretty known fact in historical Slavonic linguistics. Note that -s- can still be found in modern Lithuanian in plural locatives. 

There is only stylistic difference. Attributes in Latin can precede or follow the head noun so both are well-formed and have the same meaning. 

To achieve this you need dependencies at deep structure (stripped of all the function words) and lexical mapping. You need a good parser and a valency lexicon with mapping to semantic/θ roles, which is sufficient to produce argument structures and semantic representations. 

There is no consensus on how participles should be classified as for POS. Different treebanks/taggers mark them as adjectives or verbs. Some theories would classify this example as a verb as for form and an adjective as for function. It's all about terminology. In any case, the question in the test was unclear.