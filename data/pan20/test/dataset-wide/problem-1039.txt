I can't come up with anything better than building out a Dynamic SQL Statement for what you're trying to do. This is neither easy to read nor efficient in execution, but it should do what you want and will work with any number of parent/child levels. I'm also sure there are other ways to assemble this statement, so feel free to adjust this to your liking. DB Fiddle for the following: 

In an attempt to do proper Edition Evaluations for future projects, I'm trying to find a good way to mimic Standard Edition functionality limitations on a SQL Server 2016 Evaluation or Developer Edition instance. I'm thinking that for a given database, I could configure Resource Governor in tandem with some DDL triggers that block Enterprise-feature commands via EVENTDATA calls to emulate a lot of the resource usage and feature limitations that are specific to Standard Edition. Certain instance-level limitations such as Buffer Pool size, etc. cannot be emulated directly, but I can limit many of these things at the query level which I'll consider good enough. Is there anything that the combination of Resource Governor and DDL triggers wouldn't be able to cover? Alternatively, does anyone have a solution already available to do this that I've not yet stumbled across? Are there other approaches that would be easier? This question relates to the core database engine. I don't think there's even a feasible approach to trying this with other features such as SSIS, SSAS, etc., but if anyone knows of a similar approach for those features, I'm all ears. Final note here for anyone suggesting I just install Standard Edition; I hesitate on this approach due to the licensing costs that would be associated with it. I would either have to purchase licenses outright or have sufficient MSDN license coverage for any users touching it. Sadly, this project is going to involve more people than have MSDN licenses and the cost to fully purchase Standard Edition 6+ months prior to the project going live isn't going to get approval from a budget committee, especially when approval of the project rests of the feasibility testing we need to conclude beforehand. 

Ok, so what does that really mean? It just means that you have an ordering mismatch between how the values are stored in the Index Tree and how they are stored on disk. Low Index fragmentation does NOT mean data is stored contiguously or even on the same drive within a drive array. How does Index Fragmentation hurt you? Really, through my investigation, the only downside to Index Fragmentation that I've been able to identify is that read-ahead operations are less efficient (e.g. more are performed) against an index that is fragmented (in regards to Seeks and Range Scans operations) as opposed to said index being absent of fragmentation. Read-Ahead operations only happen when you pull an index/table off of disk. If you are able to maximize the time that the index remains cached, you will see less and less read-aheads negatively affect performance because you won't constantly be going back to the disk. Additionally, Index Scan operations (as opposed to Index Seeks or Range Scans) are less troublesome because the entire index/table is coming off of disk any way. Why does most documentation attribute High Index Fragmentation with Poor Performance? The answer is that often times, High Index Fragmentation creates unnecessary white-space within an index due to page splits and other operations. This white-space is the real detriment to performance because it requires more pages be read into memory to get less data. More white space = More I/O requests = less performance. As Kendra's article alludes to, even the BOL article makes mention that setting FILLFACTOR too low will cause performance issues. This is because of the exact same reason. More reads for less data is less performant. Setting FILLFACTOR to anything other than 0 when necessary is forcefully adding whitespace to your indexes/tables. Let's do some basic math here and see why this isn't a good blanket-recommendation against your database. Let's say you have an index that is 100GB in a completely unfragmented state with the default FILLFACTOR set. You set FILLFACTOR to 90 and rebuild the index. The index will now become 111GB (100/90 * 100), which is an increase by 11%. This also means that you'll have to read 11% more pages to get the same amount of data off of disk. Apply this system-wide, and you've just increase read operations by 11% across the system. The other detriment to white space is that it is not only stored on disk, but it's also stored when the page resides in memory. That means you're reducing your memory footprint by 11% just to help "prevent" bad page splits. Really, are page-splits so bad that you need to reduce your memory footprint and increase the amount of read operations off of disk? This setting is a negative double-whammy. There are definitely scenarios where adjusting FILLFACTOR is needed and in the best interest of your database, but to set it across the board in a blanket fashion is not a good idea in my opinion. I'm not saying you shouldn't adjust FILLFACTOR, rather you should only adjust it when other options are less desirable. So, after my rambling, what I suggest you should be reviewing instead of index fragmentation is the avg_page_space_used_in_percent field in the sys.dm_db_index_physical_stats DMV. If you see that creep up past 10% or higher, a reorg/rebuild would be worthwhile because this is the only way to compress white space out of an index. Often times, I see that this percentage of white space hits when the index is significantly fragmented, generally at 60% or more. Also, because this value doesn't creep nearly as quickly as index fragmentation, this may allow you to reduce the frequency you perform index maintenance operations. 

I'm not a MySQL guy, but maybe this works for you though it will only return records 5 and 2 because records 5, 2, and 1 have a rolling sum of 1.75 instead of the 1.5 you're filtering for. 

Here's the SQLFiddle with the clause commented out to show this would only return records 5 and 2 in the event this logic is indeed what you're after. 

Since you don't delete rows, no, rebuilding the index will provide no real value outside of updating the statistics for said index, as they are implicitly updated with a rebuild operation. A more efficient approach is to just run an statement instead. Per Benjamin Nevarez: 

A few notes here: Table Partitioning on the New Table This new table is also going to be partitioned based on your requirements. It's a good idea to think about future maintenance needs, so create some new file groups, define a better partition alignment strategy, etc. My example is keeping it simple, so I'm throwing all of my partitions into one Filegroup. Don't do that in production, instead follow Table Partitioning Best Practices, courtesy of Brent Ozar et. al. Check Constraint Because I want to take advantage of Partitioned Views, I need to add a to this new table. I know that my insert statement generated ~440k records, so to be safe, I'm going to start my seed at 500k and create a defining this as well. The constraint will be used by the optimizer when evaluating which tables can be eliminated when the eventual Partitioned View is called. Now to Mash the Two Tables Together Partitioned Views don't particularly do well when you throw mixed datatypes at them when it comes to the partition column in the underlying tables. To get around this, we have to persist the current column in your current table as a value. We're going to do that by adding a Persisted Computed Column, as follows: 

I suspect it is related to the fact that this DMV is updated asynchronously behind the scenes. Per the sys.allocation_units BOL page: 

I think this will do it, but confirmation from others would be nice. This code basically does the following: 

I've also added another on the old table (to aid with Partition Elimination from our eventual Partitioned View) and a new Non-Clustered Index that is going to act like a primary key index (because look ups originating from the Partitioned View are going to be occurring against instead of ). With the new Computed Column and Check Constraint in place, I can finally define the Partitioned View, as follows: 

The database engine thinks you're trying to perform a piecemeal restore using the UI. This is the formal name of restoring individual filegroups. Sadly, it requires you restore the entire database to maintain transactional consistency (which I don't think you want to do in your scenario) and since you're not overwriting the database (as identified by Pinal Dave's answer), you're getting this error. If you're only trying to restore that one table, you've got a couple of options available, as follows: 

I fully expect down-votes for this because my opinion doesn't fit convention in the SQL community, but I don't think you should be focusing on fragmentation levels nor should you be adjusting the fillfactor unless absolutely necessary. The definition of Index Fragmentation is as follows: 

The Import/Export wizard generally makes pretty basic 2-step packages. For instance, I quickly made one that migrates two tables from databaseA over to databaseB, and the Control Flow of the generated package looks as follows: 

This has a scent of a parameter sniffing problem. See what I did there? Punny, right? Anyway, Paul White, wrote up a great article on the various approaches available to resolve this issue, so I'm not going to go into detail when you should just read his article instead. What you can try quickly though, because this is an ad-hoc statement, is to force a recompile using the hint. If this fixes your issue, you definitely are running into what I suspect. However the best way to properly resolve this is to convert your statement into a Stored Procedure and then execute the Stored Procedure passing in the date parameters. A parameterized Stored Procedure won't fall prey to the current issue and should be reusable as well. 

I find it easier to transfer logins via dbatools.io Copy-SQLLogin cmdlet if PowerShell is an option or sp_help_revlogin if not. The Copy-SQLLogin cmdlet is a much easier approach and that link even has a video to help you step through the process, but again, if PowerShell isn't an option and you need to go the sp_help_revlogin route, make sure you execute that stored procedure (after you create via the linked article) on ServerA, copy the login statements over to ServerB and run the statement(s) there. If you get an error, you may need to drop the existing user from ServerB first so that you can sync up the SIDs. Finally, if you're running SQL 2012 or later, you may be able to configure your database to be a partially contained database instead. This will short-circuit things so the security is transferred with the backup as security is handled at the database level and not at the instance level. There are limitations to using partially contained databases, so review those first if you feel like this may be the approach you wish to take instead. 

This article can run you through the basics of getting this configured to your liking. Be warned, you may need to enlist the help of resources within your organization with elevated rights on the domain to implement it: $URL$ 

As for a SSD vendor's perspective, I point you to Argenis Fernandez's blog post about this very topic. Pure even recommends 64KB for NTFS block size, so I think it's safe to assume this is a pretty universal truth. 

If you prefer TSQL instead, you can perform this same functionality via the sp_addlinkedsrvlogin stored procedure. Final Note Here: Because you don't have delegation setup, you'll want to leave the Impersonate checkbox unchecked or if you're using TSQL, the parameter should be passed in as otherwise it will try to pass through authentication which won't work in your scenario. 

This is nested in a cursor that iterates through Page IDs of the data files within the database and executes pretty quickly based on my localized testing. 

For a given table where you're considering a NCCI, said table does not have more than 1 million records inserted per tenant, per week. Per the answer to my initial comment on your question, this looks to be true for your scenario. For the same table, there is also a column or columns included within its definition that implies new/volatile data or older/stable data. This could be a date field, or a combination of columns, but basically this column or columns will be used as the filter condition, and if no combination of columns exist that represents data volatility, a filtered NCCI becomes more difficult to properly define, and therefore, less useful.