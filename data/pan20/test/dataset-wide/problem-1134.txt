This is a typical 1 to many relationship (or parent child relationship). The parent would have the fixed infromation and the child would have the many varying information. The key of the parent must appear in the child. Example: Receipt : {ReciptID, Date} RecieptItem : {ItemID, CustomerPrice, Quantity,TotalPrice,ReceiptID, Status, Date} Item:{ItemID, ItemName, Price} Now each Recipt can have 0,1 or more Recipt Items. Each Item is references on 0,1, or more ReciptItems. The column ReceiptID in ReceiptItem is referred to as a Foreign Key. A constraint could be built to make sure that every ReciptID in ReciptItem corresponds to a ReceiptID in parent Receipt table. Example ERD: 

A user takes zero, one or more quizes A quiz contains 1 or more questions A user provides an answer for zero,1 or more specific question on a specific quiz 

Few suggestions: 1 - Don't use dynamic fields unless you really have to because they add complexity to your system. In your case, you don't have to. 2 - Don't add redundant static fields like this: Contact: ContactID,...,Phone-1, Phone-2,...,Phone-20, ... This is against normalization and is justified only in few cases (e.g. where the columns are limited in number and rarely searched directly). If you do this you can never find a Contact given a phone number in a decent way and you will have lots of Nulls to deal with also, you could have duplicate phone numbers (if you are not careful in your coding). 3 - If you have a structure like: Contact: ContactID, ... ContactMethod: ContactMethodID, Value, ContactID_FK where a Contact could have 0,1, more ContactMethods, that is a perfectly normal 1-M relationship that will perform very very well if you have millions of rows provided that you have an index properly defined on ContactID_FK. On your client side, you have to be careful what you want to show and how large is the data your are communicating with the server. This is a different issue and I don't think it would affect your case. The option in (3) is probably the best, it has been working since the 80s very well! 

WITH (NOLOCK) is the equivalent of using READ UNCOMMITTED as a transaction isolation level. So, you stand the risk of reading an uncommitted row that is subsequently rolled back, i.e. data that never made it into the database. So, while it can prevent reads being deadlocked by other operations, it comes with a risk. In any application with high transaction rates, it's probably not going to be the right solution to whatever problem you're trying to solve with it. SQL Server 2005,2008 and 2008 R2 will support Nolock. Pl look on the below link $URL$ 

The error (2003) Can't connect to MySQL server on 'server' (10061) indicates that the network connection has been refused. You should check that there is a MySQL server running, that it has network connections enabled, and that the network port you specified is the one configured on the server. Start by checking whether there is a process named mysqld running on your server host. (Use ps xa | grep mysqld on Unix or the Task Manager on Windows.) If there is no such process, you should start the server. If a mysqld process is running, you can check it by trying the following commands. The port number or Unix socket file name might be different in your setup. host_ip represents the IP address of the machine where the server is running. 

What you are describing is more like a print log where you insert data about when and whom has requested the report. If this, is so, it is valid. But the part about hiding the data is not clear in your description. The data you are reporting should, generally,not be changed in the database as part of the reporting process. There are situations where you have to mark each printed report as 'printed' and hence update its status but from your description, I can't see that this is the case. 

If you want to do this in an on-line transaction, then you probably have the wrong database design. If you are trying to do this in batch, then select only the fields of interest to a flat file, or export to a flat file then process your data from the flat file. You could even split the output file to several files and run parallel tasks to process the data. This will at least free your database server resources during your processing. This practice is not uncommon in data warehousing applications. 

In create table you have set teams as primary key, and also you are aware that primary key does not allow duplicate values. 

Check the insert query for teams field you have mentioned values as 1 and if you insert the same value for second record it will behave as below. 

Hi I need to set auto increment for a table as S1. I tried by altering the table and also by creating by new table I am aware how to set auto_increment but when i try to set auto increment as S1 i am unable to do. since i need the primary key values should be S1,S2,S3 as following records. can any one please suggest the data type for that, 

Try the above query in terminal or check the below link to repair table or databases via phpmyadmin $URL$ 

It will ask for password don't enter anything first time because it will use blank, n just press enter you are done. N later you can set password too...:) 

One good case for using MVs is that some times you want to aggregate data and get this summary information from large tables frequently and quickly. Without materialized views, you have to either deonormalize some of your tables and maintain the aggregates via code or repeatedly scan large sets of rows. Either way is not always acceptable specially with dashboard and similar online applications. If you keep the results in a separate tables, you complicate your application code and as @Justin Cave says, you will be in charge of making sure that the manually aggregated data is in synch. with the original table's data. 

"integers are cheaper to compare than characters, because character sets and collations (sorting rules) make character comparisons complicated. Integers are very good when using as an index column." - HottestGuide 

I guess that the first option is fastest (although it does not look very slick from programming perspective!). This is because It deals with exactly N rows (where N is the table size) and has to do no search or sort like method 2 or 3. A test with large sample should prove the point. As yet another option to consider (as if you need more!), is to create a materialized view over your table. if your table size is in 100s of thousands or more. This way, the min value is calculated while the row is changed and the entire table would not have to be processed with every query. In SQL Server, materialized views are called Indexed Views 

This error happens when Full back up is not restored before attempting to restore differential backup or full backup is restored with WITH RECOVERY option. Make sure database is not in operational conditional when differential backup is attempted to be restored. Please have a look on the below blog. $URL$ 

Your BHCR is 98.92% its perfectly Good and Great. If the value of the Buffer Cache Hit Ratio (BCHR) counter is "high", then SQL Server is efficiently caching the data pages in memory, reads from disk are relatively low, and so there is no memory bottleneck. Conversely, if the BCHR value is "low", then this is a sure sign sign that SQL Server is under memory pressure and doing lots of physical disk reads to retrieve the required data. Prevailing wisdom suggests that "low" is less than 95% for OLTP systems, or less than 90% for OLAP or data warehouse systems. You can also test the true nature of the BCHR 

First, I suggest you read about star schema first. Not understanding the concepts could lead to wrong results. You may use the transaction as your fact table as in the diagram below. Don't use DISTINCT, instead you want to use: SELECT ... FROM fact GROUP BY dimension data (time, transaction and bank) WHERE condition to restrict dimension data and join with fact. 

I have the following remarks: 1-According to your question's text, the fk for city and country are repeated unnecessarily in the Address table. given the city id alone, you can find the country and state by a join. Repeating them represents redundant relationships. This desing may speed search by country or by state slightly though. You'd have to include 3 FKs in the Address table if the PK of State is composed of StateID+CountryID and the PK for city is CityID+StateID+CountryID. If you do this, the the FK in the address table becomes the compound key of CityID+StateID+CountryID. 2-Table names should be singular, e.g. City not Cities. 3-The Code column is not clear to me, is it unique? If it is, why use another ID? 4-The 'Address' table is probably meant to represent the address of the resort. However, this may not be quite practical. Some resorts have more than 1 distinct (sub-resorts) that may require different (or maybe the same) address. Also, there could be a management or contact address that is different from the resort address. You need to take care of this if your business rules demands it. Edit - Attempting to show that only 1 FK is a must in the Address table and the rest are redundant from modeling perspective. FK2 and FK3 could be obtained by joinng the Address table with Country, State and City tables when only FK1 is present in the Address table. 

I had created table with engine BLACKHOLE basically the BLACKHOLE storage engine acts as a “black hole” that accepts data but throws it away and does not store it. Retrievals always return an empty result. I heard that we can retrieve the data by creating a new table same as the old table with storage engine as innodb or myisam. but i had tried that also but unable to get the result. Can any one pl help me on this issue to fix it. 

If you anticipate that your database file(s) will exceed more than 1TB in size you should configure the server to put the data files across multiple VHDs. How can spread my databases across multiple VHDs? Create one or more dynamic volumes consisting of multiple VHDs at the operating system level and put your database files on these volumes. Split your tables and indexes into separate database filegroups and then place those filegroups in different files (MDF + NDF) which are spread across separately attached VHDs. Note that this will not offer any benefit for transaction logs because SQL Server does not stripe across transaction log files but uses them sequentially. If you are migrated to Azure and want to compare the performance of your databases before and after the migration then take a SQL Server baseline(enter link description here) 

If you have a performance problem, it may be worthwhile to identify the reason for it first before you move to persisting the data. One reason for this is that the base data may change and your persisted acculturation may not reflect valid data over time. If you decide that data base data change will not change the accumulated results, then maybe this data can be generated using a batch job instead of an on-line query. This way, you will have ample window to run your query without resorting to data persistence. Now we look at which row structure to use. I must agree with Mr. Kimball's point. However, it makes sense only if you have all the data values ready once you write the row. You don't want to visit aggregated rows to update their content as a general practice. 

To make an efficient compare of all columns between 2 tables, you could use a UNION method with GROUP BY as described in Diff2Tables. Alternatively, if your profile table can have rows deleted from it without cascade delete, you could, drop indexes that are not for PK of the profile, Delete rows having the same key in both tables, and insert the rows from the other table into the profile then re-build the indexes on the profile table again. This may sound bad but in fact it is not that bad. This is a typical scenario in Data Warehousing. 

Error Cause: The control file change sequence number in the log file is greater than the number in the control file. This implies that the wrong control file is being used. Note that repeatedly causing this error can make it stop happening without correcting the real problem. Every attempt to open the database will advance the control file change sequence number until it is great enough. 

I had some changes in that table and it got executed. mainly the primary key. pl go through the same, 

I am getting the below error in mysql while trying to ping. i am using mysql version 5.5.37-0ubuntu0.12.04.1. when ever i use mysqladmin i am getting the below error. Please suggest how to clear this issue 

If the tables were dropped, ApexSQL Recover can recover them even from databases in the simple recovery model. ApexSQL Recover can recover both table structure and table records On the other hand, ApexSQL Log cannot recover records lost when a table was dropped, it can only recover the table structure In case the records that were lost using DELETE (not DROP TABLE), both ApexSQL Log and ApexSQL Recover can help The advantages of ApexSQL tools over recovery to a point in time is that ApexSQL will recover just the tables you specify (creates CREATE TABLE and INSERT INTO scripts) , while a point-in-time recovery will roll back all the transactions that happened in the meantime A DROP TABLE statement marks the MDF file pages used by the dropped table for deallocation. These pages are actually still in the MDF file until overwritten by new operations. To prevent new operations overwrite the data necessary for successful recovery, we recommend creating a copy of the original MDF and LDF files immediately 

Note: Table names may be different in the solution than from this diagram A1 = Standard answer for question 1 UA1 = User provided answer for question 1 

Having a table for each data type has nothing to do with normalization. Nomralization is about not repeating base information in tables. It is valid to have a key for a row and different columns each having its own data type. In fact this is the most common way. 

Address modeling is not universal. A universal implementation would be too complex for most applications. The different models vary according to model type (OLTP vs OLAP), country rules, customer type (organization vs. individual), how critical the address data is, etc. As said, you should separate city. Separating City will make the problem of having different city names in the customer table go away. Reasons are: 1 - Separating City names in a separate table allows you to run queries like: give me customer distribution by city and show cities where no customers are there. 2 - Allows you GUI to always refer to the correct list of cities. 3 - Allows you to maintain city information without touching your program code. 4 - Allows duplicate city names (in different states) if you use the diagram below. If you are sure that your application is for 1 country, then don't add the country in. Also, I noticed you don't separate the street address information into 2 columns which is common in North America and that you have no Zip Code. Review the country's postal address requirements from postal authorities to make sure your design conforms to them. Here is a common representation of address in an OLTP application. Here the PK of City is 2 columns, namely, CityID and StateID. A variation on this version would be to use single ID for each table (as a sequence number) and end up with a FK composed of 1 column only at the customer table. It all boils down to business rules and requirements.