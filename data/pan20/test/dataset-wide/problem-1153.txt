Store the address inline with the source record Have an address table and add a foreign key to the entity Have an address table with a foreign key that links to the entity 

I'm looking at adding addresses to an existing database, and am trying to work out the advantages and disadvantages of the following methods (and if there are any I've overlooked). 

Add a column in the table for each extra value Add a linked table which references the original table and has records only where we need to store a value Use the XML data type in the original table and store all of the values in this. 

This will return a 1 if the ticket expiry date is in the future, and a 0 if it is exactly now or in the past. 

Is the information on this page any use? It looks like you'll have to write something like the following: 

I'm responsible for creating a database on a project. We have fields that are rarely going to have a value (1 in every 10,000 records) and I'm trying to work out the best way to store this in the database. As far as I can see I have 3 options: 

Are there any other options that I've not considered? I'm trying work out the pros and cons of each method. As far as I can tell 1 would be the easiest and 2 would take the least amount of space but I'm struggling to find many resources for 3. 

I'm running an alter table, alter column on a table with nearly 30 million rows and SQL Azure fails after approximately 18 minutes saying that I'm guessing that it's not possible to break this down into modifying fewer rows at a time, so I'm wondering what my options are for making this change to the database. SQL Azure will not let me change the size of the transaction log (limited to 1gb). I'm guessing that my best bet is going to be creating a new table with the new layout, migrating the data across into that one, deleting the original table and then renaming the new table to match the name of the old table. If that's the case, how is it best to structure those commands? Scheduled downtime for our system is not an issue currently so this operation can take as long as it needs to. 

I'd like to steer clear from storing it inline for normalisation reasons, but I'm unsure as to which of the other options to go for. It's possible at some point down the line that we want additional entity types to have addresses on them, which makes option 2 look like the best one. This option (as far as I can see) has the potential to leave unlinked data around though. 

824392 isn't that many rows, but it may make for a big table - it depends on how wide those rows are, really. 

Even the most uber-admin type of connection option Dedicated Administrator Connection (DAC), which can only be used as a local connection, and lets you undo all kinds of evil, still requires login credentials. So I don't think there's an official way to do this. The quickest way to resurrect this system may be to shut down SQL Server, copy the user database files somewhere safe, uninstall, reinstall (making sure to service pack up to at least the level you were previously at), copy the files back and attach the databases. (Not sure the copying out/back is required, but just to be safe...). You'll still need to manually recover server level objects (e.g. logins) 

Imagine if each non-clustered index contained the primary key, as you suppose: This would, based on your table, presumably include the non-clustered index that's implementing your primary key. Imagine that you're performing a query against the table, and the primary key index is used. There's no way, having used this index, to find the remaining data for the table. So that couldn't possibly be the way things work. Or another hypothesis: Every non-clustered index that isn't the primary key index stores the primary key. If the primary key index is non-clustered, then it contains some magical other value (e.g. the clustered key). But that then means, than any query that uses a non-clustered, non-primary key index must now perform twice as many index operations - once against it's own index, and a second one against the primary key index. And we'd also need two separate implementations of non-clustered indexes. Since neither of these are a good fit, it's hopefully obvious why non-clustered indexes store the clustered index key, rather than anything else. 

1&2 - Every update query must update the data for the table, wherever it may appear - in the clustered index (always), in any index that has the column in its definition, and in any index which just includes this column. 3 - Removing records may or may not help, it depends on 4: 4 - Probably, because sometimes it will not use the index, but will instead choose to scan the entire table. If it's scanning the entire table, then reducing the size of the table (via (3) above) will help. It will scan the entire table if, so far as the optimizer is concerned, that will be more efficient than using the index, then accessing the clustered index to complete the operation. It may choose to do this because it's actually true that the index will not help, or if statistics are out of date, or just sometimes it gets this wrong. Examine execution plans to determine if this has happened. 4a - If there's a lot of activity on the table, then other operation may have locks that prevent your query from processing as fast as it could if it was the only activity occurring in the database. Waiting for locks can take seconds, minutes, or hours - it depends on how timely the other operations are 5 - Solution to what?