A polynomial-time algorithm for SAT (satisfiability), the problem of whether a boolean logical formula has a setting of its variables that makes it true. (It's not quite in the letter of the question because we do not know that it does not exist.) Primarily, we show that problems are NP-hard by reducing SAT (or another NP-hard problem) to those problems in polynomial time. The argument is thus that, if we have a polytime algorithm for those problems, then this constructs a polytime algorithm for SAT. Since we do not believe this mythical creature exists, we do not think those problems can be solved efficiently. (not sure if all mathematicians are already aware of this or whether the summary is useful.) If we had this polynomial-time algorithm for SAT, then we could prove theorems quickly and automatically, we could break cryptosystems, we could improve massively in all sorts of scheduling, routing, resource allocation, and other optimization problems -- in short, "useful" would be an understatement. (Let me add -- what's really "useful" is the converse: if this object does not exist, then we know that these sorts of tasks cannot be accomplished.) 

Edit: The below does rely on the assumption that knives move continuously, see the comments. I think the procedure is "safe": Each player can guarantee not to envy either of the others by following the suggested protocol. I am assuming that the rules are that players must keep their knives to the right of the sword. Intuitively, the straightforward proof of envy-freeness, following, does not use that the other players are playing truthfully (they can be playing arbitrarily). Suppose a player "Marge" wants to guarantee herself an envy-free piece. Marge follows the suggested strategy: She keeps her knife at her perceived midpoint of the section to the right of the sword, and yells "cut" if at any point she prefers the piece left of the sword to both other pieces that will result. If neither of the other players ever yell cut, then eventually Marge will yell cut and get an envy-free allocation. So the players can only disrupt her by yelling cut before she does. However, since she has not yet yelled cut, Marge must prefer one of the other two resulting pieces to the left piece. So we just need to check that, when one of the other players yells cut, Marge will get her more-preferred piece between the center and the right. If her knife was the one used to cut, then since she was following protocol, she is indifferent between the center and right pieces (preferring both to the left piece), and she gets one of them, so she is not envious. If her knife was not used to cut, then either it is nearest to the sword or it is farthest from the sword. Suppose her knife is nearest to the sword. Then she gets the center piece. But because her knife is located to make her indifferent between (a) the section between the sword and her knife and (b) the section between her knife and the rightmost edge, and because she gets section (a) and more besides, she prefers the center piece that she gets to the right piece. The analogous argument works if her knife is farthest from the sword. ... I would guess that there is another argument which says that any strategic disruptive player could be simulated by an honest player with a certain preference, implying that it would be sufficient to show the protocol is envy-free, when players are truthful, for all possible preferences. 

Now consider a similar tree where only one node at each level has an extra child; the number of nodes at each level $n$ is $f(n) = n+1$. In this case, the number of paths is $\omega$; the paths here are countable. 

(Preface: This may be a naive or easy question for experts....) Consider an infinite tree, rooted on the left, where each node has two children; the number of nodes at each level (distance from the root) $n$ is $f(n) = 2^n$. The number of paths is $2^{\omega}$, the cardinality of the continuum (uncountable). 

Edit: I make the mistake below of proving a lower bound on the maximum number of boxes Alice must open, not the expected number. So this does not answer the question. . I think this sort of thing is often argued with Yao's principle (which is really just von Neumann's minimax): $$ \max_{\text{randomized algorithm}} \min_{\text{sequence}} \mathbf{E}[\text{performance}] \leq \min_{\text{distribution on sequences}} \max_{\text{deterministic algorithm}} \mathbf{E}[\text{performance}] .$$ It is a two-player game between Alice, who chooses the (randomized) algorithm, and Lucy, who chooses the input sequence. In this case $\mathbf{E}[$performance$]$ is the probability that Alice finds a red ball. To apply it, we just need to upper-bound the right-hand side. We do that by, not actually taking the minimum over all possible distributions on sequences, but just finding one distribution on sequences that is bad for all deterministic algorithms that Alice could employ. So here's the idea. First, suppose that an algorithm can only open $o(\log n)$ boxes. Now, if we just find a single distribution on input sequences such that every deterministic algorithm has $\Pr[$find red$] < 1-1/n$, then we are done: The minimum on the right-hand-side is certainly less than $1-1/n$, since we have found an example where it is less than $1-1/n$. Then Yao's principle says that any randomized strategy of Alice performs worse than $1-1/n$ on its worst-case input distribution. (That is, on the worst-case strategy of Lucy.) So concretely, I think we can apply it here by letting Lucy choose the distribution of sending the boxes in uniformly random order. Then for any deterministic algorithm of Alice's that opens $k$ boxes, the probability that none are red is $$ \left(1-\frac{k}{2n}\right) \left(1-\frac{k}{2n-1}\right) \cdots \left(1-\frac{k}{n+1}\right) . ~~~~~~~~~~~~~ (1) $$ Why is this? Fix the $k$ boxes Alice will open (remember we are only worried about deterministic Alices). Now imagine Lucy randomly choosing the locations of the red balls one by one. The first has $2n$ choices, so a probability $1-k/2n$ that is does not choose one of our $k$ boxes. The second has $2n-1$ choices, conditioned on the choice of the first, so a probability $1-k/(2n-1)$ that it does not choose one of our $k$ boxes. And so on for the $n$ red balls. The important thing is that we can't assume that each of the $k$ boxes independently has a red ball with probability $1/2$, since they aren't independent (if one box doesn't, the others are more likely to). I think you might make this mistake in your statement of the upper bound, but if so I'm sure it's easily fixed. Anyway, I don't know immediately how to argue that (1) is at least $1/n$ when $k < o(\log n)$, but it should be true, since (1) is at least $$ \left(1-\frac{k}{n}\right)^n \approx e^{-k}.$$ (The approximation has the inequality going the wrong way, so we don't immediately get the proof.) Edit. If there is anything unclear, please let me know. I probably did a poor job explaining, but this is a primary technique for lower bounds for online/randomized algorithms and I think it gives what you want pretty simply. 

Warning: My solution may have an off-by-one error depending how you define "stays in $\{0,\dots,n\}$ for exactly $m$ steps." 

(Of course we did not really need to restrict the domain to the naturals.) In the case of Cantor we take an enumeration of reals and produce a real number not in its range. In the case of Godel's first incompleteness, I think we could for instance take an enumeration of all provable or disprovable statements and produce a statement that is not in the enumeration, i.e. neither provable nor disprovable. Now here is an algorithmic sort of philosophy towards refining question 3: A non-diagonal proof should be completely non-constructive. If it constructs an explicit counterexample, then it essentially fits our definition above. Formalizing this could be quite tricky. Here's one avenue. Suppose we can determine the computational complexity of this particular problem, $T$ (i.e. $T$ is the minimum running time of an algorithm, formalized as with e.g. Turing Machines, that given $f$ produces $y^*$). Then suppose we have a proof of a theorem in some theorem-proving language; this is equivalent to a program that produces an instance of a particular type. Then let's consider the proof "nonconstructive" if, for any algorithm "$A$" running in time $o(T)$, invoking that algorithm $A$ on the output of the proof program does not guarantee to produce a counterexample $y^*$. If you aren't familiar with complexity, the above seems to guarantee that the proof is essentially nonconstructive: $A$ cannot compute a counterexample on its own because $A$ runs in time $o(T)$ and time $T$ is required to compute a counterexample. So all that $A$ has "to work with" is the output of the previous proof. If this output somehow embeds a counterexample, then (presumably) (one might hope) $A$ can uncover and output this counterexample; but if the previous proof is entirely nonconstructive, then there is no hope for $A$ to find a counterexample. The problem with this complexity approach is that we might be interested in theorems that require Turing Degree higher than zero, or require a type II sort of TM that deals with real numbers, or so on, so complexity isn't necessarily well-defined or well-studied. But hey, it's a really hard problem to say what sort of proof argument is "necessary" for a theorem. It seems clear to me that this is the sort of approach we'll need to formalize it, but I don't know if this has been studied much.... P.S. You can probably also come at this from the direction of Lawvere, something like: Let a fixed-point proof mean an algorithm taking in some $t: Y \to Y$ and a surjective $f: A \to (A \to Y)$ and producing a $y^*$ such that $t(y^*) = y^*$. Then a non-fixed-point proof should be totally nonconstructive, i.e. should not provide any means of actually finding a fixed point. 

Some rough thoughts (not a complete or formal answer!). I don't think I've seen this written down anywhere in as many words, but as someone who studies algorithms, let me propose an answer to question 1: 

This might be more comment than answer, but -- This a standard trick -- running all TMs in parallel -- but I don't know if it has a name. You can probably find it used in introductory complexity textbooks like Sipser, somewhere. One keyword is "Godel numbering", which refers to the ordering of programs that you mention (we assign each program to a natural number). I guess UTP is a fine name, you just want to distinguish it from Universal Turing Machine (UTM), which is a TM that, given the description of a program and an input, simulates that program on that input. Your proof of undecidability is correct. Sidenote, questions like this might fit better at cs.stackexchange.com. 

Concentration of measure results might often count. I am thinking of Talagrand's Inequality, the Lovasz Local Lemma, .... I currently think of, say, Chernoff bounds as interesting in their own right, intuitive, and relatively straightforward to prove, but as a beginning student Chernoff bounds seemed to me just as described in the post (extremely useful workhorses without much interest or clarity of their own). Now more "advanced" measure concentration results appear as workhorses to me.... 

In general, you are just asking about a weighted sum of i.i.d. variables from distribution $D$, with weights $\alpha_1,\dots,\alpha_n$. The Gaussian distribution is the only one that is rotationally invariant when coordinates are sampled i.i.d., so I'd expect it to be the only one that depends only on $\|\alpha\|$ rather than on all of the $\{\alpha_i\}$. So one direction to look is for distributions where weighted sums belong to some other known distribution, stable distributions being a special case. (I guess the Levy is supported only on half the line, so it should fit what you're looking for...but it may not be a very "nice" distribution.) Another approach is via the moment generating function or characteristic function. Suppose $S = \sum_{i=1}^n X_i$, where $X_i = \alpha_iY_i$ and all $Y_i$ are i.i.d. Then \begin{align*} \mathbb{E} e^{tS} &= \prod_{i=1}^n \mathbb{E} e^{tX_i} \\ &= \prod_{i=1}^n \mathbb{E} e^{t \alpha_i Y_i} \\ &= \prod_{i=1}^n f(t \alpha_i) \end{align*} where $f$ is the MGF of the distribution. For example, with exponential$(\lambda)$ variables, the MGF is $\frac{\lambda}{\lambda-t}$, and if $Y_i$ is exponential(1) then $\alpha_i Y_i$ is exponential$(1/\alpha_i)$, so its MGF is $\frac{1}{1-\alpha_i t}$. So for exponentials, the MGF of $\langle \vec{\alpha}, \vec{X} \rangle$ is $$ \prod_{i=1}^n \frac{1}{1-\alpha_i t} . $$ Now whether you can recover a distribution from the MGF is another question, I don't have expertise in this but can point to $URL$ and $URL$ 

This isn't a full answer but some setup and initial attempt. Maybe we can formalize a class of "board games" that we're happy with first. I'd propose that: 

If you have an idea that you think or hope might be true, then don't just look for reasons to believe it. (It is amazing how strong and how wrong is this instinct.) Look for reasons that your idea might be wrong. If it is wrong, you will save yourself a lot of time and possible embarrassment. If it is right, you will learn a lot about why and have probably found some new reasons to believe it anyway. 

I think the following geometric argument is interesting and maybe sufficient to answer "why" at an intuitive level (?). When we take the powers of $x$ in the complex plane, the absolute value scales geometrically ($|x^n| =|x|^n$) and the argument (angle with the x-axis) scales linearly ($\arg x^n = n \arg x$). So the powers of $x$ look like this: 

Here's a step that seems nice enough to point out. It still leaves a parameter to pick, and I'm not sure it's ever better than applying Bernstein, but it does something different. We can get a probability bound in terms of how much $S_n$ exceeds the Renyi entropy $H_{\alpha}$ of $\mu$ (equivalently, worded in terms of the $\ell_{\alpha}$ norm of $\mu$), for any $0 < \alpha < 1$. The unresolved question is if we can to pick $\alpha$ to get a nice closed form of some kind. Maybe someone more clever than I can speak to that. Claim. Let $X_1,\dots,X_n$ be i.i.d. according to $\mu$ and $Y_i = \log(1/\mu(X_i))$; let $S_n = \frac{1}{n} \sum_{i=1}^n Y_i$. Then for any $0 < \alpha < 1$, \begin{align} \Pr[ S_n \geq t ] &\leq 2^{-n (1-\alpha) \left( t - H_{\alpha}(\mu) \right) } \\ &= 2^{-n \left( (1-\alpha)t - \alpha \log \| \mu \|_{\alpha} \right) } . \end{align} Here I'm writing $\mu = (\mu_1,\dots,\mu_m)$ as a vector of probabilities. Note that $H_{\alpha}$ is decreasing in $\alpha$ and $H_1 = H$, Shannon entropy. So as $n \to \infty$, we can pick $\alpha \to 1$ and get tail bounds for $t \to H(\mu)$. Proof. Using the general Chernoff method, \begin{align} \Pr[S_n \geq t] &= \Pr\left[ 2^{\lambda S_n} \geq 2^{\lambda t}\right] & (\forall \lambda \geq 0) \\ &\leq \frac{\mathbb{E} 2^{\lambda S_n} }{2^{\lambda t}} & (\text{Markov's}). \end{align} We have \begin{align} \mathbb{E} 2^{\lambda S_n} &= \left( \mathbb{E} 2^{\frac{\lambda}{n} Y_1} \right)^n \\ &= \left( \mathbb{E} \mu(X_1)^{-\lambda/n} \right)^n \\ &= \left( \sum_{j=1}^m \mu_j^{1-\lambda/n} \right)^n . \end{align} Hence \begin{align} \Pr[S_n \geq t] \leq 2^{-n \left(\frac{\lambda}{n} t - \log \sum_j \mu_j^{1-\lambda/n} \right)} . \end{align} Pick $\lambda$ such that $1-\lambda/n = \alpha$, for a chosen $\alpha \in [0,1]$. In other words, $\frac{\lambda}{n} = 1-\alpha$, and factoring this out and substituting, \begin{align} \Pr[S_n \geq t] \leq 2^{-n (1-\alpha) \left(t - \frac{1}{1-\alpha} \log \sum_j \mu_j^{\alpha} \right)} . \end{align} 

My question is, what happens in between? I would formalize the setting as follows (but maybe I have some details wrong): Suppose I have an infinite rooted tree, where each node has at least one child, and the number of nodes at distance $n$ from the root is $f(n)$ where $n \leq f(n) \leq 2^n$. Then what can we say about the number of paths in terms of the "growth rate" $f$? 

You can use a "dynamic programming" solution. For anyone unfamiliar with this terms, the basic idea is that there are an exponential in $n,m$ number of possible paths, so it takes too long to enumerate them all. But we can express everything we need to know about the problem using a small "state" consisting of where the walk currently is located and how many steps are left. Specifically, we compute the number of paths that reach the end goal starting from each state. To do so, we use induction starting with the base cases of having $0$ steps left, then solve all the cases with $1$ step left, and so on. Because there are only $nm$ cases, this is computationally feasible relative to enumerating paths. I'll focus on paths that leave from, say the left side. The right-side case is exactly analogous, and the case where they can leave from both sides should be easy by solving each case separately and "adding" them together. What we want is to be able to compute $$ K(a,b) $$ which is defined as "the number of distinct paths starting from location $a$ that stay in $\{0,\dots,n\}$ for precisely $b$ steps before exiting through the left side." In particular, this will give us $K(0,m)$ which is the total number of paths you care about. This allows us to sample a walk iteratively: if we are at location $a$ after taking $m-b$ steps (having $b$ remaining), then move right with probability $\frac{K(a+1,b-1)}{K(a+1,b-1) + K(a-1,b-1)}$. (This relies on all paths being equally likely, thanks to symmetry of the random walk.) Now we can compute these via dynamic programming. Say we care about paths leaving from the left side. First the "base case" where $b=0$, where we should be at location $-1$ (having just exited). There is one valid path from that location and zero valid paths from all others. \begin{align} K(-1,0) &= 1 \\ K(a,0) &= 0 & (a \geq 0) \end{align} Then in the general case, we get this: \begin{align} K(-1,b) &= K(n+1,b) = 0 & (b > 0) \\ K(a,b) &= K(a-1,b-1) + K(a+1,b-1) & (0 \leq a \leq n, b > 0) \end{align} It says there are zero paths going through $-1$ or $n+1$ at any point other than $b=0$. For $0 \leq a \leq n$, the number of paths going through location $a$ with $b$ steps remaining is the sum of the consequences of the two choices, i.e. the paths through $a-1$ or through $a+1$ with $b-1$ steps remaining. In psuedocode: