I've got a standalone Windows Server 2003 running SQL Server 2005 and a Windows Server 2003 Active Directory domain controller. Using maintenance plans/SQL Server Agent, I'm trying to write the database dumps from the standalone box to a share on the DC. I know the usual rules about accessing remote shares (e.g. must use a logon account which has proper rights, etc.). In fact, writing the dumps to another non-DC server in the same domain as the DC works fine. I'm trying to set the SQL Server Agent account's logon credentials to "domain\username" (or username@addomainname). If I specify a username in the form of "domain\username, the error I get (regardless of password) is: "The account name is invalid or does not exist, or the password is invalid for the account name specified". If I specify a username in the form of "user@addomainname", the error I get (regardless of password) is: "The specified domain either does not exist or could not be contacted." I've turned on logon failure auditing on the DC and I see no failures in the log, which suggests to me that the machine isn't even trying to authenticate, but rather failing prior to that. I know that users on non-member servers can authenticate to shares on a DC, because doing an interactive logon (e.g. "net use * \dcname\c$ /user:username@addomainname", or using the other form of the username) works fine. The above example is about SQL Server but applies to any Windows service. Why can't the service log on with the domain account, but an interactive logon (drive mapping) using that same account works? 

I would be careful with the kinds of switches you are using. Sounds like you might have a switching loop there. If you use managed switches it shouldn't be a problem as you can turn on STP. Just make sure you have taken loops into account when building redundant connections and interconnecting switches. @Maruti: So switching loops is where you build redundant paths for your switches but leave all connections up. This image sort of shows this. What happens in that picture is that switches will forward broadcast and multicast packets within a broadcast domain to infinity. Switches are kind of dummy devices, they forward broadcasts out all ports without taking loops and the network topology into consideration. This means if you build a network in a lab with redundant connections (a loop) and setup a DHCP server, as soon as any computer attempts to DHCP an address you will probably shut down the lab network. Having redundant paths isn't necessarily the problem though, because redundancy is a good thing. And most manageable switches have a feature called STP (Spanning-Tree Protocol). Just make sure STP is turned on and correctly functioning and you won't have to worry about loops. What STP does is virtually shut a connection (not physically) so that information cannot be sent out that connection. Thus cutting off the loop. And since it is done virtually that connection still exists should a connection somewhere else in the loop go down, say a bad wire or something, the connection STP shut can be turned back on if needed. In the picture it would appear connection 3/2 on switch C is shut due to STP. 

We faced a very similar problem. We eventually concluded that while integrated NTLM logon support in Internet Explorer and Firefox is convenient, there are so many exception cases which result in failure that we changed our approach. The problem with integrated authentication is that it works only when the currently logged on username and password are still correct and properly authorized to access the resource. There are more circumstances where it doesn't work however: 

where computernames.txt is your text file and and "..." are the other command line options you wish to use. There's a great reference at $URL$ 

I have a Cisco RV110W small office router (this configuration process is common to many Linksys/Cisco routers) and I am trying to define QuickVPN clients. When add a client of type "QuickVPN" the router gives me the following warning: (You can find a larger version of the screenshot here 

With Rackspace's (hosting this particular configuration) support, I now understand the situation. The "net use *..." drive mapping example and the service example is an apples to oranges comparison. With the drive mapping case it's just an authentication that's happening. In the service case, I'm actually attempting to run a local process under domain credentials, which by definition isn't possible since the server isn't in the domain. Not in domain = can't execute under domain credentials. The drive mapping works because I'm not attempting to execute a process as the domain account - I'm simply passing the credentials. This restriction applies to any type of process, regardless of it's interactive or service-based. 

Besides making your own CD, you will also need to recompile the kernel. I haven't done this in awhile so I'm not sure how much help that link will be, but most docs I have followed in the past have been fairly straight forward. If you need further help you might want to stick to the Ubuntu forums. Some of the followers on those forums are masters at this stuff. Recompiling the kernel should also allow you to make a truly customized CD that should run pretty speedy, because you can take out modules you know you do not need. 

So the initiating server was able to connect at one point and now all of the sudden it cannot? And the only thing you know that has changed is the initiating server has file services installed? Could perhaps a default setting been changed? I know that on server 2008 systems there is some setting to allow remote desktop connections for older remote desktop connection applications from older systems. Is there a setting to allow you to remote connect to older remote desktop connection systems? Has anything changed on the 2003 system? Can you verify it can accept connections? Try to initiate the connection from another host. For instance do you have an XP client or another server 2003 system you could attempt to initiate the connection from? If that works then the issue would likely exists on the server 2008 system. If not, then it is either an issue with the server 2003 system on the other side of the tunnel or a network issue. 

According to Cisco support, QuickVPN connections to this device require that the inside interface of the router be set to a 10.x.x.1 address. Assuming that your LAN isn't 10.x.x.1 this would mean a readdressing of all devices, DNS changes, etc. Ridiculous. They said this was because most of the networks where the QuickVPN clients come from will be 192.168.x.x, so this would conflict. I said that 10.x.x.1 would conflict as well in the case that the remote user was on a 10.x.x.1 and was told that the 10.x.x.1 range was picked because it's used less frequently than 192.168.x.x. So to prevent an conflict between the remote and central networks, the device was hard coded with this requirement. I'm used to Cisco VPNs where the VPN gets one or more unique subnets and NAT does all the magic. I don't understand QuickVPN connections well enough to know how the implementation is different. These restrictions apply to PPTP VPNs as well. 

My internal subnet is 192.168.1.0/255.255.255.0. I don't understand the warning message since a 10.x.x.1 network can just as easily conflict with a remote network as a 192.168.1.x network can. How shoud I proceed? 

You can create a text file containing the computer names you want to scan (1 name per line) and then run the MBSA command line program as follows: 

If anyone is interested in experimenting, just use Wireshark. If someone really gets on our case about slow connectivity or dropped packets we just mirror a port on a switch, connect up a laptop with Wireshark and take a look. 

This statement is confusing to me. We don't know anything about the address the OP gave us except the range it exists in, because the OP never said on what device it was found. We do not know if it is the address of a switch, a server, an AP, a computer, a printer, etc. So how you would know that from the small post from the OP wrote is beyond me. I agree it doesn't have to be the gateway and I have already mentioned this. As I already explained, when you look at most large companies (but this is Cisco's best practice and is usually applied to most businesses) you actually find that gateway addresses will be the last or first assignable address in a range. 4.100 would be in the middle and would make no sense to be a gateway address. While some network admins might assign it that way, keeping track of this would be cumbersome, especially in increasing network sizes. This becomes even more true when HSRP and such technologies are used which take up two address on each layer 3 interface and give out a third address for the gateway. Keeping track of hundreds of such gateways when HSRP is being used becomes very difficult if there isn't a system for assigning addresses. Think of a company that might have 100 different VLANs... 

Making backups is really a game of probability. Assuming the data is successfully written to any media (as confirmed by the backup program's "verify backup" function), the weak link becomes the shelf life/survivability of the media. Backup tapes can break and be demagnetized. Hard disks can crash. Optical media (like DVDs and Bluray disks) degrade over time. I view the question of "Is it safe to back up to media X?" less of a yes or no question and more one of your goals and retention requirements. If you're looking at a one-time/one-off/adhoc backup that you plan to use in the short term for recovery, then it's less an issue of reliability and more a question of convenience. Assuming that you're looking at a corporate server backup solution (e.g. ongoing backups, some media rotation schedule and some retention period requirement for each backup), it's still less of an issue of reliability (since you'll assumingly have at least a daily backup) and more one of convenience. So assuming your backup process is rigorous (done according to a schedule and verified for errors) and frequent, I see no issue taking advantage of the larger capacity of Blu-ray disks. Under no circumstances though would I rely on any optical media for long term storage. For long term, tape will be most reliable. To really reduce risk of long term backup storage and avoid restore failures I think it's important to have multiple backups stored in different locations. 

One of the things we found out at my company in working with Netapp is that deduplication really only works well in a VM environment if you have your drives aligned. Which is a problem for us as we have a lot of Windows Server 2003 machines and none of the drives are aligned. Which means you barely recover around a fourth of the space possible if the drives are aligned correctly. We are being told though that once the drives are aligned correctly we should be able to recover 40-60% of our space back with dedup. 

In case this helps other people, I have a Synology NAS and the NFS point actually added another item to the path. I was trying to mount "xxx.xxx.xxx.xxx/folder" as this is what I do in Windows. But for Ubuntu with NFS I had to use "xxx.xxx.xxx.xxx/volume1/folder". 

VLANs do not really have IP addresses assigned to them. They have a network assigned to them, or a subnet, or a network range, however you want to refer to it. The address the OP supplied us is an assignable address within the range of 192.168.4.1-255. So lets say the range is applied to a group of servers so on a Cisco switch and we give the VLAN a description of "Server VLAN", 4.100 would be an address that can be given to an individual server. When referring to the Server VLAN, generally one may use the VLAN number or the network address, but typically not a specific address and the whole mask. At least the network admins I work with do not. As I mentioned above, the OPs address can be a gateway address, but typically would not be because when you think of an environment like a large corporation, if you do not have a system of how gateway addresses are assigned, keeping track of them can be rather difficult. Thus most network admins use the first or last assignable address of a given range for the gateway. In the case of the OP, that would be 192.168.4.1 or 192.168.4.254. I'm not saying this is always the case, rather best practice and generally makes the most sense.