I'm currently paying $30/month for this VPS -- and then an extra $18/month for the two additional cores. Thank you for any guidance anybody can offer! Update: Thank you for everyone who took the time to read this question and answer. Unfortunately, BlueHost has had one of their infamous blackouts that occur monthly since they were acquired by Endurance International Group (EIG) in August 2014. So I'm unable to tinker with any of the solutions posed, as I have no websites, no FTP, no e-mail, and no cPanel access. 

Did I do the right thing in purchasing the 2 extra cores? Should I ask for a refund? Should I ask for something else, like more Terabytes of bandwidth? More than the 30GB of SAN storage I currently have on my Linux? Should I just wait until I have exhausted this billing cycle, which ends in September, and go with another host? 

I personally try to avoid mixing a compiled program with the rpm ones- as they require changing where each program looks for components of the other (and env Variable setting). You may find it easier to compile php than try to use the rpm version - which you are most likely going to have to munge with for hours until php plays nice with the source-built apache you have. I think you may actually find it easier to build php than try to mess with the paths in the pre-compiled version - but I have always done the rpm builds of both apache and php (which works fine). Again my rule is don't mix local build versions and centos rpms - that is bound to cause you headaches. 

you could try (from a terminal window - I'm pretty sure cpanel has that feature). It will output a couple packages that have font in the name. Then from that output use (you can try for each package): 

Okay, so I'm still not entirely sure what solved this problem, but the array is now rebuilding. The first thing I did was to remove OMSA completely and do a clean install (when I upgraded before, I went from 7.4 to 8.2 using an upgrade package). After uninstalling OMSA 7.4, I rebooted the server and went into the PERC S300 array manager that is an option on boot. There, it was showing Disk 0 as Ready and Disk 1 as being the Spare (OMSA did not show disk 1 as being a spare). I set Disk 0 to the Spare and unassigned Disk 1 from the Spare. After that, I proceeded to boot and install OMSA 8.2. After installation, I went to view my virtual disk and viola, it was finally rebuilding. 

If you're asking about 'most companies' I would probably answer VPN intranet. And on top of that they probably have some kind of redundancy built on top (Raid, Some kind of tape robot). If you're wondering what you should do, set up a backup server, just get some NAS. If you're afraid of theft then put that server in some other closet. If you're afraid of fire then don't smoke near your office... You have to make some compromise when working with a small budget in a small company (if that is your case). 

Do the same for C1Enhanced in the bios if the above doesn't work. If you do this you will be running hotter / more power (if you server is constantly idle). These settings don't really matter if your server CPU is utilized fully (near 75%+ constant CPU load- i.e. you can't save any power if you're always running close to 100%!). Finally - CPUZ is not a great choice for determining processor specs in a vm. You should use some benchmarking software (SPEC) to get an accurate understanding of what your processor is doing! I have windows machines under ESXi that read all kinds of weird ( some read 50% max cpu utilization when it is 100%- some get confused if you add/remove processors/ switch to a computer with higher/lower base clock speeds), so I wouldn't rely on any software based CPU gauges. You really need to run a benchmark to give you an understanding of whats actually going on, as there is too much between you and the CPU when virtualizing! 

They're all WordPress installations, and recently -- despite no major changes -- my administrative areas in all Siteground websites began lagging extremely slowly and eventually timing out. The techs are usually good, but this guy just told me to purchase 2 additional cores; he said I didn't need to purchase additional storage space or an extra GB RAM. The Sites: They're basically low maintenance sites and not too heavy w/ traffic. USS Vision, WebPrezence LLC (SSLd), and then my nonprofit organization, National Center For Due Process (SSLd, and is the largest and most visited of any of the 8 websites. As server experts, is it possible for anybody here to tell if this tech is correct? Will adding 2 CPUs help, so I don't get locked out of my FTP, cPanel, and back-end of my websites? Is it possible to tell with the little amount of information I'm knowledgeable enough to provide? Or am I way off here and should I be doing something else, leery of the tech's advice? Thank you for any guidance anybody can offer! 

So basically this is what I have learned from this experience: Don't use the packaged version of gjc and then install other java on top of that (even when using alternatives!). I guess this could be the 'best practice' rule: 

I have an ASUS PIKE 2208 ( this is LSI SAS2208 ROC ) configured and working with four physical drives. Additionally I have installed the LSI SNMP agent. The agent provides all the data defined by the LSI-MegaRAID-SAS-MIB. However, I've noticed that after LSI MegaRAID SNMP agent is started the values (on Objects such as LSI-MegaRAID-SAS-MIB::temperatureROC.0) do not change over long or short periods of time. Put simply, values seem to remain the same once the snmp agent has started. However simply restarting the snmp agent seems to (sometimes) update the values. Interestingly, if a consistency check or patrol read are performed, values from the SNMP agent seem to update (the drive temperature fluctuates both up and down - not just up as one may expect from increased disk activity). I'm running Centos 6.5 with the following versions of the LSI MegaRAID software: Agent:LSI MegaRAID SNMP Agent Ver 3.18.0.2 (Oct 30th, 2012) firmwareVersion = 23.16.0-0021:3.270.95-2635:Jul 12 2013:14:20:36 driverVersion = megaraid_sas:06.505.02.00 

So it seems there is something about the disk I need to configure first. I will post the solution once I figure it out. Original Post I have a Dell PowerEdge T410 server with Windows Server 2008 R2. Storage is configured in a RAID-5 array. Everything has been working fine for several years, although I've had to swap out failed drives a couple of times here and there and rebuild. Until now, rebuilding has not been an issue, and the Open Manage software makes it pretty easy to manage. With the latest drive failure, however, I am seeing something different. I bought a new drive (same manufacturer, model number, and capacity as the other three in the array), and after popping it in, I deleted the virtual disk automatically created for it and assigned the disk as the hot spare for my array. From there, the rebuild is seemingly initiated, but progress never goes beyond 0% (it has been about 18 hours since it was initiated). Here is what I am seeing: 

I'm using tomcat's jscv to start the server ** as a service ** as the user tomcat (using jscv -user tomcat "lots of other parameters"). My server runs fine, however my question is: 

Guy - There are two kinds of power management, one is c states (i.e. c1enhance for intel - something similar for amd), and the other is p states (cool 'n quiet for amd). Is this 'CPU power management' listed under advanced software features in your conifiguration -or are you seeing this somewhere else? If CPU power management is disabled that might mean you already have cool 'n quiet disabled in your bios. Power.CpuPolicy should be set to static if you want cpuz to read the clock frequency. However - dynamic 'dynamically' scales frequency up and down. It may make sense cpuz reads only 528Mhz - your server will run at the lowest frequency necessary to accomplish the current task. See Faq 1 at CPUZ . Maybe try running a load on your server and then see if the frequency scales up. If it is not scaling you will need to do 2 things to (possibly) fix them: 

("FTP (All)" service object uses the same ports listed above - TCP 20, 21, 49152 - 65535). I am not very confident in what this rule is doing, but I think this is what allows passive connections to my server from external sources. However, I don't know if it is causing problems when I am trying to connect to external sources. -- Using the Sonicwall's packet monitor, I am able to login to the server and issue a PORT command which returns a 200 PORT successful response. However, after the LIST command is issued, my server never ACKs the SYN message (frame #57/58) sent by the FTP server. The FTP Server then retransmits the SYNs (frame 64/65 and 72/73). When it doesn't receive an ACK in the timeout window, it times out. Here is some output from the Sonicwall's packet monitor during an attempted connection: 

Using sge with resource complex called 'gpu.q' that allows resource management of gpu devices (these are all nvidia devices). However on the systems there are multiple gpu devices (in exclusive mode) and if two jobs are allocated on the same node there is no way for the user to opaquely create a context on the correct gpu. Has anyone run into this problem ? I was thinking of somehow managing specific gpu resources and mapping the host and device id's. Something like 

The MegaRAID MSM is MegaRAID_Storage_Manager-13.01.04-00.noarch And MegaCli is MegaCli-8.02.16-1.i386 All of these packages are installed through the LSI provided packages. Any dependencies have been installed through yum, so they should be up to date. I find it hard to believe that there are no temperature changes (not even \pm 1 degree) at all throughout a day (as the temperature of the environment is not nearly constant). Everything else works properly so I find this odd. I should note that gives the same temperatures as - so where ever these utilities are obtaining their data from is consistent. If anyone has seen this or solved this issue I would appreciate some insight as to why these values don't seem to update. 

I've checked windows logs and don't see any failures or errors related to this within the past 24 hours (when I first swapped out the disk). Looking at the OMSA logs, it tells me when I unassign a global hotspare, but there is never an event corresponding with me assigning a global hotspare. Similarly, I see notices when I cancel a rebuild but never when it is initiated after assigning a global hot spare. I've done some looking into similar problems, but most people receive an error or an actual failure during rebuilding, which I am not. I have good backups and the system is still running okay, so my next thought is to just take an image, blow away the array and start from scratch. I am mostly looking for other ideas before going that route.