I wish to dissent from some of the suggestions in the answers above. First, inflation need not be uniform, and it has a political component. So the idea that rising hamburger prices will be offset by rising wages is by no means certain. At present, we see in many areas of the nation rising costs in housing, education, and other "essentials" that are not offset by rising wages. In some markets, like Manhattan where I live, growing income inequality can mean "inflation" inequality. Stagnant wages meet rising upper-tier incomes and rising property prices. So in certain possible worlds your hamburger might become unaffordable to wage-earners. When you ask if inflated money can become technically "worthless," the answer is yes. In the Weimar hyperinflation, there were cases of money being burned for fuel because its value as "money" was less than its value as "paper." Since paper is about as intrinsically valueless as a monetary medium can get, one might say the state certification of "money" had become "worth less" than any practical medium and thus "worthless." 

(Here $\sim$ is set notation instead of preference ordering.) The sense that Karni involves two acts that agree with each other outside of $B$. However Savage's definition of conditional preference is exactly $f\succcurlyeq_E f'$ iff $f_Eh\succcurlyeq f'_Eh$. If there really is no loss of generality in the rest of the proofs it doesn't matter which definition you pick. However Savage's definition is in line with both yours and mine, and the definition of conditional expectation. I'll look to see if that has any real consequences for Karni. ADDENDUM Using equality instead of $\sim$ is plenty general, simply because if the two acts only differ in consequences that do not have an effect on the preference ordering, you could just redefine the consequence space such that the equality was still maintained. Which is a cop out, but also true. I believe I've figure it out. The only place Karni seems to use his conditional preference definition is in P7. Savage uses an analogous P7 but with his own definition of conditional preference. P7 says (Savage's wording): 

If you enjoy ethical and philosophical aspects, you should look into international aid and intervention. Check out "Adaptive Preferences and Women's Empowerment" by Khader and work by Nussbaum in regards to social justice. These are some great texts coming off of reading Hayek. If you aren't familiar with the concept of adaptive preferences, it is the idea that preferences may adapt to circumstances of deprivation. Other readings I suggest: Creating Capabilities by Nussbaum and Robust Political Economy by Pennington. Coming back to your question on Keynesian texts, you have to start with "The General Theory of Employment, Interest, and Money", it spends quite a bit of time trying to prove the soundness of the inflationism and labor unionism components of contemporary economic policies. Written in the midst of the great depression and the New Deal by FDR it seemed to be the correct approach for the time. 

Which of these measures is most useful is not clear, and I've seen all three used at different times and in different places. This speech by Janet Yellen has some discussion of the different measures, as does this paper by Carola Binder. 

I don't feel these two interpretations are mutually exclusive, they belong to different sides of the same problem - one is empirical the other is theoretical. The conflict you seem to see between these two methods is that the "population" interpretation is individual specific. But the population component is a statistical convenience. It is "everything else" that the researcher has no time to model, and it is variation on the level of the population of observations not people - so it is not an individual specific error. In fact, one acceptable interpretation of this convenience is that conditional on non-random components individuals are the same, and the errors are such that if presented with the same decision again the individual would make a different choice. This is precisely equivalent to your neurological interpretation. Extra structure on the random component may change the degree to which this justification applies - if errors for an individual are assumed to be correlated over time it decreases the relative impact of a neuroeconomic explanation and starts to be about unobservable heterogeneity on the level of the individual. But random utility can still easily encompass both explanations as answers to different sets of questions. The residual error will include both neuroeconomic reasoning and any variables that have been omitted. To the extent that the randomness is specifically designed to capture "everything else" talking about what causes it is something most empiricists don't want to focus on. This being said, Michael Woodford has written something recently about neuroeconomics and random choice. And the "stocastic neural functions imply stochastic choice" is apparently a fairly common assertion in neuroeconomics: (see here, here, and here though some of these references point out that neuroeconomics places constraints on the form of random choice). But given the variables omitted in an average econometric study, I would hazard a guess that the random term captures relatively more of this omission and less of neurological processes. 

John Stuart Mill famously remarked that labor-saving machines have not saved one minute of labor. More seriously, Marx argued, roughly, that machines can gain temporary market advantage, but cannot produced "surplus value," which come only from increasing attachment of labor. (Though Marx's value theory is complex and slightly ambiguous.) Nonetheless, observers, neoliberal and socialist alike, seem to agree that machines are "productive" and "labor-saving" overall. For neoliberals this is productive "innovation" and for leftists "technological unemployment." The idea of a future where "most work is done by robots" seems remarkably widespread. This strikes me as a fallacy for three reasons. First, in a rough adaptation of Say's Law, someone must obviously make, fuel, and maintain the machines. Someone else must, for example, mine the additional metals, feed the miners who mine the metals, raise the farmers who feed the miners who....etc. Second, the rise of "labor-saving technology" since the 19th century has come with a huge, five-fold increase in the global laboring population. The "local" increases in per-laborer productivity appear more than offset by the attachment and utilization of increased, lower-cost labor worldwide. Third, the idea of "robots doing all the work" would seem to violate the Second Law of Thermodynamics. Machines cannot run or reproduce themselves. The only "perpetual motion machine" is life itself. Without humans all machines succumb to entropy almost immediately. This would apply to any level of technology. I am not arguing that machines make no difference. But is this difference primarily local and conditional? Is the local "productivity" of machines primarily and irreducibly a way of displacing labor globally... and redirecting labor through "machines" from lower-cost to higher-cost zones? Above all, is it logically possible for machines to produce more "work" in total than goes into them? Again, even apart from issues about "quality of work," I am always surprised at how many people seem to assume this "robot replacement scenario" is plausible. P.S. Though I would like to respond to answers and comments, the "comment" function is not working for me at the moment, I have asked "Help" about it. My argument from thermodynamics appears to be misunderstood, I will address as soon as possible. 

All these tests rely on the same, extremely basic, intuition: If the model is good then it should fit the data well. If the model does not fit the data well it is probably not good. 

ICAPM Factors People have chosen different ways to pick factors. Chen, Roll and Ross are a classic example of attempts to find reasonable ICAPM factors. Fama-French factors are often explained as correlated with underlying ICAPM factors. Other researchers have chosen to look for factors without assuming outwardly observable exposures by analyzing returns using factor analysis or principal components. Explanations of Fama-French I don't know if there are "commonly accepted" explanations for the Fama French factors. This is not to say that there aren't explanations, there are a lot of them. Just no one seems to agree on which ones are "best". Candidates include both consumption based explanations AND production based models, each of which relates Fama-French factors to underlying economic variables. Cochrane has a nice summary of the performance of both in pricing Fama-French portfolios here. Additional factors commonly used Fama and French are now talking about a five-factor asset pricing model which also includes profitability and investment, similar to Novy-Marx's paper and work by Chen, Novy-Marx and Zhang which more directly relates to production asset pricing. Momentum is also a consistent candidate, although it's a real pain to put any kind of explanation to. There are tons of others. This paper lists many of them, as well as calling the significance of some of them into question. 

Not at all. Assuming You wanted to calculate the inflation rate from between 2003 to 2004, you only need to know the CPI for the starting and ending dates. So the CPI index in 2003 is 1.84 and the CPI index is 1.889 in 2004. The formula is: (end -start)/start so we have (1.889-1.84)/1.84 = giving us 0.02663 or a percentage change of 2.663% -This is the inflation rate-. If we moved the base year to 2003 and assigned the CPI of 1, we can use this calculate the CPI in 2004. (X-1)/1 = 0.02663 X-1 = 0.02663 X = 1.02663 * 100 is a CPI for 2004 of 102.663. Made this table for you to get an idea of how it is calculated based on your example. 

Michael Woodford's book Interest and Prices, while it may not be explicitly New Keynesian, may have some of the rigor you're looking for applied to this class of models. A more direct alternative would be New Keynesian Economics edited by Mankiw and Romer. While it's a collection of papers not a textbook, if you're looking for underpinnings of New Keynesian models this'd be a good place to start. Also "The Science of Monetary Policy: A New Keynesian Perspective" by Clarida, Gali and Gertler. 

Easiest fix: if you're worried about it you should value weight your results. This is suggest by, for instance, Kothari, Shanken and Sloan (1995). Firms that are delisted tend to have extremely small market cap, so value weighting gives them very little impact on summary statistics. Delisted returns should also be used, although I'm not sure how much impact they'll have. I've seen the delisted return stuck into the month after a stock ceases to be traded. In finding $\beta$'s, I tend to see the regression used only on those dates for which the stock return is observed. The correction really comes in value weighting summary statistics afterwards. Whether all this is "correct" or just the practice I've seen is not something I'm sure about. Edit: here's a different perspective. 

Its my opinion that over the next several decades you will see wage equalization first in sectors that do not have brick and mortar storefronts to interact with their customers. From that we should see: -Shrinking inequality globally -Reduction in labor discrimination and increased focus on skill based assessment. -Reduction in the wage rates for medium-skilled workers (that is non-specialized workers) in developed countries. We are seeing that labor components across countries are becoming increasingly freely traded. Alongside this we are seeing transaction costs of labor fall dramatically, labor now is easily searchable and the price for payment processing can be as low as 1%. Makes you wonder just how fruitful these nationalist movements around the world will be in the coming years without shutting down the internet completely. 

Just to add to what is here, a lot of economists who do heavy work (dynamic programming, structural estimation) can't get away with using a language like Matlab that isn't compiled. From older economists (tenured faculty, say) I see a surprising amount of fortran for these applications. C++ may be more popular among younger economists for the same job, but fortran has had surprising staying power. 

So I don't quite agree with Alecos' answer. Let me give a perspective from corporate finance. The equality you see is based on the budget constraint of the firm. The firm has assets in place. It must've paid for those assets somehow. Either the owners of the firm paid for them (which counts as equity) or lenders to the firm of some form or other paid for them (which counts as debt). How does cash fit in to that framework? Well, in this equation, cash is counted as an asset. The cash had to ultimately come from somewhere too, either directly from external finance or the profits accrued to the firm using assets purchased through external finance. And, cash will eventually go to equity holders or debt holders in some form or another, either through dividends or payment of principle and interest. Importantly, the firm does not, itself, really 'own' anything. The firm is built of money from other places, and so everything it has is either owed to debt holders or owned by equity holders. That includes the cash it holds. Why does growth require external finance? It doesn't always. If the firm has enough cash from operations to cover any investments it makes it can plug profits from assets in place back into its production and operate for an extended period without injection of additional external finance (it still must've used external finance at some point to buy the productive assets it's currently using). However, it is almost certainly not optimal for the firm to never rely on this internal financing. One reason why is precisely because internal financing is cheap. Imagine that we have a one period firm with some internal funds and a scalable project with decreasing returns. Further, imagine the project always has returns higher than the risk free rate, $r_f$. Since the firm is only operating for one period, using a dollar of internal funds for the investment project only costs them $(1+r_f)$. But the benefit of investing is always higher than the $(1+r_f)$. So the firm will use up all its internal funds in investing. Then the firm will have to rely on external funds. External funds tend to be more costly, and greater reliance on external funds costs more, so the firm will begin to use external funds until the costs of funding is the equal to the marginal return on investment. That would be the static equilibrium. Now, what would happen if more investment opportunities were to appear? The firm would then be forced to rely on external funds to pay for that investment. And we're back to the relationship above. But you'll notice in this example the firm will always exhaust the cash it has, either through investing or paying out to equity holders. In a full dynamic model, the firm actually has a benefit to holding cash, as they want to avoid having to rely on costly external finance as much as possible. But it is still the case that for relatively large investment opportunities, they will end up relying on cash. For a model where this holds true, check out DeAngelo, DeAngelo and Whited (2011). Even in these dynamic models, the market value of assets (including cash) is equal to the value of debt plus the value of equity, modulo some accounting of taxes on corporations, because the value of those assets is the expected discounted value of future cash flows, and those cash flows will all go to either debt holders or equity holders. So the equality still holds. 

I don't believe you could successfully place a tax on robots without undermining the robotic innovation that has been driving growth across the world as well as completely undermining their sole end goal of being cheap and abundant. I understand the need for a shift in the distribution of wealth in nations. A Brookings Institute report on technological innovation from 2015 I believe stated that within a generation at current trends, it is possible that at any given time a quarter of middle aged men will be out of work. With these factors in mind, I think the most appropriate taxation to avoid stifling technological innovation is to use the tried and true taxation on a company's profits.