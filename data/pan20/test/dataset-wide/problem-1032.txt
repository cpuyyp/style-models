These images show the two elements with the highest cost. They have identical properties between both the query plans. The difference in the final row totals is the result of joins with other tables. But those joins all have a low relative cost. 

That's it. Records affected is verified as 1. UPDATE: I don't think it can be dirty reads. The read that shows the previous value happens minutes after the write. There are things about this situation that I have a hard time believing, so it's totally possible there is more to the story I'm not being told. 

This may be a terrible question, because I'm not sure how information I can include to help. We have data segregated by customer. One customer apparently has higher volume of data. The same query ran for a small customer returns in 2 seconds, and the result is 11 rows. The larger customer takes 47 seconds, and the result is 6600 rows. This is a complicated query with 11 joins. This is just for a report, but the report is timing out and the operator is complaining. It's possible the difference is just the volume of data, but I want to investigate. When I look at the query plan, the percentages for the query cost are exactly the same between the two. There are no suggestions for indexes. In both query plans, I can see the highest cost is from a join to a table that has 3.8 million rows. However, both sets of data are joining with that same table. In both cases the resulting "actual number of rows" is 3.8 million. In both cases this join has a clustered index scan that is 39% of the query "cost". So my question is this: Do the percentages and query cost even mean anything? are they referring to the estimated data from the query plan, and do not reflect the real cost? Otherwise how could the same join make up 39% of the "cost" for both, but one takes 2 seconds and one takes 47 seconds? Is SQL server lying to me and the actual difference in cost is the "nested loop" for the inner join that produces the final row count even though it lists that as 0% of the cost? 

This should give you a list of the objects that are in the WarehouseDatabase databse but not in the NewWarehouseDatabase database. 

Perhaps, if you see a big IO Queue on your SAN, you have a lot of IO bound queries on top of your memory pressure. Check the PLE and Memory Grants Pending counters on this server. If you PLE is very low and you have lots of pending memory grants, your server might benefit from additional memory. Also look if you need to add any indexes (review missing indexes DMV) -- Are you using SQL's Missing Index DMVs? 

MBR cannot handle partitions bigger that 2 TB, thus if your partition is bigger than 2 TB you have to use GPT. There is no performance gain as far as I know. It's all about the capacity! 

I am not sure if you're interested in all constraints but INFORMATION_SCHEMA.TABLE_CONSTRAINTS doesn't seem to return the DEFAULT constraints -- TABLE_CONSTRAINTS (Transact-SQL) 

No update yet because the threshold is 24,796.4 - 20247 = 4549.4 but we inserted only 4548 rows for ID 8. Now insert this one row and double check the histogram: 

You can use CNAME in DNS. When you're done with GARDB1, rename it to something else (GARDB1_OLD). Create a CNAME in the DNS server called GARDB1 which should point to GARDB2008. It should solve your problem because GARDB1 will resolve to GARDB2008 now. Ideally what you need to have is a generic CNAME that you can just re-point to a new DB server each time you are migrating. Perhaps next time you plan to migrate, you can implement it. Or you could go with what @SeanGallardy suggested. 

I'm not sure if there is a named pattern for this, or if there isn't because it's a terrible idea. But I need my service to operate in an active/active load balanced environment. This is the applicaiton server only. The database will be on a separate server. I have a service that will need to run through a process for each record in a table. This process can take a minute or two, and will repeat every n minutes (configurable, usually 15 minutes). With a table of 1000 records that needs this processing, and two services running against this same data set, I would like to have each service "check out" a record to process. I need to make sure that only one service/thread is processing each record at a time. I have colleagues that have used a "lock table" in the past. Where a record is written to this table to logically lock the record in the other table (that other table is pretty static btw, and with a very occassional new record added), and then deleted to release the lock. I'm wondering if it wouldn't be better for the new table have a column that indicates when it was locked, and that it is currently locked, instead of inserting an deleting constantly. Does anyone have an tips for this kind of thing? Is there an established pattern for long(ish) term logical locking? Any tips for how to ensure only one service grabs the lock at a time? (My colleague uses TABLOCKX to lock the entire table.) 

I am wondering about unused index in MS SQL Server. By the Index usage DMV I can identify an index which has not been used for seeks, scans or lookups. However I know from Oracle that an index might not be used in such a way in a execution plan, however it can still contribute statistics/cardinality information to the (Oracle) optimizer. This contribution is not monitored in the same way. So I am wondering if in MSSQL a Index can have a similar positive effect even when it is not directly used (in a representative time frame)? And specifically, if it can be better than a column statistic (I.e. dropping the index would be harmful). I haven’t seen this mentioned in any of the index tuning articles I have come along, so I assume MSSQL (up to 2017) does not have this concept, is that correct? 

Check the management package $URL$ It will describe procedures to,clean up the audit entries according to your likings. 

In a scenario where Oracle Golden Gate is used to replicate a primary site with an Oracle RAC database to a secondary site (and active/active back) we suspect unexpected changes from the unused secondary site. The issue is a bit hard to debug as we do not have direct DBA access. I wonder is there an easy way with unprivileged SQL access on the primary side to see if any changes are received from the other database? Can I see counters or timestamps of OGG activity which helps me to track down DML made? As I understand it I could see changes from the OGG user when setting up triggers or auditing - however both is not available in this situation. 

I can be reasonably sure there are not conflicting updates to this record (record is owned by a single user). There would be a lot more wrong than this if that weren't the case. Newer versions use a stored procedure here, but it would not be possible for me to change that in this version. 

Update 2: I was hoping to not do this, but maybe the exception handling is part of the problem. Here is the create procedure for this, and also another stored procedure being called in the exception handler. Sorry for the length: 

I have a query, that as far as I know has failed exactly one time. It's a simple select count(*) from one table, no joins. But at least this once, executing that query resulted in no data read from SqlDataReader. Not even null, just nothing. First call to Read returns false. No exception was raised. Has any one ever heard of that before? Any scenarios you can think of that would cause it? I'm not even sure what to ask for to look at beyond SQL server logs. It's not something we can duplicate. I am assuming I'll have to chalk it up to a fluke and move on if/until it becomes a chronic problem. Here's a similar query: 

Is there a class of error that will thrown an exception out of the query, but not raise it to the application when executing a command or reading from the resulting SqlDataReader? UPDATE: Here's the code executing this. 

Now the histogram show the missing ID 7 and the execution plans show the right estimates as well. Query #1: 

It depends on the session's execution plan. If a session executes a query with a serial plan then yes, you can say 1600 queries, but if it runs a query with a parallel plan, then a session can have multiple worker threads per session. Tasks, Workers, Threads, Scheduler, Sessions, Connections, Requests – what does it all mean? 

Using RCSI on the subscriber(s) is a common way to design this type of a scenario to avoid blocking. RCSI will allow readers and writes to play nice together but won't solve writers blocking writers. Since the report queries are readers and the transactional replication is a writer, this feature is a good fit for this. You just need to make sure that your TempDB is configured to support your workload for the versioning. Also remember that enabling RCSI adds 14 bytes to each inserted/updated row which might cause internal fragmentation. 

When I need to do something like this, I just use sys.objects. After you restore the NewWarehouseDatabase databse, create a Linked Server on the instance where you have WarehouseDatabase to point to the instance where you have the NewWarehouseDatabase databse. Then write an EXCEPT query using sys.objects. Something like this: 

We tried to use ODBC for QB few years ago but gave up on this idea because it was painfully slow! So our developers created an extract application using QB API to export the data we need out of the QB to a CSV file which we import into a table on our SQL Server. This process have it's own drawbacks like when a QB client gets updated to a newer version everybody needs to get on the same version otherwise the export application fails. Again, we developed this few years ago and tested with, perhaps, an old ODBC driver. Also have only handful of QB clients that need to be maintained. This has been working for us all these years. Here is a good starting point if you'd like to start developing an application using QB SDK -- QuickBooks Desktop HTH 

P.S. I did edit this a little, changing names of the procedures, the name of the table only, and removed comments only. 

I have two reports from the same customer, where they claim that an error occurred that implies a SQL Statement didn't execute. These are very simple statements, setting a value in a single column. I can see in my application logs that the command executes, and no errors occur. In one case, the records affected value is even verified to make sure that exactly one row was affected. And yet, when they view the data elsewhere, the value is unchanged. What would cause SQL Server to respond successfully to an update statement, and yet when that data is read within the next minute or two, that change is not reflected in the returned data? I'm kind of at a loss of where to even look. SQL Server logs? This is an extremely rare occurrence. It's not reproducible. Customer care informs me that this behavior occurs once every 6 to 9 months, with tens of thousands of statements working in between occurrences. But they know immediately after it happens, and could look at logs. This customer is using SQL Server 2008 (R2 I believe), my application is using ADO.NET (.NET 4.0) and written in C#. These statements do not have an explicit transaction, and are not occurring as part of a batch. One area where this has occurred is setting a single bit column to 1, for exactly one row. The other is updating a single record with an integer value. Sample query: 

The systemd unit file for the SQL server should be locally extended by a dependency on the this will make sure the start is done after the mount and the unmount waits for the stop. The most generic option to do this would be 

I think it automatically depend on the installation path of the scripts already. Note that you do not need to modify the systemd unit files in the library directory but you can amend them in the 

When using Oracle BasicFile and SecureFile one property of the implementation is that each LOB occupies multiples of the chunk size (and in case of SecureFile the smallest chunk size is the block size). This does luckily not happen for data which fits inline in the row, however for that data I could use a normal type. So my concern is more with data stored in the lobsegments - there the overhead can be up to a whole database block (per row) if the LOBs have unlucky sizes. Does this change when compression is turned on? I mean if I have a blob which is 7k and compresses to 5k It would still occupy 8k (if this is the DB blocksize). It would be somewhat unfortunate to have a tablespace with extra small blocks for (extra large) LOB segments. (But I understand that SecureFile with dynamic chunk sizes deals much better with this) 

I am looking for information on recyclebin extents. When dropping a table its segments are renamed to 'BIN$...' system generated names and the extents are hidden. Will those extents still stop me from shrinking a data file (or tablespace)? If so I would like to account for them in the script. Currently I am using a query inspired by AskTom's maxshrink.sql: