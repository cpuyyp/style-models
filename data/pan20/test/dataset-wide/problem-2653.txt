Can questions like "What could a jew do, when he arrived with a train in Auschwitz?" really be answered? Do such questions have a place in philosophy? What about analytic philosophy? Works of continental philosophy like Minimal Moralia apparently deal with such questions, but statements like "There is no right life in the wrong one." don't feel like sufficient answers to me. There is however a grain of truth in such an answer, because it is already too late to do anything at the moment where you arrive at the death camp. There were certainly enough signs that things got worse before, while it was still possible to leave the country. But leaving is not easy, temporary shelter might be easy to find, but what about the long term future? And what about the friends and family you leave behind, they might have to pay a high price for you leaving. Assume people get arrested for what they think (not for what they did or planed to do), certain groups of people are no longer allowed to leave the country, and those abroad are called back with the justification that one might want to arrest them for who they liked or disliked. Wouldn't that be enough justification for leaving the country, or at least not return if you are currently abroad? That short digression indicates that one main problem with such questions is that they cannot be answered in isolation. And because of that, they also don't have correct answers. And the partial answers that they do have imply uncomfortable political conclusions. Does philosophy deal with those issues? Is there a difference between analytic and continental philosophy in this respect? 

I wonder why "Mathematical Logic" from Ebbinghaus et al. omits 0-ary relational symbols (which would normally be interpreted as propositions, similar to how 0-ary functional symbols are normally interpreted as constants). I have to admit that this omissions seems to have nearly no influence on the text, and I only noticed this omission in chapter 11, where propositional logic is introduced. The book also omits the truth constants for "true" (1/T/⊤=top) and "false" (0/F/⊥=bottom). This seems to be a more common omission also in other books. At least the omission of a truth constant for "false" seems to have a noticeable impact on the text. This constant seems to be implicitly required for some theorems, and replacements for it like "¬x≡x" or "(φ ∧ ¬φ)" (which is an abbreviation for "¬(¬φ ∨ ¬¬φ)", but could be replaced by "¬(φ ∨ ¬φ)") keep to be invented on the fly (or the theorems are stated with more distinction of cases than necessary). It also reduces the expressive power of implication (→), which would otherwise suffice as the only logical connective (similar to the Sheffer stroke). Regarding implication, the "is implied by" connective (←) is also omitted. These omissions are certainly intentional, but I don't understand why. Can you help me? 

This makes it clear that Jaffe is talking about sociological phenomena here. Because no comparable phenomena exists (or existed) within the mathematical community itself, there was never any need to voice this kind of sentiment by type theorists such as Martin-Löf. The close relationship between verifiability, falsifiability and meaningfulness on the one hand, and experimental observation in physics, rigor and proof in mathematics, and absence of metaphysical speculations in philosophy has been clearly voiced by proponents of logical positivism. 

One obvious part of the answer is that the opposite depends on the context, and that not all contexts allow for the existence of the opposite. In some contexts, the opposite of 0 is 1, but in other contexts, the opposite of 0 is infinity, which might not even exist in the corresponding context. It may look like the set of all non-white colors would be the opposite of white, but sets of colors might not even exist in the context of the question. Note that the context is often implicit, which is one part of the explanation why opposites are often not unique. In some contexts, there are also different kinds of opposites (inverse elements of different operations, corresponding element in a dual order, ...), hence even when the context is explicit it can happen that it's unclear what it meant by the opposite. 

The last section of Ramsey's paper is called "(5) The Logic of Truth". It contrasts with the precious section called "(4) The Logic of Consistency". He stresses that making predictions close to the truth is totally different from making consistent predictions. He stresses specifically that "induction" belongs to "The Logic of Truth", and has nothing to do with "The Logic of Consistency". 

No, the laws of conservation are not synthetic a priori. They are "idealizations" of observed empirical facts. Determining whether the "idealized" conservation law are "perfectly true" requires a bit more interaction between theory and experiments. I'm not even sure that the currently accepted belief is that they really are "perfectly true locally", or just true for macroscopic objects and sufficient long time intervals. No assumptions about physical laws are a priori necessary to do science. For example, if you do chemistry, the laws governing chemical properties are more important for you than assumptions about the fundamental laws of physics. 

As I learned from "The seven virtues of simple type theory", the commonly used proof system for Simple Type Theory is equiconsistent to Mac Lane set theory. This in turn is considered to be a "good model" of "predicative mathematics". There are rumors that Randall Holmes has a proof that also Quine's New Foundations is equiconsistent to Mac Lane set theory. 

It has something to do with our concrete examples of the absolute. Many philosophers considered "god" as an example of the absolute, Immanuel Kant considered "the moral law within me" as something absolute, and for Plato "ideas" were examples of the absolute. Today, many people still believe that (at least) the (small) natural numbers are something absolute. (Also "love" is today often advertised as ... hmmm, let's say as less relative than many other things.) If you don't have access to good (and convincing) examples of the absolute, it is difficult to "measure" how relative your normal judgments are. Also, if you are convinced that there is no such thing as the absolute, everything seems to be much more absolute than it really is, relatively speaking. 

Your proof may not be fully formal, but the expectation is that it should at least be falsifiable. If it were fully formal, then it would not just be falsifiable, but also verifiable. If somebody goes through your proof and points out mistakes or holes, it is expected that you acknowledge mistakes, and either acknowledge holes, or explain how the holes can be filled. There are more expectation both on you as the author of the proof, and on your audience for the proof. If your audience just fails to notice your proof, or has better things to do then read and check your proof, then your proof is not a real mathematical proof yet. But if there are readers, and they point out mistakes or holes, then the author should try to respond appropriately. The readers will notice if you don't respond properly to questions and objections. It will sort of feel to them like playing chess against a kid that doesn't yet fully understand the rules of the game. They might try to explain the rules, but often the outcome is that your proof is simply ignored, and no longer considered to be a piece of mathematics. 

As Joseph W. Dauben explains, the opposition of Kronecker was part of the reason that Cantor gave philosophical arguments allowing him to defend set theory: 

Here comes my question: Many of Descartes thoughts have been taken as "obvious truths" by laymen, but have been deconstructed by philosophers as very questionable. But at least in "Discours de la Méthode", they are stated as something personal without claiming undeniable truth. Is it generally considered that this was just a rhetoric trick of Descartes, or is the interpretation of "Discours de la Méthode" as a personal account also common? EDIT The above question arouse from the following experience: When somebody praised Descartes' thought "cogito ergo sum" as the supreme achievement of human intellect, I once replied: "How deeply do you have to be disturbed to question your own existence?" A historically and philosophically more literal person then suggested to us that we should take the circumstances of Descartes' time into account when discussing his thoughts, especially the Thirty Years' War. 

The description of the strong AI position in the wikipedia article on the Chinese room describes the blurring of the difference between a simulation and the real thing with respect to the mind: 

Socrates was a living person, and was really sentenced to death. He wasn't the only one being sentenced to death for contrived reasons during that time in Athens. People fleeing Athens for fear of persecution sometimes referred to Socrates death for justification. I see this as some form of convention, because the accusation "not holding the gods in honor" often just masked reasons more closely related to xenophobia, and referring to Socrates allowed the victims to avoid giving the real reasons why they feared persecution. So when Aristotle explained "I will not allow the Athenians to sin twice against philosophy", he really feared persecution, albeit for reasons unrelated to philosophy (or "not holding the gods in honor"). 

I think this is a good question, also it may be a bit too broad. One philosophical movement that concerned itself with such questions was existentialism. However, existentialism doesn't assume that the human mind is paradox immune, and instead accepts madness as one possible outcome. But of course, the paradox has to be relevant to the individual in a way "concerning its existence". In a way, the human mind is mostly immune to irrelevant paradoxes. (There exists psychological disorders which impact this ability of the human mind, and it makes life much more difficult for the affected persons.) It is certainly a good idea to program artificial intelligence in a way that it doesn't hang on the first irrelevant inconsistency. This may not be trivial, but it certainly isn't impossible. More recently, Roger Penrose tried to argue that computers can't reproduce the abilities of the human mind, because they can't overcome paradoxes by reflection like a human mind. In this context, my guess is that human mind can overcome paradoxes, because it has an infinite history and past experience to draw from, which make it different from an idealized computer (where the infinite past experience was intentionally omitted from the idealized model). 

For the SEP formulation, if we add the axiom □A→A to system K in order to get system M, it seems like □(□A→A) wouldn't be a theorem of system M. The wikipedia formulation on the other hand makes it clear that □(□A→A) is a theorem of system M. But when we are using system K or M and have a premise p, we certainly don't want to conclude □p from the necessitation rule. But how to make it clear whether the necessitation rule should be applicable to axiom schemes concerning the inner structure of proposition? Consider the axiom scheme for transitivity of equality (p=q)∧(q=r)⇒(p=r) in a formal system operating on equations between terms (p=q for terms p and q) as basic propositions. Can we deduce □((p=q)∧(q=r)⇒(p=r)) from the necessitation rule, or do we have to add it explicitly as an additional axiom scheme? In case we can deduce it, what is the difference in terms of semantics between a theorem from an axiom scheme (for which the necessitation rule applies) and a deduction from a premise (for which the necessitation rule doesn't apply)? 

Even natural sciences are created by human communities, and those communities easily develop habits (without noticing it) which can negatively impact the resolvability of disputes, like 

Here is my own attempt at an answer: Descartes important contribution was his belief that there exists a "method" that simplifies geometry. This belief is in sharp contrast to the earlier belief passed down by Proclus Lycaeus (8 February 412 – 17 April 485 AD): "It is also reported that Ptolemy once asked Euclid if there was no shorter road to geometry than through the Elements, and Euclid replied that there was no royal road to geometry." 

So extension can be translated as 'étendue de l’idee', 'denotation', "denotation", 'Bedeutung', 'reference', or 'to denote'. And intension can be translated as 'comprehension de l’idée', 'comprehension', "connotation", 'Sinn', 'sense', or 'to mean'. The problem for me as German speaker comes probably from the word 'comprehension', because it doesn't seem to imply anything related to 'intention' to me, but what I want to express is an intended meaning. Probably "connotation" would have been the closest English word for this. 

Variables are a useful concept in various contexts. Their usefulness arises from their ability to refer to objects by a more or less arbitrary name or symbol. In first order logic, the need for this ability arises for the quantifiers ∃ and ∀. Also the lambda abstraction λ needs this ability, and other examples less closely related to logic are probably easy to find. There are alternatives like de Bruijn indices, which allow to avoid the usage of variables. Like a bound variable, a free variable refers to an object. Depending on context, more or less assumptions about such an object are made. In the context of first order logic, the only assumption is that the referenced object exists. So in this context, free variables are nearly the same thing as constants, except for the fact that they serve a different purpose. Therefore, the treatment and exact interpretation of (free) variables is often more restricted and slightly more subtle than the treatment of constants. For example in Ebbinghaus et. al, officially only the variables "v_0, v_1, v_2, ..." exists, despite the fact that the text normally uses "x, y, z, ..." when talking about variables. The funny thing is that "x, y, z, ..." can mean any of "v_0, v_1, v_2, ...", so that the text has to sometimes explicitly exclude the case "x=y" (the text uses "\equiv" for equality between objects). The text further defines the subset "L^S_n" of the language "L^S" for the formulas which only contain "v_0, ..., v_{n-1}" freely. S-structures and S-interpretations are two separate notions, where an S-interpretation contains a concrete assignment of objects to the variables "v_0, v_1, v_2, ...". For an S-structure, you can ask whether it could satisfy a given set of S-formulas, or whether a set of S-formulas will be satisfied by the S-structure for all possible variable assignments. As another example, my references for universal algebra explicitly specify "X" as a finite or countable infinite set, whose elements are called variables, and explicitly tag this "X" to any set of terms or formulas. What this has in common with the previous example is that there are at most countable infinite different variables. Still, in universal algebra, the free variables are also use as a sort of generic element, although not through abuse of notation but through an explicit construction of corresponding free algebras. 

How can we imagine or visualize an abstract concept? A typical way is to look at some representative concrete instances of the abstract concept. What are the properties of the concrete instance of Nothing? For the empty set, the union of any set X with the empty set is again X. For the empty list, the left or right concatenation with any list L is again L. In both cases, there can be at most one object with these properties, but this doesn't mean that an object with these properties (for a given universe of sets or lists) is necessarily the empty set or the empty list. My conclusion from that is that Nothing can exist in many contexts. There are some context where it is quite possible to imagine or visualize Nothing. However, even so Nothing is unique in each given context, the different Nothings from different contexts are neither identical nor isomorphic to each other. This also means that to imagine Nothing in a specific context is not to enough for being able to imagine Nothing in any context or even imagine "Nothing itself". 

Probably mathematics became a science only after Thales, but it relied on the axiomatic method rather than the scientific method. Modern attempts to use formalism instead of the axiomatic method for the foundations of mathematics failed spectacularly. In a certain sense, mathematics is still one of the foundations of the scientific method. (Would there be circularity issues, if mathematics relied on the scientific method?) The story for medicine is completely different from the story for mathematics. Its knowledge is never solid enough, and it has to constantly fight with many different distortions of its knowledge. There are quacksalver intentionally making false claims, economic constraints, and expectations by patients and society. The application of the scientific method is difficult due to measurement problems similar to those in quantum mechanics. The placebo effect is a well known example of this, but it is only the tip of the iceberg. Medicine has developed methods like blind studies and randomized controlled trials that go beyond the initial scientific method to mitigate these issues. It also actively seeks the help of other sciences, so that there are no reasons to doubt that medicine is indeed a science and has always been. But the Hippocratic oath might be more relevant for the foundations of medicine than the scientific method. 

I was trying to make use of the trilemma in the context of sound deduction systems for monadic second-order logic with standard semantics. Then I noticed that the presence of an (incomplete) axiom system (which I considered as given and hence didn't want to question in that context) might add further possibilities not covered by the trilemma. Let's be clear from the start that transfinite induction needs an axiom system to work reliably, so any proof using transfinite induction might already be covered by the "unsupported axioms" part of the trilemma. My confusion/question is whether the "combination of the three" part is also correct. The "unsupported axioms" proof-method can be used both "top-down" and "bottom-up", but both "circular reasoning" and "infinite regress" look like they can only be used "top-down". Because "transfinite induction" looks like it can only be used "bottom-up", the truth of the "combination of the three" part seems to imply that any deduction system trying to use transfinite induction to overcome "incompleteness" will be forced to ultimately boil down to only "unsupported axioms", and hence will ultimately be covered by the normal "first-order" incompleteness theorems for second-order logic.