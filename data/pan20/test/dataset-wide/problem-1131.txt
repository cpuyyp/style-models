Another simple SQL backup question from me.... I've got a SQL Server database running on SSMS 2008 and am trying to configure some backup jobs. I am now seemingly fine with backing up and restoring the databases but have been asked to backup from one of our servers to another's C$, though am having trouble doing so. I've got the line: 

I'm trying to write a SQL Server script to iterate through .bak files in a directory and then restore them to our server. In doing so, I've created three temp tables: #Files to keep track of the file-list returned by running xp_dirtree, #HeaderInfo to hold data returned when querying restore headeronly to get the database names and #FileListInfo to hold data returned from querying restore filelistonly to get the logical file names. My question is regarding the #HeaderInfo table. Consulting the MSDN definition of the resultset returned from restore header only, I find that the fifth column (Compressed) and the last column (CompressedBackupSize) have 'invalid data types' (BYTE(1) and uint64, respectively). This, obviously, gives me an error when I try to execute the query. To get around this, I have used tinyint and bigint, respectively, and the code now runs fine. My question(s) is/are this/these: 

Which (if I'm correct) is an access denied sort of error. Anyways.... I was told to 'just set permissions to C$' but haven't been able to find out how to, or even if that's possible. I have got it working such that the server will back up to a shared folder on the second machine if one's set up but the boss has asked, specifically, if it can be done straight onto C$. Am I just going about it the wrong way? Many thanks 

AFAIK, no, SQL will keep searching through the DB entries looking for matches to the query until it has reached the end of the table. EDIT: Not entirely sure on this, so don't rely on it. EDIT AGAIN: Thinking on it some, I have a feeling the WHERE clause may be applied before the SELECT statement, so it filters the data you are querying first. So, no, it doesn't "keep looking". The result-set is, I believe, built as the query runs - it'll take all the rows from the database/table you're querying, apply the WHERE clauses to filter it to valid records only and then apply the SELECT statement (in this case SELECT * will return the whole set. But you could've asked it for just the TOP n records or something similar.) And I'm afraid I don't get what you mean in your second question - are you asking the difference in the results that'll be returned? If so, the first query will return any results whose login is "Jhon", whereas the second will only return records whose login is "Jhon" and whosee password is "123" As you have said that login is unique, I understand that these will most likely be the same (assuming there's an entry matching the second query). However, if you had a record whose name was "Jhon" and whose password was "password", that record would be returned by the first query, not the second. EDIT: In this instance, the second query would return an empty set (assuming there are no other "Jhon"s in the DB with password "123" 

Try to batching the inserts and keep transactions as short as possible as explained on this documentation. Among the techniques to batch inserts you will find table-valued parameters and SQL bulk copy. SqlBulkCopy is 15-30% faster than table-valued parameters for batches of 1,000 and 10,000 rows. Please read the documentation provided. Consider also to scale up to Premium tiers during these I/O intensive workloads. Premium tiers are intended for that type of workloads. Once the workload finish scale down the tier. If Geo-replication is in use, please make sure the secondary replica has the same tier of the primary. 

Please install the latest versions of SSMS, then try to run the wizard again. Please download the latest version from here. 

You can use Get-AzureRmSqlDatabaseGeoBackup Powershell cmdlet to get all available backups on a specified server. 

It seems like the workstation that DMA is running on doesnâ€™t have enough disk space to do schema extract. The following is the explanation the Microsoft engineer shared with us about why DMA requires disk space: 

Azure connections are encrypted (SSL/TLS) by default when connecting to Azure SQL Database although it is vulnerable to man-in-the-middle attack. You have many choices to increase security. You can use Transparent Data Encryption (TDE) as explained here. You can also use dynamic data masking, row-level security, Always Encrypted and Cell-level encryption. To receive alerts over anomalous activities with your Azure SQL Database you can enable Threat Detection. Finally, you can make use of Azure Active Directory authentication and multi-factor authentication to enhance security. 

Instead of creating a maintenance plan on a local SQL Server instance to perform maintenance tasks on an Azure SQL database, consider creating a runbook using Azure Automation. The runbook should run a PowerShell script like the following: 

Please defrag all indexes. As mentioned on this article, fragmentation can claim a lot space. Run sp_helpfile to verify the log is not consuming space also. If the log is big run the following statement to recover space. 

You can use CREATE DATABASE AS COPY to create a snapshot of production database, and then use the same CREATE DATABASE AS COPY option to create databases for development environments using the snapshot database. All this can be automated using an Azure Runbook. The runbook can run with a SQL login with elevated privileges to accomplish all copy activities. 

With Azure SQL Database you have the option of Geo-Replication. Active geo-replication enables you to configure up to four readable secondary databases in the same or different data center locations (regions). Secondary databases are available for querying and for failover if there is a data center outage or the inability to connect to the primary database. The failover must be initiated manually by the application of the user. After failover, the new primary has a different connection end point. You can learn more about this Azure SQL Database feature here. 

I need to output a report for business users where each row in the report represents one primary-keyed row in this table, but the "Value" column is a flattened concatenation of the values related to ValueID, like this: 

We had a SQL Server 2005 server that housed roughly 200 low/no traffic archive databases, and that server was not backed up to any other device. The database files (MDF and LDF) were stored on a RAID array, and when the RAID controller failed we were concerned we were going to lose our data. We sent the RAID controller and array to a data recovery company, and they were able to recover most of the files, but in some cases we have the transaction log but are missing the main data files. Is it possible to restore the database to the last transaction (which should be just after the database was created) using just the transaction log, or is this something that can't be done? 

We had been running a SQL Server 2005 server for many years at our company when just recently, we ran into some trouble. The server was configured with RAID 5 and had one hot spare disk in the array. One of the disks failed, and then a second one failed shortly thereafter. Unfortunately we did not have backups of much of this data (please spare me the "should have had backups" comments -- I am aware of the importance of a solid backup plan). So we are two disks down, we have no official DBA, and the database is inaccessible. One of our IT personnel attempted to recover the RAID array and we believe the RAID began rebuilding but he was ordered to shut it down because there was concern for potential data loss during the RAID rebuild. We sent the entire server to a data recovery company. They spent two weeks working on the server, first by imaging all the drives and then by reconstructing the data from the RAID. I received a hard drive that contains all the "recovered" files. This is a collection of mostly database files (MDF) and trans logs (LDF). Each database that was on the server consisted of only a single MDF and a single LDF. I do not currently have access to the original server, so I have been attempting to attach these databases to another server running SQL Server 2005. However, when I try to reattach any of the databases I run into problems. Some of the various errors I have received are shown below. I have had no luck restoring any of these files. I followed the steps outlined in this article, and even then was unable to restore any of the files. We paid a large sum to the data recovery company for the recovery of the data from this server and I have been tasked with proving without a doubt that the data are or are not useful to us. I simply don't have the experience to know for certain, so I put the question to you, dba.stackexchange: Is there any way at all to restore these files? 

You can use SQL Server Management Studio XEvent Profiler and you may also see SQL Operations Studio (SSOS) supports seeing XEvents from cloud and on-premises but SSOS uses the ring buffer target. 

If you capture waits related to this particular query you will see high values for the OLEDB wait type which is common when querying a remote server using linked servers. To my knowledge you cannot avoid that. I have wrote about it here. You can capture query waits using below script: 

It seems Microsoft is recommending using a SQL Server function on this article. These two are the two options I know you can use. 

Let start by making sure you are using the current IP of your Azure SQL Database server. Let's ping the name of the Azure SQL server. The ping will fail but it should also return the current IP of the SQL Azure Server if DNS resolution is working. 

It is always important to use the latest SSMS for better experience with SQL Azure. Intellisense is now supported and many bugs related to SQL Azure have been fixed. Please download the latest version from here. For importing a database to an empty SQL Azure database, please use the Data Migration Assistant. Download it from here. 

Those BKRK are related system processes and back end connections and do not count upon the limit of connections associated with every service tier. They can safely be ignored. BKRK processes are related to service broker on on-premises SQL Server instances and Azure SQL Database may be using it as part of the service it provides. With Azure SQL Database you will always see many connections and processes related to the automated features the PaaS provides to the customer. Learn more about back end connections here. Instead of focus on system processes, please try to know if blocking is the culprit. 

C# offers the System.TimeZoneInfo class. This class exposes 2 helpful methods. The first useful method is named FindSystemTimeZoneById(). By passing the time zone id to the method you can get the TimeZoneInfo for the given Id. For example, if your Azure Website was hosted in West Europe and you wanted the time for Munich, Germany, then you could use the following code set shown below. 

If Telnet is successful, the window will change to a completely blank screen. Try connecting with that IP using SSMS again. If telenet is not successful, try tracert to identify at what step it fails to reach the Azure SQL Database server. 

100 DTU is equivalent to 1 core. It should require less than 150 IOPS in terms of storage performance. Although the definition of DTU states that DTU is a blended measure of CPU, memory, and data I/O, however none of the performance counters used by the DTU Calculator take memory into account. This does not help us to give you the memory requirements for that 100 DTU tier. Hope this helps.