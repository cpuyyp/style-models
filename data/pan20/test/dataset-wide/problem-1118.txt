Rather than trying Opensource or free tools, I would highly recommend using Redgate tools for schema comparison. (note: I am not working for or affiliated to Redgate, but have and is using the schema and data compare and trust me they are life saving !) For sql server : SQL Compare. Useful resource for automating using Powershell or command line. for Oracle : Schema Compare for Oracle Note: Just to mention, there is Data compare to sync data as well. 

Sys.dm_tran_locks  will list all current locks which are granted or pending. You can join it on session_id with  sys.dm_exec_sessions  will list all current sessions which includes client host name and login name. You can even use the GUI - activity monitor to watch for activities going on on the server instance. (You can even profile using PROFILER and then use the T-SQL to monitor your server instance). 

Never do that without understanding your workload and proper testing the recommendations. Refer to Don’t just blindly create those “missing” indexes! by Aaron Bertrand. 

follow the checklist/steps that I have listed here. stage the application connection string to include failover partner. This way when you do cutover to new 2016, the first connection of app will fail since 2014 will not be available, but the second attempt will succeed since you have specified failoverpartner 

Below is the query that will give the report you need. There are more report queries available at this link : Note: Adjust the query as per your need. 

should I create 2 distributors? You can use the same distribution database. Though, for ease of maintenance and better performance [reducing contention - both writing to and reading from the distribution database] I would highly recommend you use separate Distribution databases. Remember that distribution database is the heart of replication. So it requires proper maintenance, backups, etc. Now if you have just 1 distribution database that supports multiple publishers and a DISASTER happened, then restoring it from a previous backup will impact ALL publishers. From BOL : 

You have to read the manual to find out or you can do a stress test using SQLIO or SQLIOSim. Brent has a good article on : SQLIO Tutorial: How to Test Disk Performance To see if underlying storage is a problem, you can use . It will show you where the hot-spots are and you can ask your SAN admin to move them away. 

First of all, apologies for such a long answer, as I feel that still there is a lot of confusion when people talk about terms like collation, sort order, code page, etc. From BOL : 

Stop log reader agent, distribution and merge agent (if they are running). Offline the database using Once offline, you have to PHYSICALLY move the data and log files to the new location that you put in in step 1. Bring the database ONLINE Start the jobs that were disabled in step 2. 

The same might be selected for source tables. Check the package, you will find the incorrect setting. 

Best is to use FREE tool from Red-Gate SQL Search - quickly find SQL in SSMS. You can even profile it while it is running to get the sql code and use that. Also, refer to Aaron's post on : Keeping sysdepends up to date in SQL Server 2008. Phil Factor has a great writeup on finding object dependency at Exploring your database schema with SQL 

Remove the current table from replication. Do you data update and (To speed up things, you can truncate the table on Subscriber if the table is massive - so when you do a snapshot, replication wont have to perform delete on subscriber table) Add the table back to replication with nosync option Snapshot the publication, but this time only the snapshot of the table that was added will be generated. 

Transaction log is the most important element in SQL Server. It is like a sequential journal that logs all the changes made to the database and contains enough information to undo / redo the changes in the event of crash recovery to keep the database in a consistent state. Highly recommend to read : Understanding Logging and Recovery in SQL Server All your question are just because you dont have proper understanding of how transaction log works. To keep transaction log in shape, you should take regular log backups (more frequently) on the principal server. Check - How do we handle transaction log maintenance for a mirrored database? 

Whereas for sys.database_files just requires membership in the public role. Update : As highlighted, you can use your existing script with a way to loop all the databases and store that result in a table and then create a view (if you want). You can use below script : 

Suspend data movement of the T-DB2 database. I hoped this would free up network bandwidth for the priority database, E-DB1. No effect. Rebooted the secondary node (LDPRDENTDB1). No effect. 

Since you are running on sql server 2012, you can use my script to find out what trace flags are set as well. 

A differential or FULL backup does not break the log chain. Also, logshipping would not skip the files. There has to be something that might be taking ad-hoc log backup and then breaking logshipping. I have set up a 3 way logshipping (dont know what to call .. so calling 3 way - one primary ships to 3 other standby servers) and it works without any issues. 

No it is not a bug. Its by design. They are kept for troubleshooting and supportability purposes. From the SQL_Server_2014_In-Memory_OLTP White_Paper 

There is no easy way to STOP trace when certain no. of files are reached. Instead you can delete the older one and keep the no. of files in the limit that you defined using parameter. Refer BOL - sp_trace_create - You have to use = along with -- specifies 

To help the global dictionary be a better reflection of the data in the entire table, the column store index build process has been changed in SQL Server 2014. It now first kicks off on a single thread that reads a sampled selection of data pages from the entire table in order to form a global dictionary for each column; after that the second phase starts to use all available threads to build the actual columnstore index. The number of rows that SQL Server 2014 will sample when building the global dictionary depends on the total number of rows in the table (“cardinality”). Since parallelism operator is involved, that where the SLEEP_TASK wait type is introduced. You should ignore that wait_type (paul randal's script ignores it) until you have addressed all the other important wait types. Below is excerpt from the research paper - (warning : pdf) :