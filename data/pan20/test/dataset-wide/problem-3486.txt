If Bitcoins are accepted as payment medium to all transactions where USD is also accepted (always speaking for one national economy), as Landsburg's scenario asks us to imagine, then, it must be the case that Bitcoin enjoys the exact same credibility like USD. If this is so, then their exchange rate cannot be anything else than unity. If I feel equally safe in accepting Bitcoins and USD (safe as regards what can I do with Bitcoins), why should I ever accept an exchange rate different than unity? The moment I demand a different exchange rate than unity, it implies that the risk profiles of the two currencies are no longer perceived as being the same (risk profiles in the very broad sense, taking into considerations everything) It's all about expectations then, what we project in the future for these two currencies. This in turn requires modelling: on what foundations does the credibility of USD relies, and on what foundations does Bitcoin credibility relies, and "what we have in mind" for these foundations. This will eventually provide "the missing equation". 

It appears that what you do is not a Taylor expansion, but an application of the mean-value theorem (as one should). If it was a Taylor expansion, apart from the remainder, the gradient would have to be evaluated at $\beta_0$. With the mean-value theorem, there is no remainder, and you evaluate the gradient at some $\bar \beta$ that always lies between $\beta$ and $\hat \beta$. As regards the issue of second derivatives/Hessian, officially speaking they/it only "temporarily" appear in the derivation of asymptotic normality of the non-linear least -squares estimator, but they vanish asymptotically (while in the Maximum likelihood estimator the Hessian stays there). Apart from that, we want to minimize the sum of squared residuals so, indexing the observations by $i$, and using much simpler notation (you will have to adjust it to vector-matrix notation), you set out to minimize $\sum_i[u_i(\beta)]^2 = \sum_i[y_i-h(\mathbf x_i,\beta)]^2$ w.r. to the vector $\beta$, in order to obtain a $\hat \beta$. So your FOC is (suppressing the regressors and passing the $i$ index to the function $h$) $$\hat \beta : \sum_i\frac {\partial }{\partial \beta}[y_i-h_i(\beta)]^2 = 0 \implies \sum_i2[y_i-h_i(\hat \beta)] \frac {\partial h_i (\hat \beta)}{\partial \beta} =0, $$ Ignore "$2$" and apply the mean value theorem to the whole expression to get $$\sum_i[y_i-h_i(\beta)] \frac {\partial h_i (\hat \beta)}{\partial \beta} = \sum_i[y_i-h_i(\beta_0)] \frac {\partial h_i (\beta_0)}{\partial \beta} \\ + (\hat \beta -\beta) \sum_i\left [-\frac {\partial h_i (\bar \beta)}{\partial \beta}\frac {\partial h_i (\bar \beta)}{\partial \beta}+ [y_i-h_i(\bar \beta)]\frac {\partial^2 h_i (\bar \beta)}{\partial \beta^2}\right] =0$$ Note that $y_i-h_i(\beta_0) = u_i$, the true error, divide by $1/n$ and then multiply by $\sqrt n$ as you are allowed to and re-arrange to get $$\sqrt n (\hat \beta -\beta) = -\left (\frac 1n\sum_i\left [-\frac {\partial h_i (\bar \beta)}{\partial \beta}\frac {\partial h_i (\bar \beta)}{\partial \beta}\right] + \frac 1n\sum_i\left [[y_i-h_i(\bar \beta)]\frac {\partial^2 h_i (\bar \beta)}{\partial \beta^2}\right]\right)^{-1} \cdot \left(\frac 1{\sqrt n} \sum_iu_i \frac {\partial h_i (\beta_0)}{\partial \beta} \right) $$ Now, we consider asymptotic normality, given that consistency holds, $\hat \beta \xrightarrow{p} \beta_0$. Since $\bar \beta$ is sandwiched between $\hat \beta$ and $\beta_0$, it follows that $\bar \beta \xrightarrow{p} \beta_0$ also. This means that $$\frac 1n\sum_i\left [[y_i-h_i(\bar \beta)]\frac {\partial^2 h_i (\bar \beta)}{\partial \beta^2}\right] \xrightarrow{p} \frac 1n\sum_i\left [[y_i-h_i(\beta_0)]\frac {\partial^2 h_i (\beta_0)}{\partial \beta^2}\right] \\= \frac 1n\sum_i\left [E(u_i) E\frac {\partial^2 h_i (\beta_0)}{\partial \beta^2}\right] =0$$ because $E(u_i) =0$. So this term vanishes asymptotically and we are left with (cancelling also the negative sings) $$\sqrt n (\hat \beta -\beta) \xrightarrow{d} \left (\text {plim}\frac 1n\sum_i\left [\frac {\partial h_i (\beta_0)}{\partial \beta}\frac {\partial h_i (\beta_0)}{\partial \beta}\right] \right)^{-1} \cdot \left(\frac 1{\sqrt n} \sum_iu_i \frac {\partial h_i (\beta_0)}{\partial \beta} \right) $$ You have to assume that the first sum converges to something positive definite, and the second converges in distribution to a normal random variable, and you do indeed make these assumptions (or deeper ones that lead to them). I don't have the specific book you mention, but you can compare the above to Davidson & McKinnon "Econometric Theory and Methods"(2004) ch. 6, around eq. $(6.30)$. 

This is equation $(13)$ of Whited, T. M., & Wu, G. (2006). Financial constraints risk. Review of Financial Studies, 19(2), 531-559. It is empirically estimated as regards the specific coefficient values. The important question is, 

This still is not what Figures 1 and 2 of the paper show. The shape of the $J$-nullcline remains the issue. It should have an initial concave part and then a convex part in order to give us three steady states. But I won't dig it any further than that. 

A little head-scratcher (and a good example why we should be careful with notation). Consider a profit maximizing monopoly, that solves over price $$\max \pi = PQ(P) - C(Q(P)) \tag{1}$$ Following the routine steps (see this post) we arrive at the important result that, at the profit maximizing price, the price elasticity of demand should be higher than $1$ in absolute terms, or lower than $-1$ in algebraic terms. Namely at the profit-maximizing price we have $$\eta^* = \frac {\partial Q }{ \partial P}\cdot \frac {P}{Q} <-1 \Rightarrow \frac {\partial Q }{ \partial P}P <-Q$$ $$\Rightarrow \frac {\partial Q }{ \partial P}P +Q <0 \tag{2}$$ But $\frac {\partial Q }{ \partial P}P +Q$ is the derivative of $PQ(P)$ and $PQ(P) = TR$, Total Revenue. So $\frac {\partial Q }{ \partial P}P +Q = MR$, Marginal Revenue and we just obtained that at the profit maximizing price and in order to have elasticity greater than $1$ in absolute terms, we must have $MR^* <0$. But we also now that at the profit maximizing point we have $MR^*=MC^*>0$. So a solution does not exist, and therefore we conclude that monopolies are just a mathematical misunderstanding. Now, I went into the trouble(?) to write this smirking post, I hope somebody will go into the few dozens of seconds required to write a clear answer to point out where the trick lies. 

The profit function is an optimal relation. So it must satisfy the first-order conditions for a maximum: $$ \text{f.o.c} : w^*: PL' - L - w^*L' =0 \implies L\left[ (P-w^*) \frac {L'}{L}-1\right] = 0$$ In order for this condition to be satisfied no matter what the wage is, it must be the case that, for all wage-levels we have $$(P-w) \frac {L'}{L}-1 = 0 \;\;\forall w \implies L' = \frac{1}{P-w}L \;\;\forall w$$ Of course this looks exactly like the first-order condition - the critical difference is that it now must hold for all $w$, and so it is not any more a condition that can hold for a specific $w$, but a differential equation in $w$, that falls in the general category $y'_x = f(x)\cdot y$. It can be solved (it is "separable"). I guess you can finish this. 

(Bowing to peer pressure) According to original Marxist theory, no, he is not. Here, the defining characteristic of a capitalist is that he appropriates (as "profits") the surplus value that is created during the production process by other people's labor (capital does not produce surplus value). Your freelancer or small business owner without employees appropriates only their own surplus value, created by their own labor. And it doesn't matter how large is their income, as long as they make it without employing other people as their employees. According to some other theory? Well, somebody name the theory, and then we can clarify who is a capitalist and who is not, according to that other theory. Otherwise, it will be too much "personal opinion-based". 

I will answer the first question, I believe the second one can be found in the book's appendix. "Price stickiness" is defined with respect to the optimal price level for the period (here denoted by a star) or equivalently in inflation terms. We do not have price stickiness if the current inflation equals the optimal inflation: $$\pi_t = \pi^*_t$$ Looking at eq. $(9.26)$, written $$\pi_t = \alpha\pi^*_t + \beta \pi^e_{t+1}$$ to allow for any expectations formation hypothesis (we will need this flexibility), here are the only two scenarios under which we do not have price stickiness, while also Wickens' "general formulation" holds: A. Assume $\beta = 1-\alpha$ and $\pi^e_{t+1} = \pi^*_t$. Then we get $\pi_t = \pi^*_t$. Now, $\pi^e_{t+1} = \pi^*_t$ can be observed, it may be the case under Rational Expectations to expect next period inflation to equal the current optimal inflation. But assuming $\beta = 1-\alpha$ is a very special and uninteresting case. B. Impose immediate and full adjustment, $\pi_t = \pi^*_t$, while respecting equation $(9.26)$: we get a very specific rule for expectations formation: $$\pi^*_t = \alpha\pi^*_t + \beta \pi^e_{t+1} \implies \pi^e_{t+1} = \frac {1-\alpha}{\beta}\pi^*_t$$ This is clearly an ad hoc expectations formation assumption. It follows that in any other case, eq. $(9.26)$ implies that $\pi_t \neq \pi^*_t$, i.e. we have partial adjustment. It is perhaps more illuminating to perform the above "what ifs" using eq. $(9.27)$ of the book that is written in terms of the price level (starting in long-run equilibrium, i.e. assuming $p_{t+1} = p^*_{t+1}$). 

We assume that we start at the long-run equilibrium point $A$, (and we will discuss this point later). Then, fiscal expansion happens. If it is permanent, it is associated with long-run currency appreciation and a lower long-run price level. This means that the saddle-path of the economy moves permanently to the left and becomes the $Q'Q'$ schedule. How can the economy find itself on the new saddle-path? It must jump. But it cannot jump vertically because prices are sticky. It can only jump horizontally: so it goes from point $A$ to point $B$ which is one the new saddle-path, and then starts to travel towards the new equilibrium, that is point $C$. But at $B$ corresponds exchange rate $e_B<\bar e_2$, the latter being the new long-run equilibrium exchange rate. And lower $e$ means more appreciated currency. So the exchange rate "overshoots" the appreciation level, and then starts to depreciate as the economy moves towards $C$. Assume that the fiscal expansion is considered "temporary" (for decades, this was a good laugh in real-world macroeconomics -not so in the last couple of years). Still, even though the long-run equilibrium point remains $A$, there is short-term and mid-term horizons that must adjust. Here the point $C$ is some sort of "temporary" equilibrium point, which subsequently will start again to approach $A$. But the initial adjustment will happen exactly in the same way, and we will have overshooting. Imagining a monetary expansion, amounts to shift the saddle-path outwards -and obtain overshooting related to depreciation. A final note: if one plays around with the diagram, he will realize that if we do not start at the long run-equilibrium level, and if we are sufficiently far away from equilibrium on the $QQ$ schedule, we may obtain undershooting in either case of policy. For the case at hand (fiscal expansion), imagine that we are at point $D$ on $QQ$ that at the time of the expansion corresponds to a price level below the one that corresponds to new equilibrium point $C$, and we are moving upwards towards $A$. When the fiscal expansion happens and the new saddle-path is $Q'Q'$ we will jump horizontally alright -but we will jump on the upward moving part of the new schedule (draw an horizontal line from $D$ to see that), i.e. we will undershoot the needed appreciation, and then we will gradually continue to appreciate the currency. But this case is not unique to the fiscal expansion scenario -it can also happen in the monetary expansion scenario. But if in principle the model permits everything to happen, why everybody went bonkers over Dornbusch paper back then, and it continues to be considered one of the high peaks of economic theory? For the historical framework one can read this. The fact that the model offered a clear and optimizing reasoning to account for the continuous ups-and-downs of the exchange rates (a new "bizarre" phenomenon at the time) I guess was reason enough (and also perhaps because open economies sufficiently far away from the equilibrium point of their nominal variables are not considered very likely -prices do adjust, after all). 

What you are misunderstanding, is that in expected utility theory, marginal utility is not an independent concept from "risk aversion", as the latter is defined in the context of that theory: "risk aversion" does not mean what it means in everyday language. Being "risk averse" does not mean for the theory "I dislike risk", because taken literally "disliking risk" would imply that "risk" is a separate entity, or an aspect of a situation, which produces negative utility. A "risk averse" person is defined to be a person that has a strictly concave utility function (and so a function with decreasing 1st derivative). PS: On another front, "being twice happier" reveals that you are considering cardinal utility, where quantitative comparisons between numeric utilities is considered to be meaningful. Be aware that the predominant paradigm in economics on the matter has been that of ordinal utility (this does not affect the mathematical properties and relations, only their interpretation). 

The logic of the OP implies that all costs are fixed. In the very short-term one could say that this holds for many firms: a firm cannot leave rented buildings immediately, it can not fire employees immediately etc. Namely "costs" have the form of financial commitments. But in the mid term (and I am talking a few weeks to a few months, no more), some of the costs are in reality variable. Then if at the original break-even point average cost was increasing, the reduction of price will lead the firm to a lower quantity (where it will have lower average cost), that may even bring in profits. And the "Law of Supply" refers not to the very short term, but rather to the mid- and long-term behavior. 

which creates the needed environment for Secular Stagnation, if it goes too far down. Of course this is not the whole story. The above e-book by CEPR is freely downloadable. 

What time varying-parameters do to a model like this is to make the long-term equilibrium to stop being a specific point in the phase diagram. The zero-change loci shift and move around, and so does their intersection. A simple thing that you could do, at least from a pedagogical point of view, is to reproduce the effect of "structural breaks" and not smooth time-variation. So for a certain time period, a parameter of the model was fixed at a certain level, but then it jumped to a different level (say depreciation changed because in the past it was more buildings and machinery, now it is more IT, software and intangibles). Such an abrupt change essentially discretizes a smooth time-variation, and it is an acceptable approximation to it. This means that one draws two pairs of zero-change loci in the phase diagram, one representing the situation before and one after the structural break. And one shows how the economy behaves by jumping from the one saddle-path to the other. See this blog post of mine where I implemented this approach to reflect in simple descriptive terms the current depression of the Greek Economy. 

This sentence seems to imply that we should fault the rich not because they are rich, but because they do not spend their riches. Ok, let's scrutinize this assertion, and not go into philosophical and sociopolitical arguments about inequality, justice, etc, which is a totally different discussion and off-topic on this site (although I suspect that your aunt may in the end has something like this in mind...) Who said rich people don't spend their riches? 1) Last time I heard, we are all too ready to criticize them for "lavish life-styles". But lavish life-styles cost a lot of money, so it appears they do spend a lot...so part of their riches becomes income for the rest. But they are still rich! 2) Yes, because most of their wealth is invested in productive economic activities (directly or indirectly through mutual funds etc). It follows that wealth is NOT "hoarded" - because hoarding means "stash away wealth, don't spend it, don't invest it, just stash it away from any interaction with other humans and economic activities". A "Scrooge McDuck" kind of situation. This is not what is happening in today's world, and so the specific sentence from this excerpt is simply factually wrong. Now, the fact that even without hoarding, rich people may stay rich or become richer, the fact that there are people dying of hunger while others live lavish life-styles, in other words the issue of wealth and income inequality could be certainly an on-topic subject here if, indicatively, a) It was examined as regards its effects on economic efficiency (there is growing evidence that efficiency and distribution are linked and that inequality affects negatively efficiency and economic growth, see for example this recent IMF report) or b) It was placed and studied in the context of normative economics (and "normative" is not a synonym for "value judgements" or "opinions"). I would suggest to check this meta-thread on the matter. 

That's interesting: the flavor of the frequentist approach to probability used for a socio-political fairness criterion: if my measure as a population group is $0<p<1$, and known, then my opinion should be accepted by the whole at the same measure, as number of issues goes to infinity. In other words, current observed acceptance rate should be a consistent estimator of theoretical acceptance rate, and equal to my measure. Then it is very easy to create such a decision rule, while saving public money: no need to hold one referendum after another, just construct a die, with as many sides as there are "uniform groups", with the die's weight distributed in such a way that the side representing uniform group $i$ will have probability of turning up equal to $p_i$. It won't be difficult to construct, and publicly and objectively test it for the desired properties. Then, wherever an issue comes up for voting, just roll the die. And ok, spend some money for a suitable public ceremony. Whenever there is a census, the relative size of each uniform group can be re-measured and a new die can be constructed. Why do I have the feeling though that no uniform group is likely to ever accept such a scheme? (This of course puts aside the importance of each issue, in general, for each uniform group, etc, but I took that from the OP which concentrates on number of issues, irrespective of what the issues are about, and to whom they matter and how much they matter, and how do we measure that etc).