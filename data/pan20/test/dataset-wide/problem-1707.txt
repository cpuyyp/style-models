When the master key and certificate are created, you can create the DEK for the specific database, by using the CREATE DATABASE ENCRYPTION KEY statement: 

No issue at all, it is just common to use more relevant/logical names and preferably letters for hostname See: $URL$ $URL$ Hope this helps 

You can also try ApexSQL Diff - ApexSQL Diff is a SQL Server database comparison and synchronization tool which detects differences between database objects. It compares and synchronizes live databases, native or natively compressed database backups, database snapshots, scripts in source control and script folders Video - Introduction to ApexSQL Diff Hope this helps Disclaimer: I work for ApexSQL as a Support Engineer 

You will get a warning in the result window: Warning: The certificate used for encrypting the database encryption key has not been backed up. You should immediately back up the certificate and the private key associated with the certificate. If the certificate ever becomes unavailable or if you must restore or attach the database on another server, you must have backups of both the certificate and the private key or you will not be able to open the database. When using TDE, create a backup of the server certificate in the master database. You can use the BACKUP CERTIFICATE statement to create a backup of the certificate and private key, both of which are required for certificate recovery. The private key password does not have to be the same as the database master key password: 

EDIT: It turns out by adding a after the SSH server comes up and before trying it works fine. This seems to be an issue with the configuration of apt on this AMI combined with perhaps some strangeness on Amazon's end, or DNS issues. Here are my entries in : 

The appears to run fine and gives no errors, however (2/3 of the time or so) installing throws a "no installation candidate" error. When I ssh into the server and run I get the same error. Running by hand, then the package installs fine. Any ideas on what might be happening, or ideas for debugging? 

I'm experiencing a strange issue with a fabric script I'm using to bootstrap a server on EC2. I launch a stock Ubuntu 12.04 AMI (ami-3d4ff254), wait for it to start, then proceed with: 

We have a web service running on Amazon EC2. Currently we have some live user data stored on a single disk (EBS). We are considering moving to a RAID0 setup (we don't have to be concerned about the increased failure rate). If we do this migration, what is the quickest (to minimize site unavailability) way to reliably transfer the user data to the RAID array? One idea I had was to take a recent snapshot of the data, copy it over to the new RAID array, then when the site goes down for maintenance use rsync to copy only the changed data over. I'm not sure if this would actually save time or ensure data integrity though. 

I think that you are asking, why use OUs with GPOs when I can use security groups? I can think of a few reasons: 1) Clarity. If you create a logical OU structure (e.g. create "computer" and "user" base OUs, create "server" and "PC" child OUs under "computer", create "administrators" and "accounting" OUs under PC, etc.) and link GPOs to their appropriate OU without changing the security filter, you will be able to understand at a glance what GPOs are affecting what user or computer groups. If you use security group filtering only, you'll need to examine each GPO's security filter or use some other time-consuming method to see what is going on. If you only have a few GPOs and you are the only person managing the GPOs it may not really matter, but as the number of GPOs and the number of people managing GPOs increases the security group filtering only method becomes very confusing very quickly. 2) Efficiency and manageability. GPOs linked to higher level OUs apply to all subordinate OUs. Through a well designed OU structure, you can effectively apply group policies with pinpoint precision and minimal redundancy. For example, more generic computer policies that apply to both computers and servers should be linked to the computer OU (e.g. generic IE/Edge settings, generic security certificate deployment, powershell environment settings, etc.), policies to apply specifically to PCs (e.g. client only Windows firewall rules, MS Office related rules, client software deployment rules) should be linked to the PC OU, and custom rules should be linked at lower level OUs (e.g. specific file share mapping, special permissions, etc.). If all your GPOs are linked to the same OU, you'll need to remember every time to change the GPO priority order so that more important/specific policies take precedence over more generic policies (this is extremely easy to forget). I think you find that troubleshooting "GPOs not applying" problems becomes much easier if you have a logical OU structure with minimized security group filtering. (NOTE: some group policies, like the group policy loopback policy, can behave in unexpected ways if you use security group filtering). 3) Speed. GPOs that use security group filtering take slightly longer to process. GPOs that use WMI filtering take much longer. It may not matter with only a few GPOs, but when your GPOs start numbering in the hundreds you'll need to think about optimization. If you want to minimize GPO processing times OU-based GPO assignment is the way to go. 

I'm using fabric for provisioning instances from scratch (a stock ubuntu image) on EC2; it works great. There are a few basic functions for appending to and commenting lines in a file that make modifying config files workable. 

are there any shortcuts that can be taken on this initial "resync", since there's not (AFAIK) any data I need copied between the empty disks can I blithely format and treat it like a normal disk while it's in the middle of its initial sync, as long as I'm okay with it being in a "degraded" state for a bit? 

I understand it displays the command with arguments, or when unavailable the command in square brackets. But where do the names come from for processes such as passenger worker ruby instances, which show up as: 

I would like to configure sudo such that users can run some specific commands without entering a password (for convenience) and can run all other commands by entering a password. This is what I have, but this does not work; a password is always required: 

I have a command that runs a disk snapshot (on EC2, freezing an XFS disk and running an EBS snapshot command), which is set to run on a regular schedule as a cron job. Ideally I would like to be able to have the command delayed for a period of time if the disk is being used heavily at the moment the task is scheduled to run. I'm afraid that using nice/ionice might not have the proper effect, as I would like the script to run with high priority while it is running (i.e. wait for a good time, then finish fast). Thanks. UPDATE: This is what I ended up going with. It checks /proc/diskstats and runs my job when the current IO activity hits 0, or we timeout. I'll probably have to tweak this when I look at what kind of IO activity our servers actually get in production: 

When I install a particular program on a computer it creates an overly lenient local firewall rule that allows inbound connections to port 1234 from any subnet. I want to create a more restrictive firewall rule set using group policy that only allows traffic inbound connections to port 1234 from a specific ip address only. However, currently in my environment Windows firewall rules that are set via domain group policy are configured to merge with locally set Windows firewall rules. The result is that the more lenient local firewall rule seems to take precedence over the firewall rule set using domain policy. If possible, I want to use group policy firewall settings to change, delete, or nulify the local firewall setting (and not use a startup script to delete the unwanted firewall rule). The simplest way I think to do this I think would be to specify some sort of firewall rule priority where the local rule is rendered inert by a more specific higher priority rule specified by the domain. But I am not sure that this is possible. Any suggestions? 

I don't know if you've made any changes to the group policy task, but for troubleshooting purposes to make sure that those changes are forcefully applied to the client to try changing the action from "update" to "replace." Also, you didn't supply any information from your actions tab. Check that the shutdown executable exists on the client. Check that you've supplied the appropriate arguments to the shutdown command in the arguments field such as /r to reboot. If something is preventing the client from rebooting you may need to add /f to induce a forced reboot. 

I'm using an Alestic image which disables root SSH logins, but provides a user "ubuntu" with NOPASSWD sudo privileges. See here. In the course of trying to add a new user to the sudoers file I inadvertantly created another line for the "ubuntu" user, this time without NOPASSWD. I have now apparently lost root access to this machine. Is there some way to mount the EBS root volume on a different instance (fixing the sudoers file) and then re-launch the server? Or am I totally screwed? 

...to change my passphrase for my key, but I'm not sure what this means. If I'm encrypting data on box A and decrypting on box B (say with duplicity) do I have to change the passphrase on both ends? Will previous backups still work? Is the passphrase just the key to a sort of encrypted wrapper around the key file? Dumb question, but I don't want to screw this up. Thanks! 

Shouldn't be too many to change by hand, and hopefully your system doesn't differ too much from mine. 

What specificaly is involved in an Elastic Load Balancer health check on an instance? I know that it performas an HTTP(S) "ping"; does it just deem an instance "Unhealthy" if that HTTP request returns an error status number of times in a row? Or does it take other factors into account, such as CPU usage? 

You can use the "s3cmd" utility with the "sync" option, although I stumbled on your question because I'm trying to figure out if this syncing mechanism is screwing up my duplicity backups.