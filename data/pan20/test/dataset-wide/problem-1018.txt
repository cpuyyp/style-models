Why are the two system named indexes (SYS_IL00....) not in the USER_IND_COLUMNS view? Under what conditions might this occur? 

I know the edition exists because I can query the DBA_EDITIONS view and I can see the edition. The problem must be that the user must not have been granted use privs on the edition. I'd like to be able to verify which users have been granted use privileges on which editions. How is this done? 

Is it possible to view/monitor the listener status from Oracle Enterprise Manager (OEM)? If so, how is this done? 

I can't believe I don't know this but what type of index (Oracle) would have an entry in the USER_INDEXES view and not in the USER_IND_COLUMNS view? I see two system entries in USER_INDEXES. Here is what I am seeing: 

We are having problems with log file sync waits periodically. We have a 2 node RAC cluster using ASM. I'd like to monitor the physical device(s) containing the REDO log files using iostat, but I'm not sure which physical device contains the redo log groups. We have something like 150 physical devices so I'd like to narrow this down as much as possible. How would I map the ASM files containing the redo logs to the physical linux devices? 

I am having a problem with an application that requires very low latency (less than 100 ms per txn.) Every once in a while we will see spikes where transactions take 20-30 SECONDS. We looked at the ASH report and we are seeing "log file sync" waits, but we are unclear on the exact source of the problem. I want to compare this problem period with a normally operating period. However, the only way I know of to do this is using AWR compare period reports. The problem with this is that the periods are too long (30 mins). I need some way to compare very short periods of time after the problem has occurred since I have now way of knowing when it will happen. Any advice would be appreciated. 

The error log should contain some information about what session killed a particular process. Run the following statement to get the hostname and process id of the user the command originated from. 

TL\DR I'm looking for a way to efficiently identify the object located closest to the end of a SQL Server data file. This approach needs to remain performant against large data files. What I have so far The following query utilizes an undocumented Dynamic Management Function that shipped with SQL 2012: ; this DMF provides a rough equivalent to the command. The following query identifies the last object in a given data file (Warning: Don't run this against a database larger than 25 GB unless you want to cancel it at some point): 

As my performance tuning skills never seem to feel sufficient, I always wonder if there is more optimization I can perform against some queries. The situation that this question pertains to is a Windowed MAX function nested within a subquery. The data that I'm digging through is a series of transactions on various groups of larger sets. I've got 4 fields of importance, the unique ID of a transaction, the Group ID of a batch of transactions, and dates associated with the respective unique transaction or group of transactions. Most times the Group Date matches the Maximum Unique Transaction Date for a Batch, but there are times where manual adjustments come through our system and a unique date operation occurs after the group transaction date is captured. This manual edit doesn't adjust the group date by design. What I identify in this query are those records where the Unique Date falls after the Group Date. The following sample query builds out a rough equivalent of the my scenario and the SELECT statement returns the records I'm looking for, however, am I approaching this solution in the most efficient manner? This takes a while to run during my fact table loads as my record counts number in the upper 9 digits, but mostly my disdain for subqueries makes me wonder if there's a better approach here. I'm not as concerned about any indexes as I'm confident those are already in place; what I'm looking for is an alternative query approach that will achieve the same thing, but even more efficiently. Any feedback is welcome. 

The perceived simplicity of a single table with an abstract reference is seductive but ultimately proves to be the opposite of simple due to the above issues. I think the appeal results from a misapplication of the principle of cohesion. The cohesion here is really between the documents and the entity which they are about, not their structural similarity in that each table stores a "document." Applying the principle of cohesion with respect to structure instead of functional meaning can get you into some other ill-advised practices such as OTLT and EAV. My recommendation is to stick with the design you already have. It is correct and doesn't need "improved." 

Normalization, more formally called Projection-Join Normalization, is a scientific process in which one can remove redundancies in R-tables due specifically to join dependencies which are not implied by the candidate keys. The join dependencies are exploited by taking projections based on them to create two or more tables from the original table which removes the redundancy. It is important to note that normalization cannot remove all redundancy. Instead, it can remove only redundancies caused by join dependencies not being implied by the primary key. The first 3 normal forms and Boyce-Codd Normal Form (BCNF) deal with redundancies due to functional dependencies not implied by the candidate keys. While most designers talk about normalization to the third normal form, what they really mean is normalization to BCNF as that is the normal form with respect to the functional dependencies. Functional Dependencies In order to determine if either table is in any normal form, 1NF or 3NF, (and thus an R-Table), we must clearly state the functional dependencies. A mistake most designers make is to assume the functional dependencies based upon what makes sense to them. In simple examples this isn't usually a problem, but in complex real world scenarios it is deadly. For your example let us assume the following functional dependencies for the first table: 

First, what's so great about a filtered NCCI? Filtered NCCIs are pretty cool in that when data being returned by a query isn't entirely contained within a filtered NCCI, the engine will still use it to pull out what data it can and then go to the (traditional) row-store indexes to pull the remaining data. This is especially helpful regarding analytical/aggregate workloads where NCCIs tend to out perform their row-store index counterparts. This concept is better explained in this MS article, but I find this info-graphic, also taken from that article, to be quite effective at summing it all up: 

If you're using the option found in the Server Properties -> Security window (screenshot below), you can view the logs captured by this in the SQL Server ERRORLOGs, which are found under the Management -> SQL Server Logs folder. If you want to filter your service accounts out of this, you'll ned to setup a SQL Server Audit as you are unable to customize the basic Security auditing to anything other than None, Failed, Successful, or both Failed and Successful. If you are interested in setting up an Audit, be sure your instance is patched to SP1 CU1 or later if you're not using Enterprise edition as this feature was exclusive to Enterprise prior to this patch. Here's a blog post by Eralper Yilmaz that runs through how to set one of these up. The Option: 

I have an application that makes tens of thousands of calls per day to a stored procedure. Normally this executes in about 50 ms. However, periodically, it takes 2-3 seconds to execute. I have an application log file that shows what time and how long each call to the stored procedure took. I'm wondering...how can I track down this exact transaction within Oracle Enterprise Manager and display the associated wait events so that I can understand why this specific call is taking so long to execute. 

I have a two node Oracle RAC cluster with that runs an oltp application against it. I have both a production and a duplicate test environment with the same setup. Upon running stress tests on the Test environment (about 160 tps), The average latency is about 70ms. I noticed that when I examined the maximum transaction time within each minute, that there is a regular spike in the maximum transaction time every 5 minutes of several transactions that take about 1200-1500 ms. I've traced the individual long transactions down to the second and them seem to last for anywhere from 3-5 seconds. There don't seem to be any regular running jobs that would effect the transactions. In environment, we see the same thing and in fact once in a while, we see bigger spikes of transactions taking 15-20 seconds (15000-20000 ms) to complete. While our production environment is in a pilot mode right now, and only pushes about 4 tps, we do still see these spikes where maximum transaction lengths reach 1200-1500 ms, and they also seem to occur on 5 minute intervals. I have performed ASH analytics on the small few second intervals, and it seems that there is no common wait event that occurs during these spikes. Sometimes we see "log file sync" events, sometimes we see "gc busy" or other "gc" wait events. It seems to vary. I'm just trying to figure out how to diagnose the underlying cause of this periodic slowness. Any ideas would be appreciated. 

The unique identifier for Manager would be the personnel id. That identifier would migrate to Team as a column and thus refer back to Manager. This will ensure each Team can be managed by one and only one Manager. This constraint would be a Foreign Key constraint. The constraint on Manager, which specifies the personnel id as the key, would be called a Primary Key constraint. 

Let us address first normal form and second normal form distinctly. First Normal Form (1NF) You cannot say a table is or is not in 1NF because no two rows contain repeating information. By repeating information you may be thinking that BOOK doesn't look like this: 

For those last two relationships the verb has isn't very descriptive, but that was how the requirements were phrased. The entity has an attribute, so why wouldn't these be attributes? I discussed earlier why I would choose to make location an entity type. For dependent, the choice is more clear as its clear each employee has many dependents and so it must be broken out into its own entity. Once this is done we could make the leap to improve the verb. Perhaps something like employee cares for dependent and department operates at location. Now we are in a position to create the diagram. Here is what I have: 

I'm not a big fan of the new format myself, but in this scenario, you'll need to Google/Bing it out. I suspect this is the link you're looking for: $URL$ I got there by looking at the root feature page for AGs on 2014 here, $URL$ and then followed the legacy link trail... 

If someone has a script readily available, I'd be grateful if you could post it, otherwise I'll post something after I code it up. Thanks! 

Conceptually this is what should be happening within your database when the end of the physical file is reached: 

Just a guess, but I suspect someone likely only wanted to manage one backup file without thinking of the consequences if it became corrupted. This approach does backup the TLogs, so you can recover from this backup file. However, shrinking the log is a bad idea for many reasons (as explained by Brent Ozar and Aaron Bertrand). 

Disaster Recovery is a complex topic and providing anything resembling a complete answer can't be provided on any forum answer. If you're new to this or feel overwhelmed, I heavily suggest reaching out to MS, a MS Partner, a Consulting Firm/Consultant, etc. and ask for help. 

I have a SQL Server Reporting Services (SSRS) 2012 report that is populated via stored procedure (SP). The SP returns various fields, one of which is a datatype. When I execute the SSRS report directly the value is properly displayed, but when I export the output to Excel, it gets rounded to the nearest thousandth. For instance, this is an example of what I'm seeing. The proper value: The value shown in the Excel File: It's rounding 8660 to 9000. Truly frustrating, and Microsoft's documentation mentions nothing about this behavior. I've also dug everywhere on the report that I can think of that may affect this, such as the Text Box Properties: 

The oracle tool places an X over the crows foot on the many side to show that the delete rule is CASCADE. It also shows the keys, which show the foreign key constraint with person_id on the Account table, and the primary key constraint with person_id on the Person table. 

Department - Number, Name Employee - Name, SSN, Address, Salary, Sex, DOB Location - ??? Project - Number, Name Dependent - First Name, Sex, DOB, Relation 

Creating an ERD also helps you start to clarify the requirements. For example, I made the key to the section table the resume and a sequence number. This means you could have 2 or more sections on a resume with the same type. Does this make sense? Or does it make sense to say there can only be one section of each type per resume? Second, you want to think about what uniquely identifies a row in each table. For example, I have used a Resume Number for the key to the resume table. Wouldn't it make sense to say that each row would be uniquely identified by the User Name and the resume name? This would prevent the storage of duplicate resumes. DDL Generation Once you nail down all the business rules, you can use Oracle Data Modeler to generate your DDL. While it doesn't support mySQL, you can still generate it and get the base structure and then correct for mySQL specific syntax. Here is a sample of what I quickly generated from this model: