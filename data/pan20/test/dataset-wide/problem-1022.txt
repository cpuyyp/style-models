Make sure the existing CSV file has the two characteristics for mysql to work with it Go to the folder in your datadir. Rename test.CSV to test1.CSV Copy your existing CSV file to test.CSV Login to mysql and run 

There is a 6-month-old bug on a similar issue : MySQL Forums :: Newbie :: ERROR 1226 (42000) at line 1129: User 'root' has exceeded the 'max_updates' resource (current value: 100) 

Step 05 : When Master1 comes back up, rsync or scp from Slave_1 to Master1 Step 06 : Login to MySQL on the Master1 and setup it up to replicate from Slave_1 Don't worry about the real binary log filename and position. Using embeds command with the real coordinates on line 23 of a standard dump. You can see it with the following 

The timestamps from these two coordinates helps you figure out . Knowing these things, here is what you can do: 

This is the format to enter . For a double, just append to any integer. No need for numbers to to have quotes: 

before running Believe me, this is a much better guess that dumping a subset of the data and guessing from the filesize. 

Every subfolder under is considered fair game to be registered as a database. You could just mysqldump that entire database and then drop the database. SUGGESTION If you have to leave the database present but inaccessible, here is something radical you can try: EXAMPLE Suppose you have a database called and you want to disable access to it. Go into the Linux OS and do the following: 

You can see global variables and status in the mysql instance using SHOW GLOBAL VARIABLES; and SHOW GLOBAL STATUS; You can also see session variables and status in the db connection using SHOW VARIABLES; and SHOW STATUS; Once you have successfully connected to the mysql instance, you have a wide variety of session and server options that you can set for the current session or permamnently for all new sessions. 

Create a Subquery that Gathers Keys from the the with Maximum Date Per Entity. Then, perform an INNER JOIN of that Subquery back to the . 

Even though you are using the Query Optimizer took over as expected. The pass through the table exceeded more than 5% of the total index entries. This is what lead to the Query Optimizer to dismiss using the index. Oracle, MSSQL and PostgreSQL would not have done any different. Here is my sick attempt at refactoring the query 

SPEEDING UP SHUTDOWN / STARTUP Since you are dealing with the InnoDB Storage Engine, you need to think about what MySQL does during a startup and during a shutdown. During a shutdown, InnoDB tends to freeze everything in terms of transactions Dirty pages in the InnoDB Buffer Pool are flushed. During a startup, InnoDB crash recovery rolls forward any leftover changes in the transaction logs. The InnoDB Double Write Buffer may also be scanned during InnoDB crash recovery. The first variable I think you should set innodb_fast_shutdown to 0. Please note the MySQL Documentation: 

This will prevent any moving around of BLOB data when indexing. From here, you would have to always join data_store using its name like this: 

Did you know that MySQL has a stop word list ? For MyISAM, there are 543 words that do not get included in a fulltext index ??? In the MySQL 5.6 Docs on this (Stopwords for MyISAM Search Indexes), you can see the list. 

Afterwards, the table shold be fully accessiable. CAVEAT (Windows) If Windows has the CSV table locked, run these commands 

The first query gets all employees who are managed and their managers in the form of a Cartesian Product. Then, it looks for a common street and city. The second query collects personnel records (name,street,city) of employees and their managers and performs a NATURAL JOIN between the employess and their managers using (street,city). If you can transalate both queries back to Relational Algebra, I think you will have what you are looking for. I believe the second may be of better help. 

UPDATE 2013-09-14 20:05 EDT To demonstrate that the actually works, I ran this on MySQL 5.6.13 today: 

If you get the same kind of message, then you have corrupt table issues in terms of the InnoDB data dictionary. If you get data, then you were simply in the wrong place at the wrong time. In your case, you were in the wrong database at the wrong time. 

Step 5. Remove and from Step 6. Step 7. Now you can log into mysql as root using 'whateverpasswordyouwant' as the password. Give it a Try !!! 

This seems be an on-going bug although it is not considered a bug, such as Bug #73041: Error 1366 when loading null into columns with decimal type using strict mode According to Oracle's MySQL Documentation 

I hope is not MyISAM because an empty MyISAM table usually has a 1024 byte . A nonzero byte size is expected of a MyISAM. A zero byte sounds a little creepy to me. If you run this metadata query 

SUMMARY Look at the table definition again. There is 18 bytes for an index entry on just the index. That's 18G per billion rows. Same goes with . You just don't have enough room. It is imperative to try one of my newest suggestion to bypass the need to sort all these keys. UPDATE 2013-05-22 12:15 EDT YoU asked 

Sometimes this can occur because of a combination of a certain sequence of characters and the character set of the mysql client at the time of the import. You could have trapped such errors without additional tools by doing the following 

All you need to do is setup pg_hba.conf to make sure all users come via 10.1.2.70 Essentially, vip-up performs this sequence 

While can be attempted, it is worth noting what the output of a mysqldump looks like. At the top of every mysqldump you will see directives which disable foreign key checks and unique key checks: 

If phpmyadmin summons mysqldump using , then notice is not enabled. With enabled, a batch of rows is inserted using . Using or will cause each row in every table to have a separate . Consequently, this would blow up the size of the SQL script. Subsequently, a reload would be longer than usual. 

The first compatibility issue I worry about is the layout of the schema tables. Here is the layout of in MySQL 5.1 

I tried to produce the same ordering and having some trouble but here are two different queries you can start with: 

What you must determine is the the order to insert based on the tree's maximum height.The maximum height for a binary tree would be . This means the following: 

or you could simply make a backup of the binlogs folder and extract the incremental changes at yourt own convenience. 

If you would like to detect which databases actually contain MySQL data, you can run the following query: 

Given this explanation, making bulk INSERTs will load/unload a MySQL Packet rather quickly. This is especially true when max_allowed_packet is too small for the given load of data coming at it. CONCLUSION In most installs of MySQL, I usually set this to 256M or 512M. You should experiement with larger values when data loads produces "MySQL has gone away" errors. 

I have two suggestions SUGGESTION #1 : Use grep command You could use the grep command to filter it out 

Let's say on the new server you want datadir to be Simply copy everything and all subfolders from old server's to the new server's . You said you are using XAMPP. RUn this query . Copy that folder to . Then add this 

Since mysqldump cannot create names, how about having the mysql client get the name? In fact, I will use the function DATE_FORMAT to create a batch file called that will execute the mysqldump. The mysqldump output will be formatted . Ready ? 

You have to make sure the Slave has the exact same and as the Master you are replicating from. Goto the Master and run 

Instead of calling , which shows the current binary log, you should use . It will show all binary logs currently on disk. Here is an idea: run this every six(6) hours 

I would still set the innodb_buffer_pool_size much higher that 400M. The reason? InnoDB Buffer Pool will still cache the data and index pages you need for tables accessed frequently. Run this query to get the recommended innodb_buffer_pool_size in MB: SELECT CONCAT(ROUND(KBS/POWER(1024,IF(pw<0,0,IF(pw>3,0,pw)))+0.49999),SUBSTR(' KMG',IF(pw<0,0,IF(pw>3,0,pw))+1,1)) recommended_innodb_buffer_pool_size FROM (SELECT SUM(data_length+index_length) KBS FROM information_schema.tables WHERE engine='InnoDB') A,(SELECT 2 pw) B; Simply use either the result of this query or 80% of installed RAM (in your case 19660M) whichever is smaller. I would also set the innodb_log_file_size to 25% of the InnoDB Buffer Pool size. Unfortunately, the maximum value of innodb_log_file_size is 2047M. (1M short of 2G) Thus, set innodb_log_file_size to 2047M since 25% of innodb_buffer_pool_size of my recommendated setting is 4915M. Yet another recommedation is to disable ACID compliance. Use either 0 or 2 for innodb_flush_log_at_trx_commit (default is 1 which support ACID compliance) This will produce faster InnoDB writes AT THE RISK of losing up to 1 second's worth of transactions in the event of a crash. 

during the startup, just after crash recovery, the Forgive me for such a crude solution since I have very little dealing with MySQL for Windows (that's not sarcasm, I am serious). ANSWER #2 Here is something more serious I just checked my MySQL 5.6.10 no-install ZIP file. It has innochecksum. 

Although you have idx_complex defined, there is one more element to the query causing the idx_complex index not be used: the COUNT(inboxentities.id) clause. You are counting something in a table. The index has no reference point to the table except to_. The MySQL Query Optmizer would choose idx_to, the simplest index. To force the MySQL Query Optimizer to choose the covering index you want (idx_complex) just COUNT from the index rather than the table. My suggestion is to change the query slightly: 

Percona has comes up with its own tool for performing online schema changes The tool is called pt-online-schema-change It involves triggers, so please read the documentation carefully. According to the Documentation, the major operations done are