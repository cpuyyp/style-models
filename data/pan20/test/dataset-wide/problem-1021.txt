Based on my understanding of what I have delved into, the composite key is only a good idea if we will always be looking up the data on all three columns always. Since we are wanting to isolate the data, I cannot foresee instances where we wouldn't want to look-up the as well as the before seeking to the . Perhaps, however, I am misunderstanding the pros and cons and am better served utilizing only for the primary key, coupled with an index against and . Is my thinking flawed? I'm still in the infant stages of the schema's development, so I'll be running plenty of performance tests with large quantities of dummy data once I get the initial sketches completed. But as a general practice, what is recommended in this scenario? Furthermore and generally encapsulating multi-tenant database architecture that must ensure high levels of data isolation, has there been any significant movement forward that doesn't lean itself towards utilizing a two-valued key combination? I have read and watched a good deal on the topic, primarily referencing Salesforce's Mulitenant Magic Webinar and Google's F1 white paper. More recent articles still tend to follow the concepts they've outlined even in their age, and while I am building a schema for a database that will not be anywhere close the scale of Salesforce and AdWords, I find myself leaning towards the principles that they have resonated. 

When I perform a basic without an explicit , however, the Execution Plan shows that it is using the index in order to sort instead of the index. 

With this backup plan in place, say we want to perform a deployment against this database after the final transaction log backup has kicked off at 6:00 P.M.. We then run various tests and fill the database with data that does not need to be retained after confirmation of a successful deployment. The deployment is successful, all tests look to be in line, and we restore back to the 6:00 PM transaction log to rid of all the data just entered. We then run a final deployment against the database. At the time of this restore, is it necessary to back the tail log up if we have no need for all the data entered following the final transaction log? I am receiving conflicting reports and dated posts/articles. Could anyone outline the pros/cons of not performing the tail log backup? Thanks! 

I am wanting to replicate an entire SQL Server 2016 on-premise instance of databases (300+) to an Azure SQL Managed Instance for redundancy of a read-replica in case of on-premise downtime. It appears as though I can use Azure Database Migration Service in order to deploy multiple databases, but this seems to imply that this is a one-time migration. This will not be a one-time migration, and I am wanting to perform transaction replication with a Publish (on-premise SQL) and Subscriber (Azure SQL). Microsoft has outlined this method, but Publications only allow for a single database to be selected. I would like to explore publishing the full instance of databases, and shipping the logs at a particular interval. Each of these databases share the same schema, so merging the data into a single database poses problems surrounding application configurations in the event we need to fail-over for read purposes. This is also a temporary solution until we are able to replicate our VMware cluster for Disaster Recovery, and our transactional throughput is on the lower side and is constrained to set working hours. Is my only option to create publications for each and every database, or can the Azure Data Migration Service be extended for replicating at regular intervals? Edit: Another option I've been toying with is creating a single database with each individual tenant as a security schema and file group, and developing an ETL pipeline to pass the tenants to these matching security schemas. In essence I would mimic the database-per-tenant through tenant-per-schema, allowing me to only need to push a single database to the Azure SQL. This might cause more headaches than it would solve, though. Does anyone have experience with this at scale? 

Within the SQL Server Import and Export Wizard, select the tick box next to Source on the page. Once selected, click . This will bring you to a dialog box that allows you to . 

This is when the error described above pops up. Based the error itself as well reading the Technet arguments, I understand the issue to be in utilizing . Since the partition action of the transaction relies on utilizing the range value within the partition function, the fails, and thus the entire transaction as well. 

How can evaluate correctly but fails? Per MSDN, should evaluate the literal value correctly and return an empty value. 

We are running two servers, a PRIMARY and REPLICA within an AlwaysOn Availability Group. The VMs are housed on Azure and have 8 logical processors, 28GB of memory, and all SSD drives. The data file layout looks like: 

After delving into Autogrowth and the best practices surrounding it (primarily being proactive and adjust the file group sizes ahead of time), there was one looming question that I did not seem to find a thorough answer to. Most articles, including TechNet, always mention the practice of setting the Autogrowth to 10-15% as a fail safe. There seems to be minimal mention of utilizing the MB growth option as a fail safe. Say for instance you have a current database size of 500GB with a file growth of approximately ~1GB/day, give or take a couple hundred MB. Each weekend, you adjust the necessary file groups by X amount to cover the growth - say boosting it to 540GB to cover nearly a month's worth of growth. In the event that you fail to adjust it once it reaches 540GB (simply forgot, unexpected table expansion, etc.), MSSQL kicks into action and attempts to grow the database by 15% but ends up on a continual time out because the growth amount is too large for the server to handle automatically. This causes application timeouts, and thus manual intervention is needed regardless. I bring this up because in testing, Autogrowth by percentage seems to offer for timeouts in these test cases. However, if I set the Autogrowth to a set MB, for instance 1,000MB, it grows without any issue and does so in a timely manner. So to summarize my question, why do articles (at least the ones that I have read) heed the practice of utilizing Autogrowth by percentage instead of Autogrowth by MB in the event proactive manual file growth has not occurred? It seems to me in testing and in theory, you'd prefer Autogrow to adjust by a set MB amount. Am I false in this thinking, or am I just reading the wrong things? It'd be great to hear what type of administrative techniques you guys utilize here. Thanks! 

Login to the vSphere Web Client for your VMware cluster, and browse to the Virtual Machine that hosts SQL Server. Your VM must be offline in order to adjust CPU and memory configurations. Within the primary pane, go to , click the button in the top right-hand corner. You will open up a context menu that has . For reference, the below image is the incorrect configuration. Note that I have set to . Given the limitations of SQL Server Standard Edition, this is a bad configuration. 

*Option 3 If I do not place the unique indexes on the partition scheme, the DDL operation succeeds, but as one would expect it places the indexes on the specified file group directly. In my mind, this defeats the entire purpose of the partition scheme as we cannot leverage the performance benefits of the partitioning column. But I may be wrong on this and would like to be corrected if so. 

You're specifying , an invalid time zone as it is a misspelling. The time zone you are looking for is . Incorrect: 

I have a table that captures the host platform that a user is running on. The table's definition is straightforward: 

I am utilizing partition schemes in order to logically isolate data based on a column. is the partition column and clustering index across all tables that require tenant isolation. I would like to create unique, non-clustered indexes on the table that also holds and . I cannot create the unique indexes on the partition scheme unless they are a part of the partitioning column as a composite primary key. This fact leads me to worry about performance implications (especially in larger contexts), but this may be a misguided thought. Even with the potential performance implications, given that the combination of , , and will always be a unique composition, I'm falling towards including the non-partitioning columns onto the clustered index. However, I would like to know if it's possible otherwise, if there are detriments to not including it in the composite key, or if it's better to avoid placing them on the partition scheme at all. For optimal context, let's set up the security schema, file group, partition function, and partition scheme to a database called . 

As an addendum to Brent Ozar's plan of action, I wanted to share the results. As Brent noted, within VMware we had configured the Virtual Machine improperly with 12 single-core CPUs. This resulted in the remaining 8 cores being inaccessible by SQL Server, and as a result, led to the memory issue described in my original question. We placed our services in maintenance mode last night in order to reconfigure the VM appropriately. Not only are we seeing the memory creep up in a normal fashion, but as Brent also hinted, the number of waits went down exponentially and our overall SQL Server performance has skyrocketed. The vNUMA configurations are now happy little components that are slicing through our workloads. For those that might be utilizing VMware vSphere 6.5, the brief steps to complete the action item described by Brent are as follows. 

I am wanting to utilize partitioning based on a (and later in conjunction with date ranges). Instead of needing to manually insert the latest value within the , I thought of creating a to pull the value and to add it to the . However, I am running into an unexpected error: 

The fix is as simple as adjusting the value. In our case, we set it to so that we have . This allows SQL Server to utilize all 12 processors. 

Maintenance plans have not changed. Each night I run an integrity check, a index rebuild/reorganization (based on fragmentation level), a statistics update, a full backup, and log backups every 15 minutes before and after working hours from 6:00 AM to 7:00 PM. As far as volume goes, Tenant A sees roughly ~10,000 orders each day. With Tenant B added, we now see a combined ~25,000 orders each day. The volume has increased dramatically. The schema does not differ between the two tenants, business logic is the same save for tenant parameterization, etc.. With all this, I am a bit perplexed how we are seeing an increase in database performance with much more volume. I am going to continue to monitor and gather statistics throughout the week, but so far, both tenants are incredibly happy with the performance of the system. The tenant that was already on the database has brought up how the application is behaving much more quickly than before. Has anyone noticed such an odd case before, or perhaps know of anything to pay mind to? 

Am I better served with option 3 or option 4, or perhaps an option 5 that I am unaware of? I would also like to think outside of the context of purely this table and carry the same general understanding onto other tables that require additional unique keys as well. For example, a table with potentially billions of rows, where in addition to there is a bigint identity column of as well as a unique . 

Yesterday we migrated a tenant (Tenant B) to a new version of our software, and this included moving them to another database where another tenant (Tenant A) has already resided. This new version only had one tenant (Tenant A) on the database until yesterday. Prior to go-live of both tenants on the same database, performance was relatively speedy with optimal query execution times. We were anticipating the added volume from the new tenant (B) to decrease performance slightly, especially since the new tenant (B) produces a much higher order volume. However, the past 48 hour period has actually showed a significant increase in database performance. 

How can I work around this? Do I need to explicitly set the value of in conjunction with ? New inserts into will be rather sporadic and minimal, but will be a constraining key across other tables. This is why I decided to investigate this implementation method so to dynamically alter the partition function. 

Option 2 If I place the DDL operations outside of the scope of the operation, it also tosses the same error. 

The outputs two rows as desired. The execution plan (PasteThePlan) shows it performing a to filter the appropriate rows. If I change the clause to utilize , I am greeted with all four possible outcomes even though I have set to return only two. 

I have a column called that I am trying to evaluate an check against to return a blank value for instead of . A snippet of the statement: 

Option 4 - Composite Key Lastly, if I include them as a composite key, the DDL operation works and places them on the partition scheme. This is the "desired" end behavior where the indexes are on the partition scheme, but I want to know if it is appropriate to include them as a composite key on the clustered index at all. 

shows no table fragmentation. What am I missing that could cause for this? I thought so long that the table was created on a clustered index, it should automatically sort by it implicitly without need to specify . What is SQL Server's preference in choosing the UQ? Is there something I need to specify in the table's creation? 

If we are performing row-level security and isolating the data based on and , is the primary key better served as above in a composite key, or separately? E.g.: 

An important note: Do not set the value to where either the or the would be an odd number. NUMA loves balance, and by rule of thumb, needs to be divisible by 2. For instance, a configuration of 4 cores to 3 sockets would be imbalanced. In fact, if you were to run with this type of configuration, it would toss a warning about this. Section 3.3 in Architecting Microsoft SQL Server on VMware vSphere (PDF warning) outlines this in detail. The practices outlined in the whitepaper are applicable to most all on-premise virtualization of SQL Server. Here are a few more resources I have compiled through my research after Brent's post: 

I have looked into INSTEAD OF INSERT but have not had any success. The trigger fires once and updates the with a value of 0 (implicitly converted from NULL). I believe this is due to the not being properly captured in the scope of the transaction. 

I have built a database project within SSDT for Visual Studio 2017. I have also created Publish profiles that succesfully deploy to our dev, QA, staging, and production databases with no issue, including proper Pre/PostDeployment scripts. The issue that I now face is that I need to "dynamically" publish the database project to 300+ databases. By dynamically, I mean that each of the 300+ databases are relatively out of sync. For instance, I have objects that I want to drop in database ABC that might not necessarily be in XYZ. I'll also have objects in XYZ that I want to drop that aren't in ABC. The objects I want to keep are all in source control, so I am using the Publish option to . There might be table definitions that slightly differ from one database to the next, especially with regards to indexes and constraints, or temporary backup tables that were created during account specific investigations that should be dropped. Because of these inconsistencies across the schemas, an approach to build the project once and deploy that build script to all databases won't work as it might miss objects that need to be dropped or recreated in database XYZ. The goal is to essentially sync all of these databases to the same schema. I have read a few articles that delve into utilizing batch scripts, but they all seem to cover the case of building the database build script once and deploying that single script across all databases. I have not yet found a method that will build and publish the project for each of the 300+ databases individually. I understand that this is likely to be a rather long deployment process, but given the inconsistencies throughout all the schemas due to mixed deployment procedures prior to my coming on with the company, we need to get the schema to a state that is consistent across the board. We are using Visual Studio Team Services for our TFS repository, so I believe there may also be an option to create a Build Definition workflow in this scope. But alas, there seems to be relatively minimal information on this use case. PowerShell calls to SQLCMD might also be another option, but I have little to zero experience with this.