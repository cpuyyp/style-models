There is no way in SQL 2000 - as it does not support any DMV's introduced in sql server 2005. Your best bet is to use an AUDIT trace or run server side trace. Note: depending on what you are capturing, it will be resource intensive and might end up filling up your disk space as well. EDIT: @rhughes Refer to this para in the link : 

As a side note, since you are using Standard edition, will not perform parallel checking of objects as opposed to Enterprise edition wherein it honors the MAXDOP setting configured for the server instance. Also, BOL has a clear writeup on CHECKDB Internal Database Snapshot 

Run the bat file that will generate the .dat files in the folder that you have specified. Run below script on the 

Determining if a Table or Stored Procedure Should Be Ported to In-Memory OLTP , Determining if a disk based Table or Stored Procedure Should Be Ported to In-Memory OLTP -- informs you about which tables in your database will benefit if ported to use In-Memory OLTP and finally After you identify a table that you would like to port to use In-Memory OLTP, you can use the memory optimization advisor to help you migrate the disk-based database table to In-Memory OLTP. 

As Aaron and wBob has mentioned, start making a leap from Profiler (server side trace) to XEvents (Erin's PASS 2014 video). As a side note, for analyzing trace data, another alternative is to use SQLNexus or ClearTrace 

When you restore the database, you have to use REPLACE option to overwrite the files with the new ones. 

Yes - when you have to restore the database during a DR scenario (rebuild the server). No - when you dont have the cert Private key password and want to restore the cert to different server, you can simply backup the cert with a new password and restore on the different server. Remember that - A DMK does not encrypt the database. It is used to encrypt the certs and other keys to keep them protected. 

Need help with below code, as it fails with truncation error Truncation error occurred. Command has been aborted. 

Backup compression uses CPU cycles to compress the data before it leaves the server, and that’s why in the vast majority of scenarios, compressed backups are faster than uncompressed backups. Note that when you use Open source tools you need to uncompress the database backup file before you can start the restore process it self. e.g: When you receive a SQL database backup of 50 Gb which is compressed to 5 GB. To restore this database, you need much more diskspace: 

Your 10 jobs will be there which will be driven by your master job. Make sure you do proper error reporting. e.g. if Job 1 fails, then just fail the entire job or go to some other step to get you notified .. something along those lines. 

Best is to use Ola's script (as David mentioned) as the Backup solution is customizable as per your need. 

Initial thought reveals that snapshot replication should be OK as you thought, but I would highly recommend to go for backup/restore method - safe, reliable and less overhead of maintaining. Note that I am not saying that snapshot replication is a bad choice, but why not use a proven and reliable method - backup restore ? You can/should automate it (backup/restore) using either Powershell or T-SQL or SSIS. Drawbacks of Snapshot replication : 

Azure SQL DB is different ! The column from (dont use <-- its deprecated) will remain the same for the database. The is a function and the value will change if the Azure DB is failed over. Best is to use <-- database name column. People have reported that here and here. 

It's Query Data Store related and you can safely ignore it. Query store is a new feature in SQL 2016. Refer to $URL$ Edit: Below confirms that I was correct - You can safely ignore/filter out this wait type. Paul Randal has commented in his Wait statistics, or please tell me where it hurts 

You have to test it out depending on your workload. Read committed Snapshot isolation (RCSI) has a tempdb overhead as it has to do a lot of row-versoning. The Database engine will store a version of the previously committed image of the row in tempdb. ON a highlevel - RCSI - selects do not lock data during a read operation and Read transactions do not block write transactions and vice versa. 

So if you are using SQL Server 2012 and up, I would suggest to migrate all your traces to equivalent Extended events (this is what we are in the process of implementing). 

Forcing affinity means that you take away sql server's ability to move processes between schedulers. Consider below example : (using coreinfo from sysinternals) On a 2 socket, 12 CPU (24 logical cpu with hyper-threading enabled) with 2 NUMA Node machine, we get 12 CPU per NUMA node. 

The Blue (Current) will have the current version of the schema/build or product and will be your "LIVE" environment. At the same time Green will be your staging/testing environment wherein you will upgrade your schema/build or product to the NEXT release, do a full regression test and get signed off by your business users. Once happy, during a cut-over period, you will promote the Green to be your "LIVE" environment and demote the Blue to be a preprod/staging or testing for the next release. 

You are punching a hole in your security by allowing an unprivileged user run as sysadmin role. If you are trying to set traceflag, which disables lock escalation based on the number of locks, you can do it on table level using e.g. Below enables lock escalation to the partition level on a partitioned table. If the table is not partitioned, lock escalation is set at the TABLE level. -- valid options are AUTO, TABLE and DISABLE Now you can just give an unpreviledge user alter table rights. HTH 

Even though you can use to copy the directory, I would not recommend to do it using SQL Server. You were right in thinking of using PowerShell and/or robocopy to copy files or directory to wherever you want. Powershell will even allow you to send email once the task is completed. Currently, in my company, we use native sql server backups with compression and then have powershell scripts kick off the move to backup server on a different schedule. 

Yes, you can do that once the backup is finished. Note: Make sure you are using T-SQL as opposed to GUI which has limited backup options exposed. 

Agree with Jon. Instead of going through the pain of setting up Maintenance Plans, I highly recommend to use Ola Hallengren's SQL Server Maintenance Solution This solution is flexible (can be adjusted as per your needs) and is supported on SQL Server 2005, SQL Server 2008, SQL Server 2008 R2, and SQL Server 2012 as well as it is widely used in SQL Server community. We have a much more complex environment and we use backup as well as maintenance solutions from Ola's site. 

Another option is to use sysinternals process utilities especially : make sure you use the filter button to filter out unwanted things :-) Process Explorer Find out what files, registry keys and other objects processes have open, which DLLs they have loaded, and more. This uniquely powerful utility will even show you who owns each process. or Process Monitor Monitor file system, Registry, process, thread and DLL activity in real-time. 

Can be used to track Object Altered, Object Created and Object Deleted along with other stuff. Refer to the link below. Refer to : The default trace in SQL Server - the power of performance and security auditing Note this is very limited as it will just give you info related to who did what e.g. Who dropped the table, etc. 

This little code will help for any table that you want to delete records from. It takes care of referential integrity as well ... Below code will generate DELETE statements .. Just specify the schema.table_Name 

I would suggest to use Restore Gene written by Paul Brewer. There is a T-SQL [] and PowerShell version [] of it and you can use it / customize it as per your needs. SQLServerCentral.com has an excellent article by the author on how to configure and describes the working of it as well - Restore Gene : Automating SQL Server Database Restores. 

Your application should use 3 part (dbname.schema.tableName). Also, make sure that Instant file initialization is enabled to cutdown the restore time. 

No .. once you recover, you cannot apply transaction logs anymore. You have to start again. What you can do is ... apply transaction log and have the db in standby using .. restore .. with Standby (available for read queries). 

You can limit the tempdb autogrowth for the primary datafile (that you have on SSD) to 30GB and then you can create additional datafiles on the SAN. The drawback will be that you will see imbalance in the tempdb datafile growth. SQL Server allocates space in a round-robin fashion. Important Trace flags when dealing with Tempdb : (helps keep data base files equally sized over their lifetime, by triggering all database files in a filegroup for autogrow at the same time.). Note that you wont need this trace flag as you will be eventually restricting the tempdb to NOT grow on SSD once you reach the threshold. (this trace flag switches allocations in tempdb from single-page at a time for the first 8 pages, to immediately allocate an extent (8 pages). It’s used to help alleviate allocation bitmap contention in tempdb under a heavy load of small temp table creation and deletion.) for tempdb. Refer to Misconceptions around TF 1118 by Paul Randal and SQL Server 2008 Trace Flag -T 1117 

This might be due to MAXDOP setting on your server instance. It has been proven that Online Index Rebuild – Can Cause Increased Fragmentation when it is allowed to run with MAX DOP > 1 and ALLOW_PAGE_LOCKS = OFF directives. If you are seeing this behavior, then its better to use () (serial index rebuild) at query level when rebuilding index. e.g.