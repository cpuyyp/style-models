If you are on a consumer IP connection, chances are port 25 is blocked in one or both directions. Some ISP block outbound, some block inbound, some block both. 

I am trying to do Kerberos-auth:d NFS4 on EC2. In order to do this, it seems one wants a kernel at least 2.6.35 in order to get decent encryption algos. The distro I could find that provides this is Ubuntu/Natty, which has 2.6.38. However, the default images are the -virtual flavour, which lacks rpcsec_gss_krb5. Thus I'm trying to make an image that boots a -generic image, but these seem not to be appreciated by the EC2 pv-grub loader: 

I have a CERC/4i on a Dell 750 with two SATA drives. Due to a failing cooling system one of the disks stopped working. This has occurred twice before, so I am fairly confident the disk is not (fatally) damaged. Previously, array rebuilding has been triggered by the following set of afacli commands: 

The previous ones to this question are all worthy answers, but they are all colored by large-scale thinking. If you have only a single host, providing web sites whose failure will not immediately kill kittens, you may want to think somewhat smaller. I would suggest the following: Use logwatch or similar system that aggregates your logs and mails you a summary. Read the summary at least every other day. Use a tool that crunches your weblogs (e.g. analog) into a readable summary. If you want to be a bit more ambitious, use a cloud service to monitor your sites. However, the essence of systems maintenance remains the same: kill each issue that occurs dead so that it can never occur again. This is an important point, because it means that there should be no "regular" maintenance. Now for the bigger scale: Among the first issues you have when you scale up is that something breaks and you don't know it. This must never occur again. Then you get a full-scale monitoring solution. Also, having seen the issue occur on one host, you want to prevent it occurring on any other host. That's when you get a configuration management system. But you must not at this point be complacent. All your effort should go into permanent solutions. 

It appears the reply for pool.ntp.org changed recently. This is making my CentosOS 6 ntp servers unhappy. 

authentication failed: authentication failure Meanwhile, there is no output from my debug-logging saslauthd. I interpret this as meaning that libsasl2 tries to uses sasldb auth rather than try to talk to saslauthd. What I can't figure out how to tell libsasl that I want it to talk to saslauthd. Various instructions inform you to create a file /etc/sasl2/smtpd.conf or /etc/postfix/sasl/smtpd.conf. I have tried creating these files containing: 

If you have a line like the next-to-last, mail should be delivered to dovecot, and you need to trace the problem there. 

Having done this, AMIs created from snapshots on this volume boots fine using pv-grub kernel aki-805ea7e9. UPDATE: Actually having the .deb installed makes apt-get very angry. Rather, just extracting it by might be the better option. 

At this point I ran into the classic $releasever issue, so I had to edit /etc/yum.repos.d/* to replace $releasever with 6. Something like: 

It is not clear from the question what freedom there is to choose implementation technology. If you can choose a distributed nosql database like Cassandra, replicating data across many nodes should be doable, under the assumption that it doesn't matter if it takes a second or two for data to propagate. I don't know about pushing system files, but you could probably push static web content with a clustered files system or even rsync. For maintaining the state of multiple machines, you may want to look into cfengine that will help you with both maintaining packages and configuration over many boxen. If you're going to the trouble, you should probably do networked syslogging for these boxen as well. 

I had a similar problem some time back. I stopped the inaccessible host and started a new instance where I attached and mounted the inaccessible host's partition. (I didn't expect this to work for EBS snapshots, but it did.) After fixing the partition, I unmounted it and started the old host back up. My problem was that the installation had been trashed, but you should be able to update SSH keys or set a new root password or whatever. All this assumes that the host will not auto-terminate on shutdown: remember to check the policy. 

I would recommend you take a look at collectd. It can be configured to log numerous measurements into RRD-files for later analysis. It requires very little CPU and will help you to understand how your performance changes with load. I have not found a truly awesome tool to actually draw graphs from the generated RRDs, but unless you want to project them in realime, just using rrdgraph on command line is typically enough to periodically check for big changes. 

I would not be so pessimistic as 99%. In some parts of Europe and US, household size is quite small, so If you want to reach singles, you may have a better chance than if you want to reach families (particularly with teenage children). Also, at least one Swedish ISP modem firmwares default to port-forward on the first connected host. But, swayage is still in order, given that the IPv4 trend points strongly towards increased NAT over time. Also note that it is very common for mobile operators to simply block inbound TCP connection attempts. If you have mobile aspirations, all TCP connection initiations need to come from the mobile end. 

If you are deploying on Linux, you could package your installation as PRM or DEB packages and give them dependencies. Then rpm/dpkg will refuse to install your application if the server is too old. There are good maven modules for creating such packages. 

Don't worry about it. The config you have is not going to eat your RAM. Note that the three www-data processes are forks, so they don't actually use that much extra memory if they are not serving data. 

Obviously, these are dangerous commands, so backup, backup, backup. Also, doing this may cause issues if someone is trying to work with files in /home while this process goes on, so please make sure no users are logged in and no public services are running. 

I want to build a server that exports user home directories over SMB/CIFS and NFS. This server would be joined to a Win2k3 AD domain controller that holds our user database. As I understand it, winbind will invent UIDs for these users on the fly. This username-UID mapping needs to be available to NFS clients that mount the home directories or file ownership will not be presented correctly. I assume this can be achieved using SFU, but as far as I can tell SFU is discontinued and will not be supported on recent versions of Windows, so I would prefer not to use it. How do I best provide this mapping to NFS clients? (You would think this a common use case, but I fail to find a relevant howto. My Google-fu may be weak.) EDIT: As an aside, would it be possible in this scenario for a user to connect over NFS without first having connected via SMB/CIFS? 

should do the trick once you get a start script into /etc/init.d. Having said that, if you install supervisor from a package and it didn't install and setup a start script, you are entitled to complain to the package maintainer. 

With the caveat that I don't know much about kvm, I would guess that the partition is a full disk image. If that is the case, you should get a meaningful partition table if you do: 

Check that you have a script /etc/init.d/supervisor or similar and that there are relevant symlinks in /etc/rc?.d. If not something like 

Typically, what you wan is a package called smartmontools. It can query the SMART interface on your disks, which is in most modern disks. There is a daemon called smartd which can help you with continuous monitoring. However, if your system is a home server, just checking manually is often better. Like so: smartctl -a /dev/sda A lot of data spews forth. The stuff that most interest me are the following: 

The variants of 2 are best for complex machines (that sort of mimics traditional physical machines with multiple roles). However, one of the big advantages with virtual machines is that you no longer need to have multi-role machines. Instead, you can have several very simple machines with exactly one role each. If you go to the extreme, the answer is none of the above. If you want to be extreme, you create a master image and then each VM gets a read/write snapshot of that master as its root partition. With this design, plus a little creative scripting using a dhcp server and cfengine/puppet, you can create and start a virtual machine in under 15 seconds.