A database server running Postgres 9.4 shows no statistics when running - well only the is non-zero. The is null. I haven't been able to find much as to what should be checked in this case, to find out why statistics aren't reported. and are both set to . Per the manual: 

Your best place would be pgfoundry. But you won't find much. Doesn't look like many plugins are available. 

Time for the backup to run is a reason for doing incremental backup on Progress. My backup is running fast enough that I did not need to use this function and I'm still doing only full backups. It also depends on your requirements. For example, if you have heavy financial transactions and you want to keep a backup every hour or something like that, you would need to do incremental (or real time). But unless you have something forcing you to do incremental, I would do full backup, I find that easier to restore. In terms of performance impact, if you have a fast disk array, I haven't seen much impact of doing a backup even during moderately heavy usage. Obviously it depends also on the size of your system. I'm talking about a 37Gb DB. 

This table would store the ids for both the previous tables and any information that is specific to that exam's instance of that question. This would allow you to write queries such as "How many questions belong to one exam" 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

If you find yourself saying "nullable columns" before you have written code I generally think you need to normalize more(as always, it depends.) If you want one system to maintain, the last option seems the most relevant, but "splitting them into two tables" is not really where you would want to take it. if we want to track basic things like Players, Games, and Sport type you would simply add intersection tables between the relevant things you want to store. 

I can't find any details as to why fails. If I manually run as the user, that works, but that does not create the config files. I checked everything I could think of, including on-line and can't find a solution. 

PID 9593 is the most problematic one which other users get blocked by this one. As far as the user is admitting to, he truncated his table, then did inserts in batches of 1,000 committing after each batches. Currently this PID shows the following locks: 

I'm trying to install PostgreSQL 9.4.1 on Ubuntu 14.04.2. I'm using the packages from . The package installs but it fails running . If I run manually, I get: 

SQL Server Agent job fails to run SSIS package, but the package runs fine manually The SQL server is SQL Server 2012. The job has been working for a long time fine. This is the first time when it failed. We haven’t changed anything. Error Message: Executed as user: Domain\SQLServiceAcct. Started: 5:36:22 AM Finished: 6:56:31 AM Elapsed: 4809.11 seconds. The package execution failed. The step failed. Information about the job: It has a proper SSIS Package Credential (we use the service account as identity) The Job runs as: Proxy SSIS Exec Package source: SSIS Package Store Log on the server: Use Windows Authentication (we use service account) Package: \MSDB\FOO_BAR\name of the job Is this something to do with the SQL Server agent? 

I'm running into an issue whereby I have two connections from a user to my PostgreSQL server which have been running for about 4 hours and have been in a commit state for quite some time (at least 1 hour that I have been watching it). These connections are blocking other queries from running but themselves aren't blocked. Here are the two connections in question. 

The simplest - shell script running psql commands :) Otherwise, any high-level language can be used for the sake of experiment to learn database - ruby, python, java, etc. 

Code generators are great! Code generators are evil! 10-15 years ago, I would have said that having a code generator for quickly creating the boiler plate code for database driven applications would have been a great gift to mankind. 5-10 years ago, I would have said code generator sucks, they generate too much duplicate code and that having a data-driven user interface (where the fields on the screen, validation, etc is driven by meta-data in a database instead of coding the screens one by one) would have been a great gift to mankind that supplanted the code generators. Today I would say - write each screen individually. Use existing framework that wire fields and model objects and possibly ORM when doing simple CRUD. But do design each screen to the exact purpose of the screen. Application screens that mirror a RDMS table too much is only good for managing lookup tables. Don't annoy the user with geeky interface that are designed against a computer storage model (RDMS)... make a screen that has only what they need. That may mean that it will save to multiple tables, etc. Who cares. The user isn't a database. So my thought? Don't waste your time making a code generator. 

As a fairly newly minted DBA under the gun, I have run the gamut of free tools and done some experimentation in the paid space (DPA, SQL Sentry, and Foglight) and it really depends on what you want the tool for. In my experience the most important thing was not just communicating performance baselines (management vastly didn't care unless there was someone to yell at), but produce something in an easy to consume format that made the priorities clear and was able to track down performance issues in production. You can absolutely build up your skills by going the free route, and the tools for SQL Server are great. 

To answer your question on those 2 options, neither seem right to me. A) will lock you in and B) is a lot of work. The current schema you describe is not too bad (except for having the information name ("first name", "square foot", etc) as string instead of an ID referenced to a lookup table. However, this seems to me like a good candidate for a NoSQL database ( $URL$ ). While I never worked with such database, what you describe is a typical scenario that this solves. 

Even more, the returns zeros in all columns and null for - even if I run a manual vacuum on the table. 

I recently implemented Ola Hallengren Maintenance solution on all of my production servers(SQL server 2005,2008,2008R2 and 2012). It works great. My problem is how to implement Ola Hallengren backup plan on the server for some databases nightly full backups and for other weekly full backup only. To be specific I have 20 databases on the server. It is required 15 databases have nightly full backup plan. The remaining five databases should have weekly database full backup. I scheduled the backup job for 15 databases nightly full backup. It is working great. But I don’t know how to take care the remaining five databases weekly full backup. (SQL Server 2008R2) On native maintenance plan I used to create two plans 1. Nightly full backup for the 15 databases and 2. Weekly full backups for the remaining five databases I don’t know how to do that on ola. Do I need to install ola Backup solution again for the five databases? Or is there another way to do it? I copy and past the script from ola User database job substitute my databases on the place of user databases and create backup job as follows sqlcmd -E -S $(ESCAPE_SQUOTE(SRVR)) -d MASTER -Q "EXECUTE [dbo].[DatabaseBackup] @Databases = 'DB1,DB2,DB3,DB3,DB4,Db5', @Directory = N'C:\Backup2', @BackupType = 'FULL', @Verify = 'Y', @CleanupTime = 72, @CheckSum = 'Y', @LogToTable = 'Y'" -b I run a test backup . It worked but I am not sure that is the right way doing it or not? could you please somebody tell me whether it is ok or not. Thank you your help in advance. @KASQLDBA I have attached the error screen shot 

This means we could count all of Michael Jordan's games by sport(as he played multiple) with a query something like: 

Yes, SQL Server can report how long it took to do any of those actions (though you may have to run it to get some additional details such as actual row counts returned) Statistics Time 

SQL Server Setup Control /INSTALLSHAREDDIR Specifies a nondefault installation directory for 64-bit shared components. Default is %Program Files%\Microsoft SQL Server Cannot be set to %Program Files(x86)%\Microsoft SQL Server /INSTALLSHAREDWOWDIR Specifies a nondefault installation directory for 32-bit shared components. Supported only on a 64-bit system. /INSTANCEDIR Specifies a nondefault installation directory for instance-specific components. 

As Zoltan pointed out, unless you have MANY millions of rows, I don't see a scaling issue. There are also many libraries for scheduling things such as Quartz on Java for example. These will store the recurring schedule as a cron-like expression. Because your example above has a flaw, if the recurrence is every Monday, then it's . So you can store a date, or a recurrence pattern. 

I simply removed the + signs and replaced with ||. On the EXECUTE I removed the format and simply concatenated the strings. 

If you are referring to the base/pgsql_tmp, then you should be fine. But I don't speak from experience of having done that myself. The only gotcha is that you have to make sure that the location is accessible when your PostgreSQL server starts up. (Ref.: Book PostgreSQL 9.0 High Performance, page 93). The book in question refers to creating a simlink of the base/pgsql_tmp on the OS partition or other less "safe" partition (i.e. non-RAID or the like). 

I dont think so except for the fact that there is a 5NF, which describes a design where your joins are only on the candidate keys. Many "4NF" designs meet this criteria, but not all, and it is definitely something you can change a 4NF "into" to be be more normalized. 

For an example of windowing functions and subqueries in T-SQL(if you assume g1 is your table and not some subquery I just invented): 

Edit:Adding a little flavor text as requested. Basically what is happening is you are choosing to aggregate one of the values (in this case the counts of the salary) so that it "rolls up", and the group by generally indicates which value you want to do the rolling up by. It makes some sense to say "I want to group by the number of employees" but you are actually trying to express "I want to return the number of employees grouped by department."