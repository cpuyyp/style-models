will return even if someone wins on the last move. You should do the empty check after you check for any wins. Your comments in has twice. The second should be 

I make like in the previous example. In cases where is perfect and is good, I can just write to at the point where ends. In the cases where is bad and is food, I can just write from the start of . In the remaining cases, I have to calculate where starts, and then write . 3. 

In you are using a while loop and recursion. I would use one or the other. In your case, if you get a I would print a message then set , then just continue the loop as-is. In if you set the default to , and combine that with my previous suggestion, you can get rid of the test. isn't a function, it doesn't need parentheses. I would make the low and high limits arguments in all cases, which default to using the global values. I would make default to . Needlessly mixing types makes the code more brittle. It works now, but changes later can inadvertently introduce bugs. In , you can flatten the test to a single level. First check if the guess is correct, then check if is right, then check if is right, then check if the numbers are consecutive, then the guess is high, then check if the guess is low. The loop should terminate, or better yet raise an exception, if there is a nonsensical result. It currently just keeps running. The test should probably just run forever, and out of it when it is done. You should only run if . In other words, only run automatically if the file is run as a script. 

This will check if is even and, if so, if there is exactly one value where , if so remove from the results. 

For I can loop over them an put the values at the end. This allows me to treat all rows as either "perfect" or "bad", since the "imperfect" rows already have the values at the end and thus can be used in the entirety. For , I can loop over the rows where are imperfect and are not good and put at the end of those. Where is good, will be ovewriting those anyway so it isn't necessary as long as is at least as long as . 

The problem with your case is that this is in-between, they are simple enough that you can follow the flow easily in an test, but they are not all so simple that they can be put into lambdas without sacrificing clarity (it is possible to put all but one in lambdas, but half would be really hard to read). Further, there are not so many situations that it becomes hard to read in an chain (although it is getting close), and there are enough arguments (or enough inconsistency in the arguments) that it is getting to the point where I would not use the approach even if all the other criteria were satisfied (although again it is kind of borderline). Ultimately, what it comes down to is whether the approach helps or hurts readability. In your case, I think it hurts it. If the criteria I described above hold, then it will probably help it (although you really need to look on a case-by-case basis, since there will be corner cases either way). 

Since you are already using numpy, you can use numpy's function to read in all the data at once as numpy arrays from the start. This allows you to avoid having to worry about opening or closing files (this is done automatically), converting to numpy arrays, etc. Then it is a simple matter of converting the indexes to values. You can also vectorize the test data creation, using numpy's function to get a grid of corresponding X and Y coordinates. You can make the plotting better, in my opinion at least, by using to get a figure and axes object right at the beginning, then using those to do the plotting. So here is how I would do it: 

See that index of the last dimension has all the odd columns, while dimension has all the even columns. Now let's do a in this case instead of a . The semantics are the same, but it is easier to follow in my opinion. We will take the of dimension 1, which is what used to be columns, but now is the columns split into even and odd: 

Follow the pep8 style guide. You should use so you don't have to do all the multiplication over and over. You can use to reduce it to only one loop. You don't close the file. You should use to open and close the file. You can use (or for python2 ) to get both the first and second index at the same time. You can simplify the whole thing using and a generator expressions. You can use to simplify the printing. 

You can't make both comprehensions with this structure because changes to one would be reflected in the other. It is possible to make both comprehensions using , but you don't gain any space complexity (at least for matrices where and are similar), and it hurts your performance. If you can use numpy, this can be simplified enormously: 

You don't need . You can just iterate over files. You are leaving your files open. You should open them with , this will close them automatically when you are doing with them. You initialize outside the loop, but then immediately create a new at the beginning of the loop, so it is never used. You pick a random integer, then find if any values match it. This requires you to do one search for each part, even if there is nothing that matches. I think it would be better to pick one value from one list, then find all the values from the other list that match that length. Better yet, I think it would be better to pre-compute the values based on length, then pick a random length, then pick a random value based on that length. Your s are somewhat redundant, you can simplify them somewhat. Your loading code is also somewhat redundant. 

So, using your code, assuming we need a buffer in and out, this would be a much more efficient approach: 

The big issue I can see is that you search every element. You don't need to do this, you only need to check if is anywhere in that row or column. You can use the operation to test this. It will short-circuit after the first is found, avoiding having to search the entire row or column. This won't reduce the time complexity, but will improve the best-case performance considerably. Second, you can use to switch between rows and columns. Third, you can reduce that check to a simple list comprehension, generator expression, or generator function. Fourth, you can use to detect if there are any zeros in a given sequence. This is slightly faster than . Finally, since you are making the changes in-place, you don't need to return the modified matrix. So here is my version: 

Follow the pep8 style guide. In , you should slice initially, rather than popping. So . In , you should convert to float in the initial list comprehension. In , you should convert to a single numpy array, then slice that to get X and Y. In , you don't need to wrap the in In , don't put multiple operations on a single line like that. In , you should do the splitting once, then get the items from that. In , you should use string replacement. So, for example, . Or better yet you can define a string at the beginning, then apply the format to it in each case, so and . Use to automatically open and close files when you are done with them. When looping over a file object, you don't need . Just do, for example, . That will automatically loop over all the lines in the file. In , you should test for a slice of a string. So In the loop of , it would be better to define a string in the block, and then after the block write that string. Never, ever, under any circumstances call . If you need to exit, either allow the program to exit normally or throw an exception. In your case, raise an exception. You should put the argument parsing in a function. In , you should at the end of the block, which will allow you to avoid the case. Or better yet, do , since will work in both cases. In , you should do (which is miss-spelled, by the way). In , the list comprehension should be a generator expression. This will allow you to avoid having to store all the filenames. 

So here is how I would write it (ignoring the functions part). I am putting intermediate values inside one file, and then saving the final result to a file at the very end: 

In , I would use a ternary expression: can be made even more efficient by having return if the loop finished. Then you just divide the result of that by 100. This will result in 1. if the loop exits, avoiding the test entirely. I would only do run in your loop, and store the result of that function to a 2D numpy array. Then you can vectorize the rest of the calculation, since it is all just basic math. This should substantially increase performance. You can even move the outside the for loop to further improve performance. If the previous suggestion does not increase performance enough, you might be able use to further increase the performance of the loop. For an even more extreme vectorization, you can do all your calculations on all pixels at once. Rather than having a function, just use . Follow pep8 I would put the current contents of the block in a function and just call that function inside the block. In I would allow the code to pass a string (which defaults to , and use to dynamically call method with that name. I would rename to . I would put in an argument for that lets the user change . Similarly, I would put an argument in that lets the user change to something else. should accept that are then passed directly to the or method. I would move the and lambdas into their own methods. Or better yet, I would refactor so you just pass the and argument. In , I would let the user set the scale with an argument. The argument would default to . If it is , it would be computed automatically as is done now. In , you only ever work with integers. So I would use in to make sure it returns an integer. This allows you to avoid the later integer conversions. 

Finally, this is a bit too simple so it may not be allowed, but you can take the first elements of the sorted version of the tuple: 

Follow pep8. Your code is pretty good already, but your naming in particular is not correct. Although I can see the advantage of grouping everything into a class, the individual file parsers are not directly dependent on the class. I would split those out into their own functions. This will make testing in particular easier. You can then have wrapper methods in the class that call the parser functions. You are loading every file completely into a list. This takes a lot of memory for large files. Worse, it requires parsing the entire list twice, once for the header and once for the body. This is likely a big source of your performance issues. It would be much more memory-efficient to iterate over the lines in the file. I would recommend turning the reader into a generate that takes an iterator (which will usually be the file, but could be arbitrary lists of strings for testing), does the stripping, yields the stripped line, and skips empty lines. This has the added advantage that it will keep track of your progress, so you don't need to go back and read through the header again when you want to parse the body. If you use a generator, you can create a for loop that runs until it reaches the part you want, then breaks, then you can have a second for loop that picks up where the first left off. This will greatly reduce the number of tests you have to do. You are parsing the list of numbers yourself. Don't, numpy has a function that can parse a string to a numpy array for you. It is much faster than your approach. This is also likely a major source of performance issues. You should always use for opening files. This safely closes them even when there is an error. In the default Python version, files are closed automatically when the function exits, but that doesn't necessarily happen in other Python implementations like Pypy. It is much safer to use to close the files exactly when you intend to. You can use tuple unpacking to split the lines for your dicts. So . You create a class, but then parse everything into a single that holds everything. That defeats the purpose of having a class. You should either parse the components into class attributes, or you should just use functions and return a single . You hard-code the file names. I would make the file names arguments, with the default argument being the default name. is a directory, not a repetition. The repetition may be in the directory name, but there is no reason it has to be. So this is stylistic, but I would call it or something. There is no reason to mentally limit how you organize your files like that. You make all the parsers subfunctions of . This again defeats the purpose of having a class. They should be methods. Your classes should derive from . 

I find all the rows of and with some non-null values ("good" rows). Rows that aren't good are then filled with . I find all the rows of and where the last value is non-null ("perfect" rows). For rows that are good but not perfect ("imperfect" rows): 

Assigning a numpy array to a numpy array is faster than assigning a list to a numpy array. So is faster than . It is a little faster, and in my opinion cleaner, to use for numpy arrays instead of (or in your case versus . You can use in-place operations more. So instead of . In my opinion it would be cleaner to have two loops, one for , the other for . This will save you an indentation level, and a comparison per loop. You add, then immediately remove, from . This seems redundant. I would use another variable there. You re-do some of the math several times. I think it would be better to switch variables around. You can tell what all the idx values will be ahead of time, so you can pre-compute them. You can pre-compute the range and re-use it. 

My observations, in terms of implementing this in Python (without regards to improve the algorithm, that will be later): 

What this does is first filter out values that are greater than 1/2 the target value, since one number in each pair must be that way. Then it subtracts the remaining numbers from the target (in this case ). This gets the other value from the pair. Then it uses set logic to extract only those values where the other value is present in the original list of numbers. So to put it more briefly, it finds all values such that is also present in the list. Edit: If you don't want to include cases where both members of the pair are equal and it only exists once, such as and , as Gareth pointed out, add this to the end: 

I think you could simplify the arguments a lot by using kwargs, rather than passing a tuple of arguments. Having your dict in the global namespace, rather than the function's local namespace, will slow it down considerably. is , so can just be . For unused variables, like , I consider it better to mark it as such by using as the variable name. Half of the functions are simple one-liners that can use lambdas. Although half of the functions can be lambdas, some of them would work better by doing in-place operations on . If you want to search until the end, you don't need to specify an end to . You add to in every case, so that can be moved out of the functions. 

If I am understanding what you want, and you are already importing pandas, this can be done very easily: 

The second approach would be to roll your own generator-based solution, using the statement. This can then be converted to a tuple in the same way. You need an inner function to handle the generator, and an outer function to convert it to a tuple: 

Follow the pep8 style guide. When doing numerical code always have . Don't put multiple commands on one line. So, for example, for your kwargs, each should be on a separate line. Rather than using and getting the values out of the dict, you should define specific keyword arguments and give them default values. In cases where the default value needs to be computed at runtime (such as ) you can set the default as then test if it is at the beginning of the function. This is both simpler for you and much, much easier for people wanting to use your library. I would have and in every function which are passed unchanged to the matplotlib plotting funcion. This is a good way to keep your code simple while still allowing access to the more advanced capabilities of the library you are using. Your plot setup code and plot formatting code are pretty consistent across function. You can split those out into their own functions to reduce code duplication. I would split the fitting code (currently in the section) into their own functions, then access them using a . If I were writing a library, I would split the fitting bits (which don't require matplotlib) into their own python file, and only keep the plotting-specific bits in this file. Also, if I was writing a library, I would have an optional argument for each plotting function that lets you pass an axes object. If that happens, then the figure creation, , and parts aren't called. This allows you to use these functions with subplots or make additional formatting changes before showing it, or just save the figure to a file without showing it at all. If I was writing a library, I would also make the face color a keyword argument with being the default value. I would probably abstract the scatterplot bits of and into a function. With the ability mentioned above to pass an axes object to the plotting functions, your and would be able to create an axes, pass it to the function for plotting the scatterplot, then do their additional stuff with the axes afterwards. This code won't do anything when run as a script so it doesn't need a shebang. I would document the first three functions.