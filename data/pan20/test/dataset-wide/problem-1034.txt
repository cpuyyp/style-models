is only needed because created_at is not unique. If is not the first column, MySQL will have to read all rows and copy them to a temporary table (which could be in-memory or on-disk) to sort them. If you decide to use the id, just keep the above snippets, but replace with . 

So the question becomes, which use cases don't suffer because of the cons and take advantage of the pro's? Well, a first obvious generic answer is, avoid all use cases where data loss is a problem. More specifically, you could consider MEMORY for: 

No, you cannot alter an index. Because, even if PostgreSQL allowed this, it would need to rebuild the index anyway. Unless it implemented very smart checks on the clause, which would serve quite rare use cases. Having many indexes is also not optimal. Every time you run a query, the planner has to consider all existing indexes and pick up the best one. If you have several indexes, the planner takes more time. Not ages - but still, this delay affects each and every query involving that table. My suggestion is... be patient and build a big index. You can use to avoid locks. 

But in general, B+Tree is considered superior. How much? I don't know, but surely not orders of magnitude. 

Of course you will be able to obtain the same information with multiple queries, or with . But it seems to me less clean and, should you ever need to add or remove a table, or change the usage of (from 20,25,30 to 20,30,40), you'll need to make much more changes. There are even more obvious case when the "one table" solution is better: maybe one day you'll need a query which returns people whose age is less than 23. Flexible designs are usually better. For specific use cases, you can always create views, if really needed. 

In this way, your buffer pool will be (partially) written to disk on shutdown and reloaded at startup, so your queries should be fast even after a restart. 

If the data you are talking about is big, of course having a different table for each customer will make your queries faster, because both data and indexes will be smaller. And - assuming that you use - you will have other advantages concerning operations, for example if one table gets corrupted InnoDB will need to repair a smaller quantity of data. Another solution, probably a bit cleaner, is using partitioning. You can physically partition a table into more files, so the effect is the same: the portion of data and indexes you query will be smaller. You can partition by customer id. Some advantages are: 

A quick look to the syntax will answer your question: accepts a list of tables, so it won't lock tables created after the lock has been acquired. acquires a global lock, which affects of course . I understand that you don't want to use it precisely because the lock is global; but is meant for copying all tables, is not. There is also another risk: the connection holding the lock could die, realising the lock. This risk, of course, also exists in the case of . However, you also stated that you can stop writes to the databases you want to copy. If you can do this reliably, you don't really need to lock tables. You can simply: 

I'll assume that the last 2 queries have the same execution plan (something you can check with ). But I suggest you check if the execution plan is optimal, which means, if an index is used for both the and the clauses. The answer to your question is simple: cache. Data that are read from time to time are probably stored in InnoDB buffer pool, others aren't. Check if you can increase innodb_buffer_pool_size - the general recommendation is, 80% of total memory. 

Replication cannot work in your case, because developers databases will often change, and conflicts with the master break replication (by design). Also, developers cannot rely on data that are constantly changing. You need to take some form of backup and send it to developers machines. Here are the best solutions in my opinion: 

The obvious answer is: . There is no optimisation for this, it just counts all entries in the primary key. Of course anything requiring a full table scan or a full index scan will be really slow. Optimised queries should not be slow. Slower? Yes, because indexes will be quite big. You can always partition the table, and your queries will read smaller portions of indexes (but don't expect any type of parallelisation). Of course to be sure about the performance you should run a test. If you have the applications queries written in a binary log or slow log, replay the log with some tool or a custom script. If you don't have, write some realistic queries and make a test with sysbench (some trivial lines of lua language are required). 

Only one primary key per table is permitted. But a primary key can consist of any number of columns. Having many columns in a primary key however is often (not always) a bad choice. Some notes about this... 

Because the query has finished and InnoDB doesn't know anymore what query it was. does not guarantee to give you this information. However you have the thread id and you know at what time the deadlock was detected. You can use to check which queries that thread executed at that time or before that. Even if you use the ROW format (recommended), you will still see useful information. Or, maybe you have the slow log enabled. You probably have that query logged - especially if you have , which I recommend. 

The order columns is important. If you reverse it, the index will not be useful. Now you just need to run a query like this: 

I think that is because you don't assign it a value. And after you increment it, nothing changes because . Also your UPDATE will not work if is not 0. Unrelated: I suggest to use a foreign key instead of . 

You could in theory, but MySQL doesn't allow the trigger to query the same table. In other words you should create a trigger for table , and the trigger should run a on , but this is not possible, you would get an error. An alternative is to add a column to table, which contains the number of students. The initial value should be 0, then you could update that value with a trigger on the table . There should also be a trigger and - even if apparently it doesn't make sense - it's better to have a trigger , which checks if you are changing the class of a member. 

You should edit the original question to add information, instead of replying yourself. However, if these are really the steps you followed: 

As another answer highlighted, Query Cache is not the only cache. Its use is not even advisable in most cases - in fact, it was removed in MySQL 8.0. The reason is that it has scalability problems (it's governed by a global lock) and it invalidates data far too frequently to be useful in a normal workload. But InnoDB buffer pool contains indexes and data accessed frequently. After your first query, some indexes and data are cached, so next time they will be read from memory. Probably some data/index pages are accessed only once per query, in which case you should be able to see a slighter difference between second and third execution (first time these pages are not cached). How to avoid the difference between query execution times? Well, there is no way to make the query faster the very first time it runs, as it needs to read from disk. But then, if your buffer pool is big enough, your query will always be fast. Keep in mind that a big buffer pool is very important for MySQL performance. The general recommendation is to keep it 75-80% of total memory. But in reality, things are more complex: 

InnoDB uses B+Tree indexes, not B-Tree. All details about InnoDB data structures can be found here. You may also want to look at these diagrams. The author of both the resources, Jeremy Cole, was head of MySQL team at Google. Why is the syntax instead of ? This question should be posed to some MySQL or MariaDB engineer, but I see at least two possible reasons: 

I don't think that helps with free space reclaiming. It will only update index statistic. You could run without , I think it will reclaim space immediately if the table is empty (otherwise it will just update the free space map). But if you completely remove old rows, will be much better: it will reclaim disk space immediately. 

Why does the documentation state that InnoDB uses B-Tree? Well, not all MySQL users are supposed to know what B+Tree is. This may be an oversimplification, but in that context it seems to me acceptable. You wrote that you know the difference between B-Tree and B+Tree. Than the different performance characteristics should be clear: 

I've seen ALTER TABLEs running for days. The problem is not a timeout, the problem is that if the query fails at some point, you'll have to restart it from scratch. But what is your use case exactly? The server will continue to accept queries from the applications? If not, your idea is ok, I see no caveats. Otherwise, use . It is designed for this purpose, and it avoids to overload a server. However: don't disable the binlog! You will break incremental backups for no reason! Just run . Instead, if the server doesn't need to accepts connections from applications, and the table is InnoDB, you can restart it with . The operation will be much faster. But then remember (it's vital) to restart it again, without that option. 

...note that you should do them in the opposite order. It makes no sense to get the coordinates from a running master, those coordinates have no use for you. Note down the coordinates before restarting the master: those coordinates describe the state of initial slave state (the files you scp to the slave). 

Tables with the same structure but different sets of rows are sometimes called orthogonal tables. I don't believe in universal truths in the database world (which means: your use case could have peculiarities that I am not considering), but in general I consider orthogonal tables a bad idea. From a theoretic perspective I can tell you that I especially don't like using metadata (table names) as data (age range). But what does it mean? Tables designed in this way prevent you from running easily multi-table queries, like these: 

gh-ost is very similar, the main difference is that it creates no triggers, and uses the binary log instead to detect the changes to the original table. To answer your question more directly - yes, it is possible to add a column on the slave. And most probably it will simply work, as long as it has a default value. But it is very likely that in the future you will regret this choice, because you'll need to do something that will break replication. I recommend to sure to check the documentation and fully understand the dangers, before doing such a thing. 

Why does show ? I'm not sure. But technically, whenever you don't use a secondary index, you always use the primary key - because data are stored in the primary key, just not in a particular order. 

I will answer what you asked, but first let me tell you that I don't understand why you want to do that. An autoincremental id is very good for this task. But it is correct to also use a timestamp column, because it is a bad practice to rely on an id for sorting. Why? Because there are cases when its order might not be chronological - for example, if you use Galera cluster and you have failovers. To do what you asked, first create this index: 

First of all, I discourage you from using the slow log with output, in production. The reason is that writes are locking, which limits the concurrency of your workload. Moreover, CSV stored engine does not support indexes. Any non-trivial query will be very slow, so I don't see any advantages in doing this. Of course you could use MyISAM and add indexes, but then writes will become more expensive. That said, the correct way to "rotate" the table is copying it and than truncate it. You will probably lose some queries every time: the ones ran after the copy but before the statement. 

Disadvantages can depend on your specific use case. For example, I cannot know if, for some obscure reason, you want to be able to add a column only for a set of customers. The only "standard" disadvantage that I have on the top of my mind is that you won't be able to store some partitions on a different media. But there are reasons not to do that (for example, backups manageability). Also, MySQL never parallelise queries - so, in this respect, there is no difference between using partitioning or using separate tables. 

This will do what you asked. But in your examples "domain.com" is always at the end of the string. To look for strings ending with a certain substring: 

Nice question. My objection to your design is: you choose to consider user input and predefined input as the same thing (the only difference being the value of ). But they are not, in my opinion. Because predefined input are different options, but different user inputs could indicate the same idea written with synonyms, wrong spellings, etc, or could be just jokes or junk characters. You say that this saves you from having empty strings or NULLs. But I believe that the absence of a value (custom text) is a valid use for an empty string. 

Walter Mitty's answer is good, but I'd like to add something. If the column contains unique values, the surrogate key (the column) can be added only for performance reasons. But this makes little sense if the other column is small (integer, date...) and the DBMS does not organise tables by primary key (like InnoDB). Or it does, but the table is almost read-only. For example, even performance considerations advice against adding a surrogate key, if the other column is something like , or . On a more theoretical note: an entity can even have zero attributes. Since all tuples and tables are unique by definition, in relational algebra exactly 2 tables exist with 0 columns: one is empty, one has 1 tuple with 0 columns. But in practice, I don't know if such tables can be created in an existing RDBMS, and I don't know if SQL standard allows this.