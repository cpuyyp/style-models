The meaning of those two columns is pretty clear, is an Id of the user that the record belongs to, is an auto-incrementing index of the record. Other columns are not important, but they exist and affect execution plans. The table contains ~40.000.000 rows and ~200.000 distinct values, so each user has 200 records in average. Rows are never updated, we use only INSERT and DELETE to modify data. Our application executes the following query against this table: 

We have 3 instances of MongoDB configured as a replica set. Journaling is enabled, the storage engine is WiredTiger. Most of the time our application produces a very low load on the database server and it works well, but rarely we launch a background maintenance causing x10...x100 load increase, mostly due to operations: ~10000 inserts/sec. Today during the maintenance we noticed that the response time of our application increased every 1 minute for a short time. 

I used , , and understood that the server put a single plan for this query into the cache. So, if the query with the corresponding value of came first, the plan using would be stored in the plan cache and then would be used for all the consequent queries. It leads to an excessive number of logical reads (x10000) and unacceptable execution time for queries which should be executed using the second plan ( + ). Also I noticed that the threshold where the server switches between plans (the tipping point) depends on statistics, if it's stale, the plan can be sub-optimal even regardless the cache, because the server incorrectly estimates the number of rows with greater than the given one. In addition, we have a large set of similar tables with similar use cases and all of them have the same issue. What can I do to solve this issue? I could try to use , which is not easy with Linq-To-Sql, but it also doesn't look really optimal in performance terms. Also I could use or hack Linq-To-Sql even more and try to clause to force using the second plan, but as I said there is a lot of tables with the same core structure, so this way looks like a lot of manual work. Generally, my questions: Can SQL Server understand that the plan stored in the cache is not optimal for given parameters and don't use it? Are there some best practices of handling parametrized queries if their optimal execution plans depend on parameters? 

Our application uses SQL Server 2014 and we got an issue related to the plan cache. We have a parametrized query and its execution plan depends on parameter values. The server caches an execution plan which is not optimal in some cases and then uses it for all consequent queries. Details: We have a table consists of the following columns: 

Firstly, all IO activity metrics dropped to zeroes (that I can't explain at all), then we see a bunch of writes increasing up to 20, and then all metrics returned to typical values. So, I guess the performance degradation we experienced was related to the WiredTiger Checkpoint writing process which happens every 1 minute (docs). It looks that it takes a reasonable amount of time writing this amount of data, but why doesn't the MongoDB handle requests during this process? I thought it should happen in the background and shouldn't affect currently executing operations. 

Thank you all for answers. Here is what I have done to estimate the restore time. We can't afford currently a real test scenario of restoring the 2 TB backup and I don't know yet the final configuration for the migration. It would be close to what we have now. And it will be in Alibaba Cloud. We will be using Classic Virtual machines (ECS). I tested 2 smaller backup restores on the 4'th node in 4 node SQL cluster which is identical in configuration with the Production SQL Server instance. The restores revealed an increase in restore time of 20% to 50% from the backup time. Taking the 50% and applying to the 720 minutes backup time - I got a restore time of 1080 minutes (18 hours). I also tested the case with the backups for both tested databases split to 5 files and the restore time didn't decrease at all. It was very close to the one backup file restore case. 

Have you tried executing this, both of the scripts? Because this is not a correct syntax.I will assume that the first script intended to be something like: 

Looks like the disk subsystem on Server B is performing worse than on Server A but, I used to see disk issues not entirely related to disk specs. You could collect some other performance counters such as physical disk --> avg disk sec write (> 25 ms very slow), memory --> page file usage (> 70% bad), cpu --> processor queue length (> 12 very bad), memory --> pages/sec(> 600 slow, > 2500 very slow disk subsystem), sql server: buffer manager --> page life expectancy (< 300 memory issue). You can also limit the growth of 6 tempdb data files to the size they have now. See what happens. You should check the waitstats as well on both servers. If you see a lot of PAGELATCH_XX then you will know where to dig more. Jonathan Kehayias has a good article on this theme. And Paul Randal has also a lot of analysis which could help. 

once the filter is activated, slave server will skip all the statements which meet the given pattern. but before doing any change read how MySQL handles replication rules because you can skip important tables if you don't understand the rules. 

I remember when working with clusters that the port change from dynamic to a static one doesn't take effect if you don't disable the checkpointing of the quorum. Here are more details: How to Change the Dynamic Port of the SQL Server Named Instance to an Static Port in a SQL Server 2005 Cluster Also, applications managers should be notified about the port change because there are places where this can be used. As well, the port of SQL Server Instance is used in Firewall settings in many types of network configurations. 

It matters only related to disk space and character length. Of course search on char data types and indexes on these type of data will act slower than integer but this is another discussion. Varchar data type is a "variable" data type so if you set up a limit of varchar (500) than this is the maximum character length for that field. Minimum length can be between 0 and 500. On the other hand the disk space claimed will be different for 10, 30 or 500 character fields. I did sometimes a test for data type varchar (800) and for null values I had 17 bytes used, and for each character inserted it added one more byte. For example a 400 character string had 417 bytes used on the disk.