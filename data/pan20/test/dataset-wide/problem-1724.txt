As you all probably know, ipv4 route cache has been removed in 3.6 Linux kernel series, which had serious impact on multipath routing. IPv4 routing code (unlike IPv6 one) selects next hop in a round-robin fashion, so packets from given source IP to given destination IP don't always go via the same next hop. Before 3.6 the routing cache was correcting that situation, as next hop, once selected, was staying in the cache, and all further packets from the same source to the same destination were going through that next hop. Now next hop is re-selected for each packet, which leads to strange things: with 2 equal cost default routes in the routing table, each pointing to one internet provider, I can't even establish TCP connection, because initial SYN and final ACK go via different routes, and because of NAT on each path they arrive to destination as packets from different sources. Is there any relatively easy way to restore normal behaviour of multipath routing, so that next hop is selected per-flow rather than per-packet? Are there patches around to make IPv4 next hop selection hash-based, like it is for IPv6? Or how do you all deal with it? 

The ssh command will run only if the previous one has finished its operation. If you need to know more, see command. 

8) Show the status of slave Check for a specific error code and update your answer to provide more details. if possible 

To start with, you should know your systems abilities before you configure the server as a shared hosting server. The key components are, Network(UP/DOWN), CPU, Memory and disk space. As a side note, do not use LAMP server. Instead use a package manager provided by your Operating System and install these services one by one. Secondly, if you are configuring the server to host multiple websites from the same IP address you should be reading about vhosts (Virtual Hosts). Hope this puts you in the right track. 

Since you are using ssh as port name it is going to search on /etc/services file and map the name to the associated port which is You can simply modify the file and change port to whatever you like. Save changes and you are done! 

and the target server URL of my reports project is set to $URL$ (like the according virtual directory in the rsreportserver.config) I'll gladly post anything else you need, just tell. Any help is greatly appreciated! *This is a crosspost from SQLServerCentral.com 

We have 2 Apache webserver behind a load balancer wich are connected to 2 (JBoss) application servers via mod ajp. To those webservers, mobile devices connect via a REST API. In our performance test we rather quickly ran into a lot of NonHttpResponse: errors which we identified to be coming from mod_reqtimeout: 

so far, so good. Now I want to have my external VIP (XXX.XXX.XXX.XXX) as a source IP on external sites. So when I run like curl ipinfo.io/ip from one of my hosts in the network, my VIP should be returned. At the moment, I get a physical IP of one gateway (ZZZ.ZZZ.ZZZ.ZZZ). Is this possible? How? Could I just set the default gw on the gateway to the VIP? keepalived config 

Here, the line that starts with is treated as a single line and not part of directive which is causing the error. You could simply fix it using a backslash as follows: 

I'm trying to configure a cisco 850 router but I'm unable to ping the outside world from the Vlan1. looks like follow 

In case you don't get the desired result, figure out what parameteres you need to modify in the ping command. Hope it helps! 

First of all you need to avoid using expr. It's rarely used. Instead I've here the version that does exact the same. Not much different than the above: 

My assumption is that the server is connected on a switch port that is blocking somehow clients from sending a . But then, how are the clients allowed to send a and the server replies with a I've done a lot of debugging but unable to find a solution. The clients have a in the . For example: 

pinging the returned IP has 100% packet loss. I also did various traceroutes, here's the one to Google: 

But this can't be the solution, since with some more simultaneous users the problem occured again (There is a requirement to be able to serve 300 concurrent users - 100 worked quite ok, with 300 we had over 10,000 of those Request header read timeout errors). I suspect it's the interaction of apaches KeepAlive, our mod_ajp configuration and mod_reqtimeout that leads mod_reqtimeout to the conclusion that there is a slowloris attack ongoing (to many open connections that do nothing) and I kindly ask for your help in tweaking those parameters. Additional problem is a firewall between webserver and application server, which I suspect to kill open idle connections. I read about deactivating KeepAlive completely to solve this, but as I said, all our clients are mobile devices, so that's probably not an option (?). Here are the other configs (parts of): workers.properties: 

Today I migrated our dhcp and dns server from an old centos5 to a new machine running Scientific Linux 6.8. The server seems to work correctly, however the dhcp server is having some issues. The client do not get an IP address. Now I've done the necessary research to see what's going on and what's causing this. Current situation On Scientific Linux 6.8, the following packages are installed: 

See how creates a backup file automatically to protect us from overriding the file by accident. See lin $URL$ for examples using the editor. 

You are using the wrong answer to create custom services. To start with, custom service files should not be placed within rather you'd use . Secondly, after creating the service file you'd run to notify systemd about your newly created file. Make sure you've restored SELinux file context using After the update, the answer is obvious. 

If your replication fails, you better can start over again, instead of troubleshooting the problem. 1) On the Primary MySQL server 2) Dump all databases 3) On Primary MySQL server // Not necessarily to be CAPS. 4) your file to the Secondary MySQL server. 5) on the secondary MySQL server. 6) On Secondary MySQL server import dump.sql.gz as follo: 

I can't think of any tool that would do that out of the box. This is quite rare scenario, as you can't create correct two-way NAT mapping if you only change port. Do you really need just one-way traffic ? However you can always write your own netfilter module (it's not that difficult) and alter packet headers in any way you want. 

Well, things are a bit more complex. Modern hard drives don't just detect errors, they have some spare sectors and smart controllers that try to relocate bad sectors. That is, when you try to read some logical sector and it doesn't read at first time, the controller tries to read it several times, and sometimes it can read it after some retries; then it writes the data back to the spare sector, remaps logical sector to the new one and marks old sector as bad, and finally gives you your data. All those processes are completely transparent to the reader, you wouldn't notice any error. However this will normally be reflected in S.M.A.R.T statistics, and if this happens more and more often, you can see that the drive is going to fail before it actually fails. That's why it's really important to use SMART monitoring tools on your system. When a sector doesn't read at all, or the controler runs out of spare sectors, read error will be returned by the drive. Error detection is now pretty bulletproof, it uses some kind of CRC for sector data. When read error is returned, mdadm will see it, mark the drive as unusable and switch an array into degraded mode. 

I want to achieve the following: There is one DMZ-proxy over which we want to access multiple application-servers, represented by mod_jk workers. The web-context needs to be the same on all application servers, since they run the same application. We just want to access different servers via a additional "folder" within the URL. e.g.: some-fancy-url.com/hh1/some/application/ --> some-fancy-url.com/it1/some/application/ --> So depending on that keyword between URI and context we want to forward the request to different workers; the application server needs to receive the request without this keyword though. How can we achieve this? mod_rewrite I suppose? 

I have a really strage phenomenon in one of our data centers. I can ping facebook.com, yahoo.com, deb.nodesource.com and a few others (DNS resolution works fine). But I can't ping google.com, archive.ubuntu.com and lots more. DNS works here, too. I can also ping 1.1.1.1 (Cloudflare DNS) but not 8.8.8.8 (Google DNS). Firewall rules look ok to me - there are no recent changes and it worked before. LAN hosts have an allow all rule anyway. I am also connected to the DC via VPN, which works great, too. So connectivity wise I should be fine. Help me out here guys, I don't know where to look anymore... How can I debug this any further? edit: I did a dig @1.1.1.1 for google.com: root@dhcp01:[~]: dig google.com @1.1.1.1 

I've found the answer and it was quite funny - ARP table overflow. The traffic in the test environment was gerenated from many IPs that resided in directly-connected networks, so the system had to use ARP first to figure out MACs, and the default hard limit of ARP table in Linux is just 1024 entries, which gives number of connections between networks connected to 2 different interfaces close to 512. When I increased net.ipv4.neigh.gc_thresh1 and also .gc_thresh2 and .gc_thresh3, the problem was solved. 

If you create backup using tar, it will be enough to copy all your files, however the problem will arise when you try to extract that archive into a live system. Some binaries or libraries may not update because they will be in use by the system, and you will end up in a mess. You should get some support from your hosting provider (for the new VPS). Usually they have some features for that case, for instance my hosting provider allows me to switch my VPS to "repair mode", in that mode new VPS is created with basic Linux software, and my VPS's disk is mounted there, allowing me to change it however I want and then leave repair mode. Or maybe you can just send your tar archive to the support team, asking them to extract it to your VPS when it's offline. 

If you need to modify files content instantly you can use the . You can add a to backup the original file before any change happens. Adding, that is a Stream EDitor and you rather should use or . Update example: 

I'm connected through the console on the router and can ping the outside world only from GigaEthernet1 port which has IP address The clients that connect on VLan1 get IP addresses in the range of and these clients can ping each other, the gateway which is and the GigaEthernet1 which has IP What's going wrong in this case? UPDATE 

I installed keepalived on two firewalls to provide fail over. I'm not sure if the following configurations are correct (see configurations below). Sometimes I have problems to reach the websites which are behind the firewalls. I suspect that when keepalived runs on both firewalls, for a period of approximately one minute, the websites remain unreachable.. then the connection to the websites is recovered. What could be the problem? Can it be that the keepalived are switching state (MASTER or SLAVE) constantly? Firewall-2 runs in MASTER state. When keepalived is started on firewall-1 it jumps into BACKUP state. Are there commands or tools like to check the real state of keepalived? Configuration on firwall-1 

I want to achieve a DHCP 4 failover with ISC Kea. I have a setup with 3 mySQL servers (1 Master at facility A, 2 Slaves at facility B, replication works) and 2 Kea DHCP servers (configured read only) in facility B which have the 2 mySQL slaves as their backend and are configured identically (we enter the values into the DB in facility A) So far so good. Now I searched the net for how to create a failover for both servers. I found this, but I don't know what to do, actually. Does Kea automatically check there is another server and I don't need to do anything? I could configure Kea to respond on a certain interface only (on another subnet maybe?) and then create a keepalived config to put both interfaces behind one IP in the actual subnet (I think). But is that necessary? 

I messed up my SSRS2008 Express' configuration I'm afraid. I tried to enable anonymous access, using different methods I found on the net, so users would be able to just open the (non-IE) browser with their reports. Now, I neither can deploy (a box wanting credentials pops up, which does not even accept the domain admin account, that has the dbo role assigned) nor access the report servers web frontend through IE I tried maybe hundreds of combinations of service account, authentication methods but the ERROR in the Log File stays the same: