Does this same fill strategy apply to tempdb? And does this answer depend on the type of query, e.g. parallel vs. non-parallel? Or is the answer different based on the kind of tempdb I/O, e.g. for temp ables I create vs. tempdb usage by the database engine for worktables or spilling? 

I assume the answers to #3 and #4 are "1 volume" and #5 is "2 volumes" but it's #1 and #2 that I'm most curious about. The specific reason I'm asking is wondering if it's possible to increase the Resource Governor's IOPS limit for locally-attached SSD tempdb while having a lower limit for our SAN data storage. So I'm wondering if splitting a single physical disk into multiple partitions might be a way to do this, by putting separate tempdb files on each partition so the total tempdb If #1 above makes SQL Server treat one physical disks as multiple volumes for throttling purposes, this may be an option. I'm assuming that this won't work-- that SQL Server is smart enough to know that 2 partitions is one "volume". But was worth asking. 

Locally attached disk that's split into two partitions E: and F: Software RAID 1 set E: composed of 2 locally attached disks (yes I know software RAID is bad-- adding this case to help me understand SQL Server's definition of "volume", not to design a production setup!) Hardware RAID 1 set E: composed of 2 locally-attached disks SAN disk E: on who knows/who cares how many disks. 1 SQL Server filegroup spread across two locally attached disks E: and F: 

When setting MAX_IOPS_PER_VOLUME in a Resource Pool, what exactly does "volume" mean? Specifically, how many "volumes" would be the following cases: 

Our SQL 2014 server has a blazing-fast tempdb drive (2800 IOPS) but a much slower data drive (500 IOPS). Our application runs a few long-running reporting queries that are I/O-intensive and we'd like to avoid them starving our server for I/O capacity when they run. Ideally we'd be able to limit these queries to 50% of available I/O capacity. Unfortunately SQL Server Resource Pool's IOPS throttling is not percentage-baed nor volume-specific. If I limit to 250 IOPS, then it will unnecessarily slow down performance of queries that make heavy demands on tempdb. Slowing down these long-running queries if the server is busy is OK, but slowing them down by 10x+ if they need lots of tempdb access is not OK. So we're looking for workarounds that will defend other queries from these lower-priority, long-running queries, but without unnecessarily hurting performance of these long-running queries if they happen to use lots of tempdb. It's not practical to change the queries themselves to reduce tempDB usage-- these queries are generated by a custom reporting feature that may sometimes generate really complex query plans that spill results to tempdb. So far the best idea I have is to remove IOPS throttling and instead use the "Importance" of a workload group to defend the rest of the server's I/O capacity from these queries. Is this a good solution to the problem I'm trying to solve? What are the pros and cons of using Importance? Or is there a better way to achieve our goals? 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten? 

When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: 

So we're wondering if there's a lower-cost solution that would store as smallint but expose the colunms as ints to readers. Like this: 

A multi-billion-row fact table in our database has 10 measures stored as columns. The value ranges for some of these columns won't ever be above the +/-32K range of a . To save I/O, we're investigating whether it's practical to store these columns as instead of . But we're concerned about what problems might crop up from doing this, including: 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it. 

Make sure to give some thought to the "target size" you use with SHRINKFILE if you do this. You say it's only 10 GB of used space, but I wouldn't immediately set it to 10 GB - you are putting some data into this database, right? 

Here are a few possible solutions to your problem: Option 1: Don't rebuild that index That's a 30 GB index you have there. What measurable performance problem are you trying to solve by rebuilding it? Especially at 5% fragmentation, this seems like an incredibly expensive operation (in terms of system resources and locking) for very little gain. You can read some very well-founded opinions on why you might want to give up on the index rebuild here: 

The plain select is on top, at 12ms, while the nolock version is on the bottom at 26ms (14ms longer). You can also see in the "When" column that the code was executed more frequently during the sample. This may be an implementation detail of nolock, but it seems to introduce quite a bit of overhead for small samples. 

Outside of that method, I noticed another small difference that causes the nolock version to run slower: Releasing Locks The nolock branch appears to more aggressively run the method, which you can see in this screenshot: 

Two solutions were mentioned in the comments to your question. Let's review both of them. Use a READ_ONLY cursor 

So it has to do the key lookups to check and see if the rows you've read in have been modified. The solution proposed in the blog post, and by Jacob H, is to use a more restrictive cursor type (such as READ_ONLY) in order to avoid these extra key lookups entirely. Optimize the index 

(a in .NET classically includes the message you mentioned - "Object reference not set to an instance of an object") You should try updating to 17.6 (build 14.0.17230.0) to see if the problem is resolved there. 

If your mdf and ldf files are in a non-default location (or have a non-default prefix / name), you need to add that information to the SSDT project. This will help the schema compare operation (that occurs when you do a publish) to see that the files are in the right place and don't need to be dropped / recreated. Right-click your project -> Add New Item -> File Group File You'll get something like this: 

I would highly recommend that you test your new T:\ drive using Crystal Disk Mark. Check out the guide from Brent Ozar here: How to Test Your Storage with CrystalDiskMark Compare the results from the T:\ drive with 

Checkpoints, like most things in SQL Server, are a big topic. A good place to start is this Microsoft Docs page: Database Checkpoints (SQL Server) To respond to your main question though, check out this specific quote from that page: 

If this is a default (not named) instance, you can ignore the part. If the files you want to be deleted are not in this folder, and don't have a .trn extension, then they won't be deleted by the cleanup step. Something is "touching" your log backup files The extended stored procedure date parameter deletes based on the "modified date" metadata. If some other application has "touched" these files, causing Windows to update the modified date to a time inside the cutoff, they won't be deleted. There is a permissions issue The account that's running SQL Server Agent Services should have full control on the folder where backups are to be deleted. Make sure this hasn't been changed. Validation checks are failing or other problems occurred I noticed you have - you should check to confirm there are no error messages being reported during these times. 

Modify that script so that it points to the desired location of your primary data file (mdf) and your deploy should go much more smoothly. Note that is set by the settings of the target instance: 

Note: to get the best help on this question, please include your actual RESTORE statement, and the specific error message that you're getting. When using In-Memory OLTP, SQL Server has to create a new folder named "xtp" in the root of the default file location for the SQL Server instance. This folder contains the DLLs for compiled stored procedures and other in-memory objects. You can find more details about that here: In-Memory OLTP files â€“what are they and how can I relocate them? If you've changed the location for your data files, you may need to update SQL Server's access to the file system there: Configure File System Permissions for Database Engine Access As a test / workaround, you could manually create the "xtp" folder, and then try the restore again. 

NOTE: this might not be the type of answer you're looking for. But perhaps it will be helpful to other potential answerers as far as providing clues as where to start looking When I run these queries under ETW tracing (using PerfView), I get the following results: 

Next Steps Since it seems like the disk is reasonably fast (per the benchmarks you shared), I think it would be a good idea to log the contents of before and after the nightly batch job you mentioned. This will tell you how much I/O is happening on tempdb during that process. This is important, because maybe there really is more I/O than the disk can handle. So here's what you do: 

A few things could be going on here. The files are in the wrong place, or have an unexpected extension This one seems unlikely, since you've probably been using Ola's scripts for all your backups, but just in case you've changed a setting (like the directory location, or the log file extension). The "cleanup" part of Ola's scripts are looking for backups in this location (starting with the server / folder you specified in the @Directory parameter). In your question you used so I'll continue with that: