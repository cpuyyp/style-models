"GDP" stands for "Gross Domestic Product". It measures the income generated, prior to any consumption-saving allocation. In other words, to be able to save you must first generate product/income and then decide to save. So savings does not affect the level of the already created income, since it is a part of it. The archetypal closed economy with no borrowing is characteriazed by the identity $$Y = GDP = C + I = C + S$$ When we look the matter from an expenditure approach, we essentially consider the different uses of income (and in that sense the use of the term "expenditure" may be a bit misleading). And savings is also a way to use your income (where "use" has a broader meanign than "spend"). 

How do normal vendors know at what level to set their selling prices when times are normal? They look around to see what competitors do, look at their costs, and engage in a trial-and-error process. Where it all starts? We don't know that for the normal times, how are we supposed to know it for the abnormal ones? What is different in hyperinflationary times? That a wrong "guess" can have devastating consequences. So vendors intensify their information gathering search, they change prices daily or even, more than one times during the same day, and also (I would venture), in order to guard against the devastating consequences, they tend to set their prices even higher than what a "rational" processing of information would suggest (since they know they face great Knightian uncertainty rather than merely "risk"), fueling hyperinflation even more... ...and then, many of them still get it "wrong", and suffer huge losses as a consequence. PS: I cannot resist the temptation to link to the fascinating paper by T.J. Sargent "The Ends of Four Big Inflations" (1982), (freely downloadable here). It is only very indirectly relevant to what the OP asks, in that it provides ample data that show how fast inflation rose in the four countries examined (Austria, Hungary, Poland, Germany), during their hyperinflation incidents, and so gives a feeling of what where the time constraints under which vendors had to decide on pricing... but it is relevant to the general subject of hyperinflation of course. 

It appears that you equate "science = academic field" which is certainly not the case. Also, it may be true that the word "science" was customarily associated with what we nowadays call "hard science", which exactly proves that the use of the word "science" has been generalized. Hard and soft, humanities and social sciences, they are all "scientific fields/ scientific disciplines" now, in the sense of being fields of human activity where the accumulation of knowledge is systematically and purposefully pursued, (and no, "science", or "knowledge" for that matter, is not just about "uncovering unchanging laws"). This is what happened with economics. People of knowledge have occasionally written about economic phenomena from the ancient times, but these instances were few and far between. Starting from the end of 18th century, whole books appeared that described, debated, explored economic phenomena. And during the 19th century it became more and more frequent, ending up in "systematic". I do not understand the final question of your post about "political processes or demand" 

Questions about specific markets in specific countries require targeted applied research to be answered - this is the fine hour for applied microeconomics and industrial economics. Telecommunications is a regulated market usually -and indeed the link provided by the OP quotes the industry regulator 

Now the range of values matches those appearing in the paper's graph. It may be the case that the correct Variance is $0.007$ and the authors incorrectly used it as StDev.But it also may be the case that the correct variance is $0.000049$ and the correct SD is $0.007$. So the simulation was correct (in line with the obtained estimate), but by mistake they called in the paper "Variance" what should be called "Standard Deviation". I will attempt to contact the authors on these two matters. 

It appears that you suspect that the regressor "price" is endogenous, i.e. correlated with the error term, and that you consider what kind of instrument to use in order to tackle endogeneity. You think a possible instrument could be revenues or profits, because they are correlated with the price. Say, Revenues, denoted by $R$. But $$R_{ij} = P_{ij}\cdot D_{ij} = P_{ij}\cdot(A + aP_{ij} + bY_{ij} + cN_{ij} + e_{ij})$$ and so Revenues are a function of the error term, therefore they will certainly be correlated with it, and same goes for profits (and this should be obvious since both depend on quantity and therefore on the error term). In order to find an admissible instrument, you must first conclude about what are the variables that may be included in the error term, and are correlate with price. This will help you to search for admissible instruments. PS: I cannot see the connection of the tax mentioned with the issue of instrumental variables estimation. 

I would agree that volunteered labor is a utility enhancing activity. Seen as a good, its price could be the opportunity cost, i.e. the wages foregone (for not working for pay). But then this is not really different from the concept of "leisure", since "leisure" in economics does not mean "fun and games", but rather "time spent not working for pay". An interesting extension could be the potential value of volunteered labor as professional experience towards future employment for pay (in line with Arrow's "Learning by doing" efficiency concept). Then the issue becomes intertemporal, and also, income considerations arise: I want to volunteer, it generates utility directly for me, but also, I'd rather volunteer and build up professional experience that may lead to higher wages tomorrow, than try to work now and have lower pay due to smaller experience. 

The data sample is so small that formal testing for stationarity would be essentially worthless. Inspect visually your individual series for any obvious trend. This would be the case where even with a short sample non-stationarity would be a problem. 

Although I still don't understand what these "strengths" are all about, still, mechanically, step $4)$ is now clear. At each point in time, there is a certain allocation of Strengths to actions, $\{S_i(t),\; i=1,2,...,A\}$. This allocation changes through time. The allocation is turned into a relative frequency distribution by dividing each $S_i(t)$ by their sum $C(t)$. But then $$\sum_{i=1}^A \frac {S_i(t)}{C(t)} =1 $$ and (I presume), $S_i(t) \geq 0,\; \forall i$. So indeed this relative frequency distribution can be treated as a probability distribution, and each $S_i(t)/C(t)$ can be treated as a probability that characterizes action $i$ in period $t$. So you have to code this probability distribution, and then have the software "randomly draw" from it, as though this distribution was part of the distributions available in a "random number generator". The floor is yours now. 

It appears to be the Theory of Games and Economic Behavior (1944) by John von Neumann & Oskar Morgenstern. I have the 1953 edition which is counted as "3d", but by reading the included introductions to the 2nd and to the 3d editions, it appears that nothing of substance has been changed in chapter 3, where I locate the issue of equivalence up to linear transformations. In sub-chapter 3.4 the authors discuss in general the issue of equivalence under transformations for any system of quantities, mainly from physics. They end this sub-chapter by writing (bold my emphasis) 

We could distinguish between two kinds of "wage subsidies": A) The government pays to the firm part of the wage cost, usually social security fees. B) The government pays to the employee a markup on his wage. In scenario A, labor supply is not affected but labor demand shifts outwards: the tendency should be higher employment and higher equilibrium wage (so eventually more disposable income for the workers). Usually, what is observed is that employment rises (after all, usually such programs are on condition of net new jobs creation in a company), but wages do not increase (i.e. firms don't share the subsidy with the employees). So no betterment of standard of living of those employed. In scenario B, take-home income will increase at no cost to the firms. So now, workers know that if the current wage is $w$, if they work they will receive $w+s$. This will shift the labor supply curve outwards (because now it responds to $w+s$), while the labor demand curve will remain unchanged (which still responds to $w$ only). This will tend to result in higher employment and lower equilibrium wage. But this contradicts (and partly or wholly offsets) the very purpose of the measure, which is not to increase employment, but to increase disposable worker income. So in both cases, wage subsidies are more about employment rather than income, while minimum wage is clearly about income and standard of living of those employed, even though it may hurt employment. ...and one cannot avoid in these matters a dash of political economy and political science (as well as social psychology): subsidies bear always a risk of fraud, and are always suspect of being an instance of pork-barrel politics. Moreover, they are mostly seen as measures in time of macroeconomic (or regional) recession/depression, where the main concern is to boost employment. On the other hand, minimum wage is not about a faltering economy as regards activity and level of production/income, but about market failure as regards distribution of income. Issues of distribution are inextricably linked to issues of fairness, and this creates a more passionate political agenda, occasionally satisfying also the expectations of the public for a "strong government" which, "when markets create unjust misery", intervenes "decisively", imposing by decree "what's fair" (or fairer). 

The formulation $${\rm Adjustment \; Cost\; of \;Investment} = K\cdot c(I/K)$$ uses the same rationale as regards the unit cost, but it postulates that this extra unit cost is per Unit of Current State (Capital) rather than per Unit of Change (Investment). So the function $c()$ has a different interpretation, and it is not conceptually comparable with $\phi()$ -the latter is "unit adjustment cost per unit of Investment" while the former "unit adjustment cost per unit of installed Capital" (so if one attempted to empirically estimate both functions, one would not arrive at the same function/parameters). Here we have $$\frac {\partial}{\partial I}\big[ K\cdot c(I/K)\big] = c'(I/K) > 0 $$ which gives the same message as the previous formulation as regards Total Adjustment Costs and the level of investment. But $$\frac {\partial}{\partial K}\big[ K\cdot c(I/K)\big] = c(I/K) - (I/K)\cdot c'(I/K) <> 0 $$ and it would be positive (negative) depending on whether the elasticity of the unit adjustment cost function with respect to $(I/K)$ is lower (higher) than unity. This seems to provide a higher degree of flexibility as regards the modeling of real world situations. It permits for example to account for the costs of "rigidity" that may go together with a company that is "large" in size, where due to them Total Adjustment Costs for given $I$ may be increasing after some level of Capital. An empirically inclined and extensive discussion on various issues surrounding Adjustment Costs (functional forms, micro/macro point of view, etc) can be found in Hamermesh, D. S., & Pfann, G. A. (1996). Adjustment costs in factor demand. Journal of Economic Literature, 1264-1292. 

Following a comment exchange below another answer, the critical detail of a link offered by the OP as an example of by-passing/ignoring Wicken's comment/argument, and one different from Wickens' formulation, is that in the link's eq. $(4)$, $$\lambda_t = \beta E_t[\lambda_{t+1}(1+r_t)]$$ the multiplier $\lambda_{t+1}$ appears together with the interest rate of period $t$, which is assumed part of the information set at $t$ and so not a random variable in the specific equation. So the authors can proceed in the 2nd line after eq. $(6)$ to write the marginal rate of substitution as they do, without the need to assume uncorrelatedness between the multiplier/marginal utility of consumption, and the interest rate. This goes back to how they formulate the income resource constraint (page 2 middle), where it is essentially assumed that the household has, at the beginning of period $t$, available assets or debt $(1+r_{t-1})B_t$. The authors explicitly discuss this "timing convention" immediately after eq. $(1)$ 

It appears you are looking for literature on Ambiguity Aversion and/or "Uncertainty Aversion". You can start by looking up the work of L.G Epstein, I. Gilboa, and D. Schmeidler. It is an attempt to formalize the behavior of agents exhibiting behavior consistent with the Ellsberg paradox. 

You are missing the average cost curve in the same diagram. Basic algebra gives us the following. Let's find the minimum of the $AC = C/Q$. We have $$\frac {\partial AC}{\partial Q} = \frac {MC\cdot Q - C}{Q^2}$$ For this to be equal to zero, we must have $MC \cdot Q = C \implies MC = AC$. So when $AC$ is at its minimum, it equals $MC$. But we also want the second order condition, and we want the second derivative to be positive, evaluated at the critical point. $$\frac {\partial^2 AC}{\partial Q^2} <0 \implies ...MC'\cdot Q^3-2MC\cdot Q^2 + 2QC >0$$ At the critical point $MC = AC = C/Q$. Inserting this we get the condition $$MC'\cdot Q^3-2(C/Q)\cdot Q^2 + 2QC >0 \implies MC'>0$$ So at the point where $MC = AC$ we want MC to be rising. But this implies that it will cross the $AC$ curve from below. In turn this implies that for quantities lower than this point, marginal cost curve will be below the average cost curve, which means that if this left part of $MC$ curve was a supply curve the firm would have losses. Therefore the supply curve is only an upwards-sloping part of the marginal cost curve. You can now superimpose the $AC$ curve that is consistent with all these. 

We tend to forget that "transforming a variable" leads to a new variable, whose behavior may be totally different than the "original one". The easiest example is to compare the graphs of a variable and its square. So by considering the natural logarithms of your variables, you no longer examine the relation between them, but a relation between some function of them. It is fortunate that the mathematical concept of "logarithm" can be linked to the concept of "elasticity", which describes a relation between percentage changes, which is something we understand from an economic point of view and we can meaningfully interpret and use. If the variables can be reasonably said to exhibit a "linear relationship in logarithms", it means that their levels (i.e. the actual variables) have a non-linear relationship: $$\ln y \approx a+b\ln x \Rightarrow y \approx e^a + x^b$$ So why not estimate a non-linear model? In (mathematical) principle, there is no reason why not. Some practical issues are: 1) There are too many forms of non-linear relationships, there is only one linear relationship (structurally speaking). It is a matter of "search costs" for the most suitable specification. 2) The non-linear relationship obtained may not have a clear economic explanation. Why this is a problem? Because, we are not uncovering "laws of nature" here, unchanged through time and space. We are approximating a social phenomenon. Having an approximation which, moreover, can only be presented as a mathematical formula, without an economic reasoning that validates and supports it, makes the result very thin. 3) Non-linear estimation is less stable, as regards the mechanics of the estimation algorithm.