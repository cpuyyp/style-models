One thing that might help is switching to a "fail-fast" approach, where you test error conditions first and handle them immediately if one is detected. Execution in the method only continues if there are no errors, which means one less block is required. That would give you this: 

I agree that your assembly code is elegant, well-written, and clear. (Although, for what it's worth, I find the arrows showing the flow of control more confusing than the normal labels and branch targets. The similar-looking arrows all blend together when I try to study them. Maybe it's because I'm not a visual learner.) 

Lots of bad practices exhibited here… First of all, you probably shouldn't be using exception handling here at all, as Incomputable already pointed out in a comment. Exceptions should be for truly exceptional situations. If you expect a zero input as a possibility and are going to explicitly handle it, then I would argue that it is not exceptional at all. Thus, a simple loop and conditional test would suffice: just keep looping until the user enters a valid input. Second, your variable names leave a lot to be desired. What are and ? It's okay to use terse, single-letter variable names like when their purpose is obvious, as for a loop index, but that's not the case here. Therefore, you should use descriptive variable names. Since we're doing division, maybe and would be good choices? Third, is just terrible and should never be used in real code. I guess you're using this to work around the fact that your development environment closes the console environment as soon as the program finishes execution, instead of leaving the window on the screen so you can see its output. Don't use code as a hitch to work around your IDE's bad behavior—learn to configure your IDE so it does what you want. If you absolutely must rely on such a hitch, use . But there's really no "good" way to do this, so prefer not doing it. Fourth, if you're going to throw exceptions: 

The Main Event: Optimizing At the core of your code's logic is a call to to retrieve the current color value of a particular pixel on the screen. That is unfortunate if you're concerned about speed, because is a very slow function, indeed. Why is it so slow? The above-linked documentation doesn't give much of a hint. In fact, this API has a dirty little secret: individual pixels cannot be directly manipulated by the graphics subsystem, so (and its brother, ) have to simulate it, which makes an ostensibly simple, straightforward function call into an extremely slow operation. In addition to the usual overhead of a function call, parameter validation, and switching to and from kernel mode, the function also has to create a temporary bitmap, copy the contents of the DC into that temporary bitmap, perform all necessary color-mapping, map the coordinates and locate the pixel, retrieve its color value, and then destroy the temporary bitmap. A lot of work for a single pixel! The throughput of is on the order of 100,000 pixels per second, which is usually fast enough. If you want to speed up drawing operations that involve the manipulation of multiple pixels, the general strategy is to minimize overhead by copying the DC's contents into a temporary device-independent bitmap (DIB) and then using pointer arithmetic on this DIB to access individual pixels. This gives you direct access to each pixel, and allows a throughput of millions or more pixels per second. The problem for you is that you cannot simply use a cached snapshot of the screen—you really do need to update it each time through the loop, which means that this strategy doesn't save you a whole lot in terms of overhead. But it'll still be slightly faster. Here's an approximation of the code that would be required: 

There is one major difference, though. I've written the code to work with unsigned types, rather than signed types. In fact, bitwise operations should always be done on unsigned types because when you do bitwise operations, you are essentially treating the value as a field of bits, rather than as a number. Using an unsigned type is not only conceptually more correct, but also saves you from bugs. C# automatically differentiates between signed and unsigned shifts (based on the type of the value you're shifting), which is why you found that your code needed that extra mask. The result was being padded with the most significant bit of the number being shifted because that's how a signed shift works—in effect, it does a signed extension, where the sign bit (the most significant bit) is repeated to preserve the sign of the value. (Note that this distinction is only relevant for right shifts; signed and unsigned left shifts are exactly the same.) But that's something you don't want to have to think about, and something that you wouldn't have to think about if you just follow the rule that bitwise operations imply unsigned types. So the correct code is precisely what pgs already suggested: 

but that is unnecessary and doesn't buy you anything. In C# (unlike Java), there is no explicitly unsigned right-shift operator (i.e., one that does zero-extension as opposed to sign-extension), so the casts are still required. And once you've got the casts to force an unsigned arithmetic operation, you might as well just do a division. Let the JIT compiler optimize an unsigned division by a constant 2 into an unsigned right-shift by 1—and indeed, it will do precisely that, allowing you to write the code so that it remains readable and correct. You probably still want a comment to ensure that an overzealous maintenance programmer doesn't remove the "superfluous" casts. Although you could convert the numbers into a floating-point space to increase the range, this will be slower—probably significantly so in a tight loop. Since it is not necessary and the problem is trivially solved by using unsigned integers, I recommend not doing this. 

You know that is 0, and you know that is less than the maximum positive value representable by an , so there is no possibility for overflow. In fact, the compiler sees this as simply . You are not so lucky inside of the loop, though. There, if the value you seek is in the second half of the array, you'll end up assigning to , and then if the array is sufficiently large (i.e., if is sufficiently large), then the calculation of will overflow as described. Perhaps not terribly likely that you'll have an array large enough (famous last words!), but still a bug waiting to strike when you least expect it. 

First off, you have nice, clean, well-formatted, easy-to-read code. You have even included comments that explain the goal of each instruction. Too much of the time I review assembly-language code, these are the things that go wrong. You've gotten them all correct. Nice job! Now I don't have to pull my hair out trying to read and understand your code. Unfortunately, were I your instructor, I'd still have to give you a failing score on the assignment because you didn't follow the rules. The assignment says that you must "Do it without actually adding the numbers.", but right off the bat, you the input values together. And we were off to such a good start… :-( Now, personally, I think these types of assignments are rather silly, so I wouldn't be giving them. If I wanted you to learn how to use the bitwise operators, I'd find something useful and real-world that they are good for, and then give you that assignment. It's not like I'd have to work very hard. The chip designers didn't put them in merely for fun. Oh well; you have to do the assignment that you were given. So follow the hint, use . Maybe you don't know exactly what that means, so what I'd do is open up a programmer's calculator (all major desktop operating systems have a "programmer" mode for their calculators) and play around with it. Pick random combinations of positive and negative numbers, and compare what the results are when you add them together versus when you XOR them together. Try to get a feel for what XOR does. Then, look up a formal definition of XOR (exclusive-OR). If you're like me, your eyes glaze over at the symbolic logic stuff (which wasn't so great when I took that course in college); feel free to skip over that for the purposes of this assignment. Your real goal here is to find out what XOR actually does at the bit level. There are lots of detailed explanations of bitwise manipulation online, or you may have a textbook that covers this stuff, too. For example: 

Note that I've explicitly used the type here, instead of the short name . This is because I've hard-coded assumptions about the type's width, and I like things to be obvious. is obviously always a 32-bit type. At least for me, since I bounce back and forth between languages, that's not always so obvious with . I've also thrown in an assertion to check that we're rotating by a reasonable . That's just an assertion, though, not an exception, because the C# language standard guarantees that the shift amount will be masked, such that only the lower 5 bits will be used (resulting in a value from 0 to 31). This means that the language protects you from the undefined behavior you would get in C by trying to shift by too many places. (Again, ironically, so does x86 assembly language.) So this is basically the generalized form of the code that you have. A call like: 

Even though is 2 bytes compared to 2+2 bytes for +, is slower. On the 286–486, executes in ~8 clock cycles when the branch is taken and ~5 clock cycles when the branch is not taken. The alternative is 1 cycle for the , plus 7 cycles on 286/386 when is taken or 3 cycles when not taken (on 486, is even faster—3 cycles when taken, 1 cycle otherwise). Thus, in the worst case on the 286/386, + is equally as fast as , while in the best case, it is about twice as fast. On the 486, + murders speed-wise. Intel's optimization manuals at this point in history explicitly recommended using + instead of . Similarly, the 2-byte was generally the fastest way to exchange two enregistered values on the 808x. It executed in 4 cycles, which was the same as two instructions (2 bytes and 2 cycles each), but it was half the size, so it was quicker to fetch. Things changed on the 286, and on all subsequent processors. was now a 3-cycle instruction, but became a 1-cycle instruction, so doing two s was faster than a single . That is, assuming you had an extra register to spare. If not, given the limited register set, you might still have been better off with . The 486 changed that yet again: since ran in only 1 cycle, you could use three s in the classic swap trick as fast or faster than a single (faster if you could interleave the three s within other instructions you needed to execute anyway to reduce dependencies). The Pentium had dual execution engines, known as the U and V pipes. The U pipe was basically a full-featured 486, while the V pipe was more limited and could only execute a subset of instructions. This made instruction pairing a big deal, because if you properly paired your instructions such that one could execute on the U pipe while the other executed on the V pipe, you could effectively double performance. The trick becomes even more attractive here than a single , because paired on either pipes and could be even more effectively interleaved within a complicated sequence of instructions. Pentium Pro introduced out-of-order execution, which made perfect instruction pairing less important, but much of the same optimization logic still applied. The CISC-style instructions are very slow on modern processors and should not be used even when they are fewer bytes. The days of rote cycle-counting are long gone, however. All of that to say that you can probably improve the performance of method 2 even further by choosing more optimal instruction sequences, even if that results in larger code. Another way to speed the code up is to replace conditional branches with clever bitwise or arithmetic operations that accomplish the same thing branchlessly. This is a huge performance win on the 808x and extremely significant on any processor that lacks a branch predictor (introduced with the Pentium). Even on modern processors, there are massive penalties if branch prediction is unsuccessful (not strongly biased for taken or not-taken), so rewriting branching code to be branchless can still result in a performance speed-up. You've got a tight loop here, the body of which is a perfect place to apply such optimizations. This is equally true for either method (and, at first glance, it looks like the branches could be more elegantly removed in method 1). You could also ditch the relatively-slow instruction if you replaced with +, instead of +. Normally, you would prefer for a comparison against zero, but in this case, is likely a better choice, because it sets the carry flag (CF) according to its result, while does not affect CF. One thing that likely makes a big difference in the relative performance of method 1 and method 2 is replacing the instruction with a instruction. Or at least, depending on which processor you're using to do the timing. is extremely slow on the 386—about three times slower than . The difference isn't there on 808x, 286, Pentium I/II/III, and most AMD processors. However, on recent Intel processors (starting from Pentium M, extending through Core 2, and later up to current microarchitectures, including Atom), is once again about three times slower than (higher latency and less throughput). Finally, you aren't seeing it here because you're doing an apples-to-apples comparison, but 16-bit instructions are one-byte longer (because of a size prefix) and noticeably slower when running in 32-bit protected mode. If you're not running in real mode, you could speed up the code "for free" by switching to 32-bit instructions, using (or + ) to initialize 32-bit registers with unsigned 16-bit integer values. 

Either form could be used, since doesn't modify its destination operand. You'd just have to adjust the direction of the subsequent branch, since the flags will be set for the opposite comparison. The struggle is, which one do you use? If you're optimizing for the 286, you'd use , but if you're optimizing for the 386, you'd use . Fortunately, on the 486 and all later processors, the choice is irrelevant because they execute at identical speeds. 

Noteworthy, perhaps, but not surprising. If you don't use the integer divide operator, you get a floating-point division. Semantically, then, this is really just a bug: you're dividing two integers using the floating-point division operator. Why is it so much slower to do the division in floating-point? Well, there are a couple of reasons. First, you have to pay for a conversion from the integer value into a floating-point value, and then a conversion from the floating-point result back into an integer value. Aside from the fact that this pointlessly injects a bunch of extra instructions, it also vastly increases the latency. Second, there's the fact that the QBasic system is so old that it can only emit instructions that use the x87 FPU, which is less efficient than SSE/SSE2 instructions and slower to move values into and out of because they have to round-trip through memory. In fact, it is probably even worse than that. Given its age, QBasic probably doesn't even use FPU instructions. Back in the day, not all systems had FPUs installed, and floating-point operations had to be simulated using a routines provided by a runtime library, where you not only had to pay the cost of a function call, but the simulated code was much slower than a simple . 

This works, but aside from the readability concerns (which could be addressed by a comment), it is slower than the original. Even an optimizing compiler has little choice but to transform this into a nearly-literal sequence of machine instructions. A better solution is to do the computation with an unsigned integer value (in an unchecked context, of course). That way, you'll get the wrap-around behavior as desired, and the integer being treated as unsigned ensures that the result won't be interpreted as a negative value. The code remains essentially what you had, except with the addition of some ugly casts to force the intermediate arithmetic operations to be done on an unsigned integer, and then to cast the result back to a signed integer: 

But you might also want to consider Eric Lippert's advice about logical representations. It is merely an implementation detail that these color values are stored as unsigned 32-bit integers. Therefore, it would make sense to wrap that implementation detail up inside of a structure, so that the client never has to deal with the messiness. 

In fact, though, we can do even better. VBA supports a special type of loop that can be used to automatically iterate through an entire collection of values (in this case, all of the columns in a sheet). This is the loop, and it can be used like so: