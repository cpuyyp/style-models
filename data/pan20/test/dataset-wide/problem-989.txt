Without knowledge of your schema, query attempted and statistics from explain analyze, any response can only deal in generics. In this sense and in terms of SQL, there are generally two commonly used strategies for dealing with finding missed relations: and . NOT EXISTS: 

You have an extra unnecessary subquery check, and your subquery seems to only return a rowset if there is a single value in it, so you may want to use instead of . You can achieve the same by doing this: 

UML class diagrams do not include relations in a class' attributes list, so when creating database tables representing those classes, the database designer must identify and specify them, much like the software developer must create the pointers and lists to support the related objects. In your diagram, relates to but does not explicitly specify a attribute. When creating the table, you must include this attribute along with a foreign key to . You can find further information about this in Database Design - UML/SQL: 

I can attach databases with . I've already done this with C# code but that solution seems to be a kind of overcomplicated (I have it on github). I'm wondering if it's possible to merge them with sql only? How would you do that? 

I have 2 tables. 2nd has foreign keys to reference 1st. I need to replace the whole content of the 1st table with the new version from another database. The new version satisfies all constraints and foreign keys. I need a generic way to do it for any table so no exact name of foreign keys or constraints or columns may be "hardcoded" (but script "generation" like pg_dump does is ok). 

In the case above, is the anchor date, and is the first day of the month in . The calculated columns , and denote entries valid (value ) since the start of the month, for the anchor day only, and for the first day of the month only, respectively. The calculated column has values d, e, f and g, for last 7 days, last 14 days, last month, and last two months, respectively. Notice that values a, b, and c can overlap with each other and with d, e, and f (thus needing their individual flags), but values d, e, f and g never overlap, so are folded in a single column. With these values (performed primarily for readability of the outer SELECT statement), all the values are SUMs of values conforming to some criteria (either a, b, c, d, e, f, or g). If none of the values overlapped, the query could be simplified significantly. 

As option separates table files instead of putting all data and indexes of DBs into one ibdata file, is using this option improve speed of alter table? I have a table of 40M rows and when I alter a specific table it takes about 5 to 6 hours. Does this solution help? Is there other ways around to improve alter table speed on heavy tables? 

My current storage engine is and its compression level is as default, snappy. I've come across MongoDB documentation and it's been mentioned that using zlib compress better but needs more CPU. I want to know will store more data in memory compared to as it compress the data? I have a server with 16 CPU cores. As RAM is more expensive I'd rather to save on memory in case it keeps more data. Is this correct? Can I blindly switch to zlib to cache more data and improve read performance? NOTE: Our server is read intensive. 

But after few attempts it executes in 160 ms. The same query can also be slow without / so I'm not sure if it's really related to transactions. But executes very fast and shows execution time < 1 ms. Postgres version: 9.4. Log: 

At high load conditions the server app stops responding to client because / statements are executing very slowly (some take 15 seconds). The queries are simple - insert, update two columns on one record by id, select a,b,c from d order by limit 50, etc... pgAdmin shows a lot of locks and queries like 

(2000 granted locks and 0 not granted) After some time the server starts responding, the locks disappear (only 10 locks) but in another minute I have those 2000 locks again. The client app reports that server app response time goes from 600 ms (network worker interval) to 60 seconds, then goes down to ~4 seconds and then up again. There are ~5 actively used tables, some of them have up to 200k rows. The server app uses NHibernate with Npgsql. VACUUM/ANALYZE didn't help. This query runs up to 11 seconds when executed remotely: 

Any user (ROLE) can be a member of one or more user groups (which are also ROLES). According to the documentation for GRANT, "Any particular role will have the sum of privileges granted directly to it, privileges granted to any role it is presently a member of, and privileges granted to PUBLIC.". So you can have all of your personnel belonging to a company group (ROLE) with basic privileges, some in another group with elevated privileges, and a few with superuser privileges. 

I had a really odd and unexpected behaviour in PostgreSQL. Debugging I found that the problem only occurred while including inside a function, and by using an alternative (in this case, I resorted to an array) the error would be gone. Perhaps any one experienced a similar error or knows what this error relates to? I might need to create temporary tables in functions sooner or later. I'm porting an old Delphi XE6 program from a legacy proprietary database to PostgreSQL, changing the TxxTable components (proprietary) to TFDTable ones (FireDAC) pointing to a copy of the database in PostgreSQL 9.5. Things were working peachy until I created this plpgsql function that inserts and updates some data here and there. I ran the function via , where is my TFDConnection to PostgreSQL (same where I have many other tables open and working well), is the function in question, and is an ID I send as a parameter. The function runs well, as it does when running it from PgAdmin, but the connection becomes unstable as any already open table afterwards trying to do anything (insert/edit/move) will raise the following error: 

We have about 15GB of free memory that does not use! On peak we reach 400 OPS, and 500 connections per seconds. Is there anything I could do to improve the performance? 

Or update a field like comment_no in shares(posts) table? I have fields like shares_no,comments_no,likes_no. Everytime a user click the likes_no field will be incremented and other fields as I explained. This way I have to update every record each time a user post a comment or shares a post or likes a post. Should I use and so on for getting the record count or just updating a record. There will about 10 to 20 posts each second. Site's traffic will rose. Which approach should I use? Which one is more expensive? EDIT: I've used as below to speed things up: