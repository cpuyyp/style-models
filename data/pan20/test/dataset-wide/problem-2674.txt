Sure. The so-called "twin paradox" in special relativity trivially answers that question "yes". In fact, I believe they've flown atomic clocks on the ISS to experimentally demonstrate that kind of thing, $URL$ (and google coughs up lots of similar links). Your phrase "altering the physical properties" might need to clarify "properties" to accommodate relativity's postulates more directly. "State of motion (inertial frame)" or "gravitational potential" (for general relativity tests) would be more direct examples, but I'm supposing you'd accept those as "physical properties". 

Could be different. What you're asking about is the mathematically defined difference between a poset and a preorder. Both are characterized by a ≤ relation, typically called "weaker than". For a poset, a≤b and b≤a implies a=b, which is called the antisymmetry condition. For a preorder, a≤b and b≤a doesn't imply equality. You can google those words for some examples of preorders that aren't posets. 

Maybe think risk(to others) versus benefit(to you), and a personal "selfishness threshhold" defined by the risk/benefit ratio below which you'll comply with the request (and above which you'll refuse). And consider that in the context of alternative scenarios, e.g., your original... 

Regarding your "100%" one way or the other (and disregarding your particular issue of abortion), I'd think the first thing you'd have to decide is whether to formulate your law either as (a) "it's always legal except for the following exceptions when it isn't...", or conversely as (b) "it's never legal except for the following exceptions when it is...". There's kind of a law-of-the-excluded-middle exception here, though I'm not sure what it's formally called or how it's discussed, whereby the universe of all possible situations is too large and too fuzzy (both informally and in a Zadeh-like sense) to unambiguously decompose universe=is_legal U not_legal situations. Or in other words, formulations (a) and (b) aren't orthocomplements. If they were, then your question (be it about abortion or about any issue where the preceding discussion applies) would be much more straightforward. As it stands, first consider one formulation and then consider the other, as follows. For (a), can you clearly define the exceptions where it's not legal? And then for (b), can you clearly define the exceptions where it is legal? After considering both, choose the formulation with the more clearly defined exceptions. So that's just an elaboration of your "100%". And I'd bet there are formal textbook treatments of this kind of thing (i.e., pretty much anything you think up has already been thought up and fully developed), but I'm not familiar with the precise topic to google. 

I concur with Eliran H, so I'll answer mostly the second part of the question. First, I don't understand the pairing of Dawkins' "selfish gene" and Dennett's "consciousness explained". I am not aware of any collaboration between the two on either of these works. I know they did not coauthor the books, but I'm talking about collaboration for the concepts dealt with by them. I have read The Selfish Gene, but not Consciousness Explained. Because of this, if I am missing some connection between the two, please add an elaboration in the question for the benefit of all who have missed the connection, including myself. 

Robert Nozick, I believe, was the first to propose an argument for something like this, in his book Anarchy, State, and Utopia, pg. 169-172. I quote from pg. 169: 

The idea of like-minded people trying to start afresh is not really original either. But then, I don't see what this has to do with Italian Futurism. 

I am not an expert, but if you ask "how can you understand why they acted the way they did?", that's beyond the scope of praxeology. I quote from Mises' Human Action: 

Academic philosophy is open to discussing all sorts of questions. Your own question is of the form . However, this seems relevant only for theologians, generally not for philosophers. I'll cite just two of the many cases that discuss the . Should the baby live?: the problem of handicapped infants 

While one could say anything at all; neither was she, herself raised on any such tradition, nor did she try to mentor others into following such a tradition herself. Based on what I know of Rand, I'd say any such resemblance is only coincidental. From what I know about Rand, she does not seem to be a discursive philosopher. Her methods seem more like the methods of Marx or Nietzsche. 

The meaning is perfectly clear to the person who understood each of my words. It is true that I want espresso, in that context. It is false that I want sugar, in that context. 

If I may suggest an interesting alternative definition... The strange world of QM can be interesting if you define an object with respect to the way we think, rather than physics. Consider that nature really doesn't care if a table is one object, or five (a top and four legs). The way you define your objects matters when you start trying to predict what the table will do in certain situations (how will it break apart if I put too much weight on it?). This predictive value seems to be closely tied to our concept of what is an "object" and what is not; I consider this important. With this definition, I can go through a a series of historical experiments that lead up to QM. You will see that, as we go along, the meaning of "object" shifts. Physics purists: I'm certain I'm getting some terminology wrong. I am sorry in advance. Consider a lightbeam. It is a meaningful enough thing that we might dare to call it an object (or maybe not... but I'm going to call it an object). If you have a lightbeam pouring into a room, the room is lit. If there is no lightbeam, the room is dark. This is, of course, an approximation of reality. It is convenient to think of the lightbeam as a thing because we can use that to predict whether rooms are light or dark. This approximation is totally valid, up until the point where we start to strain it. The double-slit experiment is a famous experiment. If you shine light on a slit, and observe the light on a surface behind the slit, you see a diffusion pattern: a bright patch in the middle which fades to darkness on either side. If you shine light on two slits, side by side, you see two diffusion patterns. However, if you get your hands on some monochromatic light (only one color/wavelength), strange behaviors occur. "Fringing" patterns appear, alternating bars of light and dark, much unexpected from our simple lightbeam model. So we update the model, and have light travel as waves. We can do the math and show that the waves cancel out at the surface to generate these fringing patterns. We have had to modify our lightbeam definition a little: now each wavelength is broken out and has "phase" and "amplitude," but this is of minor concern. It's not that much more complicated, and it explains all we have seen. Now consider a strange effect: the photoelectric effect. We observe that some materials, metals mostly, will emit electrons when lit up by light (which, of course, is how solar cells work). However, experiments show that it has some curious behaviors. If you use too low of a frequency of light, it doesn't emit an electron at all, no matter how powerful the light source is. Use a higher frequency of light, and it starts knocking electrons off, even with faint light sources. Curious. The only way this makes sense is if light was quantized into little objects which knocked the electrons off like colliding billiard balls. We have to shift our definition of object... now there are billions of small objects, "photons," skittering about. High energy photons can kick an electron off of our metalic surface with ease. Low energy photons just lack the oomph. En masse, they sum to form wavelight behaviors, explaining the double-slit behavior. We've got a definition for an object that explains everything we've seen! This is where it gets odd. Let's go back to our double-slit, now that we know light is quantized into photons. We can construct a light source that emits just one photon at a time. We can record where the photon hits the surface behind the slits, then send another photon. If we repeat this process and overlay all of the impact locations, we expect to see two bands, one behind each slit, because the photon has to go through one slit or the other. Wrong. Classical physics hangs its head in shame, because that is NOT what we see. We see fringing, just like before. Somehow the photon is traveling through both slits at the same time, and interfering with itself! Quantum Mechanics can explain this behavior. QM states that photons are not particles, per se, but wave packets. They are waves that are localized in space, just like a staccato note played by a flute is localized in time. Within the note, there are vibrations, but they taper off as you approach the edges. If you model your "single-photon light source" like this, usually you will find its behavior to be approximately identical to the classical model. However, the double-slit setup amplifies the characteristics of this wave packet which classical modeling assumes "don't matter much." It shows that you should expect a fringe pattern, which you see. Freaky, isn't it. Note that, at each step, the "object" is whatever makes sense to help you predict behavior In 99.99% of your life, you will not have to think about light as anything more than a beam. A "lightbeam" is a useful object for you to work with. In a handful of odd cases, you will need to think of a light beam as something more than just one object. Perhaps you put a prism in the way of the light beam. In 99.99999% of your life, you will not have to think about light as anything more than a wave. However, perhaps you are interested in solar energy. You will have to think of photons as the fundamental "object" that light is made of. In 99.99999 999% of your life, you will not have to think about light as anything more than photons. However, if you play with the deep dark corners of physics, where new discoveries are found you wont have such luxuries. You will have to deal with light in the truest form we know: waveforms In 0.00000 0001% of your life, you will have to think about light as a quantum waveform. If you deal with experiments such as the delayed-eraser-double-slit-experiment, you will defecate building materials when you see the results if you try to think in terms of classical mechanics instead of quantum waveforms. That being said, in all other cases, simpler objects are fine. You don't always need to use the most exacting model. In 99.99999 99999 99% of your life, waveforms will be enough. Are you a theoretical physicist? How cool would it be to find that the QM definition of light is insufficient because you ran an experiment that got different results than QM predicted. Are you ready to redefine object once again? 

Your profile mentions an interest in math, which you seem to be pursuing in the formulation of your question. So let me suggest how physics approaches this same kind of question. Terminology-wise, your "object" is a "system", and its "properties" are the system's "observables". Then the collection of all possible states a system can assume is modelled by (the unit vectors of) a Hilbert space H corresponding to that system. And then the collection of bounded Hermitian operators B(H) on H represent all the system's observables, and this B(H) comprises what's called a C*-Algebra. Now, if you have two systems, A and B represented by H_A and H_B, then the composite system is represented by H_A x H_B, and the C*-Algebra of observables B(H_A x H_B) of that composite system is way more complicated than B(H_A) and B(H_B) alone (and way more complicated than their simple set-theoretic union). Now, regarding the comments, a property like "wetness" wouldn't really be a formal observable in the above sense (at least I'm pretty sure -- make that almost absolutely positive -- not). But if you're looking for a rigorous mathematical treatment of your question, it couldn't hurt (except for the mental time and effort) to google some of the above stuff and start with that kind of approach, which has been carefully developed by many people during the last ~75 years (I'm dating it since Irving Segal's 1947 seminal paper). 

Yes, he (correctly) means "a formal-symbol manipulating device". But "manipulation" needs a little clarification. The usual textbook formalism is given by the lambda calculus, e.g., $URL$ (google "lambda calculus" for lots more). But electronic devices natively implement only a much simpler (architecture-dependent) machine language, something typically more-or-less like Knuth's MIX, e.g., $URL$ (google "Knuth MIX"). However, a lambda-calculus-interpreter can always be written in any such machine language. Indeed, "Church's Thesis" and "Turing Completeness" (more terms for you to google) guarantee that just about any sensible formal idea of "manipulation" is ultimately equivalent to all others. That is, there's a class of so-called "computable functions" (google that), meaning that a sequence of symbols representing input (the function's argument) can be manipulated into another sequence representing output (the function's value). If no such manipulations exist, the function's not computable (google "halting function" for a non-computable example). And all computer languages, i.e., all formal ideas of "manipulation", can ultimately calculate exactly the same class of computable functions. So Searle's ultimately and equivalently saying that consciousness isn't (can't be simulated by) computable functions. But your comment, "...defining a computer as a symbol-manipulating device just seems to add fog to the AI landscape", seems to conflate AI with consciousness. AI, aka "expert systems", are certainly computable, but certainly not consciousness. And nobody ever claims any such thing. I think maybe your question arises from a misunderstanding about that. Edit... Hmm, now taking a look at your profile, I wouldn't imagine you'd have any confusion/misunderstanding about that. But then I don't see how you'd have any question about Searle's remark whatsoever. So what's your question, more exactly? Edit (reply to Roddus' comment below)... Firstly, for concreteness, let's please do away with this unnecessarily vague "voltage level" terminology, which you've used here and in preceding comments above. See, e.g., $URL$ for the correspondence between bits and voltage levels. We're talking about Searle's "symbols" (sequences of bits), regardless of their physical representation, which simply happens to be voltage levels in electronic digital computers. Regarding "meaning", with respect to computers, I'd guess what you might want to study could be "denotational semantics" (e.g., $URL$ and many other google hits) and "domain theory" (e.g., $URL$ ). Then it's the so-called "semantic function", which maps syntax (the denotation represented by strings of symbols) to semantics (the syntax's meaning represented in a so-called Scott domain), that captures the mathematical idea of "meaning" as it pertains to computability. The short paper $URL$ seems like a pretty good intro to me (I'm not googling any wikipedia-type stuff that comes anywhere near adequate). A longer-but-more-comprehensive intro is (seems to me) $URL$ And/or try googling "semantic function" denotational (put "semantic function" in quotes as shown, followed by denotational) for additional tutorial papers. As for Searle and his Chinese Room conclusions, you'd need to compare and contrast "meaning" with respect to computers, versus "meaning" with respect to consciousness. But only the former is well-enough-defined for any rigorous comparison. And if you're really interested, I think you might need to further study domains, maybe particularly the idea of approximation as represented by their poset ordering, whereby "meaning" can start out vague and become better-and-better defined with more-and-more syntax. But that would involve a pretty significant effort, far afield from any direct relation to "consciousness". So I wouldn't recommend it unless you're really, really interested. The theoretical computer science aspects might (again judging from your profile) be adequately interesting, but maybe not the related-to-consciousness aspects... Are computers (i.e., can computers simulate) "conscious"? Or can they exhibit behaviour indistinguishable from consciousness? Searle apparently says "no". Whether or not his argument's conclusive is maybe debatable, but his definition of "computer" is entirely adequate. Any further argument would have to discuss the ultimate capabilities of "symbol manipulation" -- just how far can that take you? And that's indeed somewhat of an open question. It's closed with respect to computable function theory, but meaning/semantics/etc not equally well closed. 

Youth and technology have been celebrated by an innumerably large number of people, especially the former. Rand abhorred violence, except in self-defense. 

By itself an utterance such as "Ouch!" does not carry a truth-value. Contrast "If I am experiencing discomfort, I will eat painkillers." with "If Ouch!, I will eat painkillers." That does not mean that a proposition cannot be derived from such utterances, the absence of any utterance, or utterances that convey the opposite of what is being stated (as is the case with sarcasm). But such things come under the purview of pragmatics, not semantics. This is why the following is plausible: 

To the extent that a philosophy of organism is required for a biology book, Dawkins already does that, with his (or Niko Tinbergen's) concept of "survival machines". 

Most Indic scripts are not as linear as Latin scripts. Devanagari features compounds and conjuncts where some letters modify other letters from left, right, above, below, or within. But why is that relevant? Indian languages can be written in a linear form as well, commonly with IAST or ITRANS. The same goes for mathematical expressions. Your two-dimensional example is often written in a linear way in LaTeX or Mathematica. 

This video gives a good overview of many kinds of implications. The source material for his lecture was mostly Jonathan Bennett's A Philosophical Guide to Conditionals. 

Electronics: We use boolean logic at the level of predicates. I've not come across propositional logic here. Some of the advances in the field are from finding optimal circuits for complex tasks. Linguistics: A few different kinds of logic are at the heart of many grammar formalisms such as CCG and Logical Grammar 

Epigenetics did not mean back then what it means now. Briefly, Waddington's "epigenetics" is roughly equivalent to the current word "phenotype". Far from ignoring it, Dawkins has written extensively on the topic. What is now called "epigenetic inheritance" is a very new field of study. I could expect a sequel to The Selfish Gene and The Extended Phenotype (by someone else) after the field of epigenetic inheritance and evolution is better established. 

Over time, the formal systems that are accepted shift to accommodate new discoveries or problems with old results. In the case of the infinite sets you describe, it was the shift to the set of axioms towards modern set theories that lead Cantor to challenge the conventional thinking. As a result, you could still write any of the old proofs, and claim them to be proofs, but they would no longer be of value because the newer set of axioms had become the "preferred" set of axioms. Proofs using other axioms were simply less useful. We also see this today with the difference between proofs in ZF and ZFC. Mathematicians are divided about the validity of the Axiom of Choice (the C in ZFC). A proof done in the formal system of ZF's set theory is considered more valuable than a comparable proof done in a formal system of ZFC's set theory. 

Not everything that shuts down our arguments is a named fallacy. In this case, I would call this behavior "efficient." It would be very reasonable to assume that an expert's opinion is more valid than that of someone who knows nothing of a field, would it not? Do we assume that the average traveler has a better opinion regarding how to fly an airplane than a pilot? Do we assume the average driver has a better opinion of how to maintain a vehicle than a seasoned mechanic? No, we don't. If one had unlimited resources, it would be highly reasonable to listen to every opinion and weigh them accordingly. A failure to do so might be called a selection bias (which could be as close to a named fallacy as you may get). However, we rarely (if ever) are so lucky. Accordingly, what you want to call a fallacy, I would call an efficient way to spend finite resources. The more limited the resources are, the more reasonable this behavior is. 

When I talk with people about science, the testing and experimentation sides of the discipline are what everyone tends to focus on. There's plenty of systematic observant methodologies out there, but we tend to think of the testing an experimentation process as the defining aspects of science. But that process has shifted over time. The idea of what it means to do an experiment and test hypotheses has changed since the 17th century. Early science was dominated by inducitivism. Inductivism was primarily built around using induction to go from a small number of observations to a universal law. It had its process of affirming and denying these inductive leaps, but it was more "free range" than we typically think of science today. Positivism restricted it a bit, focusing on models which are predictive, but it had a ways to go to reach what we consider science today. One of the more important shifts came Karl Popper in the early 20th century. Popper promoted the idea of falsification. Under Popper's thinking, you could make any claim you wish in science, so long as it was falsifiable -- there had to be a way to disprove one's statements. If I claim "all swans are white," that statement is falsifiable by observing a single black swan. In my opinion, Popper's science is what most people think about, but he wasn't the last philosopher to touch it. One of the problems with Popper's approach is that scientific observations increasingly became statistical in nature. And, in statistics, it is virtually impossible to truly falsify anything. That left Popperian science in a bit of a limbo. Thomas Kuhn provided the most successful follow-on to Popper's approach to science. Kuhn's definition of science included the scientific community itself, not just the experiment. He argued that science works in waves. There's periods of order where we're simply beating down statistical errors getting closer and closer to Popperian falsification, and then there's tumultuous periods of excitement where a bunch of theories get disproven and new theories (with less evidence behind them) come up in their wake. All of these approaches to science are substantially different, and that's where the vagueness comes from. And, if one wishes to push the concept of science further back than the 17th century, we find we have to have even wider definitions of the scientific method which start bordering on guess and check. We start seeing definitions that accept more and more of what we now call pseudo-science. And that's okay! That's the fun of working with definitions!