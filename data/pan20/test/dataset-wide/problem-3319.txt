-> This is a reachable possibility on the metahpysical level, if the world we live in wasn't necessarily our current one. Thus, could word this grammatical phenomenon as "when you switch tense, you switch levels": When you switch from present tense to simple past, you switch from one-world level to the level of different epistemic states in our actual world. When you then switch even further from present tense to pluperfect, you switch from little epistemic worlds within our actual world to the metaphysic level of different worlds out of which our actual state of affairs is only one world out of many possible worlds. In that respect, I find the work distribution between English simple past and pluperfect quite clever, the temporally closer one expressing a possibility in terms of - mentally closer - epistemic states, the temporally more distant one expressing a possibility in terms of - mentally more remote - metaphysical states. This is really just a hypothesis, but the explanation works for me. 

Computational linguistics is the modelling and processing of natural language with computers. Edit: If you have another ten seconds of time, it will help to add examples of applications: Computational lingustics can be used for automatic translation, communication between humans and computers (such as Siri or talking to your navigation system), autocorrect or language learning programs, but it is also used to improve search engines, spam filters, automatic summarisation and tagging tools, or even to help deteting criminals (don't forget that last one, probably less than 1% of linguists actually do that but it sounds pretty cool ;) ). 

Did you make sure you have understood what assimilation means in general, not only, as you said, "in this context"? I think this task is pretty straightforward if you stick close to the definitions and go through them step by step. 

The paper I referred to: Moschovakis, Y. N. (1994). Sense and denotation as algorithm and value. Lecture notes in logic, 2, 210-249. 

I could imagine that many homonyms start off as polysemes but can synchronically no longer be recognised as such, so originally the n:1 relation was unproblematic because there was a clear relation between the different shades of meaning, but over time (and in contact with other languages, new words, the need for new terminology, ...) that connection got lost and from a nowadays standpoint it seems implausible why "the language" didn't invent a seperate word for an apperently totally unrelated concept. 

Same here; with you have two terminals on the RHS, this is ruled out by T3, so this grammar is only T2. 

Now when disambiguating the English word or, we could make the following semantic disambiguation: or = 

Your first test is correct: The adverb "always" is - together with the negation "not" which modifies (negates) it - a constituent serving to modify the verb phrase (the being a result is modified w.r.t. time, namely that it is "not always" being one). Since the head of the constituent is the adverb "always", most syntacticians would call this constituent an adverbial phrase and locate it as an adjunction to the verb phrase. 4. sounds very weird to me; I'm not a native speaker though. Looks like the part starting with "always" was topicalised, i.e. moved to the front of the sentence to make it the sentence's topic (where the "not" is in focus). Might be that this information structuring legitimates such a construction, but it is at least very marked I'd say, since the adverb ("always") was detached from its negation ("not"), making it hard to reconstruct the constituent. 

It is the most natural case that languages have word categories that do not behave precisely the way that they do in the classical understanding of the terms motivated by IE. Given the huge variety of languages across the world, it would be extremely surprising if every language had their nouns, adjectives etc. behave precisely the same way as English does. However, within an individual language there will be several clearly distinguishable catgories, where ascribing properties such as "adjectival" and "nominal" is indeed not too unjustified, because such a categorization accounts for how different jobs are distributed across catgories in a language. Every natural language will have some way of expressing events, entities, properties, etc., and every language will use a minimum of different categories for these functions. Whether this distribution is precisely the same as in a prototypical IE language (if there is such a thing as a "prototypical IE language") is questionable, but most primitive notions like "verb-like" or "noun-like" are something that every language exhibits. So, even though the resulting pattern might be im some way "shifted" with regard to the precise nature of the individual components as compared to how we understand them in English (because linguistic terminology is hardly ever language-independent), in a more liberal understanding of these classification devices, Japanese "adjective" and "nouns" do fulfill the roles as we undertand them in an abstract sense, i.e. w.r.t. how gramatical functions are realized by different word categories within a language. 

Your second question is the answer to your first question: The dots indicate the head node of the respecitive phrase. For example, 

Rule notation For the rule notation, you could start out as follows: /n/ -> [m] / _[b] To make the rule a more formal and generative one, you should now try and abstract away from the concrete phoneme lables ([n m b]) by using distinctive features instead (such as [+cons], [LAB] etc.), but I hope you will be able to do this by yourself from what I wrote. 

As you correctly figured out, and start with a preposition followed by a noun phrase. This is called a prepositional phrase (= PP): The head (= the element which determines the grammatical properties and the main meaning of the phrase) is a preoposition, and the noun phrase is a complement that is selected by the prepositional head. But PPs are not the only elements that "extend otherwise basic clauses": Consider, for example, 

The magic lies in the word any: This condition triggers a universally quantified formula. What you thus end up with is 

Assimilation in this example To solve the exercise, you now just need to check those conditions step by step: 

Apart from my infantile handwriting, I'm not entirely happy about the picture yet especially because I haven't figured out a neat way to place phonetics so just dropped it - so please, everyone, feel free to criticize! - but I'll explain why I'm happier with it than the "classical" graph: IMO it's not very accuate to depict semantics as an extension of syntax. There is a crucial difference in what phonology, morphology and syntax do and what semantics and pragmatics are concerned with: The former are about form, the latter about meaning. This is a distinction that is sometimes disputed, but nevertheless still quite apparent in the way the different fields are investigated. There is a fine line between the study of the internal structure of words and that of phrases, which is why I tried to make the transition between morphology and syntax a bit blurry - some linguists might say that morphology is just word-syntax - and the way from phonemes to the items that are composed of them is quite plausible to depict as an "on top of" relation, but the step from syntax to semantics is rather different. It's not just shifting the zoom level of the elements whose structure is being investigated, but it's a whole different question that is tackeld by semantics an pragmatics: It doesn't ask about how linguistic elements are shaped and inherently structured, but how to interpret them. I therefore found it more accurate to display these areas as two rather parallel fields of study that interact which each other (this is what the lines between left and right are supposed to mean). On top of that, I find it not very useful to display semantics as the thing that comes after syntax. Semantics starts at the level of words and extends over phrases up to sentences, and should therefore touch the structural level of words. Pragmatics can only meaningfully done at the level of larger phrases and sentences, and extends beyond the sentence boundary to the level of discourses. I tried to make the vertical alignment accordingly. Again, the boundary between semantics and pragmatics is blurry. Phonetics is a bit difficult. I find the placement just below phonology not good enough; this would make it look like phonemes are a larger unit that are built up of speech sounds just like what we call words are made up of the units called phonemes, or phrases are put together from words. Rather, phonetics and phonology look at the concept of "sound" from different perspectives - one is concerned with "what comes out of your mouth", the other is concerend with "what is in your head". W.r.t. how research is done in the two fields, the questions that phonologists are concerned with and how they are approached are conceptually much closer to what is done in syntax and morphology; phonetics is not primarily interested about the rules and patterns that govern the inherent structure of linguistics, but rather on their physical properties. So maybe one should place phonetics and phonology more parallelly to each other, but in the end both are concerned with form rather than meaning, so this is a bit of a tricky one. As for the labels, I don't find the graph you showed too inaccurate. (Interestingly, the descriptions I chose are almost identical to your picture - it seems like the classical definitions of the fields are very persistent.) One might argue whether "speech sounds" precisely captures what phonetics is about in contrast to phonology, the addition of "speech" is very important here. As for syntax, one can decide to drop "sentences" if one assumes that sentences are also just kinds of phrases, but this is hairsplitting. I do agree with the summar "literal meaning" as opposed to "meaning in context of discourse". Eventually, this is what usually separates the two fields: While semantics is interested in a precise account of the meaning of linguistic entities as they are, it doesn't quite care about what could be there "between the lines". Subjects of linguistics that raise questions concerning speaker's intentions, dynamics of conversation, linguistic economy etc. have established as a field on their own, while there obviously is still a close connection between research in semantics and pragmatics. (For example, presupposition theory is usually regareded as an issue of pragmatics, but the way it is handled, it is actually a very semantic topic.) And pragmatics did take a long time to establish as a field on its own (one of my professors used to say "There is no such thing as pragmatics, there's good semantics and there's bad science." I won't attempt to comment on that here.) The element that is reponsible for when this distinction comes into play is probably best described as the "context of discourse", leaving semantics as the field concerned with the non-contextually-modified, more literal meaning, so I'd agree on these lables as long as I don't find anything better. In addition, as already addressed by Mitch, the graphical representation with circles seems to suggest an inadequate subset relationship (i.e., the questions that phonology is concerned with is part of the questions that syntax is concerned with, for example), which is inadequate. Rather, at least in as far as the left half is concerend, it should be a kind of mereological relationship (phrases are made up of words are made up of phonemes). The relationship might mot be as parallel for the right half, but pragmatics could be seen as made up of semantics + conversational context (at least pragmatics can't be done without semantics, just like syntax can't be done without words). This is why I chose ellipses that are stacked on top of each other (in hindsight I think rectangular blocks would have been nicer because they can better be thought of as being piled up from bottom to top), while allowing for some overlap/blurry borders in order to show that they are not entirely separate and sharply distinguishable from each other. tl;dr: What I find most inaccurate about the illustration is that semantics is depicted as the thing that comes after syntax, which is inaccurate in that 1) the study of form and the study of meaning are fields that exist next to each other rather than on top of each other, 2) semantics starts at the level of words already, not after sentences. The placement of phonetics is difficult; labels can in some respects be argued about, but for a three-word description, I think they are good enough. 

If AdvP immediately dominated two Advs, what would the head of the phrase be? There must be one distinct head, and in 28) this could be either of the Advs, there would be no way to clearly decide which node is the head of the phrase. Therefore, you specify which adverb should be the head of the AdvP (here it is quickly, because this is the "main" adverb that is again modified by another adverb) and the issue is resolved, since the AdvP containing the adverb very can in no way be the head of the adverb phrae. 

Compared to that, Norwegian, being a national language with 4,741,780 speakers according to Ethnologue, is huge. A "medium-size" language might be something like Chiqui√°n Quechua with ~10,000 speakers: 

It may seem unintuitive at first sight we we would need a universal quantifier here, but it helps to think about it in terms of the recursive evaluation of truth conditions: (4) is true if the implication is true for all assignments of the variable to some individual in the domain. Now go through the possible assignments step by step: Under the current assignment, is both a donkey and owned by Jake? No? Then we don't care - it could be beaten by Jake or it could not; in any case it doesn't matter because we are only interested in whether those individuals that do satisfy the antecedent also satisfy the consequent. Therefore, the implication gets true for those individuals which make the antecedent false. Yes? Then we found a donkey which is owned by Jake and need to make sure now that it is also beaten by Jake. Thus, for those assignments which deliver an indivdual that satisfy the antecedent of the implication, the consequent must hold as well. As soon as we find one counterexample (i.e. some such that is a donkey and owned by Jake, but not beaten by him), we know that the sentence must be false (because it says that if there is a donkey that is owned by Jake, then it must be beaten by him). If, however, all such that satisfy the antecedent also satisfy the consequent of the implication, then the sentence is true, no matter how many of them are there. If these conditions succeed for all assignments, i.e. any individual we look at is either not a donkey of Jake's (in which case we don't care) or it is beaten by Jake (in which case the proposition about Jake's donkeys applies), then the formula becomes true. Note that this formula now doesn't come with an unwanted existential force either: In case that there just isn't any donkey that is owned by Jake, under all variable assignments the antecedent gets false, the implication true and thus the universally quantified formula is vavuously true - so it doesn't hurt if there just aren't any donkeys that Jake owns, as long as if there is one, it gets beaten, which is what the original sentence states. 

As you can see, only 5% of langauges have a million speakers, only 1.3% have at least 10 million, and a mere 0.1% of languages worldwide has more than 100 million speakers. This is statistically nothing. The slightly-below-median, with 44.4% cumulative, is at 10,000 <= n < 100,000. The other slightly-more-than-half counts less then 10,000 speakers. Of course, Greenlandic or Faroese seems tiny compred to something like English or Mandarin with several hundred Million speakers, but these 0.1% (!) can not be taken as a reference of measurement. The vast majority of the world's ~7000 languages counts significantly less speakers, as the above table shows, and relative values like "small" and "large" should be normalized by the total distribution of speakers across languages rather than exceptionally extreme values like Spanish or Standard Arabic. As a rule of thumb, if even a non-linguist knows about a language's existence (like Norwegian, Greenlandic or Faroese), it's probably relatively large ;)