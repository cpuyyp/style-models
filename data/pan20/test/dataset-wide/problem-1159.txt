This is a bit of a broad question but I am trying to understand a storage performance behavior with two of our servers. I was following $URL$ On one server I ran dskspd with the below parameters on the same disk as the DB. 

How is your application connecting to the SQL Server? If it is not a direct connection and involve two hops(for example lets say you have an web tier that goes through an App tier and then to SQL Server) it might be related to kerberos authentication. You can check if if the connection that were coming into the principle server for the application were NTLM or Kerberos. Please check the below link Using Kerberos Authentication with SQL Server If it is, that means that your principle server was configured correctly to facilitate kerberos authentication and you would have to do the same thing for secondary server. Register a Service Principal Name for Kerberos Connections Another not so ideal situation could be that someone has added as login on the primary server, you could technically make it work by adding the same thing to the secondary server, but it is not recommended. 

So the first one is querying from a source database and inserting into target staging table. Staging table is truncated every day before loading. Below is the plan. $URL$ Now the target table and the source table have the same primary key which is an identity column. About half of the time the SSIS package fails with primary key violation on the target server. I am not sure why this is happening. What are the scenarios when a query would read duplicate primary key. Or is it some issue in the way SSIS is loading data? Initially I was thinking that a scan combined with page split is causing the duplicate read but the table which is scanning has an identity as primary key and its mostly inserts. 

Since I can't see your table structure and I don't know the size of your data, here's something that will return the right answer but may or may not need to be tweaked for performance. You can write a query that groups both created and closed tickets together by date. Although you could just write two queries and do a full outer join, I would guess you want SSRS to accurately represent days with no ticket activity in addition to filtering the data for the date range at the source instead of in the presentation layer, so I would suggest creating a date table/CTE/query. If you already have a date table, use it. If not, here is a way to create what you need for the last 30 days. You could also make it a stored procedure and parameterize it to choose the number of days. 

You cannot use an aggregate like CountRows() in a calculated field in your dataset. But you can use CountRows() as an expression in a textbox (alone or within a table) scoped to your dataset. Now that you have your dataset created, you can put a textbox on the report and populate it with the expression , and it will provide the correct answer. Otherwise, you will need to modify your dataset to include the rowcount. It would seem that using the expression in the report would be the desired option as modiying the dataset would give you a column populated with either a running total or the total rows repeated on each column. 

This morning i was woken up by a transaction log full alert on one of our database. This server is an alwayson cluster and also a transactional replication subscriber. I checked log_reuse_wait_desc and it showed logbackup. Someone had accidentally disabled the logbackup jobs 4 days earlier, I re-enabled the log backup job and the log got cleared. Since it was 4am I thought I will go to office later that morning and shirnk the log as it has grown to 400GB. 10AM- Im in office and I check the log usage before shrinking and it was around 16%. I was surprised and check the log_reuse_wait_desc, which showed replication. I was confused cause this was a replication subscriber. We then saw that the db was enabled for CDC and thought that might be the cause, so disabled CDC and now the log_reuse_wait_desc shows AVAILABILITY_REPLICA. The log usage meanwhile still steadily growing and its at 17% now. I check the alwayson dashboard and check the sent and redo queue and both are virtually zero. I am not sure why the log reuse is showing as AVAILABILITY_REPLICA and unable to clear the log. Any idea why this is happening? 

I know I can just change the connection string to remove the property, but how do I remove the configuration from within SQL Server? For example, if we query , how do I get an empty result? 

What I am trying to understand are the key locks on the non-clustered index(indid 2). Why are there two key lock on non-clustered index? If I check dbcc page on page id 248, I could locate the obvious one((1bfceb831cd9)) which is the lock for the entry for the record 6 which got changed to 7. Output of DBCC PAGE below 

I'll Start with the second question The reason for the 'missing index' advice for Plan 2 is the subtree cost calculated The parameters passed results in a subtree cost (right click/hoover on the nested loop join) that is 10 times higher than that for Plan 1, and it is because the estimated number of rows is 149, instead of 16. What is suggested as missing is a covering index, so that it doesn't have to make 149 keylookups to get the missing values from the index. For Plan 1 it was considered ok to make 16 keylookups What you are experiencing is what Erland Sommaskog is calling a “morally equivalent plan” Kendra Litttle explains it in this article $URL$ Using the feature of Query store to force a plan, but i believe it is the same when using KEEPFIXED PLAN As to why several plans for the same query exists. Many reasons including restart, cache getting cleared due to memory pressure, changing the structure or schema of a table referenced by the query. This will cause the plan to be recompiled Normally also updating the statistics, but since you have KEEPFIXED PLAN that is not he case. Having KEEPFIXED PLAN can cause performance degregation since as your data changes, the statistics change and the optimal plan may be different, but KEEPFIXED PLAN is forcing the same plan all the time. i am guessing, but is the setting 

The article mentioned is almost what you want, i think If you join with , you get what you need from columns and : 

So how does an estimate of 404986 going into the sort operator comes out as an estimate of 100? Is it just an arbitrary number since it cant sniff the variable in the TOP operator? 

This might be a permission issue. I am assuming that the monitor server is not on the primary and the issue could be that the primary server is not able to update the values in the monitor server. Can you check where your monitor server is and make sure primary server service account has permission to update the monitor server msdb. 

From my testing I do not see an option to encrypt a symmetric key with Database Master key. Isn't the Database master key a Symmetric key. What do they mean by public key of th database master key? 

I have a transactional replication which was initially synced from backup. Now I need to add a new table which is really big so we have decided to backup and restore a fresh copy of the db to subscriber to re-intializing it. My question is, in this scenario should I be dropping the subscription, backup restore and then re-add the subscription? is that the correct way or is there any other way of going about it? Thanks 

and got around 1400MB/s I was also able to get comparable throughput using a T-SQL query as below and calculating the throughput from the number of reads and time. I got this from Glenn Berry SQL Course on PluralSight "Improving Storage Subsystem Performance ". 

You can put your cube formula in a separate cell and reference the cell where users type in the name they want. For example, if you have users put the branch name in cell B2, you can put this formula in another cell. 

If you are browsing in Excel and want to be able to see dimension attribute members that do not have corresponding rows in a fact table (measure returns nothing), there is a pivot table setting you can use. Access the PivotTable Options dialog box by right clicking on the pivot table and choosing PivotTable Options. On the Display tab, check the box next to Show items with no data on rows. This will show all values of whatever hierarchy is in your rows regardless of if the measure in the Values section of your pivot table returns a non-blank result. You can do the same thing for columns by checking the item underneath the one indicated in the image. This option must be set for each pivot table and will apply to all fields in the rows (or columns) when you check the box. 

What are you trying to achieve with more than two axes? If at the end you just want those fields/values to be dumped into a tabular result, you can rewrite your query to use crossjoins. 

You can back up and restore SSAS Tabular 2012 databases, but you will have to adjust the security role memberships to include users in the correct domain. You can perform the backup and restore manually or through a script in Management Studio (or executed via PowerShell). You can also re-deploy an SSAS database from the SSDT project or using the SSAS Deployment wizard. The wizard will allow you to deploy roles and ignore members. Next populate the role memberships with appropriate users. From there, you can process the model to populate it and bring the model online. Another option is to script the SSAS database from Management Studio. Once you do this you can remove the collection from each role and execute the XMLA on your target server. 

Did you have a look at in ? We had a similar issue and I have asked a similar question. Unable to truncate transaction log, log_reuse_wait_desc - AVAILABILITY_REPLICA 

What I am trying to understanding is what the purpose of the other key lock(5ebca7ef4e2c) is and what its locking. 

In simple recovery mode the log gets truncated automatically when checkpoint is issued. Now when you run DBCC SQLPERF(LOGSPACE) and your log is more than 70% full then you can assume that your log is not getting truncated because a checkpoint is supposed to be issued when the log grows to 70% in simple recovery. If you suspect that your log is not getting truncated in simple recovery you can use sys.databases, log_reuse_wait_desc columns to find out why the last log truncation did not work. If you wanna dig a bit deeper, you can use dbcc loginfo and check the status column to see which virtual log records are active and in that way figure out if its getting truncated or not 

That is not true. if the procs are running on the secondary replica it will be cached. It could mean either that the stored procs are not running or its getting evicted out of cache and are not in the cache anymore. Off course querying the cache does not give you a complete picture and It might be better off to use profiler or xevents to track all the storedprocs. 

Read about the features of PGBadger, and with those features, Query Store is definitely the way to go, and i have only a few things to add to the answer from Dan. The data in the DMV's are reset when the server is restarted for whatever reason. This is not the case for the QueryStore and is one of the big features of this. Also for the DMV sys.dm_exec_query_stats, some of the data can be flushed if the server gets under memory pressure. In SqlServer 2017, latest CU, the query store also contains wait statistics, which you would need to help you understand why a query is regressing. This is not the case in SqlServer 2016 (hoping for a future SP to include it) You will therefore have to setup some logging of wait statistics on your own. i have build a version of the code from this article to do that: $URL$ (be sure to read the articles he is linking to as well) If you are on an earlier version of SQL Server, you need to collect the query statistics yourself (the data from sys.dm_exec_query_stats) on a busy server, this needs to be done rather frequently, since the data can be flushed in pressure situations as i mentioned. As Dan, i am not sure what you are wanting to use the 'duration' for but if it is for finding ressource/query issues, you need to monitor more numbers and changes in numbers, like I/O statistics, Wait Statistics, Memory Usage and more before going into to much detail, it would be nice to hear what exactly you are trying to achieve For a complete out of the box tool that can deliver all of the stuff that PGBadger delivers, you will have to get a commercial product like the ones from Idera, RedGate and others