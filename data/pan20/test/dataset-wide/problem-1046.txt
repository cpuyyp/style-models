Welcome to the world of Database Administration... and good luck. You're going to want to read up on as much Oracle documentation as you can, as well as other good technical sources (O'Reilly has always been good), and subscribe to lots of Oracle blogs. I'll answer your questions here, but you're really going to want to get a solid foundation in RMAN ( For 10gR2: $URL$ ). 1.Do you have to shutdown your Oracle DB when you want to make a copy/clone of it? Yes and no. It depends on if your database is in archivelog mode. If the database is archiving its logs, the backup can be done while your database is online, though you may notice some performance degradation during the backup, so it is still a good idea to schedule the backup during a non-critical time of day. If the database is not archiving the logs, then you must shutdown the database cleanly in order to make a copy of it (any other way will result in a corrupt/incomplete restore). 2.A good beginner example This is hard to do without knowing your environment. There are various RMAN commands that will happily clone a database on the same server, but when you get into moving a clone from one server to another, you have to go a different route. So without knowing your environment, I can't really tell you a good example. That said, essentially your goal is to do the following: - Get a good copy of all your datafiles - Get a good copy of all your archive logs - Get a good copy of your database parameters and control file - Create a new database with the settings from your old database (or reuse an existing database) - Copy over your datafiles and archive logs - Restore from your controlfile, and recover until there are no archives to process. RMAN does a lot of the work for you with some fairly simple commands, so I suggest learning how to use it effectively, but you can always do the hard work without it. (For a long time at a previous workplace, we did this with some shell scripts and such. Fun it was not, and was easy to screw up.) 3.Does a GUI exist for cloning an Oracle Database? I think the thing that comes closest would be Oracle Enterprise Manager (GRID). It offers several automated backup/restore options as well as cloning features. Keep in mind, however, that it uses RMAN under-the-hood, and you should never use a tool blindly without understanding what's going on underneath. Sooner or later, you'll need to delve down into the command line because the GUI won't do what you want it to do, and without a good understanding, you'll be stuck. 

Then, when sending to a group, I would go ahead and add an entry to each user's inbox with a pointer at the message. This way you aren't duplicating the message content, but you simplify the system by allowing users to perform their own actions on group messages without affecting the group message for the group as a whole. Not sure it is a complete answer to your issue, but I would go this route since it means that message management is always a user-level issue; only during send operations do groups need to enter the equation. 

Pipelining is such a useful feature that it is a shame it doesn't translate across the db-link, though. Maybe Oracle will, one day, see fit to fill that particular gap. Otherwise, though, I fear you've got your work cut out for you. 

Tough one, given that you have no access beyond a SELECT in db_A. So here's a thought, but it requires some pretty strict assumptions that may (or may not) be met: Requirements: 

It really isn't a database platform problem. All the major databases and operating systems will do fine provided sufficient hardware with sufficient bandwidth to memory, disk, and network are provided. All databases are built for this kind of scenario -- that is, where you need to update lots of rows from different clients all the time. This is what indexes and primary keys are used for, and the database is optimized for updating in this manner. (i.e., UPDATE your_table SET your_column=your_value where your_key=12) The hardware requirements are going to be your biggest issue, and I suspect you will need to think about quite a lot here, including: 

Swap out the OS, File system, and DB as desired. Your machine should scream and be just fine for a good number of databases, and is probably a bit overkill. Best thing, though, is that you can always add memory and disk space and upgrade your processors if you need more. If necessary, move to a new box, or add boxes (if your database supports it). (Even better: run your environment as a Virtual Machine/Cluster. Then moving to another physical machine is child's play.) 

Thankfully a good majority of the scaling issues are handled either at the hardware or db level. That is, you can start your database now on a slow server and as traffic increases you can adjust accordingly with minimal changes to your code or your db structure. What will change is your hardware and your db configuration. Most database servers will support failover/clustering with little to no change to your existing code or structures. (But be sure to research possible costs and the efficiency thereof first) 

I imagine you can go anywhere you want to go. But real-world experience counts too -- not just what's on the exam, so don't expect a high-level position from the start. You'll probably have to start as low man on the totem pole, so-to-speak. Having been a DBA, I can tell you several things: 

Usually a limiter goes into effect, but you get the idea. Furthermore, if data changes on a record, the sync_date column is NULLed, at which point the sync process will pick it back up again. Note: no matter the situation, you will need some sort of de-duplication handling if you are able to support data changes once a row has been sync'd. You could try a MERGE, or an INSERT with a WHERE NOT EXISTS on the SELECT clause coupled with an UPDATE ... WHERE EXISTS. Hopefully that helps. 

Really, pick a database, an OS, a file system, and build your app. With good hardware and good table design (with indexes and optimization), things should go swimmingly. All that said, here's what I'd pick (only because I'm most familiar with this type of configuration): 

By having the table CustomerOrders, you satisfy the ability to map common customer order data to common fields (simplifying reporting at the expense of making the import a little more painful since the fields must be mapped), and the CustomerOrderFields gives you the ability to have the custom fields per customer necessary for the un-mappable data. The custom fields are still reportable, but not as easily as your generics as they'll come to you in multiple rows (instead of multiple columns). There are some ways around all that depending on your report creator (e.g., pivoting the results). The only other option would be to do something like this (which, personally, I would avoid): 

Again, this only works if you have either a sequential unique ID OR an activity date that is always updated on db_A (and that date should be of sufficient resolution to detect one transaction inserted a millisecond after the previous one, so timestamps are best.) The way I synchronize data between Oracle instances (and non-Oracle instances, e.g., Oracle to mySql) is to make sure I have a sync_date column on all my sync'able tables. When a request is made to sync data, that sync_date column is filled in with the date of the sync. Therefore the actual sync process is simple: 

Note that there's not really anything here that speaks to if the data is valid and/or accurate according to your business logic. This logic is anything you put into triggers, procedures, validation routines, etc., that you build yourself to try and make sure that everything going into the database is good. (It should be noted that Oracle's check constraints could be considered a form of business logic that is enforced like a unique or primary key constraint). The database will happily reject data that doesn't fit into the above criteria, assuming they are defined, but will also happily accept invalid or inaccurate data that isn't otherwise in the above criteria. If I have a field that contains people's names, the database can't make the determination if "John Smith" or "Jonh Smiht" is accurate (notice the typo). So while the database will say everything's good, you know from a visual inspection that it isn't. (This example, while trivial to do, is hard to do anything about. After all, how do you know his name isn't really spelled this way? I use it only as an example of any data coming into the system. A field expecting percentages could easily have 5% as 50% -- how can the database tell which is valid? It can't without you writing lots of complicated business logic, and even then -- at some point, how do you know it isn't 50%? Or that it isn't 5%? Or that it was supposed to be 25%, and the typist got really confused!) Use the database integrity mechanisms to your advantage, but don't assume that because there are no integrity errors that your data is good. It can still, easily, be bad and misleading. Further, don't assume that your business logic ensures good data either. Yes, you can prevent obvious errors (like, say, a field that shouldn't have values out of the range of 0 to 100), but once ranges are satisfied, at some point, you have to trust the user who is giving you the data. And at some point, the user will give it to you incorrectly. Poof, bad data, with perfectly valid database integrity.