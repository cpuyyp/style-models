How does Oracle perform in hyperconvergence systems such as nutanix, simplivity? Oracle doesn't certify the underlying storage which nutanix or simplivity offers (file level storage) but in reality case studies say those systems perform good in general but with Oracle databases, technically how sound it is to go ahead with it? 

What is the overhead of RMAN hot backup (full backup including archive logs) being performed during business hours? Does it actually affect the performance of the database? What is the case when incremental backups are taken in terms of overhead? 

I have an Oracle 11.2.0.3 about 700GB in size running in noarchivelog mode in a pretty slow storage setup. It barely gives IOPS and there is resource crunch in terms of hardware resources. The transactions hitting the database is heavily read/write intensive and redo generation is quite high (near to 8MB/Sec) and naturally it is finding difficult to write to the disk. What can be done in such a scenario? Can nologging in certain tables boost the performance? Data corruption because of nologging is a concern, I understand that and can be taken care of. But can someone give insight of how truly enabling nologging helps in this situation? or if it doesn't help, what are the other options I have? 

Postgres comes, out of the box, configured as a general purpose database for both OLTP and OLAP types of loads. My need is to do use Postgres, in this case, purely as an OLAP running aggregate queries over few relatively large tables. I am looking for instructions how to adjust default configuration parameters relating to buffers and cacheing, including OS settings (Linux) for page sizes, so that the database can handle large sorts, joins, projections and filters more efficiently than out of the box, given the resources on the system. I have seen several tuning and tweaking best practices but these are still general purposes (addressing WAL, transaction synchronisation, etc.), where I am mostly interested in maximizing read, sort and join performance by adjusting the parameters that related to these. I have a 4-core server with 64 GB of memory and about 500 GB of SAS-2 drives. My dataset consists of about 8-10 tables of which some are 100 MB, and some are about 20 GB. This dataset will be used for read-only analysis with direct querying of the individual tables, or some complex queries joining two or three tables. Tables should be partitionable by month or year, at least. Given the hardware I have, and the fact that we are accessing few relatively flat but large tables, could someone please recommend some basic but specific Postgres 9.4 or 9.5 optimizations that would maximize the performance of the read-only queries and the database. 

They both do what is essentially a "Compact and Repair" into a new database file. (The original database file is not compacted, only the resulting copy.) 

The only method that didn't fail outright was to pull the tables from MySQL into a Microsoft Access .mdb file (!) via MySQL ODBC, then import that into SQL Server. However, even that was an incomplete solution because the transfer from MySQL to Access omitted all of the primary keys, indexes, and AUTO_INCREMENT attributes. Still, it did transfer the tables and data, which was my main objective. I just had to write some T-SQL to re-create the indexes and whatnot. Since you don't need to transfer any data you might want to consider just hacking the mysqldump SQL code (good old Find & Replace...) to the point where SQL Server will execute it. 

Yes. The Access Database Engine is designed to work with "real" Windows file sharing and Samba – or at least some versions of Samba – apparently do not implement all of the low-level features of the SMB protocol that are needed for the Access Database Engine to work reliably. (ref: Corrupt Microsoft Access MDB Causes - Samba) 

I have run mysqlcheck on all the databases as well which returned 'OK' for all the tables. Not sure where to start debugging. Any ideas? 

I have 4 servers running MySQL 5.6. A,B,C and D. A and B are Master and slave whose default port is 3306. C and D are running in non-default ports 3360. A and C are in Master-Master replication mode. C and D are in Master-Slave setup. Now I need to change the ports on A and B from 3306 to 3360, what commands I need to run and in what order to have B,C and D back in replication? Is the below steps right? 

And also, while I type change master to master_port, should I write the full commands with master_log_pos, username, pass and then port? Or just the port is enough to restart the replciation? 

I have checked the compatibility matrix, it says the RMAN client and the target database has to be the same version. But I'm using Catalog as 12c database therefore the client is 12 as well. Should I use RMAN 11g client to connect to the target? If yes, How should I do that? 

Unfortunately, no. The basic Find (Ctrl+F) and Replace (Ctrl+H) functions in the Access UI search the resultset of the form row-by-row. For an ODBC linked table (any ODBC linked table, not just PostgreSQL) once Access has gone past the end of the rows that it pre-fetched when it displayed the form it retrieves one row at a time until it finds a match or reaches the end. (Find and Replace is often considerably faster for Access linked tables - as opposed to ODBC linked tables - because the Access Database Engine works directly with the database file, retrieving 4KB pages of data at a time. If consecutive rows happen to be on the same page, or if the 4KB page for a given row has recently been cached, then Access can "get" that row without another round-trip to the database file.) However, there is one workaround that may be helpful: If a Filter has been applied to a Form or Datasheet view then Find and Replace operations are restricted to the filtered rows. Access can take advantage of indexes and (at least some) server-side processing when applying a Filter to a form bound to an ODBC linked table, so you might be able to get the job done faster by Filtering the records first (even a non-sargable filter such as ), and then performing the Find and Replace on the filtered set. 

What is the difference between having a master-slave replication and having a master-master( where in the second master acting as hot-standby - active-passive) replication? Also, what are the issues that I could face having a master master replication where both the masters are writable. I have read here that experts have said it is very dangerous to have both masters writable. Without Percona/NDB/DRBD, what are my options for setting up HA for my databases? What is the best replication that I can have? 

I need a drop a database which is a 2 node RAC configuration and I will also be drooping the disk groups as well completely. What is the right way to drop the database + diskgroups? Will it be ok to shut down the database and then drop the diskgroups including contents thereby dropping the database as well? In this case, the cluster will still have entries of the database which is dropped? 

I'm running a master-master replication with read-only second master. Now since my binary logs are not getting replicated in my passive master. 

It looks like there is a (finally) an open source columnar and compression extension for Postgres. I wonder if anyone had any experience with it yet? I've worked with Greenplum for years, but I've been looking desperately for the open source alternative. 

I am a big fan of Postgres both for its price but also for its features. I am going to have to need to upload into it both Oracle dump and SQL Server files. I will try to ask and beg for plain .csv for schema DDL but I suspect that I will be given dmp files. Is there a tool, most preferably open source one, that would allow me to read, profile and possibly load Oracle/SQL Server files into Postgres? 

Is there a best practice or coding/design conventions for Postgres both DDL and DML/SQL that you could share and recommend. I am looking for something similar to what Google has for programming. Thank you in advance. 

I was given a compressed folder that contains number of subdirectories, and, among others, directories named .pgdir with number of numerically sequenced files in a <9999>.dat.gz format and one in a toc.dat format. I cannot post the content of the files, but when I uncompress the <9999>.dat.gz, they each contain pretty structured, tab-separated tabular data with \N [sic] at the end line. toc.dat has a mix of binary and strings, starting with PGDMP where I can recognize some postgres DDL statements, schema creation. etc. To me, this looks like a postgres backup, but I am not familiar with this layout and how to restore it. Is anyone familiar with these structures, and how could they be used to restore the database? 

Is it mandatory to use FCF with AC? If yes why? If no, I should be fine I guess. Are there any specific changes I should make in Weblogic server if I'm using AC? My Weblogic server could be a cluster/standalone and i'm using normal data source only. 

I recently was going through Application Continuity in Oracle 12c. As I read through the Oracle link which I understand when used in RAC, will be able to replay the transaction (DDL/DML) in the event the node goes down. In the link, it also mentions creating a service for non-RAC (standalone) instances as well. How does Application continuity work in case of single instance. 

Is it okay to have two oracle clients (both 11g and 12c) in the same db server? What are the potential issues I could face? The reason im doing this is to connect the app server where I'm deploying the clients to two databases each running on 11g and another on 12c. 

The header of disks are in PROVISIONED status. What is the issue here? Can't I create new diskgroup with same name? How to clear the disk header?