Here is an example of the kind of rich information that can come from using one of these tools: I helped a client implement mk-query-digest to report the 20 worst-performing queries every 20 minutes. I got the idea from this YouTube video. The client would move any bad query's output to memcached thus lowering the incidence of the query's taking a toll on the the database. Here is the script I made to call mk-query-digest (examing the processlist only) 

All stored procedures and stored functions, whose code is written in the MySQL Stored Procedure Language, reside in . You can see how many of each with this 

Step 05) This will take awhile (5 min) because mysqld will create ib_logfile0, format it, create ib_logfile1, format it. Step 06) Login to mysql and load the conversion script 

SUGGESTION Perhaps you should mount on the RAID10 SAS drives. This may help cut down on the number of SAS drives you actually need. Everything else can remain on SSD. That way: 

Give it a Try !!! CAVEAT : Please note that the results come from your actual data in SQLFiddle. I simply loaded the data into a test database and ran my code against it. 

Make sure you make a backup of the database and load it into a dev/staging DB and then run ALTER DATABASE against dev/staging. 

If you would like to manually control this, here are some suggestions Login to Window Command Line Shell as Administrator and run this 

When an in-memory temp table exceeded the minimum of (tmp_table_size or max_heap_table_size), mysqld does the following: 

This would have recorded your trouble spots into What could have been done with the mysqldump to begin with ??? SIMPLIFY EXPORT CONTENTS You could have used the --hex-blob options to convert the characters into a hexadecimal representation. This could have eased any character set misinterpretations or misunderstandings on the part of the mysql client program. DUMP ONE ROW AT A TIME Even when using --force for the mysql client, this can still result in hundreds or thousands of rows not being inserted. Why ? By default, mysqldump has --opt enabled. This sets the following 

You will see that some Characters Sets have with multiple collations for Different Parts of Europe. Chinese, Japanese, Greek, and parts of Asia Minor and Scandinavia are also available. QUESTION #2 

I still run , even though it is empty, in order to reset the auto_increment value. The table should be immediately available for use once you execute the . The last line, , should only be handled by the one DB Connection that runs the above code. Give it a Try !!! 

If is an auto_increment column in the table, you may want to change the INSERT INTO command to exclude that column as follows: 

While trial-and-error may be necessary, don't go willy-nilly on these variables in /etc/my.cnf. You are better off setting them dynamically during any trial-and-error testing. To make sure of any corner cases, look at the initial variables of any mysqldump and see if character sets or collations are set in the beginning and reset at the bottom. In fact, here is a sample for a mysqldump: 

These can be managed for your app users by creating a corresponding mysql user for each app user. If you are planning to manage user limits, please remember not to reinvent the wheel since MySQL can control these aspects in its authentication protocol. 

Whenever mysqlbinlog dumps the contents of a binary logs, it has to see these event numbers, especially the binlog magic number. If this value is missing, that quickly reveals one of the following: 

Whenever you dump a mysql table that has an AUTO_INCREMENT column, the next value is always attached to the definition of the table. You should see something like: 

That should regenerate the index pages for the MyISAM. Just to be safe (if using MySQL 5.6), run this after the repair 

ASPECT #3 : MySQL When mysqld sees a connection attempt, it records it statistically as one of the following 

to have replication conitnue If you have multiple error numbers to skip, it should be a comma-delimited list of error numbers 

Doing this in your session makes mysqld bypass referential checks in your session. You are still subject to the usual locks if anyone is accessing the tables whose constraint you are trying to discard. Indexes would be dropped quickly thereafter. Your best approach would be to have a test server with the same amount of Production data. Run these two commands and perform the drop of the constraints in such a Staging server before doing this in Prod. If you need to do this in Prod, schedule proper downtime. Afterwords, do this STEP 01 : Run the following 

From there, replication was off and running. This has happened to my boss once and to me once. Of course, I reloaded the data from the Master in full to get the Slave sync'd (Good thing the database was less than 100MB). If this crazy scenario was possible from a MySQL 5.5.30 Master to MySQL 5.6.21 Slave, (as I have shown happen and two maintenance cycles to correct), then the reverse is far more likely (the reverse being a new Master and an older Slave). For the sake of your sanity and your database topology, please upgrade. 

The output will tell you what index was chosen to scan for rows. If the index is being used, then the is being scanned in the index, but the is being scanned from the table. SUGGESTION #2 : Change the If the is making the Query Optimizer choose the index, try this: 

This will force any groups other than 9,7,6,10,8,5 to appear at the very bottom of the query. UPDATE 2011-09-06 14:39 EDT 

This new paradigm will permit a Slave to be closer sync'd to its Master. Notwithstanding, latency within the network could hamper MySQL Semisync Replication to the point where it reverts back to the old-style asynchronous replication. Why ? If a timeout occurs without any slave having acknowledged the transaction, the master reverts to asynchronous replication. When at least one semisynchronous slave catches up, the master returns to semisynchronous replication. UPDATE 2011-08-08 14:22 EDT The configuration of MySQL 5.5 Semisynchronous Replication is straightforward Step 1) Add these four(4) lines to /etc/my.cnf 

The answer to that would not be a simple yes or no. It depends on what your need to read. Otherwise, transaction isolation via MVCC would become rather pointless. For example, in MySQL's InnoDB storage engine you have four levels of transaction isolation: 

I guess some developer just gave up putting it in the code and slapped up a configure file as some demented shortcut. If you have such a file, please comment out the last line and restart mysqld. MySQL 5.7's default value for is as mentioned 

You will have to put it back if you want to use LOAD DATA INFILE, SELECT ... INTO OUTFILE, or LOAD_FILE(). OPTION #2 Try setting this: secure_file_priv OPTION #3 Give a different password. When Developers login as 

From this article, I started to realize that 0 may not improve CPU a Percona Server instance, but it can slightly improve MySQL. Setting this to 64 may or may not help. Why may or may not ??? The article compares MySQL with Percona. You are using MariaDB. Therefore, you would have to experiment with this. Since innodb_thread_concurrency is dynamic, you could run 

My previous employer had MySQL Administrators and Data Administrators (DA). While a MySQL Administrator was just another term for an operational DBA whose specialty was MySQL and performed the grunt work of making sure MySQL was up and running and just cared about data throughput, the DA was responsible for checking the cohesiveness and integrity of datasets from a logical standpoint. For all intents and purposes, the terms Data Administrator and Data Analyst could be used interchanegably as the DA. Historically, a Data Analyst may have specific duties in terms of laying out ER Diagrams, Data Flow Charts, and things like these. The term Data Administrator may be somewhat newer and thus less defined. The role of the DA differentiates itself from DBAs, who may be playing the role of both operational DBA and DA.