I tried the DNS with nslookup and works well. In both servers operating system is: Debian 3.2.46-1+deb7u1 i686 GNU/Linux. I show the configuration files and then the log files. /etc/cyrus.conf (frontendlinux) 

Currently in windows computer I have installed Kerberos for Windows. This allowed me to use sso with Thunderbird against a Cyrus IMAP. Thunderbird has the option of using GSSAPI. Is possible to configure Microsoft office outlook to access via GSSAPI?. The idea is to keep the sso we got in the Outlook desktop client. It occurs to me that there may be a gateway. I know DavMail but is specific to Exchange servers. I appreciate any help with this setup!. 

Where: myanez is the name of a user that I created and I started the session by logging on to Thunderbird. In the settings I had in server and that is working for sending and receiving mail, I add the following lines. In /etc/imapd.conf add the following line: 

A long time ago I'm trying to fix this error but I'm still locked, I appreciate any help :). I detail all that I have and what I am trying to do. I am using Cyrus Murder and apparently everything works fine until It try to delivery the mail to backend using lmtp. I have: 

If sending an email to pepe1@ejemplo.org get what I want, pepe@ejemplo.org can read the mail. But if I send an email from pepe@ejemplo.org you receive the mail from pe@ejemplo.org and NO pepe1 as I want it to display. Any help?, Thanks for help. 

The mail sent by username@ejemplo.org will use as the sender pepe1@ejemplo.org And mail sent to pepe1@ejemplo.org them will be sent to username@ejemplo.org. 

I have configured Postfix and Cyrus Imap to enable SSO using Kerberos and GSSAPI. I use Thunderbird as a mail client which supports GSSAPI but I wanted to try some other client also. I tried installing Opera Mail but this has no support. That's another alternative to Thunderbird?, Know some Plug-in to add this functionality to mail clients?. I do not have much experience in these matters. I'll keep looking for information but I wanted to ask you to give a tip where I continue or indicate any alternative. Thank you!. 

I accidentally started backing up much more than was expected so I filled all volumes. I now fixed the issue of what is being backed up and what isn't; but how do I get backing up again? I cannot create more volumes and I'd rather not destroy volumes. What should I do? 

and all the files generated during those commands end up being owned by the user running it, instead of www-data. What would be the proper solution to this issue? I thought of sudoing as www-data to then run the commands, but rvm needs bash and www-data is currently using plain old sh with no profile files, so to make it work I would need to improve www-data's shell environment. Is that the way to go? Any side effect I should watch out for? 

I want that when people access my web server with the ip address, like $URL$ be redirected to the domain name, like $URL$ I'm using Apache Web Server and I've tried: 

I'm really confused by this. To do some experiments with GlusterFS I created two virtual machines with VirtualBox, running Ubuntu 12.04, each with 10GB of storage. I wrote a script that created lots of little files in lots of folders. Each file was 100k of random data generated by: 

that page, $URL$ says that there should be another error message nearby. At least in /var/log/syslog, this is all I see: 

Is there a way to require a resource so that it's executed first, but if it's not present, just drop the current resource all together? 

Using Ubuntu 12.04 or 14.04, how can I prevent an upstart service from starting until a couple of network volumes (GlusterFS) get successfully mounted? 

Are there any guides, howtos, books, etc about installing and maintaining a publicly-accessible Windows Server 2008 (with IIS and SQL server) for programmers (that want to deploy their own apps)? 

Rather than having a location block for each redirect, you can always just add rewrite rules into an existing location block: 

By default, when you create an account in WHM, it will configure local mail accounts and it will set the local DNS to use the local MX records. However, if you do not wish to rely on the local DNS for the MX records, then you can force WHM to always use a remote mail exchanger. In WHM, goto: DNS Functions -> Edit DNS Zone -> (select domain name) At the bottom, you will see something called Email Routing. Set this to Remote Mail Exchanger and then ensure the DNS server is restarted once saved. 

Providing that you have loaded your private key on your client, then it sounds like this might be a permissions issue on the 'git' user home directory and .ssh directory. Please try changing your /home/git directory to a mask of 0711: 

It's probably not running because it's unable to find the binary. Without any error log though, that's just an assumption I'm making. Try adjusting the part in your script to use its full path. I'm not sure how you installed (either via yum or manually), but you can find out its full path by running . 

Your research is right. You cannot redirect with a CNAME because DNS does not work this way. You would need some sort of web server to accept the request and rewrite the URL to HTTPS. Some domain registrars allow you to do a redirect on their web server if you use their name servers. DreamHost may provide this service but I have never used them so I cannot say. 

We did a lot of Moodle performance optimisations where I used to work and used PHP Xdebug profiling to help out with a lot of it. By enabling profiling, it will produce a *.cachegrind file which is then readable by many readers - I use QCacheGrind on Windows, or KCacheGrind on Linux. You should then be able to pinpoint the function which is taking its time. From memory, it is probably the amount of course data that it's trying to load when the user logs in. If there is too much data, and it's grabbing all of the data from the SQL server rather than only whats necessary, this could slow things down quite a bit. This can also be identified using the slow log as mentioned in a comment above, providing that its the SQL query that's causing the performance issue. I would also definitely advise upgrading your Moodle instance to something newer as I know there were a lot of performance fixes pushed across to the upstream repository since Moodle 2.2.3. We used to push these over to Moodle as an official Moodle partner. 

It was a PHP error. Enabling display of PHP errors showed that and the reason why I didn't saw it when I was running the script manually is because I was running it as root and the error was lack of permission to write temporary data in a directory. 

But I'm not so sure how it should work. It doesn't seem to be working out of the box. Even though mounting of glusterfs volumes happens after the network starts, it happens before GlusterFS starts: 

I'm trying to figure out why I need so much space for backups using Bacula with SQLite and I've noticed this file: /var/lib/bacula/bacula.db measuring above 500MiB on all the directors. I presume that file also changes every time I run a backup, which means that is backed up in every daily backup. My question is, should I back it up? 

I created a bunch of upstart jobs for my services that I'm running on an Ubuntu 12.04. I can successfully start them and stop with with: 

Since I needed this only for development, I ended up using smtp4dev, which is exactly what you need when developing an application that sends emails. The project description: 

and it did create /var/lib/bacula/bacula.sql, but when I run the job it gets deleted. Any ideas what's going on? The whole output looks like this: 

I'm running a server with Ubuntu 12.04 and we have some cron jobs that generate emails we want to receive. For handling that and other mails like that, I installed postfix and configured it to only listen on localhost since I don't want random people even trying to use it as SMTP relay. I can successfully send mail by running: 

It seems that configuration is making nptd not use those interfaces at all, not even to connect to other servers. Does ntpd need to listen on external interfaces to be able sync the clock? 

If it's not set before it reaches this location block, then it will set it to a blank string. You can just as easily add a string between the quotes. I do not get any errors when doing a configuration test using this. Please let me know if you're seeing otherwise. 

If you change your custom cookbooks, you need to update the cookbooks on each instance. You can do this under the "Deployments" area by clicking "Run Command", selecting "Update Custom Cookbooks" from the drop-down and then pressing the "Update Custom Cookbooks" button. You can usually leave all instances checked, unless you don't want to update one for some reason. 

This seems to work without any issues for my servers with multiple network interfaces. If you then restart your network service or interfaces, you should be able to check the routes to see that this is actually working: 

You can't map a port on a public IP address to more than one private IP address, it just won't work because how is the router supposed to know which one to go to? If you're using name-based virtual hosts, you could achieve this by sitting a HAProxy instance in-front of the web instances and direct all traffic from the router to the HAProxy instance. On the HAProxy instance, you create a front-end and specify the domains and the backend to use. Then, depending on which domain is accessed via HTTP, it forwards the request to the appropriate back-end to serve the request. I do it all the time when I want to conserve server or IP resources. 

I recently setup a Git server where I work which required SSH and HTTP(S) access for public and private repositories. I first setup the entire thing manually which took some time, but then I came across Gitlab which seemed to solve all my problems - it's pretty much Github, but your own private hosted version. I'd strongly recommend using Gitlab over setting everything up manually, given that it has so many great features and is easily manageable. There's a great tutorial here: $URL$ However, if you do want to run Git through HTTP(S), you will need to setup Apache or Nginx to use the git-http-backend binary which does all the work. There are plenty of tutorials online depending on which configuration you decide to go with.