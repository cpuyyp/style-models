This cannot be done without a subselect as you state. There are two steps involved. 1. For each name, find the record with the MIN(qty). 2. Return the ID for that record. There are other approaches but the two steps remain. It appears the query you posted has some missing info. This should be the whole thing. 

If going down the view approach which would limit disk IO, be more normalized and MAY perform better(depending on the use of the value), it would look something like this; 

I think given the requirements and that at some point you may want additional information that wasn't listed here, I would go with a fully normalized approach. 3 Tables: Guests, Episodes, Episode_Guests Then depending on if you want to do this for more than one show, another table for Shows(or series). As Paparazzi mentioned, the Guests table should contain sex. The Episode table should contain a date. Also, if you are going to do this for multiple shows, the Episode table should also have a foreign key back to the Shows table. The Episode_Guests table should record every instance of a Guest appearing on an Episode so all it would need is a foreign key relationship back to guests and another for Episode. 

Beginning with SS 2005, there is a DMV called sys.dm_db_missing_index_details. Google that and you'll find some useful scripts. That said, you may need to look at the problem more holistically - is your hardware adequate (Perfmon counters will help you with this), what else is running at the same time, what is SQL primarily waiting on (Google the DMV sys.dm_os_wait_stats for some great scripts). And you can look at the execution plan of the specific stored procedure to find out where the bottleneck is. 

I was bitten by that horrible error message as well. Check out this kb - I believe it is your answer: $URL$ 

My understanding is that online indexing acquires locks at the beginning (Preparation) and end (Final) phases. However, we see a lot of DML queries blocked during the Build phase. A sampling of the queries being blocked show they have a few things in common - the Wait Type is PAGELATCH_UP, the Wait Resource references a PFS page, and the query uses locking hints (NOLOCK, ROWLOCK, etc). Could locking hints be interfering? Can anyone shed some light here or point me in the right direction? Let me add, the actual command was: 

I have the print statement in there to help you get the @sqltext exactly the way you want it. Once you have it as you want, use the EXEC sp_executesql line to actually run the output of the @sqltext. 

One problem I see with your sub-query is that you are trying to join using the personID but you aren't providing that value in the sub-query. Also, I would assume the purpose of the subquery is to reduce the number of results from the table to only the latest entry for each PersonID and eligibility. That being the case, you should really add that to the join as well. 

SSIS is going to be your best bet in this case I believe. If you already license SQL Server, it should be included as well. An SSIS package has the ability to access and utilize multiple ODBC or OLEDB sources and destinations including MySQL. It can be run using a SQL Agent job or just using DTEXEC from a Windows scheduled job. Also you can easily build logging, error checking and validation. 

In my personal experience, if I were keeping score, causes for database restores have been: accidental deletion of data = many, restore to dev for testing = many, IO system failure / corruption = none. (Knocks on wood) You indicate you have "no need to recover to any point in time previous to our last backup". If that is in fact the case, then yes, switch into SIMPLE recovery. 

When I test ApplicationIntent=ReadOnly connections from a machine within DevDomain I am correctly directed to the readable secondary. Note, the connection string specifies the Listener name as I am in the same domain. However, when I test ApplicationIntent=ReadOnly connections from my own workstation (different domain) I get the connectivity error below. Note, in this case I need to specifiy the Listener IP since the I am outside the domain and the name will not resolve. 

Some comments, though: Everyone's causes for failure will be different: developer talent may dictate frequency of accidental data deletion, storage architecture may dictate whether a failed drive is fatal or just a nuisance, etc. As for SIMPLE vs FULL recovery model, that is a decision that should be made by the business folks. They need to weigh the damage to the business of downtime and / or lost data vs the cost of administration and space. Regarding data and log files on the same drive, conventional wisdom had you separate the two because access to the data file is random while access to the log file is sequential. In my experience, this works if you're talking about a couple databases on drives that you can segregate, but on a SAN with 100 databases spread across 48 disks all data access becomes random anyway. 

Scott Hodgin is correct. You should award him the answer. In addition, if you do not want to grant the SQL Agent Login rights on your network, you can alternately run using a Proxy account with stored credentials. Then you can apply that proxy to the Job step and have it run with those credentials instead of the SQL Agent login account. Of course you will still have to grant permissions to the folder to that account. 

If you want to see all values from Table2 and see all records from table1 where there is a matching email, do this; 

Here's a good article on the subject; Given the overhead involved in re-opening a database after it's been closed, I would say, no. Do not enable it. The more databases you have, the more this problem is exacerbated. That being said, if there are some other circumstances that makes you think it should be enabled, please include those details. As sp_BlitzErik asked, what are you trying to achieve? 

For posterity, after much trial and error, we figured out how to get the expected throughput. As mentioned above, the NetApp had one virtual interface backed by four physical NICs. The host has two NICs and I had configured MPIO through the MS iSCSI Initiator so that there was a path from each NIC to the one virtual interface. The results were the throughput above - writes made sense at close to 200 MB or the speed of two NICs, but the reads were half that or the speed of one NIC. Upon closer inspection, our SAN guy noticed that traffic was only flowing through one of the physical NICs for the reads. I'm not sure if there was a configuration mistake on our end, but there were two things we tried and both got us our throughput. One was to change from one virtual interface backed by four NICs to two virtual interfaces, each backed by two NIC's. Then map one host NIC to one virtual interface. The other thing we tried was to use "aliasing" on the SAN side to present multiple virtual interfaces. (I'm not a SAN guy, so hopefully I said that correctly.) My take-away is that we just needed the SAN to present more than one interface so the Initiator truly saw multiple paths. Here is our throughput now: 

100% agree with Marcin Gminski. The SA account is definitely not supposed to be used this way. Windows domain credentials certainly are preferable although not necessary. If you are going to use a single login account for all users, at least create a new login that is not a part of the System Admin role and make sure it has a strong password. As for your performance issue, we would need to know more. First off, what is your method used for data access in the MVC app. Are you using Entity Framework, Linq or something else? Next, have you debugged the app to see if it is making excessive, unnecessary calls to the database. Next, have you attempted to run a SQL Profiler trace to capture the activity on the server? Those steps should at least provide a little more insight in to where the problem exists. 

Less of a race condition and more of an issue with order of operations and dependencies. The way you stated your question, in steps, is how you want to think of the SSIS package. First it has to do this, then it has to do that. Since the line items are dependent on the latest information from the header, that should always come second. Letting them go simultaneously, you don't really know what happens first. 

The service account is a domain account. It is not a domain admin (nor is it a member of any group that is a domain admin). It has neither "write servicePrincipalName" nor "Write public information" permission (nor a member of a group with these permissions). Yet it is still able to register / deregister it's SPN upon startup and shutdown. What permission am I missing? 

This is a great example of how the Window Functions can be used. Based on your data, I'm assuming you want to do this for every unique LINE_NO. I'll leave it to you to format the percentage column. 

Server: Windows 2012, SQL 2012. My machine: Windows 7. I believe I've got the Routing Lists and URLs correct (and again, this works fine from within the domain). I know the nodes of the cluster need to be in the same domain, but I would think I'd be able to connect from outside the domain by specifying the Listener by IP, but maybe not? I thought perhaps I had a DNS issue in the DevDomain, but everything seems to resolve fine inside that domain. Can I do this / what do I need to be looking at to resolve this?