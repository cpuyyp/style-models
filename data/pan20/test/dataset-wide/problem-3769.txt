A random $k$-coloring of the vertices of a graph $G$ is more likely to be proper than a random $(k-1)$-coloring of the same graph. (A vertex coloring is proper if no two adjacent vertices are colored identically. In this case, random means uniform among all colorings, or equivalently, that each vertex is i.i.d. colored uniformly from the space of colors.) 

Shall we try teamwork? Please feel free to edit this post if you have simplifications. The original sum may be re-expressed as $$ \frac{1}{2^{2m+1}} \sum_{k=0}^m (-1)^k \binom{m}{k} \binom{2(k+m)}{k+m} \frac{1}{2^{2k}} \sum_{j=0}^{k+m-1} \frac{2^{k+m-j}}{(k+m-j) \binom{2(k+m-j)}{k+m-j}}. $$ If we're trying to prove this is 0, we may drop the fraction out front. Also, change variables from $j$ to $\ell=k+m-j$: $$ \sum_{k=0}^m \left( -\frac14 \right)^k \binom{m}{k} \binom{2(k+m)}{k+m} \sum_{\ell=1}^{k+m} \frac{2^\ell}{\ell \binom{2\ell}{\ell}}. $$ At this point, my idea was to change the order of summation based on $$ \sum_{k=0}^m \sum_{\ell=1}^{k+m} \Diamond = \sum_{\ell=1}^m \sum_{k=0}^m \Diamond + \sum_{\ell=m+1}^{2m} \sum_{k=\ell-m}^m \Diamond, $$ but I can't get quite it to work out. The first sum simplifies, but the second sum I can't do much with. Any ideas? 

Each vertex in a graph is randomly and independently colored either red or blue with equal probability. A coloring is called $r$-good, for some fraction $r\in[0,1]$, if at least a fraction $r$ of the edges touch at least one red vertex. Define $p(G,r)$ as the probability that a graph $G$ is $r$-good. Obviously $p(G,r)$ is decreasing with $r$. What is the largest $r$ such that $p(G,r)>1/2$ for all finite graphs $G$? There is an upper bound of $2/3$ (as noted by Kevin P. Costello) and a lower bound of $1/2$. 

MY QUESTION IS: Is anything known about such "directed cages"? Specifically: how many nodes are needed for a directed cage when $r=2$, as a function of $g$? 

Three agents, failed directions Again the goal is to use at most 4 cuts (the lower bound). Let agent #3 make $Q$ marks on the circle, such that the piece between each two consecutive marks is worth for him exactly $1$. We would like to give one of the three agents a fair piece, while making sure that the remaining cake is sufficiently valuable for the remaining two agents. This can be done in three ways: Way 1. Give agent #1 an arc of length at most $p_1$, whose value for agent #1 is at least $p_1$ and for agent #2 at most $p_1$. Way 2. Give agent #2 an arc of length at most $p_2$, whose value for agent #2 is at least $p_2$ and for agent #1 at most $p_2$. Way 3. Give agent #3 an arc of length $p_3$, whose value for agents #1 and #2 is at most $p_3$. Unfortunately, there are cases in which none of this ways is possible. For example, suppose the weights are $p_1=3,p_2=2,p_3=1$. Agent 3 cuts six pieces equal in his eyes, and the valuations of 1 and 2 for these pieces are: 

Theory of Statistics by Mark Schervish Bayes Theory by John Hartigan The Bayesian Choice by Christian Robert 

Suppose I am dead-set on using Bayesian inference on independent and identically distributed data, but I'm lazy and insist on using a parametric likelihood function come what may. I'd be reassured to know that as long as I stick to a particular class of parametric densities I can be sure to recover the true quantiles. Is there a recipe to construct a parametric family given a vector $q \in (0, 1)^k$ such that the inferred quantiles at $q_1, \dots, q_k$ line up with the true quantiles at those points? Here's my stab at formalizing this. 

Turing has already been mentioned in previous excellent answers as someone whose ideas sit at the boundary of philosophy and mathematics, conventionally understood. I want to mention Ludwig Wittgenstein in this context as an example of someone grappling with a host of similar ideas but who arguably took a more "philosophical" approach to them, one that explicitly resists formalization. The book "Wittgenstein's Lectures on the Foundations of Mathematics, Cambridge, 1939" is a transcript of a seminar given by Wittgenstein which was attended by Alan Turing, among a handful of other notable young Cambridge scholars of the day. It contains an interesting exchange between Turing and Wittgenstein about what happens when people disagree as to the result of a calculation. Turing insists that some are right and some are wrong, and Wittgenstein wonders what that might mean. Turing says that if you build a bridge that depends on a wrong calculation, it will fall down. My recollection is that Wittgenstein argues that this doesn't cause the bridge to fall down, but that the bridge falling down might serve as a definition of what it means for the calculation to be wrong. I gather that many people find Wittgenstein to be obtuse and/or just plain confused, but a more charitable reading suggests that he was wrestling with ideas related to undecidability, albeit from a much broader sociological vantage point. See, for example, "A note on Wittgenstein's notorious paragraph on the Gödel theorem". If a bridge standing or falling down depended on a proposition that was undecidable, what then? I believe, but am not sure, that the issue of "in which formal system" would not necessarily have been in the air at the time. So, on the one hand we have a nice example of philosophical questions stimulating what went on to become a much more formal (and elaborate) theory in the work of Turing. On the other hand, I feel that the hypothetical scenario about the bridge is a "purely" philosophical question that is both interesting and challenging and that is not addressed by the subsequent formal developments. Naively put, which formal system does mother nature obey? Moreover, how can we make better sense of this question? Philosophy gropes at such questions; once they have been sufficiently sharpened, mathematics constitutes the work of refining and extending our understanding. 

Let $H$ be a set-family (a set of sets). For every set $g$, define the intersection set-family: $$ H\cap g := \{h\cap g| h\in H \}$$ For every set $g$, the family $H\cap g$ contains at most $2^{|g|}$ sets (the subsets of $g$). Call $H$ simple if, for all two-element sets $g = \{x,y\}$, the family $|H\cap g|$ contains at most $3$ sets. (in other words, $H$ is simple iff its VC dimension is at most $1$). My goal is to characterize the combinatorial structure of simple set-families. I found some sufficient conditions for simplicity of $H$. In all cases, I assume that $g=\{x,y\}$, and prove that $H\cap g$ cannot contain one of the four subsets of $g$. 

Let's call a function $f$ k-podal if, for every multipodal set $X$ of size $k$: $$\sum_{x\in X}f(x) = 0$$ In particular, a 2-podal function is just another name for an odd function: $f(-x)=-f(x)$. I am looking for information about these "multipodal points" and "$k$-podal" functions. In particular: 

Cover and Thomas's Elements of Information Theory has a chapter on maximum entropy stochastic processes. The relevant quantity in that case is the entropy rate. See section 12.5, for example, which is visible in Google books. 

To expand a bit on Arthur B.'s comment that you can use samples even if they are dependent, consider this quote from Andrew Gelman and Kenneth Shirley: 

I suggest taking a look at the work of Ulf Grenander. His 1963 book laid out the basis for applying probability theory to groups (Chapter 4 is on stochastic Lie groups) and other algebraic structures. He continued to develop these ideas (see his later book) in the context of pattern recognition. There is definitely newer work in this area (some of which is mentioned in other answers), but Grenander has been investigating these ideas for 40+ years and is worth looking into. As an aside, Grenander's approach seems to be a bit more formal than a lot of contemporary machine learning research, which is a strength or weakness according to the reader's taste. 

For semiprimes, computing the Euler totient function is equivalent to factoring. Indeed, if n = pq for distinct primes p and q, then φ(n) = (p-1)(q-1) = pq - (p+q) + 1 = (n+1) - (p+q). Therefore, if you can compute φ(n), then you can compute p+q. However, it's then easy to solve for p and q because you know their sum and product (it's just a quadratic equation). If you believe factoring is hard for semi-primes, then so is computing the Euler totient function. Update! Factoring and computing the Euler totient function are known to be equivalent for arbitrary numbers, not just semiprimes. One reference is "Riemann's hypothesis and tests for primality" by Gary L. Miller. There, the equivalence is deterministic, but assumes a version of the Riemann hypothesis. See also section 10.4 of "A computational introduction to number theory and algebra" by Victor Shoup for a proof of probabilistic equivalence. 

There is a whole research area called Algebraic Statistics, although its boundaries are pretty blurry in my opinion. But you could do worse than to start with Seth Sullivant's web page for some idea of what it is all about: $URL$ Titles like "Algebraic factor analysis: tetrads, pentads, and beyond" and "Algebraic statistical models" suggest to me that this may be what you are looking for. 

I think holographs are a compelling technology that seems like magic except in the light of some pretty cool mathematics. If some kind of learning module could get these ideas across, it'd be neat. 

The intersection between computability theory and statistics is pretty interesting. From this paper by Vovk (2009): "It is widely accepted that advances in computing have brought about deep changes in the theory and practice of statistics. However, the use of the theory of computing, and, in particular, of its core notion of computability, has been very limited in the classical areas of statistics, such as parameter estimation and hypothesis testing." Relatedly, Ackerman, et al. (2011) demonstrate a computable random variable $(X,Y)$ with non-computable conditional distribution $P(Y \mid X)$. Certainly this area is pretty "mathy"; it remains to be seen if it has implications for statistical practice. 

Recall that a category C is small if the class of its morphisms is a set; otherwise, it is large. One of many examples of a large category is Set, for Russell's paradox reasons. A category C is locally small if the class of morphisms between any two of its objects is a set. Of course, a small category is necessarily locally small. The converse is not true, as Set is a counterexample. Now, I can construct categories that are not locally small. However, what's the most common or most reasonable such category? 

I don't know if this deserves to be an answer or a comment, as it includes information you surely know. The terminology of matroid theory borrows heavily from graph theory, linear algebra, and other fields. A dependent set in a graphic matroid corresponds to a cycle in the underlying graph, so a general dependent set in a matroid is called a circuit. The length of the smallest cycle of a graph is its girth, so the same word is used for a general matroid. As you're well aware, the spark of a matrix is the girth of its corresponding vector matroid. You're correct that your notion appear to be dual to spark. The appropriate terminology here could have been "cutset", as that's the corresponding notion in a (connected) graph, but for consistency it's cogirth. The co-spark of a matrix is the cogirth of its corresponding vector matroid. A matroid is representable (ie, as a vector matroid) over a field if and only if its dual matroid is representable. Moreover, the transformation taking a representation of a vector matroid to a representation of its dual matroid is completely effective. It follows, in a sense, that studying the co-spark of a matrix is equivalent to studying the spark of another matrix. In particular, any computational hardness results for spark carry over to co-spark. It seems, then, that the notion of co-spark would be useful primary as terminology and as a way of getting a handle on things, but not actually for new ideas. Here's one paper that has explicitly looked into these sorts of things: On the (co)girth of a connected matroid. 

I have found this book a useful reference: Maximum-entropy Models in Science and Engineering It may contain some pointers to applied work that you will find convincing (I don't have it on me right now) if you can get a copy. 

The result you want, I think, is in Stationarity, Isotropy and Sphericity in $l_p^*$. It is behind a pay-wall, but the form of the distribution is stated in the abstract. 

I would make the problem sharper if I could, but the reason I want examples is precisely to help focus my thinking. I find it intriguing that it is not enough to have a well defined condition and a well defined model, one must also justify (by way of interpretation) which limit to take! I anticipate there are many examples from physics of which I am unaware and perhaps some from the literature on finite elements for solving PDEs. (Apologies for the pay-wall links.) 

the square is carved 3 times. The rectangle-numbers are written in each resulting polygon. The final rectangle-number is 6, since $P'$, which is a union of two polygons, can be partitioned to 6 pairwise-disjoint rectangles. Let $R(n)$ be the largest rectangle-number that you can achieve in $n$ carvings. The example above shows that $R(3)\geq 6$. What is an upper bound on $R(n)$? In the example above, each carving adds at most 2 to the rectangle-number. This hints that there may be an upper bound such as $R(n)\leq (2n+1)$, or maybe another linear upper-bound. EDIT: while there are scenarios in which a single carving can double the rectangle-number: 

By the construction, it is clear that this $LL$ pair cannot be used in the same way with any other $RR$ pair. Now we know that this cannot happen to all consecutive $R$-corners, because the number of $R$-corners exceeds the number of $L$-corners by $4$. EDIT: I am not sure about the latter counting argument. What if there is a sequence LLL, such that the first LL pair matches an RR pair, and the second LL pair matches a disjoint RR pair? 

Both of the penalties can be thought of as arising from the linear regression setting in a Bayesian framework with predictor matrix $K$ and a Gaussian prior over the vector $\alpha$, centered at zero with prior variance $V$. In the ridge regression case $V = n^{-1}\lambda^{-1}I$ and in the other case $V = n^{-1}\lambda^{-1}K^{-1}$ (as a kernel matrix $K$ is symmetric and PSD; I'm also assuming it is invertible). This follows just by equating terms; the posterior mean has the form $(K^tK + V^{-1})^{-1}K^tY$. Plugging in $V = n^{-1}\lambda^{-1}K^{-1}$ gives $$(K^tK + n\lambda K)^{-1}K^tY = (K^t + n\lambda I)^{-1}K^{-1}K^tY = (K + n\lambda I)^{-1}Y.$$ Anyway, this is all just definitions, but the perspective might be intuition-boosting: the RKHS version stipulates explicitly that the prior over alpha has higher precision (more regularization) along directions of high variation as defined by the kernel function.