If, for example, a room had different rates in the morning and afternoon and your transaction went from 11am to 2pm the above would now show two rows. Some simple maths will give you the duration of each billable rate i.e. 11am to noon and noon to 2pm. Multiply by the rate, the result and the user id. You will need a flag on transaction to show which rows have been billed and which not. Edit: on further reflection it looks like a row in the table is a single invoice (i.e. bill) set to a user. Rather than add a flag to add the foreign key to . Then you will know not only that a transaction has been billed but exactly when and how. Note that billing systems, invoicing and payment is a specialist subject in itself and full of intricacies. 

The phrase refers to different columns; in the first it will be , in the second . Since is the key it will be indexed and the will be a trivial amount of work. To , however, the system will have to retrieve every row, sort the complete table by , then choose just one of those rows. Change both queries to and I think your execution times will be almost identical. 

Query processing takes the given SQL, applies a set of rules and heuristics and comes up with an execution plan. This states which indexes are used and how. The outcome is dependent on the input and how the rules have been written. Given a slightly different start point a different end point may result. It's somewhat like dropping a ball in a landscape of river vallies. Drop it once and it will roll into a river. Drop it in a slightly different place and chances are it will roll down into the same river. Occasionally, if you're near a ridge, you drop the ball in a slightly different place and it rolls into the next river instead. 

The UNION operator combines all the rows from one query with all the rows from another, eliminating duplicates, and forming a single list. The hash operator builds a hash table from the upper input and probes that table with the lower input. I can see how this would work to implement a UNION, given the row-at-a-time pull model the execution engine uses. I imagine it works something like this. The Hash operator is asked for a row. It, in turn, pulls a row from the upper table, hashes it and compares it to its current list. If it is not found in the list it is a new value, it gets added to the hash list and also returned to the calling operator. This continues. Eventually a row is read that has a match in the hash table. That row is rejected (UNION eliminates duplicates) and the next row read. Eventually the upper input is exhausted. Processing continues with the lower input, reading rows, rejecting matches and passing on fresh values, until it, too, is exhausted. Under what conditions would a hash match be used instead of another operator? The trivial answer is because the optimiser has determined that the cost of a hash operator, for the given datasets, is less than the cost of any other operator that could perform this task. More specifically (I'm extrapolating somewhat from joins) hash match typically occur for larger datasets without appropriate sorting. Here's an example that shows the usage. I have a Numbers table, which I've copied to create dbo.Numbers and dbo.Numbers2. The query 

Now to the body of the SP. If you use the names as the primary key in tables and the statements are straightforward. Assuming you have primary keys and foreign keys defined -- you should -- you have to insert in the correct sequence to respect these key definitions i.e. into and and only then into the mapping table. 

Run a job nightly which counts the rows in each table. Store the output and see how it trends. Obvious problems - 

It is OK to have many columns in one table. Indeed, this is the normal situation. It is not OK, however, to mix different types of information in a single table. Each table models a single class of "thing" in the real world. The rules of normalisation guide a database designer to separate classes from one another. Denormalisation (storing several things in one table) is a legitimate technique but should be used sparingly, with caution, and proper risk mitigation in place. In your specific case you will have to consider what is your business domain. Are your users interested solely in the balance sheet bottom-line numbers for several corporations over several years (e.g. investment and trading point of view), or do they need the detail of how those numbers are calculated (e.g. an accountancy / book-keeping point of view)? For the former the table you mentions would be OK in my opinion. For the latter you need to model individual transactions, double-entry ledgers etc., and calculate your bottom-line numbers from these as required. 

Explanation My general approach is to step forward from the earlier date, first in years, then months, then in days. At each level of granularity the objective is to get as close to the end date without going over it, then continue at the next lower level. I use a numbers table to facilitate the close-to-but-not-over calculation. From this table and I can find the largest number of years/ months/ days that precede - comment (B) in the code. Since I was looking for the MAX number and my Numbers table is clustered on it, the optimizer was performing a descending scan, feeding values to DATEADD. This was causing date overflow errors as Numbers contains over 100,000 rows. is greater than 9999-12-31 and an error is raised. Predicate (A) gives an upper limit on the Number value from which the backward scan starts, avoiding the date overflow. Consequently the query plan traverses very few rows for even very large date ranges. This approach is used for finding years and months, except the starting point for months is brought forward by however many years I found in the first CTE. DAYS is my lowest level of granularity so a simple DATEDIFF is sufficient. This could be extended to finer granularity, returning the interval in hours, minutes and seconds if required. 

You might be getting lock escalation. This is when SQL Server replaces many fine-grained (row) locks with a single coarse-grained (table) lock to save system resources. This is most likely if you are processing many rows within a transaction. That link suggests some work-arounds. 

If I understand your question correctly you are looking to store, let's say, a entity with and attributes. You will also have an entity which is defined as everything a has plus plus but doesn't have . I'm guessing you want to be able to add a column to a base type and automatically see it showing up in the sub-type(s), and sub-sub-type(s). In short you are looking for an object-oriented data store. RDBMSs in general are not will suited to deliver this featrure. If you know what these differences are and they are stable, at least between schema relases, you can define a type/ sub-type model. The base table has the columns which are common to all types. It has a surrogate primary key. From there you define further tables, strictly adding columns as you go. The same ID is carried through this hierarchy. The case where you need to remove columns from one type to a descendant is handled by defining an abstract common ancestor and placing the removed column in one branch but not the other. For my example above we would end up with these tables