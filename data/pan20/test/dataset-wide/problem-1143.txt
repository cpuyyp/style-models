I understand your frustration with this behavior. There seem to be internal obstacles to implementing this and could lead to larger indexes than are necessary. 

A possible alternative would be to create materialized views of A on B. You may have to union the data from the materialized views with the existing tables or create views that do the union. The materialized views could be refreshed daily. The options already listed may also be good fits depending on the requirements. You should consider them all. Update: As your comments indicate that you would like to replicate packages as well, you could integrate those into your change management process so that when changes are made to A they are also made to B. You would need to use a different schema since you want to preserve the originals. On the other hand, the requirement for moving lobs as well makes things more difficult. If you want to use just one technology to handle all of your requirements you will probably have to go with datapump. B could be setup to pull the tables with blobs and the packages from A into a separate schema. You may still want to consider creating some views to union the data from the existing tables in B with the transported tables from A. 

These possibilities are from this documentation. If neither is applicable then perhaps you are running into a bug and should open an SR with support. I assume you are running commands like this: 

There would have to be data indicating that the row is both a child of row A and a child of row B. This can be done, but does not seem like it would be typical. Think of an employee. Most employees only have one direct managers. In a simple design an employee could have two managers, but it would require multiple employee records. For example: 

The answer to your question is no, however.... It sounds like a flashback query is what you need. Query the data as of a time when it existed and when it returns the correct data, insert it into the current table. This solution does require space in the UNDO tablespace sufficient to meet your requirements. It also doesn't use RMAN, but is significantly simpler than importing the entire schema. 

Our database deployment process logs in as sys to deploy database objects. We ran into a problem when we wanted to deploy a private database link for another user (AU). Our solution was to: 

If you are on Standard Edition, look into Stored Outlines. The feature is deprecated on 11g, but is the only plan stability option available with that edition. Here is a good example. 

If a user has access to the OS and the the folder containing the PREPROCESSOR program then theoretically they can do anything the Oracle OS user can do. 

If the "bunch of work to translate it" is occurring during the transaction, then that is adding to the transaction time. If you can do bulk processing on the data to translate it before you start dong inserts/updates/deletes that would at least decrease the transaction time providing fewer opportunities for contention. An alternative would be to maintain a copy of the table and make your changes on the copy and then during a brief outage window swap the table names. The downside is the added storage and the outage window, but the advantage is zero contention. Update: I see a couple difficulties with your solution. Inserts would still cause contention if there were unique indexes on the table. Updates would become inserts, so those wouldn't be any more difficult. Deletes would have to be handled somehow. You could insert a blank record with the delta number to indicate that the record has been deleted. I'm concerned that the view necessary to pull this data together for querying would be too slow. Perhaps someone with MySQL knowledge could suggest something. 

When you delete records there is nothing that automatically compacts the segment, therefore you will need to do a segment shrink to reclaim the space. Here is excerpt from the 11.2 Administrator's Guide on Reclaiming Wasted Space: 

I don't think it matters, but you also don't need to have the duplicate command in a run block. If you haven't already, you may also need to set the db_file_name_convert and log_file_name_convert parameters in the spfile. Documentation: $URL$ 

Your design is reasonable and may be preferable depending on how many and how large the columns in SpecialEvent will be. If they are small and few then they might as well be in the Event table itself (as Phil said). Such a design would be simpler and not sufficiently slower to merit concern. The development time you save will likely be better served optimizing something that true is slow. You should not create a separate table for each special event. This would add repetition, overhead, and a general sense that design is an afterthought not a planned activity. 

This method allows you to use date types as dates when doing comparisons or date arithmetic without having to convert them. For most scenarios it would be the simplest solution. The solution is also flexible in that it could later be enhanced to store new averages every year (rather than replacing the old averages). The year would then be the year in which the average was calculated. Of course that would introduce complexity, but the complexity would be deferred until it is necessary. 

As Ivan said, your problem is not with dual specifically. However, the context switch required to switch between PL/SQL and SQL is slowing things down. Often, SELECT statements like this can be converted into simple PL/SQL assignments. If that isn't possible, perhaps the logic can be moved into the later look up. Something else you could consider is doing a join with your reference table outside of the UDF. Before doing anything though, you should consider profiling the function with DBMS_PROFILER. This will show you what part of the function to focus on for your tuning. 

Close, I think you should be concerned with the redo itself rather than the log switches it is causing. I think this is because is set to zero, but again this should not be the issue to focus on. Here is the heart of the matter: 

I have some XML schema definitions that have been registered using DBMS_XMLSCHEMA. I have created tables using the XMLTypes generated. I can create XML using XMLQuery and if I understand correctly, the query results can be inserted into the table if the definition is correct. Ultimately the XML needs to be written to a file, but the part I am unsure about is if writing XMLQueries from scratch is the best way to generate the XML. Is there any way to have the XML schema generate an XML stub for a guide or generate template XMLQuery? The data is currently all in regular Oracle tables, but arranged quite differently, so any information on easing the export to XML based on a specific set of registered schema would be useful. 

You might classify the third as simply a less efficient variation of the second, but it does technically "use a FORALL statement to insert the data". The second variation is probably your best option. 

Backing up Archive logs is only necessary when running in Archive log mode, so the question comes back to whether the database should do this or not. This is covered in the same document you reference under the heading Deciding Between ARCHIVELOG and NOARCHIVELOG Mode. Here is an excerpt: 

Yes, you should test the whole chain of events as a unit. So, in your example with a procedure that inserts into a table and causes several triggers to fire, you should write unit tests that evaluate the procedure for various inputs. Each unit test should pass or fail depending on whether it returns the correct values, changes the state of tables correctly, creates the correct email, and even sends the correct network packets if it is designed to do such a thing. In short every effect the unit has should be verified. You are correct, that designing unit tests takes some work, but most of that work has to be done to manually test the unit, you are just saving the work required to test the unit so that when a change is made in the future the testing can be just as thorough and significantly easier. Changing data does make testing more difficult, but it doesn’t make testing less important and actually increases the value of unit testing as most the difficulties only have to be thought through once rather than every time a change is made to the unit. Saved datasets, inserts/updates/deletes that are part of the setup/teardown, and narrowly scoped operation can all be used to make this easier. Since the question isn’t database specific, details will vary. There is no complexity threshold on the high or low end that should stop you from testing or unit testing. Consider these questions: 

As far as the database is concerned, the most powerful user is the one connected as sysdba. As you have seen and by default, you can connect as sysdba locally using . While it is advisable to create another user for your work, a new user only granted the DBA role will have fewer privileges than . 

There is No Valid Reason to use a magic value instead of NULL. This might be the thought process of someone creating this mess. They write something like this: 

30-90 minutes according to Oracle's Best Practices for Upgrading. This is about the closest estimate you will get given all the unknowns in this situation. The size of the database really matters very little in determining how long the upgrade will take. Here are the main factors effecting the duration (from the Oracle.com upgrade blog): 

The analytic removed the leading zero speed rows and the analytic allows us to eliminate rows starting with the first zero speed row remaining. The sample data can be created as follows: 

The response from Oracle is that this is not really a bug because the application is catching the 4068 and handling it, thereby not allowing the normal resolution of the error to occur. Their explanation doesn't seem adequate to me because if the package throws an ORA-01476-Divide By Zero (for example) doing a raise_application_error does not effect whether the ORA-01476 is included in the stack or not. 

AVG and other aggregate functions work on sets of data. The WHERE cause does not have access to the entire set, only to data for the row it is operating on. If you created your own AVG function (as a normal function and not a custom aggregate function) it would only be passed one ID value when called from the WHERE clause not the entire set of ID values. Mezmo's solution will give you your expected results, but if you want to avoid two full table scans (assuming no indexes) you can use a windowing function like this: 

Datapump can do this without a dump file assuming you have a database link to the system you are importing from. Oracle Enterprise Manager has a GUI for this or you can just use the commands directly. It will look something like this: