UPDATE Here is something you can do, and something you can't do. With $C$ and $BC$ as above, and $r : C \to BC$ the refinement map, there is a homotopy inverse $s: BC \to C$. (More precisely, $C \to BC \to C$ is the identity, and $BC \to C \to BC$ is homotopic to the identity.) Working the same trick with $r' : C' \to BC$, we get quasi-isomorphisms between $C$ and $C'$ which are homotopy inverse to each other. As you will see, however, this construction is very nongeometric and inelegant. Construction: Let $q:BC \to Q$ be the cokernel of $C \to BC$. An easy computation checks that each $Q_i$ is free. Since $C \to BC$ is a quasi-isomorphism, $Q$ is exact. An exact complex of free $\mathbb{Z}$ modules must be isomorphic to a direct sum of complexes of the form $\cdots \to 0 \to \mathbb{Z} \to \mathbb{Z} \to 0 \to \cdots$. Choose such a decomposition of $Q$, so $Q_i = A_{i+1} \oplus A_{i}$ and the map $Q_i \to Q_{i-1}$ is the projection onto $A_{i}$. Now, consider the map $q_i^{-1}(A_i) \to A_i$ in degree $i$. This is surjective, and $A_i$ is free, so choose a section $p^1_i$. We also define a map $p^2_i$ from the $A_{i+1}$ summand of $Q_{i}$ to $BC_i$ by $p^2_i = d p^1_{i+1} d^{-1}$. In this way, we get maps $p_i = p^1_i \oplus p^2_i: Q_{i} \to BC_i$ which give a map of chain complexes. We note that $qp: Q \to Q$ is the identity. Therefore, $1-pq$, a map from $BC \to BC$, lands in the subcomplex $C$ and gives a section $s:BC \to C$. Proof of the claim about homotopies will be provided on request. 

Here is a much better exposition. Let $X$ be any connected compact manifold and let $\Delta$ be a simplicial complex realizing $X$. Let $P$ be the face lattice $\Delta$, with added minimal and maximal elements $\hat{0}$ and $\hat{1}$. (I suspect we can replace "simplicial complex" with "regular CW complex", but I don't want to think carefully enough.) Then $P$ is a thin, graded, strongly connected (the OP says this is in Browns book), lattice. I claim that the isomorphism of algebras holds if and only if $X$ is orientable. As discussed above, an isomorphism of the algebras is equivalent to a choice of weights $\epsilon(a)$ for each arrow $a$ of $P$ satisfying $\epsilon(x \to y_1) \epsilon(y_1 \to z) = - \epsilon(x \to y_2) \epsilon(y_2 \to z)$ for each diamond $x < y_1, y_2 < z$. We adopt the convention that, for any path $\gamma$ through the quiver, $\epsilon(\gamma)$ means $\prod_{a \subset \gamma} \epsilon(a)$. If $\gamma_1$ and $\gamma_2$ are two paths with the same endpoints, then $\epsilon(\gamma_1) = \pm \epsilon(\gamma_2)$. In particular, there are two values in $K^{\ast}$, negatives of each other, such that $\epsilon(\gamma)$ takes one of these values for any path $\hat{0} \to \hat{1}$. We choose one of them to call $u$ and one to call $-u$ A path $\hat{0} \to \hat{1}$ looks like $0$, $\{ v_0 \}$, $\{ v_0, v_1 \}$, $\{v_0, v_1, v_2 \}$, ... , $\{ v_0, v_1, \ldots, v_n \}$, $\hat{1}$ for some maximal simplex $\{ v_0, \ldots, v_n \}$ of $\Delta$. So, given an ordering $(v_0, \ldots, v_n)$ of the vertices of some simplex of $\Delta$, we get a sign. Using diamonds at heights other then the top, we see that this gives a chosen orientation of each simplex of $\Delta$. Then the diamonds at the top show that the orientation on adjacent simplices is compatible. So such a set of weights exists if and only $X$ is orientable. 

This is a vague question, but here is an attempt at an answer. Let $G$ be a graph, let $E$ be the set of edges of $G$, and let $C \subset 2^E$ be the set of cycles of $G$. Then knowing $C$ is equivalent to the matroid of $G$. Two graphs produce the same matroid if and only if they are related by a sequence of the following moves: (1) Taking two connected components and gluing them along a single vertex, or undoing the above. (2) If $G$ has two vertices $u$ and $v$ so that $G \setminus \{ u,v \}$ is disconnected, cutting along those vertices and regluing some of the pieces back with $u$ and $v$ switched. In particular, if a graph is $3$-connected, then it is determined by its matroid. So one answer could be "The cycle structure of a graph is its matroid" and, as the above shows, this contains almost as much information as the graph. 

If a matrix $A$ is self-adjoint/skew-self-adjoint with respect to a symmetric bilinear form, then it is diagonalizable. True for matrices over $\mathbb{R}$, with respect to a positive definite inner product. False over other fields. For example, over $\mathbb{C}$, $\left( \begin{smallmatrix} 1 & i \\ i & -1 \end{smallmatrix} \right)$ and $\left( \begin{smallmatrix} 0 & 1 & i \\ -1 & 0 & 0 \\ -i & 0 & 0 \end{smallmatrix} \right)$ are nilpotent, but self-adjoint and skew self-adjoint respectively with respect to the standard inner product. False for other nondegenerate symmetric bilinear forms: $\left( \begin{smallmatrix} 1 & 1 \\ -1 & -1 \end{smallmatrix} \right)$ and $\left( \begin{smallmatrix} 0 & -1 & -1 \\ 1 & 0 & 0 \\ -1 & 0 & 0 \end{smallmatrix} \right)$ are nilpotent, but self-adjoint and skew self-adjoint respectively with respect to $\left( \begin{smallmatrix} 1 & 0 \\ 0 & -1 \end{smallmatrix} \right)$ and $\left( \begin{smallmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1 \end{smallmatrix} \right)$. You can exponentiate the skew-self-adjoint matrices to get examples of matrices preserving a nondegenerate symmetric bilinear form, with Jordan blocks of the form $\left( \begin{smallmatrix} 1 & 1 \\ 0 & 1 \end{smallmatrix} \right)$. 

I ask because I'm teaching a rigorous undergrad analysis class. My students keep asking me whether they have to believe that $V \to V^{\ast \ast}$ can fail to be an isomorphism. Of course, I'm trying to change their intuition to point out why most mathematicians find the failure of isomorphism plausible and point out that there are more subtle ways to salvage the claim, such as Hilbert spaces, but I'd also love to be able to give them a choice free proof that there is some vector space where this issue comes up. 

If $n \geq 3$ and $K$ is a field of characteristic not dividing $n$, containing a primitive $n$-th root of unity $\zeta$, then the $3n$ points of the form $(1:-\zeta^a:0)$, $(0:1:-\zeta^b)$, $(-\zeta^c:0:1)$ are a Sylvester-Gallai configuration. In particular, taking $n=p-1$, this gives an SG configuration over $\mathbb{Q}_p$ for $p \geq 5$. 

Let $n \geq 2$ and let $a < b$ be real numbers. Then it is easy to see that there is a unique up to scale polynomial $f(x)$ of degree $n$ such that $$f(x) = \frac{(x-a)(x-b)}{n(n-1)} f''(x).$$ 

More generally, regarding "a better way": For any matrices $A$ and $B$, let $F(x,y,z) = \det(z \mathrm{Id} + x A + y B)$. This defines a curve in $\mathbb{P}^2$. Your condition is every line through $(0:0:1)$ intersects this curve in a point of multiplicity $\geq m$. This turns out to imply that $F$ must have a component of multiplicity $\geq m$: i.e. $F$ factors as $G^m H$ for some homogenous polynomials $G$ and $H$. If $A$ and $B$ had a common eigenspace, then $G$ would be linear, and $A$ and $B$ would have direct sum decompositions $A = A_1^{\oplus m} \oplus A_2$ and $B = B_1^{\oplus m} \oplus B_2$ where $(A_1, B_1)$ and $(A_2, B_2)$ give rise to the plane curves $G$ and $H$. The preceeding examples show that life can be more complicated. In the first example, $F=G^2$, for some nonlinear $G$. In the second, the factorization of $F$ does not correspond to a direct sum decomposition of $(A,B)$. There is a fair amount of literature on expressing plane curves as determinants, so there is probably someone who has looked specifically in the case of a multiple factor, but I don't know who. 

The closest thing I can think of to what you want is triangulations of even dimensional cyclic polytopes. These are nice because, unlike triangulations of the cube, they all use the same number of simplices. See the above linked paper for more. There are a number of other important generalizations of Catalan numbers, but these are the only ones I would particularly call higher-dimensional. 

If $n$ is prime, I will show that $Tr(A^r B^s)_{1 \leq r \leq n,\ 0 \leq s \leq n-1}$ is algebraically independent. This is one less the potentially optimal $n^2+1$, since all such traces live in the ring of $GL_n$ conjugacy invariants and that ring has dimension $n^2+1$. Since smaller matrices include into larger matrices, this shows that $Tr(A^r B^s)_{1 \leq r \leq p,\ 0 \leq s \leq p-1}$ is independent where $p$ is any prime less than $n$. By Bertrands postulate, this gets me up to at least $(n/2)^2$ (and $n^2(1-o(1))$ for $n$ large). I suspect that the stated list is always algebraically independent, but I don't want to work more on this. We recall the use of Jacobian determinants to establish algebraic independence: If $f_1$, $f_2$, ..., $f_r$ are polynomials in variables $x_1$, $x_2$, ..., $x_r$ (and possibly other variables as well), and there is a polynomial relationship between the $f_i$, then $\det(\partial f_i/\partial x_j)$ will be identically zero. (Proof sketch: Implicit differentiation of the polynomial relation.) Thus, if I show that this determinant is nonzero at any point, it shows that the $f_i$ are algebraically independent. The indices of my matrices will always be modulo $n$. I will consider the above traces as functions of the $n^2$ variables $A_{ii}$, for $1 \leq i \leq n$, and $B_{jk}$ for $1 \leq i,j \leq n$, $k \not \equiv j+1 \bmod n$. I will evaluate the Jacobian at $$A = \begin{pmatrix} \alpha_1 & & & \\ & \alpha_2 & & \\ & & \ddots & \\ & & & \alpha_n \end{pmatrix} \qquad B=\begin{pmatrix} & 1 & & \\ & & \ddots & \\ & & & 1 \\ 1 & & & \end{pmatrix}$$ where the $\alpha$'s are distinct and nonzero and all unlabeled entries are $0$. First, observe that (among the variables we are differentiating with respect to), $Tr(A^r)$ depends only on the $A_{ii}$. Therefore, the monomials $Tr(A^r)$ and the variables $A_{ii}$ form an $n \times n$ block in our $n^2 \times n^2$ matrix, and it is well known that this block is invertible whenever the $\alpha_i$ are distinct. So we can concentrate instead on the $(n^2-n) \times (n^2-n)$ matrix which uses the monomials $Tr(A^r B^s)_{1 \leq r \leq n,\ 1 \leq s \leq n-1}$ and the variables $B_{jk}$, $k \not \equiv j+1 \bmod n$. Fix $s$ and think about $Tr(A^r B^s)_{1 \leq r \leq n} = \sum_i \alpha_i^r (B^s)_{ii}$. As $r$ varies, we get $n$ different linear combinations of the quantities $(B^s)_{ii}$. Since the $\alpha$'s are distinct and nonzero, this linear transformation is invertible, so we may think instead about the $n^2-n$ functions $((B^s)_{ii})_{1 \leq s \leq n-1,\ 1 \leq i \leq n}$. Let $k \not \equiv j+1 \bmod n$ and consider $\partial ((B^s)_{ii})/\partial B_{jk}$, evaluated at the $B$-matrix above. It will be zero unless $j \equiv k+s-1 \bmod n$ and $(k,i,j)$ are weakly circularly ordered. In that latter case, it will be $1$. Thus, the $(n^2-n) \times (n^2-n)$ Jacobian breaks into $n \times n$ blocks. For each block, we fix and $s$ and look at the $n$ functions $(B^s)_{ii}$ and the $n$ variables $B_{jk}$, $j \equiv k+s-1 \bmod n$. This block looks like $$\begin{pmatrix} 1 & 1 & 1 & 0 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 1 & 1 & 1 & 0 & \cdots & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & 1 & \cdots & 0 & 0 & 0 \\ & & & & & \ddots & & & \\ 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 1 & 1 \\ 1 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 & 1 \\ 1 & 1 & 0 & 0 & 0 & \cdots & 0 & 0 & 1 \\ \end{pmatrix}.$$ Here there are $s$ ones and $n-s$ zeroes in each row. The eigenvalues of this circulant matrix are $1+\zeta+\zeta^2+ \cdots + \zeta^{s-1}$ when $\zeta$ runs through the $n$-th roots of unity (including $1$). If $n$ is prime, all these sums are nonzero, so the determinant is nonzero and we win. 

You might be interested in the Equivariant cohomology wiki, which has a table of which cohomology theories have been computed for which symmetric spaces. 

I'm not an analytic number theorist, so take this all with many grains of salt. Let $S(x,y,p)$ be the set of integers in the interval $(x,x+y)$ which are NOT divisible by $p$. The argument you are imagining for the prime number theorem is $$\pi(x+y)-\pi(x) = \left| \bigcap_{p \leq \sqrt{x}} S(x,y,p) \right| \approx y \prod_{p \leq \sqrt{x}} \frac{|S(x,y,p)|}{y} \approx y \prod_{p \leq \sqrt{x}} \left( 1 - \frac{1}{p} \right).$$ Your point is that the second $\approx$ is not good on a term by term basis if $p>y$, because the true value of $|S(x,y,p)|/y$ will be either $1$ or $1-1/y$, not $1-1/p$. You therefore suggest that the whole composite approximation should also not be good. I see two immediate issues: A product of inequalities is not an inequality While it is true that $|S(x,y,p)|/y$ is not $1-1/p$, this formula can be wrong in either direction. So it is possible that the errors cancel and the products are close to equal. The sketched proof doesn't work for large $y$ Even when $y$ is as large as $x$, there is a huge issue: $\prod_{p \leq \sqrt{x}} (1-1/p) \approx e^{- \gamma}/\log \sqrt{x} = 2 e^{-\gamma}/\log x$, not $1/\log x$ as we want. So there is already something sketchy here. I don't have an intuition for why the right constant in the PNT is $1$, not $e^{-\gamma}$ or $2 e^{- \gamma}$, but since I already know that there is an issue with this sort of argument, I wouldn't take it too seriously in predicting exactly when PNT would fail. (To clarify, I know many arguments why the constant must be $1$: For example, $\sum_{p \leq n} \frac{n}{p} \log p$ should be $\approx \log n! \approx n \log n$, and this only works if the constant is $1$. And I know why the sieve argument doesn't rigorously prove that $\pi(x) \approx e^{-\gamma} x / \log x$. What I don't have is a gut level understanding of why the sieve formula is right up to the constant factor, but not actually right.) None of this amounts to an argument FOR the conjectures of Granville and Soundararajan, it just argues that I wouldn't take your heuristic particularly seriously.