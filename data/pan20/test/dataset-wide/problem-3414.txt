Let $X = h(Y)$. A first-order Taylor expansion around $E(Y) = \mu_y$ gives $$X \approx h[\mu_y] + h'[\mu_y]\cdot [Y-\mu_y]$$ This easily leads to $$\sigma^2_x \approx \big(h'[\mu_y]\big)^2\cdot \sigma^2_y$$ Since $$\eta_{x,y} \equiv h'\cdot \frac Yh \implies \eta_{x,y}(\mu_y) = h'(\mu_y)\frac {\mu_y}{h(\mu_y)} \implies h'(\mu_y) = \eta_{x,y}(\mu_y) \cdot \frac {h(\mu_y)}{\mu_y}$$ Substituting, $$\sigma^2_x \approx \left(\eta_{x,y}(\mu_y) \cdot \frac {h(\mu_y)}{\mu_y}\right)^2\cdot \sigma^2_y$$ Taking the square root we are led to $$\frac {\sigma_x}{h(\mu_y)} \approx \eta_{x,y}(\mu_y) \cdot \frac {\sigma_y}{\mu_y} $$ The difference of the above from the expression in the question is that a) The elasticity must be evaluated at the center of the Taylor expansion used and b) $h(\mu_y)$ is usually equal to $E(X)=E[h(Y)]=\mu_x$ only to a first approximation, due to Jensen's inequality. Of course if the relation between $X$ and $Y$ is linear, and/or the elasticity is constant, things specialize. In any case, it is valid as an approximation to write $${\rm cv}_x \approx \eta_{x,y}(\mu_y) \cdot {\rm cv}_y $$ where "cv" stands for "coefficient of variation". 

After a comment exchange, let's provide the answer under disrete-time formulation. The problem is now written \begin{align} &\max_{\{u\}_1^T, \{y\}_1^T}\sum^T_{t=1}{F(y_t,u_t)}\\ \text{s.t.} \quad & y_{t+1}-y_t = f(y_t,u_t)\\ & y_1 = \text{given}\\ & y_{T+1}~~\text{free} \end{align} where $y_{t+1}$ is the value of the stock variable at the begining of period $t+1$/end of period $t$. The Lagrangean is \begin{align} \Lambda = \sum^T_{t=1}{\big(F(y_t,u_t) + \lambda_t[f(y_t,u_t)-y_{t+1}+y_t]\big)} \end{align} Note that $\lambda_t$ is associated with $y_{t+1}$. The FOCS are (to hold $\forall t$ in the horizon) $$\frac {\partial \Lambda}{\partial u_t} = F_u(y_t,u_t) + \lambda_tf_u(y_t,u_t) = 0 \tag{1}$$ $$\frac {\partial \Lambda}{\partial y_{t+1}} = -\lambda_{t} + F_y(y_{t+1},u_{t+1}) + \lambda_{t+1}[f_u(y_{t+1},u_{t+1}) +1] = 0 $$ $$\implies F_y(y_{t+1},u_{t+1}) + \lambda_{t+1}f_y(y_{t+1},u_{t+1}) = -(\lambda_{t+1}-\lambda_{t}) \tag{2}$$ which are the conditions we would have obtain if we have used the Hamiltonian formulation. To obtain the terminal value for $\lambda_t$, we set $t=T$ in $(2)$, (since the last variable in the $y$-sequence is $y_{T+1}$), and we obtain: $$F_y(y_{T+1},0) + \lambda_{T+1}f_y(y_{T+1},0) = -(\lambda_{T+1}-\lambda_{T})$$ $u_{T+1}=0$ because there is no decision to be made in period $T+1$. Moreover $\lambda_{T+1}$ should also be set to zero, since it is associated with $y_{T+2}$ (end of period $T+1$) and we are out of the horizon. So we are left with $$F_y(y_{T+1},0) = \lambda_{T}$$ Whether now we will get $\lambda_T=0$ or not depends on the formulation of the objective function $F(y_{t},u_t)$: does it indeed include the state variable? If yes what is the value of its derivative with respect to the state variable when the decision variable is set to zero? Etc 

From this second quote, one could indeed indirectly validate the OP's second quote - that in order to provide a rationale for wage stickiness, we should restrain ourselves to cases and models where wage stickiness, in order to arise does not require blatant inefficiency of the contracting parties (because, after all, "blatant inefficiency" can explain everything). The important remark here is that not all sticky-wage outcomes are necessarily inefficient. But this essentially concurs with Barro's (1977) approach: Barro criticizes specific sticky-wage models where contracts have provisions only on the wage and not on the level of employment (or they have arbitrary such). Barro shows that not providing for employment determination may create dead-wight losses for both parties. On the other hand, providing an optimal rule for employment determination (optimal in the sense of "maximizing the pie", as he writes, available to employers and workers), would eliminate this dead-weight losses, and then, the issue of wage determination is to be considered, which may very well imply fixed nominal wages. pp 311-312: 

The Accounting mistake is in step 1, obviously, since you performed a single-entry and not a double-entry: The Central Bank books are not imbalanced: schematically speaking the CB credits the account "money creation" (or "Our future") and debits its own "Cash Available": this is the essence of "the printing privilege": the CB has the right to create money out of thin air. Then, it credits its Cash Available account and debits the customer/borrower "Commercial Bank" -because Commercial Bank don't just get money from the Central Bank, they borrow it (or the Central Bank instead of lending money, it buys shareholder stock of the commercial bank or any other so-called "open market operations"). So all books balance. As regards growth in such a balanced-book situation, the key is in step 5, "People buy good or service from company": What tells you that the firm has the capacity to serve the demand it faces? If demand is now increased because new money have flown into the economy (and initially people treat money as a store of value/purchasing power, not just as a medium of exchange in a static situation), the firm may have to increase its productive capacity to satisfy total demand. If it cannot do that, the firm will see an opportunity to raise prices, hence new money will just cause inflation. But if the firm can increase its productive capacity, namely, if there is people unemployed and capital laying idle, then the firm will "call to arms" these currently unemployed factors of production, and it will put them to work and produce, thus increasing output and so creating growth. This effect can certainly be written using the balanced-book Accounting Language, but I will leave that as an exercise. 

As for the term "isoquant" Lloyd writes "In his second note on the elasticity of substitution in the same journal, Lerner (1934) called the curve the “isoquant.” This is one of the first uses of the term isoquant in the English-language literature." (Lerner A.P. 1934. “Notes on Elasticity of Substitution II.” Review of Economic Studies 1(2): 147–48.) As regards the question "why isoquants took so long after the indiferrence curves invention", from the three possible (and not antagonistic) explanations that Lloyd puts forward I find very interesting the third one: in his words (p. 658) (bold my emphasis) 

$b_t$ is the decision variable, and the value function is formulated as a function of the state variable of the problem. In your case, the role of the state variable is played by the $p$ parameter, which may change in each period (updated). So your Bellman equation is $$V(p_t) = \max_{b_t} \{u(b_t) + \beta E [V(p_{t+1})\mid t]\}$$ and I guess now you see where the updating of $p$ enters the picture. On "Value function iteration", or "iteration on the Bellman equation", the book "Recursive Macroeconomic Theory" of Ljungqvist & Sargent contains clear and practical guidance. 

By getting into a credit card contract, you effectively obtain a revolving credit limit, essentially a loan that once paid is again available (in contrast, under a debit card contract you only get the facilitation of "plastic money/carry no cash", but you are contractually obliged to fully pay the balance on the date due. Diner's Club, the first card, was and remained for decades, such a "debit" card). Now, people don't just "forget" to pay the credit card on time in order to not incur interest costs: they may purposefully leave some amount unpaid, in order to smooth their consumption patterns, effectively using the credit card as a loan facility to do that. What? With the usual "exorbitant" interest rates credit cards charge? Well, if you cannot get any other kind of credit, you take what you can get, if you really need it, or want it. So the bank looks at: 1) The fees that collects from the supplier 2) Handsome interest income from customers leaving some amount of the utility bills unpaid, turning it into a loan 3) If they are new customers, lured from the "bills payment" ad, that they will use the credit available to make other purchases also, so more fees and more interest income. 

I agree with the members that have voted to close, but I felt obliged to point out an important confusion in the question. The OP writes 

"Putting them into the paper" is indeed "packaging". This could refer to bulk packaging -say, put many pins into a (paper) large bag, and the final customer takes out the desired number, in which case you also have a on-the-spot retail packaging (which is how it is still done today in many terittories), or create small packages of pins to be sold as a unit (usually round numbers of them). 

Let the states and the institutions declare what they want. To study economic phenomena we must look at what economies actually do. Think of something totally unrelated: each state has a specific tax structure, a range of various taxes, progressive, proportional, whatever. The official structure matters, because it anchors the effective taxation, but effective taxation itself matters more. In your case, the Irish Economy incorporated in its workings the official monetary union, while the UK Economy didn't. So one has to go per case, which creates a testable hypothesis: a) postulate an hypothesis based on "casual" (i.e. true but unquantified) observations like the 2nd sentence in the above quote b) derive the theoretical implications of the hypothesis c) send your theory and your theoretical results out to meet the data. For your example, assume that UK increased steadily its money supply to accommodate a growing economy, while the Irish economy was stable (or stagnant, depending on your philosophical point of view), or grew at a lower rate. If we add the GBP quantity in the money supply variable of the Irish Economy as equivalent, $1:1$ to the IEP, shouldn't we observe excessive inflation? We should. Did we? Let's say "partly". Then we can estimate the effective ratio between the two currencies as regards their effect on the Irish Economy. 

From the above we can see that the $J$-nullcline, although it starts increasing, it never approaches again the value $8$, which is $\approx$ the low asymptote for the $u$-nullcline. 

The critical points here are the phrases "it is recommended that...", and "using four lags is a norm". "Recommended" and "norm" based on what? On a specific prevailing theoretical model, or on past experience with the specific kind of data? If it is the latter, you do not conflict with any economic theory, if you try something different. This is also a good example of the need to arrive at a synthesis where the model is both theoretically supported and also statistically adequate (consult the works, especially the books, of prof. Aris Spanos on this important methodological matter). 

Regarding the first question, @Oliv suggestion in the comments to formally write down the consumer's problem, budget constraint, Kunh-Tucker multipliers and all, and solve it (carefully -linear objective functions easily mislead when we are used to deal with non-linear ones), is the way to go in order to see how the mathematical set-up will lead you to the answer that stares you in the face: Our purpose here is to maximize the utility index. Given unitary prices, isn't it obvious that the multivariable function $$\hat{u}(x_1,x_2,x_3) = x_1+x_2+2x_3$$ will take its maximum value if we purchase only good $x_3$? For any tiny amount $\Delta x_3$ of $x_3$ that you don't purchase, you lose $2\Delta x_3$ in utility terms, while you gain (in utility terms) only $\Delta x_1 + \Delta x_2 = \Delta x_3$ for any combination of $\Delta x_1$ and $\Delta x_2$. Since $2\Delta x_3 > \Delta x_3$ it will be suboptimal to purchase anything else than $x_3$. That was the "plain logic" part, and I hope it answers the "what I don't understand..." aspect of your question. Put the model in math to reconcile this obvious arithmetic with the formal approach. 

The original paper (of Altman) is Altman, E. I. (1968). Financial ratios, discriminant analysis and the prediction of corporate bankruptcy. The journal of finance, 23(4), 589-609. We read (p. 593) 

Responding to comments We check returns to scale for this function by examining, for $k>1$, the expression $$ \prod_{i=1}^{m}(kx_i)^{a_i} = k^{\sum a_i}\cdot y$$ If the sum of the alphas is higher than unity, output increases more than $k$ so we have increasing returns to scale, and correspondingly for deceasing returns to scale when the sum of the alphas is smaller than unity. Regarding decreasing marginal returns, the rate of change of marginal output generated by a factor keeping the others fixed is given by its own second partial derivative, which here is $$\frac {\partial^2 y}{\partial x_i^2}= a_i(a_i-1)\cdot \frac {y}{x_i^2}$$ When $0<a_i<1$, these second partials are all negative. 

Normative questions require a standard of fairness in order to then be analyzed and answered with some degree of "objectiveness" so that the answer is not just a declaration of philosophical-ideological positions. The OP starts by writing 

As already noted, the constraint is such that it turns itself into an equality constraint. Your problem then is simplified to $$\max_{x,y} (xy) \;\;\; s.t.\;\; x+y-2=0$$ Rearrange the constraint and substitute for $y$ to obtain the single variable unconstrained problem $$\max_{x} (2x-x^2)$$ This provides immediately the solution $x=1$ and so also $y=1$. Using the Karush-Kuhn-Tucker conditions on the original problem, may be good practice in order to see for yourself that the complementary slackness condition must also hold (and "Slater's condition" is one of the formulations of it), but Occam's razor would require that the problem should be actually solved as above. 

It goes the other way around. VAR's are a special group of models for time series data, developed mainly for atheoretical forecasting purposes. In a VAR approach we do not assume some structural economic model, we just want to use the past of a variable and its interelations with other variables, to form a good quality predictor (and it is for this reason that when they were first introduced and propagated by Sims, they created a lot of controversy). In general, estimating a production function of a single firm using time-series data does not alter the econometric approach described in the answer to your other question, only here the assumption we make is that the unknown parameters refer to a single firm (and not to many firms), and so stay constant across time (and not across firms on the same time period. A new econometric issue is that with time series data we are forced to deal with the aspect of serial correlation (in a cross-sectional setting, "serial" correlation makes no sense because we can permutate the index without consequences). Serial correlation may emerge in two ways: a) In the regressors b) in the error term. A new economic issue in a production function setting is the intertemporal change in Total Factor Productivity. 

UPDATE After e-mail communication with one of the authors G.W.Kaplan, I recalibrated the value of the vacancy-posting cost parameter $k$ in order to obtain a cross of the two nullclines for $u=0.05$. This is achieved for $k=7.41$ (rounded). Moreover, with this value of $k$, I get a second (but not a third) steady state. A close up diagram : 

The emphasis on the word "willing" is mine. It reveals that the increasing relation is between price and intended quantity supplied, it has to do with the suppliers' "wishes". As the answer by @denesp points out, what will actually happen in the market is a more complex story, since apart from the suppliers, we have to take also into account the reaction of the consumers (at least) to (actually or prospectively) increased prices.