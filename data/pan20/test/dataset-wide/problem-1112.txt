a) categories (db3) - I think you will use this for the tag cloud b) products (db1) - name and details of each product c) product_categories (db2) - a single row for each product and each category 

I have refactored the query to use left joins and IF statement checks for the joins so that you do not have to use nested sub-queries. The group by for sku and site will give the totals by site if needed, but this can be changed to suit your needs. I am not sure of how the responses_authnets table joins to the orderlineitems for the siteid so this may need additional clarification 

Schedule a) and b) as cron jobs, but remember to removed files which have been loaded so that you do not get duplicated data These are just guidelines and will be made more complicated depending on the formats of the data and of the database into which they are loaded 

Whenever you want to see what SSMS is running, you can always fire up a Profiler/Extended Events trace and filter on your login. Doing this from MSX master and viewing a Multi-Server's job history gets you this query: 

The exec @distproc section is roughly at Line 537 of sp_MSrepl_helpsubscription. It's building the proc call and should be something like [SeverName].distribution.dbo.sp_MSenumdistributionagentproperties From the error, it appears that @distproc variable is not getting set properly, or at least getting set to an empty string. Why? That's hard to know without more information or being able to test on your system. Some unusual setup with the distributor, perhaps? But this will hopefully point you in the right direction so you can walk through it. (Run that code on the Database that is being published). 

I dont think so except for the fact that there is a 5NF, which describes a design where your joins are only on the candidate keys. Many "4NF" designs meet this criteria, but not all, and it is definitely something you can change a 4NF "into" to be be more normalized. 

You are grouping by the thing you are counting, not by the department name. Change your group by to: 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

This table would store the ids for both the previous tables and any information that is specific to that exam's instance of that question. This would allow you to write queries such as "How many questions belong to one exam" 

We maintain database scripts as part of our application codebase which is maintained under version control. However we use different "processes" for development and production code Development we maintain the following scripts: 

MySQL Workbench does not have the capability to "infer" the relationships from table column names, or MyISAM tables. It requires that the constraints be defined in the database or SQL script being reverse engineered. 

Oracle is a Humvee, MySQL is a Camry or Prius, while SQL Server is a RAM truck (in between the two). However you can follow $URL$ for insights of an Oracle DBA to MySQl. Alot has changed over the past year but it still a good primer. 

Since your plan is to re-use schedules I would recommend a change in your thinking as follows: a) Each service location can have one or more schedules - called location schedules (basically this is an association between a location, a predefined schedule and other data) b) Each location schedule can have a priority (which allows you to create a custom schedule that overrides the default schedule) c) Each location schedule has a start date and an end date when its active - which allows you to define a custom schedule which is active for only a specific period of time d) A schedule has a name and description e) A schedule has one or more schedule details - day of week, opening time, closing time I have included an explanatory data model (MySQL) 

Yes, SQL Server can report how long it took to do any of those actions (though you may have to run it to get some additional details such as actual row counts returned) Statistics Time 

For an example of windowing functions and subqueries in T-SQL(if you assume g1 is your table and not some subquery I just invented): 

This table would only store information relevant to an exam, such as the title of the exam, the exam's attributes and categories. 

As a fairly newly minted DBA under the gun, I have run the gamut of free tools and done some experimentation in the paid space (DPA, SQL Sentry, and Foglight) and it really depends on what you want the tool for. In my experience the most important thing was not just communicating performance baselines (management vastly didn't care unless there was someone to yell at), but produce something in an easy to consume format that made the priorities clear and was able to track down performance issues in production. You can absolutely build up your skills by going the free route, and the tools for SQL Server are great. 

Short answer is yes, that is the default behavior. SQL is a data access language that will run statments in batches, you'd have to use other features (Service Broker for example) to run statements asynchronously. One of the main concepts of the language is the idea of Transactions. There are several articles out there which explain this concept in detail with straightforward examples. For Example. I'd highly recommend practicing this because understanding how to use Transactions becomes especially important when one statement relies on the successful execution of the previous statement. 

The trick to Resource Governor is thinking about what you want to protect rather than what you want to throttle. You set minimum/reserve resources for your good users which protects them from the bad users. My advice is to start conservative and slowly turn the knobs. You'll likely just need one Resource Group for your "Good Users". You would set their minimum CPU and/or Memory to, say 10%. Everything else, including your Bad Users, will go to the Default pool. If Bad Users were all running CPU heavy queries, and the Good User's query came along, it would throttle the Bad so the Good would be guaranteed to at least have 10% of the CPU. You may decide that a 10% minimum isn't enough and you need to increase it. Just be measured in your approach. I personally have never found the need to touch the Maximums, though I suppose there are use-cases. Also, the Resource Governor DMVs provide some great information. You can create your "Good Group" and just leave it at the defaults to see the stats. They are quite interesting on their own. Another thing to be aware of is to make your Classifer function very simple. Remember, this function will get called on every connection so SQL will know which Group to route the connection, so don't put a lot of crazy logic in there. Typically you'll just do simple CASE compare based on Application Name or Host Name or Login Name. for example: 

With these and some additional databases/tables and jobs and time you can build out a basic monitoring system (but it isn't pretty) these are tools for DBAs; unless you are good at BI stuff you will struggle to find time to produce useful business friendly stuff from it, though the Ozar sp_blitz app is pretty dang cool. After spending around a year doing the free thing and resolving plenty of issues (but not getting much buy in) I was able to make it clear, after a major issue, that perf monitoring software was a priority, and we were going to buy it come hell or high water. After demoing the previously mentioned clients, I chose DPA because management could easily consume the results, though I definitely have client licenses for SQL Sentry Plan Explorer Pro (1000% worth the money) and really liked using the server version, it just didnt grab them the same way. I also tried getting SQLNexus working at one point but I ended up working a lot than I was interested in, it may suit your needs. 

You can get that information from PHPMyAdmin by clicking the "Data Dictionary" link at the bottom of the list of tables in the database. I thought that PHPMyadmin would create an ERD or database documentation, but I cannot seem to find the link. However you can use MySQL workbench ($URL$ to generate an ERD by connecting to your database or from an SQL file export 

I would recommend modeling it as a sale with an adjustment for the trade-in, assuming that as a car dealer you accept trade-ins without a related sale of a car. Therefore within your system you re-use the purchase features, and can later report on metrics like how many sales are linked to trade-ins, what values of sales are linked to trade-ins, whether trade-ins boost sales, etc 

That warning is just checking to see if you've ever backed up your encryptor certificate using T-SQL. This could put you in situation where the database backup would be unreadable if you were to lose that SQL Server instance for whatever reason. No, you don't need a regular backup cycle for the cert. You wouldn't need to back it up after the initial one unless you've lost your copy. That warning is just checking to see if the pvt_key_last_backup_date IS NULL. 

Perhaps that backup was taken with a different certificate than the one you've backed up? You can check to see which Certificate was used with this query: 

The old-school workaround for this was to have your SQL Trace log to a table, then once the trace started, create an AFTER INSERT trigger on that table. The trigger would then write out the Execution plan to a different table. 

It looks like based on your error that the user you are trying to create a login for already has a login in that database with another name, so you are effectively "double mapping" them. If you have access to SSMS you can see what their mapping is by using the Object Explorer and navigating the following path: Security -> Logins -> Double click on the username This should pop up a GUI, you can see the "User Mapping" in the top left hand corner, this should show you to whom the user is mapped. 

To explore a bit more about the question and comment, I want to note that generally when normalizing your goal (among others) is to reduce duplication. A naive example of your table might be everything you have but adding things like the CSV list you mention, this ends with a very wide table that has multiple areas of duplication and which will require you to manage and clean your data (and is generally bad for performance.) A solution to this problem is to simply project the data into another table and provide the keys to join back to the original. Eg: