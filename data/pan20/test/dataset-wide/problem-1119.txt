...which would the provide basic (bitwise ) and detailed (bitwise ) information, depending on where you inserted the relevant code. I had issues once while running a stored procedure that wasn't producing the right results and I had to debug the individual statements, so I just entered the various debugging levels in the stored procedure and when required ran the sproc with the relevant values depending on the level of information I required. Examples of input values: 

SQL Server Configuration Manager When you run the SQL Server Configuration Manager and modify settings the configuration manager will add and remove permissions on files and directories based on the configuration changes made. This is why Microsoft states, that configuration changes should always be made with the SQL Server Configuration Manager. 

It's not highly recommended and is best tested in a non-productive environment first. Currently there are no documented cases of the collation being changed during the setup of a service pack on SQL Server. If you don't find any information in the setup logs, then I would try and contact Microsoft and open a case/ticket. 

I expect with a single call to each source table (AdWordsData and WebSkuLookup), this will shave the execution time down quite a bit more. 

To shut down the Report Manager web portal, open up the RSReportServer.config file. This is located in the install directory (usually C:\Program Files\Microsoft SQL Server\[Instance Name]\Reporting Services\ReportServer). Under the element, there will be several elements that begin with "Is". These control which services/functions are enabled. The last entry will be . 

To take the Reporting Services databases out of commission on this server without disrupting the service itself, you'll need to point the service to databases elsewhere. To do this, open Reporting Services Configuration Manager on the server running the RS service, and change the "Database Name" entry in the Database menu page: 

Note: I can't tell what date type is, but be careful to account for exact matches. Because there aren't any or in the ranges, exact matches may be unaccounted for if the date type isn't DATE. By creating a multiplier column, we can make just one trip to get the source data and do the work from there. Try using this as your view definition: 

Still on distribution agent job history. You can look at your stats on writer and reader thread of distribution agent. (eg.) 

Aside from what David already answered, I believe these are the most comprehensive guide to SQL Server Transaction Log Internals and Understanding How SQL Server Executes A Query that are free from the web. A must read blog. Thanks to Remus! 

It could be someone or a process/apps deleted and inserted the data in subscriber db (while cmds are being applied). Review your subscriber security settings and check what process/apps are accessing the subscriber db. Multiple publications connected to subscriber db. pub1 (deleted the data first) then pub2 tried to UPDATE/DELETE the data which could cause an error 20598. Triggers on subscriber tables that could delete/insert/update the data. 

No. This is normal. You are experiencing too many applications installed on your database server. Start migrating your database (SQL Server database engine) to its own server or start uninstalling all other applications in the database server. Currently your server have the following: , , , , installed. Not to mentioned if you have monitoring tools or anti-virus installed. When you login to the server, you also use memory. filter drivers also use memory. Check out this blog post by Jonathan. Let's say, you've a barebone database server (without the other application installed fighting for memory), you can at least have 27GB of memory for your SQL Server. 

Change this value from True to False. Save the file and close. Some changes to RSReportServer.config require a service restart, so you may have to restart the RS service to get this change to stick. Once it does, the RS service will no longer respond to requests to the Report Manager portal but the separate web service that processes report requests will continue running. You can read more about RSReportServer.config and switching individual services on/off here: $URL$ 

It looks like this query is making a lot more round trips than it needs to. Whenever you have multiple select statements bound together by UNION, you can look for ways to consolidate down to a single query. If I'm reading correctly, the date ranges in the WHERE clause are conveniently exclusive of each other yet inclusive of all possible dates. These date ranges are linked to a multiplier: 

Window functions aren't permitted in UPDATE statements because UPDATE isn't compatible with SELECT or ORDER BY. Window functions are like scoped SELECT statements that re-examine the relevant rows and apply conditions like PARTITION BY and ORDER BY. In addition, many window functions require an ORDER BY clause (ROW_NUMBER, LAG, and FIRST_VALUE, for example). UPDATE statements use SET instead of SELECT, so SELECT is not allowed anywhere in the same query level. Any SELECT appearing with UPDATE must be contained in a subquery. Disallowing the ORDER BY makes sense considering an UPDATE statement is indifferent to the order in which it updates rows. There's no inherent downside to using a CTE or other subquery as a workaround to get an UPDATE to use a window function. That's the common practice advocated by T-SQL experts like Itzik Ben-Gan. (See page 29 of his book, Microsoft SQL Server 2012 High-Performance T-SQL Using Window Functions where he covers this exact scenario.) 

Deleting the Filegroup If you still have a file associated with one of your filegroups, then the complete command to delete the filegroup's logical file and the filegroup itself would be: 

(Reference: An in-depth look at SQL Server Memoryâ€“Part 2) In a Data Warehouse setup each time you load new data and each time the users request this new data it has to be read from disk into memory (buffer cache). This results in all pages that have a zero in the internal counter to be removed from the buffer cache. Page Life Expectancy Removing data pages from the buffer cache and moving new data from disk into the buffer cache, results in the Page Life Expectancy value to fall below the recommended threshold of 300, because the new pages initially have a PLE of zero (0). The sum of all new pages in memory (PLE=0) and all old pages in memory (PLE > 300) can fall below 300. A PLE of 300 is not much of a recommendation nowadays: 

This gives us 160767 ms. 5) Calculate the rows in this step based on rows per second: .0299311 rows/ms * 160767 ms = 4811.9332 rows 6) Remember how we subtracted the AVG_RANGE_ROWS earlier? Time to add them back. Now that we're done calculating numbers related to rows per second, we can safely add the EQ_ROWS too: 4811.9332 + 16.1956 + 16 = 4844.1288 Rounded up, that's our 4844.13 estimate. Testing the formula I couldn't find any articles or blog posts on why the AVG_RANGE_ROWS gets subtracted out before the rows per ms are calculated. I was able to confirm they are accounted for in the estimate, but only at the last millisecond -- literally. Using the WideWorldImporters database, I did some incremental testing and found the decrease in row estimates to be linear until the end of the step, where 1x AVG_RANGE_ROWS is suddenly accounted for. Here's my sample query: 

To see how the estimated rows decrease as we approach the RANGE_HI_KEY, I collected samples throughout the step. The decrease is linear, but behaves as if a number of rows equal to the AVG_RANGE_ROWS value just isn't part of the trend...until you hit the RANGE_HI_KEY and suddenly they drop like uncollected debt written off. You can see it in the sample data, especially in the graph. 

It didn't mention about backups. For the meantime, you can change log backup param to as a workaround. Or consider moving to another backup tools (eg. Minion Backup or dbatools) or roll your own custom code because the last update of Ola's Maintenance Plan was on Oct 7, 2016. There's a github if you like to raise an issue/enhancement. 

This will give us an idea on how long it will take to apply the commands in subscriber db. The before and after picture of commands stats is important as we can determine how slow is slow or what have changed since last baseline. 

Don't wait for 10hrs or 200GB of unsent data, setup an alert and be proactive. Additionally, consider migrating/upgrade to SQL Server 2016/2017. Use Availability Groups (better than mirroring and faster) because Database Mirroring is now deprecated by Microsoft and you won't be able to get support or updates from them (except you paid for extended support). If you run into Database Mirroring bug it is unlikely Microsoft will patch it for you. You can read the SQL Server 2008 lifecycle support here.