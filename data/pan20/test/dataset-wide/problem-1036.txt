Reference: BACKUP (Transact-SQL) Solution Nr. 1 Changing the parameter to will initialise the backup file each time you perform a backup with the same filename and will overwrite the contents of the backup file. Your backup comamnd should then look like this: 

A detailed explanation can be found here: Report Embedded Datasets and Shared Datasets (Report Builder and SSRS) There are some limitation in Report Designer: 

This will give you a list of all system generated statistics for your table and the columns in the statistics. Option 2 Just insert the data and let the database engine created new system generated statistics as soon as queries are performed against the table. 

It depends on the DBMS system being used and the time frame you are trying to examine. Your question is very simplified and leaves a lot of room for interpretation. I put your transactions and crashes into a timeline and added one for the CHEKPOINT, because (depending on the DBMS) without a CHECKPOINT no write to disk. 

Find Objects Related to Obsolete Filegroup I rigged up this script to check as much possible hiding places for tables/indexes/partitions/etc. that could be still relating to the dropped filegroup file: Please replace with the name of your obsolete filegroup (e.g. ) 

The big advantage of using DMVs is that you learn a lot about the internal workings for SQL Server. What is sp_who2? You could for example find out how works by running the following command: 

Reference: Populate Full-Text Indexes (Microsoft Docs) You might want to consider using Incremental population based on timestamp which is mentioned in the same document. 

The following article describes a similar situation and show how the author detected the culprit and resolved the situation. Reference: SQL Server: switch partition and metadata inconsistency issue (Blog dbi-services.com) 

Generally speaking: backups are advisable when you want to take additional backups outside of your normal backup sequence without breaking anything. Your normal backup sequence should not be bases on backups. 

This will output a list of corrupt blocks. In most cases rebuilding the index where the corrupt block is located will fix the corruption. Rebuild all the indexes (Script) Run the following script to rebuild all the indexes for the SYSAUX tablespace: 

You don't require at the beginning and at the end A date will not concatenate with a nvarchar without conversion (CAST or CONVERT) Because your table will contain spaces you need to put the table name in square brackets: and . You will be better off with 

_Instead of dbo you could use for example db_securityadmin_ This will create a new role in your database. Feel free to use a different name than my suggested . 2. Assign Permissions to Database Role After you have created the new database role, you will have to grant the role permissions to delete data from your talbes. This is done for each table: 

Reference: Business Logic Vulnerabilities And Some Common Scenarios Of Business Logic Flaws (Cyber Security Community) 

Pros and Cons The benefit of implementing this solution is that the departments and sub-departments are irrelevant. Everybody has to report to somebody. You can now query the hierarchy with a script to return a lovely tree view. The disadvantage is, that you have to change the each time somebody leaves the company. Solution 2: The hierarchy is in a separate table. Separate the hierarchy from the employees data The employee has a link to a hierarchy/department table. The hierarchy is not stored together with the employee data, but in a separate table Example of EMPLOYEE table: 

Depending on what the original name of the schema was in the source instance, you might want to try running the following statement: 

Reference: .MDMP File Extension (fileinfo.com) If you have a Microsoft debugger and the symbols for the relevant progams, then you can have a look at the dump file yourself: 

This can help ensure that you can restore to the RPO requested by your company's Business Continuity (BC) team and Information Technology Service Continuity (ITSC) team. You will also have to somehow guarantee that the database backups (FULL, DIFF and TLOG) are stored in a location that will not be affected by a down-time/data-loss in your current location (e.g. separate data centre, off-site data centre). When storing data off-site, ensure that you can still guarantee the RTO if you have to calculate in some additional time for data copy operations or restore over slower network connections. References 

That said you can reduce the impact of other SQL Server Agent jobs during the upgrade process by disabling the individual jobs instead of stopping the SQL Server Agent. Then I would recommend performing the SQL Server upgrade/update using the recommended steps outlined in the Microsoft Docs article: Upgrading Mirrored Instances 

Reference: SQL Server Profiler (Microsoft Docs) Extended Events Extended Events is a light weight performance monitoring system that uses very few performance resources. Albeit with the slight drawback that there was no GUI for SQL Server 2008 (ok, there is an add-on for SSMS, which can be found on codeplex.) An introduction into Extended Events can be found on Microsoft's site. 

_2105058535 being the object_id of his procedure_ My musings Now if you take the information retrieved with Mark's analysis and compare that to the original statement using the code, it is probably safe to assume the following: is similar/related to because the data retrieved contains the source code for the dummy Procedure containing the string . 

You have to be careful to run the SELECT first to check if the correct commands are being returned. You could also backup the msdb before you start as a precaution. To be on the safer side, you can right click a job and "Script Job As | Drop and Create To | New Query Editor Window" and then modify the path and execute. Be sure to remove the line containing in the sp_add_jobschedule part before you attempt to recreate the job. 

...and the feature is also documented in the Administrators Guide in the section 5.10.5 Cancelling a SQL Statement in a Session as follows: 

Run DBMS_REPAIR() Have a look at the official documentation for more details (see references) Open up a Service Request If all else fails, you might want to consider opening up a Service Request with Oracle. Reference: - sysaux corruption tips (Burleson Consulting) - Using DBMS_REPAIR to Repair Data Block Corruption (Oracle) - V$DATABASE_BLOCK_CORRUPTION (Oracle) 

From my understanding of SQL Server the default behaviour is for the second query to not display any results until the first query has been committed. If the first query does a ROLLBACK instead of a COMMIT, then you will have a missing ID in your column. Basic Configuration Database Table I created a database table with the following structure: 

Explanation Your Oracle instance has a restriction on the maximum amount of sessions allowed. This could be 100, 500, 1000 or even more. Check the parameter of your Oracle instance. 

You might have noticed that this job will report a success when the first step fails (On Fail = 1 ). References 

SQLSKills.com has a good article The Accidental DBA (Day 27 of 30): Troubleshooting: Tempdb Contention which has some general information about tempdb contention and a bit further down a script which will retrieve information about tempdb contention from some DMVs. 

Additional information in response to comment: Transaction Log backup was performed with the option TRUNCATE_ONLY - when this happens, is there any way to know this by T-SQL query Backing Up Transaction Log With Truncate_only In previous versions of SQL Server prior to SQL Server 2008 you could use the following statement: 

** Before you continue** Create a database backup () and store this database in a safe location. Even better: Seeing as your disk might be faulty, backup the database () to an external drive and store in a safe location. Possible reasons for data corruption on data growth 

Transaction Log Chain When a Transaction Log (TLOG) backup is performed, the backup information is stored in the msdb database in various tables. The information stored will contain information like , , , , , and various columns (lsn = log sequence number). You can retrieve the transaction log backup chain information from your SQL Server instance via the msdb database with the following script: 

Solution Run your query with and see what the query optimiser tells you. Then proceed from there. If you have an index on the table it might retrieve the data according to the index. If you don't have an index, then it might fall back to the algorithm. The conditions dictate the ORDER BY Optimization. Reference 

Explanation The value of ATKINSON's is . When the record is retrieved by the database engine it checks the WHERE clause, and compares the left side of the expression (which is for ATKINSON) and compares it with the converted value of which is then . This is never true. Solution 

At some point-in-time your Transaction Log backups (and Full backups?) are copied to a network drive and backed up from there by means of a backup solution, possibly combined with some kind of tape and/or disk storage. As soon as the first occurs after the last your previous files are gone... Questions to ask yourself 

I've been kicking PostgreSQL backup around the last couple of weeks, because of a new job, and I find the implementation of PostgreSQL's backup quite hard to grasp. What I have learned however, is that the a missing WAL file is the worst situation you can have. A missing WAL file will not allow you to continue your restore past the missing file, because the PostgreSQL database relies on the WAL file to be ACID. You could try to retrieve some of the information from the non-restored WAL files, but you wouldn't have any guarantee, that the resulting database/instance is consistent. 

The recommendation is to upgrade MySQL Workbench to version 6.3.9 as it has been solved in that version: 

You are only looking at the tablespaces and not at the database files. Try querying the Data Dictionary View. Here an example command: 

You can prepend an exclamation mark in SQL*Plus to any shell command which will allow you to execute the command without leaving SQL*Plus: 

So if the application is using COMPUTE or COMPUTE BY, then this will be no longer work in SQL Server 2016 as it was removed in SQL Server 2014. The list of discontinued features from SQL Server version to SQL Server version varies. References: Discontinued Database Engine Functionality in SQL Server 2016 Discontinued Database Engine Functionality in SQL Server 2014 Discontinued Database Engine Functionality in SQL Server 2012 Discontinued Database Engine Functionality in SQL Server 2008 R2 Compatibility Level and what it isn't When you set a compatibility level you are telling the database engine to behave in certain ways. 

You can even go a step further and, as you have already mentioned, split the data and the interface. Split an Access database 

E.g. if your database is YOUR_DB, the SQL Login (SQL Server level) is YOUR_SQL_LOGIN and the database user you are relinking is YOUR_DB_USR, then the command would be: 

This output will display information about how the query optimiser went about retrieving the data. Example output: 

I am not a mongodb developer, but I am not surprised that you had hit the 30 second problem. See Do you want a timeout? (mLab Blog) for hints. 

You might want to consider analysing the growth of your database over the last n days. This can be achieved by analysing the backup information in the msdb database. The following scripts are two variations of how to achieve this: 

Moving further forward (in the reference) the examined time (vertical axis) moves forward and reaches the END of T2. The transaction T2 is put into the REDO queue. Moving forward (in the reference) the examined time (vertical axis) moves forward and reaches the END of T4. The transaction T4 is put into the REDO queue. 

Answering the initial question No. Providing a solution The procedure of restoring the system databases is explained in the following Microsoft Article: Reference: Rebuild System Databases (SQL Server 2012) Prerequisites 

Reference: Identify SQL Server Database Growth Rates (mssqltips.com) Based on the information provided with these scripts you could then set the Growth Setttings of your database to be near the (Microsoft Technet Script) value or near the value (Mssqltips.com Script). If you interpolate into the future, you could possibly set the Max Size of the database files (*mdf) to be a multiple of the returned values (12 month worth?) plus the current DB size. 

The user is a SQL Server login and its password is encrypted and stored in the DMV (Database Management View) in the database. Reference: sys.sql_logins (Transact-SQL) You might notice that these views can only be found in the following branch: 

Try running the job step with a different account Try adding the BUILTIN\Administrators group to your SQL Server instance and provide them with enough privileges to run the sendmail. 

If you changed the Active Directory policy recently to lock accounts after n wrong password attempts or if you reduced the number of wrong password attempt to cause a lockout, then you might be encountering a locked sa account due to ... 

Sometimes the index and/or statistics are not up-to-date and SQL Reindex Jobs are thus unable to determince if the index is fragmented or not. Running beforehand will ensure that the indexes are then recreated or reorganized according to fragmentation level. 

Please also read the articles Backup Under the Simple Recovery Model and Restore Restrictions Under the Simple Recovery Model which will explain in depth what you have to expect then relying on a Simple Recovery Model. 

I would search the system tables in your restored master database directly, that contain the data you are looking for. You can then access for example the sys.sysxlgns table without having to rely on the sys.server_principals view which actually accesses the master.sys.sysxlgns system base table. sysxlgns = server_principals For a list of system base tables read the following MSDN article: System Base Tables Microsoft states that ... 

Amazon Support If your account has the required permissions and you are not able to free up cache, then you may want to open up a support ticket with Amazon. 

This has the advantage that the statistics are created correctly and do not have to rely on fake values. You can determine the columns of the system generated statistics by running the following statement: 

Matching and Mixing Backing up the transaction log is not the same as truncating the transaction log file, and truncating the transaction log file is not the same as shrinking the transaction log file. Oh yes, and backing up the transaction log file does not have to trigger a truncation. Depending on the current load the database engine might decide to set a checkpoint, but to wait a bit with the truncation. Explaining The transaction log file is where the database engine stores modifications made to the data in a database, regardless of whether the database is in SIMPLE recovery model or in the FULL recovery model. (Important) Now the database's transaction log file is not just one continuous storage container, but a collection of Virtual Log Files (VLFs) which are created in a sequential order inside the Transaction Log (TLog) file. The size of the VLFs varies depending on which version of SQL Server you are currently using and also on the initial size you selected during the creation of the TLog file and also which size you selected (if any) for the auto-growth setting of the TLog file. References: - Important change to VLF creation algorithm in SQL Server 2014 (SQLSkills.com) - Initial VLF sequence numbers and default log file size (SQLSkills.com) - Inside the Storage Engine: More on the circular nature of the log (SQLSkills.com) ...and maybe in the reverse order When data is modified in the database, the Database Engine will write these changes into the TLog of the corresponding database to uphold transactional consistency. This is also know as ACID - Atomicity, Consistency, Isolation, Durability. The actual transcations of these changes are stored in the VLFs of the TLog (file). When a VLF is full the newest transactions will be stored in the next available VLF in sequential order. Exceptions However if the end of the TLog file is reached, the modifications will be stored in the first VLF at the beginning of the TLog file. (explained in Inside the Storage Engine: More on the circular nature of the log) When no available VLFs are free to store new transactions and if the auto-growth setting is configured, the Database Engine will grow the TLog file by the amount defined and create additional VLFs depending on the size defined in the auto-growth settings and the formula explained in Important change to VLF creation algorithm in SQL Server 2014. Further transactions can then be stored in the next VLF inside the TLog file. Backing Up The TLog File When you trigger a backup of the TLog file all you are doing is telling the database engine to 

Creating a new table When you create a new table in your game you just initialize a new ID by setting off the following statement: 

My initial answer referenced deprecated features instead of discontinued features. This has been corrected Service Pack Levels (incl. RTM) are not the same as compatibility level settings. I'll explain: Features in different SQL Server versions SQL Server provides a rich set of features for different versions of SQL Server. These features can be removed from one version to another. Your application could be using individual system stored procedures, system functions or some other features that are no longer available. Here an example: