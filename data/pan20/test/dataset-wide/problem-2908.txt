I've been reading a bit about Kant's moral philosophy. What I'm now trying to figure out is if the Categorical Imperative would demand a redistribution of wealth. Or in other words, if according to Kant, it would be morally wrong to possess more wealth than others. Apparently, Kant himself did not think so, and considered protection of private property an important, or even the most important, purpose of the state. However, one of the examples he gave for applying the categorical imperative is that one should not refuse to help others who are in need of help. I'm not sure if he provided a rule on how to decide if others are in need of help. From what I understand, he would reject all criteria that argue with emotion or experience, and demand a rule that is based on pure deduction. In another example, the problem of lying, analogous reasoning led him do the rule that lying would be always wrong, even in cases where common sense seems to justify it, like lying to a murderer who asks if I know where his intended victim is hiding. So I assume Kant would not be afraid to choose a generalization, even if it has startling consequences, over leaving a problem undecided and open to vague, ambigious judgement. With that in mind, let's look at the example about helping others again. When do others need help, i.e. when does the rule kick in? One could suppose: Only in cases where someone's life is in immediate danger, like a beggar left out on the street in a freezing night. But what if the beggar was not in immediate danger of freezing (like possessing a warm coat and enough old newspapers to light a fire), but I would nevertheless be pretty sure that continuing to live on the streets would drastically reduce his life expectancy? And what about people who are not beggars, but are poor and can't afford health care of the same quality as the rich? Where do I draw the line? Would that not turn the problem into a question of experience and emotion, which would cause Kant to choose the safe side and retreat to an impeccable generalization: That no one should own more than any other? From what I've read, Kant explains that other humans should not be treated as means to an end, but as an end in itself. With which he means that they should not be exploited, and not be restricted in their freedom to act according to their own reasoning and moral judgement. But is being poor not a restriction of the power to act? Maybe not, if you assume that the poor became or stayed poor due to their own (ill-advised) decisions, in a free market where all deals are fair. But can it be objectively decided if a market is fair? Would Kant not prefer redistibution of wealth to that ambiguity? Again, apparently he did not. So where is the flaw in my reasoning? 

Before discussing such a question, it's better if we clearly define about what sort of god we are talking. The god of deism, i.e. a "clockmaker" who created the universe but never interferes with it afterwards, would be more or less compatible with science. But that's not the kind of god most people are interested in. I assume this question is about a concept of god that resembles the christian god: A god who interferes, who answers prayers, who performs miracles. Such a god is clearly at odds with science, because the mission statement of science is to explain the world without any supernatural interference. People might try to find some sort of compromise between the bible and a strictly scientific point of view. For example, one could claim the supernatural events in the new testament did happen exactly like that, but not the ones in the old testament; or only the resurrection of Jesus did really happen, but not his other miracles. However, to me that seems like an attempt to "hush up" the conflict between religion and science. That might allow a more or less comfortable armistice, but cannot provide an intellectually satisfying solution. So what if one got rid of all supernatural events in the bible and interpreted the resurrection of Jesus only as something symbolic? Even then the problems remains, if and how god interfered in his creation by revealing himself and communicating with humans. Maybe god does not communicate by means of angels or miracles, but by enabling you to discover his truth in your own soul when you open your mind to him? But if that method to get information about god would be reliable, would you not expect that everyone discovered in his soul more or less identical or at least not contradictory information? Whereas looking at the real world it is hard to deny that most persons' religious views are very much determined by education and cultural environment, and that different cultures have very different religions. We could try to gain knowledge about god by sticking to strictly intellectual arguments. But I guess then we would have implicitly given up everything that distinguishes religion from philosophy, and the god we construct looks probably a lot more like the god of deism than the one of christianity. To summarise, unlike some of the other answers, I agree that if people took science seriously there would be a lot less - or a lot different - belief in god. As to your actual question, why (traditional) religious faith still persists today, I can only speculate, but I don't think it's hard to see a lot of reasons: 

I would argue that humans first have a "built in" sense of what's good and bad to themselves - in the form of passions and emotions. Pleasant and unpleasant feelings provide our basic set of motivations, especially in early childhood. Only later do we develop a capability for logical and abstract thinking, which is related to the ability to "reprogram" ourselves and edit our motivations. I'm not sure what you mean by an understanding of right and wrong that does not depend on cultural background. If you mean that things like needlessly hurting others, killing and stealing are considered "bad" in pretty much every culture, I would argue that in those cases the human ability for empathy and compassion has the strongest and most obvious results. From what I know about child development (I'm no expert in that matter, though), children may have a born in sense of empathy, but it needs to be trained and developed by early education. The problem with that "childish" concept of right and wrong, i.e. an ethics based on emotions, is that we emotionally tend to care only about people we know and about members of our social group; when strangers are hurt, we remain pretty cold. I guess that is the reason why philosophical ethics was developed, to put moral judgements on a more abstract, logical foundation and avoid emotional bias. But I'd argue that logic and philosophy only provide a "bug-fixing" for the problems of purely emotional compassion; the motivation why one would need or want ethics is still mostly based on compassion and empathy. One could maybe argue that pure reason tells us that cooperation yields better results for most people than non-cooperation, and that intelligent beings profit from sharing knowledge and ideas and can do that best in a social framework of trust. But even if we develop an ethical model like that on pure reasoning, without mentioning a need for emotional empathy, the intelligent beings would have no compelling motivation to stick to those ethical rules. Maybe we could assume the basic motivation of intelligent beings is just to use their intelligence, but we know about humans that they have other, stronger emotions. As an aside, you mention the fear that an artificial intelligence might be dangerous. The classical example that comes to mind is the science fiction trope of rebelling robots. In my view, those stories are based on the questionable assumption that the most basic motivations of every intelligent being must be egoism, self-preservation, and lust for power. Of course, for humans that's a somewhat cynical but quite realistic assessment. But I see no reason why an AI would need to have a similar emotional foundation for its motivations. (That's not to say an AI might not be dangerous. I guess it depends on how independently an AI would be capable (or allowed) to act. One could imagine that even if its capable to reprogram itself, its "motivations" to do that get edited with every task it is assigned. And those external set of motivations might be poorly designed or even contradictory. For example, if we order a robot to fight in a war, we might forget to give him sufficient instructions on how to deal with non-combatant civilians. And his "motivation" to fulfill his orders might override his built-in equivalent of Asimov's first law.) 

I don't know very much about the positions on free will that important philosophers have expressed, so here's my question: One could argue if we discuss the topic "Do we have free will", the "hidden" question we actually discuss is "Do we have free will, and if not, how should we change our behaviour as a consequence?" Expressed in the latter way, the question seems to become futile, because the implied ability to change behaviour (even if only in our way of thinking) also implies free will (at least to me). And if there were no free will, we could not change our behaviour whatever the answer, so that the question becomes pointless in each possible case. Maybe one could reject the hidden question and be interested in the free will problem as a purely academic exercise. But I'm not sure if that would be so easy, since even deciding to deal with the free will problem or trying to convince others of a view on it could be regarded as changing behaviour. Since all that seem pretty obvious thoughts to me, I assume it has been discussed before, and/or contains a flaw in the reasoning. Can you point me to examples where philosophers have discussed this?