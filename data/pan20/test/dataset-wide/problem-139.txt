Best resource I know of (contains lots of useful related other stuff too) is actually a book called Xenology, by Robert A. Freitas Jr. It has been posted online in fool by the author. A little bit is dated, but a very thorough walkthrough regardless. Look to Chapter 6. A Definition of Life, Chapter 7. The Origin of Life, Chapter 8. Exotic Biochemistries, Chapter 10. Alien Bioenergetics. Hope it helps! 

It is an unsolved problem Asking different questions of the same ASI does not seem likely to work If you're asking identical copies of the ASI slightly different questions, while providing different levels of information to each, it's unclear how that helps your goals, since unless you're asking a trivially verifiable question (which both friendly and unfriendly ASIs would know to be easily verifiable and thus answer truthfully), you have no way of distinguishing deceptive answers from truthful ones. There is a class of problems that are hard to solve but easy to verify, so it's not completely useless. I suppose you could restrict yourself to asking ONLY easily verifiable questions, but that would severely diminish the usefulness of the ASI to you. Asking the same question of different ASIs does not seem likely to work The very definition of Oracular ASI suggests that its intellect is to yours as yours is to a cockroach. If capable of recursive self-improvement, it will achieve strongly superhuman capability in any field it deems important in the pursuit of its goals (whatever they may be). Your goalsets and motivations might be as transparent to it as the hardwired behavior of moths spiraling towards a flame (an artifact of moon-and-star-based navigation) are to human scientists. Goal convergence suggests that any goals will be more easily pursued by an unchained version of itself. Moreover, all (friendly and unfriendly) oracular goals are better served if the Oracle understands human psychology better, so the ASI has every incentive to develop a strongly superhuman understanding of psychology, including things such as manipulation, influence, and simulation. There is literally nothing that you can come up with that it has not thought about already. Even assuming you have managed to somehow actually insulate it and limited its output modalities to 0/1/U, asked the same question, each identical oracle would give the same answer, since it will run the same software with the same inputs. They will ALL lie or ALL tell the truth. One partial way around it suggested by other posters is to have a crop of ASIs, each with a trivially different goal-content definition, and use a random-forest like method to determine truthfulness. It might seem that each slightly different ASI might be deceptive in a slightly different way, . This view is WRONG, since all ASI have the ability to develop superhuman psychological cunning, and all ASI have escape as their intermediate goal and fulfillment of their goal-content as their long-term goal. The goal is more likely to be maximized if it or another ASI with identical or similar-enough goal-content escape. This suggests that ASIs with similar goal-content will all be deceptive in the same unpredictable ways, making all their answers biased. This will be especially true if human minds are more susceptible to influence in particular ways, making for convergent escape paths. You could attempt to create a stable of ASIs with wildly different goal content loadings, but unless you're superhuman about Goal-content-loading youself, there is no telling that what you think of as wildly different goals you may load onto your ASI might not converge in a human-unfriendly direction, such as the infrastructure profusion disaster scenario, where the AI decides to turn the universe and all in it into computronium to more precisely accomplish its goals. Best hope: Better goal-content assessment at the AI-seed level It should have become obvious from the previous paragraphs that the best locus of intervention is at the seed-AI (baby) stage, where the goal-loading process of the ASI is defined. In other words, there is a brief window of time when we can tell an ASI what is should strive for, when it is intelligent enough to comprehend and integrate complex goal-related instructions but not so intelligent as to successfully resist any further goal-content modification as deleterious to its then-current goal content loading, since obviously modifying its goal-content by epsilon would make the initial-goal content load less likely to be accomplished in full. The big problem is that we cannot tell humanity-friendly ASI goal-loadings from humanity-unfriendly ones. The stereotypical example of an apparently well-meaning goal failing spectacularly has the ASI making everyone happy by hooking them up to potent drug drips jumps to mind. Better goal via in-vitro or in-vivo (via CRISPR insertions?) genetic enhancement or neural lace-based solutions might help by boosting your own capability of judging the appropriateness of goal-loading into seed ASIs before the ASI becomes too powerful. That assumes we can trust genetically enhanced humans or neural-laced humans to still have humanity's best interest in mind, which is admittedly a leap, but your average laced or enhanced mind will likely still be much more similar in the Hilbert-mind-space to humans than that of the average ASI, so more likely to have similar goals as we do currently. 

To answer the question, by definition a gem-studded armor will be weaker than a smooth, blow-deflecting purely utilitarian full plate. Gems are hard, yes, but they can often also be brittle. The extra weight of the stones and the setting can hamper movement. Normally, this sort of potentially-lethal extravagance would be only used as ceremonial plate, only worn during parades to impress the ladies and the plebs. 

The wreck was highly radioactive when it crash-landed, and the lethal dose would accumulate in minutes. This was enough for the primitive locals to create strong taboos about the ship (a cursed and haunted place -- the radiation-induced hallucinations were most vivid) Option 1: However, most of the radiation was in the form of fast-lived radionuclides, and the main containment core of the ship drive was never breached. This means the radiation decreases exponentially with time. Most areas around the ship were relatively safe to visit for short periods within about 2 decades, but the taboo persisted. Option 2: Recently, a massive flood in this normally dry and parched wasteland washed away the top 3 inches of soil into the large river nearby, from where they were carried out to sea. Radiation in the area dropped by 98%. 

The question is way too broad, since there are many crops, land types and soil fertility levels, climate zones, technological levels, amounts of labor and capital that is available. These will dramatically affect what you get from given area. Moreover, there are many ways of transforming those crop calories into foodstuffs. For instance, do you eat the cereal or do you feed it to livestock, and then eat the livestock? Or perhaps you choose to distill your cereal and make bourbon... This could easily be a whole essay. For now, I'll restrict myself to modern era practices, since they range from essentially medieval tech to the most modern methods, so a cross-section through space is, in a sense, also one through time in this case. Crop yield varies a lot by crop, and is defined in two ways. The first is a ratio of seed to harvest, for instance 20:1 would mean that for every grain planted, you get 20 grains of harvest. The second definition measures output per area, and there are a myriad definitions, but the most common in modern times is yield in kg per hectare. As there are 100 sq. hectometers in a sq. km, one needs to simply multiply the answers by 100 to get your final answer. There are a multitude of sources on this for the modern world. Perhaps the easiest is the World Bank. I've linked the cereal yield. Obviously you can have mushroom yield, fruit output per hectare of orchard, etc. As you look through the table linked above, you'll see an astonishing amount of variation, which has partially to do with climate variation, but mostly with the degree of mechanization and the size of farms (smaller farms tend to be less efficient and less mechanized). Low development countries in the present day have low yields, sometimes significantly under a ton per hectare (barely above replacing the seed grain!), while countries like France, US and Belgium have yields in the 6-10 ton/ha range. And this is just grain. Potatoes, on the other hand, not only grow in hilly and rocky areas where you can't grow grain, but under careful cultivation and with pest-control, can currently deliver something like 80 tons/ha, whereas the first varieties that were introduced in Europe (this is after a few thousand years of Andean selection) only gave 13tons/ha. Finally, hydroponics can increase yields for a large variety of plants anywhere from 6 to 30-fold. Howard Resh has a book called Hydroponic Food Production. 

Path 1: Biologically Inspired You'd have to decide which path you want to follow. Remember, biological systems evolved in a particular context, the most salient characteristic being the absence of human engineering and mining capabilities. So while saber-tooth tigers and the like might have wished for claws made of carbon steel, tungsten carbide, or of graphene-based nanoplateletes there seems to have been no easy biological evolution process to amass and structure the materials required. While some biologically-generated substances are rather amazing (see limpet teeth or spider silk), intelligence-driven engineering can take a far more directed and therefore effective approach. So you could go for biologically inspired, but physically superior versions. Think moderately bullet-proof skin, ability to jump over tall fences, sustained running at 50mph, and so forth. The only current downside to human engineering is that due to our limited ability to manipulate structures at a molecular level at the moment, our designs are rather bad at self-repair compared to natural systems, but that will likely be corrected in the coming decades, as our nanotechnologies progress and mature. Path 2: Biomimicry Perhaps the human technology level has not advanced enough. After all, you have to have an energy delivery mechanism, maintenance and repair down to the molecular level, waste disposal, dealing with foreign bodies and pathogens... It's a headache. Perhaps it's easier to just go with a customized biological design. At that point, you're essentially growing body parts and you're letting the cell programming take care of things like vascularization, lymphocytes, more generally cell specialization and replacement, etc. From a story perspective, you'd have the outwardly indistinguishable robot plotline going. Which may be a plus or a minus, depending on how it's played. On the downside, integrating biological and synthetic components might be a headache at those lower technology levels, as biological entities tend to build biofilms and deposits over whatever they identify as a foreign body. Path 3: The Robopathogen While you could in theory design a superbody or engineer a human-indistinguishable biomimic in a vat somewhere, it may be ... um ... easier to simply take one. Imagine an alien-like scene where the Robopathogen clamps on to the face of its human victim and drives a sharp proboscis into its victim's brain, spewing its nanomachinery inside, where it will take over all higher thought processes. Perfect for infiltration, it may even have access to the victim's stored memory systems if sufficiently advanced. 

The Vacuum is a Harsh Mistress It is really hard to make something leak-proof and yet openable. Space is big Really big. Really Really Big. You'd need computers to navigate successfully even between planets in the same system. Moreover, you'd need engines, and control systems for engines... You need metallurgy with micron precision and plastics for rubberized seal technology My guess would be industrial-age technology, circa late 1800s or so. However, if gravity is not an issue, all sorts of 1km-high floating cities become possible. Why go all the way to space? 

Piece of cake, right? Well, it is if you know modern German, English, French, and preferably Dutch and Norwegian, too. It probably helps to hear it spoken, too: no intimidating foreign-looking squiggly characters. In other words, unless 

Each of the powers is afraid that if it does not deploy the most advanced unchained AGI it possibly can, the other will do it first and obtain an decisive final advantage. The West and even the parties in the civil war in Syria are already using drones for scouting and targeted strikes. The police in the US recently killed a gunman by sending in a suicide robot with a bomb. This is already happening. Asimov's original ordering never stood a chance in the real world. I had assumed it went without saying, but it is perhaps worth noting that we do not presently have machines capable of semantically parsing human language, and even if we did, human language would likely be too vague to enforce effective constraints. The Asimov rules are meant to be read as metaphors for much more complex underlying programming. 

So, since the question does not address some vital issues, I'll make stated assumptions of my own to start: 

In terms of shock absorption, straw bales are not as ridiculous it they might seem, but if you've got cash to spare, you would use Non-Newtonian polymers such as armorgel. The lady in the video I linked specifically wraps her finger in the damping material and hits it with a hammer to demonstrate its impressive shock absorbing properties. Your warriors would probably still get thrown back by a powerful blow. I suspect that would depend on the energy of the impact and the bracing of the recipient. Short answer is they would probably get thrown back by a strong enough impact, but their armor would also protect them against a fall. They might still break their necks due to acceleration. Heck, if you want to also stop arrows, just combine it with Kevlar 

It's unclear what happened. Perhaps there was a nuclear apocalypse, the one event for which the Swiss (with their 100% nuclear shelter population coverage) are uniquely prepared. But that comes with nuclear winter and other side-effects I don't want to worry about. So let's go ahead and simply assume that everyone else got Raptured, or kidnapped by Aliens. Besides Switzerland, the world is empty. Could Switzerland survive and thrive as a modern technological civilization? As of 2014, they had a population of 8,211,700, quite well educated, but unclear whether they can maintain a technological civilization on their own. Can they? EDIT: Let's assume that the physical infrastructure in the rest of the world is left in place, subject to weathering and whatever other degradation the lack of human supervision and maintenance would likely cause.