If you're on SQL Server 2012 or 2014, this is an aggregate that you can calculate with an ordered window function. 

Another solution, that might scale/perform better, uses ordered window functions (available on SQL Server 2012 and 2014 as well as a few other database platforms, but not Azure). 

Assuming an item moves between some type of state or location (, ) and the timestamp of the move is in the column, you could put your query in a common table expression, along with a function that calculates a sequence number, like so: 

Here's a long shot: I think your Management Studio has for some reason changed the keyboard mappings to the 2012/2014 default setup, which is more similar to the regular Visual Studio apps. This changed as of SQL Server 2012 so, among other things, Ctrl+F1 now opens Books On-Line. To verify if this is the case for you, you probably won't be able to execute queries using Ctrl+E or view estimated query plans using Ctrl+L. I found a way to reset the keyboard shortcuts to the "classic" setup and wrote a blog post about it a while ago. The short version is: 

This query will (a) eliminate the Sort operator (which is expensive because it is blocking and requires a memory grant), (b) create a Merge Join (which you could force with a join hint, but it should happen automatically with enough data). As a bonus, it'll also (c) eliminate the Index Scan. All in all, the MIN/MAX query uses a highly optimal index on the Orders table to identify a range of order numbers (included in the clustering key) from a non-clustered index on the date column: 

Simply for query performance, I would consider sorting the datetr column DESC in this index, although that may not be optimal for INSERTs into the table. Alternatively, you could just INCLUDE the datetr column: 

This is not sargable because SQL Server needs to evaluate for every row instead of looking up a single value in the index: 

No. To my knowledge, changing a column that is included in an index is not possible (you'll get an error message to that effect). You would have to drop the index before changing the column, and then re-apply the index again. The only exception to this that I am aware of is that you can change a column from to without having to drop the index (but not the other way around). However, if you use SSMS to change a table, SSMS may drop and re-create the index for you, but it shouldn't drop the index either way. On a side note, if you have an indexed view, modifying the view till drop any indexes on the view. 

If your table's clustered index has as the first column, this may be the most optimal query. You could simplify it by breaking it into two parts, like so: 

Even though swapping an inline function for a multi-statement function comes with its own set of advantages and drawbacks, an important difference in your case may be that the output is stored in a temp table with a clustered index before the join happens. If you've properly aligned the clustered indexes on the two functions, joining their output tables should be a speedy affair. Here's an idea of what kind of query plan you could expect (excluding the inner workings of the functions, obviously). 

There are two ways (not mutually exclusive) to improve query performance. First, you could re-write the correlated subquery into a subquery with an aggregate: 

If the cast fails, the result of TRY_CAST will be NULL and the comparison is false, but it won't crash. TRY_CAST requires SQL Server 2012 or newer. It's worth mentioning that you're potentially opening up a pit of bad things when you allow text values in a numeric column, but judging from the question, you already know that. :) 

I'm self-joining the common table expression that we created before, : On one side, groups with a positive , on the other side groups with negative ones. To further filter out which rows are supposed to match each other, the swap of the positive and negative sides' rows must improve , i.e. get it closer to 0. The and selects the "best" match to swap first. Now, all we need to to is add an , and loop it until there's no more optimization to be found. TL;DR - here's the query Here's the complete code: 

Which solution is best for you is dependent primarily on how much data you have. Try the different solutions. 

Pretty sure the problem is in the username. When you connect to an Azure SQL Database instance, foo@bar implies that you're connecting as the login "foo" to the instance "bar.database.windows.net". The credentials you use when logging into the Azure management portal won't work when you're using SSMS. Verify that you've set up a SQL Database login, and use that login. The admin account is set up in the Azure portal - you can verify the login name by clicking "Properties" on the server instance. Regular logins/users are set up as 1) logins on the database server, or 2) as contained users in each database (if you have enabled containment), or 3) in Active Directory (if you've configured one). 

And if the clause is always the same, you could even turn that index into a filtered index (available as of SQL Server 2008, if memory serves) 

So where _gap<=1, we don't want to return any rows. Where _gap=4, we want to insert 3 rows, from to . There are a few ways to do this, but I'm going to stick with here as well. I'm putting the query above in a subquery (called in my example), and for each row in that result, I'm going to any dummy table. If you expect large gaps, you may want to create a separate table with a single column for this purpose, but I'm just going to use the table along with a : 

From my limited mysql knowledge, I don't think you can use PIVOT or ROW_NUMBER(), so here's a more generic suggestion on how to solve your problem: 

Next, we need to drop the existing column (you can't just "add" an to an existing column, you have to create the column as an ). The primary key also has to go, because the column depends on it. 

IMO, the balance sheet, income statement and cash flow tables can be the same. Here's my take on it: 

Note that we have an invalid date in the table on the "29th" of february 2015, stored as a varchar. But since we're querying 2016, we don't expect to touch that row. However, there are multiple ways to solve this query: 

Most likely because the expression is interpreted as a numeric expression, i.e. which is 0 (with integer division rounding values down). Telling Access that this is a date can be done using quotes (not really recommended) or by using a date function, like this: 

Your intermittent errors probably stem from the explicit conversions that you're performing, combined with the filters. For any given query, SQL Server will create an execution plan, which you can think of as a roadmap of operations in a given order. The execution plan for any query can often vary in a number of ways, and SQL Server tries to choose the plan that will be the most efficient for every particular scenario. In this choice, lots of factors will influence how the plan is generated. Once created, the plan is cached, so it can be re-used. If the (verbatim) query isn't re-used any time soon, the data that it depends on is changed significantly or the server is restarted, the cache is cleared and the query plan will be regenerated the next time you run the query. This may very well result in a completely different plan. Now, for your query, consider the following example table: 

You can't use COUNT inside another aggregate. What you're trying to do is add 1 for each row that meets the criteria, so use the CASE to return 1 or 0 depending of whether the criteria is met or not. Then aggregate the results of that CASE with a SUM: 

To clear things up, language-wise: In SQL Server, the login is global for the SQL Server instance (a "server principal") and allows a client to log on to the server with very limited permissions. Once you've set up a login, you can assign users to this login. A user is the login's security principal in a specific database ("database principal"), which means that a login can have multiple users, one for each database where he holds permissions or role memberships. You might have added a login, but not the user, or you may not have assigned the user enough permissions in the database. To view the definition of database objects, your user will probably need , either on the database, the schema or on the individual object. These permissions are inherited. If you want to be able to read the data of tables, add permissions. Membership in the database role gives the user complete read-only access to all tables and views in the database. I've recently written a series of blog posts on SQL Server security that might give you a little more details. 

Also adding to the indexed columns (you can't INCLUDE it) is certainly not pretty, but if you're desperate, it'll probably help you. 

Obviously, you should have an index on as well for this to work properly, but I'm guessing that's already taken care of. I've written this code for SQL Server, but since you haven't tagged your dbms, I can't be sure it'll work on your setup. 

I suspect that there's a database error that you're not seeing here. If you remove the statement right before the two final statements, I'd imagine that the query crashes with something like "unknown column 'group_id'". This happens because an execution plan is generated for each batch at a time, i.e. every time you pass . In my experience, this plan does not always consider statements that make schema changes (except operations and index operations). If you try to generate a query plan that includes the column before the statements have actually been executed, that column won't actually exist yet. My best advice may not work for every conceivable scenario, but it will for yours: create the temp table correctly from the start, in this case by including the columns that you're ing in the original statement at the top, so you won't have to it later on in the code. 

To put things very simply, if the column is in the SELECT clause, it has to either appear in the GROUP BY or inside an aggregate function. In your case, you're probably looking for this: 

With a grand total of 727, each group should have a score of about 182 for the distribution to be perfect. The difference between the group's score and 182 is what we're putting in the column. As you can see now, in the best of worlds, we should move about 40 points worth of rows from group 1 to group 2 and about 24 points from group 3 to group 0. Here's the code to identify those candidate rows: 

I would add the events and/or as well. That way, you can compare the SPID (the session ID) and find which query executed on which SPID. For the record, Profiler is being replaced by Extended Events (XE for short) as of SQL Server 2008, though the GUI is really only usable as of 2012 in my opinion. 

If rows in can span multiple week, you may need to calculate some type of distribution of those. Here's a simple idea: 

Is there a practical way of doing this? I would use /, but this is harder to implement with the automated build process, and given that I would first like to inspect the results (from a different database connection) before committing or rolling back. 

My take on this is that you may perhaps want to try using a parent-child dimension, so the model will allow for a theoretically infinited amount of levels. That way, you can connect facts to each level of the hierarchy. Here's an example: 

.. you can add a new partition by "splitting" the last range from (30 to infinity) to (30-39) and (40 to infinity). Here's the syntax: 

My go-to solution would be to the offending LOB columns in the source SQL statement to a fixed-length type, but I can't seem to figure out the correct syntax or data type name in 4D. I think the following error message points to a syntax error from 4D: 

Then, you could modify your query a little bit to help the optimizer make some smart choices, like aggregating certain data streams before they're joined and create a much larger product that will take more time to aggregate: 

I haven't the slightest idea if this is the "correct" solution according to Kimball, Inmon, etc. Instead, I would urge you to see if it applies to your particular needs. ;) 

This code is from a system stored procedure called , which returns the amount of allocated space (pages) for database objects. Look for home-brewn monitoring jobs or maintenance scripts that may be running a bit too frequently. 

You should definitely (in my opinion) not have 142 tables - it'll be a complete mess to name, index and maintain, and you'll generate yourself a lot of extra work if you some day add another category, if you need to move ads from one category to another, etc. Storing JSON blobs in the database will kill performance when you're performing searches, so I wouldn't go with that either. A good relational table design (yes, with joins) is really the best approach when you're using a relational database system. For instance, one table with categories (12 records), joined to one table with items (142 records), joined to a table of ads. 

I would say that having and in the table is still normalized. Having the data in different tables seems excessive, and in practice you'll have a 1-1 relation between the two. If you expect a lot of read traffic on the User table, add appropriate indexes to that table instead. 

When you add a clustered index to an existing table, SQL Server needs space to build the new index structure, in essence the entire table. If space is an issue, you could create a new table with the clustered index, then migrate the data in chunks, drop the original table and rename the new one. It goes without saying that moving 1.4 TB if data like this is really only realistic if you're in recovery model, since you'd fill the log with that data otherwise. You could probably use partition switching to avoid renaming, as renames carry some risks. That would involve truncating the original table, adding the clustered index to it, and finally using . Note that this operation will interrupt users, and you'll also have to be mindful of foreign key constraints and other schemabound objects. However, creating a clustered index on a TB+ table will also block the table for a long time, so it's your call. I would probably just ask my SAN admin for more disk space if that's an option.