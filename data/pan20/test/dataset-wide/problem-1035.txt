| Student (StudentId) attends school (SchoolId). Each student may attend more than one school, for each school is is possible that more than one student attends that school. If a student attends school, then that student must exist. If a student attends school, then that school must exist. 

If you prefer single-column then you can ADD them to and tables, but you must keep the existing ones as (unique) and reference them in foreign keys where applicable. 

| Teacher (TeacherId) is licensed to teach in district (DistrictId). For each teacher, that teacher may be licensed to teach in more than one district. For each district, more than one teacher may be licensed to teach in that district. If a teacher is licensed to teach in a district, then that teacher must exist. If a teacher is licensed to teach in a district, then that district must exist. 

This is simply not true, you should modify your belief to something like: "Often it is beneficial for a table to have a single-column primary key because ... " 

You'll want indexes on domain_id on domain. for domain_settings a compound index across (domain_id, is_keyword_checked) would be beneficial. 

The .conf normally did have a replSet declaration, but this was removed for the repair as I'm trying to run the repair on a secondary. (Per the docs repairs can only be done on primaries or standalones; so need to make it look like a standalone temporarily). Any idea what the problem could be? 

It seems maybe there's a config option that didn't come through in what you pasted that's looking to work with /run (instead of /var/run) ? 

I think the biggest gotcha would be around innodb being transactional. You'll want to know if the MySQL libraries being used by your applications auto_commit by default or not. [Python|mysql-python.sourceforge.net/FAQ.html#my-data-disappeared-or-won-t-go-away], for example, does not auto commit. This means if an application was inserting a row right before closing it's connection that insert would now be rolled back after you alter to innodb. The python script for example would need to be sure to call connection.commit(); Another point of difference could be around around multi row inserts or updates. Consider a single multi row inser insert into tbl values (...row1...), (...row2...), (...rowN....); Consider what happens if there is some kind of error such as a unique key collision on row3. With MyISAM the first two rows would have been written, under innodb all rows being written would be rolled back leaving nothing written in the even of such an error. With innodb you will enter the world of deadlocks. These aren't inherently bad unless they're occuring with such frequency to prevent any work from being done. However your applications will need to be coded in a such a way they anticipate deadlocks and handle them appropriately (which most likely means just retry). Consider memory/storage limitations. Innodb is a lot more resource intensive than MyISAM. If you have enough RAM to keep your buffer pools large enough to accommodate all your tables then you're golden. Look for tables that have large primary keys. Innodb's clustered indexing means each secondary index holds another copy of the corresponding row's PK. If you have 2 secondary indexes that means each rows PK is stored 3 times (PK + each index). If the pk spans across several column and large datatypes (char(N) for example) you can see how the index requirements can quickly explode under innodb. 

Though there is nothing cyclical here it does have few problems. For example, the model has no relationship between and , although you do state that "District has many Counties". It is also possible to have such that that teacher does not teach in the district the county is located in. 

And now a bit modified model where is a dependent entity (note rounded corners). Here can not exists outside of the context of the . 

As far as mandatory address is concerned, verify that on the application layer and wrap the loading statements into a transaction -- that way you'll get all or nothing. 

First thing to notice is that the PK on the LineItem table has three attributes , as opposed to just two in your example. Second thing to note is the confusion resulting from the use of the generic name for an attribute. The should ideally be (1,2,3..) for each customer and (1,2,3 ...) for each order. Well, this is nice if possible, but requires a query looking for the previous max value, like 

innodb foreign key. It's just short hand naming convention. You could call it asdfqwerty and the name would still work. 

In order to do rate limiting the script will need to connect to each of the slaves to look at the slave status. Make sure you have the same username, password, grants for this user on the master and all slaves. Ensure you can connect to the slaves via mysql client on the master. If you can but the tool still cannot for some reason first run 

Innodb slave side only seems odd to me. The main reason you should look toward that engine is data consistency and disaster recovery. This means you want it on the master if you have to choose one. Further, row level locking innodb provides will better enable you to potentially avoid table locks blocking master selects while writes are happening. This of course depends on your access patterns. I've had mixed results with compressed rows. In my experience they can definitely save space on things such as log records, but obviously you're taking a cpu hit. If you're resource constrained anywhere carefully weigh and bench mark whether it's better for you to be CPU bound or I/O. You stated you are high I/O but i'm not sure what else is going on, if this is a dedicated server, etc. If you're running percona look at the *information_schema.INNODB_CMP* table for compression stats on time spent doing those operations. Play around w/ differen key_block sizes when setting compressed rows if you go that route to see what works best for you. Further, be sure your file_format is set to Barracuda. If it's set to antelope the alter will succeed leaving you just wasted time as your rows are not really compressed then. Partitioning by date could be wise for logs as you'll likely be querying by that dimension. Keep in mind to partition by range that axis has to be part of your primary key. If you'll be querying by more than just date you'll probably want a composite key. If you're doing that and innodb remember large PKs quickly bloat your index requirments as a copy of the full PK is stored for every secondary index. If you have secondary indexes keep in mind partitioning by range can actually hurt your performance as it now has to scan every partition for matches and you loose the benefits of partition pruning. 

Note: I am using RDBMS wording for constraints, in general is an internal uniqueness constraint, foreign key is a subset constraint (inclusion), and is an internal value-comparison constraint. 

| County (CountyCode) is located in district (DistrictId). County is identified by CountyCode. Each county is located in exactly one district; for each district that district may have more than one county. If a county is locaeted in a district, then that district must exist. 

Note that is propagated to the to serve as a FK. If you squint a bit, this is close to your example, but with only -- as opposed to two column PKs from your example. Now the question is, why not simplify to something like this? 

which is often not preferred in high-transaction-volume environments, so it is common to see these replaced by an auto-increment, essentially serving the same purpose. It is true that this auto-incremet is now unique, hence it can be used as a KEY -- but you may choose to look at it as a necessary compromise for the . So, with some renaming and -> you may arrive to this model 

This will prevent the accidental creation of a nopassword user. You had mentioned keeping general_log off to prevent storing set password commands there. Another thing to keep in mind is if you have any kind of replication running that statement will get written to the binary logs to be pushed out to the slaves. Depending on what your needs are you could prevent the inital grant from getting written to the binlogs by running 

You have many state changes you want to happen in order for your interaction to be considered a success. This pretty much defines the "atomic" goal of a transactions. 

The table you see is just a pair of table schema (db name) and table name you wish to exclude for the "everything but these" list. The authorized users tables came in a second iteration to allow certain users access to otherwise default restricted tables. Some other columns there include a ticket column which is a reference to our internal ticketing system for audit trail purposes, and a couple of date columns for create and expire. I then have a user maintence script which runs a query like 

And here is how it looks in an ERD; note that comments (grey) are not necessary, used only to illustrate the method. You may also note that it is not easy to express all the constraints here -- if comments are removed the [c2.3] will not be obvious. 

I would say both are OK, but I prefer the second example. The confusion stems from something that you know as absolute. 

Which is fine, but introduces PATH DEPENDENCE -- you can not join with directly, must use in the join. 

P1 Course named (C_NAME), assigned a course number (C_NUM), exists. .. and constraints c1.1 For each course name, exactly one course has that name. c1.2 For each course number, exactly one course is assigned that number. The predicate leads to relation; the constraints to -- well, constraints like: PK, AK, FK, CHECK etc. 

One possible fix may look like this, though I do not think this would be complete solution, but it may be a good starting point. I will use predicates -- marked by | -- and constraints (italics) to describe the model. Predicates map to tables and constraints to PK,AK, FK. 

I have a couple questions for those more familiar. Most of my instances have been running Antelope despite having support for Barracuda. I was looking to play around with some compresses innodb tables. My understanding is this is only available under the Barracuda format. 

Big Percona fan here. I'm really excited about a lot of 5.6 features (although many seem to be things Percona already provided). At the same time there's no way i'm rolling out an alpha into production and no way I'm going back to stock and loosing thing I get from percona 5.5 Largely I suppose those "things" are related to information schema (which is just for my DBA satisfaction; not performance implications) as well as knowing I have a trusted xtrabackup situation going on w/ percona builds. I'm personally waiting for Percona's 5.6 GA before even deploying to QA for evaluation. As for the fulltext feature you're wanting I'm of the opinion search like that is not the role of a relational database and should be delegated to a lucene solution such as an in house elastic search install, or searchify for a cloud based solution if you don't have the resources to manage your own search service. My opinion on your implementation isn't really an answer though, so to get to that more directly: Do not deploy alphas to production. Alpha is the disclaimer saying "play around if you like but we're not sure we trust what we've built yet ourselves" 

The only problem that the author of that article has is not understanding database design. The problem in his examples is simply lousy design stemming from insistence on single column PKs (IDs), and not understanding how business logic relates to DB constraints. In the second example, his design assigns re-sellers commission based on the user and the product, instead on an actual purchase, which makes no sense business-wise. In the first example it is possible to assign a user to a task outside his project scope; again nothing to do with "circular", but not knowing how to implement constraints. 

I prefer the first case when possible -- you choose your favourite. And obviously, there is no need for direct FK from to in these three cases. 

There is nothing circular in any of these cases. In "circular reference" scenario you have "a chicken and an egg problem" -- can not insert a row into a table because it is always missing a reference to some other table, like in: 

This is not about natural and surrogate keys, but about concept of independent and dependent entities. Here is your original example 

You certainly have a problem but that does not look related to enabling general logging. Are you sure it's not that those errors have been filling your error log previously and you only just noticed them after an investigation stemming from enabling general_logging? Enabling general_logging can and cause your server seemingly hang. General logging will log every single query thrown over the fence to mysql (even one resulting in syntax errors). This can cause the file size to grow a lot more rapidly than expected. Run 

Finally, if you're worried about the ordering to the limiting effectively creates the correct range restrictions for you then this concern is completely eliminated to begin with. 

What version of the client are you using? I remember this being the older behavior but recent versions of 5.1 and 5.5 seem to just cancel the running command without exiting the client. Then again it may be that I'm using the Percona builds. You could try just running their client if that's the case. 

Is there anyway to tell with SQL if an indexed column is the first part of an index. The closest thing looks to be information_schema.columns. But that seems to only tell me if it's a PRI or MUL key (not if it's the only member of that index or what position its in if it were a composite key). 

| Teacher (TeacherId) teaches in school (SchoolId), in district (DistrictId). Each teacher may teach in more than one school in a district. For each school in a district it is possible that more than one teacher teaches in that school. If a teacher teaches in school in a district, then that school must be located in that district. If a teacher teaches in school in a district, then that teacher must be licensed to teach in that district. 

| School (SchoolId) is located in county (CountyCode), district (DistrictId). School is identified by SchoolId. For each school, that school is located in exactly one county, district. For each county, district; more than one school may be located in that county, district. If a school is located in a county, district; then that county must be located in that district. 

This is the root cause of your problem. There is no such thing as a parent table in relational model; foreign keys are constraints, not navigation paths. You are dragging OO terminology into a relational DB and these two do not match -- paradigms are different.