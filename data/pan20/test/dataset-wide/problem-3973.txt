Maybe this question is not suitable for this platform, I already put that same question in math.stackexchange and I find only vague answers. I'm studying the book of Rick Miranda; Algebraic Curves and Riemann Surfaces. I'm studying about degree of projective curves and I find a term used very often and that is very important by the amount of times it appears: "general hyperplane $H$ in $\mathbb{P}^n$" I need to understand what this means, but in the context of the subject. I searched all over Rick Miranda's book, but I could not find it. I read Geometry of Algebraic Curves-Volume I, I often encounter the placement "Let $H$ be a general hyperplane in $\mathbb{P}^n$. Important facts follow from this fact, as for example the calculus of the degree of the map of Gauss constructed from the divisor theta, for the demonstration of the Theorem of Torelli. So it may seem trivial, but it's quite important the meaning of: "general hyperplane $H$ in $\mathbb{P}^n$". It turns out that I can not find it in any book. Everyone just uses it, but I need to know. So I am also grateful for references. Thank you! 

Let $R$ be compact a Riemann surface of genus $g$ and $ J (R) $ be its Jacobian. For a subvariety $X$ of $J(R)$ of dimension $d$, denote the set of non-singular points of $X$ by $X_{reg}$. Then the Gauss map of $X$ can be written as $$ \begin{array}{llll} G:&X_{reg}&\longrightarrow&(\mathbb{P}^{g-1})^* \\ &x&\longrightarrow& \mathbb P[T_x(X_{reg})]\\ \end{array} $$ here $(\mathbb{P}^{g-1})^*$ is the dual projective of $\mathbb{P}^{g-1}$, that is, $(\mathbb{P}^{g-1})^*$ identified with the set of hyperplanes in $\mathbb{P}^{g-1}$. Let's say I want to calculate the degree of the Gauss map above. I'm following Principles of Algebraic Geometry, Griffiths and Harris and also Advances in Moduli Theory, Shimizu and Ueno. It follows from facts proven in the cited references that $G(X_{reg})$ is a set of points of $(\mathbb{P}^{g-1})^*$ corresponding to hyperplanes of $\mathbb{P}^{g-1} $ with certain properties $P_1$, and in addition also $\overline{G(X_{reg})}= (\mathbb{P}^{g-1})^*$, to Zariski closure of $G(X_{reg})$ . As I said, I would like to calculate the degree of the Gauss map $G$, so the references calculate $\#G^{-1}(H)$, at where $H \in G(X_{reg}) \subset (\mathbb{P}^{g-1})^*$. I imagined that first, we would need to verify that $ H $ is not a branch point of $G$, but this is not done in the references. What they actually do is take $H \in A \subset G(X_{reg})$, where $A$ is a dense open subset of $(\mathbb{P}^{g-1})^*$ that has certain properties $P_2 \subset P_1$.Thus, taken $H \in A$ is then calculated $\#G^{-1}(H)$. Only in future steps do the references verify that $H \in A$ is not a branch point of $G$. Why is this correct? That is, why do not you need, in this case, to verify that $H \in A$ is not a branch point of $G$, before calculating $\#G^{-1}(H)$? It would be because $A$ is a dense open subset of $(\mathbb{P}^{g-1})^*$?? Thanks! 

You could then investigate the extent to which you have to introduce 'connections' between the two sets to get a meaningful comparison. You would still need to define what you mean by meaningful comparison in your context. (Perhaps, define a meaningful comparison as one where you are able to correctly predict the outcome of competition between two players 95% of the time?) 

Another perspective that may help here comes from Bayesian statistics. You can model the outcome of competition between any two players using an unknown strength variable (say , $S_i$ ). Then, define: 

I do not think the answer changes much from the basic situation. Consider a situation where $N$ takes two possible values: $N_1$ and $N_2$ with probabilities $p$ and $1-p$ respectively. Then you want to maximize the following function: $f(.) = p E\Bigl(\displaystyle\sum_{i=1}^{N_1} u(a_i X_i )\Bigr) + (1-p) E\Bigl(\sum_{i=1}^{N_2} u(a_i X_i )\Bigr) $ Without loss of generality assume that $N_2$ > $N_1$. Then we can simplify the above and re-write as: $f(.) = E\Bigl(\displaystyle\sum_{i=1}^{N_1} u(a_i X_i )\Bigr) + (1-p) E\Bigl(\sum_{i=N_1 +1}^{N_2} u(a_i X_i )\Bigr) $ The maximum value of $f(.)$ can be computed by taking the maximum of the two terms independently as the set of 'parameters' ${a_i}$ are disjoint between the two terms. Thus, the maximum for the above case occurs at: $a_i= \frac{1}{N_1}$ for $i=1, 2, ...N_1$ and $a_i = \frac{1}{N_2-N_1}$ for $i=N_1 + 1, 2, ...N_2$ 

You can use the logistic/normal distribution to model the above probability (like how the wiki page for ELO mentions. See the link provided by Joel). As you pointed out, in the degenerate case, estimation of the strengths among all players in the set $A$ and all the players in the set $B$ would be possible but the estimates would not be comparable between a player from $A$ and a player from $B$. The above suggests that we have to introduce some 'connections' between the sets $A$ and $B$ that would be informative about the outcomes between two players from these sets using either statistical or context-specific approaches to make the scores comparable. A few thoughts along those lines (one could use one or more of the following approaches): 

Let $G$ be the 3-dimensional Heisenberg group equipped with its Carnot-Caratheodory subriemannian metric $d_{G}$. Let $U$ be a domain in $G$ of the form $V \times I$, where $V$ is an open subset of $\mathbb{R}^{2}$ and $I$ is an open interval (so $I$ is the third coordinate in $G$ corresponding to the center of the group). I'm going to consider uniformly continuous maps $f: U \rightarrow V$. Let $H$ be the left-invariant contact distribution on $G$. Let $d_{\mathbb{R}^{2}}$ denote Euclidean distance on $\mathbb{R}^{2}$. I want to consider a "quasiconformal" map $f: U \rightarrow \mathbb{R}^{2}$ in the following sense: there is a constant $C \geq 1$ independent of $r$ such that for each $x \in U$, $y \in H_{x} \cap U$, $$ \frac{\sup\{d_{\mathbb{R}^{2}}(f(x),f(y)): d_{G}(x,y) = r\}}{\inf\{d_{\mathbb{R}^{2}}(f(x),f(y)): d_{G}(x,y) = r\}} \leq C. $$ In other words, for each $x \in U$ $f$ maps concentric circles in $H_{x}$ to quasiconformally distorted circles around $f(x)$. I want to conclude that if $A$ is a measurable subset of $V$ above with $m_{G}(A \times I) = 0$ ($m_{G}$ the Haar measure on $G$) then $m_{\mathbb{R}^{2}}(f(A \times I)) = 0$. So a form of transversal absolute continuity. Is this known? What makes me think this is true is that, if we replace $G$ by $\mathbb{R}^{3}$ with the Euclidean distance with $H_{x}$ now being parallel planes orthogonal to the third coordinate, the conclusion follows from standard quasiconformal mapping theory on the plane + Fubini's theorem. Unfortunately this tactic does not work with $G$ because there is no foliation by transverse parallel planes. Note: Overhauled an earlier version of this question that was nonsense. 

Let $(M,g)$ be an almost flat Riemannian manifold with boundary, i.e. we have $diam(M,g) \leq 1$ and curvature $K \ll 1$. Let's suppose $M$ is diffeomorphic to $N \times [0,1]$ for some smooth manifold $N$ with $\partial M \cong N \times \{0\} \sqcup N \times \{1\}$. Does the generic geodesic starting from one component of $\partial M$ eventually cross to the other component of $\partial M$ and thus exit the other side? Here I am parametrizing geodesics starting at one component of $\partial M$ by their entrance angle and asking for genericity in either the topological sense or the measure-theoretic Liouville volume sense. This came up because I'm constructing some Riemannian manifolds by gluing along almost flat pieces and carefully smoothing the metric, but I'm wondering if I actually need to be careful at all about this smoothing in order to get this crossing property. Edit: I'm open to taking a generic choice of almost flat metric as well, since some brief reflection suggests that the original formulation of the problem is likely false even for a cylinder... 

I am not entirely sure what you are asking but ... The model you reference in the wiki article has a memory-less distribution for waiting time and inter-arrival time. Thus, the total time for a request to get processed is not dependent on time. 

Now that I understand your question better, here is another attempt: Let $S_i$ be how strong item i is intrinsically. In a competition between items i and j the probability of i winning will depend on $S_i$ and $S_j$. For simplicity, let us assume that: $\theta_{ij} = S_i / (S_i + S_j)$ Use the likelihood ideas as I outlined earlier but now estimate $S_i$ instead of $\theta_{ij}$. Obviously, you will now need to include all trials in which i participated to estimate $S_i$ which I think takes care of the non-dependent nature of your dataset. You can of course take different functional forms that relate $\theta_{ij}$ to $S_i$ and $S_j$ (e.g., the logit). Does that make sense? 

You can model the arrivals as a poisson distribution with different arrival rates Î»i for each one of your sources. $f(n_i;\lambda_i) = \dfrac{\lambda_i^{n_i} e^{-\lambda_i}} { n_i!}$ You can then assume that the arrival rates are random draws from a gamma distribution which would let you pool information across all your data sources. $\lambda_i$ ~ $\Gamma(k,\theta)$. Assume suitable distributions for $k$ and $\theta$. You can then construct the posterior distribution for $\lambda_i$, $k$ and $\theta$. The gamma distribution will help you 'shrink' your estimates for $\lambda_i$. In other words, if the frequencies of arrival is too low then it will be nudged a bit higher and if it is too high it will be nudged a bit lower depending on the data you have. Basically, the pooling of information will enable you to arrive at more robust estimates of the arrival rates. Estimate these parameters conditional on the data you observe using standard Bayesian methods. Once you have the estimates you can find out the probability that source $i$ is ranked the highest etc from the estimates. Do note that if the observed arrivals are of order of magnitude different from one another then you would be better of simply ordering on the actual arrival numbers as the bayesian computation above would not give any different results. Google the following terms to get some traction: hierarchical bayesian models, gibbs sampler, mcmc sampler, conjugate priors etc.