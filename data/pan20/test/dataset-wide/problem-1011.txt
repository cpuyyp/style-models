if your windows user has sysadmin privilege> login to your server as you do and after you login to your sql server using SSMS, go to security>logins>sa double click on sa and reset your password 

i suggest if you have an ID in the table to use it in the joins to find the exact record inserted to make it accurate like below 

from simple-talk Section 13. When is the Auto-Update to Statistics Triggered? support.microsoft Section: Automating Autostats determination msdn.microsoft section: Maintaining Statistics in SQL Server 2008 

Last part of your questions Please let me know this is good practice to move transaction log to some other drive. If not please suggest some other solution to recover from shrinking database log. Detaching & Attaching is good method if you can stop the the application that is in your case stop users from using sharepoint.. 

While trying to Process a cube i go this error "Error 133 The following system error occurred: No mapping between account names and security IDs was done. 0 0" i know its some user which is available in the roles have been removed from active directory. the annoying part in this is the error doesn't indicate which user has been removed from active directory, our SQL is setup as windows authentication. and i have to go and check one by one. is there a way to identify the user, so i remove it and continue with the processing 

How often you need to run index maintenance/rebuild stats depends on your database load, specifically how often your data is modified (i.e. //). If you're modifying data all over the show (i.e. a staging table for a weekly batch process), you probably want to update stats/reorganize indexes nightly. If your data is rather more static you can probably make it a weekly or fortnightly schedule. 

If you distribute your application with scripts to build the database (not a database backup or something like that), then in theory it should make no difference which version (of 32-bit and 64-bit) you develop against. Back when 64-bit was new and shiny (which was SQL Server 2000), there were some advanced features that 64-bit didn't support, but in 2005/2008 they should be functionally identical. (Not that it should matter if you're using Compact Edition, but there are some configuration differences between 32-bit and 64-bit once you get to the point where your DB performance is an issue.) 

For Aurora Postgres, there's two relevant cluster-level parameters (note they're not instance-level parameters): and . I haven't tested this myself but you should be able to modify them in the usual way using DB Parameter Groups. 

I know I just spent this entire post detailing why EAV is a terrible idea in most cases - but there are a few cases where it's needed/unavoidable. however, most of the time (including the example above), it's going to be far more hassle than it's worth. If you have a requirement for wide support of EAV-type data input, you should look at storing them in a key-value system, e.g. Hadoop/HBase, CouchDB, MongoDB, Cassandra, BerkeleyDB. 

i wont repeat what @Trisped said, that if you convert to simple recovery what will be the consequences and also the need to back up your database before doing the steps below Code below will convert your database recovery to simple and will shrink the transaction log file to its minimum and will set the recovery to full again incase you want it 

When To Update Statistics? if and only if auto update statistics feature is not good enough for your requirements. i mean if auto create and auto update statistics are ON and you are getting a bad query plan because the statistics are not accurate or current then it might be a good idea to have control over statistics creation and update. but if you are fine with your sql server performance and Query execution times. then i suggest stopping the Updates Statistics command from your Maintenance Plans updating statistics is important and useful 1. allows the SQL Server query optimizer to produce good query plans consistently, while keeping development and administration costs low 2. Statistics are used by the query optimizer to estimate the selectivity of expressions, and thus the size of intermediate and final query results. 3. Good statistics allow the optimizer to accurately assess the cost of different query plans and then choose a high-quality plan 

These might be the reasons and i suggest using a product like sqlping to discover your sql on the network, because if it doesnt show up it doesnt mean its not there, its just the way its broadcasting is the problem 

I didn't mention the PK on , for efficiency it would be a compound PK on , but there's also an argument to be made for a seperate surrogate primary key (which I personally think is a waste of space, unless you need to allow for multiple s between one and one ). For a purely associative table in a many-to-many relationship, though, the compound primary key should work fine. Edit: The complicated part of this isn't in the relational design, it'll be in the application code, because you'll need best-match/partial matching in order to show users the groups that most closely match their interests (rather than having accidental splinter groups all over the place). And if you want to out-Facebook Facebook, you'll need a smoother and better user experience than they offer. 

Access is a perfectly fine database system for small scale individual-user apps. Here are some criteria for shifting: 

Re: Shrinking. I see so many people getting their claws out at the very mention of 'shrink' and 'database' in the same sentence, so time to clear things up a little. Shrinking data is baaaaad. It literally turns your indexes into quivering shells of their former glory. Don't do it. Shrinking log should not be done routinely, but if you have a ridiculously outsized log (i.e. in one case I saw a 40GB log for a 100MB database, due to whoever set it up putting recovery model to full then never dumping the transaction log), you can shrink the log (after ) to reclaim that space without any ill effects (although, obviously, the will chew up I/O while it's running). PS: Speaking of log files, check your log file size and autogrowth settings. Small autogrowth settings can lead to underperforming log I/O (due to a poorly-explained feature in SQL Server called Virtual Log Files), particularly on batch transactions. 

you can use the DateID as a foreign key in your main table This way you have day, month and year seperate and you also have the Date in dateformat for any date related functions for reference the script to generate a date dimension i used this site $URL$ 

the Overwrite the existing database, do it only if you are sure you want to override your existing database as you mentioned you dont care to delete it RESTORE WITH RECOVERY is the default behavior which leaves the database ready for use by rolling back the uncommitted transactions. Additional transaction logs cannot be restored. That should bring the database online. Then you can delete it & try again. 

In SQL Server Configuration Manager, expand SQL Server Native Client Configuration, right-click Aliases, and then click New Alias. In the Alias Name box, type the name of the alias. Client applications use this name when they connect. In the Server box, type the name or IP address of a server. For a named instance append the instance name. In the Protocol box, select the protocol used for this alias. Selecting a protocol, changes the title of the optional properties box to Port No, Pipe Name, or Connection String. $URL$ 

the second part of the questions I will detach the database from SQL Server, then after the move particular .LDF file to some other location and attach only the database file back to SQL Server if you detach and move the ldf file, when you try to attach again you will see a message (Not Found) and be given the option to either point to its new location or just remove it and if you remove it a new transaction log file will be created . 

is a session-level command in Sybase ASE, it's not a server-level setting (if it was a server-level setting you'd be able to alter it via ). Can you run wireshark (or something similar) on the packets being sent from the JDBC client to see if it's setting showplan on as part of the session initialization? That said - showing the plan should not affect database CPU or memory usage, the plan is generated by the query optimiser anyway. However, showing the plan will increase network utilization. 

First things first: MS Access was not designed for multi-user access. Every version of Access I've used had a disturbing habit of corrupting tables at a vastly increased frequency if there were >1 users using it. If the two users are connected to the Internet all the time, I'd recommend shifting your table storage to SQL Server and having the users connect to that (use a VPN or some other form of security! If they're on a company LAN it's even better, you shouldn't need a VPN then). It's a fairly straightforward process to convert to SQL Server. The users will still use the Access front end, but instead of having the tables stored inside the .accdb file and having to merge them, the Access tables are converted to linked tables to the SQL Server tables. This is possibly a bit more up-front work, but it'll save you hassle down the road (how often do you need to merge? who's going to do the merging?). Also, if the application ever gets more widely used, you can easily build another front end (in C#, Java, ASP.NET, whatever) and connect it to your SQL Server back end.