It isn't sufficient. Suppose that the elements of $X$ are $$x_1 < x_2 < \cdots < x_n,$$ and suppose for simplicity that $\epsilon = 1$. Then the values $f(x_k)$ carry the same information as the integrals $$\langle p, f'' \rangle = \int_{x_1}^{x_n} p(x) f''(x) dx,$$ where $p(x)$ is a continuous, piecewise-linear function with $p(x_1) = p(x_n) = 0$. You can convert between such an integral and linear combinations of values of $f(x)$ by integrating by parts twice. For any such test function $p$, the inequality $$|\langle p, f'' \rangle| \le \int_{x_1}^{x_n} |p(x)| dx \qquad \qquad$$ holds. Moreover, many of these inequalities are logically independent. More precisely, the extremal choices of $p(x)$ are those for which $p(x_k)$ alternate in sign for $i \le k \le j$, and $p(x_k) = 0$ when $k < i$ or $k > j$. The idea to show that such as $p$ is extremal is to make a dual choice of $f''$ that switches between $1$ and $-1$ when $p$ crosses the $x$ axis. (I apologize for skipping some of the logic in the extremality calculation, but I think that this is correct.) The resulting set of values for $f(\vec{x})$ is not a polytope, but a certain convex region with both flat and curved boundary. For example, consider your statistic, $$f_k = \frac{(x_{k+1}-x_k)f(x_{k+1})-(x_{k+1}-x_{k-1})f(x_k)+(x_k-x_{k+1})f(x_{k-1})}{(x_{k+1}-x_k)(x_{k+1}-x_{k-1})(x_k-x_{k-1})},$$ for consecutive triples of points, and suppose that $n=4$. Then the pair $(f_2,f_3)$ must lie inside the square $[-\frac12,\frac12]^2$; you identified two other pairs of inequalities, but they do not clip the square further. What actually happens is that two of the corners of the square are rounded by parabolas. The supporting lines of the parabolas come from the test function $p$ that is a piecewise-linear interpolation of $(0,-1,a,0)$ with $a > 0$. 

What you want is to define a positive subset of the group that satisfies trichotomy (every group element is either positive, negative, or the identity), that is closed under the group law, and that is invariant under conjugation. I think that the Heisenberg group is an example. This is the group of matrices M = [[1,a,c],[0,1,b],[0,0,1]]. Say, integer matrices. Then we can say that M is positive if a > 0, or if a = 0 and b > 0, or if a = b = 0 and c > 0. I think that this works? 

I found several papers on a related question called the "quasi-projective dimension" of a group ring $R[G]$. The original paper on this is Groups of finite quasi-projective dimension, by Howie and Schneebeli. Their definition of a quasi-projective resolution is a finite resolution of a module $M$ by projective terms, and at the end a term which is a permutation module. I assume that, certainly for finite groups, it would work just as well to use a free resolution as a projective resolution. Among other results, Howie and Schneebeli establish that if $G$ is a finite group and $R = \mathbb{Z}$, then the quasi-projective dimension of $R[G]$ equals the period of its Tate cohomology. But another theme of the paper is that these questions, both theirs and surely Leonid's also, are perfectly interesting for infinite groups too. The papers that cite this initial paper use the second idea that I propose above. They make CW complexes with an action of the group $G$, and then make chain complexes from these CW complexes. So these CW complexes seem like a main way to understand complexes of permutation modules. In my opinion, the CW complex picture suggests generalizing the original question to include signed permutation modules. 

On the other hand, the averaging argument is also "slick" in my opinion, and I don't really see anything wrong with it even for undergraduates. Arguably the problem with any slick argument is that it is too adroit for some students. 

It is a strange question, but maybe a useful answer can make it a bit better. Certainly for many purposes a graph will look totally different from its complement. For instance, a graph and its complement have completely different spectra, diameter, perfect matchings, etc. So that side of the question is kind-of lame, I agree. On the other hand, for some purposes a graph is much the same as its complement. One obvious case is when you are interested in the automorphism group of a graph, or in the computational problem of graph isomorphism. Then you might as well think of a graph as a bicoloring of the edges of a complete graph. It then has a natural extra automorphism given by switching the two colors. More generally a colored graph with $n$ colors is equivalent, for graph isomorphism and graph automorphism questions, to a complete graph with $n+1$ colors. This viewpoint is more useful than you might first think, because a natural partial algorithm and preparatory step in the graph isomorphism and automorphism problems is to recolor every vertex by its valence, then color every edge by the colors of its vertices, etc., until the recoloring process stabilizes. Many graphs can be completely identified this way in practice. Recognizing the equivalence between a graph and its complement makes it easier to understand what these algorithms are really doing. Even some specific graphs, such as the Higman-Sims graph, are mainly used for their automorphisms and similar purposes, and it might be better to think of them as colorings of a complete graph. A much deeper example is the perfect graph theorem of Lovasz. The theorem is that a graph is perfect if only if its complement is perfect. For perfect graphs, taking the graph complement is closely related to the dual or polar polytope of a convex polytope. 

[Edit: I'm rewriting a lot in response to comments and clear shortcomings and errors in the earlier versions of the answer.] As in your paper, let's express the sequences as binary power series $a(x)$ and $b(x)$, so that their relationship is the equation $a(x)b(x) = 1$. (I am just changing your variable from $q$ to $x$.) My conjecture is that if you choose $b(x)$ at random with slowly decreasing density, then with probability 1, the reciprocal $a(x)$ is uniformly distributed in every congruence class modulo every $n$. In fact, I conjecture a lot more, that $a(x)$ is indistinguishable from uniformly random according to a variety of local statistical checks. It's important to consider a few statistical principles of random bits. First, given a finite list $L$ of random bits, all of the joint correlation information them is expressed by the biases of sums of subsets of the bits in $L$; these biases are a Hadamard transform of the joint probability distribution of the bits. In particular, of all of these sums are unbiased, then the bits in $L$ are independent and unbiased. Second, if you define the bias of a bit $b$ to be the expectation $E[(-1)^b]$, then these biases multiply when you add independent bits. Third, if $L$ is a finite list of bits and the average bias of $a+b$ for a pair of bits $a$ and $b$ in $L$ is low, then about half of the bits in $L$ are 0 and about half are $1$. If the size of $L$ goes to $\infty$ and the average bias $a+b$ goes to $0$, then in the limit $L$ almost surely has density $1/2$. Consider first a toy model in which $b(x)$ is random as in the previous paragraph, and $a(x) = b(x)c(x)$, where $c(x)$ is some specific power series such as $$c(x) = 1+x+x^4+x^9+\cdots.$$ Let's suppose that the $x^n$ coefficient of $b(x)$ is 1 with probability $1/\ln (n+3)$ (say). Then $a_n$, the $x^n$ coefficient of $a(x)$ is a sum of independent biased bits. There are easily enough terms in the sum that the bias of $c_n$ converges to 0 as $n \to \infty$. Moreover, instead of one term $a_n$ you can look at the bias of $a'= a_k+a_n$ with $k,n \gg 0$. Again, the bias is low because there are contributions from many independent bits. The relevant calculation shows that $a(x)$ has density $1/2$ almost surely. A more refined calculation shows that the bits of $a(x)$ are also equidistributed modulo $n$ for any $n$, moreover that local substrings of the coefficients of $a(x)$ of a fixed length look random. In the real problem, $a(x)$ is given by the functional equation $$a(x) = b(x)a(x^2).$$ This is more complicated, but $a(x^2)$ can still play the role of $c(x)$. In particular, we can say that the $x^n$ coefficient of $b(x)a(x^2)$ is a sum of simple and compound terms, where by definition the simple terms use $b_k$ with $k > n/2$. These random variables do not influence $a(x^2)$. What I think happens is that the sum of the simple terms is a bit with very low bias, and the bias cannot be boosted by adding any hypothetical combination of compound terms. And, as in the toy model, you can take pairs of bits $a_n+a_k$ with $n, k \gg 0$. Assuming that $n > k$, $a_n$ contains simple terms that do not appear in $a_k$, and this is enough to give their sum a low bias. Statistics of substrings is yet another complication, but I again think that a more complicated version of the same idea should show that $a(x)$ is equidistributed, etc.