To me, the only "unintuitive" applications of uncountable choice is when it turns up in physics. The only case I know where this happens is in the maximal-extension theorem of Choquet-Bruhat (QM does not use uncountable choice). This uses local extension properties of solutions to General Relativity to prove, using Zorn's lemma, that there exists a maximal extension. The use of axiom of choice is, I think, essential. I couldn't see how to sidestep it when I read the paper a long time ago (somebody please correct me if I am wrong). What is the axiom of choice doing in physics? I believe that it is entirely due to the issue of double-sided maximally extended black holes. A maximal extension of General Relativity can contain "wormhole" like solutions (for example, a charged black holes with two patches connected by an interior region), and there can be countably many such bridges in any asymptotically flat patch. But each of these branches can connect you to another different asymptotically flat region, which might have its own countably infinite collection of bridges to other flat regions. The resulting spacetime is like a tree with countably many branches at each node, where each node represents an asymptotically flat spacetime, and each edge is a double-sided maximally extended black hole bridging the two nodes. Such a tree can have infinite depth, and you must extend the solution to the whole tree. It seems intuitive that to patch the solutions together you need to extend the local solutions over continuum many nodes, and since GR is hyperbolic, you will get to make some arbitrary choices at each extension step. The dependence on choice then simply shows how unreasonable the maximally extended model of General Relativity is for physics. 

This gives a negative answer assuming CH. Explanation: consider R as a vector space over Q. Let A be some basis. Take any bijection A -> A + A, where + is disjoint sum. It induces a linear isomorphism f: R -> R * R. (You can think that there's a linear isomorphism between reals and complexes if that helps.) Then, if you were given a monochromatic rectangle a=(x1, y1), b=(x1+x2, y1), c=(x1, y1+y2), d=(x1+x2, y1+y2), certainly a+d=b+c. Using that isomorphism, f(a)+f(d)=f(b)+f(c) gives a monochromatic solution of quoted equation. 

This is a stronger requirement than your problem, so assuming CH the answer is no. Their solution, assuming CH is false, proves that there's a monochromatic rectangle. 

This will not really be an answer to your explicit question but when I used to teach Kan extensions in category theory courses at Master's level, I used to explore the case in which $A$ and $B$ are groups (which you can take to be finite if you like, and which are, of course, to be considered as groupoids with a single object in the usual 'delooping' way) and $V$ to be the category of sets or of vector spaces. The interpretation of $f:A\to V$ is then a set/vector space with an $A$-action. I leave you to flesh out what the two Kan extensions are, but note the ubiquity if this sort of situation and the insights that it gives you for the general case if you are wanting examples to understand your 'claim' better. I hope that helps. 

This is equivalent to CH. Quoting "Problems and Theorems in Classical Set Theory" by Komjath and Totik, chapter 16, Continuum hypothesis: 

What are interesting, illustrative examples of Borel sets, situated in Borel hierarchy higher than $\Sigma^{0}_{2}$ /$\Pi^{0}_{2}$? 

Three problems from G.Rosenstein "Linear orderings" (from the end of Chapter 2 and beginning of Chapter 4): 1) Is there a nondecreasing function from irrationals onto reals? 2) Is there a nondecreasing function from reals onto irrationals? 3) Is there an increasing function from reals into irrationals? (In other words, are reals a subordering of irrationals?) Any hints would be appreciated. (Please tag the question set-theory order-theory) 

If I understand the definitions correctly: Let $C$ be the category of pairs (V,q) where V is a vector space on a fixed field and q is a quadratic form. A morphism $f: (V,q) \rightarrow (V',q')$ is a linear map $V \to V'$ preserving the quadratic form. Let $D$ be the category of unital algebras over the field. Morphisms are linear maps preserving multiplication and identity. We've got a forgetful functor $D \rightarrow C$ that maps an algebra V to the quadratic vector space $ (V,q)$ where $q(x)=(x \cdot x) \cdot 1$. This functor has as left adjoint the Clifford algebra construction. (I'm inexperienced, so this might be plain wrong. But surely an adjoint functor is hiding here.) 

The first question you have to ask yourself is why do you think you have to read ALL of either set of sources. In my limited experience in Algebraic Geometry, it pays to get the basic definitions under your belt, then to look at a theme, following that through several sources. When you are in some distance, pause that theme and take up something that has caught your eye along the way. Why not look through Grothendieck's Esquisse d'un Programme (available on the net with commentary / translation in English), then follow up some themes from there. When you get, for instance, to fundamental groups (for the anabelian stuff in Esquisse) check back with BOTH SGA1, and Stacks plus any other surveys, books, etc. until you feel happy with that, then move on. Along the way, no doubt you will have met ideas that you do not yet know, so note them down and return. Every so often check back on other ideas then use both EGA/SGA and Stacks project and n-Lab and .... Get to klnow where to look for the stuff, rather than thinking one source will fit all. 

Constructing quantum field theories is a well-known problem. In Euclidean space, you want to define a certain measure on the space of distributions on R^n. The trickiness is that the detailed properties of the distributions that you get is sensitive to the dimension of the theory and the precise form of the action. In classical mathematics, measures are hard to define, because one has to worry about somebody well-ordering your space of distributions, or finding a Hamel basis for it, or some other AC idiocy. I want to sidestep these issues, because they are stupid, they are annoying, and they are irrelevant. Physicists know how to define these measures algorithmically in many cases, so that there is a computer program which will generate a random distribution with the right probability to be a pick from the measure (were it well defined for mathematicians). I find it galling that there is a construction which can be carried out on a computer, which will asymptotically converge to a uniquely defined random object, which then defines a random-picking notion of measure which is good enough to compute any correlation function or any other property of the measure, but which is not sufficient by itself to define a measure within the field of mathematics, only because of infantile Axiom of Choice absurdities. So is the following physics construction mathematically rigorous? Question: Given a randomized algorithm P which with certainty generates a distribution $\rho$, does P define a measure on any space of distributions which includes all possible outputs with certain probability? This is a no-brainer in the Solovay universe, where every subset S of the unit interval [0,1] has a well defined Lebesgue measure. Given a randomized computation in Solovay-land which will produce an element of some arbitrary set U with certainty, there is the associated map from the infinite sequence of random bits, which can be thought of as a random element of [0,1], into U, and one can then define the measure of any subset S of U to be the Lebesgue measure of the inverse image of S under this map. Any randomized algorithm which converges to a unique element of U defines a measure on U. Question: Is it trivial to de-Solovay this construction? Is there is a standard way of converting an arbitrary convergent random computation into a measure, that doesn't involve a detour into logic or forcing? The same procedure should work for any random algorithm, or for any map, random or not. EDIT: (in response to Andreas Blass) The question is how to translate the theorems one can prove when every subset of U gets an induced measure into the same theorems in standard set theory. You get stuck precisely in showing that the set of measurable subsets of U is sufficiently rich (even though we know from Solovay's construction that they might as well be assumed to be everything!) The most boring standard example is the free scalar fields in a periodic box with all side length L. To generate a random field configuration, you pick every Fourier mode $\phi(k_1,...k_n)$ as a Gaussian with inverse variance $k^2/L^d$, then take the Fourier transform to define a distribution on the box. This defines a distribution, since the convolution with any smooth test function gives a sum in Fourier space which is convergent with certain probability. So in Solovay land, we are free to conclude that it defines a measure on the space of all distributions dual to smooth test functions. But the random free field is constructed in recent papers of Sheffield and coworkers by a much more laborious route, using the exact same idea, but with a serious detour into functional analysis to show that the measure exists (see for instance theorem 2.3 in $URL$ This kind of thing drives me up the wall, because in a Solovay universe, there is nothing to do--- the maps defined are automatically measurable. I want to know if there is a meta-theorem which guarantees that Sheffield stuff had to come out right without any work, just by knowing that the Solovay world is consistent. In other words, is the construction: pick a random Gaussian free field by choosing each Fourier component as a random gaussian of appropriate width and fourier transforming considered a rigorous construction of measure without any further rigamarole? EDIT IN RESPONSE TO COMMENTS: I realize that I did not specify what is required from a measure to define a quantum field theory, but this is well known in mathematical physics, and also explicitly spelled out in Sheffield's paper. I realize now that it was never clearly stated in the question I asked (and I apologize to Andreas Blass and others who made thoughtful comments below). For a measure to define a quantum field theory (or a statistical field theory), you have to be able to compute reasonably arbitrary correlation functions over the space of random distributions. These correlation functions are averages of certain real valued functions on a randomly chosen distribution--- not necessarily polynomials, but for the usual examples, they always are. By "reasonably arbitrary" I actually mean "any real valued function except for some specially constructed axiom of choice nonsense counterexample". I don't know what these distribtions look like a-priory, so honestly, I don't know how to say anything at all about them. You only know what distributions you get out after you define the measure, generate some samples, and seeing what properties they have. But in Solovay-land (a universe where every subset S of [0,1] is forced to have Lebesgue measure equal to the probability that a randomly chosen real number happens to be an element of S) you don't have to know anything. The moment you have a randomized algorithm that converges to an element of some set of distributions U, you can immediately define a measure, and the expectation value of any real valued function on U is equal to the integral of this function over U against that measure. This works for any function and any distribution space, without any topology or Borel Sets, without knowing anything at all, because there are no measurability issues--- all the subsets of [0,1] are measurable. Then once you have the measure, you can prove that the distributions are continuous functions, or have this or that singularity structure, or whatever, just by studying different correlation functions. For Sheffield, the goal was to show that the level sets of the distributions are well defined and given by a particular SLE in 2d, but whatever. I am not hung up on 2d, or SLE. If one were to suggest that this is the proper way to do field theory, and by "one" I mean "me", then one would get laughed out of town. So one must make sure that there isn't some simple way to de-Solovay such a construction for a general picking algorithm. This is my question. EDIT (in response to a comment by Qiaochu Yuan): In my view, operator algebras are not a good substitute for measure theory for defining general Euclidean quantum fields. For Euclidean fields, statistical fields really, you are interested any question one can ask about typical picks from a statistical distribution, for example "What is the SLE structure of the level sets in 2d"(Sheffield's problem), "What is the structure of the discontinuity set"? "Which nonlinear functions of a given smeared-by-a-test-function-field are certainly bounded?" etc, etc. The answer to all these questions (probably even just the solution to all the moment problems) contains all the interesting information in the measure, so if you have some non-measure substitute, you should be able to reconstruct the measure from it, and vice-versa. Why hide the measure? The only reason would be to prevent someone from bring up set-theoretic AC constructions. For the quantities which can be computed by a stochastic computation, it is traditional to ignore all issues of measurability. This is completely justified in a Solovay universe where there are no issues of measurability. I think that any reluctance to use the language of measure theory is due solely to the old paradoxes. 

There are numerous calculations that are easily done with crossed module techniques that are much more difficult to obtain using `traditional' homotopy theory. Some of these use the next stage up, that is crossed squares, and the resulting non-Abelian tensor product. A neat sample calculation is of the homotopy type of the suspension of a K(G,1), if I remember rightly. This is given as the kernel of the commutator map from $G\otimes G$ to $G$. You ask is it good at distinguishing spaces. The answer is most decidedly yes. (But I would say that wouldn't I.) MacLane and Whitehead proved that the crossed module models the homotopy 2-type, extending the classification of homotopy 1-types by groups. (Ok there is a price to pay. The correspondence gives 2-types correspond to equivalence classes of crossed modules but the equivalence relation is algebraic not topological in nature so that is reasonable.) Loday proved that homotopy n-types had algebraic models which were crossed n-cubes, n-fold generalisations of crossed modules. The cool thing is that conceptually they are one of a linked set of models for low dimensional homotopy information that have geometric significance, and yet are relatively easy to manipulate. I like to say that a crossed module is a normal subgroup that is not a subgroup. Crossed modules 'are' also 2-groups, cat$^1$-groups and various other equivalent formulations. For some higher dimensional vKT applications, look at higher Hopf formulae in work by Brown and Ellis. For applications of crossed modules in non-Abelian cohomology etc. look at Larry Breen's work, or for a gentle introduction, my Menagerie notes which you can find on the n-Lab. I could go on listing things but will stop here. If you (or anyone else needs more detail) ask me or ask here. 

Geroch, Mathematical Physics Don't be scared by the title: it teaches algebra, topology and measure theory, using category-theoretic language. 

Call a computable function a total function $\mathbb{R} \to \mathbb{R}$, for which there exists a Turing machine outputting arbitrary close approximation to $f(x)$ given arbitrary close approximation to $x$. 

Obviously not every computable function is differentiable (for example, absolute value). For arbitrary continuous functions, the set of points of differentiability is $\Pi_{3}^0$. Can this be improved for computable functions? Suppose $f$ is computable and continously differentiable everywhere. Must $f'$ be computable? 

The class of all ordinal numbers $\mathbf{Ord}$, aside being a proper class, can be thought of an ordinal number (of course it contains all ordinal numbers that are sets, not itself). Then one could consider $\mathbf{Ord}+1$, $\mathbf{Ord}+\mathbf{Ord}$, $\mathbf{Ord} \cdot \mathbf{Ord}$ and so on. Does this extension of ordinals make sense/is interesting? Maybe it was described by someone? Could it go deeper - to create a "superclass" of all ordinals that are classes?