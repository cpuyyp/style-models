If the alternative is to have a data structure that efficiently filters by tenant, then no. If TenantId is the leading column in your clustered index, then single-tenant queries will be pretty fast. The NCCI row groups will not be segregated by tenant, and so all the row groups will need to be scanned to find rows for a single tenant. As rows are inserted, they are inserted into the "middle" of the clustered index, typically at the "end" of the rows for the tenant. But rows are always inserted into the delta store for an CCI/NCCI. And whenever the delta store has 1 million rows in it, it's rebuilt into a columnar row group. So each row group will end up with the last million rows inserted across all tenants. You can fix this with partitioning by TenantID, which could give each tenant (or group of tenants) a separate physical table (or NCCI). Each partition will have its own delta store, which will fill up with single-tenant rows. If you do partition, each partition might end up too small for a Columnstore to be useful. You'll want a few million rows per partition at least. In general with multi-tenant data, you want to avoid interleaving tenant data in a way that requires you to read all tenants data to retrieve a single tenant's data. 

I think that is the most restrictive lock locks currently held on the resource, as opposed to the locks requested. This is often the lock that is incompatible with the blocked session's request, but not always. 

For PowerBI DirectQuery you'll need to write a single view that returns all the data. Internally you might use a table-valued function if that is convenient. If the execution of the queries against the view is too expensive, you'll have to cache the results, either by loading them into a table, or by switching from DirectQuery to Import mode. David 

Oracle uses a case-sensitive catalog, and hides that ugly fact from you by silently converting non-quoted identifiers to all caps, both in DDL and DML. The name of your table is actually RS.RS_MYAPP, and SQL Server may be sending the identifier to Oracle as "rs"."rs_myapp", which would fail at Oracle. So try select * from [rstest-Link]..RS.RS_MYAPP 

An "object name" is technically called an identifier. In some contexts an identifier will be appear TSQL code surrounded by [ and ] or " and ". These characters are not part of the identifier, and you should never store them. Instead store the identifier as a nvarchar(128) (or sysname), and add the delimiters at runtime using the QUOTENAME function. The inverse of QUOTENAME is PARSENAME, which has the additional ability to navigate multi-part names. Note that QUOTENAME has an optional second parameter, and if you specify a single quote character for that parameter QUOTENAME does not create a valid delimited identifier expression. It emits a varchar literal expression. 

With ordinary column encryption (not AlwaysEncrypted), the columns are simple varbinary columns, with no special configuration or metadata. So replication of the cyphertext "just works". To decrypt on the subscriber you would need the symmetric key, either by Creating Identical Symmetric Keys on Two Servers, or by restoring a backup of the publisher database containing the symmetric key before initializing the subscription (and restoring the Certificate or Master Key that encrypts the symmetric key). 

No there's no way to change this behavior. But this truncation only happens with variable and parameter assignments, not table inserts. EG 

You can use SQL Server Merge Replication for this. The SQL Enterprise instance would be the publisher, but Merge Replication can synchronize data both ways. So changes made at the subscriber would be replicated to the publisher. If you configure this to be a Push Subscription the agents will all run at the central instance, and you can schedule them with SQL Agent. 

Just to add to the other answer. SQL Server supports two different flavors of READ COMMITTED, legacy locking READ COMMITTED and READ COMMITTED SNAPSHOT. If you've ever built and supported a high-volume OLTP application on locking READ COMMITTED you know why RCSI was introduced (in addition to making it easy to port Oracle applications to SQL Server). Locking READ COMMITTED is tricky to get concurrency and is prone to deadlocking, as readers block writers and writers block readers. Deadlocks are a kind of bug in your application, but they're famously hard to find in testing. So you have a chance to make a choice that reduces the number of hard-to-find-in-testing bugs. That's worth a lot. RCSI also increases the concurrency of your application, enabling you to scale up to use multiple cores more easily. So, RCSI increases the scalability and reliability of your application, at the cost of a little extra bookkeeping for UPDATES and DELETES (and INSERTS if you have a trigger), and an extra 14 bytes per row. RCSI is simpler to program to over all, the main thing is that you sometimes need to opt in to a locking read using the UPDLOCK table hint when you want to read a row and immediately update it and need to ensure that no other session does the same thing concurrently. 

Ensure that the NUMA configuration of the host is reported accurately by VMWare, and he out-of-the box configuration of SQL Server will do that. Be sure to apply Service Pack 2 to get all the latest fixes. There's some good stuff back-ported from SQL 2017 in there. 

Add EmployeeId to columns to both tables. Populate the master EmployeeId with the query you have, and join the transaction details table to the employee master on EmployeeName and lookup the EmployeeId . eg 

Not in the DMVs, but you can turn on the Blocked Process Report: blocked process threshold Server Configuration Option 

The problem is really just about error handling, as SSMS has no way to handle errors across batches when you run the script in an SSMS query window (ether normal or SQLCMD mode). So your transaction might abort, and SSMS would run the next batch. If you run the script with SQLCMD you can start a transaction in the first batch, and use the -b switch. Any error will abort the script and roll back your transaction. EG 

This is simple and quick to test. Take that 2TB backup file, copy it to Azure using AzCopy. Provision a SQL Server using the "Free License: SQL Server 2017 Developer on Windows Server 2016" image configured with 12-15 TB of SSD storage. Remote desktop to the SQL VM and restore the backup directly from Blob Storage, or copy it to a local disk and then restore it. Just remember to shut down the VM when you're not testing, and destroy it and the disks when you're done to minimize the charges. 

Process Working Set 31GB, which is the portion of the process committed virtual memory currently in RAM. So the SQL Process is using 31GB of RAM. SQLOS VM Committed is only 5,687MB, and Locked Pages Allocated is 0. SO SQLOS can only account for 6GB of the memory usage. So something in the process is using 25GB of memory, and it's not SQL Server. The typical culprit here a linked server driver, which allocates memory in the process which is not tracked by SQL Servers memory clerks and pools. You mentioned linked server to Oracle. What OleDB driver and version are you using? Any other linked server drivers used? Eventually you need to figure out what's leaking memory in the SQL Server process and fix or eliminate it. In the short term you can bounce the SQL Server process periodically. A good way to eliminate loading linked server drivers in SQL Server is to use SSIS. For reading data from remote sources the SSIS Data Streaming Destination enables you to isolate the 3rd party data access components in a short-lived process and still query from them like a linked server. 

My guess is log writes. If you commit after each row, then this will be much worse. In EF each call to SaveChanges() uses a separate transaction, so you want to batch 10s to 100s of rows in each call. You also don't want to have too many rows added to a DbContext instance, as they are all tracked for the lifetime of the DbContext. But more generally, SQL Server will tell you where the bottleneck is. You can look at the session wait stats for the session performing the insert in SQL 2016+: 

rule_id should be the leading column in the index. This supports efficient lookup by rule_id and ensures locality for inserting multiple rows for the same rule_id. 

In SQL Server for column encryption, you typically use the Service Master Key to encrypt your Database Master Key, which, in turn, encrypts your Certificate, which encrypts your Symmetric Key, which encrypts your data. See $URL$ For Transparent Database Encryption the hierarchy is a little different. The Database Encryption Key in your database is encrypted by the Database Master Key in the Master Database. $URL$ In both cases the key used to encrypt/decrypt data is protected by an encryption chain that goes through the Master database. I think both meet your requirement in effect. 

Tenant data is isolated. This is important in selling to your customers, and it makes it very easy to make sure that tenants can never see each others' data. You can upgrade and patch tenants separately. You get the ability to move, backup, restore, export a tenant-at-a-time for free. Critically this gives you point-in-time restore for single tenants. Databases are a real security boundary, and you can enable users to see only a single tenant's data. You don't have to write tenant export, and tenant import processes to move a tenant from one database to another, which you otherwise would have to write. You can scale tenants separately, giving large or premier tenants differentiated access to server resources, or even giving them their own dedicated resources. This makes your solution easier to manage, mitigates risk, and creates pricing opportunities for you. Each tenant gets their own query plan cache, so you don't have query plans optimized for a small tenant used for queries for the data of a large tenant or vice versa. And you have visibility into the performance metrics for each tenant. 

For "management and scalability" Azure SQL Database is clearly better. Azure SQL Database was designed for this scenario. You can spread out your databases across multiple pools, as needed, or move specific databases to non-pooled DTUs. And Azure SQL Database has built-in patching, HA/DR, monitoring, and backup. Azure Virtual Machines give you way more flexibility to design SQL Servers that exactly fit your needs, but that comes with a cost of complexity and management. In both cases you should plan to spread your databases out over a few Elastic Pools / VMs, so you can scale out as well as up. 

eg this will emit a script based on the configuration of an instance that can be applied on another instance. 

Each session is deleting multiple rows from the same table based on the value in a secondary index. Even if you don't attempt to delete the same row in multiple sessions, the sessions will search for the rows to delete using the secondary index, reading with a U lock, and then convert to an X lock for each row. This creates the possibility of deadlocks. You can avoid this by searching for the rows to delete without a U lock, and then deleting them in a separate statement. Some of the rows you find in the first query may be deleted by the time you attempt the second. But you probably don't care about that. So something like: 

A #temporary table is only visible from the session that created it. So even the same user in another session would not be able to see it. If you load a temporary table in SQLCMD it would be visible to that instance of SQLCMD. So a subsequent command could load the data into a permanent table, or open a cursor over it and call a stored procedure for each row, etc. Alternatively you could provision a private schema for the user and load the data into a permanent table there. Then no other user (other than a database owner or other privileged user) would be able to read it. The user could then move the data into another schema later. 

I assume this is some sort of IT management or monitoring agent, otherwise it shouldn't be running on your SQL Server. If you don't run any other code as SYSTEM on the server, there's not really a lot of extra risk over and above running 3rd party code with Windows administrative privileges. If the service runs as any local administrator, it can get access to SQL Server. If possible you should do both of: 1) Grant the service the least privileges is needs to run 2) Provision the login and grant permissions to the per-service SID instead of the service account. So if the service is has a Name of 'someservice' (not DisplayName), create a login like: 

Just use a passthrough query. And get rid of NOLOCK (it doesn't even do anything here as the catalog is always read with locking read committed semantics). EG 

Please read: Enhanced Azure Security for sending Emails – November 2017 Update which outlines the recommendations, options and restrictions for sending emails in Azure. In particular: 

As @Kin suggested, you can SET NOCOUNT ON and no rowcount messages will be sent to the client. Then after you COMMIT you can turn NOCOUNT OFF and run a query to that generates the desired rowcount message. Perhaps something cheezy like: 

This is a common misconception. Single-row inserts, like EF generates, are not expensive in CPU cycles. Typically bottleneck is writing to the log file. Your local machine may well give you better log write performance than a powerful server. Your local machine may have a fast, local SSD. Or you may have write caching enabled on your local disk. 

IX_User_Email is enough, although you might want to make it a unique index to prevent multiple users with the same email. And this query, 

-Please don't use short, cryptic column names. -There's no reason to wrap individual statements in explicit transactions. You'll want to extract the key value from the table variable in a separate statement. Something like: