The nova help is a bit confusing, as it frequently uses the word "server" where "instance" is actually meant, and "host" where "compute node" is meant. OpenStack's terminology has changed over its history, and some of the terms used here are the historical old terms. So I experimented a bit to find out what these commands actually do. What I came up with is: Commands which operate on failed compute nodes: 

Rather than storing host-specific data such as ssh host keys on the SD card or other read-only media, you can store this in NVRAM, which is what it's for on an embedded system. You'll need to do some custom scripting to store and retrieve the keys at boot time, but the scripts will be exactly the same for every device. 

Since you are updating existing packages, rather than exclusively installing packages, you need to use the update option instead of the install option . This will allow you to both install and update packages at once. 

You have placed your directives in the wrong places. should be defined in the block, not in each block. This is one of the most common nginx misconfigurations. To resolve the issue, remove all of the directives from each block, and place the correct directive within the block, not within any . The only reason to have in a block is when you actually want a different document root for that , such as for serving the nginx default error messages (as you are doing here). 

The virtio serial console should appear as , if you don't have any other virtio serial ports defined for the virtual machine. If so, the number at the end for them may be something other than . 

Both client VMs actually have IPv4 addresses; has a static IPv4 address and gets its IPv4 address from DHCP. This doesn't seem to make a difference. 

The kojihub and kojiweb certificates must have their CN set to the fully qualified domain name of each respective server. This was in the documentation: 

This actually looks like a problem with your server's motherboard. We can see from your output that it is failing to communicate correctly with the PCIe card in the failing server, but works correctly in a different server. So you most likely have a bad PCIe slot, or bad motherboard components. You can try using a different PCIe slot, if you have another one available, checking that your NIC and riser card (if any) are firmly seated, or replacing the riser card or motherboard. It could also be, if you haven't actually tried this specific NIC in a different server and had it work, that the NIC itself is bad. 

That IP is not valid with that netmask. You can only use 5 of the 8 addresses. The sixth is the gateway, which you also can't use. And the first and last addresses in the block also aren't usable, as they are the network and broadcast addresses respectively. 

This happens because of your NetworkManager settings for the network interface. You need to ensure that both of these settings are enabled: 

Once you change the prefix your router is advertising, addresses in the old subnet will be deprecated and eventually removed without any intervention on your part. The address will be deprecated for the difference between the preferred and valid lifetimes advertised by your router. (On my network this works out to 90 minutes, but it depends on how you configured radvd.) To see deprecated addresses on your Windows network interfaces you can use , or PowerShell . 

BTW, those requests you are seeing are Internet background noise. In particular, they are probes to determine whether your web server is an open proxy and can be abused to hide a malicious user's origin when he goes to perform malicious activity. Your server isn't an open proxy in this configuration, so you don't really need to worry about it. 

Your hard drive is reporting unrecovered read errors. There is nothing you can do except to replace the drive (it's eligible for warranty replacement if it is still in warranty). 

The shell prompt you received is not a normal system prompt; it's intended for you to fix the problems that prevented a normal boot. So let us look at the problems. First, there was an important clue: 

Unless you're doing something highly unusual and bizarre, the smallest subnet you should use is /64. Anything smaller risks breaking various IPv6 necessities like neighbor discovery and the nice-to-have stateless autoconfiguration. Might even break DHCPv6, though I haven't tried that myself. To be crystal clear, the tool to which you linked appears to be generating bad data, and I wouldn't use it. (Some of their other tools look fine, though.) As for "translating" IPv4 to IPv6, you generally don't. Instead, you run dual-stack, where each device talks both IPv4 and IPv6, with independent addresses for each protocol. The addresses themselves are provided by stateless autoconfiguration, DHCPv6, and/or privacy extensions, or (typically for servers) assigned manually. So let's say you get a block of addresses from your ISP, . This gives you 256 subnets to work with, which is more than sufficient for a small office environment. If you need more than 256 subnets, you can get a /48 without too much difficulty, giving you 65536 subnets. That's enough for all but the largest deployments. If you can tell us more about what you're actually trying to do, we may be able to provide better answers. 

If your mail server says and company.example can't be resolved to an address, then it's perfectly valid to reject that connection. The same is true of the domain names used in the sender and recipient addresses (with the exception of postmaster, which doesn't require a domain name at all). (Prior to RFC 2821, the governing standards were RFC 821 and RFC 974, which date to the 1980s and had to accommodate many non-Internet networks which no longer exist, thus the standards were much less restrictive.) 

In addition to ClamAV, consider using Maldet for additional malware detection. According to the docs, it has the ability to integrate with ClamAV, though I haven't personally set this up. 

The clamd socket file has somehow gotten mislabeled. It has the type , but it should be in current SELinux policy. Anything matching should be labeled . This could be because the socket was created while an older version of the policy was installed, or a program or user could have manually mislabeled it. Since you say the system is up to date, I would recommend relabeling the file (and, for that matter, the entire system, just to be sure), to correct any mislabeled files, and then restarting. 

The ansible module does not pass commands through a shell. This means you can't use shell operators such as the pipe, and that is why you are seeing the pipe symbol in the output. As far as ansible is concerned, it has executed the command with all of the rest of the line as arguments to . If you need the command line processed by a shell, use instead of . And, there ought to be a better way to regenerate ssh host keys, but I can't find one right now... 

Your problem is not . Your problem is that you have specified to connections back to the very same nginx thus creating an infinite loop. When a connection comes in, nginx immediately reconnects to itself 2048 times, throws that error, and gives up. To solve the problem, you need to to the correct web application, wherever it is, or remove it entirely. 

Don't bother with using any of the "advanced" features of upstart on RHEL6. It only uses upstart as a "replacement" for the original SysVinit, and only uses old-style init scripts. RHEL itself doesn't take advantage of any new upstart features, and RHEL7 does not include upstart. In fact upstart has pretty much been abandoned by everyone at this point except Ubuntu (only because they invented it) and Debian. RHEL7 uses systemd, which many other modern Linux distributions now use, and Debian is in the midst of switching. To future-proof your script, write a "normal" init-script; these are also supported in systemd for backward compatibility, though unlike upstart in RHEL6, systemd's new features actually are used in RHEL7. A number of tutorials are available on the Internet for writing SysVinit scripts; one very good one is provided by the Fedora Project. When you move to RHEL7, you can switch to a systemd unit file if you like. 

SMTP server software such as sendmail, postfix and exim is designed to handle large quantities of mail, try again in case of temporary problems, etc. Your script isn't, and shouldn't be, smart enough to manage all the intricacies of SMTP. If it is your server, then you'll need to look at the server logs that it's generated to find out why it rejected the mail. If you're using a third party server, you'll need to contact the third party to find out what's going on with the mail server. 

I don't think is what you really want here. That just disables sending the and headers entirely, and you get browser-default behavior (which is usually RFC compliant, resulting in the resource being cached). If you mean to have the resource not cached at all by user agents, use instead. 

This means you probably forgot to set up a section in Postfix's configuration file. It should look something like: 

You can see here that the IPv4 address is , the IPv6 address is and the MAC address is . Problem solved. 

A program is already listening on the port you are trying to bind. You can inspect the output of to determine if a process is already listening on port 110, and which process it is. After killing or reconfiguring that process, try binding the port again. The firewall is preventing the program from binding to the port. In this case, you will need to configure the firewall to permit to listen on the port. 

in (or a file included from it) works around the problem at a slight performance penalty, though far less of a penalty than turning off opcache. 

Zabbix is capable of monitoring SNMP-capable devices, and even displays their status in its web interface: 

Your domain's certificate has two paths to two different root certificate authorities. On modern desktop browsers such as Google Chrome, as well as on newer Android versions, the path being taken is to the more recent USERTrust RSA Certification Authority root certificate. (I get this on Android 7.0 NPD90G.) On older Android versions, the path being taken is to the older AddTrust External CA Root root certificate. On this second path, you are missing an intermediate certificate. This is the one shown in the SSL Labs test as an "Extra download". In order to resolve the problem, you need to obtain this intermediate certificate and add it to the certificate chain in your web server. 

You've installed some packages from a newer version of Ubuntu, or from a third party PPA, which conflict with the Ubuntu version you originally installed. To fix this problem: 

A VGA or DVI dummy plug will allow the GPU to start without a monitor. For example, these can be purchased or built yourself. 

Because timed out, no was created, returned an error exit code, and was never called. When someone at Goddard gets their head out of their ... whatever ... and fixes their server, you'll be able to see that the file gets created as expected. 

This indicates that Asterisk sent a SIP message to your IP phone, but never got a reply back. It was therefore forced to drop the call. The wiki article linked in the log entry has further information that you can use for debugging this issue. The most likely cause of the problem is a firewall or NAT issue. 

You set in your configuration. This means that only can log in to the server with ssh. Not even can log in since it is not named in . To resolve the issue, login and fix or remove the directive. 

You should do only one change at a time, so that if something breaks, you know what it is. Instead, you tried to change two things at the same time, so you can't be sure if it was changing the disk format or trying to get the installed system converted to virtio. Start over from the beginning (with the original qcow2 image) and change only one thing at a time so that you can be sure of what's going wrong. Also note that CentOS 5.8 can be installed directly to a virtio disk. You may find it easier to install a new virtual machine and transfer your data from one virtual disk to the other, or better yet use the installation DVD to update your existing virtual machine. 

The main difference I see in your configuration compared to my live FreeIPA (on Fedora 20) is that I do not use the kernel keyring as a ticket cache. 

Check the permissions of the root user's home directory, . An example of working permissions from a live server: 

You may want to check the security context that your container's processes run under. To do so, add the option to . For example: 

Your Amazon EC2 instance is itself a virtual machine, and there is no "BIOS" in the way you mean. And Amazon doesn't support any form of accelerated nested virtualization. This means, among other things, that you cannot use VirtualBox inside an EC2 instance to run 64-bit guests. (You could in theory run 32-bit guests unaccelerated, but it would be painfully slow and expensive.) You haven't explained what you're trying to accomplish, but whatever it is, using VirtualBox inside an Amazon instance isn't going to be the way to achieve it. 

The end state you probably want to be in is one where you have 100% test coverage, with unit, functional and integration tests for everything, so that you can take advantage of continuous integration to deploy your changes -- or catch mistakes -- as rapidly as possible. Start small, with a single "testing" server, and work your way up, until you get there. 

This looks like a kernel divide by zero bug that was fixed in version 3.1.5. A patch is available if you want to attempt to backport it. 

It's impossible to prevent a MAC-spoofing attack on a completely open Wi-Fi network. However, it's relatively easy to detect the attack: Both the attacker and the victim whose MAC address is being spoofed will have trouble using the network, as each computer will send TCP resets for connections the other has initiated. Thus the person whose MAC address was cloned begins having mysterious "trouble with the Internet" and either calls technical support, or gives up and tries again later. The latter is what the attacker wants, as it leaves him free to use the network once the other person is gone. Again, you cannot prevent this attack on an open network, and while you can attempt to limit it by making legitimate users login again every half hour or so, this is a great inconvenience to legitimate users, and not much inconvenience to the attacker, who can just wait for the legitimate user to login again, or go clone someone else's MAC address. So this is not practical.