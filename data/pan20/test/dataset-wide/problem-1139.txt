I am sure you are aware about Ola Hallengren script for index maintenance, I suggest you download it and use it. It takes care of various things on AOAG database. 

Transaction log backups are mostly small and if you have a busy database producing lot of log records you might need to change the backup frequency. Mostly I have seen transaction log backup completing fine when frequency is 15 mins. I don't think should be matter of concern for you. 

The hardware( the underlying disk) on which master,model and msdb file resides is incorrectly formatted or corrupt. You need to get the Storage verified. This might be bug ( Which I dont believe, actually some external factor is preventing SQL Server from coming online which can be known by analyzing dump ) 

Plus and fact being that since index is small pages are most likely to be in memory so no physical I/O would be required to get page and hence fragmentation would not come in picture. So the small indexes which have or for that sake < 3000 is hardly going to cause any performance issue, it is quite likely that pages would be in memory and physical I/O would not be required hence fragmentation would actually not matter. If I look at your output only two indexes with should really cause some performance issue if SQL Server has to scan index pages. People can argue that page_count of 5000 could also be deciding factor and to which I would say "not really", considering the definition from old books online quoted above. You must also note that the fragmentation we are talking is logical fragmentation where leaf level page order does not matches that of clustered index key. So performance issue, if any, could only come from large fragmented index not the smaller ones. 

Let me ask you what is output of . What is SP and CU level to which your SQL Server is patched. The reason I am asking this is because there was bug in SQl Server 2012 which forced PLE to plummet like what you are observing. Ths bug was fixed in SQL Server 2012 SP1 CU4. Or to be on safer said I would recommend you apply SQL Server 2012 SP2 instead of going for CU4 Its sometime normal for PLE to fluctuate on system having high activity. Actually this is by very virtue how PLE code works in SQL Server. But the fact that its plummetting to zero quite frequently make me believe you might be hitting the bug I have mentioned above. As per Microsoft Bug fix detail 

What is current max server memory setting and please dont change it to lower value it will cause issues for you. Its normal for SQL server to utilize all memory if allowed to use via max server memory setting. 

Backup operation but remember this is Not the parallelism driven by Optimizer in SQL Server its driven by number of disks involved from where backup has to read the data file and where backup writes the data file and amount of backup files created. You cannot use hint while taking SQL Server backup You cannot generate execution plan in SSMS for simple TSQL backup operation. The parallelism driven by query optimizer in SQL Server is basically for operators involved ( actually its more complex but for sake of simplicity you can take this ) since backup operation does not involves any operator as such it cannot use parallelism driven by optimizer. I wrote an article on Technet Wiki about Backup and parallelism where I used simple examples to explain parallelism during SQL Server backup. Following is the conclusion 

I am quoting a paragraph Appendix A: Using Management Studio to Change Data Types from SQL Server 2005 document on Impact Of Changing Collation and Data types 

For cases where you have just read only databases or databases where you just do select operation and there is no DML operation, in that case you can keep the option to false but again no harm would come if you keep it true. We mostly see database with certain amount of activity. 

Consider this as answer SP3 is new and thoroughly tested its highly unlikely it would create any issue but again I would recommend you not to proceed without any testing and that too when Cluster is involved. Also asking whether anybody faced issue will lead you in problem, what if somebody writes 'Yes I have done and its working fine' yes they are correct it worked in there environment but I am sure your and his environment are not same and after applying SP if something stops working who would you catch other than Microsoft. I have applied SP3 in my environment and every thing is going good as of now. But if you ask my plan of action I have now deployed in DEV then I will deploy in UAT and then after a month, because couple of SSIS packages run monthly, I would apply in production. Please create an environment and test first believe me it will save you lot of hassle. If still you want to proceed 1.. Backup system and user databases. Use TSQL backup command [This link is external to TechNet Wiki. It will open in a new window.] or SQL Server Management studio GUI [This link is external to TechNet Wiki. It will open in a new window.] to backup system and user databases. Since you backup system and user databases you don't need to backup jobs, SSIS packages, mail profiles, linked servers and logins as all of these would be included in MSDB and Master database backups. 

Whenever you run patch SQL Server writes in Log what all instances and features it has found on the current system and as per log you only installed 

NO IT IS NOT. One should only rebuild index which is fragmented above certain value. Probably widely used value is, if index fragmentation is >30 you should go for rebuild and it is in between 10 and 30 you should go for reorganize. Please note you have standard edition so index rebuild would not be online. If you rebuild index blindly it would cause more downtime and produce more logs and thus more overhead on system. If you have narrow maintenance window you need to be highly selective with index rebuild and all this can be taken care if you go for Ola Hallengren Index rebuild solution Other thing to note is if page_count for index, page_count can be seen from be seen from sys.dm_db_index_physical_stats , is less than 1000 you don't need to rebuild such indexes. The reason is since page count is less pages allocated to index would be from mixed extent and these mixed pages can be lying anywhere and hence producing Logical fragmentation even after rebuild. But rest assured such small page count indexes would not affect query performance at all. You can read more about why such indexes still remain fragmented even after rebuil here There are various options you can choose with index rebuild you can see more details here. Lot depends on your environment and configuration You can read more about query plans from This Article. You should focus more on actual query plan than estimated query plan. Actual would tell you what exactly SQL Server is doing and what resources it is using. You can also try SQL Sentry Plan explorer tool to get better insight into SQL Server execution plan 

I am not aware about any significant changes in backup code for various versions, such things are not documented. I only know about the enhancement introduced in enable backup and restore from the Windows Azure Blob storage service from SQL Server using TSQL or SMO. Read here 

The backup strategy is to make sure you have valid backup when needed and using that you can loose as less data as possible, there is hardly a strategy for making backup size less unless you use data compression or backup compression. 

2.Take frequent transaction log backup specially after the above delete comand completes. You have to ultimately balance the number of records which can be deleted without inflating transaction log much. Even if recovery model is simple the logging behavior is not going to change much logs will be produces. A checkpoint statement would help you make sure logs are getting truncated automatically after transaction completes 

My first question where are you looking for SQL Server memory consumption. Please don't use task manager or process monitor, these do not show correct value when SQL Server service account has Locked pages in memory privilege. just use below query to see memory consumed by SQL Server 

One important thing with shrink operation when Shrinking through management studio is the option 'Release unused space' what this does is ''Cause any unused space in the files to be released to the operating system and shrink the file to the last allocated extent, reducing the file size without moving any data. No attempt is made to relocate rows to unallocated pages. '' You should try this option since there is no page movement chances of fragmentation is very less and this option can be used if there is free space in database. Its good that you are aware about drawbacks of shrinking I am sure recommendation by Paul, Brent and Gail are more in terms of educating user about drawbacks of shrinking than stopping them. IF you want to reclaim space you have to shrink at some point and its fact that shrinking causes fragmentation so you have to deal with it. Its not like nobody uses database shrink people do use but don't recommend on public forum because they are not aware about their surrounding and impact it will make. I would say if you know about growth about database and know in coming year space would not be utilized shrink it cautiously may be in small chunks check out option which I pointed see if it works 

Meanwhile if people complain about anything you know who is or which team is probably using it and then you can discuss further. Even after making offline after a month someone comes and complains that certain job failed because you made DB offline you can very well say that you mailed about this quite before and there is no point in complaining now. In all cases you would come to know who is accessing and when. PS: This is personally tried approach after referring to various solutions provided online and it has beautifully worked for me. 

If drive on which index resides runs out of space the index rebuild operation will fail and changes will be rolled back. Index rebuild works like a transaction either complete index would be rebuild or it would go back to its previous state. EDIT: 

Yes number of connections are limited by the OS you are using. Please refer to this thread which says only 20 connections are allowed on windows 7. I am not sure about other client OS but if you use server OS there is no such restriction. Unlimited for practical purpose. There is no restriction as such if your OS is Windows server 2012. 32767 databases is what maximum capacity specification states. But you wont be able to go far than 10 due to express limitation on memory and CPU 

Highly unlikely but you should plan for SQL Server 2012 SP4 ASAP. From my past experience I could say that since SQL Server and other applications like SSAS and SSRS are all running on same machine SQL Server might be facing memory pressure and so might be the case for SSAS/RS. I have seen lot of systems like this and it all boils down to memory pressure. You have 32 G of RAM and you have given just 16 G to SQL Server make it to 20 G and see if this help. Adding more memory would definitely help if you can go for it. There can be plethora of reasons why queries are timing out and Troubleshooting SQL Server performance issues can help you to find the root cause of it. 

No it is not just for tape backups, you can take local disk backup and specify media name. More over its a name given to particular backup set.For example 

What this error means This message means SQL Server is installed on your machine but when it is trying to come online it failed. And the reason for the failure would be located at SQL Server errorlog file. Open the content of file in notepad and post its content in the question. This would tell you why SQL Server is not able to come online Workaround: You should go to SQl Server configuration manager and locate SQL Server service. Change SQL Server service account to Local System and start SQL Server service. You may or may not succeed.Complete troubleshooting steps are documented in Could not find Database engine startup handle. If this does not work. Post content of errorlog file as suggested above. The contents would tell us what stopped SQl Server from coming online. Note: Its not a good security practice to run SQL Server service account via . After you are done with cluster installation. Create a domain account and change the service account with this domain account. This domain account will have least privileges as documented here