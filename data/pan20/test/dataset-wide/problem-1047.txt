I would try this on the fields where some characters display as and compare with the fields where the characters display correctly. The goal is to identify what's exactly behind the . 

The specific problem of having the trigger consider a group of related changes rather than independant updates could be solved by : 

or have to infer the column types for the relation to create. Sometimes the context is not sufficient to guess a datatype, for example when it's just a string literal. In this case it's created as . Example : 

You don't have to specify and the port number, so it connects through Unix domain socket instead of a TCP/IP connection, which differs in these two ways: 

Even if extracting fields from a date would always produce results that could fit in an integer, according to the doc, doesn't directly work on a type: 

Here does not matter because the COPY format is independant from it. When this COPY is executed, it will make no difference whether it's on or off. 2. Dump of 8.1 data with 8.1 pg_dump with --inserts option 

happens in case of a decompression failure of a LZ-compressed TOAST'ed value. See $URL$ At the row-level storage, large values are stored as pointers to tables in the schema containing the actual data. How do I find and delete all corrupted rows in my table ? Assuming you know what table it is (if not, iterate over the tables, starting with these with the largest columns), something that might work is: 

To redirect to a file, it's the same as the previous solution. You probably want to remove the decoration, padding, footer and choose a separator. 

Now if several sessions issue the same in parallel, there is indeed a potential race condition that is dealt with by relying on the unique index on , and comes back to the caller as an error. The error mentioned in the question happens if, for instance: 

I think there's no way to indicate to that the file is relative to the location of the script, but you may use to change the current directory before . interpolates variables so the directory can be passed on the command line with : 

is not a command, it marks the end of data in a COPY stream. If the corresponding data file inside the dump contains only that, then the source table was empty. Also note that you can to list the contents of a dump in directory or compressed format. You may also restore the contents of an individual table into an SQL file rather than in the target database, then modify that file as convenient, and play it into the target. See the pg_restore manpage for more. 

should be used only when it is certain that no other session is using the table at the same time. Otherwise should be used. It's slower (if the number of rows is significant), but it's designed to support concurrent access within the rules of the current isolation mode. The doc says: 

There's a misunderstanding here, because does not estimate, it runs the query for real and reports the actual time taken by each steps, as opposed to without that just reports the estimates without running the query. Consider this line from your EXPLAIN ANALYZE output: 

it's actually harmless, it's just the way of your to check for a new WAL file that's not necessarily there yet. Presumably this command looks like: 

the username you're trying to log in with is (G,D in capital letters), which does not exist with this capitalization To create that exact name, the identifier must be enclosed in double quotes, as in: 

For current PostgreSQL version (up to 9.5), queries are received by a backend in a buffer, which is limited to , defined as: 

You may also use instead of the semi-colon at the end of the query to have psql redirect the results of that query to a file, and to remove headers and footers. 

is the right method. The reason output does not appear in is that your standard error redirection is wrong at the shell level. You want: 

Besides the pointlessness, it doesn't work, as shown in the question. The reason is mentioned in the doc in 7.8.2. Data-Modifying Statements in WITH: 

That would indicate that pgadmin is using a PostgreSQL 9.3 command to take the backup, instead of 9.0. This is recommended only when migrating to 9.3. Since the target server is 9.0, better use the binary that has been installed with the 9.0 server. The path of external binaries can be set in pgAdmin preferences. To fix the problem, you may set it to the bin directory of your 9.0 installation and try to backup again. Other than that, as a one-time fix you may edit the dump file if it's in plain text, remove the offending line in the hope that it's the only problem caused by the version mismatch. 

the two decimal constants are implicitly casted to double precision numbers, since it corresponds to the closest matching types for the function: 

Why not just call the plpgsql function that raises a notice, from inside the SQL function? According to the doc: 

By default, the results are entirely buffered in memory for two reasons: 1) Unless using the option, output rows are aligned so the output cannot start until psql knows the maximum length of each column, which implies visiting every row (which also takes a significant time in addition to lots of memory). 2) Unless specifying a , psql uses the synchronous function directly on the query, which buffers the entire resultset. But when setting a , it will use a cursor-based method with successive fetch calls and freeing or reusing the client-side buffer every rows. So a big resultset should be fetched by a command like: 

has no password after an automated install, it's expected to authenticate through the method. is the first step to it. The second step is to connect through the Unix local domain socket, but your command doesn't do that, it connects through TCP, that's why you're stuck with the password problem. The installed file probably starts with these two rules, apart from comments: 

What happens under the hood to execute the first comparison corresponds to what is described in Operator type resolution: $URL$ 

So again it's being taken care of automatically, albeit differently than with 8.1's pg_dump, but this dump is going to be reloadable directly in any version of postgres>=8.1, without tweaking anything. 

The privilege to DROP an object does not exist as such and thus can't be revoked. Owning an object implies the right to drop it, and having the right to drop an object implies owning it. Here's a relevant excerpt from GRANT's documentation, which is pretty explicit about this point: 

It does not normally take several minutes to add a column if there is no default value or if the table is small, and in your case, you're good on both. But in order to be granted that lock, other transactions should not hold locks on the same table. There are several things to consider: 

Option 1: remove createdb for 9.1 When in doubt, use to learn which packages provide a certain command. Example: 

To create a cursor in a plpgsql function that may be used outside of its "parent" transaction, it's just a matter of syntax. You want the SQL implementation of a cursor, not the plpgsql variant. For this, must be used. As an example, here's the skeleton of a function similar to yours, but using SQL-level cursors that outlive the transaction: 

The results of comparisons between strings depends on . In practice, the most visible effect is the sort order. as LC_CTYPE means that the byte order is going to drive comparisons, whereas a real locale means that cultural rules will drive the comparisons. An example with french names, executed from inside an UTF-8 database: 

The execution plan shown does not seem to match the big query because the and steps are missing. Anyway you are correct than when retrieving ~50% of a table, index don't help. The best strategy is a big sequential scan of the main table and only fast hardware helps with that. For the 2nd part of the question: 

If this view was pg_dumped and pg_restored, the column would be created as with again the warning mentioned: 

The in applies to all users and all databases, but if it's not what you want, you may provide per-database and per-user values, with: 

Assuming is used as the pager, you can turn off this screen-clearing feature which indeed is annoying within psql. Use the option: 

The quotes around fields and inside fields are part of the CSV format, and they're required here, because, according the CSV spec: 

The index is probably corrupted. Hash indexes are only WAL-logged since PostgreSQL 10. In previous versions, they don't have that mechanism that makes them persist correctly across unclean shutdowns. The doc on CREATE INDEX for 9.5 has this warning: 

Postgres will never know the offset that the timestamp had when it was inserted, it's lost on insertion. What is returned to a SQL client is always the current offset depending on its current time zone. In order to keep the original time zone, you'd need to store it in an additional column. 

First let's note that at the time of this answer, PostgreSQL 10 in still in beta stage. Some issues about the ICU integration and how it's documented are still under discussion, and there might be changes before a GA release. 

When not interested in piecemeal retrieval, may also be used to stream all results to the caller in one step. 

For instance newer versions of psql (9.6) have a function that executes a query and reinject the result as an SQL query. Reworking a bit your function to return a query instead of a function: 

The locale specified with must also be supported by the system. Check with or some equivalent if that doesn't work on MacOS. 

There's no point in using UPDATE to rows INSERTed in the same statement, as the INSERT could set the missing column in the first place: 

In Debian/Ubuntu, per policy it's the package that is in charge of handling log rotation and purge for all services, PostgreSQL included. From $URL$ : 

You could abuse the setting to pass the HTTP client IP from PHP to Postgres. In PHP, once connected: 

The same as with any text literal. Or they don't need to be escaped if using prepared or parametrized statements with -style parameters, if your client-side environment supports that. 

that does not want to quit. You may try the next option, sending to the master process (pid ). This is likely to make it quit and it remains to be seen whether the subprocess with pid will quickly abort or stay stuck. It it's stuck, I would kill it with as the last resort and then restart postgres normally with the init script. 

Let's assume that the target table of the INSERT ("TableD") has a column with a foreign key to "TableC". An INSERT to this table must acquire a shared lock on "TableC" and will keep that lock until the transaction ends. Say two concurrent transactions, T1 and T2, start with such an INSERT. Each takes a shared lock on "TableC". Later on, in order to do a CREATE TABLE involving a foreign key to "TableC", T1 must acquire an access exclusive lock on "TableC". Since T2 has already a shared lock on "TableC", T1 will be put to wait until T2 commits or rollbacks. At this point, if T2 tries to create a table also involving a foreign key to "TableC" (which again implies acquiring an access exclusive lock on "TableC"), it must wait for T1 to commit or rollback, because T1 holds a shared lock on "TableC". So now T1 is waiting for T2 to finish and T2 is waiting for T1 to finish: there's the deadlock. To avoid this situation, an explicit and exclusive lock may be taken at the beginning of the transaction, or at least at the beginning of the sequence of instructions that can't be run safely in parallel by concurrent transactions. 

Using ISO 8601 time zone notation also yields the opposite from the expected result: In the same page, the accepted formats for time zone literals are listed, here's a condensed version: 

These are sane values that should not produce log overflow. The log rotation is handled by logrotate and configured in . By default, it's: 

The nice thing with the shell is that it has no problem concatenating with ,whereas psql will not accept or (as far as I know) offer a simple alternative. 

This double connection depends on whether a password has to be submitted by the client (according to ) and, if yes, whether the option is passed to . In fact avoiding that second connection attempt is the only raison d'être of this option. According to psql manpage: 

No buffer space available corresponds to a network error where the client is not enable to allocate the resources to initiate a connection. Generally, this means that there are other programs holding too many connection handles (sockets). According to a post on IBM developerWorks: 

That invocation outside of SQL might be handy in the rare cases when these expressions may have to be computed in the middle of a failed transaction, or when disconnected from the database. 

Assuming the default as shipped with Ubuntu's PostgreSQL package, a plausible reason for these huge log files would be that your data import would generate tons on warnings like this: 

You want to do an incremental backup of the archive folder to remote storage. Should you need to restore from the backup, the basic scenario is that you'd need your base backup as the starting point, and the entire contents of the archive folder to replay the transactional activity that happened between the starting point and the crash. Also to avoid having the files in the archive folder piling up forever, you want to do a new base backup from time to time and delete the files that were archived before the new base backup. 

In order to not rely at all on , you might transform the INSERT queries in systematic way following this pattern: 

Aside from invisible characters, the possibility of an OS patch changing the results of such an query exists through a change in locales. An index involving text types is only valid if the underlying collation never changes. Postgres relies on the OS for comparing strings, through the libc, so a change in an OS collation may have the effect of an index becoming unusable, as if it was corrupted. If there is an index on the column, you can test this hypothesis by issuing before the offending query and see if it gives the same results. This check may also reveal an index corruption independantly of any problem with the collation. 

Then call the SQL statement , stream the results to the web client, until returns no row. Then close the transaction, and the cursor will disappear with it. It's also possible to not use a transaction if the cursor is declared . 

No, it's the same issue. The error message used to refer to the relation in older versions of PostgreSQL, but it was changed at some point (version , probably) to refer to the corresponding file instead. I think the switch to the new message format happened here: 

Yes, the SELECT executing in addition to the INSERT is the behavior that should be expected. Based on the qualification (the where exists()... clause) and the clause, the RULE produces a sequence of queries looking like this, in pseudo-code: 

Note that has be run to initialize an empty data directory before creating databases or users. That would be the equivalent of if you prefer to look at this from the MySQL analogy angle. 

Still, concurrent transactions should be blocked by the program to work on the same "boxes", otherwise a trigger-based check could let incoherencies pass. This is because the trigger can't see the potential changes of other sessions that would not have yet committed when it runs. 

Probably not, because blindly executing on random databases would be foolish. Unlike that is pretty much unavoidable on a normal live database, fixes an oversight, a situation that should not arise in the first place, since applications should unlink the large objects that they no longer use, at about the same time that they delete the references to these large objects. In a database with a large number of large objects that are properly handled, would burn CPU cycles and I/O at each invocation, just to compute every single time that there is nothing to remove. Moreover, there are two assumptions that must do that are debatable: 

That's the result you got on your db on MacOS. But when I'm logged to a different database, with an UTF-8 locale this time: 

This is a close approximation to the number of bytes that will be retrieved client-side when executing: 

The fact that UNION deduplicates the tuples takes care of filtering cases when the same word is found among different subqueries of the UNION construct. This query produces only IDs of tasks. They'd need to be joined against and again to get back the columns that need to be displayed or returned. 

If you have installed postgresql with brew there's one that comes with it and that will connect to brew's postgresql. Then if you have also installed Postgres.app there's another at a different location that will connect to Postgres.app's postgresql. Since OS X also ships with its own in , that's a third that expects to connect to Apple's postgresql, which is not running unless it's OS X Server Edition. According to the error message, this is the one that you're trying to use and which fails. The solution is to use the command that comes with the PostgreSQL you're actually running. Locate its directory and type the full path of , as in , and/or change your as suggested in postgresapp's doc. This is a common problem for PostgreSQL on Mac OS X due the existence of many concurrent packages and of client-side programs pre-installed by Apple.