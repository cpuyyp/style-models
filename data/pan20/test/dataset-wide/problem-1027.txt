First, let's assume that is at , and not the impossible reported (impossible with a total of on the machine). Anyway it's much too high. As said in Resource Consumption in PostgreSQL documentation, with some emphasis added: 

The JSON representation of in a trigger is just , so that part is straightforward. But writing into files on the server is a different story. It's only allowed to superusers because it gives the ability to corrupt or destroy all the instance's data. I don't think there's a builtin function to write on the filesystem, but the contrib module adminpack provides one: 

Also, the binary contents are sliced into tiny chunks of 2000 bytes in . There is one row per chunk, so when importing large contents (large by today's standards), the number of rows in this table tend to grow quickly. Although that does not imply a hard limit, users should be aware of that for performance reasons. 

Also, there's something rather unusual in the output of your question: the postgres server is running under a Unix user, whereas generally, the dedicated Unix user is used for that. 

When doing a TCP connection with the first rule doesn't match. The second rule matches and it triggers the demand for a password, but as none was set this can only fail (empty passwords are not allowed either). The solution is to remove from psql invocation, for the first rule to be taken instead. As it's on a Unix system, it will attempt a connection through the Unix domain socket. Once logged to psql as the postgres user, you may set a password with the command or See also $URL$ In the section Using pgAdmin III GUI , they suggest to change the first rule from auth to , after having set a password. Personally I don't quite get why. I'd rather leave that rule alone and run pgAdmin from my own Unix account, choosing localhost TCP connections. 

Case insensitive or accent-insensitive collations cannot be used consistently because internally PostgreSQL considers that strings with a different binary representation are not equal. When the collation-aware comparator says they are equal, it uses the non-collation-aware function as a tie-breaker. It must do that, at least because of hashing. Hashing is used in joins and having whereas is a killer problem for a normal hash table implementation. This happens with any collation that ignores case or accents. This is discussed in detail in this thread in the hackers mailing list: strcmp() tie-breaker for identical ICU-collated strings. The developers expect to solve that problem at some point, but not as of PostgreSQL 10. 

You can start troubleshooting this by looking at the bytes behind the characters, to check if there are invalid UTF-8 contents independently of the display layer. An UTF-8 string can be converted into its series of bytes in SQL with 

Normally we'd expect that when postgres was restarted, the crash recovery process would have removed files related to a rollback'ed index from the data directory. Let's assume that it didn't work, or at least that it has to be checked manually. The list of files that should be in the datadir can be established with a query like this: 

And it creates there a schema named with a handful of tables prefixed by It's probably good enough to connect to the database and interpret the result of: 

No, the directory can't be omitted even if the server has been properly shut down before copying the data directory. If you try, you should see that the target server will fail to start, complaining that it can't find a checkpoint record. 

It's also consistent with $URL$ You may try on your system and check if is part of the results, and if yes, to what package it belongs, and then what is the origin of that package. Maybe for some reason it's the same as the sample postgresql.conf, except it doesn't have the suffix. 

This assumes than any simplified version of a word is already part of the table, as seems to be the case in your sample data. Otherwise they should be inserted. 

I'd venture that it has nothing to with version but that it may be an invisible spurious character on the command line that is causing this mess. As a demo, if I type this in , it works as expected at first: 

The formats accepted by include localized patterns that make it not immutable. Example of different results with the same input: 

Notice that there's no directory: a PG instance does not need or create such a directory. It had to be created by a person or a script that is not part of PostgreSQL itself. Since you mention , if the files in this directory follow the naming convention of WAL files (24 numbers in hexadecimal in a 3x8 layout) found in , it's plausible that they're WAL files copied for continuous archiving. Check the in . In this case, the name for the target directory make sense, but its location doesn't. Such backups are useful for disaster recovery when the data directory has disappeared, so obviously the data directory itself is the last place where you want such backups. 

Instead of running , use pg_createcluster in the first place. The problems you're trying to solve are already taken care of when using the tools provided by the debian/ubuntu packages to handle multiple clusters and multiple PG server versions simultaneously: See also: pg_wrapper pg_dropcluster pg_ctlcluster pg_lsclusters 

generally doesn't need much memory when playing large SQL files, since it doesn't buffer the whole file, only one query at a time, or it uses a stream. The main situation when it may run out of memory is not when importing, but when SELECT'ing a large resultset, especially on 32 bits systems. This situation is generally solved by setting . On import, the dump would need to contain unusually large rows to cause client-side memory issues. The maximum size of a column is 1Gb, so it's theorically possible that it would cause trouble on import, if the database has that kind of contents. If the system doesn't have enough RAM, maybe you could just add swap space. 

For that, it's overkill to write a plpgsql function. Or if you really want one, just replace this by a function parameter put the above UPDATE query inside a block. 

any subsequent call to will act as if it has been invoked with the command-line option. See Environment Variables in the libpq documentation for all these variables . They can be used to provide default values to almost every parameter of a connection to the database. 

Then the invocation will work. Or alternatively, don't recreate the user but always refer to it in lowercase. 

Yes, it looks like is an enum type. Assuming you're using the command-line interpreter, try this command: 

As to what is logged, it can be configured with log_line_prefix but it doesn't go as far as logging tables and columns names. To log which columns are affected by UPDATEs you'd need to do that in triggers in every table, as in tablelog 

... or, if you really meant that having the role doesn't let read the tables even tough it lets write to them, then you must grant the role to (in addition to granting the role ). Not having either of those has the consequence that was never granted the right to from tables in the mentioned schema. 

Since you mention a salt, I assume you're looking for short strings that are hard to guess or reproduce by an outsider, despite being originated with a database sequence. The permuteseq extension essentially does that. The function that produces a short string from an integer has to be provided, for instance: 

Since the is an internal notion of postgres, the implication would be that the data structure for this relation is in a corrupted state, beyond troubleshooting at the SQL level. The safest way out of this mess would be, if at all possible, to the affected databases, then drop and reload them, and then recreate the constraints on this clean state. 

as a type is a 32-bit unsigned integer that stands for , and is used by the system as an all-purpose surrogate key to refer to different objects and sometimes rows in . Being the type for keys to large objects is just one of its many uses. Other "objects" like tables, sequences, types and more are refered to by their . 

Indeed an empty password is equivalent to no password so it's not going to be accepted by the server when says that a password is required. Still, there are various ways to avoid inputting the password: 

Yes, these statements can deadlock each other. The fact that the transaction they're part of has only this single instruction does not suppress the possibility of a deadlock. The false hope that it would might come from an optimist interpretation of what atomicity provides at the instruction level. An UPDATE does not acquire several row locks in a single atomic grab: it has to grab them one by one, and any of them has a separate chance to be impossible to acquire. The question's query comes from the documention at $URL$ except there it's shown as two independent queries processing a single . Surely, if updating two rows in the same query solved the deadlock problem, the doc would mention it at this point. It does not mention it because it's not true. 

The command tests that the process (corresponding to ) does exist from the point of view of the operating system. In the positive case, it reports that it's running (note that the word is not used). Essentially that's implemented by reading and checking the result of . It does not mean that this PostgreSQL instance does accept connections or is able to execute queries. To that purpose, better use or a script like Nagios check_postgres plugin. 

The restore must copy from the archive directory (which typically is outside and entirely under the DBA's control) into (as expanded by ). I'm assuming that is your archive directory since I don't see what else it could be. Personally I'd place it outside of by principle but that's not mandatory. 

Edit: after the above command, only a superuser may create new objects inside the schema, which is not practical. Assuming a non-superuser should be granted this privilege, this should be done with: 

Consider implementing the check in a trigger, not a rule. The trigger is the perfect tool for the job in your case. On the other hand, it is a known fact that rules are touchy and much harder to master. 

will not issue statements for databases like or that have been created by at cluster initialization time. However it will dump the contents of these databases if you created any objects in it (by default they're empty). So when restoring the result of into the same instance, it's likely that such errors will occur if these databases happen to have contents. 

If you can't use , as an alternative still based on encrypting the ID, you may look at the plpgsql implementation of SKIP32, or XTEA for 64-bit (bigint) which won't require a compilation or being superuser. The drawback is that you cannot fine-tune the size of the output by changing the sequence's range. 

In this example, there is both and in the table, but is technically redundant with . You could create a sequence outside of the table without the syntax like this, and not even have in the table: 

TL;DR: forget that, run the SELECT in the same psql session than the INSERT. From the point of view of PostgreSQL, there is no second user, it's the same user on the same connection, which is seeing the effects of its own not-yet-committed INSERTs. Otherwise it would be a dirty read, which PostgreSQL documentation states as "Not possible" in any of the isolation levels. A middleware routing queries from different connections from its clients to the same connection to the backend could produce the effect of a dirty read to its client. In practice, your goal is out of reach with a normal pooler, because these tools actively forbird situations where a client would see uncommitted changes of another client. It's part of the effort to be transparent to users, to whom the results should be the same whether their connection is shared or not. For example PgBouncer when using statement pooling, says: 

Based on the first error message, the .gz backup file seems to be truncated. This could be confirmed with the command: 

This means that I have to wait until all of my files are copied back from the new master the old one, which means a lot of data traffic When using , yes. But, if you're confident that the existing files at the destination are almost identical to the source, may be used. The rsync remote-update protocol is able to identify what parts of files changed to minimize the amount of data to transfer. See examples of uses on the postgres wiki: Binary Replication Tutorial / Starting Replication with only a Quick Master Restart My personal experience with versus is that is good if the "new master" has not drifted that much from the state when it became master, like a few minutes of transactions on a moderately busy system. Otherwise it still works but appears to be faster. 

Now letters are put in lower case. The fix is to create the database with the correct . It cannot be changed afterwards. By default, this setting comes from , but it can be overriden by choosing , if does not suit you, for instance: 

The cast to in the query might even be superfluous if the planner is able to infer the type by itself. 

If the resultset has many wide rows making the sort inefficient, sorting a hashed representation instead might be a better idea: 

With a client-side pivot (psql) A client-side, -only solution is more straightforward with (psql 9.6+). The query would be quite different: it must first "unpivots" (with the subquery) the columns to get the desired column names in the 2nd column, the corresponding values of these parameters in a 3rd column, and the desired column order for the pivoted data in a 4th column. This would look like: 

Fortunately implementing fast hash-based search at the SQL level is not hard, and in your case (large arrays, no updates, exact match) it may even be the most effective method. Steps: 1) choose a hash function. I'd suggest on the text representation of the array: 

(as opposed to ) When will invoke with a file relative to it, the data file will be found since it's in the current directory. That should work recursively, assuming scripts in subdirectories also follow that convention, and that they don't do an unrelated that would interfere with the waiting to happen on the way back to the top. 

Parallel INSERTs may be faster, but only with a very fast disk subsystem (when data is interleaved on many drives). Otherwise this will be slower. Once you're done with modifying , it can take the new name with: 

does not match for the same reason than the previous line. If you want to "trust" everyone locally through the Unix domain socket, you may change the first line to: 

Note that the empty select (no column) is a relatively recent addition in PostgreSQL.On older versions, the above query will cause a syntax error. 

On the other hand, is a client-side feature. It benefits a client by ensuring that it connects to the intended server. It does not benefit a server as it's the server that is being checked. Having a server being configured to "refuse to not being checked" doesn't seem to make much sense, as if a server would say "I don't trust myself". 

The function, as the first choice to format dates, is quite limited with time zones. According to Template Patterns for Date/Time Formatting in the documentation we have and and they'll output a time zone name, not a offset against GMT. You may use to get the offset as a signed number of hours and to get the number of minutes that comes with it. This offset is documented as: 

This can fail if the server's clock goes backward or if non-log files are dumped into the same directory as the log files. 

comes before , because the accented E compares against O as if it was non-accented. It's a cultural rule. This differs from what happens with a locale: 

If the entry doesn't show up, the most plausible explanation would be that you're editing a at a wrong location. If on the other hand it's found at startup, when later creating the trigger file to fail over, this entry should appear: