The limit exists for the first two examples that come to mind, namely topological entropy on the full shift and on certain simple Markov shifts. If $X \subset \Sigma_d^+ = \{1,2, \dots, d\}^{\mathbb{N}}$ and $\sigma$ is the shift map, then for the topological entropy the quantity $a_n$ denotes the number of words of length $n$ that appear in some sequence $x\in X$. If $X$ is the full shift, then $a_n = d^n$, the entropy is $h = \log d$, and we quickly see that $a_n / e^{nh} = 1$ for all $n$. Slightly more interesting is when you have a Markov shift, say $X\subset \Sigma_2^+$ determined by the transition matrix $\begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}$. In this case it's not hard to show that the sequence $a_n$ is actually the Fibonacci sequence, and thus writing $\phi = \frac {1+\sqrt 5}2$ and $\psi = \frac { 1-\sqrt 5}2$, we have $$ a_n = \frac 1{\sqrt 5} (\phi^{n+2} - \psi^{n+2}). $$ Since $|\psi|<1$, this shows that the limit of $a_n / e^{nh}$ exists. My guess is that a similar argument works for other Markov shifts and shows that the limit exists in those cases, based on obtaining a recurrence relation for $a_n$ and then an exact formula using standard tools for solving such sequences. All that said, it's not immediately clear what the significance of the limit is, and I don't know of any name for it. For other interesting shifts, such as sofic shifts or shifts with specification, I'd be surprised if the limit always exists. What is certainly quite important is to have conditions under which the ratio $a_n / e^{nh}$ is bounded away from $0$ and $\infty$. Such estimates are a significant part of arguments on the uniqueness of a measure of maximal entropy (and more generally uniqueness of equilibrium states), in particular the proof that such a measure satisfies a Gibbs property. For example, see Bowen's 1975 paper "Some systems with unique equilibrium states". (Dan Thompson and I also struggled with this not too long ago in Section 5.1 of this paper.) It turns out that in the general setting, one of those bounds is immediate -- the sequence $a_n$ is submultiplicative, and so it's not hard to show that $a_n \geq e^{nh}$ for all $n$, whatever other properties the shift space has. Getting an upper bound on $a_n / e^{nh}$ is harder and requires some sort of specification property. 

Given the handedness of the spiral and the direction of the spiral when hitting the last point, there is a unique spiral that passes through all the points. Enumerating the 8 possibilities yields the answer. Proof: Assume without loss of generality that the spiral turns clockwise and ends facing south. Clearly, the spiral ends on a point in $p = (x, y) \in S$. No point can lie strictly to the east of $p$, and among points sharing the same ordinate, $p$ must be the southernmost. Thus, $p$ is determined uniquely. If we remove $p$ from $S$, we can now determine the last point the spiral hit when it was headed east, prior to turning south. It is the northernmost, then easternmost point in the set. Repeating the process, we exhaust all the points and determine the unique clockwise spiral that ends facing south. The same process can be repeated for each direction, clockwise and counter clockwise and the length of the 8 spirals can be compared. 

Alice and Bob respectively know a vector of $N$ real numbers $u$ and $v$. They would both like to know $\rho = \langle u,v \rangle/N$ but Alice does not want Bob to gain anymore information about $u$ than given by $\rho$ and vice versa. The easy solution is that they go to a trusted third party that calculated $\rho$ for them and hands them the solution. That's not very interesting though. Given that homomorphic encryption exists, we know there's a way to do this without a trusted third party, there may exist a simpler and more elegant solution to the problem though. Let's relax the problem a little bit and allow for asymptotic solutions in three respects 

Regarding the first question: if we take $G=\mathbb{Z}$ or $G=\mathbb{R}$, then there are plenty of group actions that cannot be realised as isometries. Example 1: Let $G=\mathbb{R}$ and $X=\mathbb{R}$, and consider the flow $\phi_r(x) = e^r x$. Then $G$ is locally compact and $\phi$ defines a continuous $G$-action of $X$, but there is no metric that makes this an isometry; indeed, given any neighbourhood $U\ni 0$ and $0\neq x\in U$, there exists $r_0$ such that $\phi_r(x)\notin U$ for all $r>r_0$. This property holds for any equivalent metric but cannot hold for an isometric action. If you want $X$ to be compact, you can consider $G=\mathbb{R}$ and $X=[-\pi/2,\pi/2]$, and given $r\in G$, consider the map $\phi_r\colon X\to X$ defined by $\phi_r(x) = \tan^{-1}(r + \tan x)$ for $|x| < \pi/2$ and $\phi_r(x)=x$ for $|x|=\pi/2$. Then $\phi$ is a continuous group action that fixes the two endpoints; one is attracting, the other repelling, and $\phi$ cannot be an isometric action for the same reasons as above. Example 2: Let $G=\mathbb{Z}$ and $X=\{0,1\}^\mathbb{Z}$ with the product topology; then $G$ acts on $X$ via the shift map. In other words, $\phi_n(x)\_i = x_{i+n}$. Once again, topological considerations using fixed points can be used to show that this action is not isometric for any metric on $X$ that induces the product topology. So I guess one can sum up those obstructions by saying that in order for $G$ to act isometrically, every fixed point has to be topologically stable. That is, suppose there is $p\in X$ such that $\phi_g(p)=p$ for all $g\in G$, and that there is a neighbourhood $U\ni p$ such that for every $p\neq x\in U$, we have $\phi_g(x)\notin U$ for some $g\in G$. Then $\phi$ is not an isometric action for any equivalent metric on $X$. 

The Viterbi algorithm works by maintaining, at all times between $[t,T]$, for all states $S^k_t$, the likelihood of the maximum likelihood path between this state and any state $S_T$. You can run it a second time, but this time, maintain the likelihood of reaching any state $S_T$ following a path that maximizes likelihood while being less likely than the one previously calculated. To avoid dealing with exact duplicates, you can add a very small noise to every edge. In maybe-compiling C++, 

If you know the exact distributions, why throw away this precious information? You do not make it very clear what hypothesis exactly you are trying to test... but the Bayesian way to solve this problem would be to start with a prior for the distribution where each sample is drawn from. Using the likelihood that each distribution would produce each sample, you can compute a posterior probability for each sample to come from each distribution. I can elaborate if you specify what it is you're trying to test for. 

Assume a directed graph $G = (V,E)$ is drawn from a random graph distribution, for instance Erdős–Rényi's $G(n,p)$ (but with directed edges). Let $S:V\rightarrow\mathcal{P}(V)$ be the direct successors function, that is $S(u) = (\left\{u\right\} \times V) \cap E$ Let $f_0: V\rightarrow\left\{0,1\right\}$ be an initial, arbitrary, labeling of vertices with $0$'s and $1$'s Define $$n_t(u,x) = \left|\left(\{u\} \cup S(u)\right) \cap f_{t}^{-1}(x)\right|$$ $n_t(u,.)$ is simply the number of vertices labelled one or zero among $u$ and its direct successors. Now define recursively: $$f_t(u) = \left\{ \begin{array}{cc} 0 & \textrm{if} & n_{t-1}(0) > n_{t-1}(1)\\ 1 & \textrm{if} & n_{t-1}(1) > n_{t-1}(0)\\ f_{t-1}(u) & \textrm{otherwise} & \end{array} \right.$$ Simply speaking, at each step $t$, a vertex's label is changed to reflect the majority among itself and its direct successors, or is unchanged in case of a tie. I am interested in the convergence of the sequence $f_t$, and in particular convergence to a limit that is constant over $V$ (all $1$'s or all $0$'s) for all initial conditions. How is the probability of convergence affected by the statistics of the graph? I'm sure the problem has been studied and I'd happily take some references on the topic. 

If $a_i = b_j$, then there is a single outgoing edge from $(i,j)$, labeled with the common value, going to $(i+1,j+1)$. If $a_i < b_j$, then there is an edge $(i,j)\to (i+1,0)$ labeled with $a_i$, an edge $(i,j)\to (0,j+1)$ labeled with $b_j$, and for each $a_i < c < b_j$ there is an edge $(i,j)\to (0,0)$ labeled with $c$. 

the endpoints of $I$ are fixed by every map in $\mathcal{C}$; the maps in $\mathcal{C}$ are strictly increasing; $\mathcal{C}$ is closed under composition; every positive real number appears exactly once as the ratio of the derivatives of $\mathcal{C}$ at the right and left endpoints of $I$. 

Motivation: For the sake of concreteness, I'll state a very particular context, but my question is a little more general. I'm trying to find a function $\gamma\colon [0,\delta) \to [0,\delta')$ that satisfies the following functional equation: $$ \gamma(y) + \gamma(y)^{1+\varepsilon} - y = \gamma(y - y^{1+\varepsilon}). $$ Here $0<\varepsilon<1$, and I'm most interested in the behaviour of $\gamma$ for very small values of $y$. I've persuaded myself that $\gamma(y) = y^{1/(1+\varepsilon)}(1+o(1))$, but I'd really like an exact solution, which I imagine would have to come in the form of a power series $\gamma(y) = \sum_n c_n y^{a_n}$. Due to the form of the equation, though, the exponents $a_n$ aren't going to be integers, and unless I'm mistaken, they won't even all be integer multiples of some fixed $a_0$, so I can't get back to "regular" power series by doing a simple change of coordinates. This motivates my question... Question: Can anyone suggest a reference on dealing with power series where the exponents take non-integer values (and are not all integer multiples of some fixed exponent)? Or suggest a paper where such power series are used (for any purpose)? Ideally I'd like to see how similar functional equations have been solved, but any references at all would be appreciated. 

One road is for Alice and Bob to generate vectors $\epsilon$ and $\eta$ where every entry is a random normal number with a very large variance $\nu$. Alice sends $u+\epsilon$ to Bob who computes and publishes $u.v + \epsilon.v$ Bob sends $v+\eta$ to Alice who computes and publishes $u.v + \eta.v$ They also compute $(u+\epsilon).(v+\eta)$. With all that, they can compute $(u.v - \epsilon.\eta)/N$. There remains an error term, $\langle \epsilon.\eta \rangle/N$ which has standard deviation about $\nu/\sqrt{N}$ The problem is that there is a tradeoff here. If the $\nu$ is too small, too much about the vector is divulged, if it's too big there is too much noise in the result. Another idea would be for Alice and Bob to agree on a random base of $\mathbf{R}^N$. Alice could then pick $\epsilon$ as a random linear combination of the first $N/2$ vectors, and Bob could pick $\eta$ as a linear combination of the last $N/2$ base vectors. This guarantees $\epsilon$ and $\eta$ will be orthogonal, but now Bob would know the projection of $u$ over a very large subspace. Not that good. Thougths? 

Not a complete answer, but for $n=4$ if you start with an arithmetic progression ($1,2,3,4$ works) then you have probability $1$ of ending up with them all being identical. Consider the following set of states for the quadruple $(x_1,x_2,x_3,x_4)$ once it has been ordered so that $x_1 \leq x_2 \leq x_3 \leq x_4$: State $A_0$: All the $x_i$ are equal. State $A_1$: $\vec x$ is of the form $(a-d,a-d,a+d,a+d)$ for some $a,d$. State $A_2$: $\vec x$ is of the form $(a-d, a, a, a+d)$ for some $a,d$. State $A_3$: the four numbers are distinct and form an arithmetic progression, so $\vec x = (a - 3d, a-d, a+d, a+2d)$ where $a$ is the average value. Given $\vec x$ in one of the four states, choose a permutation from $S_4$ at random and apply the procedure described in the question. There are really only $3$ equivalence classes of permutations, since you only need to know which of the three numbers $x_2,x_3,x_4$ does not get added to $x_1$, and then everything else is determined. It's not hard to show that if you are in state $A_j$, then with probability $1/3$ you remain in state $A_j$, while with probability $2/3$ you move to state $A_{j-1}$. (Of course if you're in state $A_0$ then you stay there no matter what.) This means that almost surely you eventually end up in $A_0$, where all four numbers are equal. I'm not sure what happens if you try this for $n\geq 5$. 

Suppose I have a random variable $X_0$ with a p.d.f $f_0$ supported on the real interval $[a_0, b_0]$. $X_1$ is the restriction to $[a_1, b_1]$ of the sum $X_0 + g$, where $g$ is normally distributed $g \sim \mathcal{N}(0,1)$ $$f_1(y) = \frac{\int_{x=a_0}^{x=b_0} f_0(x) e^{-(y-x)^2/2}~dx }{\int_{x'=a_1}^{x'=b_1}\int_{x=a_0}^{x=b_0} f_0(x) e^{-(x'-x)^2/2}~dx~dx'}$$ or $$f_1 = \frac{1}{Z} L( f_0, (a_0,b_0,a_1,b_1))$$ Where $Z$ is a normalizing factor. $$Z = \int_{a_1}^{b_1}L( f_0,(a_0,b_0,a_1,b_1))(x')~dx'$$ $L$ is a linear operator over functions in $\mathcal{L}^2$ What are its eigenvectors? What happens when I replace the interval with a $n$ dimensional box and the normal distribution with a multivariate normal? Thanks! Clarification: I'm looking at f in $\mathcal{L}^2(\mathbf{R})$, not $\mathcal{L}^2([a_0,b_0])$ otherwise $L$ is obviously not an endomorphism. 

Let $(x_1 \ldots ,x_n) \in \mathbb{R}^n$ and $f_i = \Pi_{j=1, j \neq i }^n ( x_i - x_j )$ I'm trying to evaluate $(f_1, \ldots, f_n)$. A trivial algorithm runs in $\mathcal{O}(n^2)$ but given the very specific form of the problem, there's got to be something faster. Maybe I've overlooked something simple, maybe a fourier transform is in order... What are your thoughts? 

It's intuitively desirable for the answer not to depend on a unitary transform of the matrix. To estimate the distance of our estimate to the other matrices, a natural choice is the Kullback-Leibler divergence. The equivalent of a mean is then to pick: $$\hat{\Sigma} = \text{argmin} \left( \sum _{k=1}^{n} \text{tr}\left(\Sigma^{-1}\Sigma_k\right)-\lg \left(\left|\Sigma^{-1}\Sigma_k\right|\right)-d\right)$$ Matrix calculus actually tells us that $$\hat{\Sigma} = \frac{1}{n}\sum _{k=1}^n \Sigma_k$$ too see why differentiate with respect to $\Sigma^{-1}$ Handwaving follows: In a way, the KL-divergence plays the role of the squared distance here, since the average matrix minimizes the average KL-divergence. Note that this is similar to the Riemann metric, but instead of looking at $\sum_i \lg{(\lambda_i)}^2$ we're looking at $\sum_i \lambda_i-\lg{(\lambda_i)}-1$. If the matrix are contained in a small ball, the $\lambda_i$ are close to $1$ and the difference between the two functions - up to a scaling factor - is $O((\lambda_i-1)^3)$. The KL-divergence has a probabilistic interpretation which isn't clear with the Riemann metric. We could get a median by using the square root of the KL-divergence. $$\hat{\Sigma} = \text{argmin} \left( \sum _{k=1}^{n} \sqrt{ \text{tr}\left(\Sigma^{-1}\Sigma_k\right)-\lg \left(\left|\Sigma^{-1}\Sigma_k\right|\right)-d}\right)$$ It's easy to compute iteratively since $$\frac{df}{d\Sigma} = \sum _{k=1}^{n} \frac{(\mathbf{I} - \Sigma^{-1}\Sigma_k)\Sigma^{-1}}{2\sqrt{ \text{tr}\left(\Sigma^{-1}\Sigma_k\right)-\lg \left(\left|\Sigma^{-1}\Sigma_k\right|\right)-d}}$$