You need to get a certificate that has all those names as subject names or a wildcard certificate. So, single certificate will be used for all of them. After that, you can configure each website with that certificate and SNI. 

Did you try downloading and using SSMS 2014 to see if you can connect? This problem used to come up when using different versions of SSIS and SSMS. 

You can't do it the way you want. As long as Windows DNS server sees itself auhoritative for the zone, it won't ask anywhere else. And, your clients will not look for the missing records in Tux, even if you tell them to use it as secondary. If Tux support dynamic updates, you can use it as your primary DNS server for Active Directory as well. Like this You should've used a subdomain for your AD domain name. Consider it, if it's not too late. 

If you read "The [homes] section" on this page, it says that //server/myusername is cloned on the fly from //server/homes . So, you need to make homes not browseable, aka 

Now you know your snap-in's ID, you need to use it in GPO. There are couple of ways to do that. You can edit the administrative template file () you used before and add a new policy section with Hyper-V snap-in's ID, or, you can use admx file to see what is modified in registry when you change that Group Policy setting and deploy correct settings with GP preferences. Instead, I just changed one of the settings using gpedit.msc , opened in notepad to see what it changed, opened regedit and added the new key with snap-in's ID and value. After testing to see if it worked, you can export it and deploy it using GP preferences. 

If you're truly asking, "can users run an interactive program without being logged into Windows" then the answer is "No." But if you mean, "Can users run an interactive program without having to know or enter their Windows credentials" then the answer is yes: you simply set up an auto-login process, and as soon as the PC boots, the user is ready to run an interactive session. You can set particular programs up to autorun, so that the user doesn't even need to launch them. 

I use PolyMon to monitor these kinds of things. You can define various "alert" conditions about which you'd like to receive notification, so I get notified if one of my servers is having a problem. But it also stores all these results in a long term database, so I can look back at the memory usage of server "X" and see it's memory usage trends over the last N days/weeks/months/years. $URL$ 

powershell? save this as a .ps1 script and execute. (substitute the IP address you care about...) $ipaddr = Get-WMIObject win32_NetworkAdapterConfiguration | Where-Object {$.IPEnabled -eq $true } | Foreach-Object { $.IPAddress } if ($ipaddr -eq "10.10.10.10") { "Yes it's here" } else { "Nope" } 

I'd do the whole thing inside Powershell. I wrote a script a while back to do some poor man's load balancing. It called some CLI code to return the state of a VM and acted accordingly. You'll do the same: Have the Powershell call a CLI statement (Using getstate) to get the state of your various guest VM's. If any of them are down, fire an SMTP email or use CLI to start the vm or whatever. Launch the Powershell by scheduled task on whatever interval you choose. Ought to work like a charm. 

My server distribute two main websites, says : www.google.com & www.facebook.com (yeah I know :p) I want them to be distributed via https. Using Apache, I defined a vhost file in sites-available/enabled containing this : 

Every three months, my Let's Encrypt certificate expires, and my customers get an invalid https certificate. So I recently placed the following cron task : 

does not work, with the error that "/usr/share/phpmyadmin/phpmyadmin/index.php" was not found. So is my actual configuration secure enough regarding the access of ? Thank you for your help! 

But still, the problem persist. I don't know what I missed and where to fix it. And everything I tried to search on Google doesn't really helps me (it's about /etc/aliases most of the time). Thank you for your help. (if you need more details, please ask, I'll add them) 

I'm having some security issues regarding my uWSGI configuration. Here's the current issue: I have a front server, called api.domain.tld, that have NGinx installed with that points to the uWSGI instance using a file. This uWSGI instance on the frontend server is configured that way: 

I know it's possible to server static files using error_page in NGinx, but I was wondering if it was possible to server an url from my local (socketed) Flask app that is served through UWsgi. Here's the NGinx configuration : 

On your local machine, (The "admin" box that has the external hd plugged in) share a folder on the external hd with "everyone", full rights. On the remote server, start:run, type \adminbox\newsharename Once you can browse that unc from your rdp session on the remote db server, you'll be able to back up to it using sql studio. 

Yes. You want option B, so that the OS is referencing 2 different disk files. One DB can be optimized for writing, the other for reading. Or you could accomplish the same in a single db, if you split your tables into separate filegroups/separate files. 

Honestly can't say WHY you're getting the error without being able to dig around. I would try removing and re-adding all rights/permissions on your My Documents folder, to make sure you don't have some wierd sub-permission set on it. As a work-around you might also try something like Unlocker. It may actually be that some kind of process has the file locked.... $URL$ 

Rather than figuring out the perfect permissions, you could launch the update process using the "runas" utility 

try using a fully qualified username: psexec \192.168.0.4 -u {targetmachinename}\Administrator -p adminPass ipconfig That'll tell the machine what context to use for that username. BTW, the "Administrator" account is enabled on that machine, right? Can you manually login using that account? 

These are the privileges. You already know what permissions are. If you think about it , there isn't a distinct line between them tho. "Allow log on locally" is a privilege , but, when you add a user there, you're just giving them permission to log on to that computer. Again, you can say that permissions are given on resources, but, "computer" is also a resource in my example above. Instead of trying to understand what they are called, you should learn the list of privileges from the web page I linked above. That is the whole list and that will help you solve your problems. 

This will export all the tasks under "CustomTasks" folder to C:\Temp folder and file names will be "WhateverTheTaskNameIs.xml" . For Windows Server 2012 and later, you can use and cmdlets. 

Google for "mmc snap-in registry location" Open regedit and go to HKEY_LOCAL_MACHINE\Software\Microsoft\MMC\Snapins Install Hyper-V Management Tool and check the new snap-in's ID in this location If snap-in is already installed, just browse the keys and find related snap-in and its ID 

You can block DHCP traffic on Windows 7 with its local Windows Firewall and add PXE options on your other DHCP server if it's possible. So, everyone gets IP address and PXE information from a single server. 

Why don't you just check "Log on" tab of those services? I think it's any of them tho. Did you find any events in Security log when you searched for that username? 

I made a shell script that deletes an unix user and among other things, here the command used to delete the user : 

And that, in the same server. I thought about installing only Apache on the server, that would redirect request to the dedicated Docker instance based on the server name, but then I would have to install Apache (again !) andMySQL on any Docker instances. Is this possible and moreover, is this interesting in term of performance (or not at all)? Thank you for your help. 

What does that mean ? I did the same command with a smaller file and I got a , so clearly, the first try didn't succeed completly, but I don't know what went wrong neither what to do to make it work completely. Is there a possible solution to that ? Thanks for your help ! 

I manage servers where users have their own websites on it that can be accessed by FTP (like an hosting company) and instead of working on isolating LAMP stack processes, I was wondering if it was possible to implement Docker and use an images per website. From what I understand, you can expose Docker instance via their ports, so if you run two docker instance on the same server, you'll have to expose two different ports. But is it possible to export not ports, but server name, like : 

It basically resets your DNS setting to get the DNS server that your DHCP provides, and converts it to a static setting adding 8.8.8.8 in the end. "Wi-Fi" is my network connection's name. Yours might be "Local Area Connection", "Ethernet" or something like that. This might also work. 

If you're just trying to redirect example.net web site visitors to example.com , that's what you should do. Point example.net A DNS record to example.com's IP address. After that, you have a couple of options; -Create a new website on your host that accepts example.net visitors and redirect them using HTML/Javascript in your index page , or using your server's redirection capabilities. -On your single website, check to see if visitors' URL matches example.net and redirect them to example.com using Javascript in your index page. Pick one. 

You can use Activity Log from the new portal. It's under Management Services - Operation Logs in the old portal. 

If you're able to ping www.google.com , you're all set. Services and ports part is only for port forwarding from outside to inside. You should check your antivirus on the host. Easiest way is to disable it and try again. If it works, you can look for the specific setting. 

nsupdate can't do that. The first parameter after "delete" is "name" of the record. You're trying to delete with "data" of the record. You can scan the zone file, find matching records, add them to a file with a format that nsupdate understands and feed it to nsupdate. 

I'm going to venture outside the "standard" reply and guess that what you need may be less a "centralized infrastructure" (single domain) and more of a centralized management tool. You might have a look at something like Italc. $URL$ It provides you a central command/control console, a singular view of your PC fleet. You'll still have to treat each PC individually, but you can do it all from one place both in the geographical and logical sense. And it's free... 

IF the SQL Agent job is actually finishing. (And that's a big 'if') then the last step of the job could use xpcmdshell to fire psservice (from the PSTOOLS suite) and tell the SQL Server service to restart. It's real bad kludge-ey. But it could be done. 

I can't speak to "5-6 TB of data", but I currently have 1700 full time fat-client users (Application built in .NET) hammering against a 1.5 TB database using SQL 64bit Itanium. It performs fine. I think the scaling question is not so much the size of the DB, as it is the number of users and transactions per second. Oracle can do clustering to expand capacity as far as transactions/second (in certain circumstances) but I wouldn't necessarily prefer either one regarding raw DB size. 

You could set up a jet-direct and then packet-sniff the output to the printer. Personally, I'd copy all the fonts (.ttf or whatever) from the working machine to the misbehaving machine. Finally, check on your Wordpad settings. A lot of "general" printing output done in the Windows shell is by default routed through Wordpad. 

But on Play Framework, asking for the client IP (request.remoteAddress) results in a . I'm sure it's because of the proxy, but I don't know which parameter to set to make this works. Thanks for your help ! 

I saw the directive, that is exactly what I want, but do I have to specify for every location {}, the proxy_* configuration ? I have a specific message to display if the user reach the quota allowed, in my app (located at /errors/413). If I go directly using my browser, it works. But using the configuration below, I've got a "504 Gateway Time-out". Why? 

I want to execute a local (or remote (via http)) script when someone hits one of my website with GET request. I saw the mod_actions module for Apache which would be great if it would also work for simple GET requests : 

I know it's a security issue, but just by curiosity (I can't find a proper answer to that!), is it possible? I defined cifs.broadcast=255.255.255.255 in the Alfresco configuration, but whatever I define, I can't access it. I'm trying : 

I recently configured a Debian 8 with Apache 2.4. Since I have a fairly recent version of Apache, I used ProxyPassMatch instead of FastCgiExternalServer. But when configuring my alias for PhpMyAdmin, I wondered if this was secure. Here's my configuration :