The reason the round trip is not occurring is because your subscription from Server B to Server A has its @loopback_detection property set to TRUE, which is the default setting. As we can see from sp_addsubscription, @loopback_detection specifies if the Distribution Agent sends transactions that originated at the Subscriber back to the Subscriber. If set to true, the Distribution Agent does not send transactions that originated at the Subscriber back to the Subscriber. If set to false, the Distribution Agent sends transactions that originated at the Subscriber back to the Subscriber. I'm aware that the documentation states that this property is used with bidirectional transactional replication, however, I have reproduced your scenario and it appears to be being enforced with plain vanilla transactional replication as well. Dropping the subscription and adding it back with @loopback_detection set to FALSE alleviated the problem and allowed for the round trips to occur. Technically your topology can be considered Bidirectional Transactional Replication since both servers are publishing and subscribing to and from each other, even though the articles are different. Keep in mind that you will need to drop and add the subscription back with @loopback_detection set to FALSE since sp_changesubscription does not allow you to change the @loopback_detection property on the fly. I hope this helps. 

While @user22610's answer will work great if your DB2 platform is Linux/Unix/Windows, it will not work on Mainframe DB2 (z/OS). If you're using that platform, this would be your command: 

I think you should be able to use the command to set up your connection in SSMS. Create a shortcut on your desktop (or wherever), and set this as the target location: 

There is a free version of DB2, the Express-C Edition, which includes most of the features of the "real" DB2 servers. It doesn't include some features, like label-based access controls or table partitioning; and it can only use up to 2GB of RAM and 2 processing cores. Have a look at this article to see what you may be missing out on from some of the "upper tier" editions of DB2. 

According to this Microsoft KB article, SQL Server 2005 (in any form, even with all the Service Packs installed) is not supported on Windows 8. 

When you run a program through a DB2 pre-compiler, runs through your program, and if it finds any embedded SQL (in COBOL, these are statement blocks that go from to ), it carefully rips the SQL out, and replaces it with a call to the COBOL-DB2 interface. After this, there are two outputs of the , the COBOL source that has had all the embedded SQL removed ( from now on), and a that contains all the SQL that was removed (). Precompile does do some basic syntax checking, but be aware that the checks are only based on your table declarations within the program. It doesn't attach to DB2 to verify these! These two files are completely separate, and when you run the COBOL program, it has to find an and a that were generated at the same time. At this point, is compiled and linked with the standard COBOL compiler into a and placed in a load library to be used later. However, there is still a lot of work to be done with , the DBRM. This is where comes in. is sort of like a compiler for the embedded SQL code, and the output of the "compile" is a . In order to BIND the SQL into an executable "package", the BIND process attaches to DB2 and does a few things: 

The recommended approach is to have replication agents run under Windows accounts, not the SQL Server Agent service account, and the accounts should be granted only the required permissions. Create a dedicated Windows account for the Log Reader Agent, grant it the appropriate permissions covered in Replication Agent Security Model, and use this account for your Log Reader Agent Security. 

It's hard to say exactly but it looks like something went wrong during the reinitialization process. Transactions are flowing from Publisher to Distributor but the Subscriber is requiring a new snapshot. I'd recommend generating a new snapshot and then it will be reapplied by the Distribution Agent. At that point you should be back in sync. 

You will need to add both the principal server and the mirror server to Replication Monitor. This is covered in Database Mirroring and Replication (SQL Server): 

None of the above. If the 5 new subscriptions are initialized with a snapshot then by default all subscription database objects will be dropped and recreated from the snapshot bcp files. So the data from the week offline will appear to be deleted. 

(Assuming DB2 for Linux/Unix/Windows here, since you don't specify, but this will probably also work on the other platforms.) You don't have to do funky date math, there is a built-in function that will do it for you: 

During the last step, all of your SQL is run through the Optimizer, which takes into account all of the statistics and various paths that the DB2 engine could take to fetch your data. It then chooses the path it came up with that has the lowest cost associated (with newer versions of DB2 [DB2 10 for z/OS], it may decide to take a "higher cost", but "lower risk" path). Once the path is selected, it is compiled and becomes a package, which is stored in the catalog (you can see all of your current packages with (z/OS)). Finally, there is a last piece that allows our programs to reunite with their packages, the . You create a plan by doing another BIND (). A plan is a collection of packages that the program is allowed to look through to find the package that shares the same name. With COBOL, you specify which plan the program should search in in your JCL. 

At the moment, replication is not supported on Linux. However, according to the release notes, replication support will be added in a future release. Release notes for SQL Server 2017 on Linux 

Replication across non-trusted domains or workgroups can be done using Push Subscriptions in conjunction with SQL Authentication for the replication agent process accounts. Alternatively, you can also use Windows Authentication by configuring pass-through authentication. To use pass-through authentication, create a local Windows account on both the Publisher and Subscriber that has the same username and password. Use this account for the replication agent process account and have the connections to the publisher, distributor, and/or subscriber impersonate this account. Ensure the account has the permissions required in Replication Agent Security Model. This approach is covered in the section Use Windows Authentication to Set Up Replication Between Two Computers Running SQL Server in Non-Trusted Domains in HOW TO: Replicate Between Computers Running SQL Server in Non-Trusted Domains or Across the Internet. If you have anymore questions, please let me know. I hope this helps. 

The function will ensure that you'll always have at least 14 characters (it implicitly casts the first argument to , and then pads on the left with the last argument to at least the number of characters specified in the second arg), which the function can then cast to a . This won't work if you're on the Mainframe DB2 (the only type of generated columns DB2 for z/OS supports is identity columns, row change timestamps, or rowids). I'm not sure about DB2 for iSeries. 

In short, compiled code goes through these steps to generate a usable : Precompile -> Creates a DBRM (with C[++], the precompiler outputs the precompiled SQL to an HFS file, which can be sent through the command-line bind program) -> the DBRM is optimized and a set of access paths (a ) is created -> The package is added to a , which is a group of packages that allow you to create a "search path" for your programs to look through. Since these programs are statically bound, if your table statistics change drastically, then the access path the optimizer chose at bind-time might not be the best path anymore, and re-binding will allow it to re-evaluate the SQL and perhaps choose a better path. 

However, the stored procedure sp_startpublication_snapshot is executed at the Publisher on the publication database and is used to start the Snapshot Agent job that generates the initial snapshot for a publication. When you configured replication and created the publication, you specified the accounts in which the Snapshot Agent will run under at the Distributor and the Publisher. This account information is saved. This is covered in Snapshot Agent Security. Even though you execute sp_startpublication_snapshot from the Publisher, it already has all of the necessary information to connect to the Distributor and start the Snapshot Agent. 

There isn't a built-in or easy way to do this with Web Synchronization. You're best bet is to programmatically monitor Merge Agent sessions at the Publisher/Distributor using sp_replmonitorhelpmergesession and sp_replmonitorhelpmergesessiondetail. You can script this out and poll on a schedule. 

If you change a user's permissions to a particular table BEFORE a Subscriber has been initialized or reinitialized, if the article property Copy permissions is set to true, the permissions will be copied to the Subscriber when the snapshot is applied. If you change a user's permissions to a particular table AFTER a Subscriber has been initialized, the permissions will not be replicated on the fly, unless you generate a new snapshot and mark the Subscriber for reinitialization. Note that the article property Copy permissions must be set to true. What is typically done is if you need to change a user's permissions to a particular table AFTER a Subscriber has been initialized, and you need the permissions to be present at the Subscriber without reinitializing the Subscriber, the script to grant permissions are posted to the Subscriber using sp_addscriptexec. I have a post detailing sp_addscriptexec located here at Executing scripts with sp_addscriptexec. 

I see you already have an answer, and I see that you can't change the table definition because of other programs that use the table, but can you add in new columns to the table? Are you also on DB2 for Linux/Unix/Windows (tested on 9.7)? If you can, and you are, then I might suggest you create a to create a column, which you could then index for searching. 

Edit (update for comment): If you are using the command line processor, you can pass in either a single bind package (), or a list of bind filenames (). If you pass in a list, the filename has to be prepended with a (e.g. ). Inside the .lst file, you can either put each package on an individual line, or you can separate them with : 

I see your Info Center link goes to LUW 9.7, and you mention that you've programmed in Java, but most of the experience I have with binding is with DB2 on the Mainframe with COBOL. So, you may need to adapt the explanation a bit (but generally, the concepts should be the same). I believe that binding is only relevant when you are compiling programs that include embedded SQL which is precompiled (statically bound SQL). If, for example, you're using JDBC, you aren't required to run a BIND. The JDBC driver will the statement dynamically. 

Republishing data Serve as alternate synchronization partners (This has been deprecated) Resolving conflicts according to a priority 

Adding tables (or articles) involves adding the article to the publication, generating a new snapshot, and synchronizing the subscription(s) to apply the schema and data for the newly added article. See Add Articles to and Drop Articles from Existing Publications 

There are cases in Merge Replication when parent and child rows can be processed out of order on synchronization resulting in constraint violations and conflicts. To understand how and why this can occur and how it may apply to your specific scenario, I recommend reading through: 

There is no hard limit but you will limited by hardware considering the agents will consume cpu, memory, and i/o. There is also a desktop heap issue that some have run into when running a large number of push subscriptions, ymmv. I personally have 1 topology I administer which currently has approximately 3,000 pull subscribers. 

Will transactions be safely cued on the Subscribers - I imagine this will potentially cause transaction log growth? Transactions will be safely cued on the subscribers and transaction log growth will be negligible. Merge Replication utilizes DML triggers along with change tracking tables in the publication and subscription databases that contains metadata to determine which changes need to be propagated. So you might see some growth in the Merge metadata tables while the publisher is offline. What is a reasonable time period for the Publisher to be offline - do I need to worry about expiry? Subscriptions will expire if the they do not synchronize with the publisher within the publication retention period. The default retention period is 14 days. You can check what your current publication retention period is by executing sp_helpmergepublication at the publisher on the publication database and inspecting retention and retention_period_unit in the result set. Another way to check your publication retention period is to right-click the publication in SSMS -> Properties. On the General page there is a section labelled Subscription expiration. If you restart the publisher and it comes back online within the publication retention period then Merge Replication will pick up where it left off.