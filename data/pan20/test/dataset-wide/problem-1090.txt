Firstly: SSRS PowerBI on premise is not an update to the original SSRS service. Instead it installs its own service. The original still exists. When i installed it i used the existing reportserver db and same URL. In hindsight that wasnt such a great idea. The install changes the db and hogs the URL crippling the original SSRS service. We found the reporting services configuration manager for the new service allows you to create a new db and assign a new url base. To get the original service working again we had to restore our reportserver db. We had to play around with the urls and ports to get them back to normal and it now works again. I would highly reccomend if you are installing PowerBI on premise that you consider giving it its own db and URL. 

Substitute the fields you want to select and filter, and the tablename. I would highly reccomend that you take some time to start reading about sql queries. W3 schools would be a good place to start 

The problem you have is caused by sqlite not enforcing its own field definitions. This means a field defined as varchar(50) might have more than 50 characters in it. Run some select statements over your data to check field lengths. For example: 

It is possible to process individual rows and redirect the records that failed. You could even validate records before you try to insert them. How do you want it to work? Unless they specifically need to be processed together as a batch I would be processing individual records and storing failed records for correction and resubmission. 

No, if you are talking about partitioned tables, sql uses the where clause to filter rows. if your query doesn't have the where clause it will return all rows. if you can't test your changes you probably shouldn't be making them. 

The answer will depend upon what tools you have available. What database system are you trying to get the data into? Oracle? Sql server? Windows machine or unix box? One alternative could be install software on a 3rd machine. (your PC?) Extract the data and convert it into a suitable format to import. Another option could be to export the data to a .csv file and then import that. You could use Excel, MS Access or even sql express. However, this is going to become part of a regular process. I suggest you try again to have the required drivers installed. That would be the cleanest and most straight forward option. Everything extra step you have to take costs time and introduces risk. 

I need to migrate some data from another system, and so I'm trying to insert data into this field/XML. So far my attempts have resulted in singleton nodes which the app doesn't like. Using some examples i found in the web I've tried a the following: 

I have a table with an XML column. I need to insert various nodes and then set those nodes using values from a sub-query. Setup: 

Right click on the database in question, select Properties> Options> set Auto Update Statistics = True. click Ok. 

I would take the lowest level transactions and transfer them to a central location. Load and then batch process. Preferably without triggers. Id have create_dt and change_dt fields which id use to identify deltas that need to be resent. Id then periodically reconcile source and target. Whether you overwrite the changed records or append deltas will depend on what your till captures, your account practices and level of detail required. I.e Cash vs accrual and whether you need to reconstruct how it looked at a point in time. (stationarity of record) I do something similar with my Datawarehouse. We ship low level data using ssis, but also have a step in our etl which performs sums/counts on both systems and stores the result. If it varies we know to reload. 

SQL authentication: means using a user id and password which is not the same as your domain credentials. 

Grant read privileges on only the tables you want them to have access to. Or, Grant them access to all tables and then Deny access on the tables you dont want them to have access to. 

No auto shrink can't permanently damage a database. It may lead to highly fragmented tables and indexes which may hurt performance. The fragmentation could be difficult to remove completely, but minor fragmentation itself shouldn't be a concern. You need to assess whether this is really an issue for you worth spending time and effort to fix. 

I have a an update statement which uses a subquery to filter records. My input table contains dates in a varchar field, and some of them are invalid. 

This has now run successfully for ~1.5 weeks. The more I think about it I believe that the ssis error (posted in title) was a side effect, and was the result of the timeout caused by the dataload connection. I can't prove or disprove if installing SP3 had any effect at all. In my opinion the comment from @TheGameiswar is the best indicator pointing to root cause. 

Yes you can work offline but it will be difficult to develop and test without the input/output connections files/tables your package will use. One option is to install SQL Server on your pc, create a database and copy the source and target tables and some test data to your pc. Then you can develop and test locally. Deployment will be difficult unless you have ssdt on the server. Otherwise you will need access to deploy to the server from another machine. 

I have curves consisting of X/Y points (an array?) stored in a string with the same delimiter. There is 1 curve per row. Below is a simple example: 

As recommended I have run DBCC and no errors were found. I have also restarted SQL. I then restarted the ETL process and got the same error. My searches for this error show that is was known error in SQL 2008, 2012 & 2014 and fixed in subsequent hotfixes & cumulative updates. so I'm a bit surprised to see it reappear in 2016. The links I've found say it affects SSIS when trying to do inserts if database is in Simple or Bulk Logged recovery model. (I'm running in Simple recovery model) A suggested workaround is to change Db recovery model to FULL. I have tried this and it works, but it's not much of a solution for a Data Warehouse. Has anybody else encountered this with 2016? Can anyone suggest alternative workarounds? Updates: 26/7/2016: I applied the Critical Update KB3164398 (v13.0.1708.0) and the problem still exists. 27/7/2016: I have applied Cumulative Update CU1 KB3164674 (v13.0.2149.0). 3/8/2016: Error occurred overnight on our smallest cube. CU1 did not fix the issue. Today I reported the bug on MS Connect And I have also logged a support call with Microsoft. 12/8/2016: MS-Support responded initially, but the responses was "We don't have a fix for that". The Support guy was going to discuss it with his colleagues and get back to me. 8 days later I haven't heard from him. Although I don't have a 'fix' we did find a workaround that suited us. See my posted answer. 29/9/2016. I applied CU2 last week. On Thursay we accidentally ran an old version of the merge which failed again with the same error. So.. CU2 hasnt fixed it either. 23/1/2017: I Applied 2016 SP1 CU1 and I believe this has resolved the issue. Specifically KB3205964 

Short version is yes its possible to do what you are describing. Your report will have a spatial query to display the shapes you want. Whether that's a map or a floor plan. You can then add analytical data in a second query to colour the map. The two queries needs to be joined by a key. Im not familiar with qgis but ideally you'd want both the spatial data and analytics in sql server. The other option is to have 2 layers. The first layer gives the 'map' shape and the 2nd adds points on top. If you search Google images for "SSRS map" you'll see lots of examples. 

Why would it only fail sometimes? Why is it failing now? What is the root cause? How can I fix this? 

Other things to think about: Will this database be used for many different exams? (consider exam id) Or could a student sit the same exam twice? How would you handle that? 

Only you can answer if the the change is worth the effort. Yes restructuring would be marginally more efficient. The main advantage being that new sensors can be added in the future without altering your table structure. The downside is that it's harder to enforce business rules and logic. The advantage of your current design is that if the sensor types are consistent per column you could have different data types, int, float, bit, varchar etc, whereas if you put all of your sensor values in a single column you'll need to use the lower common denominator (most likely a varchar). Given that you're talking about sensors, I expect you will be recording over time, so you'd probably need a timestamp, and decide on what else need to be stored with the value. Quality codes? non-detects? what about the sensor type?