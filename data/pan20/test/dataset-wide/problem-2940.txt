I will make a brief argument against pirating as a form of demo. Business owners make business plans based on the expectation that certain rules will be followed. Prices reflect the amount that consummers are willing to pay for the item, but they also cover things like low manufacturing yields, high risk experimental products, and losses from shoplifters and pirates. In order for a business to be sucessful they must get this balance right. The only reason they offer a product or service is because they are expecting a certain return on their investment. If they thought they could not get enough return they would repurpose that capital into something else. A person could argue that the return on investment is unreasonable and greedy and therefore not unethical to dip a tiny bit out of it, but what is reasonable is relative to the provider. Some are non-profit, some make small margins, and some make hefty margins, but they all are going into it expecting what they are planning to get. When that balance falls away from the expectation they may increase prices or stop offering the product. Demos are available because the business has made a decision that providing the opportunity to test a product will ultimately bring the expected returns. Products that do not offer a demo are restricting that testing because they feel it will not bring the expected returns. That is why one-time-use products do not offer demos (at least not full demos) and products with a long life are more likely to offer demos. Some people will say that they are bringing value to the company if they pirate once and then become a long-time customer (and thus are doing it ethically). That may be true for the individual in some cases, but in other cases they will try and never buy. All of these points have been general enough to be independent of legal issues. If the law were suddenly changed to allow for pirating then companies would adjust their business model based on the expected relationship between company and consumer, and they would continue under the new model. There is an implied understanding between companies and potential consumers (consumers as a group) that both sides only have a relationship because of the expected norms of the environment. Pirating is unethical because it violates that understanding. If that doesn't sway you, you can also consider that as pirating becomes more commonplace companies will compensate by increasing the cost of their products. This is fine for the pirates but unethically harms a large group of other people. 

Logical positivism, later called logical empiricism, was a school of analytic philosophy famously connected with the Vienna circle and with a significant following up until the 1950's. What were the main criticisms that were articulated to refute logical positivism, who articulated them, and why were they so successful in displacing the movement from its previous stature? 

We don't know. There are some very valiant attempts to engage the question here, and many of them even explore concepts well worth exploring. But just because we live in such a complex, information-packed age doesn't mean we need to pretend we know things we don't. The oracle at Delphi said that Socrates was the wisest man in Athens simply because he realised that he knew nothing, and supposedly that one statement is responsible for the existence of Western philosophy in the first place. She had a pretty good point. I've never heard a good reason offered for why there is a universe/multiverse instead of there not being one. I may one day hear such a reason, but I couldn't begin to imagine how it might proceed. Nor can I think of an ultimate reason why "me" began. This is contrast, of course, to the scientific observations and models I can build of how a big bang might have lead to stars, galaxies and planets, a human being might have evolved on one of them, how physics could give rise to consciousness, and so on. But as to why that entire business is busily, well, businessing--a complete mystery. Sometimes the most honest thing you can do is admit that. Is it really so hard to do? 

The fact that we can be mistaken about Qualia can actually be a good argument against the existence of subjective experiences. We can't know, for example, if in two different cases we experience different Qualia or interpret one Quale differently. Some very strong examples are given in Daniel Dennett's "Quining Qualia" . As always with Dennett, it's nicely written, and supplies some strong arguments. 

I can't seem to understand this line of thought. A computer can be programmed to learn a language by use. it can even be hardcoded that "no" means a negative response (hence "normative") and "yes" means a positive response. The computer, a purely causal machine, can then have norms. Why doesn't the argument from normativity accept it? 

It is claimed by a lot of philosophers that because we are normative creatures, it is impossible to explain our minds in purely causal terms. Jerry Fodor writes (in LOT2) 

What are some good resources for a layman to get a basic overview of modern philosophical theories, positions, and open questions in plain language? I'm reminded somewhat of Bryan Magee's videos, only looking for something a little more up to date. 

Although the future-humans-of-considerably-advanced-technology possibility raised by Lennart is indeed formidable, I think there is a fair leeway for a rational person to be convinced that a book having been written by some sort of omniscient/omnipotent entity is the best explanation currently available to them, and on that basis become convinced until further information should present itself. 

As just one concrete counterexample, a person who becomes depressed enough to commit suicide (assuming they don't believe in an afterlife) cannot really be said to be acting out of happiness. Granted, their feelings of depression may be directly connected to their desire to be happy and their sense of failure at not being so. But hopefully this helps to delineate the edge case of how connected feelings no longer serve an original goal. 

This is called a "Straw Man Fallacy" which is a variation of ignoratio elenchi (irrelevant conclusion). The general form of a straw man argument is to misrepresent an argument to make it easier to defeat. Your scenario would be a specific case of this. Stanford Encyclopedia of Philosophy, Fallacies 1.8 

All of the above happen regularly. For example, it is common for a new study to provide a highly qualified statement in statistical terms which is then taken up in the media as a "possible" effect which is then repeated as a cause-and-effect which then goes a million directions using the above fallacies and will live on as a misunderstanding for decades. Then when the next study finds overlapping statistical data, the opposite gets said and confusion begins. There are also the legitimate "reversals" of scientific consensus which is merely an update to our understanding of things. People who don't understand the most basic concepts of science don't understand the concept of updating understanding and can only see things in the black and white dichotomy of true or false (hey, there's another one - false dichotomy). 

Kripke and Quine argue both for 2 different ideas (about which I will write shortly) but their objectives are common - to say something about the nature of sentences. Yet, It seems to me like they don't share the same assumptions. I will try to explain: Kripke In "naming and necessity" Kripke claims that some sentences will remain true in every counterfactual world. "2+2=4", is an accepted example, but he claims that the same is correct for sentences like "water is H2O". I won't get into details, but I will mention that the truth of this conclusion can be only established if we assume the existence of common language in our world and in the counter factual world. If we didn't use the same language in both worlds then "2+2=4" could have had different meanings in each of them (Kripke himself writes it somewhere, if I remember correctly). Therefore we can say that - Kripke's conclusion about the existence of necessary a posteriori sentences is true only under the limitation of a given common and specific language. Quine In "Two Dogmas of Empiricism" Quine claims that analyticity cannot be defined whatsoever. In order to show it he draws some kinds of attempts to reduce analyticity, and shows that they all fail. One of his suggestions is seeing a dictionary definition as an analytic truth. It seems to be possible, but Quine says that the work of the lexicographer who writes the dictionary is an empirical work. When he defines a "bachelor" as an "unmarried man", he doesn't do it on some analytic grounds, but on the grounds of his experience of language use among his co-speakers. The definition, therefore, is a posteriori and hence is not analytic. It seems to me that Quine, here, suggests that while defining analyticity we should not limit ourselves to the scope of a specific language. If we did so, then a dictionary definition could have been satisfied as an analytic truth of an examined language. If I'm not mistaken, then, It seems that both Quine and Kripke try to say something about similar entities (sentences), and have two discussions of the same "depth", but they don't speak from within the same scope, and don't hold the same "axioms" or assumptions when trying to present their arguments. Am I correct about it? If so - How can we define the scope of discussion in philosophy, and how should we relate to Quine's and Kripke's arguments knowing that they don't share common grounds? Is it possible that one argument should assume more than the other, and still both will come to conclusions that are in the same "level" of depth? 

The most approachable resource I've found is: $URL$ It's not 100% rigorously correct in all cases, but it provides a simple place to start learning. For more detail, you can always read through wikipedia which is maintained well and is well referenced for more detail: $URL$ 

Holes (as the word is commonly understood) are almost always filled with something else, so a filled hole can, in at least some cases, still be considered a hole. The most obvious example is a hole dug in the ground. It is filled to the brim with air (which are particles with mass just the same as filling it with sand would be). In many cases, it is also filled with electromagnetic waves of all sorts (light, magnetic field of the earth, UV light, infrared, microwaves, radio waves, etc.) Common knowledge and understanding that not all matter and energy that fills volume can be seen should suffice for documentation. 

For me, if a book described some sort of one-way function built into the physics of our universe, a Grand Unified Theory within which to thoroughly test said function, and a substantial set of predicted output from that function; combined with complete working instructions for a time machine in order to verify that the one-way-function changes its output under time travel; and then after verifying all this personally and traveling forward in time to the end of the universe to see that none of this technology is ever discovered by human beings (presumably the book instructs me to destroy the included knowledge after doing all this); then I would consider it highly rational to believe the book was likely written by some sort of omniscient/omnipotent entity (with respect to my universe at least) until or unless contrary information of some sort became available to me. Under the circumstances, it would be the best explanation currently available to me. 

There are three reasons for the capital punishment: punishment, deterrence, and removal from society. Punishment is questionable. Is it worse to be in prison for 60 years or to die quickly? Is this a martyr case where the offender will actually see death as a reward? For most people, it is a punishment, but it's case-by-case and definitely not certain. Deterrence is commonly cited throughout history and even today. The argument is that loss of life is highly motivating. Without experiencing it, many would think that loss of life is worse than life in prison (maybe those that have experience prison would not agree). If you are in prison, there is always a chance you might be let out, so there is hope in that. Even in a life term case, most societies have pathways for release (executive pardon, for example). Removal from society is really unique for capital punishment. Life in prison requires that the victimized society pay to sustain the murderer, certain people still have to interact with that person, and there is a possibility for release or escape. If he gets out, he may re-offend. If you look at modern society, the cost of keeping someone in prison for 60 years is small relative to all the other things we spend money on, so it doesn't seem like a big negative. However, this question is not relative to a particular culture, so you could imagine a small village that would be greatly burdened by providing room and board to a non-working person. From the perspective of the other people, they want him permanently removed from them and their options are kill him or keep him around, pay for him, and interact with him. So, there's the ethical complication of the survivor's mental and emotional health (which could go either way). Note that there are practical issues to consider outside of the question such as wrongful convictions (OP assumed guilt) and lengthy appeals process that may actually cost more than indefinite incarceration (in some times and places). 

In General, a behaviouristic approach to psychological entities wants to identify a specific mental event with a specific behavioural event. according to this, pain will be fully identified with the behaviour of being in pain: screaming "ouch", for example. Yet, sometimes we do not present the behaviour of pain while being in pain. To avoid this problem, behaviourists would say that pain is not the behaviour that is identified with being in pain, but the behavioural disposition of pain, or the behaviour identified with pain under normal conditions. A behavioural disposition, then, is the disposition to behave in a specific way under normal conditions. The meaning of the symbol "+" will be its use by a person under normal conditions, or in other words the behaviour of addition. 

The multiverse theory explains your question directly. It states that the universe was created when "nothing", which is in an unstable state, sprang into equal amounts of matter and anti-matter, energy and anti-energy. When this happened, the sum of these is still equal to "nothing", so it is allowed by physics (or at least not disallowed by our current understanding of physics). This happens an infinite amount of times and thus there are an infinite number of universes - the multiverse. Stephen Hawking, world renowned theoretical physicist, authored "A Brief History of Time" and other books which explain these concepts in detail in a way that non-physicists can understand (i.e. lots of words, few numbers and equations). 

You have several questions in here, but I'll address only the title question about the relationship between freedom and democracy. The definition of "democracy" is "rule of majority". This means, that in an isolated democracy, there is absolute freedom of majority opinion (outside influences such as other countries with military power would change the totality of this statement). This structure is not tied to any moral standard (although the majority opinion may be guided by moral standards). If the majority decided that speeding should be punished by immediate death without a trial, then that's what would happen. Democracy is not freedom of any one person (anarchy), democracy is freedom of the majority of a group of people. It should be noted that no democracy can exist in a strictly defined form for long - no current governments are strictly democracies. John Adams said, "Democracy... while it lasts is more bloody than either aristocracy or monarchy. Remember, democracy never lasts long. It soon wastes, exhausts, and murders itself. There is never a democracy that did not commit suicide." REF