SQL Server asks for a root snapshot folder path when you Configured Publishing and Distribution. After configuring publishing and distribution, all subsequent publications created will use that snapshot folder but will have their own sub-folder within that folder. If you wish to have your snapshot files for a particular publication be put into a different root snapshot folder, specify this by passing in the path to the @alt_snapshot_folder parameter of sp_addpublication or sp_addmergepublication when you create the publication. Alternatively, this can be done from the Publication Properties dialog after you have created your publication and before generating a snapshot on the Snapshot page, Location of snapshot files section, Put files in the following folder. 

Since you are a programmer, not a DBA, I would recommend the correct programmer's way to do such things. Don't update the database directly - never. Maintain all your schema changes, and base data changes in files which are then pushed with your release. I don't know what your language of choice is, but there are many tools to do this, Ruby has it's own tool - rake - with which you do that. And here are two others: 

A database server running Postgres 9.4 shows no statistics when running - well only the is non-zero. The is null. I haven't been able to find much as to what should be checked in this case, to find out why statistics aren't reported. and are both set to . Per the manual: 

It's hard to say exactly but it looks like something went wrong during the reinitialization process. Transactions are flowing from Publisher to Distributor but the Subscriber is requiring a new snapshot. I'd recommend generating a new snapshot and then it will be reapplied by the Distribution Agent. At that point you should be back in sync. 

There is no need to add an identity column for no reason. My guess is someone is issuing a SET IDENTITY_INSERT ON when there is no identity column in this table. You need to check the code to make sure it is correct. 

This will apply the snapshot locally and will be significantly faster then applying it over the wire. Downtime at the Subscriber will be minimal. 

Every table should have a primary key (I really can't think of a reason not to have one). So having a paymentID column to your payment table is definitely a standard design. 

My first answer is: why is your database even accessible from outside through the Internet? That network traffic really ought to be blocked by router Internet gateway router or firewall. If you really need to allow some connections from the Internet to your database, then limit it to the valid IP address who should be connecting. At this point that's not really a dba question but a network admin question. 

You could write a procedure with the EXECUTE and build the statement in the procedure by looping over the list of schema which match your query from the information_schema tables. Then you can call your procedure from psql or other passing it your criteria such as 'ceu_shard_test_merge_%'. You could have a parameter to do it or just dry run and instead of execute then it could output the statements or something along those lines. 

There is no hard limit but you will limited by hardware considering the agents will consume cpu, memory, and i/o. There is also a desktop heap issue that some have run into when running a large number of push subscriptions, ymmv. I personally have 1 topology I administer which currently has approximately 3,000 pull subscribers. 

Adding tables (or articles) involves adding the article to the publication, generating a new snapshot, and synchronizing the subscription(s) to apply the schema and data for the newly added article. See Add Articles to and Drop Articles from Existing Publications 

The reason the round trip is not occurring is because your subscription from Server B to Server A has its @loopback_detection property set to TRUE, which is the default setting. As we can see from sp_addsubscription, @loopback_detection specifies if the Distribution Agent sends transactions that originated at the Subscriber back to the Subscriber. If set to true, the Distribution Agent does not send transactions that originated at the Subscriber back to the Subscriber. If set to false, the Distribution Agent sends transactions that originated at the Subscriber back to the Subscriber. I'm aware that the documentation states that this property is used with bidirectional transactional replication, however, I have reproduced your scenario and it appears to be being enforced with plain vanilla transactional replication as well. Dropping the subscription and adding it back with @loopback_detection set to FALSE alleviated the problem and allowed for the round trips to occur. Technically your topology can be considered Bidirectional Transactional Replication since both servers are publishing and subscribing to and from each other, even though the articles are different. Keep in mind that you will need to drop and add the subscription back with @loopback_detection set to FALSE since sp_changesubscription does not allow you to change the @loopback_detection property on the fly. I hope this helps. 

The simplest - shell script running psql commands :) Otherwise, any high-level language can be used for the sake of experiment to learn database - ruby, python, java, etc. 

Yes, that would be fine if your design goal is that Persons have only 1 Addresses. With this design, each Persons can have 1 Addresses but two or more Persons can have the same Addresses. It's all a matter of your business needs. If the above is what you are trying to get, then yes, it is correct. However, I think it's most common the other way around where the Addresses would have a foreign key to the Persons because a Persons could have more than one Addresses. As for your constraint to check the postal code - well first off you are missing the space and it's lower case. Whether that will work will depend on which database system you are using. I tested it with PostgreSQL and it does not work. I don't think you can really have such a simple constraint to fully validate a Canadian postal code. For example, there are some letters and some numbers which are never used. I'm a bit rusty on my Canadian postal office codes but I seem to recall that number 5 is never used as it's too similar to S, etc. 

This is a known issue with SQL Server 2012. It used to work in previous versions. The Microsoft Connect item can be found here: $URL$ We first noticed it on the MSDN Forums here: $URL$ Microsoft indicates that the issue was fixed in a service pack but I am unable to locate any release notes stating this. You may want to try applying the latest service pack to the Publisher/Distributor and Subscriber(s) and see if that helps. If not, the current workaround is to post the GRANT statements using a post snapshot script or on-demand using sp_addscriptexec. 

Republishing data Serve as alternate synchronization partners (This has been deprecated) Resolving conflicts according to a priority 

I simply removed the + signs and replaced with ||. On the EXECUTE I removed the format and simply concatenated the strings. 

My input is that it should be within the application database since it is part of the application. I would put them in a different schema however to keep them separate from the business tables but with only a couple tables, the table prefix should be sufficient too. 

I'm trying to install PostgreSQL 9.4.1 on Ubuntu 14.04.2. I'm using the packages from . The package installs but it fails running . If I run manually, I get: 

Code generators are great! Code generators are evil! 10-15 years ago, I would have said that having a code generator for quickly creating the boiler plate code for database driven applications would have been a great gift to mankind. 5-10 years ago, I would have said code generator sucks, they generate too much duplicate code and that having a data-driven user interface (where the fields on the screen, validation, etc is driven by meta-data in a database instead of coding the screens one by one) would have been a great gift to mankind that supplanted the code generators. Today I would say - write each screen individually. Use existing framework that wire fields and model objects and possibly ORM when doing simple CRUD. But do design each screen to the exact purpose of the screen. Application screens that mirror a RDMS table too much is only good for managing lookup tables. Don't annoy the user with geeky interface that are designed against a computer storage model (RDMS)... make a screen that has only what they need. That may mean that it will save to multiple tables, etc. Who cares. The user isn't a database. So my thought? Don't waste your time making a code generator. 

It looks like you have some orphaned replication bits and/or orphaned replication agent jobs. Note that Distribution.dbo.sp_MSremove_published_jobs will not remove your Distribution Agent jobs. You will have to manually delete the orphaned job(s). Locate the job(s) under SQL Server Agent -> Jobs. The Distribution Agent job names will have the format Publisher-PublicationDatabase-Publication-Subscriber-integer. Right-click the job -> Delete. I hope this helps. 

There are cases in Merge Replication when parent and child rows can be processed out of order on synchronization resulting in constraint violations and conflicts. To understand how and why this can occur and how it may apply to your specific scenario, I recommend reading through: 

This is a very good question but at the moment, initializing a subscription from a backup stored in Windows Azure Blob storage is not currently supported. I believe this would be a very cool feature for replication and I'm sure the folks at Microsoft would agree. I suggest proposing this idea on UserVoice. However, since you've already restored the backup, and the schema and data are already in place at the subscriber, try executing sp_addsubscription with the @sync_type of 'replication support only' and see if that meets your needs. More details for initializing a subscription manually can be found in How to: Initialize a Subscription Manually. 

Even more, the returns zeros in all columns and null for - even if I run a manual vacuum on the table. 

You should not have two collections, that will give you headache. Have a user collection which has admin and regular users. You can then query users as a whole or look for a given type only. So if you are looking for user id x you don't have two search two collections. As for the question on duplicate ids with 2 collections, it simply depends how the ids are generated. You say your ORM does it... so you will need to check the doc on how it does it. 

I can't find any details as to why fails. If I manually run as the user, that works, but that does not create the config files. I checked everything I could think of, including on-line and can't find a solution. 

None of the above. If the 5 new subscriptions are initialized with a snapshot then by default all subscription database objects will be dropped and recreated from the snapshot bcp files. So the data from the week offline will appear to be deleted. 

Yes, your publication database is your source database. There are some considerations when backing up the publication database, as well as other replicated databases, such as the distribution and subscription databases. This is covered in Strategies for Backing Up and Restoring Snapshot and Transactional Replication. 

This is a permissions issue, the Distribution Agent process account does not have rights on the distributor. Verify the Distribution Agent process account is a member of the PAL, has read permissions on the snapshot share, and is a member of the db_owner fixed database role in the subscription database.