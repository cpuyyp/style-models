Again, . Secondly, this is interactive: requires you to be there to write the password. It might be more useful to have a script which could run with cron, although this would mean storing the password somewhere. In that case you should definitely create a new user with restricted rights who can dump the database but not change it. Thirdly, experience has taught me to play it safe by adding . I have had difficulty restoring when this option was not used and encoding differences mangled the blobs. 

But that's irrelevant If you look at how the \$B(s, m)\$ are used, you see that it's only in \$A(n, s) = \sum_{m=1}^{s} \binom{n}{m} B(s, m)\$. So \$B\$ sums over all partitions with a given number of parts, and then is summed over all numbers of parts. If you cut out the middle-man you get just $$A(n, s) = \sum_{\lambda\vdash s} \binom{n}{a_1, \ldots, a_k} r(1)^{a_1} \ldots r(k)^{a_k}$$ where the sum is over all partitions of \$s\$, \$\lambda = 1^{a_1} 2^{a_2} \ldots k^{a_k}\$ in the frequency representation. Generating all partitions is (obviously) simpler than generating all partitions with a given number of parts. 

That's an interesting trick, which I'd not come across. I've never really looked into because I favour static type checking, even at the expense of longer code. The static way of doing this in C# would be the visitor design pattern: 

We can refactor the second into something similar to the first: firstly, observe that there's no need to repeat the assignment : 

confused me. I had to read the code to understand the comment, which rather defeats the purpose. Why are you splitting it? 

Good to see that you've thought about corner cases. It's debatable whether is better here or (since you're in ). 

What's the lifecycle of that ? It's , so you should take care of tidying it up yourself, probably with a statement. 

It's difficult to address the question you're most interested in without more comments (or documentation for the library you're using). However, the differences between the two functions do seem very minor (especially once I rename to in the second one). Comments There are three comments in each function: 

And then with respect to performance, SQL Server has pretty good profiling tools, but the obvious bottleneck is the matching: neither table has any useful index. I'm surprised that isn't a unique key, but since it isn't the obvious way to speed things up would be to add an index to the temporary table, either on or on . 

I understand this to be at the level of throwaway code rather than production: if it were production code, I'd expect to see either some code to handle the case or a comment to explain why that is unnecessary. 

has and , which are cleaner than the mucking around with . Appending to an external accumulator is quick and easy, but may give more readable code. If you combine that with passing the minimum next denominator and the remaining fraction (rather than respectively extracting them and recalculating them on each call) you get 

The sanity checks are reasonable, but why throw an exception? IMO the correct thing to do is to return an empty set of solutions. 

Finally, you could consider normalising each cyclic word. Given the possible rotations, choose the lexicographically smallest and insert only that one into the set. Whether this gives an efficiency improvement will depend on the statistics of the strings, but if they're random and of a length comparable to the size of the alphabet then a na√Øve algorithm for finding the smallest will be essentially linear, and you gain two things: firstly, the set becomes smaller (less memory usage, faster lookups); and secondly, you can drop because the number of cyclic words will be just the size of the set. 

I find a confusing name to use here. I also note that this gives a if : if you want to require that callers check this before calling, then you should document it. similarly. 

I find the coding style is somewhat unconventional, which might make maintenance more costly than it needs to be. Particular smells are: 

or ? I'm not a C++ programmer so I don't know whether style guides have a preference, but I'm quite certain that you should be consistent. 

This should surely be present in both methods or in neither? But it's only in the second one. Secondly, 

Finally, a note on magic numbers. If I asked you to modify this to solve 16 x 16 Sudokus, how much would you need to change? How about 12 x 12 Sudokus, with the blocks being 3 x 4? 

Now apply that to your search problem. Consider : is it a permutation of ? Now consider advancing one character through : you remove and add . How can you update the data structures used to test whether it's a permutation of in constant time? Hint: 

The class is a nice optimisation for lookups, but the way it's built is expensive. This is a further argument for an extra phase: if you first accumulate direct counts in a map then the can be accumulated in a single pass with a running sum, rather than updating many entries for each . 

Most of the names follow C# capitalisation convention, but not all. and are both local variables and should start with a lower-case letter. 

I find it quite hard to follow your approach. There's a static field which is updated by a static method whose variable names are virtually meaningless; this update method is called in a loop; and the contents of are used each time round the loop. It would definitely help to have more meaningful names, and might also help if the accumulator (, but should probably be called something like because it seems to contain sums of two abundant numbers) were passed into the update method and back out. You asked about performance. There are a couple of basic observations: 

Despite what I said above about inlining , there is an argument for ditching instead. This is because when is a single bit can be expressed as (assuming I've got the notation correct) . Note that if you do go down this route you have to use longs, because the intermediate value can reach 7987904512. 

Maybe I'm missing something, but where's the conversion to feet? I see a calculation using a speed in miles per hour and an acceleration in metres per second squared. It would be an astonishing coincidence if that gave a distance in feet without an extra conversion factor. 

I think there is a bug in the program in a corner case: if is a one-digit prime, I think the output should be , but I think the code will output . 

The more complex one, but arguably better, would be to inject some CSS into the SVG such that you can just use 

Just a minor note to add to the existing answer: can be constructed quite cleanly from standard Linq methods as 

The first, obvious, issue is that generating the set in memory uses a lot of memory. However, making a lazy version is a bit more advanced, so don't worry about it for now. 1. Code to the interface, not the implementation 

There most certainly are. Step back to a simpler task: given strings and which are guaranteed to be of the same length, is a permutation of ? Write a solution which doesn't calculate any permutations. Hint: 

and (once I'd modified the debug output to be less frequent) the entire program ran to execution in 75 seconds. It's still painfully slow compared to my C# implementation using a handrolled permutation class (1.5 seconds), but it's a massive improvement. Run again under the profiler, it takes 105.676 seconds of which the bulk of the time goes to (57.537s), (37.974s), and (36.705s). So further speedups might be possible by hand-rolling a permutation class, but the real possibilities lie in changing the set representation. Since you're working over a very finite universe, a bit set might be a possibility. Of course, the disadvantage would be a loss of readability. On that subject, I have some suggestions. Readability 

Note: I don't really think is a good name, but it's not an easy class to name so I'm using it as a placeholder instead of . 

The scope of is the loop, so by pushing in the definition you improve readability. (And since it's only used once, and in a simple expression, you could even eliminate it entirely). 

The second is to do the remote hashing remotely. Rather than fetch 500 files (of unknown size) across the network in order to calculate their MD5 sums, use remote PowerShell to calculate their MD5 sums and just return the filenames and the hashes. You might even be able to manage to get the two computers to calculate the hashes in parallel, giving a two-fold speedup. 

As discussed in comments, there are better ways of doing the calculation, which in particular avoid the need for so many special cases. This is important because the failure to consider all of the special cases means that the code you've posted is buggy. Consider the following unit test: 

A suitable name for this method would be . A non-recursive implementation can be written which simply counts from to in base , then converts each digit to the corresponding object. 

That isn't quite a tree, and could be taken further, but backtracking over the shared by and isn't nearly as bad as testing twice (once for and once for ) if someone decides to use as a name. There is an argument for combining the as 

It can be improved by adding a sanity check that isn't too high; and the evaluation of can be removed by adding an extra parameter: 

If you're worried about complexity, you might want to consider whether you can flatten your tree into a canonical (wlog) prefix-order string and use an advanced string matching algorithm. 

The two are pointless. But more importantly, there's no tie-breaking. This absolutely needs a comment explaining why it's not necessary to break ties. 

Having said that, I think the best approach would be to start from scratch. The problem you're trying to solve is indeed, as discussed in comments, finding all exact matchings in an almost complete graph. (Or even a complete graph, if there are no couples). Take a quick sanity check to ask yourself if you really need to find all of them or whether it would suffice to pick one at random. Bear in mind that with \$2n\$ people and no couples the number of exact matchings is \$\frac{(2n)!}{2^n n!}\$. With 18 people that's already 34459425 matchings, so no algorithm which enumerates all of them is going to scale very far. Whether you decide to generate them all or to randomise and generate one, I would also reconsider the graph structure. is essentially a list of the edges in the graph, but since it's almost complete it would be more efficient to represent the graph as a list of vertices (people) and a list of edges which aren't in the graph (couples). If you implement the couples as an associative array then testing whether two people form a couple is very fast. The approach would be something like