We are trying to eliminate a single point of failure for our files without a SAN. We currently have a single server for file serving (locally), web server, and database (in VM). Looking at using DRBD with GFS2 to create a cluster file system and then run as much as possible from that file system, including the Host OS, VM's, and shared files. We would like to have two servers setup in a master/slave setup running from a shared source; but the servers would have slightly different hardware configurations (drive port assignments, actual brands and models of main board, NICs, etc) so the hardware configuration files would be kept local (fstab and mdadm for example). Ideally, one server would act as primary. Package management would happen by the primary server only on the clustered file system because the slave server should not need to if it is using the same data (data from same block device). The master server would also be the single point of administration for both servers simply because they share the data and configuration files, and the second server simply sits as a redundant set of hardaware to run the software that is configured on server 1 and does so via STONITH, Hearbeat, Pacemaker, etc. But as I read, it seems as though most people administer the servers independently of each other, and try to automate using cron jobs and rsync. It also seems that most of the files are fairly static and user modify; that is the number of files/folders that are actively modified by the OS (logs, swapfile, etc) independent of user input and/or dependent on hardware are very small. We would like to keep the files served by the native OS, (not in a VM). So now the questions. 

This assumes the array can be taken offline which is not always the case. In the end though, some have found the same that building a new array from scratch and transferring data back in one fell swoop is easier and faster, than attempting a full rebuild on a large multi TB array. Further, I suspect that reading the data and writing the data off the array sequentially in a degraded state effectively only once would greatly lower the chances of a second drive failure before the data is duplicated compared to a full thrashing rebuild, although the chance is still there. In the end, its all about risk management which varies on the plethora of specific circumstances. In my particular case, I can usually find time within a 24 hour window to restore my array and thus freshly backing up, rebuilding, and restoring from the fresh backup would be best in my case. 

Windows 7 Pro only supports (2) discrete CPU sockets so make sure you have allocated the cores over 2 sockets or less. In my case, I ran quickly through the settings and allocated 4 vCPUs over 4 sockets with 1 core per socket; only 2 of the 4 cores showed up in the VM. When I changed to the cores being in a single socket, all 4 cores were visible and usable in the VM. I know this years later, but I just experienced this and figured I would share. 

If the source array has significant reads/writes (from scheduled backups) during a tape write, throughput to the tape would drop dramatically even if temporarily. So some questions centered around source array/tape write throughput: 

Virtualizing will help with your needs tremndously. We have a small business and virtualizing allows us to consolidate hardware, increase segregation of services to help with security, and helps with uptime because we can migrate VMs between hosts (hypervisors) very easily; something that is very difficult with baremetal. We use dated enterprise hardware in pairs with backup parts kept on site (fans, drives, etc) but have both a primary and secondary host; Our host runs VMs serving DRBD, Apache, MYSQL, Samba, NFS, Reslio Sync, Dropbox, etc. We let our host manage RAID arrays using MDADM. Utilizing DRBD the VMs are kept in sync on a backup server so downtime is almost a non-issue even with a catastrophic hardware failure on the primary host. But being a small business it simplifies hardware management, allows us to run less hardware which has far reaching implications on budget and IT resources, and consolidates the management of our services because it is natural to administer all the VMs from a single console on a workstation; for us through Xen Center as we use XenServer. Further it allows us to segregate things so things like cloud services can be virtually segregated from internal services providing a high degree of security. For example we serve two separate cloud file services in two separate VMs; one for field personel accessible via mobile devices and one for office personel accessible via the internal network. As a note, our backup server (not secondary host) is not virtualized purposefully so we have baremetal access to our files in case of a software or configuration failure with our host. That is if our host corrupts our VMs or data stores somehow we have baremetal access to the files and VMs still. In the end we can provide our company with enterprise grade file, backup, web, cloud, and other services all in house for minimal cost and maximum uptime. It also allows us to expand as we can integrate other services; planned in the near future are VPN services for remote book keepers and Android form services for field personel that needs a windows software intermediary to interface with MySql. Without virutalizing we would need to buy, run, and administer more hardware; virtualizing has eliminated the problem of hardware all together when adding such services and we can simply focus on integration of the software/service which can be daunting enough. 

I know this is old, but extremely relevant as duplicating 100's of GB's to Tb's of uneeded files often is problematic for anyone budget conscious. I have been using Areca Backup. and it seems to keep track of duplicate files well. I recently moved 300GB one day and it only backup up 8GB actual data meaning it did not recopy the files and just referenced them My manual file mirror, versions, and deletions had bloated 350GB (300 noted above) in 3 months where ArecaBackup only bloated 20GB over the same time frame. I appreciate it's ability to backup the files in such a way that you can access the actual file in a directory tree on the backend; that is files can be stored in a directory tree in their original format instead of a proprietary format to fight possibility of corruption of a backup. Although typically you would browse through the GUI. 

Is it bad practice to run a 3 copy MD RAID10 array on two drives with a missing drive in place of a 2 copy MD RAID10 array on two drives to allow for the addition of a third drive in the future? We are migrating servers and I will build a new array on the new server. The plan is to build a 3 copy array with a missing drive and leave it this way possibly for months to allow for the addition of a third drive in the future. Is this equivalent to running a 2 copy array or are there caveats to this? Our current array is a 2 copy MD RAID10 array on two drives. We would like to add another level of redundancy but we do not have the third drive yet or the tray for it and nothing is pressing us make the purchase to implement this immediately. 

I am assuming a sustained drop in throughput to below 10-20MB/s (or less) on the source during a tape write would be a problem? Do I need to have a source guaranteed to have no backups scheduled to it? Essentially 2 arrays minimum; one for backups and one for archives and tape writing? Is there a QOS for drives/arrays that could prioritize the tape writing over all else? LTO-4 tape drives throttle, so is there a common lower throughput limit to maintain for LTO-4 or does it vary widely per drive? Again, documentation mentions max designed speed and "variable speed transfers", but no mention of how variable. Am I missing something in this source-throughput equation, or have unfounded worries? 

In the graphic Xen Server and the VM DRBD/NFS server is hosted on an internal USB flash drive. Hard drives are a local MDRAID10 array. The other VMs would be stored on a storage repo via NFS on a DRBD via a VDI attached as a block device on the MDRAID array. VMS are typically Debian. We DO NOT have a dedicated shared storage device (NAS, SAN, etc.). If performance is not a concern (5-10 low use users for sporadic web access to a company site from the field), will there be stability concerns based on the data path being a storage repository served by another storage repository via NFS on DRBD. TL;DR In lieu of dual SANs with supporting networking, we are trying to use DRBD on two servers with local storage to allow for easy manual failover during an outage on the primary server. During an outage the secondary server would become the primary and the VMs could (theoretically) be instantly be fired up with very little configuration. The servers are same RAM and CPU generations even. Xen seems to have its quirks and I forsee Xen Server having a problem that wipes out the entire server until it gets "fixed" given Xen is running EVERYTHING. I doubt we would have permanent data loss, but storage repositories disappearing happens more often than I would think based on the reading; and unless all your disaster recovery ducks are in a perfect little row, it could take a bit of time to bring things back up correctly if we had to reinstall Xen from scratch while carefully filling in the holes we were missing in our documentation as we went. With DRBD thought, in the event of a problem we could be running on our backup server very quickly with active file and VM mirrors. Then worst case we could easily just start from scratch on the primary server if need be and not have to worry about "fixing" anything. What is not shown are a couple more VMS to serve files, but the data would be housed on another SR, so only the VM data itself which is rather static would be served by the same path depicted in the graphic. 

Temporarily duplicate data from array, tape is cheapest if array is large and HDD space is not available. Remove and replace failed drive. Build new array with new drive from scratch. Reload files to new array from step 1. 

I have not found a way to delete a file from the archive, so if there is a large file I want gone I delete it in the directory structure and if someone tried to recover it for some reason I'm sure it would throw an error when recovering it. Although I only do this for known large temporary or duplicate files so this has not been an issue; namely from users using backed-up locations as a scratch space and the scratch work ends up getting backed up. The biggest danger is if your configuration files get corrupted or go missing you cannot rebuild them; but you still would have your files. Overall if you do not like vendor lock in or proprietary file formats, it is a great solution. I have no affiliation with them, they just provided something that solves a problem for me! 

Is an effectively nested Xen Server storage repository a problem for stability? Consider the graphic for the data path of the web server to a physical drive: (Starting on the right) 

Update: I decided to tax things minimally with a single I/O stream via a 600GB archive job reading from the array at about 30MB/s sustained while a tar was being written to the tape from a 4 drive RAID 6 with consumer SATA. The tape definitely slowed to a crawl via listening to the drive but did NOT seem to run out of data or shoe shine. This tells me to NOT expect things to keep up during a full scheduled backup for our hardware configuration but it can handle a less taxing I/O job wile writing to tape. As of note, the LOT4 tapes must do 56 end-to-end passes so effectively it writes in ~14GB chunks before it stops for some seconds to slow down and then "go" the other direction. I think this helped keep the drive "fed" with data under lower throughput as I have read ahead and async writes set in the stinit.def. Another note is a read of "dd if=/dev/st0 of=/dev/null" only produced a result of 107MB/s. This, I would assume, is the real-world max effective throughput of this the drive and NOT 120 MB/s. The drive is currently on a dedicated SAS PCIe HBA with no other PCIe cards installed In the meantime, I setup a 1TB RAID0 as a Disk2Tape buffer and had to add another disk to server to make this feasible. I would still love to find away to do some sort of QOS for the tape drive and set writing to tape top priority so we can simplify our arrays and reduce parasitic hardaware costs, but in the mean time, I'm not seeing a way to NOT get around having a dedicated disk2tape buffer if I want to ensure continuous writes no matter what scheduled jobs hit the array.