Getting multiValued data from separate fields to maintain position: Since asking this question, I've done some reading about dimensional models. It seems that what I was trying to do is poor design, because it places too many expectations on the application, to much complexity in the warehouse, or both. When trying to preserve the relationships between two field values on a single record, it's better to store them separately as well as together. Here's a comparison of my former input to the new input: Former CSV input: 

A hobby web project of mine uses data from many sources, some flat, some highly normalized. My ETL process boils all sources down to one logical table (which is soon to include some multi-valued fields). The final table is loaded into a few separate data presentation environments (Solr or MySQL based). My Solr environment demands a denormalized model (it just loads the transformed output table directly as a flat file), and my MySQL environments can each use a highly normalized model (loading involves some normalization). Both presentation environment load processes are very simple right now. To facilitate the use of flat files throughout my ETL process (important because I use a variety of tools to summarize and cleanse the data, and make heavy use of pipelines for performance reasons), I've been able to reduce multi-valued fields from my sources to a single field using grouping or best candidate selection. Now I'm designing for the multi-valued fields I need to preserve in my output. The MySQL load process is easy because I've already normalized it with these fields in mind. But I'm having trouble with the Solr part: 

Here is why s are being used as predicates. The column is formed by concatenating: During testing of these, a simple from the view returns ~309 rows, and takes 900-1400ms to execute. If I dump the strings into another table and slap an index on it, the same select returns in 20-75ms. So, long story short (and I hope you appreciated some of this sillyness) I want to be a good Samaritan and re-design and re-write this for the 99% of clients running this product who do not use any localization at all--end users are expected to use the locale even when English is a 2nd/3rd language. Since this is an unofficial hack, I am thinking of the following: 

I have a client agent application that stores a replica of user metadata for the entire organization in a or earlier database. It gets accessed fairly often based on user activity so low latency is appreciated by everyone. Normally this is all well and fine, but some organizations have to store multiple orders of magnitude more metadata, which would total around 10-300 million rows, instead of 1 million or less that a typical agent uses. The Jet DB runs on each client workstation, and must operate locally on aging laptops without consistent network access. Full user data syncs get pushed to it every 24 hours or the next time it can reach the servers upstream at corp, and metadata stored while offline gets replicated back to corp whenever convenient. I have a copy of the schema that works for the server side agent which should have not changed since it was first implemented back in the early 2000's, should have very similar data access patterns, and runs on SQL Server 2008 R2 and up. The server agent does a substantial amount of reads and writes in a RBAR fashion, so I am going to assume the same access patterns are used in both. If I profile the activity for a few hours on the server agent to generate some queries to replay activity to benchmark, how can I run it against a Jet database? Are there testing or development utilities that can create a new Jet database and populate it with a schema and data? I am wondering if there would be any benefit in pushing for the development team to upgrade the client database engine to one of those new SQL Server engines that get loaded up only when the application code needs it. Not sure what the best options are out there, or if there is either a useful benefit either in performance or memory consumption. The client agent code runs in an old version of Java, .Net, and C++, but I have not been able to determine which language the data access layer is written in. 

but I think I'm barking up entirely the wrong tree. Also, the performance is horrendous (not a deal breaker, since it will be automated, but still something to be mindful of). Question: How would you break this up by a foreign key? Some sort of pivot? The number of diagnoses is quite variable... Ideally, it would be something I can easily smush into a larger query, but there's obviously ways to work around that if needed. Here's my current solution, but I'm pretty sure it's not a good one: 

You'll get that error when one of the columns in the select won't fit into the destination table. Tracking it down can take some time, but it's just like kindergaten - try and fit the shapes in the holes. If you compare #Variable_info to #stored_proc_info, you will probably find that one of the columns in #variable_info is larger than its corresponding column in #stored_proc_info. I can't find #variable_info in my version of the sp_blitzcache, so I can't tell which columns don't match. Often, I track it down by running a query like this: 

Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

This seems pretty rotten to me, but I only have a few years experience with TSQL. It gets better, too! It appears the developer who decided that this was a great idea, did all this so that the few hundred strings that are stored can have a translation based on a string returned from a that is schema-specific. Here's one of the views in the stack, but they are all equally bad: 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

Step 2 will be the .dtsx package. Change the step type to be SQL Server Integration Services Package, and at the bottom, specify the path for the package, which we saved at 

Here's how you can join a table to itself randomly and update a column. This method will also maintain the distribution of your data: 

I've got a query that checks for new patients in the ER, and if they have been there before in the last 72 hours, it notifies the killbo... uhhh... care managers. I could set this up as a trigger, but I don't think this is a good idea. What I want to do instead is set it up as a job that runs every 5 minutes, with this in the where clause: 

It turns a list like this: Albuterol Sulfate Amlodipine Besylate Aspirin Benztropine Mesylate Bisacodyl Ciprofloxacin Collagenase Divalproex Sodium ... into a string like this: Divalproex Sodium, Collagenase, Ciprofloxacin, Bisacodyl, Benztropine Mesylate, Aspirin, Amlodipine Besylate, Albuterol Sulfate I can write it as a dynamic stored procedure, but you can't call sp_executesql from a function (or at least, I don't know how). Question How would you write this function in a way that it could be used on any table? 

Nested views are non-deterministic, so we cannot index them Each view references multiple s to build the strings Each UDF contains nested s to get the ISO codes for localized languages Views in the stack are using additional string builders returned from s as predicates Each view stack is treated as a table, meaning that there are / / triggers on each to write to the underlying tables These triggers on the views use that stored procedures which reference more of these string building s. 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

How often do I need to run it? It needs to run at the same interval that I keep my backups, right? If I keep the backups for 1 day, I need to run it every day? Can I restore the databases to my test environment and checkDB there? If I've got lots of sql servers, but they all use the same SAN, am I going to want to stagger my CheckDB's? 

I'm trying to break all the rules of databasing using the stuff function. I want to smush every applicable row into just one, for science, you know? Problem is, I have to write a new function every time. They look like this: 

One thing to experiment with is changing your clustered indexes - in a lot of ways they aren't really indexes at all, they are the order in which the data is stored. make copies of your current clustered indexes first, then try adding the columns in the joins & where clause to the clustered index. Clustered indexes are often the same as primary keys, but they don't have to be. your primary key and clustered index can be completely different. Another thing - the - do you need every column from that table? If you only take the columns you need, it might help. Then, there must be a way to stop a query if a website user bails... Other things you can do - archive some data. I'll bet some of your tables are pretty big. Do you need all the data there? With Enterprise edition, you can also partition tables. Check for locking - if you have two users putting this query to your DB at the same time, are they blocking each other? I'm going to catch some shit for suggesting this, but you could join with (NOLOCK) and see if that helps. Oh, and when did you last update your statistics? The query tuning videos on Brent's site are really good, if you need more detail. 

While profiling a database I came across a view that is referencing some non-deterministic functions that get accessed 1000-2500 times per minute for each connection in this application's pool. A simple from the view yields the following execution plan: 

Create a new String table populated with a cleanly joined set of data from the original base tables Index the table. Create a replacement set of top-level views in the stack that include and columns for the and columns. Modify a handful of s that reference these views to avoid type conversions in some join predicates (our largest audit table is 500-2,000M rows and stores an in a column which is used to join against the column ().) Schemabind the views Add a few indexes to the views Rebuild the triggers on the views using set logic instead of cursors 

That seems like a complex plan for a view that has less than a thousand rows that may see a row or two change every few months. But it gets worse with the following other observances: