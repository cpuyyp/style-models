The recommended approach is to have replication agents run under Windows accounts, not the SQL Server Agent service account, and the accounts should be granted only the required permissions. Create a dedicated Windows account for the Log Reader Agent, grant it the appropriate permissions covered in Replication Agent Security Model, and use this account for your Log Reader Agent Security. 

Since your Subscriber is within two versions of your Publisher version, you are good here. However, this configuration is not supported if you are using Merge Replication since your Subscriber will be a later version than the Publisher: 

At the moment, replication is not supported on Linux. However, according to the release notes, replication support will be added in a future release. Release notes for SQL Server 2017 on Linux 

There are cases in Merge Replication when parent and child rows can be processed out of order on synchronization resulting in constraint violations and conflicts. To understand how and why this can occur and how it may apply to your specific scenario, I recommend reading through: 

This is a permissions issue, the Distribution Agent process account does not have rights on the distributor. Verify the Distribution Agent process account is a member of the PAL, has read permissions on the snapshot share, and is a member of the db_owner fixed database role in the subscription database. 

Secondary indexes (non-primary keys) in MongoDB and MySQL are very similar. Secondary indexes declare fields or columns to be sorted separate from the rest of the data, and use row identifiers to reference the rest of the row for a query. 

If you use , MySQL will use timezone which is configured in . If no value is set in , then MySQL will determine your operating systems' timezone and use that upon startup. You can see how your server is configured with this query: 

I don't see why would't you just store the coordinates for every user. You can keep them either in the same table as users or in some kind of user details table, depending on how your current schema is designed. There is no point in having all geo to postcode locations mapped. To get the locations you can use something like google maps API. 

MySQL queries are not case-sensitive by default. It is possible that you have created case sensitive tables when importing data. Check if you have collation, that makes it case-sensitive. Reimport your data then using . Also, if you have collation, it will make queries case sensitive. Your collations changed when re-importing data. 

You have probably read that you are able to deliver the initial snapshot via FTP. This is covered in Deliver a Snapshot through FTP. It is possible to replicate two SQL Servers across a VPN. This is covered in Publish Data over the Internet Using VPN. Once the VPN is setup, you should be able to connect and authenticate as though the servers are on a LAN or WAN. If you have anymore questions please let me know. I hope this helps. 

There is no hard limit but you will limited by hardware considering the agents will consume cpu, memory, and i/o. There is also a desktop heap issue that some have run into when running a large number of push subscriptions, ymmv. I personally have 1 topology I administer which currently has approximately 3,000 pull subscribers. 

It's hard to say exactly but it looks like something went wrong during the reinitialization process. Transactions are flowing from Publisher to Distributor but the Subscriber is requiring a new snapshot. I'd recommend generating a new snapshot and then it will be reapplied by the Distribution Agent. At that point you should be back in sync. 

MySQL is not cutting it at 233. The problem is likely in your save method which cuts it to 233 before the data even reaches MySQL. Also, don't forget that 233 limit is not character limit, and as some character might need more han 1 byte to be stored, you might see less than 233 characters. Also please make sure that data in MySQL server is really stored as latin1, this can be accomplished with: 

While you seem to have fixed the issue, I will quickly explain why it happened in case anyone finding this will want to understand where the problem was. When setting foreign keys, the Primary Keys Columns must be of exact same type and attributes. E.g. If you have unsigned attribute on one primary key, you must have it on another. If you have INT data type on one column, then another column must also be INT (NOT TINYINT, MEDIUMINT etc.). As you have only one ID set to unsigned, I would go and set it to all IDs. As it is usually good idea to have unsigned attribute on primary keys (if you do not use negative IDs), I would have changed all IDs to have unsigned attribute, as it will improve your query performance. Also, take a look at what values you can get with various integers (when they are unsigned). WHat you set your lenght to - does not matter: 

SQL Server asks for a root snapshot folder path when you Configured Publishing and Distribution. After configuring publishing and distribution, all subsequent publications created will use that snapshot folder but will have their own sub-folder within that folder. If you wish to have your snapshot files for a particular publication be put into a different root snapshot folder, specify this by passing in the path to the @alt_snapshot_folder parameter of sp_addpublication or sp_addmergepublication when you create the publication. Alternatively, this can be done from the Publication Properties dialog after you have created your publication and before generating a snapshot on the Snapshot page, Location of snapshot files section, Put files in the following folder. 

Republishing data Serve as alternate synchronization partners (This has been deprecated) Resolving conflicts according to a priority 

Adding tables (or articles) involves adding the article to the publication, generating a new snapshot, and synchronizing the subscription(s) to apply the schema and data for the newly added article. See Add Articles to and Drop Articles from Existing Publications 

There isn't a built-in or easy way to do this with Web Synchronization. You're best bet is to programmatically monitor Merge Agent sessions at the Publisher/Distributor using sp_replmonitorhelpmergesession and sp_replmonitorhelpmergesessiondetail. You can script this out and poll on a schedule. 

The answer relies on what kind of data you store and how. I would never suggest making separate database. Depending on the data you can either: 

Yes this is normal. When RAM is no longer needed it is not freed at the same time. It is kept as cached in case the server would decide that it needs to access it again. This would save you extra time that you would otherwise need for data to appear in RAM. Cached memory is freed only when new applications request more RAM. 

After starting query log, investigate the file (or table) for further information. MySQL Query Log Documentation: $URL$ 

For inserts, you can use . This lets you update certain fields if primary key is already used. The syntax would be something like: 

Every process under linux runs under specific user privileges. Services (like MySQL) usually need to open ports and access various system resources during startup, so they are required to be started as user. However, it is not safe to have all the processes run under as it is not required for continuous operation of services, thus it is recommended to create a special user, which will be used to run MySQL service. MySQL will only be able to access what special user can, and this is going to be limited to MySQL files on the system. This is usual practice in linux. If you, however, use you distributions built-in package manager to install MySQL, this will be done for you automatically (in most distributions at least). 

Yes, your publication database is your source database. There are some considerations when backing up the publication database, as well as other replicated databases, such as the distribution and subscription databases. This is covered in Strategies for Backing Up and Restoring Snapshot and Transactional Replication. 

There is no need to add an identity column for no reason. My guess is someone is issuing a SET IDENTITY_INSERT ON when there is no identity column in this table. You need to check the code to make sure it is correct. 

Yes - With Merge we can copy the snapshot files to the Subscriber and apply the snapshot locally using the Merge Agent -AltSnapshotFolder parameter. 

Will transactions be safely cued on the Subscribers - I imagine this will potentially cause transaction log growth? Transactions will be safely cued on the subscribers and transaction log growth will be negligible. Merge Replication utilizes DML triggers along with change tracking tables in the publication and subscription databases that contains metadata to determine which changes need to be propagated. So you might see some growth in the Merge metadata tables while the publisher is offline. What is a reasonable time period for the Publisher to be offline - do I need to worry about expiry? Subscriptions will expire if the they do not synchronize with the publisher within the publication retention period. The default retention period is 14 days. You can check what your current publication retention period is by executing sp_helpmergepublication at the publisher on the publication database and inspecting retention and retention_period_unit in the result set. Another way to check your publication retention period is to right-click the publication in SSMS -> Properties. On the General page there is a section labelled Subscription expiration. If you restart the publisher and it comes back online within the publication retention period then Merge Replication will pick up where it left off. 

What I would do, is compare both files with tool. can come in help here as it has ability to diff word by word and if you pass it option don't even have to place them in git repository. This command can help 

I don't think there will be a noticeable difference. Query optimization has very little to do with the syntax of your query and a lot to do with the RDBMS query optimizer. The optimizer pulls apart your queries and optimizes it as it sees fit. 

This is likely because your User has status on your system. Users without administrator access should not be able to access these files. If you enable Guest User, you can login and try same thing with Guest. You should not get access to the files as guest or normal user without administrator rights. 

I would use for this. - $URL$ - The script imports only a small part of the huge dump and restarts itself. The next session starts where the last was stopped. 

Which way to go depends on your data, the way you plan to perform queries etc. and is too broad question to answer. You will likely have to apply different method for different tables. 

This will apply the snapshot locally and will be significantly faster then applying it over the wire. Downtime at the Subscriber will be minimal. 

If you are using Transactional Replication then yes, this configuration is supported. This is covered in Using Multiple Versions of SQL Server in a Replication Topology: 

The reason the round trip is not occurring is because your subscription from Server B to Server A has its @loopback_detection property set to TRUE, which is the default setting. As we can see from sp_addsubscription, @loopback_detection specifies if the Distribution Agent sends transactions that originated at the Subscriber back to the Subscriber. If set to true, the Distribution Agent does not send transactions that originated at the Subscriber back to the Subscriber. If set to false, the Distribution Agent sends transactions that originated at the Subscriber back to the Subscriber. I'm aware that the documentation states that this property is used with bidirectional transactional replication, however, I have reproduced your scenario and it appears to be being enforced with plain vanilla transactional replication as well. Dropping the subscription and adding it back with @loopback_detection set to FALSE alleviated the problem and allowed for the round trips to occur. Technically your topology can be considered Bidirectional Transactional Replication since both servers are publishing and subscribing to and from each other, even though the articles are different. Keep in mind that you will need to drop and add the subscription back with @loopback_detection set to FALSE since sp_changesubscription does not allow you to change the @loopback_detection property on the fly. I hope this helps.