I'd suggest the first thing to determine is whether or not the server workload actually struggles during those spikes. Do you see for example IO being maxed out, or other queries being blocked (due to the splits increasing the time the write transactions take to complete) or slowed? If not, the page splits are not an immediate concern, but still may be worth looking into. The question is, are these "good" or "bad" page splits. This link might help you determine what you're seeing. Logging your index fragmentation levels before and after a spike might also be a simpler way. "Good" page splits are simply inserts to the end of an increasing index (clustered or otherwise)that require a new blank page, so it's really not a page split as generally thought of, even though SQL Server counts it as such--presumably because there is some overhead, but probably not more than the inserts cost in general. "Bad" page splits are updates or inserts to the middle of an index that overflow the page, and are the ones that cause both internal and external fragmentation, with external not much of an an issue with SSDs and/or shared storage, and internal being of more potential impact due to the IO and cache memory they waste. It could be that you've got a mix of good and bad, perhaps good into the clustered index and bad in multiple non-clustered indexes. That's pretty much unavoidable, and you'll just need to consider your index maintenance and possibly a specific fill factor on indexes that are frequently affected. But read $URL$ first. However if you find your clustered index is being bad-splitted, then it may be worth considering whether a clustered index that better supports your inserts would be in order. Or if the splits are caused by updates adding more data during the life of a record, a specific fill factor might be in order, but really only if the updates are evenly distributed throughout all your data, since a fill factor to support only your recent data would waste a lot of space/IO/cache if most of your data is static over time. The ideal clustering config really depends how your table is used overall though, not just on how it's written to. 

Sounds like your terminal's pager is taking over the output: you can tell psql not to send its output to the pager when you're using the invocation via: 

If you'd like a snapshot of your primary database refreshed nightly, you could do this with a cron job restoring RDS snapshots every night. I don't think RDS has a button to do this automatically for you, but it shouldn't be too hard to script up a nightly create-db-snapshot + restore-db-instance-from-db-snapshot using the AWS CLI, or boto, or whatever interface to AWS you like. You could even maintain a Route53 entry which would always point to the most-recent instance, and leave the old instances lingering for a day or so before being killed off, so that sessions running against existing instances overnight wouldn't be interrupted. 

Postgres can actually (in the following contrived case) use an index to satisfy queries without adding range scan kludges like the suggested . See the comments on Craig's questions for why Postgres is choosing this index in this particular case, and the note about using partial indexes. 

I'd be interested to hear what privileges your existing superuser role had which were not automatically granted to whatever new superuser role you created by default. Edit: and if you're interested in copying over the per-role configuration parameters (i.e. those documented under configuration parameters), then you could use a function like this (demo only, you may need extra error handling, security considerations, etc. for production use): 

Since both servers show similar reads and similar CPU "seconds", but the new server example shows that the duration was the same as the overall CPU time, the spikes you're seeing are likely due to less-parallel execution plans sometimes being generated--perhaps simply due to the CPUs being unusually busy at those times. Note that the CPU "time" shown is the total CPU milliseconds used over possibly multiple cores--which would have to be the case in your old server example where the CPU time was 6 sec but the actual duration was 1.5 sec. 

Read-only users being able to hold locks makes complete sense if you consider SERIALIZABLE and REPEATABLE READ isolation. And for cases like this where there's no logical sense in the lock, the designers probably decided it was better to have less complexity and consistent behaviour than to consider permissions in the locking code. 

Scott's answer made me realise you might have enough space for this: Create a datetime2 column with a temporary name, and transfer the original column contents to it in batches (to prevent your log running out of space -- and I'm assuming your database is in Simple recovery model). Then drop the original column, and rename the new column to the old column name. 

I believe type = 4 only applies to pre-2008 fulltext files which have been upgraded, because since then there has been no way to create separate fulltext files -- only separate filegroups. (See type = 4 doc for SQL 2012 at $URL$ which confusingly says the same as for 2016 except version number.) Something like $URL$ may technically do what you are looking for, but won't actually be relevant for your report. 

The comments so far are roughly correct, but to give an authoritative answer from looking at src/backend/tablecmds.c: If you're only performing , then will be invoked to handle the , and it uses to perform a WAL-logged block-by-block copy of the table. However, if you were to specify additional actions to the command which require table rewriting, then the new copy of the table should be built (and compacted) in the new tablespace via . 

In your example, the you have set of 256MB should be seen by the subsequent command, because you have changed this GUC inside your session. In fact, the docs suggest bumping up (as you showed) for just such a purpose. 

Erwin's answer does a good job of answering the question as originally stated, however you added the comment: 

The output of that command will give you a decimal count of what number WAL file you are on. Run that command again in, say, 10 minutes. Subtract the first number from the second, and that gives you how many 16 MiB WAL segments would need to be synchronized between the primary and the standby in that period of time. That will answer your question about: 

and raise an alarm if that max. age is over 1 Billion or so. And if it gets close to 2 Billion you are in imminent danger! You can use a tool like check_postgres.pl which will handle this check for you with some configurable thresholds. 

You can create a user that has certain non-db_owner fixed database roles assigned, and then you can create a user-defined role which contains additional permissions. (See $URL$ So in effect, you can create a user that has all the rights of db_owner except say, managing users and dropping database. But I don't think there's a way to directly deny such privileges, you have to instead add everything but them. Your database creation SP can configure new databases in this way, and youâ€™d have to do it for each existing database. 

Since the three operations ended within 3 miliseconds of each other, it looks to me like the first two simply were able to wrap up and report their completion just that little bit faster than the third. (Note the End_Time is the end time of the operation, not the time the operation was logged, which would be slightly later.) Note row E is the only stored proc call. It's possible there's slightly more overhead in wrapping up such a call. 

$URL$ shows SQL 2016 Dev as supported on Win10 Home, and I don't know of any explicit feature limitations--though you are right in general that you're limited to what the OS supports. Have you tried to set up replication using the built-in service logins? I believe the instructions around adding Windows logins are for security best practice, which may not be an issue for a dev environment. 

Not sure why Teradata has that limitation, but should be fine in PostgreSQL even when other tables have foreign keys depending on that index. PostgreSQL has fairly sophisticated tracking of such dependencies -- for example, if you tried to do you would see a complaint like: 

You are presumably looking at the raw return of , which is not a 32-bit value like the XID Limits you are looking at. Instead, it is 

Supposedly it is possible to hook up Bucardo to RDS now that RDS Postgres supports the session replication role, but if you want a nightly snapshot I think you'll be much better off using RDS instance snapshots. 

What happens when a hits is that dirty buffers held in are guaranteed to be written (i.e. fsync'ed) to disk. The WAL files must have already been fsync'ed to disk at COMMIT time, assuming synchronous_commit=on and fsync=on, etc. Your question of: 

And if you don't have to write that as a separate CTE, you could of course just LEFT JOIN directly against instead of . 

So canceling that , either through or a Ctrl-C issued from the controlling psql prompt, will have a similar effect as if you had done 

is not going to see the not-yet-inserted row, which is why it returns true on this second INSERT in your example: 

Assuming that: - You have the looping and sleep code in the CLR code - You know how to call said code from SQL agent (a stored procedure wrapper if nothing else) - Your process is ok with the occasional sub-minute downtime - Your process will not lose any data if it fails part way through (i.e. the next run will process the data the failed run was trying to process) then I would propose a SQL agent job that is sceduled for every minute, and the job step itself have say, 10 retries set (advanced page) so that it will usually restart itself immediately if it fails. Why just 10? Because if it starts consistently failing, you will want to get notification quickly (via agent job failure notification settings). That will be an email a minute. Of course, if it fails 10 times sporadically, you'll get a false alarm notification, but you can quickly see from the job history whether it's failing consistently or just sporadically. 

Breaking up the data file as described would almost certainly have no performance benefit. The practice of splitting clustered and non-clustered indexes may benefit if you put them on separate mechanical arrays, but not on a single array, SSD or otherwise. Although, if you were maxing out throughput on your one SSD drive/array, splitting out to two SSD drives/arrays could theoretically benefit depending on controllers etc. Your server is much more likely to benefit from getting Tempdb and its log on to SSD. And if/when your workload is write-heavy, having the data logs on SSD would also likely give a substantial boost. And if you've been surviving with the current config, then I think you can be certain you won't see a downside to having all your database and log files on the same SSD array (though perhaps partitioned for space management). 

It's not clear why monitoring pg_database_size(your_database_name) doesn't give you the information you are after. Plot the size of all your databases daily, fit a curve to the plot, and you should be able to come up with an estimate of when you need to buy more disk space. What else do you need to know, and why? For this part of your question: 

has some logic by which it determines whether it is safe to keep and reuse the password you entered initially, see the logic about in command.c. Presumably when you are able to use the meta-command to reconnect from within an existing session, is preserving your initial password and reusing it -- either that, or your pg_hba.conf rules allow a connection without a password with the given user/database, e.g. a trust or ident rule. 

H/T to Erwin for this tip. Remember that these XID values wrap around at 2^32. For what it's worth, I think that blog post you linked to is excessively complicating this topic. To watch out for XID wraparound, you really just need to check: 

I am wondering if anyone knows the history of why is the default transaction isolation level for PostgreSQL, SQL Server, Oracle, Vertica, DB2, Informix, and Sybase. MySQL uses default REPEATABLE READ, at least with InnoDB, as do SQLite and NuoDB (they call it "Consistent Read"). Again, I am not asking for what the differences are between different isolation levels, but rather for some explanation of why the default was chosen to be in so many SQL databases. My wild guesses are: small performance benefit, ease of implementation, some recommendation in the SQL standard itself, and/or "that's the way it's always been". The obvious downside of this choice is that tends to be quite counterintuitive for developers and can lead to subtle bugs. 

Note that "3MillValue" in two places would need to be hard-coded with the value that returns that amount, and that OPTIMIZE FOR is the key to this technique. 

Here's a start from $URL$ There are plenty of ways of monitoring DDL changes. You can set up DDL triggers to output changes to some set of tables. You could set up a task to monitor the default trace and try to capture changes into a table again. You could use Profiler to set up a server-side trace and capture events that show structural changes which output to a file. You might, if youâ€™re feeling terribly smart and brave, use Extended Events. All of these are â€˜intrusiveâ€™ in that they require actions that make changes on the server and possibly impact performance. We can discuss details of any that might be an option in your scenario. By the way, DDL triggers can actually capture the client IP using this: 

As far as SQL Server config: Since you're not using SQL Server clustering with shared storage, I believe you don't need to do anything but configure all your instances with Tempdb on T drive (or whatever standard path you choose). See Example A at $URL$ As for how to ensure that T drive is available as local storage in all your VMs for when SQL Server starts up, that someone else will have to help with (perhaps stackoverflow with virtualisation tags). And be sure to test your whole failover process, don't take anyone's word for it.