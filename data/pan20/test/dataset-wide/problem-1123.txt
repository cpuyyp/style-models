No, that's pretty much what they're doing. Now, if there is not a leading wildcard and the field is indexed, which is the usual situation, the database engine can apply the regular expression to the index. So, for example, if you write 

Our estimate of the average row length was spot on (note that, in reality, you won't be nearly this close-- your estimate of variable column sizes will not be nearly so accurate) 

Assuming that we are talking about jobs (rather than, say, jobs scheduled using the package), you can use the package to generate the DDL. Specifically, the function. Something like 

would work assuming all your data is perfect and that all your session are configured to use English month names. I would strongly recommend, however, that you fix the data model. Storing strings that represent dates rather than actual dates is a terrible practice. Inevitably, you end up with at least a handful of columns where the data is invalid which would cause this query to fail. You could work around that by creating a function that attempts to convert the string to a date and catches the exception if there is an error and then use that function in your query, i.e. 

will give you the tables in the order they should be loaded (assuming there are no cycles in the data). If you want to load in parallel, all tables with the same can be loaded simultaneously. 

If the definition of will potentially be different in different environments (which is, of course, a separate issue that needs addressing), your options would generally be to either create an object type that does have a defined order and into that or to define a different collection for each column that you specify as the targets of your . For the object approach, you'd define an object 

Since you're using a standard Oracle-to-Oracle database link, Oracle automatically does two-phase commits. If you commit (or rollback) on one system, any changes made as part of the same transaction on the other system will committed (or rolled back) as well. If there is an open transaction on the remote system that has row-level locks, that strongly implies that the local session is still there as well. So you probably just have to do a (or a ) in the local session to remove the locks. If the ETL process is really stuck waiting on locks that are held by the other session, you should be able to query on the remote database and see that the session that the ETL application is running is waiting on row-level lock contention. Specifically, you would expect to see an of in . You would also expect that if you queried that you would see the session ID of the session that connected from the local database over the database link and that if you queried that you would see that the ETL process's session was blocked by that session. 

Generally, you should never use in a PL/SQL block-- just execute the procedure. I assume that is some ancient leftover syntactic remnant from some ancient version of Oracle. It allows you to replace the entire PL/SQL block that is the trigger body with a single statement, i.e. 

If you do that, however, then you would have to fully qualify table names if they exist in the Alice schema (unless, of course, you change the again). 

When you create the clustered index on , SQL Server has to materialize the data in the index. Normally, a view does not have a separate copy of the data. It is normally just a stored query. An indexed view changes that. Since the indexed view has a separate copy of the data, it can be sorted in whatever way you'd like. Normally, of course, you wouldn't have an indexed view that simply replicates all the data in a table. You'd normally only create an indexed view if you're going to materialize a subset of data based on various conditions or if you're going to materialize some aggregate data. 

I'm not sure that there is a problem. A session in has a status of "INACTIVE" if it is not executing a SQL statement at that particular instant in time. If a user opens a dedicated session in a client-server application and is using the application intensively, it would not be at all unusual for the session to be "INACTIVE" 99% of the time because the vast majority of the time is spent waiting for the human to read and process the data on the screen. The fact that a session is "INACTIVE" doesn't imply that it wasn't "ACTIVE" a few seconds ago or that it won't be "ACTIVE" again a few seconds from now. The column in shows you the number of seconds since a session was last in an "ACTIVE" status if the session is currently in an "INACTIVE" status. That may give you some idea whether you are dealing with sessions that have been active for a while or not. If you have a three-tier application, your middle tier servers will generally maintain a pool of database connections. The middle tier opens a number of connections and keeps them open more or less indefinitely. When users need to interact with the database, they get a connection from the pool, run a query, and return the connection to the pool. At peak workloads, connection pooling can concentrate a large number of application users into a relatively smaller number of database connections which will mean that the sessions spend a larger fraction of their time in an "ACTIVE" status. But during periods of low usage, the middle tier probably maintains a bunch of "INACTIVE" sessions. In either of these cases, having a number of "INACTIVE" sessions is not a bad thing. And "cleaning up" those inactive sessions will cause errors in the applications that have opened them. If you have a large number of inactive sessions because the client processes have failed (i.e. the client application crashed and the database is unaware of that fact), you could use the sqlnet.expire_time parameter on the server's sqlnet.ora file to periodically send a probe packet to the client machine. That may negatively impact performance but it will allow the database to more quickly discover that the client process has failed. If you have a large number of inactive sessions because your middle tier servers have created too many connections to the database in their connection pools, you would need to talk with the middle tier admins to ask them to reduce the number of connections they open in their connection pools. If they have recently added more servers to the middle tier farm, they may need to reduce the number of connections each server maintains in its connection pool. If your problem is that too much RAM is being allocated to all the dedicated server connections that need to be open (particularly common if you've got a lot of client server applications), you could configure Oracle to support shared server connections since that reduces the amount of memory that each connection requires. 

Partitioned views are a (very) old technique for partitioning data that are very rarely used today. Oracle added the ability to partition tables back in Oracle 8, which provides much more functionality than partitioned views, at which point partitioned views became obsolete. The only reason to consider using partitioned views would be if you can't afford a license for the partitioning option and you're willing to accept the reduced functionality and extra maintenance required to use partitioned views. The documentation on partitioned views will pretty much all date back to the Oracle 7 days. In order to use partitioned views, you would define individual tables for each logical partition, create a constraint on whatever column you want to partition on so that the optimizer knows which table a particular row must be in, and then create a view that does a on each table to create the partitioned view. Queries against the partitioned view using the partition key can then be resolved by the optimizer to access only one of the physical tables comprising a particular logical partition. Of course, if you use this technique, you have to create a new physical table and rebuild the partitioned view every time you add a new partition which is quite a bit of maintenance. Getting the partitioned view to work correctly and making sure that partition pruning is happening correctly is much, much more challenging than when you're using a partitioned table. Plus you end up with a ton of separate tables with very similar names polluting your namespace. 

The "format picture" refers to the format mask. If you are doing an explicit conversion using the function, the format picture is the second argument 

If the table is in a single database, you don't want to replicate it. You don't want to create a second copy of the data. You simply want to give access to whatever schemas need access to the data. You may also want to create some synonyms so that you don't have to use fully qualified names. If owns and you want that data to be visible to 

You can then call that in SQL*Plus by declaring a host variable (most front-end languages will treat a function returning a very much like a statement that returns a cursor) 

A would be more efficient since you're only executing the query once. Even better would be to do a set-based rather than doing a bunch of single-row statements which is what I'm assuming you'd be implementing given the alternative of doing a to see if the row exists. Rather than doing a and then determining whether to do an or an , you would be better off doing the , checking to see if you updated 0 rows or not, and then doing the if your didn't affect any rows. That way, source rows that require an would be a bit more efficient and rows that require an would be essentially just as fast as if you had done a and . This approach is still going to be slower than a , but it's better than doing the extra . 

You've pointed to instructions for installing the Oracle Instant Client. That is a very slimmed down version of the Oracle Client that is intended to be bundled with third party applications and installed as part of their installer packages. It's not something that normal people are expected to manually install (other than the third-party application developer that is working on building the installer that includes the Instant Client). If you download the full Oracle client, you'll get a nice, friendly GUI where you hit Next a bunch of times. The issue is that it's much bigger (because it contains a lot more options) and it's more difficult to transparently embed in a third-party's installer. 

is a function that returns a . Oracle SQL, however, does not support the data type so you cannot directly the result of the function just like you couldn't a function you write that returns a . 

When you issue an , Oracle merely marks the session as killed and does the actual work of killing the session asynchronously. That may take just a couple seconds, it may take many hours if the session has an uncommitted transaction that did a lot of work that now has to be rolled back or if the session needs to stay around in a killed state in order to notify the client that their session was terminated. It may, therefore, require a substantial amount of waiting before a user could be dropped. You could do something like this where your loop will wait indefinitely for all the sessions to disappear. 

Note that you need to include the in the query in order for it to make sense. Oracle may use a table scan (depending on whether or not and/or the column(s) in the are indexed) but there will be a stopkey limiter so Oracle knows it can stop scanning the table (or the index) once 60 rows have been read. In 12.1 and later, you can simplify this a bit 

The only way to be sure that the remote database is up and that the listener is up and that the database is registered with the listener properly would be to actually make a connection. You could use the SQL*Plus utility (assuming the Oracle client is installed on the linux box your application runs on) to attempt to make a connection. Something like Create a file check_db_up.sql 

This sounds like a situation where Oracle Change Data Capture would be appropriate. You would need to write some code that would subscribe to changes to this table and process them by pushing the changes to your SAP BW system. You can also use Oracle Streams to do heterogeneous replication from Oracle to a non-Oracle database using an Oracle Transparent Gateway. 

In general, procedures should not commit. Those sorts of transaction control decisions should be left to higher-level code that knows when a logical transaction is actually complete. If you commit inside of a stored procedure, you are limiting its reusability because a caller that wants the changes the procedure makes to be part of a larger transaction cannot simply call the procedure directly. If you call a procedure interactively, you will have to explicitly commit or rollback the transaction because Oracle has no idea if you intend the procedure call to be a logical transaction or if you intend to compose a larger transaction involving multiple procedure calls. If you use , assumes that a job is a logical transaction and commits at the end of the job assuming it was successful ( does the same thing). Functions should not manipulate data in the first place. A function that manipulates data cannot be called from a SQL statement (barring the corner case where the function itself is declared to use an autonomous transaction which is almost never appropriate). The whole point of having both functions and procedures is that functions can be embedded in SQL statements and can be more freely granted to users because they do not change any data. 

will repeat the headings every 100 rows rather than every 15. You can set the value to (just about) whatever you'd like. If you want to ensure that the headers are only written once, you can set it to a number much greater than the number of rows that the query will return.