This answer doesn't solve the problem, but it's left here because 30+ people found it useful, otherwise I would have deleted it long time ago. Write to . The subdirectory contains the descriptors of all the opened files and file descriptor is the standard input (1 is stdout and 2 is stderr). Example Terminal 1: 

Most Unix programs don't use locking or when they use it, it's not mandatory, so I doubt locking is stopping your log from growing. More likely the SCP transfer is slowing down the log writing. 

You could try making it harder to reset the permissions of the root directory by making it append only with chattr: 

Network performance can be degrated during the snapshot process. But in my opinion a little bit of a perofrmance hit is better than losing data. Depending on how critical the data is I would maybe do one during the night. Unless your data is that critical that you must do Hourly etc. Only you can decied how much data you can lose should you need to restore. If you can live with a snapshot 24 hours old stick with it. But if you need your data no less than an hour old from a restore do that. We run snapshots hourly for our main database server as this changes hourly. But our Exchange Server is snapshotted every 12 hours. 

If you use Postfix, you can set the mailbox_command option to run your Python script on every message, but I think you'll have a lot of functionality to implement. Another solution would be to use procmail for local delivery and configure it to send (pipe) the messages to your Python script. This autoresponder example might help you. The advantage of this solution is that your script can be simpler. There's no need for it to be a full local delivery agent. 

I don't think you've generated the required certificates, therefore you're getting that error. So either generate the certificates or use another transport, for example tcp (unencrypted, good only for trusted networks) or ssh (e.g. ). 

First off product recommendations are OT on ServerFault, but.. Here’s the but; I have just been in your predicament. I have been trialing SP2010 and Umbaco for our Intranet. I needed something that would take the onious out of IT to manage the content. The users needed to be able to do this. Every product I tried I went back to SharePoint 2010. My reasons; 

Yes this is possible. A lot of companies do it for 2 reasons; Increased bandwidth. Also failover. I would get 2 different DSL connections from 2 different ISP’s. That way if an ISP has an issue there is less likely hood of downtime*. 

or . Restricted means that only the user and perhaps the web server can read it (e.g. modes / or some ACLs). Immutability can be done with or by changing the ownership to something like . An easy way to create that hierarchy on Ubuntu would be to edit and set to . 

Adjust the and parameters as you like. The is needed so that every occurrence in a line gets replaced, not just the first one ( stands for global if I remember correctly). You can also pass a value to to make a backup. 

The link needs to be created where you're building the RPM and it also needs to point where you're installing the RPM. Before creating the link make sure that the destination directory exists, i.e. . You can use or for this. 

I’m thinking that this is more a problem with the physical disk. Have you checked for Disk Fragmentation? Also have you run a checkdisk? This might shed some light any on physical problems. You say the website isn’t responding. Does it do this when you access the website locally? Or just external from this server? Sorry to ask more questions in an answer. 

WHere are the Forwarders pointing to? Are they ISP or 3rd Party? If using the ISP's DNS in forwading zones add in 8.8.8.8 (for google public DNS) and see if this helps. It may be an ISP DNS issue. If not then you will need to post some more details on config etc. 

You didn't specify the transport and the default transport is tls. The documentation regarding transports also says this about tls: 

The manual for ST32000641AS (alternative link) says that the drive has 3,907,029,168 guaranteed sectors, while the specifications for WD2003FYYS (alternative link) say it has 3,907,029,168 sectors. Therefore the drives have the same capacity. 

That's the whole idea, to use systemd (or upstart etc) inside a container. Docker is used most of the time to run a single service per container, so for a complex site you would need a container for the web server and another one for the database server. With a process manager you could run both in the same container. Which approach is better is opinion-based. Though from what I've seen running systemd inside Docker isn't easy as of June 2014, for example there's bug #3629 - "running systemd inside docker arch container hangs or segfaults". For more details also read "Running systemd within a Docker Container". 

VMware usually takes DHCP from the first nic. From the screenshot it looks like that the network cable is plugged into NIC2. Try NIC1 and it should work. From there you can set NIC bindings to be NIC2 etc etc. 

After a battery replacement you will need to run a calibration and also set the battery replacement date. This will simulate the UPS going onto battery power, at around 5% the UPS should go back onto mains to re-charge. Once that happens you should get the correct readings in your GUI. There shouldn't be any risk to the system but with anything like this make sure you assess the risk. 

I recommend the Fedora Live CD on a USB stick. It's not very small (it has 700MB), but it's easy to use and since it's a bit bleeding edge you shouldn't have any issues with new hardware or filesystems (although ext3 is already a bit old). 

I have just tested an automated kickstart install (driven by cobbler) and it works fine for me. All I had to do is press Ctrl+Alt+F2 (virt-manager has a menu for this). The only problem is that the shell is not avaialble right away, you have to wait for the installer to reach a certain stage. Regarding debugging, you might find the Anaconda logging page helpful. 

If you RAID is hot swap it will rebuild. Before pulling the drive check in the HP management agent and check it says that "HOT Swap" is enabled. Also what is the RAID Version? As pulling a drive that isn’t in a HOT Swap RAID will cause you more issues than it’s worth. 

If the data set is that small can you not store it on a file server that you may already have and include it in that backup? Also if this file server is set up correctly you would have a RAID array on this as well. 

This is exactly why you have redundant power supplies and dual UPS’s. If power supply A fails instead of the load being spread over A/B it is transferred to power supply B. Also most UPS’s can isolate each output so if there is a problem only that output is affected. Without knowing what UPS you have I couldn’t say yes or no if it has isolated outputs. As for UPS configuration you have got it spot on. No point in having 2 power supplies going into the same UPS as then the UPS becomes a SPOF (or single point of failure) so many times I have seen people attached 2 power supplies to a single UPS. If you don’t have 2 UPS’s the best thing to do is put one power supply into the UPS and another into the mails via a surge protected power cord. 

This will delete all the jpg files from , which haven't been modified in the last 7 days (168 hours). If you don't care about the case of the filenames use instead of . Here's a correspondence between the parameters of forfiles and the parameters of find: 

prints a relative timestamp upon entry to each system call in case it's needed later for extra performance analysis. traces child processes and it might not be needed. The output looks something like this: 

I want to manage the mounted partitions from puppet which includes both modifying and creating the directories used as mount points. The resource type updates just fine, but using for creating the mount points is a bit tricky. For example, by default the owner of the directory is root and if the root (/) of the mounted partition has another owner, puppet will try to change it and I don't want this. I know that I can set the owner of that directory, but why should I care what's on the mounted partition? All I want to do is mount it. Is there a way to make puppet not to care about the permissions of the directory used as the mount point? This is what I'm using right now: