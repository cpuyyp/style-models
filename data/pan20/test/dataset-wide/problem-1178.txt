No, you do not need to enumerate the columns used in a materialized view when creating the materialized view log. In fact you cannot create a materialized view log using the primary key method and include all the columns because you would be including the primary key column itself, which is not allowed. The concept of a materialized view log is to store the rowid or primary key of the data that changed. The refresh can then look up the entire record from the table. Adding specific columns to the WITH clause of the log explicitly records the data in the materialized view log itself. If your materialized view query is filtering on these columns or joining on them it could speed up the refresh. Theoretically if all the columns used in the MV are in the log, then it should be able to refresh without referencing the table. The documentation does not indicate that this is being done. It would be interesting to trace this to see. Even if it does, the additional storage requirements may not make this route worth the trouble. You should probably just create the materialized view logs without specifying a column list, like this: 

You might also consider removing the first commit so that the insert and procedure call are done in the same transaction. 

My question is what do these row represent? They have file#, block#, and objd columns, but the objd values do not exist in dba_objects. I dumped one of the blocks and it appeared to contain an index, but I couldn't tell anything else about it. There are plenty of rows in v$bh that do match indexes in dba_objects, so the unmatched v$bh rows can't be unmatched just because they are blocks of indexes. Something else must be going on. There are more unmatched objects on one of my systems, but they all have unmatched objects including the 11.2.0.2.6 system. Update: For the system with the most unmatched objects, the majority can be matched when comparing the bh.objd with the dataobj# field from obj$. I'm not sure why these objects aren't reflected in dba_objects, but perhaps an examination of the view (or a separate question) will answer that. For the remaining 391 unidentified entries, here is some other information of interest. 

This statement does not make sense. Either you are selecting all entities in set 1 or you are not. If you are only selecting collection 1 2 3, then you are not selecting all entities in set 1. You can select all entities in set one that are also in collection 1 2 3, let us know if this is what you meant. It looks like you have different patterns of ways you would like to transform the data. In Oracle, if you give each pattern a function returning a pipelined result set, you can then pass the appropriate collection identifiers to the appropriate pattern function and have it return the requested results. So, for you first example you could call: 

Since the CTE is part of the same SQL statement, it should not contain a semicolon. So, there should be no semicolon on the second line of the third block or the third line of the fourth block. The fifth block does not have a contiguous SQL statement. 

This appears to be due to the fact that the validation is using the database home for it's work area and there isn't enough disk space there. I can actually see the 8GB free there drop to zero before the command fails and the space is returned. This location is not affected by setting the location in the format when allocating the channel for the device. DB_RECOVERY_FILE_DEST is set to '+FRA', but currently most of the backups are in /u01/back rather than ASM. '+FRA' does have some control file backups. Update: I haven't found a way to do this, but I also haven't found anything definitive saying it is not possible. There is an Oracle community topic discussing this, but no resolution. Interestingly and do not exhibit this behavior, which may be why more people haven't run into it. Update 2: I opened a SR with Oracle support to see if they can shed light on this question. Update 3: Oracle support was able to reproduce this issue in house and they are treating it as a bug. 

In case someone comes here looking for a way to find the size of a long field, below is a way to do it. I will remove this answer if the question is separated. Sample data... 

If you just want the data in the database so you can display it in entirety much like you would a picture stored in the database, then a clob for each set of tabular data would work fine. If parts of the data will change, if you want to be able to write queries against parts of the data, if you want to join data from different tabular sets or with other data already in the database, and if you want these things to be fast and scalable, then you will need to do some design. Figure out what data needs to be in what tables and how it will relate. Normalize the data for smaller more manageable chunks. This means there could be multiple tables for each tabular set you have. 

Start by focusing on the query used by the materialized view. Examine why the query is running slow and you will solve why the materialized view creation is running slow. If this is the case it may require another question on the query itself. In the unlikely event that the queries run in the same amount of time, let us know. 

All that being said, it does seem prudent to be prepared for change. Just because all your product names today fit in 20 characters doesn’t mean they always will. Don’t go overboard and make it 1000, but do leave room for plausible expansion. 

There are some tradeoffs for creating the name_dupe table. To avoid them you could combine your first query with this one as follows: 

The Oracle 9.2 and 8.1.7 documentation say essentially the same thing, so even though Oracle continually discourages the use of VARCHAR, so far they haven't done anything to change it's parity with VARCHAR2. 

Update: FrustratedWithFormsDesigner's comment is essentially a Materialized View for platforms that can't do materialized views. Using a script the tables could be periodically synchronized. If one database is designated as the master then all scripts could make their changes based on it. This would have all the pros and cons of a materialized view; it just couldn't take advantage of built-in functionality. 

You could do this by grouping by the OrderNumber first and then grouping by the CustomerKey. Here is an example: 

Depending on what your requirements are about how it meets your criteria, an Oracle database would apply. If your buffer cache memory area is as large as the database itself then as the data is accessed it would be copied into memory eventually causing most reads to be done in memory. Writes would still go to disk. You could also consider a RamSan type device that allows memory to behave as a disk. They use disks for their persistent store, which would give you some of the same benefits and allow you to use any database you like. 

The wait event “log buffer space” according to the documentation can indicate that the log buffer is too small or the I/O is slow. We have a system seeing this event in which the redo buffer allocation retries is going up in V$SYSSTAT as observed with... 

Getters and Setters implies retrieving the data for one row and changing the data for one row. It sounds like you are planning to call a get procedure for each row you want in the file level table and then for each of those rows loop through getting the corresponding data from the batch level table etc. My suggestion is that you write one SQL statement that selects all the data you need from the file level table joining in the data you need from the batch level table. I don't know anything about BI Publisher, so based on your comment, none of this may apply. Perhaps someone familiar with BIP can say whether these parameters could be bound into the query to allow the query to determine what data should be returned. A PL/SQL method may be required for this application, but it needn't be more than a wrapper for your SQL. 

Bi-directional replication is technically feasible, but is anything but simple, which is why it is not used unless absolutely necessary. Streams can be setup to do bi-directional replication. See the Oracle® Streams Replication Administrator's Guide. However, Streams cannot handle automatic client fail-over and since it seems that high-availability is your goal, this is not an option. The Oracle solution for this type of problem is RAC. It meets both the bi-directional replication requirement and the fail-over requirement. However, if you are looking for storage redundancy, it does not provide that as the database storage must be shared between the servers. If your primary concern is instance failure, then it will help you. Since you are on Enterprise it is an extra cost option, but if you can tolerate downgrading to Standard Edition, you can run RAC without any additional cost. Another option would be to turn your two servers into Virtual Machine Hosts and virtualize your two databases. This again would not provide storage redundancy, but would allow you to transparently migrate either database to either server, running them on the same server when there is maintenance to be done or a server outage occurs. The Golden Gate product Oracle acquired does bi-directional replication and is more flexible than streams, but at a higher cost. There are other options at various levels of granularity and capability all the way from Materialized Views over database links to storage based snapshots. Consider carefully what you need and whether a solution meets your primary requirements.