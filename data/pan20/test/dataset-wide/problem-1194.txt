In SSMS I need to CTRL+K, CTRL+X, mouse, mouse, find, go back, find, mouse. This is not a shortcut, nothing "short" about it. I've added this sample snippet to my list of snippets: 

would the correct guess. I can iterate through a list of options and pick the one that works. Are there any other possible folder where SqlLocalDb.exe can be installed? Or is there a registry value somewhere that I can check? 

I'm developing an application that is using SQL Server for storage. For development/testing purposes I need a subset of production data that will be anonymised, converted into statements and checked into version control. So far I've been doing 

Though I'm not absolutely sure about the last path. And I can't install SQL Server 2016 on my machine, so can't confirm if 

Then generating scripts with data and tick only . That takes a fair chunk of time and error-prone - especially the bit where I remove personal data (think what would happen if I put wrong table name into the update). Is there a better way to get sample data from production into scripts? 

I can find it via CTRL+K, CTRL+X, but I'm unable to invoke it by + TAB shortcut. Am I missing something? Can SSMS even do this? I'm using SSMS v13.0.16100.1, but database is SQL Server 2014. 

Monitoring CPU usage using task manager is not really a reliable source. There are many other(such as core OS activity,device drivers) non-sql processes running in the background that could be adding extra overhead without you even knowing. PerfMon is the tool you should reach for in these cases. Processor/%Privileged Time, Processor/ %User Time, Process (sqlservr.exe)/ %Processor Time Will give you an idea of what is actually happening with your SQL server, without explaining each of these counters, turn on description checkbox and read from there, but it will essentially show you the ratio of SQL Server vs Other processes usage. Even though its easy to spot, it is not so easy to diagnose. There could be other "hidden" issues that are indicating that the processor is the problem. Such as having lot of compilations/recompilations, which are issues related to non-parametrized queries or forced recompilations. You can find these metrics in Perfmon: SQLServer:SQL Statistics/SQL Compilations/sec, SQLServer:SQL Statistics/SQL Re-Compilations/sec. SQLServer:Plan Cache/Cache hit Ratio Indicates memory problem, but excessive page flushing in/out of memory also add extra CPU usage. DMVs can also help you diagnosing the problem. 

2) This one will show you records that are placed in a log buffer, in what state transaction was, how many records are logged, size in bytes ,and a query that executed it. In a nutshell it displays all inserts/deletes/update from an active transaction that are not committed/rolled back yet 

I'm writing a plugin for Cake Build tool where I need to start a LocalDB instance. But this code will be executed on different machines and LocalDB can be installed in different folders depending on the version of SQL Server installed in the build machine. I know at least 3 paths possible: 

For LocalDB you'll need to install SQL Server Express on all the machines where you want to run your application. If you don't want to do that, have a look on SQL CE. That allows you to have only DB file moved about and all the required libraries can be included in your project. 

I have about 150 reports generated from SQL statements. These reports are saved independently from the server. Some generated from sprocs, some from table-valued functions, but mostly just SQL statements. Now the server goes through a big refactoring of tables and sprocs. And I need a way to validate that all the reports are still running, as it is possible the changes will break the reports. One way to validate that all the reports are valid - run them and lookout for errors. The problem - some of the reports are very heavy and include aggregation of of many tables with many joins with millions of records. Is there a way to validate SQL statements without actually executing them? I'm working with SQL Server 2008 and have ability to apply C# processing to the statements. Ideally I'd like a way to validate the reports often and without much effort, i.e. 10 times a day. So making a schema-only copy of DB is not an option. 

After delete trigger is executed after the record has been removed from the table. Joining a table Emp will yield no result because record with that ID does not exist in that table. Also note that inserted table will always be empty in after delete trigger. 

Just like the master key, you need to back up this certificate as well. guide Now we are creating a user with certificate which will have all the rights: 

As specified ,returns table therefore SQL server checks the underlying structure of the table because it needs to know what kind of table is begin returned. How many columns it has,if any of the columns are without names(in aggregate cases) and inform you about it. You can test this by running the query above without referencing the column name( 'as user name' ) in case username table does not have a specified column name. Note: I had to tweak this two functions a little bit, to make them work. Hope this clarifies what is going under the hood for you 

When you are all set and done, you can transfer those views from ViewSchema to dbo or some other more meaningful schema with command: 

Dynamic quorum basically dynamically adjusts votes depending on available servers. Each time when one of nodes goes down, dynamic quorum will remove the vote from that node. In your scenario you have 2 nodes only and dynamic quorum will automatically remove the vote from your passive node, so the 1st node will have the majority of votes. In planned maintenance scenario when you are shutting down the first node quorum will transfer the vote from first to the second, and remove it from the first node. However in scenario where first node just crashes quorum does not have time to transfer the vote and your second node wont get to vote, which basically will just shut down your cluster. Therefore in scenario with 2 nodes only, it is recommended to have a witness. 

I have a short (15 rows) look-up table that lists the valid values for several columns in the database. I don't think it can be considered a One-True-Lookup-Table, it is about a single definite concept, but it may conceptually be partitioned in some subgroups (3). I now happen to have to add a column that actually needs to accept only the values from one of these subgroups. The proper thing to do now would probably to make one table for each subgroup and turn the original table into a simple list of IDs, from which the subgroup tables take their primary ids from. It is though very unlikely that I ever further need to refer to the subgroups, I will instead frequently use things for which all the values in the original table are valid. So the partitioning option would make things more complex for the vast part of the application just to support one case. The only other option I know of is to add a Type column to the original lookup table and a fixed-value column to the single table that need to refer to the subgroup, and use a two-columns foreign key (to ID + Type) in this single case. This is very ugly, and I'm not sure if it is theoretically correct to use columns that are not the primary key as foreign key referents, but given the context, is it acceptable, or even the best way? 

Your second query runs within a second because it does not have to go through each record in (130 million record table) and compare whether it matches the record from a temp table. And there is not much you can do when you are using a temp table with a single record within. One solution would be to save it within a variable and use it in where condition without joining it, but you said temp table will contain more records. Note that ,it does not necessarily mean that more records will increase your execution time. With more rows in temp table, query optimizer will use Hash Join which could possibly give you even better results. However you could optimize your query like this: 

will show you the most CPU extensive queries which you might want to optimize further. Optimizing these queries you might find missing indexes, outdated statistics, Non sarg-able queries which are real issues that are behind high CPU usage. Its not the only blueprint how to fix CPU problems but i hope it gives you a good start! 

Like i said there are other ways, including roles but these two could get you a job done. Just remember you cannot track a user that executed procedure by specifying 'execute as'(login is possible tho). Also creating a certificate/database keys can be a headache when you`re migrating DB, or simply restoring it somewhere else. 

I have a table that describes a set of properties (yes it's metadata, in this case there's a valid reason to have it in the database); among other things I tell the type of the data that can be assigned to them (Type) and a default value. The valid values for the default value are thus restricted by the Type column, but the default value is genuinely an attribute of the table, they are not uniquely determined by the Type column. I'm not sure though, should this be somehow normalized or is it right as it is? Edit: here's an approximate description of the current structure of the relevant part of the database, as requested. Don't mind the Values table, that's a separate issue. Just consider that ValueType restricts the set of DefaultValue permitted values. Also, consider this only as an example, I'm interested in the problem in general, the problem being namely columns that limit the set of valid values of another column but that don't determine its exact value, thus as far as I understand don't constitute multivalued dependencies (but, as an aside, it would be useful to have an integrity constraint that enforce the limit - I'm not sure that this issue can be separated from the former). 

At the end I avoided to use the solution with the Type column because it seems very unorthodox to put a foreign key that doesn't refer exactly the primary key of the other table; I did not make a table for each subtype though, only one for the specific one I needed with only one column with the appropriate list of the ids, leaving the general properties in the original table. This seems the best option since it is really, extremely unlikely I'll ever need to refer to the other subtypes.