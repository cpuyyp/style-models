I have a two-node SQL cluster (2008 R2). Some of the databases within that SQL instance are mirrored to another server on a remote site, using the High safety with automatic failover. The mirroring connection timeout value for those databases is set to 90 seconds. When I move SQL from one node in the cluster to another node, using the Failover Cluster Manager application's option of "Move this service or application to another node" the databases that are mirrored are instantly failing over to the mirror. This is undesirable behaviour. My reason for setting the mirroring connection timeout value is that I only want to fail over to the database mirror if the cluster fails completely and there are no functioning nodes. Is there any way to achieve this? It feels as though it should be possible, otherwise the concept of mixing clustering and automatic failover database mirroring would be unworkable as every node failover within the cluster would trigger a mirror failover. Thanks. 

I'm importing a large amount of data into an empty database, and before I start I disabled all non-unique non-clustered indexes to see if I could improve the performance of the import. Now I want to re-enable the indexes, and I'm wondering if there is anything that I can do to optimize this. There are > 100 tables and almost 2,000 indexes to be rebuilt. The database is 200GB in size. The key section of the script I'm running is this: 

I have two tables, namely Employees and Payments. Below is the contents of the two: The Employees table 

EXP(SUM(LN(YR.RATE))) is just a trick to get the multiplication of the column values as there's no ready function available in Oracle. 

After the process is over, I run queries against all the tables. Everything goes fine with 9 of them, but with one of the tables the executed query hangs. The next day, when we run the same query against the same table, the execution takes only a moment. I have no idea what the problem is. I tried not adding indexes but it didn't help. I know he way we copy table content might seem weird. 

I think it's obvious what it does but to make it clear let me give a brief explanation. In my table there's DOCNUMBER column. I assign a value to this column in Before Update trigger. And in the above trigger, which of course should fire after the Before Update Trigger, I check if the DOCNUMBER has value (and if it's for the first time) and if it does, I write that value to another table called . The problem is, the above trigger sometimes, although very rarely, fails to insert the to the table. That is, sometimes I see a record in table with DOCNUMBER value that does not exist in table.The table has only one column called DOCID and it's part of a UNIQUE KEY constrsraint. Do you see any problem with this trigger? EDIT: I think you'll get a better idea if I post the BEFORE TRIGGER too: 

Perhaps the easiest way is to just do a insert...select, re-referencing at the same time by using set identity_insert on and adding a fixed value to the existing identity to ensure no conflict between the data sets ? Something like this: 

I have used SQL Servers Import Data wizard to import data from an Excel spreadsheet. The Import Data Wizard worked fine to import the data. It also worked fine to save the SSIS package. I can then log in to Integration Services on the server and run the package from within Integration Services. However, when I try to run the package from SQL Server Agent, I get the error message: 

As a very generic answer, if you want to test code of this nature without actually changing the data then it is possible to run your code, query the results, and then undo any changes before they are committed. 

I have a database that seems to be functioning fine with no apparent errors except that any full backup taken is broken - attempting a restore fails with an error "RESTORE detected an error on page (18469:-1767164485)" DBCC CheckDB on the database completes without errors. EDIT #1 The backup is created with the following command: 

I'm currently doing some work on monitoring SQL Server performance. I've found the following script[1] which calculates 'Page Lookups Percentage' - the suggestion being that a "good" value is something less than 100. 

You really want to read first: $URL$ If you want to keep your history in a separate table (and probably you do), you will probably want option #3; it tends to be the easier to implement and more convenient, #1 are #2 are pretty ugly and "un-relational". 

Memory can help you by caching and thus reducing I/O. However, that won't reduce CPU usage which is your problem. This is an unusual bottleneck, as CPUs are insanely fast for most database work and I/O tends to be the bottleneck. In your case, it is even more inusual, because you have 16-cores and VPSs tend not to have great I/O performance. First of all, make sure that you are CPU bound. should help here.If you have high numbers on the column, you are probably I/O bound; high numbers on the column could indicate that you are really CPU bound. If you are CPU bound, analyze what queries are you executing and why they take long to complete. You are either executing a lot of queries/s or they include complex calculations (i.e. aggregates, functions, etc.). The solution for the former is usually caching on the frontend, which means executing less queries. The latter is solved by simplifying your queries (if possible- you might have queries which are needlessly complex) and calculating stuff once and reusing it (say you have lots of aggregate queries; create a table with the aggregation results and query that instead of running aggregates continuously). The most efficient way to research about this is by logging which queries you are running and analyzing the log- tools exist which do this neatly. If you are I/O bound, then you can tune memory usage, although the OS cache is often working correctly. Take a look at : 

The title was best I could find to explain my question, I don't think it helps though. Anyways, I have two tables: 

Now if I want to show only the first ten rows between 50 and 60, the only way I can think of is to first run the above query with ROWNUM pseudocolumn and then select from the result of this query. Something like this: 

Now, there might be multiple entries for one customer_id value in the table B. I need the latest date to be updated into the table A. I wonder, if I order the result by last_login_date column in Ascending order, will the merge statement eventually update the record in A with the latest last_login_date? 

I've been asked this question but neither I seem to be able to answer it on my own, nor can I find anything related on the web. So what are the cases that might cause a runtime exception when committing a transaction in Oracle?The only thing that I can think of is the low disk space. Are there any other? 

But this way, I'll end up first fetching all the employees whose positions are 1 or 3 and then extract the 10 rows among them. I find this a bit inefficient. Is there any way to achieve this without making full scan first? 

So we are writing an app whose schema should reference data which lies currently in an external PostgreSQL instance. We are negotiating being able to put our schema within the external database, but we are evaluating different possibilities. One option I'm pondering is basing our app on PostgreSQL and use its facilities for accessing external PostgreSQL instances. What's the status of this? PG's documentation contains $URL$ and $URL$ , which allow you to reference tables in an external server. What's their status? Are queries performant (i.e. sends WHERE to the other side)? Can you reasonably join between local and foreign tables? There's also pgsql_fdw ( $URL$ ), which seems more featureful. Does it offer an improvement on the above? Anyone using it? Thanks, Alex 

, but it seems a bit overkill to me. You could also force the creation of an artist row in any case, but sometimes this doesn't seem appropriate. Are there any better approaches? Cheers, √Ålex 

I've seen a few instances of entities which have a field which can be either a reference to another entity or free text. You typically represent this in the UI by having a dropdown widget which can optionally have text instead of one of the dropdown items selected. Say you have a song entity which was recorded by an artist- the artist can be either a reference to an artist entity or a random string. I'd usually represent this like: