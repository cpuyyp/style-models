You can use a properly formatted method on the Chef recipes you run on the web server that return the node object for your DB server. Then you can get the aforementioned IPs via node["ipaddress"] Î¿r, if you need the public IP/hostname when running on EC2, node["cloud"]["public_hostname"]/node["cloud"]["public_ipv4"]. For example, after deploying the DB server use something like: 

The open file cache is a caching system for metadata operations (file mtime, file existence etc), not for file content, so it helps but not as much as you would expect. Some workarounds you could try are: 

to get your sqlite extension installed and automatically loaded. Note: Atomic offers a lot of updated packages and PHP there is on the 5.3 version. Bear that in mind if you are running 5.3-incompatible applications. 

This can happen almost surely if this is a micro instance you're running. In this case, add a swap file recipe before everything else (try this one: $URL$ and your issues will be over! 

You are trying to communicate with the FTP server in passive mode while not allowing data ports correctly. Try adding a range of unprivileged ports like: 

Aside from removing or disabling the default.aspx file? Not sure there is a way. Rendering the HTML as opposed to the index is the designed behaviour. 

Databases don't automatically shrink, just because there is less in them. Resizing the database on the fly is a performance nightmare, so most people take great pains to be sure that the file is big enough that they'll be able to grow it during off-peak hours. You need to run DBCC SHRINK DATABASE 

Well, Powershell has $?, so I'm assuming the problem is that it only populates on exit? When you're dealing with errors in running code, the best practice is to use a Try/Catch block. Code inside a "Try" block will automatically fail over to "Catch" in the event of problems, and, even better, you can have different catch blocks for different exceptions, so you can handle different problems on the fly. Very cool. 

It may be that it allocated around the bad sectors and "fixed" the problem. A certain amount of that is perfectly tolerable in a drive. 

This will send evry 300 seconds a keep alive signal for a maximum of 2 times. If you like to have it to the infiniti set ServerAliveCountMax to 0 If you wold like to keep alive FROM the client, do exactly the same but without "Host *" 

How about creating a local user which logs on (automaticly or not, as you like), allow multiple login and set incomming connections to use the session of this local user? For security you could restrict this user kinda hard. So if you wan't to do some work where you need to be admin it will ask you for an admin-authetification. And if someone could ever be able to log into this account, he still has no rights to do silly stuff. I haven't tested it yet. And i guess HopelessN00bs answer might be better, because you do not have to mess around with the security Problems my purpose would bring along. 

240 would be the amount seconds the Host will keep your SSH-Session alive. After 240 it will close the session. 0 means do not keep alive As i said, it's mostly better practice is to keep your client doing the keep-alive-work, and keeping your Server kinda tight. 

Check the umask for user Proftpd and group nogroup. Since you're trying to set permissions higher than the system default, they may be restricted by their OWN umask values. A way to check it might be to change the umask in proftpd.conf to 777; if new files show up as 000, then you know that configuration line is working. 

I've never had problems, and my system logs are routinely larger than 2 gigs on some of my servers with external IPs (the logs rotate weekly, not by size). I also run a couple of massive feeds that produce files that are 3-6 gigs in size, and I haven't had problems with those either. I'd say it's completely dependent on what user-land programs you need: if there is a deal breaker, you may need to re-evaluate. 

The runlevels are completely configurable from your inittab, so it's possible that runlevel 1 is mapped to a mode with networking activated. Does look odd though. Try booting into runlevel 2: in Suse that should default to multiuser/no networking. 

Do you have a domain name? That's what the problem here is; the machine doesn't know where to look until you tell it that it's .nuthin If not, dnsmasq has a couple of options. You can set up ALIAS (CNAME) records to point the name to the A record (which defaults to A.domain.tld, which is probably your problem). If your router has a "hosts" file, you can add the hosts there, and it should pick them up (make sure you do 10.1.1.2 host host.domain.tld, so it'll know that host = host.domain.tld). 

If you really need to farm out to FastCGI servers instead of nginx + php-fpm boxes, you can try the fair module for nginx found here. The plugin assesses reposponse time from each of the backends and rotates respectively. Note that this will require you to recompile nginx. If you don't want that, make sure at least that you are not using the directive, since you will not get rotated when requesting over a benchmark (since the source IP is always the same) and try (found in nginx >= 1.2.2). More information here. Finally, adjust your criteria for nginx selecting the next server using 

You can use the command line option in order to define attributes you may want to override the default ones. If you want to directly define the options via text, you can use the input redirection feature of bash like: 

Finally, when testing your cookbooks via Vagrant you can use the local Chef's address to do it! As a helper you can define an function that can server as the tool for your local Chef. Just put in your user's .bashrc 

Well, I'm lucky in that I'm in house, but also corporate. So I have my site, and my network, but I'm also a corporate "expert" who gets called by everyone and their mother to remote in and fix this or that. I even get to travel a bit. So it's nice. On the other hand, I've worked in house in places where there was no money, and no good toys, and you had to put everything together as best you could, and weather the inevitable storms when some piece of substandard gear kicked the bucket. So I'd say, on balance, it all depends on the location. Being a contractor can be fun because you're seeing new things, and you get to do huge satisfying jobs, but you've also got to deal with angry local employees, and you don't ever get to rest, or deal with the big system (aside from epic contracts). And being an in-house guy can be fun...in the right house...but in the wrong house it can be ugly. 

I'd say it depends, but in general, yea, you're going to have to support users and desktops. System admin covers a lot of ground. I do unix and mainframe admin and I rarely touch a desktop...but rarely is not never. And I've never seen a windows admin who didn't have to get his hands dirty every now and then. I think, these days, that a dedicated hardware support guy is almost always a waste of money. You're paying a guy a salary to deal with an occasional problem, on commodity machines? Cheaper just to use that money to buy new hardware. And admin work, if you're doing it right, shouldn't be a constant hassle.