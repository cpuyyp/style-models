I am not suggesting that any mathematician should read all of them, but any one of them will do. In fact, the actual content of these papers does not matter so much. It is rather, that they give an insight how a new idea is born. So, if you want to give birth to new ideas yourself, look at them, not at some guideline. 

I am surprised to see that so many people suggest meta-mathematical articles, which try to explain how one should do good mathematics in one or the other form. Personally, I usually find it a waste of time to read these, and there a few statements to which I agree so wholeheartedly as the one of Borel: "I feel that what mathematics needs least are pundits who issue prescriptions or guidelines for presumably less enlightened mortals." The mere idea that you can learn how to do mathematics (or in fact anything useful) from reading a HowTo seems extremely weird to me. I would rather read any classical math article, and there are plenty of them. The subject does not really matter, you can learn good mathematical thinking from each of them, and in my opinion much easier than from any of the above guideline articles. Just to be constructive, take for example (in alphabetical order) 

Arbitrary Zariski-dense subgroups in a semisimple group can be very small from a real-analytic point of view. It seems that algebra cannot distinguish between "small" and "large" Zariski-dense subgroups, so most criteria to distinguish between the two have a strong non-algebraic flavour. (Of course one can also characterize arithmetic groups algebraically, but this has even less to do with the line of argument you seem to suggest.) From a dynamical point of view, the key difference between lattices and arbitrary Zariski-dense subgroups is that the former act transitively on the product of the Furstenberg boundary of the ambient Lie group with itself ("double ergodicity"). This is a sort of "largeness" property. There are various ways to capture this property, the most systematic way seems to me the concept of a generalized Weyl group due to Bader and Furman. 

The Erdős-Kac theorem gives that, for a fixed function $M(N)$ with $\limsup M(N)/\log \log N < 1$, $\frac{1}{N} |\{n\in[N,2N] : \omega(n) > M(N)\}|\rightarrow 1$. Likewise if $\liminf M(N)/\log \log N > 1$, $\frac{1}{N} |\{n\in[N,2N] : \omega(n) > M(N)\}| = o(1)$, where here $\omega(n)$ counts the number of prime divisors of $n$ without multiplicity. (So $\omega(4) = \omega(2) = 1$, while $\omega(6) = 2$.) The same results will be true if prime factors are counted with multiplicity however. (i.e. we consider $\Omega(n)$, where $\Omega(4) = 2$ for instance.) More precise asymptotics can be obtained, especially easily for $M(N) = o(\log \log N)$, by using formula of Sathe and Selberg, and its extensions. These are uniform versions of the theorem of Landau which has been mentioned by quid. Where $M(N)$ grows like $\log \log N$ or faster, I'm afraid these formula become somewhat complicated, and I wouldn't expect a nice asymptotic expression (but I could be wrong). A reference is "On the number of prime factors of an integer" by Hildebrand and Tenenbaum, the easiest offshoot of which (due to Sathe) is that Landau's formula holds uniformly for $k = o(\log \log x)$. Formula (1.7) of Pomerance will give you (with a little patience) nice upper bounds. The book of Tenenbaum already mentioned is also nice reference for some of these questions, as is chapter 7 of Montgomery and Vaughan's "Multiplicative Number Theory I". Kac's book is great for anyone to read, interested in these questions or not. 

I'm looking for an elementary combinatorial/generating function/etc proof of the following result: For nonnegative integers $r$, $$\frac{1}{r!} = \sum_{p_0+p_1+\cdots = r} \frac{1}{(p_0!)^2(p_1!)^2\cdots{p_0+p_1+1\choose 1}{p_1+p_2+2\choose 2}{p_2+p_3+3\choose 3}\cdots}.$$ Here the sum is over all sequences of nonnegative integers $(p_0,p_1,...)$ that sum to $r$. (Only finitely many terms in each such sequence will be nonzero.) It is related to a result of Diaconis and Shahshahani that the trace of a random unitary matrix (with probability measure being given by the Haar measure) is distributed like a Gaussian variable, and indeed can be proven using this result, but I had initially hoped to proceed in the other direction. The above sum, after all, can be evaluated for specific $r$ by inspection (although this rapdily becomes a bit tedious for $r > 2$), and it ought to be possible to somehow summarize this information in a general. Edit: Alternatively phrased, we want $$e^x = \sum_{p_0,p_1,.. = 0}^\infty \frac{x^{p_0+p_1+\cdots}}{\left(\prod_{j \geq 0}(p_j!)^2\right)\cdot\left(\prod_{k\geq 1}{p_{k-1}+p_k+k\choose k}\right)} = \lim_{\lambda \rightarrow \infty} \sum_{p_0,p_1,.. p_\lambda = 0}^\infty \frac{x^{p_0+p_1+\cdots + p_\lambda}}{\left(\prod_{j=0}^\lambda(p_j!)^2\right)\cdot\left(\prod_{k=1}^\lambda{p_{k-1}+p_k+k\choose k}\right){p_\lambda+\lambda+1\choose \lambda+1}}$$ 

While the answer to your question is negative in general, as pointed out before, the answer is positive for certain type of loop groups. This can be proved using topological twin buildings. See Linus Kramer, Loop Groups and Twin Building. (It is not stated very explicitly, but the topology used on the twin building and hence the algebraic loop group is meant to be the ind-topology coming from the Bruhat cell decomposition.) 

If we ignore the trivial case of the affine line, then irreducible symmetric spaces come in pairs compact - non-compact. The compact ones are naturally projective varieties, while the non-compact ones are affine varieties. Thus question (4) is problematic, unless you mean "locally symmetric" or a more general notion of symmetric space than I understand here (i.e. "globally symmetric Riemannian symmetric space"). As far as non-compact symmetric spaces are concerned, they are Kähler if and only if they are biholomorphic to a bounded symmetric domain. Equivalently, there exists a compact quotient with non-trivial H^2 or, equivalently, the point stabilizer of the automorphism group has infinite center... I could give many more characterizations, but I do not quite see what you are after, so maybe you can provide more detailed information? 

Here is a complete answer: Every semigroup $S$ of invertible $2\times 2$*-matrices which is transitive on* $\mathbb R^2$ is either conjugate to $SO_2(\mathbb R) \times \mathbb R^+$ or $SO_2(\mathbb R) \times \mathbb R$ or it is a product of $SL_2(\mathbb R)$ and a multiplicative subgroup of $\mathbb R$*.* Proof: Let S be such a semigroup. Then the intersection $S_0$ with $SL_2(\mathbb R)$ is a subsemigroup of $SL_2(\mathbb R)$. By a theorem of Hilgert and Hofmann (see their beautiful paper on "Old and new on $SL_2$") there are only three choices for $S_0$: Either $S_0$ is all of $SL_2(\mathbb R)$, a circle group or contained in a conjugate of the elements of $SL_2(\mathbb R)$ with only positive entries. If $S_0$ is a circle group, then $S$ will be conjugate to $SO_2(\mathbb R) \times \mathbb R^+$ or $SO_2(\mathbb R) \times \mathbb R$. If $S_0$ happens to be all of $SL_2(\mathbb R)$, then we have $SL_2(\mathbb R)\subset S \subset GL_2(\mathbb R)$, so $S$ is a product of $SL_2(\mathbb R)$ and a multiplicative subgroup of $\mathbb R$. In the third case, we may assume that $S_0$ is actually contained in the semigroup described above. Then $S_0$ maps every vector with two positive entries to a vector with two posiitve entries, hence $S$ maps the upper right quadrant to a subset of itself and the lower left quadrant. In particular, $S$ cannot be transitive. 

(The apparent changes in convexity for $c$ near $1$ in the bottom two graphs are just a remnant of polynomial interpolation and should be ignored.) 

In both the case of 3 different types and 4 different types, it is now easy to verify that the group generated by $s$ and $d$ is commutative, proving that the statement about permissible marriages made earlier will always hold for such a type system. ...Hopefully at least a little interesting. I suppose some of the more geometric examples above might make for a more visceral application of group theory, at least on first exposure. 

There is a connection! (Though see the edit below.) Keating and Snaith make their conjecture by modeling the distribution of $\zeta(s)$ by the distribution of the characteristic polynomial of a random unitary matrix, distributed according to Haar measure. This is connected to symmetric function theory by Schur-Weyl duality. To my knowledge the first time this connection was used to compute moments (of a characteristic polynomial) was in a beautiful paper of Bump and Gamburd. See in particular Prop 4 (and perhaps especially the quick proof) and Corollary 1. As to other connections, slightly different averages of the zeta function, such as $\frac{\zeta(\alpha+s)\zeta(\beta+\overline{s})}{\zeta(\gamma+s)\zeta(\delta+\overline{s})}$ are also be conjectured to be related to analogous averages in random matrix theory, the computation of which can be reduced to symmetric function theory in a similar way. This too is done in later sections of the Bump and Gamburd paper. There is also a very nice paper of Dehaye that you might find interesting expressing lower order terms in conjectures for the moments of $\zeta(s)$ in terms of counts of taubleaux. Edit: it occurs to me that I may be leaving something unsaid. In particular you ask specifically about $S_{k^2}$. The paper of Bump and Gamburd shows in a pleasant and transparent enough way that $$ \int_{U(n)} |\det(1-g)|^{2k}\, dg = s_{\langle n^k \rangle}(1^{2k}). $$ To see what this should say about the zeta function we need to know that $s_{\langle n^k \rangle}(1^{2k}) \sim g_k n^{k^2}/(k^2)!$, where $g_k$ let's say is defined as the count of tableaux you give. While it is plausible that once symmetric function theory is involved more symmetric function theory will come into play, and while there are various ways to prove this, I don't know of any proof that makes this asymptotic relationship entirely transparent. I'd be happy to learn of one. It may also be that you're wondering whether there is a way to get from number theory to the symmetric function theory without passing through the middleman of random matrix theory. I think this is a very interesting question but I don't have anything conclusive to say about it. One remark that can be made is that the conjectures of Conrey, Farmer, Keating, Rubinstein, and Snaith that Dehaye uses in his paper came out of a recipe that keeps track of various symmetries in products of the approximate functional equation for the zeta function (identity (36) in Dehaye's paper).