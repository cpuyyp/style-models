The compiler is required to store members of a struct or class in the order they are specified in the definition. And it is also required to respect the requested alignment. So the above code will form a structure in memory that looks something like this (depending on platform, unsigned is at least 16 bits but most often 32 bits): 

Hard points and maximum weight is defined by a chassis. Each chassis has different characteristics when it comes to movement, hard points, and other constraints. Upgrades A mech can have so called "upgrades" which affect global properties of the mech. For example the internal structure can be "standard" or "endo-steel" where endo steel is a space/mass trade off. Similarily other upgrades for mech's cooling, armor and missile guidance can be installed that will affect the performance of the mech. Modifiers and weapon groups It is worth noting that many attributes of items and the mech can be altered by modifiers. These may be anything from special equipment that enhances some properties of other equipment, to player level (called Efficiencies) based. Weapons can also be grouped and fired in groups. This is a vital part of game play and effective piloting, these weapon groups are modeled and statistics are calculated per group. All of the above, weapon groups, player skill levels (efficiencies), upgrades, equipment, armor distribution, modifiers and chassis make up the loadout. Problem with the code The class I have for representing the loadout has grown organically over several years from what started as a small class to something that is turning into quite a beast. It has been complicated by the fact that there are two types of chassis with slightly different rules for the loadouts: Omni Mechs and Standard Mechs. Which was not originally thought of when the class was designed but has been added in as an after-the-fact. Omni Mechs have fixed equipment that can't be moved and fixed engine and upgrades, but they allow you to change between several "omnipods" per component that alters the set of hardpoints the component supports, giving more freedom in design. I'm happy for any feedback, but I'm mostly interested in suggestions for how I can manage this unwieldy beast. If you need to see the rest of the code, it is available in the github repository linked at the top. 

Lets take a look at the function too, adapting it to use a list of lists for chaining and cleaning it up a bit: 

Notice that it calls into instead, this is a 512 bit wide AVX optimized memset that will only function on modern CPUs (it did this because I passed asking it to throw backward compatibility out the window). There is some branching code at the end which deals with cases where isn't evenly divisible by . But here is the key take away, we clearly expressed our intent with the trivial loop. The compiler could then replaced the trivial loop (after deducing it's functionality) with an (512 bit) AVX optimised memset written by Intel's engineers specifically for modern Intel CPUs. There is this thing called the "as-if rule" in C/C++. Which pretty much says: 

you allocate a new position for each of the $$4\cdot range^2$$ elements checked when you could get away with simply allocating once at the top and re-using it. Or here: 

So you are very unlikely to have correct closed/open sets after 300 000 iterations. Remember with A* you're searching for the optimal solution and you will expand a lot of nodes to find it; As opposed to if you're just looking for a solution then you'll take the first solution you find (greedy best-first search will do this quickly). The closed set is not necessary (and neither is the open) The purpose of the closed set is to guarantee that you do not expand the same state twice as the consistency of the heuristic guarantees that the first time you expand a node is the best way to expand it. It is just an optimization. If you get rid of the closed set you will still find the optimal solution but you may expand each state multiple times. However as A* remembers the cost to get to a specific state you're going to expand better (less costly) paths first. If you don't have the closed set, you can simply remove the open set as it is essentially the same as your fringe of nodes to expand. This should help you on larger problems, but running out of memory after 300k nodes sounds odd, which brings the next point: Adjust your JVM memory limits If I'm not mistaken the default memory limit on some JVM's is quite low, you can increase this by passing for example as command line argument to java. 

Performance Lets look at . You're using which is fast as long as you don't get excessive collisions. Seeing as you are dealing with many numbers that are possibly small, you could be having many hash collisions. If profiling indicates this is the case, you can try a better hash function (for example multiply by a large prime number). Here: 

Use properties of prime numbers The only prime number that is even is 2, for the simple reason that if the number is even, it is evenly divisible by 2 and is thus not a prime. So after checking if 2 is a factor and removing it, simply start with and in the update do . And you're twice as fast! But there is more, consider \$x=6k+l\$ where \$l\in\left[0,5\right]\in \mathcal{Z}\$ and \$k\ge 0\$. It's obvious \$x\$ can represent any positive integer. Assume for a second that \$k>0\$ and lets look a bit closer: 

just in case the JVM is unable to deduce that those stack variables shouldn't be initialized unless you actually get there. 

This variable is never used outside of the constructor, in fact it is almost always shadowed by a method parameter. As such I would remove this member and the accessor which isn't used either. Variable shadowing is something I detest and find makes code much harder to read. I would recommend that you get into a habit of avoiding shadowing. Instead of having: 

To create a binding, a kind of numerical expression that can be evaluated. And whenever you need the scale factor simply call: . The result is cached and automatically updated as soon as either the canvas or the sliders change. So the above snippet will eliminate the method. Going with the Model-View-Controller idea, your "model", the would have: that you would bind to the value of the sliders of your UI ("view") in your "controller". And they would automatically be kept in sync. Avoid allocating new objects in your game loop. I see that you frequently allocate new elements in your game code. While memory allocation is cheap in Java, frequently allocating objects and throwing them away like you're doing will cause a significant strain on the GC and can cause your game to appear "jittery". I would advice to allocate the position objects once and re-use them where it makes sense. For example here: 

Note that this has the same asymptotic time complexity as OP's own answer \$\mathcal{O}(whr^2)\$. But this computes the reciprocal of the hypotenuse only \$\pi r^2\$ times while OPs answer does it \$\frac{wh}{5000}\pi r^2\$ times. The calculation of the reciprocal of the hypotenuse is the most time consuming part in this algorithm and should dominate the execution time. Note: The observant reader will have noticed that the grid is symmetrical in all quadrants. This means you can reduce the number of calculations of the reciprocal of the hypotenuse to \$\frac{\pi r^2}{4}\$ but that's left as an exercise to the reader. 

Complete and Incomplete Algorithms Search algorithms can be classed into two categories: complete and incomplete. A complete algorithm will always succeed in finding what your searching for. And not surprisingly an incomplete algorithm may not always find your target node. For arbitrary connected graphs, without any a priori knowledge of the graph topology a complete algorithm may be forced to visit all nodes. But may find your sought for node before visiting all nodes. A* is a complete best-first method with a heuristic to try to avoid searching unlikely parts of the graph unless absolutely necessary. So unfortunately you can not guarantee that you will never visit all nodes whatever algorithm you choose. But you can reduce the likelihood of that happening. Without pre-processing If you cannot consider pre-processing your graph then you're stuck with a myriad of on-line algorithms such as depth-first, breadth-first, A* and greedy-best-first. Out of the bunch I'd bet on A* in most cases if the heuristic is even half good and the graphs are non-trivial. If you expect all routes to be short, a breadth-first algorithm with cycle detection and duplicate node removal may outperform A* with a poor heuristic. I wouldn't bet on it though, you need to evaluate. With pre-processing In your case I'd see if I could pre-process the graph, even if you need to repeatedly re-do the pre-processing when the graph changes as long as you do sufficiently many searches between pre-processing it would be worth it. You should look up Floyd-Warshall (or some derivative) and calculate the pairwise cost/distance/jumps between all nodes and use this table as a heuristic for your A*. This heuristic will not only be admissible, it will be exact and your A* search will complete in no-time. Unless you of course modify the algorithm to store all pairwise routes as they are detected in a matrix of vectors, then you have O(1) run time at the cost of memory. 

I know this may sound silly but what exactly do mean by connected to the internet? You're going to have to define what this means to your application. In your example, if "www.google.com" goes down then the user is no longer connected to the internet and this is not necessarily true. Last time I checked, google isn't the internet (although it is very close). The problem all boils down to the fact that the user may be connected to some part of the internet but unable to reach other parts due to corporate/national firewalls, broken equipment, wrongly configured hosts files etc. Or just a very zealous ISP who insists every one go through their gateway googlez.net to show you ads on every google search. It makes more sense to test for an URL that actually means something for your application (I can't tell what this is going to be used for so I can't offer anything more specific). Like a service that you interact with. As you see it's a tricky question and you need to answer some of these questions before you can come up with a good solution. 

to make sure resources are properly closed in all scenarios. Performance Here's something you can try. Consider this you're taking \$M\$ steps and have \$2N\$ possible ways to go each step. Assuming for a second that the grid is so large that you never exit, then you have \$(2N)^M\$ different ways to step inside the grid. It might be easier to calculate in how many ways you can end up outside the grid... Better algorithm hints Lets look at a simple version of the problem see if we can't find some ideas. Let \$N=1\$ and \$M=3\$ and lets draw the states we can be in, specifically lets count in how many ways we can end up in each cell:                                        We start with step zero, there is only one possible state we can be in. Taking one step we can either go left or right so there is one way we can end up in either cell shown under \$M=1\$. From there we can either take left or right from both cells. So there are two ways we can end up back at the starting position and one way we can reach each edge. Try to do the steps yourself you'll see you'll get the same values. So by now you see how this is going. The number of ways you can end up in a cell is the sum of the number of ways you can end up in the neighboring cells. For \$M=4\$ this would be . There are two things to note here: First, the sum of all the values in the cells is actually equal to the possible routes you can take with \$M\$ steps. Take a moment to think about it, it makes sense, this is your answer (after applying suitable modulus). Second, notice anything familiar with the shape of the numbers? Yepp, that's Pascal's triangle with zeros injected between each cell! The answer for \$N=1\$ is the sum of \$M\$th row of pascals triangle after cutting off the edges due to the size of the grid (the zeros wont affect the result)! So solving this problem for arbitrary \$N\$ you just need to generalize Pascal's triangle to \$N\$ dimensions. You can easily do this by using a hyper-cube and using the above realization that: 

Thread Safety I'm not going to review thread safety as I'm not confident enough in the correct behaviour of the code. Addendum: Graceful Shutdown Requested in comments. To make a graceful shutdown when you may have other threads waiting on data on the queue you need two things: