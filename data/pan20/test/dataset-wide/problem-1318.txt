Since you're already requiring ECMAScript 5 you may as well use and to perform the iteration. This removes the need for (which @Dagg points out isn't necessary anyway), but it avoids the problem of modifying the underlying object's shape while iterating over it. I do not know which browsers, if any, would be affected by it. Combining this with the above yields 

I have some high-level advice based on a cursory read of your code. It would really help if you could refactor it into multiple methods as this function is too long and does too much. That won't help your speed, but it will make it easier for others to grok your code and provide more advice. Profile your code You can't hope to make things faster in an orderly fashion without finding out what parts take the most time. Yes, you can devise a faster algorithm, but you might end up spending days optimizing an O(n^2) algorithm to O(log n) only to find out that part took 1ms anyway. Split each word first If you only split at word boundaries, you may as well seek forward for a boundary and extract the substring in one call. Calculate word widths instead of letter widths First, when I wrote a similar function in Java many years ago, the cost of calling calculating the width of a string had a fairly high overhead that didn't change with the length of the string. Getting the width a character at a time was significantly more costly than calculating the length of a word. Also, since there may be kerning involved that adjusts the space between letters based on the letter pairs, you can't add up the character widths to arrive at the word width. If you end up needing to split a word midway, use a binary search to find the cutoff point This goes with the two above, but you won't really know the best route until you profile. Make sure you're not using a linked list You add the string to the list (?) and modify it for each character. If this is a linked list instead of an array, it will be very costly. I'm not very familiar with the STL if that's what you're using (those declarations would help), so maybe it maintains a tail pointer. In any case, I would build each line and add it to the list only once it's complete even if you're using an array-based vector. It just seems cleaner. Show a disabled scroll bar instead of restarting once you determine that scrolling is necessary Really, a disabled scroll bar isn't that horrendous. ;) Calculate the widest line as you wrap I'm betting that the work done in is very similar to what you're already doing in . Take advantage of that and maintain a maxLength local variable as you do the wrapping. 

Timing Variations The runtime of sorting algorithms can be greatly affected by the initial order of the elements. Each algorithm has a best and worst case initial ordering. For example, Bubble Sort requires \$O(n^2)\$ swaps when given the elements in reverse order and none when already sorted. Even by sorting one million arrays, when you run the program again you start with a new random seed which produces an entirely different set of random arrays. One seed may produce a higher percentage of worst cases. You can remove this element by choosing a random seed to use on every run with . In addition to that--and probably responsible for more of the outliers--you are calculating wall clock time. If the system swaps out your program to do other work, the clock keeps running. You can mitigate this by timing at the system level (e.g. using on Linux) which will track wall clock, CPU, and system time. Finally, the smaller arrays will involve a higher ratio of overhead compared to the actual sorting work. I recommend fewer repetitions with larger arrays, say sort 10,000 arrays of size 10,000. You may also want to consider creating initial arrays by shuffling the numbers \$(1..N)\$ to remove the unpredictable effects of equal elements. 

The only real problem I see is that the lists will grow unconstrained. Create a new list as a copy of the unacquired URLs. 

You'll need to decide if you want to combine some generic not-null checks with class-specific ones (name is not null) or use solely class-specific checks and duplicate the generic checks for each. One simple refactoring of the above is to extract the checks to : 

In addition to the great points by palacsint and barjak I have a few things to consider. There are two ways you could increase the concurrency. The only thing that must be serialized is the head/tail pointer update. 

One problem I see is that is shared across every click, but I suspect you want to bind it to each addition so that removing it removes the approach damage. To do this you'll need to store each value with an "added damage" object attached to the character stats. In order to remove it, you need a way to find it. If the UI works like a stack (removing always undoes the previous addition), you can maintain a global array. However, global state can be quite brittle and difficult to refractor later. Instead, embed the UI actions either in a UI controller (look up the MVC pattern) or the character builder class itself. If the player should be able to interact with each attack type independently, you'll need to separate the actions by type. The best model depends heavily on your desired UI capabilities so I can't recommend one over the other without more info. 

Since you're placing each URL into the array before making the GET request, the isn't even necessary. Separately, you can delay writing to the file until the end of the inner loop so you write one big chunk rather than one thousand tiny chunks. It may be that I/O is more costly, and perhaps you're getting killed by seeking to the end of an ever-growing file? Seems unlikely but maybe after a million writes it adds up. General Review There are a few things that could be cleaned up here as well.