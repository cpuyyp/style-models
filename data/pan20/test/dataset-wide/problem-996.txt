Look for warnings about CPU and/or memory nodes being offline. SQL Server Standard Edition only sees the first 4 CPU sockets, and you may have configured the VM as something like 6 dual-core CPUs. It'll end up hitting an issue similar to how Enterprise Edition's 20-core-limits cap the amount of memory you can see. If you want to share sp_Blitz's output here, you can run it like this to output to Markdown, which you can then copy/paste into your question: 

And count to 5. Within 5 seconds, SQL Server wakes up the deadlock monitor, who looks around, finds that these two queries are blocking each other, and...the select wins! The select returns data, and the update's window says: 

OPTIMIZE FOR UNKNOWN doesn't use a value - instead, it uses the density vector. If you run DBCC SHOWSTATISTICS, it's the value listed in the "All density" column of the second result set: 

The challenge here is that numbers don't take into account end user experience. Great example: I have database server used to track every web site that the company employees visit. I don't care if it can't keep up with inserts during peak loads because the front end app batches the inserts off periodically, and slow inserts don't cause a problem for users. Users are still able to surf the web without being held up by slow inserts. At SELECT time, the HR department just fires off reports when asked for suspicious browsing history for a given employee, but they don't care how long the reports take - they just open the report and go off to do other things. Performance needs to start with a question: are the users happy with performance? If so, leave the system where it is. 

The protection is that if someone gets a copy of this database (via stealing the backup, doing a SAN snapshot of the user databases, stealing the hard drives of the user databases, etc) and tries to attach 'em to another SQL Server, they won't be able to do it. It doesn't protect everything - it's only one part of the picture. For more info, check out the excellent book Securing SQL Server by Denny Cherry: $URL$ 

My bigger concern would be the fact that your databases are in simple recovery model. That means you can't do a point-in-time recovery. If something goes wrong, you're restoring all the way back to your last good full backup. Businesses are often uncomfortable with that much data loss. 

You mentioned that you're using SQL Server 2016, which means you can use Live Query Plans. In SSMS, click Query, Include Live Query Statistics, and run your query. You'll be able to see exactly which part of the query it's working on. Keep in mind that it's possible the query is only compiling, not actually executing. With really ugly queries - and the one you've posted just might qualify - you can see compilations that take hours. 

If both of those tables are in the same database, and we try to place orders while we're writing documents too, the performance of order transactions can be slower. Now, this is an extreme example, but generally this is why I recommend storing mission-critical, high-value, small-width-record tables in a separate database from less-critical, lower-value, large-width-record tables. However, if you need to restore both sets of tables to the exact same point in time, or fail them over together between servers, then they need to be in the same database. SQL Server doesn't guarantee transactional consistency between databases on failover with things like AlwaysOn Availability Groups. Q: If I have fast-changing data, slow-changing data, and never-changing data, should I separate them into different databases? Yes, for a few different reasons: 

I wrote the free (and open source) sp_Blitz for this exact reason. People kept handing me SQL Servers and going, "You're the DBA, you manage this thing." I needed something that could quickly analyze stuff like: 

As far as SQL Server is concerned, whenever results are going out the door, it's ASYNC_NETWORK_IO whether it's shared memory or TCP/IP. 

Let's break this question up into a few parts. Q: I need to insert 1mm rows a day. Is that a lot? Not really. 1mm divided by 24 hours divided by 60 minutes divided by 60 seconds gives you about 12 inserts per second. For a rough frame of perspective, it's not unusual to see 1,000 inserts per second in typical commodity servers with no tuning. Granted, your load won't be perfectly averaged out like that - you'll have bursts of load - but I wouldn't make database platform decisions based on less than 10k-20k inserts per second. Any platform out there will work fairly well. Q: How should I structure the data? Zoom out - don't think table, think databases. If you're going to be keeping this data permanently, and it's truly insert-only with no updates, then you probably want to start a new database for time lengths. Your inserts may only go into one table in one database, but every year, create a new database (MyApp_2015) and seal the old 2014 data as read-only. You can stop backing it up (as long as you've still got a good backup once), stop doing index maintenance, statistics updates, etc. The PHP will only ever have to know about the current database for inserts, making your design a lot easier. The archival process becomes a DBA task much later down the road as long as you go in knowing that there will be more than one database involved. If you were doing more than 1,000 inserts per second sustained, and you wanted easier performance management, then I'd also suggest building sharding into the initial design regardless of the database platform. Don't get me wrong, any modern database can handle over 1,000 inserts per second, but designing sharding in now just gives you more flexibility later on. At 12 inserts per second, it's just not worth the design/testing hassle. Q: How should I do reporting? In an ideal world, reports would not be done against the live server. Run the reports against a restored or replicated copy of the database. This does two things: it reduces load on the live server, and it validates your backups, guaranteeing that you've got your valuable data elsewhere. 

Change the name of the read-only filegroup - that way, if someone has hard-coded scripts that alter the filegroup by name, their scripts will fail, and you'll catch the culprit. Or, a little harder: Use Profiler or Extended Events to track anyone who does an ALTER DATABASE. 

Here's my Perfmon tutorial for SQL Server: $URL$ For more counters and thresholds, here's a poster we did when I was at Quest: $URL$ 

You have a bunch of different questions in here. Q: What is the "Always On" thing? Microsoft uses that brand name (which was written without a space before 2016) to describe two different features: 

No. JSON support was new in SQL Server 2016. If you want to convert data to JSON, you have a few options: 

For automatic failover, you need a tie-breaker. Otherwise, what would happen when the two servers had a network split and couldn't see each other? You wouldn't want them both automatically promoting themselves to primary, and both accepting writes. Think about what would happen on a table with an identity field, for example: both servers could quickly end up with two different records with the same identity field. This is called a split brain scenario, and you always want SQL Server to fail into a non-available state when that happens. To break the tie, database mirroring can use a witness server, but it can be an absolutely free SQL Server Express Edition. Always On Availability Groups is different than mirroring in that it's built atop Windows clustering, which means it relies on quorum voting to achieve the tiebreaker. Normally, you would want a third Windows server involved, but good news: it doesn't have to have SQL Server installed. You can use a file share to act as a quorum witness, or even a cloud witness. You could even get fancy and remove the quorum votes from one of your servers - but if that server goes down, your entire cluster will go down. 

And all that was only the first cumulative update. Want your query results to be accurate? Get your patching on. 

If your table doesn't have a clustered index, then deletes don't deallocate empty pages by default. Your options are: 

Check out Rodney Landrum's SQL Server Tacklebox. It's a free book from Red Gate that includes scripts to do that: $URL$ 

Say you've got a 200GB database that you're dealing with, and you can't get enough storage throughput to keep your cores busy. It's not unthinkable to need not just 200GB of RAM, but even more - because after all, SSIS and SSAS really want to do their work in memory, so you have to have the engine's data available, plus work space for SSIS and SSAS. This is also why people try to separate out SSIS and SSAS onto different VMs - they all need memory simultaneously. 

By default, there's no index on the DisplayName field, so SQL Server has to scan the entire table, then sort it by DisplayName. Here's the execution plan: 

Here, you've already got a single field that satisfies all of those requests: CLIENT_ID, which you defined as an identity field. You could just start by using that field alone - adding SERVER_ID doesn't really buy you anything. However, I'm a little suspicious of the table design because SERVER_ID is only a CHAR(1). That gives you a limited number of servers to work with per CLIENT_ID. I don't know what you're trying to model, but generally an ID wouldn't be a single character. 

When we renamed it sp_BlitzFirst and went open source, we made several breaking changes, one of which was changing the history table's DATETIME field to be DATETIMEOFFSET instead: $URL$ I would use a new history table name, and then it'll get automatically created with the right datatype. 

There's nothing stopping you from doing that - unless you specify something about the table that enforces uniqueness. 

Instead of using maintenance plans, check out Ola Hallengren's free maintenance stored procedures. They're way more powerful than maintenance plans and they don't require SSMS (per your requirements) to implement. You can set them up with any T-SQL connection. 

If you monitor the Perfmon counter for Available Memory over time, and you consistently notice that most of the memory is going unused, then you can inch up max server memory to a higher number. 

Start with sp_BlitzLock. It's a free open source stored procedure written by Erik Darling that examines the built-in System Health Extended Events session in SQL Server 2012 & newer. You can download it from the First Responder Kit Github repository. To install it, just run sp_BlitzLock.sql to install the stored procedure, then run: 

But as you've discovered ANSI_PADDING is also involved - both the incoming & outgoing tables have to have the same setting. There is no way to fix this after the data is loaded. 

No, you don't need to rebuild indexes or update statistics. You can't do any writes to a readable replica at all. 

Not really. You can't update in bulk because you could be changing rows all over the table. This is where index design comes in (as explained in Louis's book). 

You've written a lot of stuff, but here's your main question: Why are my TPC benchmark numbers not what I expect? To find the answer for that, check your SQL Server's wait stats while your workload is running. They're stored in sys.dm_os_wait_stats, but only as a cumulative number, so I wrote sp_BlitzFirst to do differentials for you during heavy workloads. Run it like this: 

Hope that helps get you started. Also, just FYI, I edited your question to remove the part about sp_WhoIsActive being mine - it's not. It's Adam Machanic's. If you ever meet him in person, you should say thank you to him and buy him his drink of choice. 

The eager spool has to do with the size of data you're deleting. In this case, you're updating a large quantity of rows on a table with several indexes. You're also updating a field that's covered by a lot of nonclustered indexes, and SQL Server has to update all of those indexes as well. To avoid this operation, update less rows (as a percentage of the table) or index the table less heavily. 

I have a confession to make. Once, when I was young, I built an ETL process started with changing read-only filegroups to read-write, doing its ETL work, and then setting them back to read-only. So just in case you have a coworker who was diabolical like I was (I was young, I needed the money), you can test by: 

SQL Server databases start up, but then they need to analyze the transaction log to roll transactions forward & backward. This process can take anywhere from milliseconds to hours (even days!) in the event of a long-running transaction, many (think thousands) of user databases, or databases with lots (think tens of thousands) of virtual log files. If you only want the app to get in when recovery is done and the database is ready, have the application retry its connection. If you want the app to be able to get into SQL Server immediately, but maybe not be able to run any queries yet, set its default database to TempDB instead of a user database. It'll likely be online right away even when user databases are unavailable. 

Theoretically yes, practically no. If SQL Server can guarantee that a field is unique, then it can build execution plans differently. Rob Farley blogged an example with AdventureWorks where the unique index isn't even shown in the execution plan, and yet its existence let SQL Server build a better plan. If your data really does qualify for a unique index, then specifying it as unique will ensure that SQL Server always estimates 1 row to come back when you search for one value. That'll mean you're more likely to get more granular locks instead of table locks. However, if you're worried about lock escalation, it's probably because you're not searching for equality. You're joining to other tables, or using range searches. In those cases, the uniqueness of an index isn't going to save you.