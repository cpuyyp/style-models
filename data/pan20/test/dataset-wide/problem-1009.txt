We need to migrate the storage of our production database. What is the appropriate method of doing so and what specific/generic precautions do we need to take?? Database Configuration: Volume Manager - ASM using Raw Disks. ASM External Redundancy as storage is published from storage subsystemDatabase version - Oracle 10g R2Database Size - 5TB approx. Existing Storage (source): HP MSA 2312sa Dual Controller. Directly connected to our database server (no fc or ethernet switch). The Oracle binaries are also on this storage New Storage (Target): HP EVA6300 FC storage. This storage will be connected to the hosts via FC switches. Can we use host based storage migration like VxVM Plex attach/dettach to copy data from source LUNs to the target LUNs? Do we have to use Oracle RMAN backup and recovery method for storage migration? 

Help in sizing oracle 10g database sga We've an Oracle 10g r2 database, size = 4.5TBASM is used to manage the storage. It uses "External Redundancy" option on "Raw Device" (i.e. no os file system caching).Server Configuration is Intel Xeon 4CPU (8core per cpu) i.e total 32 cores; Total Internal Memory is 128GB. Operating System is Solaris 10 update 9. The SGA size is 80GB, if we increase the SGA to 84GB~85GB the cpu utilization shoots upto 99%~100%. With 80GB SGA the cpu utilization is 20%~50% depending upon the load. What is the relation between cpu cores and sga size? I used 4GB memory per core, is this a correct estimate for the Intel Xeon platform Why does the cpu util shoot to 100% with an increase in SGA?Is there a rule of thumb for sizing the oracle sga for oltp environment? vmstat output: r   b   w   swap          free         sr   s0 s1 s2 s3 us 0 26 27 128146676 6448872 115 0 3 0 94 19 1 74 31 123050360 4031544 0 0 2 0 56 16 0 83 31 122930884 3921080 0 0 0 0 31 10 1 75 31 122934416 3921480 0 0 0 0 61 13 0 78 31 122787016 3802200 0 0 0 0 104 25 0 84 31 122595368 3626344 0 0 0 0 110 22 0 91 31 122717320 3759480 0 0 0 0 108 25 0 88 31 122831348 3889744 0 0 6 0 111 27 0 79 31 122581408 3647472 0 0 0 0 101 44 1 84 31 122306816 3388964 0 0 0 0 71 27 The "sr" column reports zero for most of the entries (but the first where it shows a high value 115)Moreover the vmstat shows an unusually high blocked and swapped out processes. 

I am Avinash. I am willing to install an additional MySQL instance on a UAT server for testing purposes. I am using MySQL Community Server 5.5.58 installed on RHEL7.2. I am going through the MySQL Docs $URL$ , which tells me that I would require certain unique operating parameters such port, socket, pid, datadir and log files related parameters. To achieve better performance, it also recommends to use different tmpdir. This is well understood. I can either use different binaries or the same binary for this purpose. If I am using same binary, I'll have to use mysqld_multi and use the same config file. If I use mysqld_multi, I'll have to put different sections for mysqld daemon such as mysqld1, mysqld2 and so on. All using unique values for above mentioned operating parameters. Please help me out here. Am I correct about mysqld_multi? My second question is about bind-address variable. $URL$ Here, it explains that My question is that will I need an additional NIC and an additional IP address for the second MySQL instance? For the existing instance, I have disabled the bind-address option. Do I need to add the bind-addresses for both the instances. Also, the second instance will be used only by me (for now). Thanking you. 

in the first command the author uses only --master-data and in the second command he/she uses --master-data=2. I don't understand the difference. Could you experts please help me out? Does --master-data=2 indicate that it's an incremental backup of level 1? Does it indicate anything related to master-slave configuration? What command should I use to make incremental backups? The second command again mentions backup of Sunday itself, that's why may be I am getting confused. The author uses Sunday as full backup (in the first command) and under incremental backup (for Monday) explanation (second command) also he uses Sunday in the dump file name. Thanking you. Regards, Avinash 

Enter in the user name you just created, and click the Check Names button. If it finds the user you will know, and click the OK button Now, assign it to a server role, or assign it a database with the permissions needed for your application. 

EDIT - I spaced it on what Matt was were trying to accomplish. I gave a query for what has played within the last 30 days. He is looking for what HASN'T played in the 30 days. I need to redo the query for that logic. I will get that updated and posted soon. Also, after evaluating the need for the 3rd table I spaced it there too. I'm truly winning at this whole data thing today. :) 

Now try to connect your application server to your SQL Server. Now, you can cheat. However, I wouldn't recommend this, but if you don't really care who is making the call from your application server. Run this script below. Change the domain to your domain, and replace SERVERNAME with the name of your server. Once this created on the SQL server give it perms to the server or database in question. 

As long as your AD account is on both servers, and has access to the database(s) in question this will work for you. Run this script on server A, replace LinkServerName with whatever you want to call it, and update the @datasrc aspect of this script to be your server B. 

The biggest table size is around 97 GB, for now, which is expected to grow more and more. I was asked to look into some compression options for all the 362 tables in order to reduce the size of the database. I took the biggest table which is 97GB in size and has 208 columns. On Oracle out of the 208 columns 142 columns were VARCHAR2(500) and 50 columns were VARCHAR2(4000) and when the database was migrated, because of row size limit imposed in MySQL, a lot of VARCHAR2 columns were converted to TEXT. I don't if the database was designed properly or not because rest of the tables don't have so many columns in them. When I tried to compress this table (on test server) using , there was no difference in the size of the table. KEY_BLOCK_SIZE=8 didn't work, it threw an error regarding row size limit, hence I chose KEY_BLOCK_SIZE=16. I took another table, which doesn't have any TEXT columns, and it got compressed to 50% and it doesn't have too many columns in it. I don't understand what could go wrong with the 97GB table. Is it because of the TEXT column or could it be anything else? Well I read that the TEXT and BLOB columns are best candidates for compression. Is there any other method to compress the TEXT columns? Thanking you.