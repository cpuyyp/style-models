B-tree indexes get less efficient to update as they get larger and have more tree levels. So indexes take longer to insert a the millionth value than they did the 10th. At certain sizes, you exceed certain thresholds which cause extra IO on the system, resulting in lag while the IO takes place. These thresholds, which interact in complex ways, include: 

The query planner tends to favor partial indexes which match portions of the WHERE clause exactly. If that doesn't work, I'd try creating an index which covers everything: 

"Seq scans" means full table scan. It's always going to read all of the columns on your table. It's a full table scan. PostgreSQL uses row-based storage, so it can't just read one column at a time. The reason your query takes so long is that you're reading 7.2 million rows in the table and aggregating them. Apparently, on a fairly slow computer. 

The most comprehensive way in my view would be to encrypt/decrypt the database with TDE. This will ensure that each and every page will change in memory and will be flushed to disk. I've tried this with success on 'legacy' dbs that were originally created in SQL2000, after I discovered that several pages didn't have actual checksums on them (0x...200) if you look at the header with dbcc page. If you were to try this, I would recommend testing it on a restored version of the live db, just in case you have undetected corruption that could be caught and stall the encryption process. There are flags to deal with it, but better play it safe. Obviously you'll want to backup the certificate used by the encryption, so you are covered for any eventuality during the time the db is encrypted. If anyone has a better idea for writing checksums on all pages, I'd love to hear it :-) 

Well, it all depends on your scenario. If it is a one-off import, you create a junction table and use it for your joins. 

To learn all you want to know (and what you’d rather not know) about sequences in PostgreSQL, read the docs. 

(Edited to be consistent with new version of question) You cannot if you want to single elements of the lists which make up the field. You have to disaggregate those lists in a subselect: 

The safest way is to define product_id's with the finest reasonable granularity, to the point that if you decide to sell half the stock at a discounted price, you should define a new product_id for the items in promotion, all things being equal but the price. In this way you will manage to balance sales and returns without too many corrections. So I basically agree with blobbles's answer. You will have many ways to group your products together, e.g. same name, same size, same producer, same provider, and so on, and you will do that in separate tables associating the product_id with those features. 

Trace flag 5004 is useful for starting/stopping TDE. Might be worth giving it a go in case it helps. DBCC TRACEON(5004) GO DBCC TRACEOFF(5004) GO 

Also if you want to restrict number of concurrent connections then Resource Governor comes in handy. Can even tailor it to specific logins only. 

Just came across this 2-year old link: $URL$ It implies that 64K NTFS cluster size is still recommended for SSDs To improve this answer it would be ideal to hear from real-life experience with latest generation SSDs (FusionIO or SATA-controlled). Maybe 256K is even better for columnstores on SSDs! 

There is enough info for you to estimate roughly how many pages/extents were lost (deallocated by the REPAIR_ALLOW_DATA_LOSS option). What good is that though? Without backups there is no natively-supported way to recover the data. What logs are you referring to? Transaction logs or Errorlogs? TLog entries need to be interpreted (not for the faint-hearted) and then applied to a consistent database file (which you haven't got). Errorlogs are useless for data retrieval anyway. 

You might have to explicitly cast the first row of the with the types of and if PostgreSQL doesn’t figure the correct ones out. E.g. 

However, the example you posted does not match the column names you gave later. This means you are not posting your actual code, and this makes everybody’s job harder. Maybe this is only a toy example to learn how to use MySQL triggers. As such, it’s fine. In production, the very idea to change the price in the main table upon insertion of a new promotion is questionable. A better design would be not to change the table, add and columns to the table, and read the current prices either from a view which applies the active promotions for the day, or, if you have millions of products and performance becomes an issue, from a temporary table regenerated each day (and possibly replicated to slave servers... you know your scale). 

If performance is important, Option1 has the clear edge. Every time a query goes across a linked server, performance takes a hit as what you expect to behave like set-based operations become serialised by OLEDB. They go row-by-row. It helps if you follow best practices for linked server queries (OPENROWSET for example) and ensure all remote processing is done on the other side, but the results will still come across serialised. (Look for Conor Cunningham's presentation on Distributed Queries). If all your remote tables are small lookup tables this may not be a serious issue, but if millions of rows have to come across linked servers then performance will suffer. There used to be an extra layer of problems with invisible statistics (for data readers) over linked servers, but as you're running SQL 2014 this will not affect you. If you can afford the Dev cost of eliminating the linked servers, Just Do It! 

You should read about string literals in MySQL. There you’ll learn that a backslash character is used to escape some special strings, and to have literal backslashes in your strings (as in Windows pathnames), you have to double them: 

Having such junction table might improve performance, and it allows creating indexes. If the junction data is dynamic in such a way that keeping a junction table in sync is too difficult (a very unusual situation, I’d say), we can avoid the junction table but, at the very least, using instead of arrays is way simpler: 

The complete list of characters in need of a special treatment is at the above link. However, it is best to have the intermediate language deal with such problems. For instance, in PHP you would call ; other platforms and languages provide similar facilities. 

I'd look at how you have your IO stack set up, which you say nothing about in the question. It sounds like actually committing the rows is taking a really long time. Virtual file systems on top of other file systems often display the worst characteristics of both layers. You also should try doing an actual test run, as well as just populating a database. 

The word LOOP is a reserved word in PL/pgSQL. If you want to use it as a column name, you need to quote it. 

I don't think copying pg_xlog after you've lost the master is going to allow you to make ServerB or ServerC the new master. If you might need to remaster, you really need to add log-shipping archives as well (this is a current limitation in Postgres, at least through 9.2). Also, if the master has been shut down cleanly, you don't need to copy anything. 

which outputs the SQL definition of the table or view, even with the original comments (exactly the same as , with less typing. works too.) 

To resume the situation for people who don’t want to follow the link to the original question on SO: the OP is querying a Mysql DB via an unspecified interface which (sensibly) refuses to show the values stored in fields with encrypted data, and (rather dumb-mindedly) still shows a label for the value returned by calling on them. The answer given on SO was to cast those results as , which from a DB perspective makes little sense, but in this way the interface shows the results as text and everybody is happy. The question here is: if I have to use , how can I handle longer values? My answer is: you don’t say which interface you are using (your screenshots are not enough for me to recognize it), but I’d bet that if you cast as instead of it still works and you have no size limits that you should care of. So: 

SSDs tend to have a natural 4K sector size as far as I know, but would it be beneficial to format Windows cluster size at 64K? This is good practice for spinning rust, but is it relevant on SSDs? 

So you've got 5 'stacked' instances on a single Windows server. You haven't said exactly how many sockets/CPUs are available and how much memory though. I like setting affinity for each instance in such cases, even if I decide to have CPUs overlapping in the struggle to balance the overall CPU load (depends on each instance's load). Any instance with more than 4 CPUs could use an explicit DOP setting in my experience - rarely over '4' in stacked cases like yours. Don't forget to set 'Cost threshold of Parallelism' for each instance to something reasonable (50?) to avoid excessive parallelism - in your case this is even more important. Remember that the memory left "for the OS" should be more now, since you have to account for the footprint of each instance (on top of SSIS etc). Check in SQL Config Mgr if SSAS is also running and adjust its 'max memory' accordingly, by default it goes for 80% of the whole server memory (!) Also maybe worth taking away and 'Lock pages in memory' rights of the SQL Service account(s) so that the OS can breath and do its job better (if it pages, everyone suffers!). Also good practice is to set some reasonable 'min memory' for each instance. I think running sp_blitz and sp_blitz_first on each instance would give you some quick pointers on more pressing issues. You may also want to monitor some windows permon counters like 'available memory' and 'working set' for each of the processes running there in case you find particular times of the day/night when the server is suffering. 

However, this is an interface question, not a DB engine question, and it is rather strange. I suspect the interface settings have been changed from the ones used when asking the first question, because now the strings which the interface thinks are binary are automatically displayed as . Giving the first query to a commandline interface does not produce the results shown here. To get them from a commandline interface I have to write the second field as ; similarly, the supposedly wrong result of the second query is simply instead of because the interface does not know it is actually readable text. Casting it to should work. Please be aware that all those conversions the interface is forcing you to perform are not needed when data is exchanged between different parts of your application.