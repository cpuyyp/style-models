Perhaps. It is proposed in Generative Phonology that a # boundary precedes and follows every syntactic constituent in an expresssion (including the words), and this with various conventions makes ## correspond roughly to the spaces in conventional spelling. The convention is given in detail in The Sound Pattern of English. (Personally, I don't believe it.) 

In sentence structure, V' modifiers are adverbs, which correspond in NP structure to adjectives and other N' modifiers. 

Two VPs have been coordinated, "[walk into a bar] and [see whom]', and a relative pronoun has been extracted from the second conjunct. This is an exception to Ross's Coordinate Structure Constraint of the sort Ross mentions here. 

I think you're asking about how to learn syntax, but I'm not sure. If you are, I think McCawley's text is excellent, The Syntactic Phenomena of English. The best part is the exercises at the ends of the chapters. Also, the footnotes are crammed with information. I've used it as a text for several graduate courses in syntax, and up to the last two chapters, I've done all the exercises myself (it wasn't easy). I don't really believe in the syntactic theory McCawley pursues (a sort of neo-generative semantics), but it doesn't matter, because the book is mostly about sentence structure -- how you tell what the structure for various constructions is. McCawley's book is rather dense and the discussion is at a sophisticated level. I've found that most grad students don't like it very much (but they just don't know what's good for them). 

The sentence formed by combining an element with others is the scope of that element. (Sometimes the element which is said to have a scope is itself excluded from that scope, but including it comes closer to the original account given in Hans Reichenbach's Elements of Symbolic Logic. It doesn't generally matter which policy one follows,) For instance, in sentence logic, when negation is combined with a sentence to form a negated sentence, the scope of the negation is everything within that negated sentence. Similarly, when a quantifier is combined with a sentence containing a free variable to form a closed sentence, everything within that closed sentence is said to be in the scope of that quantifier. The term "scope" is often handy in discussing the logic of language expressions, but it does not have an exclusively logical sense. Really, it just gives a convenient way of discussing the sentence structure of a complex expression. For example, in "((If ((John leaves early) or (Mary stays late))), then (Henry will be angry))", where I've parenthesized the sentences, "or" is in the scope of "if ... then", but not vice versa. Statements about "command" relations (in Langacker's sense) can be formulated using the term "scope", and vice versa. An element A is said to "command" an element B when the smallest sentence containing A also contains B, or equivalently, B is in the scope of A. For instance, a condition on the reflexivization of a pronoun with antecedent NP could be given as either that the pronoun and the NP must command each other or that the pronoun and the NP must be in each other's scope. 

A change that responds to contrasts among neighboring sounds and reduces those contrasts is a lenition. Assimilations are lenitions, while dissimilations are fortitions. Assimilatory voicing of consonants between voiced vowels lessens the constrast between the vowel preceding the consonant and the consonant, and also that between the consonant and the following vowel. In the case of intervocalic voicing of obstruents, it may be that the assimilation winds up taking more articulatory energy, since the natural tendency of obstruents to devoice has to be overcome. 

In the very early days of transformational grammar, it was supposed that transformations could change meaning, and so your two examples could be given the same deep structure. The fact that they mean different things could be ascribed to a question transformation that applies in the derivation of one but not the other. In 1964, Katz and Postal published their very influential Integrated Theory of Linguistic Descriptions, which argued that the instances in which meaning-changing transformations had been proposed could be shown to be misanalyses, including the meaning-changing question transformation. Katz and Postal proposed that the deep structures of yes-no questions differ from their indicative counterparts by being introduced with "whether", which appears explicitly only when the questions are complement sentences. Many transformational grammarians adopted the Katz-Postal hypothesis that transformations don't change meaning, but seem to have taken it more as a convenient assumption than an empirical hypothesis (as Katz and Postal had proposed). In place of "whether", one often sees an abstract "Q" element assumed, as for example in McCawley's SPHE. 

The "word-star" grammar describes all the sentences of every human language with a single rule: S -> word*. The rule says that a sentence is a string of words. Of course, you need a lexicon, also. 

I don't see any difference between signified and reference. George Lakoff proposed that definite anaphoric constructions are based on identity of reference while indefinite anaphoric constructions are based on identity of sense. "Hank saw a deer before I saw {one, it}." "One" has the same sense as its antecedent "a deer", while "it" has the same reference. "Hank saw a deer before I did" is an indefinite anaphoric construction, with the missing verb phrase "see a deer" after "did" having the sense "see a deer" as its antecedent, which is the non-finite part of "saw a deer". If you follow the theory of these indefinite anaphoric constructions that they have an underlying form with two instances of the same indefinite expression, so that we begin with "Hank did see a deer before I did see a deer" and substitute "one" for the second instance of "a deer", or delete the second instance of "see a deer", then this analyzes identity of sense anaphora as being, really, identity of form anaphora. Then, since the form is the signifier, in effect this equates the signifier with the sense. 

Now that the question has been edited to clarify that it is really explanation you're interested in, I find that I have something to add to my previous answer. What's an explanation? It tells you why something happens the way it does, when you didn't know that before you heard the explanation. Obviously, there is no known theory which explains all known syntactic phenomena. In fact, it's tough to find a syntactic theory that explains anything at all. But not impossible. Consider the constraints on movement described by John Ross -- in particular the Coordinate Structure Constraint (CSC) and the Complex NP Constraint (CNPC). There is a theoretical explanation for these constraints. Now, stop me if you've heard this one. I said an explanation tells you why something happens, when you didn't previously know why. So, do you know why there is a CSC or a CNPC? Does your syntactic theory, DG, CG, Minimalism, or whatever, tell you why? If so, read no further, because I can tell you nothing you didn't know already. The reason for these two constraints (and others) is that there are no transformations. Note that although I've couched the matter rather theoretically, this is a factual matter. Speaker-hearers, by and large, find sentences that violate the Ross constraints to be unacceptable, and it's not obvious why. We generally have a choice between describing movement constructions directly with phrase structure rules or appealing to a movement transformation. Gerald Gazdar exploited a notation from Categorial Grammar to show how we can describe constructions that seem to involve movement: we append to a phrase structure grammar a finite number of "slash categories", which represent constituents out of which something has been moved, and a finite number of phrase structure rules to describe the composition of such categories. The numbers of categories and phrase structure rules have to be finite, because this is a fundamental restriction on phrase structure grammar. 

I have used a notation above that extends the non-terminal symbols of a CFG by adding suffixes to their names to add phonemic information, because I wanted to make clear that I was not changing a CFG into some other sort of grammar. But this notation conceals the fact that the above is just a GPSG metarule. In the notation of GPSG the book, Generalized Phrase Structure Grammar, I would have instead have described CF rules using features, instead of suffixes. Then, the structural description of the n->m/_p rule would be described with "foot features" which percolate up a syntactic tree and the structural change of the rule with a "head feature" which percolates down a syntactic tree.