If you are working with other people (or you want to remind yourself), you can use the data type to suggest that the data is only intended to take on a value of 1 or 0. 

If I do #1, I will have hundreds (or maybe a couple thousand?) of columns and many rows will have a lot of 'blank' spaces because a lot of data sets will not have data for each row (e.g. some data that is only available quarterly will always be 'blank' in columns for rows describing monthly data). Also, I think there will need to be a corresponding table for describing the sources and units (e.g. U.S. Dollars, Square Miles, etc.) for each dataset column. I am concerned that the large number of columns and blank spaces may be problematic. Alternatively, if I do #2, I will have very few columns but millions (potentially billions?) of rows and will have many queries with a clause searching to see if the column equals a certain string (e.g. ). Also, since all types of numeric values are stored in the column, the data type here will have to be pretty flexible (Maybe something like ?). I'm primarily concerned this may become slow with a lot of data. I am using MySQL. My Question: Is possible to say which of these 2 general table designs is most likely the better design choice if my primary concern is query speed? Or is it fairly clear that I should be approaching this in a completely different manner? 

I am creating a database table for storing numeric time-series data sets regarding cities that will be accessed via a web app. Primarily annual, quarterly and monthly data will be stored in it. There will be hundreds, potentially a couple thousand data sets, for each city. Some data sets may have data on a monthly, quarterly AND annual basis while other data sets may only be available on a monthly, quarterly OR annual basis. I am trying to decide between 2 different designs for the table's columns: Option #1 Option #2 ... With Option #1, a hypothetical row look like: 

I have a database table with 900,000+ rows, 20 columns, mostly string data. Using MySQL Workbench, I want to page through the entire contents of the table to see what exactly has been inserted into the table (I am reviewing it because I believe my code somehow accidentally only partially loaded the contents of a CSV file and I want to understand why and where the process ceased loading the entire CSV file). The problem is that, because of the somewhat large size of the DB table, running a "SELECT * FROM..." query with "Don't Limit" causes my computer to take far too long (over 10 minutes) to produce a result in MySQL Workbench's result grid and basically appears to freeze up. Is there a way to make the MySQL Workbench result grid load much much faster? Or should I be using a different tool? I'm a bit of a noob, so it's possible I'm doing something pretty stupid here. Thanks in advance! 

The TLog has come around and because their is free space at the beginning there is no need to increase the TLog size More modifications 

If you can install or already have the MySQL Utilities, then you can use the mysqluserclone script. Reference: 5.28 mysqluserclone â€” Clone Existing User to Create New User The command is then a simple: 

Looking at the above questions consider using a different cleanup time (e.g. ) to keep additional hours worth of Transaction Log backups on your database server's disks. Benefits 

Reference: Password Policy (MSDN / SQL Server 2012) Setting Policy (locally) So if you require an account to be locked out after 3 wrong tries, then you either have to define a local policy with or define a domain policy for account lockout. Example in secpol.msc 

Starters Setting the SQL Server option is not really a solution to fix high recompilation values in the query plan cache. It is however a good solution when your application is performing lots of ad hoc (hence the name) queries that run only once and which would otherwise pollute (waste) the query plan cache space. This can be, for example, an application that allows the users to select the columns of tables dynamically that they wish to see the results of. Main Course When SQL Server Database Engine (DBE) executes a query and this query has never been executed before, then the Database Engine has to determine how it will access the data. As soon as the DBE has determined the best way to access the data it will store this information in the Query Plan Cache (QPC), so that the users will benefit the next time the application performs the same query again (albeit, probably with slightly different values). The SQL Server DBE will search the QPC each time the application requires a statement be run. If the DBE finds an adequate Query Plan in the QPC then it will select that Query Plan to retrieve the data. If however, the DBE is unable to determine an adequate Query Plan in the QPC (not found or timeout value reached for querying the QPC), then the DBE will create a new Query Plan. This is the resulting compiles/s you are observing. Dessert The root cause however can vary. SQL Server may be under memory pressure and is unable to store enough compiled Query Plans in the QPC. The DBE will kick out old plans and insert the new ones. (Solution: Add more memory to the SQL Server instance) The application is indeed generating a large amount of queries that have never before been executed and/or have slightly different values than the Query Plans stored in the QPC. (Solution: Remove complexity in application) Answers to your questions 

Reference: Oracle DBMS_UNDO_ADV If you find your database doesn't have sufficient Undo Segments, then you might have to resize your UNDO tablespace to accommodate for the large amount of modifications/moves your MV is performing. 

etc. Best practice Consider deploying the solution to a development server as is (ok, modify the database you will be using) and then have a look at the individual scripts, jobs and tables. Familiarise yourself with the solution and then modify according to your requirements. And of course consider reading Ola's documentation on his site: 

I need to select PERSON_ACCOUNT.PAID_YEAR, PERSON_ACCOUNT.PAID_AMOUNT,PERSON_ACCOUNT.PAID_AMOUNT * multiplication of the YEARLY_RATES.RATE WHERE YEARLY_RATES.RATE_YEAR >= PERSON_ACCOUNT.PAID_YEAR. So for instance if I have the following data in PERSON_ACCOUNT 

EXP(SUM(LN(YR.RATE))) is just a trick to get the multiplication of the column values as there's no ready function available in Oracle. 

I think it's obvious what it does but to make it clear let me give a brief explanation. In my table there's DOCNUMBER column. I assign a value to this column in Before Update trigger. And in the above trigger, which of course should fire after the Before Update Trigger, I check if the DOCNUMBER has value (and if it's for the first time) and if it does, I write that value to another table called . The problem is, the above trigger sometimes, although very rarely, fails to insert the to the table. That is, sometimes I see a record in table with DOCNUMBER value that does not exist in table.The table has only one column called DOCID and it's part of a UNIQUE KEY constrsraint. Do you see any problem with this trigger? EDIT: I think you'll get a better idea if I post the BEFORE TRIGGER too: 

After the process is over, I run queries against all the tables. Everything goes fine with 9 of them, but with one of the tables the executed query hangs. The next day, when we run the same query against the same table, the execution takes only a moment. I have no idea what the problem is. I tried not adding indexes but it didn't help. I know he way we copy table content might seem weird. 

Sorry for the title but I could not find anything better that could suit my needs. I'm designing the database of a Document Management (Circulation) application. There are two types of documents - A and B. I've created a table one for each. Any document independent from its type has Instructions information that has to be attached to it. Here are the details of the Instrcutions: 

I know that there's a one to one relationship between the document types and Instructions, so according to database normalization I should not have a separate table for it. But on the other hand I'll have to include the same columns in two different tables. What do you think? P.S. I won't be able to create a foreign key constraint between Instructions table and the two other. 

I've been asked this question but neither I seem to be able to answer it on my own, nor can I find anything related on the web. So what are the cases that might cause a runtime exception when committing a transaction in Oracle?The only thing that I can think of is the low disk space. Are there any other? 

I have two schemas namely A and B. I regularly copy contents of 10 tables from A to B. Here's is how I do it 

I have two tables, namely Employees and Payments. Below is the contents of the two: The Employees table 

The title was best I could find to explain my question, I don't think it helps though. Anyways, I have two tables: