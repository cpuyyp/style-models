OOM is usually a result of bad my.cnf config and actually your my.cnf overcommits memory big time. Run mysqltuner.pl to get a sense of what parameters needs tweaking. Many of your configuration values are way high and some of them doesn't even make sense. For example: 1) binlog_cache_size 3Gb? default value is 32768 bytes(!) According to dev.mysql.org: 

These are the most common situations for splitting I can think of right now. The question is do you experience any issues right now? Is there an explicit need to fix something? Can it be done by other ways of optimization? Configuration, indexes, query tuning, etc? Testing You can test your query time by replaying queries from tcpdump or slowlog. You can also enable binary logs and replay insert/updates from ther eand see how fast they finish. This is going to be a balance between many things including operability of the system so find what's work best for your use case. I hope this helped. 

One remark on : in most cases you will find that it is quite expensive. Although very useful for OLAP type scenarios it might not be very well suited for OLTP because the operation can take significant time of your query and as you can see sometimes the suboptimal execution plan is actually faster. Fortunately MySQL provides switches for optimizer so you can customize it as you wish. For all the option you can run: 

SQL alchemy returns a list of tuples. This is the standard behaviour. A row in PostgreSQL is represented as a tuple. Even if you query a single columns it's going to be a tuple with one element. Hence the representation. If you need a list of the first column you can easily transform your resultset in python: 

The optimizer evaluates the possible gains of using indexes vs doing a full scan (filtering unwanted rows on the fly). As the ratio of filtered and total rows gets closer to 1 the benefit of using index decreases. The exact tipping point is dependent on the actual data, query, etc. so it's hard to say an exact number when it becomes useless. Generally speaking a 50/50 split is not sufficient for a B+TREE index to work efficiently and most cases full scan will be preferred over using indexes. (Unless it can be used for index-only scans) The index is still getting updated regardless of its usefulness in queries. 

It depends on your table structures, data layout and usage pattern. However I wouldn't consider 12 gb or actually anything that can fit into the RAM of a reasonable priced commodity server big data (~100-400 Gb). I work with MySQL servers happily running with terabytes of data. If your tables are well designed, indexed and your queries are well written you won't see issues. After that it's usually much easier to split your dataset by common usage pattern and logical dependencies. Move tables to a different database. For example split your log tables to a new server. You can repeat that as long as want until you reach the write limit of a single server and your cannot split your database anymore (all tables are strongly related). Then you can think about sharding. 

You can always check MySQL manual if a variable is dynamic: $URL$ Unfortunately is not. So you have to restart MySQL to make this variable picked up. 

The insert time could only decrease not increase by having more tables. Insert (and update) time can be improved by having more smaller table because the B+Trees become smaller. Given which storage engine you use it also matters if you insert in primary key order or random order. In case of InnoDB if you insert in random order there is a performance penalty of constant B+Tree rebalancing. In that case smaller tables could help. However it does involve more logic on the application side and I don't know how much overhead that would be for your developers. Table partitions could be a middle ground here. Do you delete from these table frequently? If you do purging whole tables (or partitions) is much easier and can be done without affecting your production queries and replication (if you have) while has to lookup the rows, it involves some locking (level is dependent on storage engine dependent), etc. In this case splitting is certainly beneficial. Do you want to archive the data? MyISAM tables can just be simply copied over to another mysql server and it will just work. This is a very common and easy way to archive old data to a different server. Splitting to tables by date work quite well in this case too. Do you change your table structure frequently? ing many smaller table is more manageable than one huge. 

In the background this history is being cleaned up depending on how old your oldest transaction is. If you have long running transactions the history can grow pretty big. 

Ibdata1 file holds a lot of information and allocates space for different innodb functionality (double write buffer, undo logs, tablespace header, etc.). Therefore even with you cannot drop/replace the ibdata1 file. One common reason why ibdata can grow is because it has the undo space. If you have long running transactions the previous version of rows can pile up in the undo log easily. More about this can be read on person's blog: $URL$ In MySQL 5.7 there are a lot of improvements around undo tablespace management including truncation and separate files to hold this. 

Use Innodb unless you have a specific reason not to. If your coming from Oracle you will find its internal behaviour familiar therefore learning curve is shorter. For what you described you probably will want to enable (check which format suits your use case better: , or ). For hot backups I recommend to use percona-xtrabackup. It can stream data directly to a remote server so you don't even have to have double the free space available on the backup host. The documentation is very good and detailed and you can also find many source online. I hope this helps getting you started. Good luck! 

If it is not then you have to restart MySQL to make it happen. Oracle is working on getting more and more variable dynamic and you can see the trend going through 5.5 -> 5.6 -> 5.7. Unfortunately though there's always going to be parameters which require restart. 

Timestamp would be a great candidate for PK if you can keep it since it's most likely will participate in most of your queries and most like as a range. (PK is at the end of every secondary key). You could also experiment with the an instead. Use whichever gives better performance. Size Try to keep the row size as small as possible to avoid sharding/partitioning/archiving as long as possible (eventually you will need to but MySQL is quite good with tens of millions of rows even). To achieve this use only IDs in this table and have lookup tables where necessary. For example: instead of use . is 1 byte whereas action will be always at least 2 assuming , or similar small character set but longer the string becomes the more byte it will use obviously. can use up to 4 bytes / character. Only put index on columns where you really need to. Daily, monthly, etc aggregation tables will serve you better than heavy indexes on this table. A possible schema 

If you see [tablename].MYD and [tablename].MYI files it's MYISAM. If you see [tablename].ibd or only the .frm file then it's InnoDB. Based on that the process: MyISAM You can simply copy the .MYI, .MYD and .frm files to any existing MySQL instance's database directory. For example: You have your database in then you find the table which will be (.MYD, .MYI and .frm). You can copy only these three files to an arbitrary database on a newly installed MySQL like . InnoDB It's possible to import tablespace since 5.6 but if you can it's much simpler to copy the whole directoy. It's a common procedure to clone new replication slaves. In your case you copy the whole directory onto the new server and start MySQL over it. Make sure the innodb_log_file_size and innodb_log_files_in_group matches the size and number of what you have now. These are the files called and .