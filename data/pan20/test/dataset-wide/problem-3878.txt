I think your formula is not correct. The right kernel is invariant by interchanging $x$ with $y$. This symmetry must be preserved. Then, note that $$ L=-\Delta+ax^2+bx=-\Delta+a\left(x+\frac{b}{2\sqrt{a}}\right)^2-\frac{b^2}{4a} $$ and this operator is invariant for translations. This means that the kernel for $b\ne 0$ can be obtained from the kernel for $b=0$ by translation of a shift $\frac{b}{2\sqrt{a}}$ and multiplying for the overall factor $e^{\frac{b^2}{4a}t}$. Once this is observed, we can consider the kernel for $b=0$, just the argument of the exponential, to be $$ S_0(x,y;t)=\sqrt{a}\left[(x^2+y^2)\coth 2\sqrt{a}t-\frac{xy}{2\sinh 2\sqrt{a}t}\right] $$ and apply the above translation to get $$ S(x,y;t)=\sqrt{a}\left[\left(x^2+y^2+(x+y)\frac{b}{\sqrt{a}}+\frac{b^2}{4a}\right)\coth 2\sqrt{a}t\right. $$ $$ \left.-\frac{1}{2\sinh 2\sqrt{a}t}\left(xy+\frac{b}{2\sqrt{a}}(x+y)+\frac{b^2}{4a}\right)\right] $$ and this gives the right kernel $$ K(x,y;t)=\left(\frac{\sqrt{a}}{2\pi\sinh 2\sqrt{a}t}\right)^\frac{1}{2}e^{\frac{b^2}{4a}t}e^{-S(x,y;t)}. $$ 

This is a stochastic differential system of equations. I think this can help. You can also check the full syllabus of the course here with the proper references and other material to download. 

Given Tanaka sde $$dX_t=[a{\rm sign}(X_t)+b]dW_t$$ is there associated a diffusion process and so a Kolmogorov (Fokker-Planck) equation? What is this equation? References answering the question are welcome. Thanks. 

There are two different views about the semiclassical limit in quantum mechanics, the first is based on a somewhat shaky ground due to the fact that the existence of the Feynman integral is not proved yet. On the other side, Wiener integral, its imaginary time counterpart does exist and one could pretend to work things out from this and then move to the Feynman integral. The other approach relies on substantial mathematical theorems due to Elliott Lieb and Barry Simon in the '70 and is essentially valid for many-body physics. These latter results make the limit $\hbar\rightarrow 0$ and $N\rightarrow\infty$ equivalent while the former is not really a physical limit due to the fact that Planck constant is never zero. Starting from Feynman path integral, the standard formulation applies to a mechanial problem described from a Lagrangian $L$, normally $L=\frac{\dot x^2}{2}-V(x)$ but one can extend this to more general cases, and then the postulate is that, given a path $x(t)$, this must contribute to the full quantum mechanical amplitude of a particle going from the point $x_a$ to $x_b$ with a term $e^{\frac{i}{\hbar}S}$ being $S=\int_{t_a}^{t_b}dtL(\dot x,x,t)$ the action. All the possible paths contribute and so, the full amplitude will be given by the formal writing $$ A(x_a,x_b)\sim\int[dx(t)]e^{\frac{i}{\hbar}\int_{t_a}^{t_b}dtL(\dot x,x,t)}. $$ Be warned that this integral is not proved to exist yet, but the Wiener counterpart, that can be obtained changing $t\rightarrow it$, exists and describes Brownian motion. Now, if you take the formal limit $\hbar\rightarrow 0$ to this integral you will immediately recognize the conditions to apply the stationary phase method to it. This implies that the functional must have an extremum and this can be obtained by pretending that $$ \delta S=\delta \int_{t_a}^{t_b}dtL(\dot x,x,t)=0 $$ that is, the paths that give the greatest contribution are the classical ones and one recover the classical limit as a variational principle as learned from standard textbooks. While this is a quite common approach, to extend what really happens to a macroscopic system that we can see to respect all the laws of classical mechanics, we have to turn our attention to the limit of a large number of particles $N\rightarrow\infty$. In this case one has more rigorous results. These are due to Lieb and Simon as already said above. They published two papers about Lieb E. H. and Simon B. 1973 Phys. Rev. Lett. 31, 681. Lieb E. H. and Simon B. 1977 Adv. in Math. 23, 22. In the first paper, their theorem 4 states Theorem: For $\lambda < Z$, let $E_N^0$ and $\rho_N^0(x)$ denote the ground-state energy and one-electron distribution function for N spin-$\frac{1}{2}$ electrons obeying the Pauli principle and interacting with $k$ nuclei as described above. Then (a) $N^{-\frac{7}{3}}E_N^0\rightarrow E_1$, as $N\rightarrow\infty$; (b) $N^{-2}\rho_N^0(N^{-\frac{1}{3}}x)\rightarrow\rho_1(x)$ as $N\rightarrow\infty$, where convergence in (b) means that for any domain $D\subset R^3$, the expected fraction of electrons in $N^{-\frac{1}{3}}D$ approaches $\int_D\rho_1 (x)d^3x$. Where $\rho_1(x)$ and $E_1$ refer to the Thomas-Fermi distribution and the corresponding energy. This theorem states that the limit $N\rightarrow\infty$ for a quantum system, under some mild conditions, is the Thomas-Fermi distribution. A system with this distribution is a classical system. The fact that a system with a Thomas-Fermi distribution is a classical one can be seen through the following two references: W. Thirring(Ed.), The Stability of Matter: From Atoms to Stars - Selecta of E. Lieb, Springer-Verlag (1997). L. HÃ¶rmander, Comm. Pure. Appl. Math. 32, 359 (1979). The second paper just gives the mathematical support to derive Thomas-Fermi approximation as the leading order of a classical expansion for $\hbar\rightarrow 0$ that I will not present here. 

The classical Besicovitch covering lemma (BCL) asserts that for any $d \geq 1$, there is a constant $N(d)$ with the following property. If $A \subset \mathbb{R}^d$ is any subset and $r : A \to (0,R]$ is a (bounded) function ($R < \infty$ is fixed), then there are (at most) countably many points $\{a_j\}_j$ such that $\{B(a_j, r(a_j))\}_j$ is a cover of $A$ with multiplicity at most $N(d)$ (i.e. for any $j$, $B(a_j, r_j)$ has nonempty intersection with at most $N(d) - 1$ balls $B(a_k, r_k)$, $k \neq j$). I should underscore the fact that the the multiplicity $N(d)$ depends only on $d$ and on none of: the 'ball size function' $r$, the bound $R$, or the set $A$. I am interested in a generalization of this fact to manifolds, replacing $\mathbb{R}^d$ with a (finite dimensional, smooth, Riemannian) manifold $M$ and Euclidean balls with geodesic balls. Unfortunately, it seems that the BCL is generally false in this setting, due to a result of Chi: indeed, according to that paper, if the BCL holds on a Hadamard manifold (i.e. everywhere nonpositive sectional curvature), then that manifold is $\mathbb{R}^d$ for some $d$. So, one must compromise to obtain a positive result. I think that it suffices to constrain the maximum ball size $R$: given a manifold $M$, the BCL holds for ball size functions $r \leq R$, where $R$ depends on $M$. For e.g., one can apply the Nash embedding theorem and the classical BCL, and this implies the following: for any compact $M$, there is are numbers $N = N(M)$ and $R = R(M) > 0$ such that for any $A \subset M$ and any $r : A \to (0,R]$, there is an at-most countable cover $B(a_j, r(a_j)$ of $A$ by geodesic balls with multiplicity $\leq N$. Question: Is there any way to tell what $R$ should be by looking at only local features of the manifold? The constraint $R$ comes from how 'folded up' the Nash/Whitney embedding is in $\mathbb{R}^d$, and I have no clue how to control it. So, this result is highly unsatisfying: for small enough $R$, the BCL is a local result (isn't it?), and so the multiplicity shouldn't have anything at all to do with the global 'folding up' of an embedding in $\mathbb{R}^d$. Instead, I suspect that one ought to be able to cook up a constraint on $R$ that depends only on some local feature, like the sectional or Ricci curvatures. Has anyone done this or know of prior work in this direction? 

I want to consider the following problem, which generalises the decision problem to decide if a given finite permutation group is a Frobenius group: 

In his classic textbook Foundations of the Theory of Probability Kolmogorov defines Independence a little bit differenent then it is usually done today. He denotes a probability space by $(E, \mathcal F, P)$, on page 9 he wrote: 

Every finite abelian group is the direct product of its cyclic groups of prime order, and every commutative monoid divides a product of its cyclic submonoids. Could these results generalized to locally commutative semigroups? Here a semigroup is called locally commutative if for every idempotent $e \in S$ the semigroup $eSe$ is commutative. 

And the open balls are the sets of the form $v \cdot X^{\mathbb N_0}$ for some finite sequence $v$. Now this metric could be extended, for this define $I_n(u,v)$ to be true iff every infix up to a length $n$ of $u$ is contained as infix in $v$ and vice verse, namely $$ \begin{array}{lcl} I_n(u,v) & \Leftrightarrow & \forall i \exists j : u_{i+k} = v_{j+k}, k = 0,\ldots,n-1\\ & & \forall i \exists j : v_{i+k} = u_{j+k}, k = 0,\ldots,n-1 . \end{array} $$ and define $$ d'(u,v) := \max\left\{ d(u,v), \frac{1}{2^r} \right\} \mbox{ with } r := \max\{ n : I_n(u,v) \}. $$ Here $d'(u,v) < \frac{1}{2^n}$ iff $u,v$ coincide in all their infixes (including the first $n$ symbols) up to a length of $n$. Now $d(u,v) \le d'(u,v)$, so that convergence in $d'$ implies convergence in $d$, and every set that is open with regard to $d'$ is open with regard to $d$ (and also for closed sets), so that the topology induced by $d'$ is a refinement of the topology induced by $d$. Now I want to get results about the open sets and the topology induced by $d'$, and their relations. I already know that $d'$ is not complete. In particular what closed sets with regard to $d$ are open with regard to $d'$ (I already know that not all $d$-closed sets are $d'$-open)? Any suggestions or hints how to attack this problem? 

Here we are concerned with the space $X^{\omega}$ of infinite sequences. Denote by $F_n(\xi)$ the set of factors (consecutive finite subsequences) of length $n$ and consider the set $$ K_n(\xi) = \xi[1\ldots n] \cdot X^{\omega} \cap \{ \eta \in X^{\omega} : F_n(\eta) = F_n(\xi) \} $$ of all words which share with $\xi$ the first $n$ symbols and all factors of length $n$. Then I want to show that there does not exists a finite number of sets $X_1, X_2, \ldots, X_m$ which represent these sets in the sense that for every $\xi$ and $n$ every set $K_n(\xi)$ could be written in the form $w \cdot X_i$ for some $i$ and some $w\in X^*$, i.e. for each $K_n(\xi)$ there is some $i$ and a finite sequence $w$ with $$ K_n(\xi) = w \cdot X_i. $$ I tried to prove that a finite number is not enough, but run out of ideas. Any ideas or suggestions? (A note which might be helpful, the $K_n(\xi)$ are $\omega$-regular sets, meaning there exists a finite automaton accepting them, and so contain an ultimately periodic infinite sequence, and two such sets are equal if they coincide in all there ultimately periodic sequences) 

Since $E^u, E^s$ are equivariant, the projections of $df_x (u + v)$ according to $E^u_{f x}, E^s_{fx}$ are exactly $df_x u$ and $df_x v$, respectively. So, $$ \frac{\| df_x v \|}{ \|df_x u\|} \leq \frac{\| df_x|_{E^s_x}\|}{m( df_x|_{E^u_x})} \frac{\|v\|}{\|u\|} \, , $$ where $m(A) = \min \{ \| A v \| : \| v\| =1\} = \| A^{-1} \|^{-1}$ is the minimal norm of a linear operator. The desired cone membership follows from this estimate. 

No. With $n = r = 2$, set $$X = \bigg(\begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \bigg) \, , \quad V = \bigg( \begin{array}{cc} 0 & 0 \\ 0 & 1 \end{array} \bigg) \, .$$ In particular, $X^T V = V^T X = 0$, the zero matrix. If you restrict to invertible square matrices, the statement is still false. Set $$X = \bigg(\begin{array}{cc} 1 & 0 \\ 0 & \epsilon \end{array} \bigg) \, , \quad V = \bigg( \begin{array}{cc} \epsilon & 0 \\ 0 & 1 \end{array} \bigg) \, .$$ Then $X^T V = V^T X = \epsilon \operatorname{Id}$ and $\operatorname{Tr}(X^T V) = 2 \epsilon$. So, your LHS is $$ 2 \| \epsilon \operatorname{Id}\|_F^2 + (\operatorname{Tr}(\epsilon \operatorname{Id}))^2 = 8 \epsilon^2 \, , $$ which can be made arbitrarily small, while your RHS is $$ c \| X \|_F^2 \| V \|_F^2 = c (1 + \epsilon^2)^2 \geq c $$ The moral of the story is that the matrix norm is submultiplicative (not the Frobenius norm as above, but bear with me) in the sense that $\| X V \|$ can be arbitrarily smaller than $\| X \| \| V \|$. In the end, your constant $c$ depends either on how $X, V$ map each others singular value spaces to each other, or on some lower bound on the lowest singular value for either $X$ or $V$. 

Writing this as an answer since it seems to constitute one... The book "Dynamics of Evolutionary Equations" by Sell and You provides several examples of evolution equations for third and fourth order problems. A perhaps harder and more general version of what is essentially the same theory is given in the book of Henry, "Geometric Theory of Semilinear Parabolic Equations". These books study evolution equations of the form $$ u_t = A u + F(u, t) $$ in a Banach space $X$, where $A$ is a sectorial operator on $X$, roughly, a closed (unbounded) operator whose spectrum can be contained in a cone about the real axis. This class includes self-adjoint and skew-adjoint operators on Hilbert spaces. This formulation is appealing to dynamicists because it provides a common framework for a large class of PDE. Typical applications include criteria for the existence of global (compact) attractors and bounding the dimension of these attractors, for which there is a considerable literature nowadays. 

Given a finite permutation group $G$ (a subgroup of the symmetric group on a finite set) in terms of its generators, what is known about the decision problem of deciding if $G$ is $k$-transitive for a given $k \ge 1$? What can we say about the complexity of this problem, and in what complexity class is it contained? 

A $\sigma$-algebra $\mathcal F$ over $\Omega$ is generated by an countable partition if there exits a countable partition $\mathcal B = \{ B_i \}$ of $\Omega$ such that $\mathcal F = \sigma(\mathcal B)$. Now let $\mathcal G$ be an arbitrary $\sigma$-algebra over $\Omega$. Is it possible to find $\sigma$-algebras $\mathcal G_n$ generated by countable partitions approximating $\mathcal G$, i.e. such that $G_1 \subseteq G_2 \subseteq G_3 \subseteq \ldots$ and $\mathcal G = \bigcup_n \mathcal G_n$? One naive idea of me is to take some arbitrary $A_1 \in \mathcal G$, and then set $\mathcal B_1 = \{ A_1, A_1^C \}$ and $\mathcal G_1 = \sigma(\mathcal B_1)$, then take some $A_2 \in \mathcal G$ with $A_2 \cap A_1 = \emptyset$ and set $\mathcal B_2 = \{ A_1, A_2, (A_1\cup A_2)^C \}$ and $\mathcal G_2 := \sigma(\mathcal B_2)$ and so on. But this will not work, for example if $\mathcal P(\mathbb N) \subseteq \mathcal G$ and I choose $A_i := \{ i \}$, then as the "limit" $\sigma$-algebra I will get $\sigma(\{ A_i \}) = \{ M : M \subseteq \mathbb N \mbox{ or } X\setminus M \subseteq \mathbb N \}$, which has not to be the the original $\sigma$-algebra with which I started, for example if $\mathbb G = \mathcal B(\mathbb R)$ if fullfills $\mathcal P(\mathbb N) \subseteq \mathcal G$. 

So my question, how this seeminly simple argument in the footnote works? When I have $n$ equations, dependent on $m \le n$ parameters, where these parameters underlie $k \le m$ degrees of freedom, why does it follows that $n - k$ equations are independent? Or more abstractly, if I have $n$ equations \begin{align*} y_1 & = \mbox{ equation on } x_1, \ldots, x_m \\ y_2 & = \mbox{ equation on } x_1, \ldots, x_m \\ \vdots \\ y_n & = \mbox{ equation on } x_1, \ldots, x_m \end{align*} Then if I know that the $x_i$ have $k \le m$ degrees of freedom, why does I know instantly that just $n - k$ equations are indepdent, i.e. the other ones follow from the independent ones? For the statement in Kolmogorov's book I have another explanations. If I have $n$ experiments $\mathcal U^{(1)}, \mathcal U^{(2)}, \mathcal U^{(n)}$ then by additivity I have for each single event $A_q^{(i)}$ the linear equation $$ P(A_q^{(i)}) = \sum_{ \begin{subarray}{l} 1 \le q_1 \le r_1, \\ \vdots \\ 1 \le q_{i-1} \le r_{i-1}, \\ 1 \le q_{i+1} \le r_{i+1}, \\ \vdots \\ 1 \le q_n \le r_n \end{subarray}} P(A_{q_1}^{(1)} \ldots A_{q_{i-1}}^{(i-1)} A_q^{(i)} A_{q_{i+1}}^{(i+1)} \ldots A_{q_n}^{(n)}) $$ i.e. summing over all other sets except the one I am looking at. This gives me $r_1 + r_2 + \ldots r_n$ linear equations. Fix some event $A_{q_i}^{(i)}$ from each experiment $\mathcal U^{(i)}$, $i = 1,\ldots, n$, then by using that the events in each individual experiment have to sum to one, and that the sum of all $r$ combinations have to sum to one I can replace the equation $$ P(A_q^{(i)}) = \sum_{ \begin{subarray}{l} 1 \le q_1 \le r_1, \\ \vdots \\ 1 \le q_{i-1} \le r_{i-1}, \\ 1 \le q_{i+1} \le r_{i+1}, \\ \vdots \\ 1 \le q_n \le r_n \end{subarray}} P(A_{q_1}^{(1)} \ldots A_{q_{i-1}}^{(i-1)} A_q^{(i)} A_{q_{i+1}}^{(i+1)} \ldots A_{q_n}^{(n)}) $$ by $$ 1 - \sum_{s \ne q} P(A_s^{(i)}) = 1 - \sum_{\substack{q_1,\ldots, q_n \\ q_i \ne q}} P(A_{q_1}^{(1)} A_{q_2}^{(2)} \ldots A_{q_n}^{(n)}) $$ and substracting one I see that this equation is linear dependent on the other, i.e. by adding all the other for the experiment $\mathcal U^{(i)}$ it could be eliminated. Therefore in the end I have $r_1 + r_2 + \ldots r_n$ linear equations, from which $n$ could be eliminated (one of each $r_i$'s equations). The linear equations have $r$ variables (the compound probabilities), which have to fulfill the additional requiremtn that they sum to one, so $r-1$ of them are independent. So I am left with $$ (r-1) - (r_1 + r_2 + \ldots + r_n - n) $$ parameters, i.e. variables that could be choosen freely, and where the others depend on. Cause I have used everything these no more could be choosen freely. A little example, if I denote the probability space by $(\Sigma, \mathcal F, P)$ and have two experiment (i.e. disjoint decompositions) $$ \Sigma = A + B + C = D + E + F. $$ I get the equations \begin{align*} P(A) & = P(AD) + P(AE) + P(AF) \\ P(B) & = P(BD) + P(BE) + P(BF) \\ P(C) & = P(CD) + P(CE) + P(CF) \\ P(D) & = P(AD) + P(BD) + P(CD) \\ P(E) & = P(AE) + P(BE) + P(CE) \\ P(F) & = P(AF) + P(BF) + P(CF) \end{align*} from which I get \begin{align*} P(A) & = P(AD) + P(AE) + P(AF) \\ P(B) & = P(BD) + P(BE) + P(BF) \\ 1 - P(A) - P(B) & = 1 - ( P(AD) + P(AE) + P(AF) + P(BD) + P(BE) + P(BF) ) \\ P(D) & = P(AD) + P(BD) + P(CD) \\ P(E) & = P(AE) + P(BE) + P(CE) \\ 1 - P(D) - P(E) & = 1 - ( P(AD) + P(BD) + P(CD) + P(AE) + P(BE) + P(CE) ) \end{align*} together with $$ 1 = P(AD) + P(AE) + P(AF) + P(BD) + P(BE) + P(BF) + P(CD) + P(CE) + P(CF) $$ the following $5$ independent equations are left: \begin{align*} P(A) & = P(AD) + P(AE) + P(AF) \\ P(B) & = P(BD) + P(BE) + P(BF) \\ P(D) & = P(AD) + P(BD) + P(CD) \\ P(E) & = P(AE) + P(BE) + P(CE) \\ 1 & = P(AD) + P(AE) + P(AF) \\ & = P(BD) + P(BE) + P(BF) \\ & = P(AD) + P(BD) + P(CD) \end{align*} Therefore from the $9$ "variables" $P(AD), P(AE), \ldots$ I could choose $9 - 5 = 4$ freely, i.e. are independent.