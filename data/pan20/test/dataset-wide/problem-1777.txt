One way would be to install a free VNC server/client on each machine. www.tightvnc.com . They would then login to each machine to see that machine. a free two way non setup way to see someone elses computer is $URL$ . However I believe this is only good for 1 to 1 at most. Now instead of having multiple machines login you can broadcast your desktop using $URL$ . However this only works on the LAN I believe. 

From my perspective I would just pop it on EC2, why do the extra step. I wouldn't use the HSQL memory database but set up a different one such as mysql as I have had a bad experience performance wise with the default memory db. Also, the default amazon AMI has been a pain imo to set up graphical programs as X Window System is not listed under yum grouplist; so you will end up having to do some manual config me thinks. As far as the amazon reservations go. On Demand is the most expensive hourly. if you run it all year your bill will probably be around 30% more. Then there are the 3 RI levels: light,medium, heavy. All require some intial money down. light means the least money down but also the least savings from on demand. heavy requires the most down but has the most savings hourly compared to on demand. If you going to run the instance all year round for sure then go heavy. As as far as performance of a micro goes. Well, it is the cheapest but if I remember its not guaranteed even 1 cpu; its more for burstable hits. If its going to be used lightly you might be able to get away with it, but if you got a lot of developers and they are going to be generating those reports you might wanna aim for something a little more beefier. It is a shame m1.smalls are still 32-bit. 

I'm surprised you were able to generate a certificate at all on their servers. Anyways, I doubt it'll work. You need to configure the web server to return that SSL cert and that probably requires admin priveledges that you don't have. So when you go $URL$ it will show their default SSL they set up on the site. Its ok to self sign your certs using your own CA in general. There is nothing less secure about them if you know they are yours. What you pay for from a CA is their signature saying your verified and webbrowsers store that CA's cert with the default value of trust. So it doesn't bitch like a self signed one does. However, again, I doubt you can do what you are planning since it requires access to places a normal web host shouldn't have. It is perfectly normal for most webhosts to charge for an extra IP and installation. 

if you want a script to start once at startup and theoretically it stays up then you should add it as a service to init.d with the appropriate runlevel set. Of course if it should go down then you would want to to come back up. to do that you can have a shell script run ps -aux | grep 'nameOfYourScript' like that. Of course don't include the grep command which will match as well lol. have that script check every five minutes with a cron like this */5 * * * * user checkScript.sh the checkScript you make could be written to start up the program. 

not quite sure what you mean. If you mean can a host set up a maximum file usage then yea. My host, hostgator has a 50,000 inode limit or something like that. that means any more than 50,000 files and I get banned. If you mean like filesystems if you can set that up quotas. You can in Linux, I am sure there must be some mechanic for windows as well. $URL$ 

You need 32-bit version of libraries to run 32-bit applications on 64-bit system. Unfortunately Redhat doesn't have package like ia32-libs which would install most of them, it is supposed that you should install all 32-bit applications with yum and it will install the appropriate libraries for you. If your application is third-party, try installing 32-bit version of each library it needs, they usually have .i586 suffix, so you execute something like "yum install libusb.i586". 

As you all probably know, ipv4 route cache has been removed in 3.6 Linux kernel series, which had serious impact on multipath routing. IPv4 routing code (unlike IPv6 one) selects next hop in a round-robin fashion, so packets from given source IP to given destination IP don't always go via the same next hop. Before 3.6 the routing cache was correcting that situation, as next hop, once selected, was staying in the cache, and all further packets from the same source to the same destination were going through that next hop. Now next hop is re-selected for each packet, which leads to strange things: with 2 equal cost default routes in the routing table, each pointing to one internet provider, I can't even establish TCP connection, because initial SYN and final ACK go via different routes, and because of NAT on each path they arrive to destination as packets from different sources. Is there any relatively easy way to restore normal behaviour of multipath routing, so that next hop is selected per-flow rather than per-packet? Are there patches around to make IPv4 next hop selection hash-based, like it is for IPv6? Or how do you all deal with it? 

I can't think of any tool that would do that out of the box. This is quite rare scenario, as you can't create correct two-way NAT mapping if you only change port. Do you really need just one-way traffic ? However you can always write your own netfilter module (it's not that difficult) and alter packet headers in any way you want. 

If you create backup using tar, it will be enough to copy all your files, however the problem will arise when you try to extract that archive into a live system. Some binaries or libraries may not update because they will be in use by the system, and you will end up in a mess. You should get some support from your hosting provider (for the new VPS). Usually they have some features for that case, for instance my hosting provider allows me to switch my VPS to "repair mode", in that mode new VPS is created with basic Linux software, and my VPS's disk is mounted there, allowing me to change it however I want and then leave repair mode. Or maybe you can just send your tar archive to the support team, asking them to extract it to your VPS when it's offline. 

You can setup transparent proxy on your client machine that will have your company's proxy as a parent and add authentication information when forwarding requests to the parent. You will need to install a proxy server that supports transparent proxying, I'd recommend squid; and you will need a firewall that will redirect your traffic to a proxy server, many windows firewalls can do that. Google for "squid transparent proxy", there are a lot of manuals. 

Well, things are a bit more complex. Modern hard drives don't just detect errors, they have some spare sectors and smart controllers that try to relocate bad sectors. That is, when you try to read some logical sector and it doesn't read at first time, the controller tries to read it several times, and sometimes it can read it after some retries; then it writes the data back to the spare sector, remaps logical sector to the new one and marks old sector as bad, and finally gives you your data. All those processes are completely transparent to the reader, you wouldn't notice any error. However this will normally be reflected in S.M.A.R.T statistics, and if this happens more and more often, you can see that the drive is going to fail before it actually fails. That's why it's really important to use SMART monitoring tools on your system. When a sector doesn't read at all, or the controler runs out of spare sectors, read error will be returned by the drive. Error detection is now pretty bulletproof, it uses some kind of CRC for sector data. When read error is returned, mdadm will see it, mark the drive as unusable and switch an array into degraded mode. 

I don't know what the best practice for kerberos is with regards to security. I was wondering is it a good idea to allow a kerberos server to be public so public servers can use single-sign on or is it something that is only reserved for internal lan. Also the kerberos server is an Windows Server that has other services on it as well. What would you guys do? 

I was wondering if I am editing a file such as /etc/hosts or /etc/sysconfig/network should I put a . at the end of the name. such as test.example.com. Is there a difference? Would anything break either way. 

I don't see why a temporary password should expire? If the user never logs in then it shouldn't expire because the user needs to select the new one so he/she knows it. According to this on the first access of a record by a user, the user will have to change it on first authentication $URL$ are you saying the user can ignore changing it when he first logs back in? 

I am not familiar with pip or easy install but my guess is that /opt/python2.6/include is not hardcoded in those two app installers. It probably just searches /usr/include and /usr/local/include. Try creating an alias to the python2.6 directory in /usr/include or something like that or check the flags for ./configure on those apps. You might be able to specify a directory. 

I don't know if this would work but you could try greping the contents of the cd for the string BUILD? 

I use HostGator, they've been helpful on their forums and are generally explicit about what they don't and do accept. 

You can write a script that parses the log file of say ftp daemons such as vsftpd and proftpd and do something once it finds a line that matches what you want. However it would be different for each daemon as each has their own log style. On top of which, there will be delay as you will probably have to poll the file to see if its changed. A perhaps more portable solution and better response time would be to make a PAM module(WAY MORE WORK THOUGH) A lot of daemons have PAM support built in. so when someone logs into ssh for instance it will query PAM and the PAM config will pick for the login whether LDAP or UNIX file. then it has like password modules like cracklib to check if it works. it short you can append this line to the pam config of the particular service once you make a pam module. session required /lib/security/pam_yourmodule.so $URL$ for a quick primer on PAM. 

I noticed that looking at registrars some top-level-domains had different rules it seems. Like on some domains I can get a 3 letter second-level-domain but others say minimum 4 letters. Is there supposed to be some standard rule or is it just each top level registrar makes their own rules? 

Someone correct me please if I am wrong, I could have sworn I read somewhere that class A Ip address owners basically owned that space and IANA had no way to get them back and it was possibly that they would be able to auction their own IP space? Is this correct and if so does anyone have a link to that source. 

Well if you got a ton of VMs running at the same time. having one VM hog all of the CPUs would be very bad no? With the CPU choice you could have 4 VMs each with dedicated access to one CPU without any context switching happening. 

Sounds like you might want a distributed file system $URL$ I had a Grid project and we had to built a compute cluster and decided to use OpenAFS which is the only one I know. It kinda sucks from a learning point of view cause its not easy imo. However that link I posted has a list of them. Some support replication and striping (fault tolerant and parallel. Just research those and pick one that you like).