In Hungarian: Simk√≥, Katalin Ilona, et al. "An Empirical Evaluation of Automatic Conversion from Constituency to Dependency in Hungarian." Proceedings of COLING. 2014. 

Brat rapid annotation tool (brat) BRAT's output uses standoff annotations (and non-tokenized text): Text: 

Petrov, Slav, Dipanjan Das, and Ryan McDonald. "A universal part-of-speech tagset." arXiv preprint arXiv:1104.2086 (2011). contains an interesting comparison of the number of tags between different annotated data sets in different languages: 

GeneTag (1) is a tagged corpus for gene/protein named entity recognition. The corpus is annotated as follows: 

I wonder what types of errors arise when automatically converting consistency parse trees to dependency parse trees, and how frequent they are. 

I am looking for pre-trained, downloadable English language models. Where can I find some good ones? 

The data set should contain texts annotated with entities and whether they are synonym/hyponyms/hypernym. 

I am looking for treebanks that contain both constituency and dependency parse trees. Both should have been manually annotated (i.e. the dependency parse trees shouldn't have been obtained from the constituency parse trees). 

I am looking for negation detection software or libraries. Linux, Microsoft Windows or Mac OS X are all OK. Any price and license is good too. I am mostly interested in the quality of the detection. I am aware of: 

Is there any structured database of the ACL Anthology (former website; new website)? By structured database I mean some SQL dump (NoSQL is fine too) that would contain all papers' meta information (venue, date, authors, etc.), and if possible the text as well. I am aware of the ACL Anthology Reference Corpus (ACL ARC) as well as the ACL Anthology Network (beta), but they don't contain database dump. 

This Theano tutorial provide a dialogue state tracking systems (slot filling) with a graphical model based (recurrent neural networks) dialogue state representation. You probably need a decently large training set though: on the Fourth Dialog State Tracking Challenge (DSTC4) last year, we (and other teams) unsuccessfully tried some neural networks but in the end a simple classifier with decent features beats them. (more details: Franck Dernoncourt, Ji Young Lee, Trung H. Bui, and Hung H. Bui. "Robust Dialog State Tracking for Large Ontologies". International Workshop on Spoken Dialogue Systems. 2016.) 

I am looking for corpora of extractive summaries. I am mostly interested in English, and any type of document. 

I am trying to use Apache cTAKES to detect negations in physician notes. I have loaded the provided analysis engine in the UIMA CAS Visual Debugger. However, when I run it, I don't see any annotation (except one ): 

Regarding the performances, see: How does Kaldi compare with Mozilla DeepSpeech in terms of speech recognition accuracy? 

Is there any publication venue for natural language processing focusing on reproducing results from other papers? 

For Hungarian: Szeged Treebank contains manually annotated constituency parse trees; Szeged (Dependency) Treebank contains dependency parse trees that were automatically derived from the constituency parse trees, then corrected by human annotators: 

I am looking for an exhaustive list of upsides of using dependency parsing over constituency parsing (if possible, with references for each upside). 

In the ICSI Meeting Recorder Dialog Act (MRDA) Corpus, what is the interlabeler agreement in the ICSI Meeting Recorder Dialog Act (MRDA) Corpus, when using the 6-class tagset? The 6-class tagset I am referring to is mentioned in (1), which indicates there exist a widely applied classmap that maps the dialog acts onto 6 distinct classes: statements (S), questions (Q), backchannels (B), fillers (F) and disruptions (D), and (Z) unlabeled. (2) gives the interlabeler agreement, but for all classes used in the original annotations. The provided data set seems to come with the consensus annotations, i.e. it doesn't comes with the annotations of each labeler, so I couldn't find a way to compute the interlabeler agreement myself. 

I am looking for corpora for text classification with annotated support phrases. Support phrases = phrases that explain why a text was given a label. For example, if the text is "The patient works at Firm X. He uses the computer all day long, and goes to the gym twice a week. He has 2 dogs." and the label is "Patient is a computer user", then the support phrase is "He uses the computer all day long". 

The number of tags depends on how much fine-grained the annotations are intended to be, as well as the language itself. The appendix contains a proposed mappings from language-specific part-of-speech tags to our universal part-of-speech tags. 

There are many reasons for using jargon, but the primary one is precision. Words that are used only (or at least most commonly) within a particular profession are defined accurately by practitioners of that profession to mean something precise. Hearing a common word can lead many people to THINK they know what it means, and be slightly wrong, whereas a word they don't know will have to be defined, and can therefore be defined precisely. In your examples, "desist" means more than just stop. It means something like "stop, and don't start again, and don't do anything else so similar that we can show it has the same effect". This is why the letter you send to someone using your intellectual property is called a "cease and desist" letter, rather than just a cease letter. Parity, on the other hand, is slightly less specific than equality. It suggests equality in one specific way, or mere connectedness, rather than actual equality. I don't know enough about medicine to know the specific meaning of onset, but it is highly likely that it is slightly different to "start". Even if you choose a word to represent something that is already accurately contained within a word in common usage, if the concept is important within the profession in question, it is likely that a jargon word may be coined in order to avoid the word's meaning migrating over time, as words in common use are wont to do. 

I agree with P Elliot above, that your two sentences are: with regards fact "X" (as an example, "swans are white") I know X, that you know as well. (I know that swans are white too.) vs I know that you know X (but may not know it myself). (I know that you know what colour swans are.) (As has been pointed out in comments, purely syntactically, the first version of "I know what you know" implies that I share all of your knowledge, not just a specific fact. However, I feel that in use, nobody would ever suggest that they know everything another person knows, and so there must be an implied context to the use of this sentence. The discussion around the differences between the two sentences, and other potential cases of this ambiguity still apply whether we are talking about a specific context or a general totality of knowledge, so this point is largely irrelevant to the rest of the discussion.) In thinking about these two a little more, I came up with this, that I think also explains the ambiguity well: 1) I can answer [a question/all the questions] that you can. 2) I know [a question/all the questions] that you can answer. In speaking each of the sentences, it would be emphasis that would separate the two. "I KNOW what you know" is different from "I know WHAT you know." Besides re-phrasing, which could be done in many ways, an example above in this answer, I can't think of how you could distinguish the two in writing. I am trying to think of how to classify verbs where this kind of ambiguity, and coming up blank. The reason that it works with "know" is that the object of "know" is a fact, and "the thing you know" and "that you know a thing" are both facts. Of course, with any verb, if the object of the verb, call it "X" is a "Y" then "the thing that you X" must also be a "Y", but "that you X a thing" is always going to be a fact. So a verb that can be applied to a fact in this manner, should be a possible candidate for this ambiguity. As you pointed out, it doesn't work with "think", because although you can "think" a fact, the phraseology is somewhat different. "I think what you think" (I think like you) vs "I think that you think [that]." (I know what you think) Attempting to force the sentences into the same structure to allow the ambiguity brings us back to using the word "know". One other word I've found that works is "say". "I say what you say" can mean "I say the same thing as you" or "I tell you what to say". "I suspect what you know" also has similar ambiguity - but only because suspect basically means "think I know" so the sentence "I think I know what you know" still contains your original sentence. I can't help with other languages, but I suspect that any with common forms in the declension of a noun, or a specific form for a verb when it's used in a noun phrase, will have this same feature.