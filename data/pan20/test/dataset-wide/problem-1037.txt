I'm stuck at setting transaction isolation level. Here's my scenario that happens in the application: 

I have table that has fields, and another table called , which has a one-to-one relationship to the table. stores information about the message, including its text, and subject, and the recipient, etc. 

But frankly, that doesn't seem the proper way for natural locking detection, because I've manipulated the default behavior using transaction statements. Do we have a tool, like to show us some information about locking of a given query? If not, how can we find out what type of lock a given query can apply on a database, because they get executed TOO fast to be detected. 

It turns out that the person migrated the jobs from old server to new server edited the jobs manually. One of the edit was to this was set to default log location on old server. On the migrated server it was edited to where the directory did not exist. (that person missed deleting the text in the path) On the other jobs (FULL and DIFF) the text had been removed so it was set like this and these both were working fine! 

I have setup a SQL Agent job to delete files older than 7 days. The script does it job when run through windows powershell window. However the same script does not work from SQL Agent job 

I'm trying to find out the position of each given record of a query in 0-100 scale. I use ranking function this way: 

How string comparison works in SQL Server Why comparison doesn't behave the same on one machine, and one platform, but different environments These 4 characters represent one human-understandable character. Why they are so abundant in Unicode character map? 

Get unprocessed messages (using the flag) Set their to true (in RAM) and update their status Do the business Set their to false (in RAM) and update their status 

Based on transitive relation, it means that SQL Server considers them all to be the same character. However, in other environments, say for example C#, they're not the same. What I'm confused about is: 

Also, I see another backup on the same database started 30mins later (Backup to VirtualDevice - I know this is AppAssure backup tool). From SQLSkills preemptive_os_waitforsingleobject, I see And this is blocked for 2 days - I don't think one of the session will terminate itself until I kill the other. However, LOG backups were happening without any issues. Only FULL backup and DIFF backup for that particular database was blocked. I killed the AppAssure session and all the long queue went away from AppAssure. I killed the DIFF backup as well. Now the only process left is this FULL backup with 100% complete with same and not willing to complete!! I had no other options but to kill the FULL db backup and restart backup jobs. And, the USER_DATABASES full refuses to start saying but showed nothing. Also, there was nothing in state. CurrentJobActivity in msdb showed that the job was active. Had to right-click-stop under jobs and then start the job again!! Any idea on how do we avoid this situation? (other than telling the IT-team to stop AppAssure?) [EDIT]: Adding the version 

Last night all of our systems failed and connections to the database kept dropping. We watched the log today and found out that this was the message: 

As you can see, this is the only information that I have. That a delete operation is timed-out at time X. I'm stuck at how to debug this and find out the cause. I don't know even from where to start. I'm using SQL Server 2014. 

Of course it can be solved in a minute of Googling around, via some tricks and scripts. However, the more we search the less we find out about the reason behind this. Why it has happened in the first place at all? What possible reasons caused it, and might cause it again? Was it because of an attack? We have no clue, and any help is appreciated. Update: in this post it's been argued that a policy check can make this happen. We haven't set a policy so far, and we do not even know where are the policies. Update2: We realized that some of the threads in some applications kept working, even though other threads and other applications were encountering message. Could it be that doesn't require re-authentication? How is that possible? 

P.S. The Service Account was granted rights and the old files are being cleaned up. Version Info: SQL Server 2012 SP3 CU8 / Windows Server 2012 

I just by chance added alert for Error 825 and ran sp_Blitz again. And the message for Finding "No Alert for Corruption" was not displayed for Error 823 and 824 !! After Alert setup: 

Server Config: RAM: 24GB - 21.5GB for SQL Server Processors: 8 Not much activity in this server - hardly 50 people access this SQL Server via Sharepoint. 

Is there a way to resolve this to make work on Domain3 SQL Server? Also, creating a group and adding the individual userID's from other domains worked too. We don't want a new group just for this. 

Of course the overall design and query is much more complex and more details are in action in selecting next new word for a given learner. Now, imagine that words list contains 100K words, and a learner has already learnt more than 5K words. Using the given query, this gets slower and slower and slower by more learners learning more words. Is there a better design for these types of business requirements? How to design for scale in this case? 

This works fine when runs sequentially. But when I run multiple instances of my application (concurrency), I see that some messages are processed twice or thrice. Here's what happens: 

And I verified by only setting up 823 and ignoring 824 and 825, still the sp_Blitz does not report for other 2 missing (824 and 825) !! 

Here is the scenario which I am working with IT team to get it right with no luck. Following are the users who work on a SQL Server. 

I had a situation where the Native Backups were being made on a Server. I happened to see in that there was a third party backup tool () that was also taking VSS (kind-of) . At some interval, the AppAssure (backup being made to ) was doing a and at some other interval it was doing a breaking the log chain. Is there any way() to know when a backup log chain is broken? Here is a screenshot of the situation from February. 

But when I look at the results, instead of seeing a column that starts from 0 and goes up to 100, I see a column that starts from 0 and goes up to 37.xxxx. Though BOL does not explicitly mention that the result is distributed over 0-100 scale, my understanding from the word made me use this ranking function. What do I miss here? 

And it has non-clustered indexes on and fields. I want to find what a has sent us in a specified period. Thus I run this query: 

I have an application in which some reports time out before getting data. We started increasing timeout througout our code in different places, and as our database is growing bigger and bigger, we see that we're repeating/duplicating more code. So we decided to increase database timeout for queries globally. But I have this feeling that this is wrong. Yet I'm not able to bring forward any reasons. Is it bad to increase timeout everywhere? Why shouldn't we do this? We're using SQL Server and Entity Framework. 

I am working with some old jobs and I found this code snippet. I am just wondering why would someone dump transaction log files from databases in to one file ? 

DatabaseBackup - USER_DATABASES - LOG: This job fails saying "Executed as user: Domain\XXXX-SVC. Unable to open Step output file. The step failed." The error is only with LOG backup job. The other DatabaseBackup jobs (FULL, DIFF) works just fine with same SVC account. So the service account have appropriate permissions. The Output File(Job Step properties-->Advanced) is F:\SQLAgentLog\ which is same for all jobs. Only problem is with LOG backup job. Has anyone else experienced this and is there any solution? Current environment: SQL Server: 2012 SP3 CU8 OS: Windows Server 2012 Note: This was working all good on a Windows Server 2008!! 

SQL Server database which is very large (4TB) is stuck in recovery state. Reason: Data center had a planned downtime and we had to turn the physical machine off and when it turned on, as I read in error logs, SQL Server couldn't access the files, because files were on a SAN machine, and probably things didn't went smooth. 

Begin transaction (serializable) Get unprocessed messages (using the flag) Set their to true (in RAM) and update their status Commit transaction Do the business Set their to false (in RAM) Begin transaction (serializable) Update their status Commit transaction 

As soon as I start this query, it finishes. Thus, I get no chance to see to find out what type of lock this query has applied on database objects. I can inflate this query with transaction statements: