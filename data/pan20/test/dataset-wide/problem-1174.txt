the General Journal and General Ledger are an aggregated summary of all transactions used for generating Trial Balances and the Financial Statements. The Account Receivable subledger receives entries from the Invoicing Journal (debits to AR by Customer Number) and the Cash Receipts Journal (credits to AR by Customer Number). 

The theory is all fine and good, but it only starts to really make sense once you understand the practice. The Lawyer's version of the first three Normal Forms is: 

I have often found the IN operator to be a source of performance problems in the past, and avoid it whenever I can (which is almost always). 

As an aside; Why on earth do you want to insert a two-character table identifier into every instance of this new field? The table that the row came form is immediately obvious from the table being inspected. If you really think that your data collision rate across a large number of sites is so extreme as to justify a scheme such as this, at least use a GUID, an already implemented technology, instead of this NIH design. Update The monotonic nature of the autoincrement key is made use of in some very efficient tally-table algorithms for running totals( at least) to enforce proper sequencing of running-total calculations. This scheme would invalidate the use of those algorithms. 

I detected that SQLAgent was not running on one of our servers due to a unexpected memory leak error that caused SQL Agent to shutdown. There were not alerts configured so one main job that was supposed to be running there was not. Is there a way to monitor SQL Agent from SQL Server itself? I've done lot of searches but can't find a way to do it. All I've found lot about monitoring jobs, schedules, etc. But not how to monitor if the SQL Agent stops working, does not restart or anything similar that prevents it to run. PS: I'm planning on asking another question for the memory leak issue 

We have an SSRS 2014 already in production and we want now use a secure site by linking it to a certificate (URL like "$URL$ So now the URL is fully qualified domain name(FQDN). We can access correctly to the report manager with this https URL. We have to register to access with the Windows account. The bug we have is that when we navigate, sometimes, we pass to an http page. Each time that we pass from an https to an http page, the browser (IE) ask to register again. For example, here we are on a https page. When I click right on a report -> manage: 

It seems that FLEXFIELD4 is NVARCHAR. If there is some other conversion involved that your code does not show, then that might clarify the issue. 

How much memory does SSIS need? (The real answer is: It depends.) However, Jonathan Kehayias has a formula that is pretty good, or so I think, since it works great for me. $URL$ The simple formula for reserving space for needs other than SQL Server in Windows is: Start with 1 GB, plus 1 GB for every 4 GB in the machine between 4 and 16 GB, plus 1 GB for every 8 GB in the machine above 16 GB. After that you should do some monitoring to determine if you need more or less memory for your SSIS packages. That is the it depends portion of your planning for memory use. If you see problem with your SSIS processes then you need to determine if more memory is needed. Because you have two instances on the same server this will likely require further tuning. 

-- Creates the user "shcooper" for SQL Server using the security credential "RestrictedFaculty" -- The user login starts with the password "Baz1nga," but that password must be changed after the first login. 

I guess I should use @@SERVERNAME and DB_NAME(db.database_id) someway but so far can't find how to do it, maybe is trivial but no luck find in it. 

Parallelism it not disabled after adding an index, so the reason is some where else. From your text I understand that you ran the query on a restored database on a different server (legacy server you say), is that correct? If yes, then I would check that both instances have same configuration, specially MAXDOP and the Cost Threshold for Parallelism. Check that underlying data is the same on both tests. As Shanky commented, the 2nd query shows a higher number of Estimated number of rows, 132432 against 99489. Also, as recommended on the comments, be sure that all statistics related to the query are up to date, if not, update them to be certain than both queries run with same information to get the same execution plan. 

Is there any security implications on this action I should be aware of? Other option is to use advanced properties of the linked server object on main server: 

I would suggest that you use Ola Hallengren's scripts, freely available and widely used. His website is: $URL$ 

If you have defined the Foreign Key constraints as then the Primary Key value that was changed should cascade down to all the Foreign Keys with that constraint. If you do not have the constraint, then you will need create scripts to complete the update. EDIT: Since you do not have the constraint, but you want to have that set up, it is a bit of work. SQL Server does not support altering the constraints to a new setting. It is necessary to iterate through each table that has a FK constraint to the PK table. For each table with the FK: 

The MSDN Page Configure Windows Service Accounts and Permissions has some detailed recommendations for the SQL Server 2014 service accounts. The subheading for configuring the services is at Service Permissions This is quite a detailed outline of recommendations for the SQL Server Service Accounts. These are good recommendations, however you can make your own decisions on configuration, based on your needs for security and for being able to manage the configuration. Truth in reading: Although we use many of these settings, we also have some variations that suit us. 

It can be as complicated as you want, no issues with that. My own recommendation, for better manageability, if the tsql you are planning to use on the job step is too long, better create a stored procedure. This way you just call the sp from the job step and you do all your modifications there. And on the job itself just take care of the scheduling. 

Well, it was difficult to solve, but with the help of our sysadmin and reading through lot of places, we managed to get it working. It was a mix of things, but the 2 main ones were changing some parameters on SSRS config file: 

is an value First time I see it and I've no clue on what is being achieved with this. I made some searches, read the definition of CHECKSUM on the tech pages and couple of sources more but can't understand the point of using here. I'm not looking for an answer that explains exactly this case (if possible, even better) but I would be satisfied with an answer explaining the point of this way to use . 

One thing missing from the sample tables above is and (or whatever you want to call them). Since you are tracking history, the dates are obviously valuable and can be used when querying older entries. Having a and a table does split out the information, but that is a mixed blessing, since the student's full information is split across two different tables. You might consider implementing like this: 

Since 'NUL' just throws away the data, this can help you test your tuning more quickly. NOTE: If you are using Ola Hallengren's backup solution, these parameters are also supported 

A SQL Server 2008 R2 mirror is not itself readable. If you are using Enterprise Edition, you can create a named snapshot database from the mirror. This allows you to create a point-in-time readable database that can be used for pretty much any query that does not do any updates. (Though you could use another database (tempdb or a workdb) to do auxiliary work that would not fit the read-only snapshot.) See: $URL$ When the snapshot gets 'old' enough you create a new snapshot database. You will eventually need to drop the older snapshot, but it is possible to support several snapshots. (This depends on the amount of activity in your database and server.) If you use Standard Edition and have the same need, look into log shipping using RESTORE with STANDBY. You will need to manage when the next restores get made in order to use the STANDBY feature. See: $URL$ It says of the STANDBY option: "The standby file allows a database to be brought up for read-only access between transaction log restores and can be used with either warm standby server situations or special recovery situations in which it is useful to inspect the database between log restores." Therefore, this method requires you to delay the restore of logs to match the schedule for how long you want to keep this point-in-time in action. A LOG Restore will end the STANDBY and restore to the following point, which could be restored with a new STANDBY period. Depending on your needs one of these approaches can give you a readable point-in-time database. No updates become visible in the database for the period of (a) the snapshot, or (b) the period between restoring updates. 

The Shipping Address is a property of each and every order, not of the customer. Even if it was a property of the Customer, it would be necessary to know where past orders were shipped to if a customer relocated. Therefore all that can be stored as a Customer property are the Default Shipping Address (and Default Billing Address), for pre-populating each order and invoice as they are created. This is not a denormalization, because it directly reflects and captures the required business rules. Now that the historical accounting records are taken care of, there should be a CustomerLocation table with FK's to three Address records in an Address table: Physical Address; Default Billing Address; and Default Shipping Address. The first of these records should be associated with an EffectiveDate, so that again a historical record exists as changes occur. 

This has the additional advantage of being data-driven; in the case that your masking needs to be amended, only a data change is required instead of a code change. 

Adding some SSD RAM to the server devoted to the indices of the largest tables; and Adding more disks (not more disk space, more actual disks) in order to have more heads and axles servicing the massive table scans. 

It would be interesting to understand why you want Level = 100 when you do not want the code to change. If you install SQL Server 2008 R2 the SQL Server features available for the system (such as compressed backups) will work without regard to any database's compatibility level. So, keep your database (a little while longer) in compatibility level 80. EDIT: Responding to your comment on Mirroring. Mirroring is a server feature first introduced (for production use) with SQL Server 2005 SP1. Since this is a feature of the server, it will work for all compatibility levels running on your SQL Server 2008 R2 server. Therefore, keep your compatibility level at 80 (SQL Server 2000) level and use mirroring on the databases where you need it. There is a lot of documentation on mirroring, but since this is new to you, you should start by reading Ron Talmage's white paper at: $URL$ Please note, however, that SQL Server 2008 R2 is the last version to support SQL Server 2000 (80) compatibility. And SQL Server 2008 R2 is no longer under mainstream support since 7/8/2014, though the limited extended support will continue into 2019. This means that for your SQL Server next upgrade you must upgrade your T-SQL code. END OF EDIT Since you mention DDLs and EXEs, you seem to be indicating that your SQL queries are in the client not existing as in the SQL Server. If you decide to write a wrapper code for use in your EXEs and DLLs and include it in all the locations. That sounds as disruptive as just changing the SQL Queries. Since you are using the *= syntax your code should look like this: 

Setting to I guess is the best way to free that space, correct? Then, issue is that after runnning the updates the size of the database remains the same size. I guess the space is not freed up inmediately. Found several references to a ghost process that is supposed to be running behind the curtains freeing space but it takes quite long time to regain all the space that was used by the column. Then found recommendations on running statement to force to free space and afterwards. Also some recommend running at the end of the process to update the reports related to used pages, reserved pages, etc. Yes, I read about the transaction log growing. My plan is to run by chunks, as recomended on BOL pages and as I've done with the updates. Running these commands will help me reclaim the space occupied by the image columns? EDIT Just ran the commands. Database size remains the same. Any advice? 

place that step where you need to stop job execution save the job, it will detect that some of the steps will not be executed and warn you about, select yes and you're done. 

You have forgotten the join criteria . If you had used the modern JOIN syntax this would have stood out like a sore thumb. 

A Non-Clustered Index on the child table by ParentID and TypeID will cover the subquery. An Indexed View on the subquery is possible also. 

If you decide to keep a single entry for both sides of a transaction, then by definition you are engaging in single-entry bookkeeping. This may be the most appropriate solution for some simple applications, but be clear that you are losing all the functional and robustness advantages of double-entry bookkeeping, in exchange for a simpler design. Note that when viewed stand-alone Subledgers (though not their corresponding Journals) are often implemented as single-entry, since a control-account in the General Ledger captures the subledger total and the balancing side of the transactions are in the General Journal (GJ) and General Ledger (GL). You also appear to be confusing the distinct concepts of Ledger and Journal in traditional double-entry bookkeeping. The various Journals (of which there will be numerous specialized varieties for specific common transactions of the business in addition to the General Journal) is a chronological history of all transactions entered into the system. The General Ledger is an ordering by account of all transactions entered into the system, and the various subledgers are an ordering by subledger-code of all transactions entered into the corresponding Journal. Two examples of common Ledger and Journal combinations: