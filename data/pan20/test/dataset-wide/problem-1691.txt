Unfortunately it does make sense, but I don't see how the card has much to do with it. The 5V / 12V line is provided by the power supply, and the drives might be drawing on 5V, which may be underrated or non-existant at all. So the drives won't spin up when the card tells them to. If you are very lucky, you only need to enable staggered spin-up in the card BIOS to lower the peak power consumption (you wrote it sometimes does boot). If that works, perform a random write load test before doing anything productive. I looked up your power supply, it's 560W which is a really low total (2U storage chassis have 900W or 1400W). I'm using a dozen Seagate Momentus 750GB in a 2U storage chassis with 900W and Adaptec 5445Z. Slightly different story, but the Momentus series is also "notebook class", so Seagate disabled the activity LED (now THAT's gonna stop me). The drives work very well (good overall IOPS). Can't tell anything about long-term, though. I planned to go for WD Scorpio next time, but now I'm going to reconsider this idea. Seems the manufacturers really don't want us to do this. 

SATA Interposer Cards are another solution. I recently experienced exacly the same fate and found this thread. The overall tenor is that SAS protocol is better suited for RAID than SATA, because SATA is lacking features. This is why the same physical drives are equipped with SAS controllers, then sold as Nearline SAS. Searching further, I found: $URL$ I'm investigating upgrading one of my storages with a batch of these. Right now, the price difference between 3 TB SATA vs SAS is 400% (vanilla price, same brand, specs and shop, Germany). I obviously can't tell if this strategy works out well, but it's worth a try. Comments very welcome :-) 

Longest answer, with a bit more of the detective work details: the breakthrough happened when we had this pathology occur in parallel on two separate setups. Checking out all the parameters on those setups and comparing, we realized that on the drive was showing this result: 

I would be happy to share/provide any other suggested findings. Secretly I hope that understanding this weird behavior would shed light on the pathology causing this whole mess. But, that's just my private hope :) 

In fact, I get the same error under multiple different folders that reside under , but not under directly, and not - for example - under . I can't explain it for the life of me, and can only resolve using . Any suggestions on how to fix the issue and be able to again without restart? 

And that when there was clearly more than 197GB of data on the disk. So, we tried out the two methods mentioned above (chkdsk and recreating the disk) on two individual clean setups, and on each of those the weird behavior would no longer appear. Our best guess is that at some point, when the AMI was created, something went wrong in the snapshot process - most likely because we had taken a "snapshot without restart" (though we don't usually, and I have no evidence to back this up, so I hope our DevOps don't go mad at me for blaming her without cause!). All in all, an interesting experience. 

I suspect caching at the load-balancer. Try adding anti-caching headers in the response and see if the problem persists. This may increase your server load alot. 

Server not found means DNS resolution failed. Check your DNS. Do you have both www.domain.tld and domain.tld setup? Do the IPs appear in access.log? 

If you're willing to step down to IP based user identification, you can expect openvpn to reassign the same VPN-IP for a user's lifetime. See assigned IPs in ipp.txt (/etc/openvpn/servers/VPN/logs, path may vary). Then, check if traffic passes through linux kernel. It might stay within the OpenVPN daemon, not sure about that. tcpdump and see what happens. If it passes through the linux kernel, normal IP accounting would work well (MRTG et al). If it does stay in the daemon, openvpn-status.log contains the external IPs of the currently connected users. Correlating that with MRTG data may be a tough exercise, though. So check if there's any config option for openvpn to force traffic through the kernel. You're using a Linux server, right? 

If you're using syslog, you can configure your log server to replicate logs live to another server (e.g. rsyslog) for real-time backup. Then backup all rotated files as already suggested for long-term archival. logrotate can be configured for custom applications, too, and can apply bash scripts on rotated logs. So you may skip /var/log in external backup tools altogether and copy logs to an archive directory which is more static. 

Try turning on error_reporting(E_ALL); and point your browser to the image URL. Comment the type header to see any error messages that may be hidden if your browser only displays a broken pict message. And fix that missing semi-colon Ben mentioned. Edit: And sanitize $_GET['src'] you may be opening a big security hole there. 

Running returns (62M), which could reach some OS limit; on a similar healthy system it's more like 1800000 (1.8M). The hugely occupied subfolder appears to be (~62M items compared to ~1.7M on the healthy system). This is likely just a reflection of my 100K fds (x2, for both fds and fdinfos) over 300 individual "task" folders. This appears at the end of my dmesg dump (my java pid in this example is 105940) - not sure how this might relate: 

Some pointers and clues that I've gathered: The setup is CentOS 6.5 running under AWS with the said tomcat disk mounted from an EBS volume. Running shows that the disk is evidently not full: 

The question: I have a tomcat running a java application that occasionally accumulates socket handles and reaches the ulimit we configured (both soft and hard) for max-open-files, which is 100K. When this happens, the java appears to still be alive, but we can no longer access it. However my question is about a bizarre phenomenon that accompanies this situation: I cannot inside the tomcat folder. 

Now, this didn't catch our attention before, because the disk still has plenty of space left. But it was the exact same disk usage (197G) on both setups, and that has no reason to happen. From here things quickly unfolded. As mentioned before, our AWS instances had been created from an image that has a disk snapshot of 200GB, which is extended on individual instances using - usually to the maximum size of 1TB. We were finally able to recreate a "bad state" by launching a new instance, resizing to 1TB, and creating a big file of 300GB. When this was done, the system didn't freeze, but it did show the same strange behavior: 

My understanding of SCSI timeouts is that any read, write, flush and other commands have a limited time to complete. If exceeded, the command is aborted and an error is reported to the upper layer. While waiting for the command to complete, any application depending on the I/O will stall. My next layer would be mdraid, the Linux software RAID. From what I read, mdraid has no timeouts on its own but relies on the lower layer to timeout commands. The default SCSI timeout value is 90 seconds for Kernel 3.2 (Debian). A hard disk encountering a read error will try hard to correct the error within a time frame defined by firmware. That timeout is set high for desktop drives (typically stand-alone, so correction has high priority) and low for server drives (typically RAID, so report bad sector soon, let other drive answer). Sometimes it can be adjusted via smartctl (SCTERC, TLER, etc.). So I guess if an HDD is set to a high ERC timeout, kernel will wait for 90 seconds by default before aborting the request. Only then will mdraid redirect the application's request to another disk. 90 seconds is a loooong time for a webpage to load. Is it correct to assume the default SCSI timeout is meant for desktop purposes or non-hdd SCSI equipment (tape drive, tape library come to mind), and safe to tune down to, say, 7 seconds for RAID usage? 

I have a 45-disk array of Seagate Barracuda 3 TB ST3000DM001 (yes these are desktop drives I'm aware of that) in a Supermicro sc847 JBOD, connected via LSI 9285. I have found a solution for the problem description below by reducing speed via 

I found the answer to my question of "how to fix this scenario". I don't know all the details of how this came to be, but I know enough to put out an answer. Short answer: unmounting the disk, running on it, and mounting back again solves and prevents the issue from reoccurring. As an alternative, creating a new disk (remember we're on AWS) and copying all data to the new disk ( was my command of choice) and using it to replace the original disk also solves & prevents. 

I am running into the following Linux error: . This seems to be caused by the system limit of 8 maximum link-chain size, and I'm looking for a way to increase this limit. Some background: I am writing a system which makes use of symlinks to pass on file resources between working elements. This results in long chains of symlinks (e.g. ). I am creating a chain intentionally, as I'm interested in preserving this structure of who-provided what. It should be noted that this system is physically disconnected from the outside world, so I have practically no concerns for security or exploit prevention. All help would be greatly appreciated! 

Longer answer: the disk file system (ext4) appears to have reached some unstable state when the disk's snapshot had originally been created. When later the original snapshot of 200GB had been extended (using ) to 1TB, it appears that in some sense it kept remembering internally the original size of 200GB, creating all sorts of weird phenomena which ended up with the OS unable to close handles, thus making the Tomcat reach its file limit, thus having all hell break loose. 

I'd like to use "ipmitool ... sol activate" to record any kernel panics that may happen. I managed to keep GRUB2 in text-mode with "GRUB_TERMINAL=console" in /etc/default/grub. But right after the message "Loading initial ramdisk" (which is still GRUB), when Linux starts printing text on VGA console, messages on the serial console stop. I guess either GRUB or Linux is doing something at that point, like loading a font, which kills off the vga-text-to-serial line. I'd like to keep it from doing that, whatever it is. I have tried so far: 

Then you can browse $URL$ and use the KVM as usual. The forwarded TCP ports are 5900 and 5901 for control and video, 5120 for virtual CD and 5123 for virtual floppy (I didn't test the latter two). Added -C for compression, though I don't know if anything sent is suitable for compression. Another, slightly more comfortable (and, in theory, better performing) method on linux, is to use sshuttle, which transparently forwards all TCP connections via ssh using iptables and a python interpreter on the proxy-server. 

D) A VPN solution would be superior in that it can be used for more than one service. But you'd have to bind the service to an internal IP which may become available only after starting the VPN service, thus complicating startup sequence. 

A) Once an SSH server listens on 8078 on the server, no other application can, Jenkins included. B) I get confused: SSH does not redirect unencrypted requests. C) I assume you want to do the following: