If I understand your goal (unassisted speech to text conversion, using Praat), this is not doable out of the box, but you might eventually develop the tools for doing this. An alternative interpretation is simply "how do I find phonetic landmarks, using Praat and my ears?", which is actually quite doable, if you know basic phonetics. Before you get to using Praat, you need to decide what the scope of your project is: any language, or a specific language? If you can't do it for a specific language (and then for 6 specific languages), you can't do it for all languages. If you know what the language is, then you can use that information to rule out possible interpretations of the signal. Within Praat, you have basically two problems, segmentation and identification (the latter assumes the former). The easiest case for segmentation in picking segments in a VCV sequence with voiceless stops, then fricatives, then voices stops, voiced fricatives, nasals, laterals, rhotics and glides. The reason why VtV is easiest is because a big change in the signal matches segment boundaries. The endpoint of the process is parsing similar vowel sequences such as [eɛ, ɨi], where it's impossible to find a crisp segment boundary (and the usual approach is to divide the contested zone in two parts). 

Maybe too late for you, but... The largest language-group of that type is Beti-Fang language (Fang, Bulu, Ewondo, Ntumu, Eton) which is probably large enough; otherwise, there are Douala and Cameroonian Pidgin spoken by a million or more people (and French and English, the official languages), but for the most part, you have to aggregate across languages to get a million speakers and then the languages are not mutually intelligible (such as, the various languages referred to as "Bamileke"). The safest best for Cameroonian languages is that any two languages are not mutually intelligible, and then you can list some exceptions like Ewondo-Bulu etc. 

IMO the English is better than the Indonesian, though both are 10 times better than the Afrikaans which is 100 times better than the Swahili (which is infinitely better than the Zulu). Icelandic is ignominiously the same as Swahili. Simply asking Indonesian speakers to judge the quality of synthesis for English vs. Indonesian is, experimentally speaking, not valid evidence regarding the quality of synthesis across languages. It's not at all clear what would be a valid procedure to test comparative quality, since there are so many variables to control (subject knowledge of the details of language A vs. language B; subject exposure to other synthesis of a language such as comparing original Klatt synthesis of English with Google synthesis). It might be best to use people with absolutely no exposure to the test languages (maybe impossible in the case of English), and play Real-X:Synth-X::Real-Y:Synth-Y sets (where "real" and "synthesized" are identified), which could get at subjective judgments of similarity. Even so, there would have to be good controls over the native languages of the subject (so that Germanic speakers don't overwhelm the subject pool, or Chinese speakers). We don't know exactly how Google does its synthesis, but it is likely that for some of the worse syntheses, they are using a text-to-IPA type filter with some kind of knowledge (or misunderstanding) of surface phonetics in the language fed into Klatt synthesis. Whereas in the better languages, they probably rely on a large corpus of sampled speech (which is the basis for some kind of diphone resynthesis). The combinatorics of vowel-consonant-vowel configurations are more complicated and probably speaker-variable in English compared to Indonesian, so it's easier to acquire an ideal corpus of Indonesian that generalizes to the ability to handle any arbitrary input (not just actual words). 

Brahmic scripts such as Devanagari (the standard script for Sanskrit) employs a phonetically-based order of letters, which is based on Vedic phonetic instructions (śikṣā). The creators of the IPA were well aware of Indian phonetic tradition, though IPA order diverges superficially from the varṇamālā sequence (Sanskrit places are ordered back-to-front and IPA is front-to-back). Because Vedic phonetic theory differs somewhat in details and purposes from IPA, the specific order of letters differs as well, but the organizational idea of IPA order comes from from Vedic phonetics. Earlier version of the IPA were closer to Sanskrit order (back-to-front, fricatives last), in 1905, but in 1912 place of articulation order was reversed. Fricatives bubbled upwards in 1932, then in 1979, the order was nasal-plosive-fricative-trill. At the Kiel convention, the current order was set (here), but some items were exiled from the main chart to the non-pulmonic box. One would probably have to interview participants in the 1989 convention to get a good history of why those changes were made; there are no survivors from the earliest days. One could read through Le Maître phonétique to see what discussion there is preceding presentations of new orders of letters. 

Some presuppositions of the question need to be exposed, mainly regarding the term "allophonic". The historical distinction between "allophonic" vs. "morphophonemic" has fallen into desuetude, to the point that it is rare to find any contemporary mention of "allophony" that accepts the idea that being allophonic (as opposed to something else) is an important and well-defined theoretical concept. Instead, the question is generally expressed in terms of whether a process is "phonetic" or "phonetic implementation", vs. "phonological". In SPE theory, things that are typically called "allophonic" are simply (and by fiat) the result of phonological rules, and the term "phonetic" is reserved for unavoidable, physically mandated facts that are not in grammars. Unfortunately, there has been huge confusion in the literature over what kinds of things are "automatic". For example, vowel to vowel coarticulation is considered by some to be "automatic", probably because of the word "coarticulation" (since in the transition from an oral segment to a nasal segment, there is, necessarily, some transitional interval). But "vowel to vowel coarticulation" is just another term for "vowel harmony", which can be contrastive in some languages, and just allophonic in others. Because of the huge uncertainty attached to the term "allophone", questions about whether a particular fact can be called allophonic is fundamentally unanswerable, unless you add a vast amount of theoretical context (e.g. "assuming Bloch's Postulates"). Sensible answers are possible if you ask whether a process is phonological vs. phonetic, and that does require you to accept that both things exist (SPE denies that there is such a thing as phonetics as we now understand it; Bob Port denies that there is such a thing as phonology; so you have to at least presuppose that "phonetics" and "phonology" are referring terms). I would especially tag the work of Abby Cohn as productively making that assumption, as well as work by Pat Keating and Nick Clements. Zsiga's paper in Language on Igbo likewise persuasively makes the distinction. This brings us back to the wording of your question, referring to "this kind of variation" -- specifically making me ask which kind of variation. If we define "allophonic" as "phonological, but not contrastive in the language", then in the context of theories that distinguish phonetic and phonological, ZCR would not be phonological. I haven't ever looked at (even computed) ZCR, but simple visual inspection of waveforms indicates that zero-crossing can be quite volatile, to the point that it does not give way to yes/no categorization. If a property could be analyzed in terms of a small set of categories, then the property could be phonological, but if it is truly continuous (e.g. formant frequency, duration, amplitude, number of zero-crossings), then it can't be phonological. Unfortunately, Mark Hale doesn't do acoustic phonetics, so his take on the matter is not coupled with a bunch of measurements that argue this, but he distinguishes numerous "outputs", which includes not just the acoustic wave detected remotely, but also the physical output of the mouth, the output of the articulatory system, and the output of the phonetic computation (which takes phonological outputs as its input). The import of these distinctions is that the idea of acoustic differences "no matter how minor" is actually a misconception of acoustics. As you probably know, if a speech event is recorded by multiple microphones (e.g. a "stereo" microphone) which are the "same" from an engineering perspective, the resulting waveforms will be different. Acoustic waveforms will always differ by some small amount, and clearly we would not say that my choice of microphone brands or positioning brings about allophonic differences. You have to start with some kind of definitional distinction, which I maintain is the "categorial" vs. "continuous" distinction -- phonology is categorial, phonetics is continuous. You also have to have a theory of what kind of information a process has access to, for example, can a phonetic rule access the underlying form of a word (the conventional answer is "no"); can a microphone access the surface mental representation of an utterance (patently not); can a phonetic rule access the output of the phonological computation (probably, and this is where the discussion gets complicated). Given that, plus a proper grasp of the statistical logic behind saying "is same" / "is different", then the question can be answered, and would tell you that not all systematic detectable variation that can be attributed to surface phonological context is the result of a phonological rule. 

The main difference between dead languages and living languages in this respect is that it's possible with a living language to resolve an empirical question by interrogating a speaker of the language, and this is not possible with a dead language. Therefore is you want to know how the N. Saami word guossi is pronounced, you can ask a speaker to say it, and if you want to know if Ich habe gesagt das is grammatical in German, you can ask a speaker. If you want to know how Ancient Greek γλῶττα was pronounced or whether ʔəshaydxʷ ti stubš ci sɬaday is grammatical in Lushootseed, you'll have to use secondary sources since there are no native speakers of these languages. In dealing with dead languages, we analogize from what is known to a conclusion about the unknown, based on similarities. In fact, we usually do that with living languages, though we are not supposed to invent "data" by analogy to directly-elicited data. If there is a very large corpus of a dead language, there is a reasonable chance that a particular desired form in a paradigm is actually available and if not, there is probably a reasonable basis for conjecture based on what is known. However: there can easily be severe limits on knowledge especially when it comes to matters of pronunciation. With very rare exceptions (most notably Sanskrit, and to a considerable extent Biblical Hebrew and Quranic Arabic), one cannot call on trained phonetic descriptions of ancient languages, whereas one can call on numerous trained phonetic descriptions of recently extinguished languages such as Tonkawa, and for Lushootseed there are recordings of native speakers. 

The claim is not that Articulatory Phonology "supports" Autosegmental Phonology, rather, it is a factual recognition that ArtPho has a certain resemblance to AutoPho. The graphic representation is introduced in Goldsmith's dissertation in (2) of ch. 1, where he gives the "score for the orchestration of 'pin'", contrasting the standard segmental view with a physical "score". Chapter 4 then talks about "laryngeal gestures". B&G pick up from that perspective, and try to say in detail how this kind of graph can be related to actual physical measurements. As Goldsmith points out (ch. 4), without autosegmental phonology, the linguistic existence of segments is axiomatic, but with autosegmental phonology, it becomes an empirical hypothesis. The reason is that Goldsmith's gestural score is also a conceivable physical output, so the obvious question to ask is whether something else (other than sheer logical necessity, the need to have a way to talk about what is produced) motivates having segments. Phonologists generally hold that there is something – grammatical computations – that do motivate segments. The ArtPhon program has been to subsume as much of phonology as possible under the kind of "gesture-sliding" analysis that they give in that and other papers. (For example reanalyzing apparent category-changing assimilations of /t/ to [p] before [p] not as change of thing, but an earlier initiation of labial closure which obscures the lingual gesture). There is, not entirely incidentally, a de-linguistification, of phonology in ArtPho, where the goal is to reduce at least phonetic implementation to a general motor control system, similar to walking or chewing. 

The problem that you're encountering is due to different definitions of "phoneme", a problem which is about as old as the tern "phoneme" itself. Some questions here that center around the definition of phoneme are this, this, this, and most recently this. Formal views of the phoneme can be broadly classified as "data-oriented" versus "derivation-oriented". The former view starts with only narrow phonetic transcriptions and some indication "Utterance X is / is not the same word as utterance Y" (for example "heart" said one time is the same word as "heart" said a second time; "heart" is a different word from "hearts"). The data-oriented view then looks for patterns within the narrow transcriptions and extracts generalizations like "aspirated [tʰ] only appears before a vowel", "aspirated [tʰ] does not appear after [s]", "[t] appears after [s]" and so on, arriving at the conclusion that [t] and [tʰ] are in complementary distribution, and a rule can be given for stating when you get one versus the other (looking at the narrow transcriptions). The derivation-oriented view is what you might call "holistic" in that it looks at narrow phonetic transcriptions, but also considers hypotheses about the phonological source of those transcriptions (underlying forms), also the syntactic and morphological status of an utterance. It especially shifts the focus from asking "is it the same word?" to asking "is it the same morpheme?", so while "heart" and "hearts" are distinct words, they contain a common morpheme. Accordingly, the status of being a "phoneme" in this kind of approach is not a statement about sound relations at the surface level, it is a statement about whether certain sounds occur in underlying representations. Because of the nature of derivational mechanics, these two approaches to "phoneme" can give different classifications to sounds. They have in common the idea of "being able to predict", but differ in what the basis of the prediction is. The data-oriented perspective looks only at the surface form, and cannot tell whether [ʃt] comes from /sVt/ versus and [st] comes from /st/; or from /st/ versus /s+t/ – from that perspective, there is a phonemic contrast. If you expand the range of things that can be considered in judging "predictability" beyond that which can be directly heard as in the derivation-oriented view, you have greater "predictive power" and thus fewer things will be "phonemic". The confusion rests on the second part of your summary: "no other factor can explain the difference" is not universally accepted. It is impossible to determine which definition is "correct", since there is no agreed-on independent means of determining whether two sounds "are distinct phonemes", or not.