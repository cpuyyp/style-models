Part of the reason for this is that the RSS and UAH datasets are both very sensitive to ENSO, and the trend over the last 20 years is dominated by the 1998 El-Nino. So how long do you need to get reasonable statistical power? Well climatologists generally use a period of at least thirty years (IIRC this is a WMO guideline). Santer el al performed a study and found that you would need at least 17 years of data for identifying human impacts on surface temperatures. Note this is a bare minimum, and refers to the detection of a trend in a randomly chosen time period. If you wait until a period of 17 years comes a long before making the argument for a hiatus, the assumptions of that analysis are violated by the fact that multiple hypothesis tests are being implicitly performed, and in practice you would need a much longer period. Now are scientists ignoring the apparent "hiatus"? No, there have been many studies analysing it, for instance Foster and Rahmstorf show that the apparent hiatus can be reasonably explained by ENSO and volcanic forcing, and so there is no real reason to suppose that there has been a pause in anthropogenic global warming, and the apparent hiatus is probably due to a redistribution of heat between the atmosphere and oceans. There has even been a special issue of Nature Geosciences devoted to this topic. UPDATE: @user1286792 notes in the comments below: "!UAH, and !RSS show it has been 21.5 years since there has been any tropospheric warming.". This clearly isn't true, at least for UAH. If you plot the trend since 1995, it is essentially the same as the long term trend since the start of the dataset. The fact that the RSS trend for the last 20 years is very different is I suspect the reason skeptic blogs switched from using UAH to RSS, but as always don't believe everything you read on line, and woodfortrees is a great tool for checking things. As I said, UAH and RSS vary considerably over the last 20 years in their trends and the reason for this is structural uncertainty. That doesn't mean that either of them are right or wrong and if your argument is not valid for both, then you probably shouldn't be confident that it is true. Basically the timescale is too short to reliably estimate the trend given the uncertainties in the data, including the structural uncertainties. 

Evans has been making predictions of this nature for a while, for example, from 2014: "Itâ€™s already baked in the cake; we can see a few years into the future." 

Atmospheric CO2 rises because more CO2 is being put into the atmosphere than is being taken out. The airborne fraction (the proportion of cumulative carbon dioxide emissions from anthropogenic sources that remains in the atmosphere) is about 50%, suggesting that total natural uptake currently exceeds total natural emissions by about half of annual anthropogenic emissions (i.e. about 5GTC per year). Thus if we keep emitting at a constant rate of 10GTC per year, then atmospheric CO2 will initially carry on rising by about 5GTC per year, which explains what we see in the second graph, because we will still be emitting more than the natural environment is able to sequester. Unfortunately, the net natural sink depends on the disequilibrium of CO2 between the atmosphere and the surface ocean (vaguely similar arguments also apply to the terrestrial biosphere), so if we keep emissions constant, then the disequilibrium will be reduced slightly (as the surface ocean absorbs more CO2 than it emits) and so net uptake will start to fall and the airborne fraction will start to rise, however this will take time. It is a bit like putting your foot on the accelerator (gas) in your car. If you put increasing pressure on the pedal, your car will accelerate. If you keep it in the same position, you will stop accelerating, but you won't stop moving. 

The paper is deeply flawed from both the climate science and machine learning perspectives. The most obvious being the most eye-catching claim that equilibrium climate sensitivity is approximately 0.6C, which if true would overturn our understanding of the climate system. However the paper doesn't actually explain how this figure of 0.6C is obtained from a "largest deviation" of 0.2C, it is basically just a hand-wave. Also the largest deviation is not 0.2, this is the largest average (mean absolute) deviation seen in the proxies, and you can have a mean absolute deviation without there being a trend that you could relate to increased GHG concentrations and hence estimate ECS. More importantly, this would give an estimate of transient climate sensitivity, not equilibrium climate sensitivity, and you can't reliably estimate ECS (which is global) from regional or sub-regional proxy records. Approaches such as the one taken in this paper, which seek to see how much of the data can be attributed to "climate cycles" with the remainder being taken as the anthropogenic component are inherently biased towards low estimates of ECS. This is because of omitted variable bias; because the anthropogenic forcing signals are not included in the model, if the net effect of independent changes in the forcings is correlated with a sinusoidal component, they will be wrongly attributed to these climate cycles, when in fact they are produced by the forcings. Models like this can only be used to estimate lower bounds on ECS, likewise if you make a model using the forcings as inputs and treat the residual as being "natural variability" it will tend to over-estimate ECS, giving an upper bound estimate. The Abbot and Morahasy paper cites a similar paper by Loehle, but sadly does not also cite the comment paper (of which I was the lead author, note the corregendum). This is poor scholarship, and sadly Abbot and Morahasy have gone on to make many of the same basic mistakes (but with a more complicated model), which is a shame. Rather than using the original datasets (many of which are freely available) the authors chose to digitise images of the datasets instead. This seems somewhat bizarre, and Gavin Schmidt points out via twitter that in at least one case the dataset has not been scaled or aligned correctly (andends at 1965, and so does not include the recent warming where anthropogenic contributions are most evident). It also transpires that Figures 5 and 9 are identical. The paper says "However, superior fitting to the temperature proxies are obtained by using the sine wave components and composite as input data. This was established by comparing the spectral analysis composite method versus the ANN method for the training periods.". Evaluating performance on the training data is a classic error in the use of machine learning that people used to make all the time in the late 1980s and 90s, but is rarely seen today. If you have two nested models (one can be implemented as a special case of the other) of different complexities, then the more complex model will always have a lower training set error, if only because it has more capacity to memorize the random noise in the data, but that doesn't mean it is the more accurate model. For that you need out-of-sample comparisons, which are absent from the paper. There is no handling of uncertainty in the model (for instance the periods of the cyclic components are not known exactly, cycles with slightly different periodicities will explain the observations almost as well), and likewise there will be uncertainty in the parameters of the neural net, but none of this is propagated through to give the uncertainty in the estimate of ECS. As seen in the comment on the Loehle paper, this can be substantial. Table 13 seems to suggest that paleoclimate studies give lower ECS estimates than GCMs, which suggests a rather selective view of the paleoclimate studies, which generally indicate high ECS IIRC. The study also has problems with too many degrees of researcher freedom (e.g. how was the particular subset of proxies chosen?) and there is a lot of (automated) exploration of model architectures and feature selection, which is often a recipe for over-fitting in model selection. It is also not clear why the observation should be a non-linear function of the cyclic variables (especially given that the cycles were obtained from the data by linear analysis). 

Firstly, the RSS and UAH datasets do not "line up nearly perfectly", the diagram below shows both products over the last 20 years (which appears to be what this question is actually about) here is the url used to generate it. 

Caveat: I think WFT may be using a previous version of UAH, however this rather illustrates my point about the structural uncertainties, if successive updates to the algorithm make such large differences to the trends, that means the structural uncertainties are very large and we should take that into account when looking at the trends. However, the basic point remains that (i) the trends are too short to expect the warming to be significant (ii) the apparent haitus is adequately explained by ENSO and volcanic forcing (iii) similar periods of little warming are seen in the output of GCMs, and so it is not entirely unexpected. 

As you can see, they disagree quite considerable on the value of the trend. UAH and RSS are different methods used to infer trophospheric temperatures from them MSU instrument carried aboard various weather satellites. The reason why they differ is because they use different methods to deal with the various drifts and biases involved in the measurements, and the fact that we don't know exactly how this is best performed is what is known as structural uncertainty. Note there are a couple of other similar products, but UAH and RSS are the main ones. As to "no significant warming for over 20 years", this is the subject of a fair bit of discussion in the journals, but not nearly as much as in the blogsphere, and a lot of it stems from not understanding what the term "statistically significant" means (and more importantly, what it doesn't mean). The key point is that "no significant warming" does not mean "there has been no warming" or that "the rate of warming has slowed". Unfortunately the reasons for this are a little complicated, so I will start with a simple example. Consider the usual statistical test for the bias of a coin, we start by stating the null and alternative hypotheses: $H_0$ - null hypothesis, unbiased coin, probability of a head = p - 0.5. $H_1$ - alternative hypothesis, biased coin, $p \neq 0.5$. We then flip the coin $n$ times and count the number of heads, if the p-value is less than some pre-determined value, typically $\alpha = 0.05$, then we say that we can "reject the null hypothesis", implying that the data provides some support for our alternative hypothesis, $H_1$. However, if we are unable to reject $H_0$, there are at least two reasons for this, it may be that $H_0$ is true, or it may be that $H_0$ is false but we haven't seen enough data yet to be confident of this. For example, if we only toss the coin four times, then even if we get a head every time, we still won't be able to reject the null hypothesis as the p-value cannot be smaller than 0.0625. The statistical power is the probability [in a frequentist sense] that the test correctly rejects the null hypothesis ($H_0$) when the alternative hypothesis ($H_1$) is true. In this case, the statistical power of the test with only four flips is zero, because we can never reject $H_0$. So, if you want to claim that it is in some way a surprise that we can't reject the null hypothesis, you need to show that the power of the test is high, and in the case of the trends over the last 20 years, it isn't particularly high, as similar trendless periods ocurr both in the observations and in the model output, as shown by Easterling and Wehner. For example the trend in RSS to 1997 is very similar to the trend from 1995 to the present day, and that is almost as long: