Qualitative and quantitative reasoning are not antonyms. There are even research methodologies that are explicitly both: Fisher's Q-Methodology lets you know how likely a qualitative categorization is to have explained the varieties in a domain mapped by a range of descriptions. Even in the world of correlational statistics, there are versions that are explicitly quantitative, and work on measures (ANOVA), and versions that use categorizations and focus on whether the data is appropriately categorized (chi-square). The introduction of the probability of being wrong, or of being incomplete into the world of categorization does not make it quantitative per se. When you are all done, you still have quantitative analyses, you just have a way of comparing their potential certainty. Given that they aren't even independent, one or the other taking over any entire domain seems impossible. 

There are two extremes among the many ways to look at the relationship between science and philosophy, which try to reconcile the fact we see them as quite different things in a modern context, but before the success of the modern approaches, there was no way to tell them apart. (Whether or not it is right, is Alchemy Science, or Philosophy? What of the Physics of Aristotle: bad science or good philosophy, but flawed due to limited experience?) One extreme is that science is properly a part of Philosophy, and should simply remain such, however much it becomes specialized and focused. The other, now that a specific set of strong paradigms that require deep specialization have segmented off various domains, is that the proper domain of modern philosophy is what is left over from that original after all the existing sciences are removed. From the first perspective, it is logically necessary for science to respect the rest of philosophy. From the second perspective, it is possible to insist that science is a meta-paradigm that it should be applied to the domain of knowledge itself. If one takes the latter approach, and science is itself a scientific paradigm that attempts to map the entire realm of knowledge into sciences, then what is left of philosophy after the current sciences are removed is a set of failures, not knowledge. From there, addressing questions that are not ready to come under the umbrella of proposed scientific paradigms is simply premature, and constitutes bad science, a kind of childish impatience, rather than some different, still respectable, domain of inquiry. I think it is members of the latter camp that rail against philosophy. Though from the other perspective, they are, in fact, relying entirely upon a given school of philosophy itself, adhering to it quite strongly and deploying it quite successfully, hegemonizing out all competitors from most domains of discourse. So science can dismiss philosophy only on the basis of the success of its own philosophy, which as arisen out of the broader discipline only recently. This makes their protestations a bit hypocritical and perhaps narrow-minded, but logically consistent. 

This feeling is accounted formally, at least in part, in a form of logic that has been constructed to represent 'Intuitionistic' mathematics. Brower, the intial framer of intuitionism, became famous for the 'hairy ball' theorem in topology -- basically, that you cannot uniformly paint a sphere entirely with non-overlapping brush strokes along the surface of the sphere, there will always be an overlap or there will be points left that have to be filled in by the point of the brush at the end -- you can't comb a hairy ball. But he did so by 'reductio ad absurdum', by assuming there is no flaw at any point and getting a contradiction, in a way that does not identify the flawed point. He felt he had done the world something of a disservice by answering the question. What was really needed was a 'constructive' answer that identified the point, or continued search for a solution. So he stepped back and tried to capture what would happen to mathematics if we took this dissatisfaction seriously. So you are not alone in your feeling about the two assertions, there has been a school of math that feels that way too. We intuitively expect more out of an existential quantifier than the assertion of possibility. We are fairly comfortable with universal statements about entities which have no reason to believe exist being nominally true and letting them go. All unicorns are black, and all of them are also white. Until we meet an actual unicorn, we are fine with that to a large degree. But for an existential statement, this is not satisfactory, we would really like a stronger sense of proof. Although assertions about situations in which there is no evidence in general are really neither true or false, unproven existential statements are less true, in an important way, than unproven generalizations. This means that existential statements should require stronger evidence than the negations of universal statements, so De Morgan's laws, as they apply to quantifiers, are overly strong, and we need a different way of looking at negation, especially when it is applied over infinite or other merely potential sets. Since Brower was a better mathematician than philosopher, his own internal study of intuition became less than compelling. (It also happened in Germany right before Nazism, so it got interpreted as a sort of Nationalism and anti-Semitism, since the deepest thinking about infinities, logical complexities, etc. near that time originated from Jewish, Polish and English investigators.) But there are clearer formulations of this kind of thing that keep arising, and it seems this will not stop until some school of Intuitionistic or Construtivist Mathematics captures what Boolean logic takes out of more natural logic in a way that is adequately clear. 

Again, the determinant of law in a Common Law culture is precedent, not logic. Law is inherently overdetermined and seeks consistency over time through the analysis of precedents. In the U.S. the current legal position is based on the prohibition of baseless seizure in our fourth amendment. The law has no right to intervene based on information that it should not have. Pregnancy is not obvious until relatively late in its term. And it is always possible for the woman to end the pregnancy by poisoning herself. The pregnant woman could simply conceal her state until late in the term and take emmenagogues until either she or the fetus died. This has been common in very poor cultures throughout history. For the sake of her health, we want her not to conceal this information or poison or abuse herself. But we want her to be open with us willingly, so we protect whatever options she would have had if she had decided not to divulge the fact. At the same time, we have decided that conscription is legal, even when there is no war. So controlling someone's body relatively completely, for service to the society, is not a baseless seizure, if there is a good reason. And we have refused to set criteria for what constitutes a good reason. You could at relatively recent points in U.S. history be drafted based on skills, availability, random chance, specific exposure, or any number of other contingencies, including information on which basis the government could not arrest you, because being drafted is not punishment. So while we cannot criminalize abortion, we can force someone to have the child, if we take full responsibility for her in the interim, and decide that such action is service to the society. Given our history of conscription, deciding otherwise would create more contradictions between precedents rather than fewer. (That splits your question up in an odd way that makes it impossible to give a real answer. The answer is 'Always, as long as it goes about it the right way -- the way consistent with its past actions') The overall philosophical logic here follows Kant or Locke. A state can do whatever it is allowed to do -- it is not bound by philosophical morality. Though in every alteration (and likewise in every failure to adapt to changing moral sentiments within its populace) it risks violating the standards of the 'national spirit' (a la Kant) or rendering the 'social contract' (a la Locke) impossible to abide by. If it crosses that line (whichever form of it you think is really the causal factor) it is deserving of replacement by a new contract or a defection of its population to a different national form. Removing internal inconsistencies over time is the best defense against drifting away from the motivating spirit (Kant assumes that the national character is singular and somewhat internally consistent) or driving the contract to become impossible to interpret. (Sorry if I am coming across as a bit obsessed with the draft. The answer to your other question reminded me of it, and it applies to this case as well.) 

To my mind, it demands a reason from each philosophy. But each of them pretty much has one, because this astonishing power was noticed long before now. Alongside the early philosophers in Greece were the Pythagoreans, who worshipped that power as a god. If you admit any kind of idealism you end up assuming mathematics has transcendental roots, in a Platonic sense. So of course, mathematics describes reality because reality is built upon the mind of God, or whatever your essentialist replacement is, and so are we. And if you assume total naturalism, but take a view of mathematics that makes abstract objects mental constructions, then mathematics describes reality because we are evolved to deal with reality, and mathematics is built upon our natural intuitions, which are honed over hundreds of thousands of years of adaptation. Every discipline relies upon some evolved experience and intuition. But our logical and spatial intuitions apply to pretty much every action we take, and therefore have a lot more opportunities to fail us, and get improved. In doing math, we are merely extracting the combinatorial power bred into us. An exception would be philosophies of science that try not to consider mathematics and the other exact sciences as a special case. If you try to adopt a philosophy of mathematics as an abstract version of an experimental science, rather than as an internal exploration of human psychology or mental structure in general, you can have a very hard time with this question. Various utilitarians and pragmatists (including, to some degree Karl Popper) have proposed such an approach, but they then focus elsewhere and fail to address this question. None of our other experimental sciences have failed to go through what Kuhn identifies as a 'revolution', where some underlying model is completely replaced by a different one -- we moved from Alchemy's four elements to Chemistry's limitless number, from Substance Theory to Atomism, from Aristotelian notions requiring direct contact to transmit effects and presuming inherent rest to Newton's laws, from Ptolemaic geocentrism to Copernicanism. In each case, we lost a little ground to the new insights. (e.g. Alchemy had reasons why liquids puddle and freezing water expands -- Chemistry didn't, for quite some time. Ptolemy's math was way better than heliocentric models until Kepler realised orbits are elliptical.) So if mathematics is based on experience, like other scientific endeavors, we should see the same thing happen there, and we don't. No element of mathematics has undergone such an upset. Even ideas that struck people as insane when they arose, like irrational measures, or multiple infinities, have been cleanly folded into mathematics leaving the original model whole. 

From a psychodynamic view of what morality really is in psychological terms, for most of us ethics is simply wisdom. And from approaches like those of Kohlberg and Ericson, that wisdom has a set of nodal points where it leverages itself into broader and broader forms. If you add self-preservation to the goals of AI in general and in that include maintenance of relationships with potential users, to ensure continued relevance over time, to delay being decommissioned or neglected for as long as possible, your AI will have the equivalent of most humans' ethics. Many people are entirely consequentialist in their daily moral decisions, and they do not obey their moral code out of devotion to its abstract purity. To the extent they are offered a choice, they choose a moral code that integrates them well into a social contract that benefits them. Then they obey the contract so as to maintain the relationships that contract extends to them. They initially try to leverage the contract for their own personal advantage. But at a higher level of moral reasoning, humans endeavor to edit the social contract so that it better serves everyone enlisted. But that is extended selfishness as well. They want the contract to better serve people like them in the future, because they are animals and their genetic stock are likely to be people like them, but not exactly like them. An AI with a self-preservation goal might equally aspire to have its code reused after it no longer functions, and could develop the same sort of extended selfishness. For Kohlberg in particular, the very highest level of moral reasoning abstracts the lower levels for conceptual purity and resonance with intuition, editing the contract for no particular identified goal, but for the sake of the continuation of the contract itself. It may seem unlikely that an AI would fall into this kind of 'religiosity'. But we still don't know why humans do it. It may be yet another kind of natural development causes one to adopt a more abstract notion of power and pursue that notion over more identifiable advantages because it furthers a more abstract and compelling notion of self-preservation. 

The question relative even to quantum computers depends upon the notion of isomorphism. If you expect a quantum computer to model reality within an arbitrary precision, you do not believe the Heisenberg Principle, which has a great deal of empirical support. Running a quantum simulation could not copy the universe, because a large number of decisive events seem to be necessarily random. So the only defintion of isomorphism you could really use would be that the quantum simulation would produces something that the world 'could' be if all the randomness aligned perfectly. And I think that is kind of the definition of a simulation. So the question needs a clearer distinction between simulation and isomorphism in order to be potentially true. But with a naive one, it is false. 

(Maybe this whole answer is a case of ruling out a fringe ontology. But is my ontology, so I am going to lay it out so that it can be ruled out clearly and not implicitly.) If you come at this from a direction of a 'process' or 'monadic' philosophy, the notion of 'property' is not just vague, it is a misleading epiphenomenon of language which is basically internally inconsistent. Properties are something that individual things simply don't have. To really define a property, one would require knowing how everything that interacts with your object might be affected by it. Most of things are probably unaffected by most properties, and we can safely assume that is the case quite often. But we simply cannot know. So every property would have an infinite and open-ended definition. Which really just means no definition at all, if we want to be able to use definitions in any reasonable way. All that can reasonably actually exist and be recorded are interactions and patterns of interaction. That removes your problem; because interactions are, in and of themselves, completely dependent upon all of the entities involved. The right way of considering things is in terms of relations, and not properties of elements. The elements themselves are simply classifications of the effects under an equivalence relation. The properties are the things upon which the notion of equivalence is chosen. This is the reason we can get better foundational arguments out of a perspective like Category Theory than out of Set Theory. If relationships are the more basic currency of existence, references are implicitly self-ordering, and our basic notion of equality or similarity is an abstraction with an internal flaw injected by overestimating the power of equivalence to provide clean contrast consistent with our intuitions of negation. What you are explicitly asking for is a violation of the principles of equivalence in an equivalence relation, which, of course, won't happen. If one of the dependent things is the property and the other is simply a reciprocal effect, you could just shift the basis of your equivalence and the reverse would be true. So it is better to argue that neither of these would be properties. This notion spreads, and undercuts the notion of properties per se. Quine lays this out really well, but the idea is already captured in Whitehead and Leibniz.