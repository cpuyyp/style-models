I am finding what is the best way to estimate size of a table for that i have studied lot of blogs and forums but unable to find any accurate answer For an example we have a table City with InnoDB engine,lets say in future (in next 1 year) it will have 1 million of records so what will be the estimated data size and index size of that table in this period. 

For logging slow queries do like below, It will log all queries those are not using indexes as well as which are taking more than 1 sec 

I found that the bottleneck in the query was ORDER BY . DESC. The table coupon have PK and the field is also of type INT where is stored, So I observed that can do instead of which i found executes query in 0.00 secs. 

You need to grant the Execute Permission to that user.For that you need to login as root user and grant the permission as 

I am bit confused between setting the global and session parameters , I am trying to set The default settings are 

EDIT Someone has opened the City.frm file and modified it's content.So How will I recover it.? I am using . UPDATE: Adding my.cnf as asked by some peoples. Innodb Specific variables in My.Cnf 

In order to change we should also change variables. The definition of : The number of seconds the server waits for activity on a noninteractive connection before closing it. On thread startup, the session wait_timeout value is initialized from the global wait_timeout value or from the global interactive_timeout value. Here is the test Query without changing any parameter , all the values are set to 8 hours by default 

You can try below procedure first test it on local before going to production. This consider that user has unique I have taken that as scored_at Table Structure 

Second: change write permissions to rwxr-xr-x (755) for mysql directory, and all its files and subdirectories, by running the following command: 

Let us address first normal form and second normal form distinctly. First Normal Form (1NF) You cannot say a table is or is not in 1NF because no two rows contain repeating information. By repeating information you may be thinking that BOOK doesn't look like this: 

Under no circumstances would it be justified. A column containing two different data elements as one value is a violation of first normal form. 1NF requires, among other things, that each column contain a single value. This practice also destroys the guaranteed logical access of every data element by table name, column name, and key value. Now if each data element were in its own column, and the combination of columns made up a key, that is not a problem. Ultimately this depends on the entity type for which the table holds entity occurrences. The key chosen should be something used by people in the real world to identify occurrences. Perhaps this makes sense if we are talking about a menu, and the menu has a section for Chinese food, and then gives each dish a number like #1, #2, etc. 

As explained above, whether or not the table is in 3NF (or more importantly BCNF) is orthogonal to how many candidate keys it contains. 

Read Steven Levy's "In The Plex: How Google Thinks, Works, and Shapes Our Lives". This book is a fascinating read about all things Google and does discuss at a high level some of the technology and engineering behind search. Aaron sums it up really well in his answer and Levy's book will give you some more detail about how they do it. 

Another benefit of the relational approach is that the performance of the system can be managed to a great extent at the physical level without impacting the logical database schema used by applications to work with the data. With the relational approach you can focus on the correct logical schema first and then in most cases let the DBA implement a correct physical design and infrastructure to best support the read heavy workload. This is not the case in navigational systems where the programmer must specify the access paths directly in programs. 

The MySQL Performance Schema is a feature for monitoring MySQL Server execution at a low level. The Performance Schema is available as of MySQL 5.5.3. From MySQL Manual there are some of the features of the performance_schema database. 

You are missing column name of parent table to which your child refer, while creating child table, do like below 

So I need to delete 611992998 records from biggets table. We have One MySQL Master and 4 MySQL Slaves, We need to delete data from all the servers, What I am thinking is i will delete data in chunks from master so that slaves also didn't lag too much.For that i have created a procedure here is the procedure, I have not yet tested it 

In your code add lines as follows here i will insert those record into log_table which you was selecting please look the line /**Added by abdul */ 

Above shows that all privileges are granted to myuser on all databases.Now grant privileges for single db. 

This will give you the desired result. Also Write one line in your code before you open the cursor otherwise each procedure call will be appending data in that table. 

I know i can tune by setting it to some low value The default value is as below, but i have a single table involve in query so i don't think will help. 

If you don't have privileges to DROP table you should DELETE in chunks. Use MySQL Limit clause to delete in chunks. 

And one more thing in your insert you are missing value for one column, as you have 17 columns to insert but you are inserting only 16 values. I hope you are missing value for column , Your insert should be like below 

Does the approach i am going to opt will work and is it optimized way to go. Because we don't want any issues in servers (Like slave lag) due to deletions. What would be the best approach to do it without issues and downtime. How can i use pt-archiver for doing this. 

I have never used the relationships tool for anything other than understanding the relationships between base tables on the back end database. I could see however where it would be very useful on the front end if you have many users of your application who write a lot of ad-hoc queries and you provide them queries stored on the front end database which they can use as an abstraction layer. On the other hand, if the users only interact with the database via forms I would think the work to add the queries to the relationship tool and connect them up would not be worth it. 

Row Wise A second option would be to make the statistic more generalized by having a Statistic Type table. There would be a row in Statistic Type for each kind of statistic you want to record. Then, you would associate Measurement to the Statistic Type and record the Value. The advantage of this approach is that you can easily add more statistics, and you only have to record just the values for the statistic types you measured. The disadvantage is that this is more abstract and complex, and you have to use a generalized data type that can support all of the various units of measure, or you have to create a mutually exclusive set of columns each of a data type matching a statistic type. If you go with the first approach, you can really turn it into a science project in order to support all the various data types and still ensure data integrity. 

Its pretty easy to do this but I wouldn't recommend it. To implement a single "document" table for all possible documents and parents, you just abstract the id into a "parent_id" column, and add a "parent_table_name" column. There are a lot of issues with this approach. Some are: 

Short Answer Closure in relational algebra has nothing to do with the concept of a key. A relational variable holds a relation so long as that relation has the following properties: 

Alias inner as something different than as it is good practice to not use it as alias , As i have used . 

You're getting this error because you're trying to update a row to table that does not have a valid value for the field based on the values currently stored in table. If you post some more code someone can help you diagnose the specific cause. 

Where is in table. When I execute the query independently it is being finished in but i have observed many times that query is hanging is processlist with state statistics. What may be the issue, I have found count of this query in slow log more than 10K times. 

A temporal database is a database with built-in time aspects, for example a temporal data model and a temporal version of Structured Query Language. More specifically the temporal aspects usually include valid-time and transaction-time. These attributes go together to form bitemporal data. 1.Valid time denotes the time period during which a fact is true with respect to the real world. 2.Transaction time is the time period during which a fact is stored in the database. 3.Bitemporal data combines both Valid and Transaction Time. UPDATE Firstly i am removing that part which i have added previously as MySQL5.1 and its above versions of MySQL does not support .have a look at This Link.But in My Opinion what the was doing the is capable of Doing all. As BDB is not supported anymore By MySQL.But the InnoDB is able to do all which the BDB was doing. For More Information About temporal database have a look at This. With MySQL, The unavailability of ,, and some limits on , make it harder to build in MySQL. CHECK CONSTRAINT A CHECK CONSTRAINT is often of the form CHECK( [NOT] EXISTS( select_expression )) MySQL implements foreign key constraints in INNODB tables, but does not yet implement CHECK CONSTRAINT. Until it does, such constraints must be enforced by other means. That has onerous consequences for time-valid tables. Some time-valid constraints can be enforced in triggers, but most of the temporal constraints we will consider cannot. Until MySQL implements CHECK CONSTRAINT, they must be enforced in application code. That is a heavy penalty. Deferred constraints MySQL does not yet implement deferred constraints, either. Furthermore, constraints are applied row-wise rather than at COMMIT time. This raises a problem for many complex constraints, even for some simple ones. For example to delete a MySQL row which refers to itself via a foreign key, you must temporarily SET foreign_key_checks = 0. A transaction fulfilling a complex constraint must leave the database in a consistent state.But there is nothing in relational database theory to suggest that a database should be in a consistent state after each statement within a transaction. Triggers MySQL 5 triggers cannot issue UPDATE statements on the trigger table, and cannot raise errors. These limitations create difficulties for implementing transaction validity in MySQL, but the difficulties can be overcome. 

I hope this helps! A great book on data modeling like this is Steve Hoberman's Data Modeling Made Simple. 

Both of these are not technically correct. Regarding person_id on the Account table, since the relationship is one person to many accounts, the same person_id will be in the account table for each account that person holds. It would look like this: 

This means that the value of A and B determines the value of C. Perhaps A is employee number and B is Dependent Number, and C is the dependent's name. So in this sense they apply to each and every table in each and every schema in each and every database. Some good references on relational theory are Fabian Pascal's Practical Database Foundation Series and Chris Date's book Relational Theory for Computer Professionals. 

The pattern you are describing is often called a "parts explosion" or "bill of materials." It is part of the graphs and trees portion in the study of data structures. The essence of the solution is to realize that any given "product" can be made up of other "products." The design is then a network structure where there is a table that has a row for each product - whether it is made up of other products or not, and then a table that has a row for each product that is made up of other products and each corresponding product that is a component of that product. In your case, each product has a price. So you would have something like this 

Again, under no circumstances. If there is a column whose values uniquely identify each row that column must be declared as a unique constraint so as to ensure the occurrences entered are not duplicated and thus become inconsistent with the real world entities they represent. 

This really isn't the definition of 1NF. Instead, BOOK is in 1NF only if each column in every row will have a single value of whatever type, no matter how arbitrarily complex, that is defined for that column. While adding multiple AuthorNames columns may be bad design, it does not violate 1NF as each contains the name of a single author. Now what would violate 1NF is if we had an AuthorNames column and defined it to contain a comma separated list of author names. In that case we know absolutely BOOK is not in 1NF. Its also possible we have designed the table variable to be in 1NF, assuming only a single author name would ever be placed into the AuthorName column, but our users start putting in comma separated lists of authors. Now the resulting table is not in 1NF even though we meant it to be. In both these cases the table isn't even a relational table anymore because it no longer follows the discipline necessary to gain the properties of a relation. Secondly, you cannot say BOOK is or is not in 1NF without the presence of at least one candidate key! As stated right now, BOOK has no candidate keys and thus the same value is allowed to be entered for every column in two or more rows. This would result in duplicate rows and a table with duplicate rows is not a relational table! Second Normal Form (2NF) It appears from looking at the attempt to decompose BOOK a candidate key of BookTitle,AuthorName was assumed. Even if this assumption were correct, it is not correct to say the original table is now in 2NF. Instead, of the 3 tables resulting from the decomposition, each must now be evaluated with respect to the functional dependencies. R2 is in 5NF as its single non key column is fully functional dependent on the key and that key is the only candidate key and it is a single column. R3 is in 5NF as its only non key column (the assumed "extras") are also fully functionally dependent on the single candidate key and there are no additional candidate keys. It is only R1 that is now in 2NF but not 3NF as it has a non key column, ListPrice, that is functionally dependent on another non key column, BookType, and thus forms a transitive dependency. The assumption of a single candidate key of BookTitle,AuthorName may also not be correct! Perhaps in this particular book world every book of interest has only one author. If that were the case then the creation of R3 is incorrect with respect to the actual functional dependencies. Thus it is vital to clarify all the candidate keys and all the functional dependencies, join dependencies, and multi-value dependencies, with the subject matter experts and to never assume them. Otherwise the wrong design results. Assuming many authors per book when really there can be only one per book results in an R3 that allows the FD of BookTitle --> AuthorName to be violated. Assumming one author per book when really there can be many per book results in a table similar to the original BOOK where users find they only have room for one author name, not the many they need, and they find their only choice is to enter sets of author names, delimited by something like commas or pipes. References Fabian Pascal's Practical Database Foundation Series and CJ Date's Relational Theory for Computer Professionals are excellent references for understanding what makes a table a relational table and the fundamentals of normalization. It is from these sources that all the information in this answer was derived.