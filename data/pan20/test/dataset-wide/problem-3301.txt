You can divide the properties governing phonemicizable vowel differences into two sets, which we can call "place" and "other stuff". "Place" would include the geometry of the lips (i.e. "mouth" position) and position of the tongue, relative to the mandible. Anything else would be "other stuff". Some examples of "other stuff" include: position of the velo-pharyngeal port, which controls whether air flows through the nose (yielding the difference between oral and nasal vowels); tension on the vocal folds, which in various ways yields tonal differences in vowels; degree of constriction of the vocal folds, which yields breathy vs. modal vs. laryngealized vowel differences; constriction of the lower pharyngeal walls, which yields pharyngealized vowels; lowering of the epiglottis, which yields epiglottalized vowels. In addition, the duration of a vowel can be manipulated without messing with "place" properties. Each of the things I described under "other stuff" forms the physical basis for vowel distinctions in some language. So, given a single tongue and lip configuration, you can still produce quite a range of potentially distinctive sounds. Of course, no language realizes that full potential, and many languages don't realise any of that potential (i.e. there are no tone, nasalization, pharyngealization... contrasts). The chart of IPA vowels above doesn't give all of the vowel distinctions, it only gives the vowel distinctions based on "place". 

Here is an outline of what one would need to do to set up a ToBI system for some Kurdish language. I will make up somewhat hypothetical examples to make the point clearer. You first have to establish what the lexical contrasts are. You might find that there are two distinct words like [hɑɫo] "eagle" and [haɫu] "plum", which show that the vowels [o] and [u] "contrast" in the language. These are systematically different words, and this feature of pronunciation is how the difference is signalled. You may also find differences in pronunciation which are the result of a rule, for example there are different vowel qualities in [kæʃ] "mountain" and [bɑ:ʃ] "good": looking at many examples you discover that [æ] is always short, so there is a rule that allows you to predict when you get [ɑ] versus [æ] – these particular vowels do not contrast in the language, even though both are used in speech. ToBI being about intonation, you would move on to determining what the lexical contrasts are, if any, for word stress. You may find words like [raˈma] "he ran" versus [ˈbramæj] "run!", or [ˈgo:ʃe] "ears" versus [sɛˈre] "heads", which suggest the possibility that stress position is also contrastive, and isn't just the result of a general rule of pronunciation. This is like the English difference between the noun ˈpermit and the verb perˈmit. For the sake of discussion, I will assume that the position of stress in the word can't be predicted by a general rule. If you record many words and separate them into groups according to which syllable has the stress, you will find that even within a group with the same stress position, even looking at two different recordings of the same word, there will be differences in the pitch profile. These would be non-contrastive differences, and they don't go into a system of phonological transcription. Instead, you would describe the differences quantitatively, by saying for instance that the pitch peak is delayed by 30% of the syllable duration, or the pitch reaches only 50% of its target value, or something like that -- a difference in amount. However, the kind of difference difference reflected in [raˈma] vs. [ˈbramæj] is one of category: which syllable has the stress? Now turning to what ToBI is for... The implicit claim of that kind of analysis is that there are meaning-type distinctions (pragmatically distinct classes of utterances) where different pragmatic categories are signalled by picking different intonational patterns. Although it's usual to give longer utterances, you can find different pragmatic categories just looking at the pronunciation of a single word. A common place to find such a difference is pairing statements and questions, especially if the language doesn't change word order in questions. Then you might find a systematic difference between declarative [maħa:sɨl e:ne] "they are students" and interrogative [maħa:sɨl e:ne] "are they students?", where the difference is that in the question, the final pitch is higher (which I indicated with bold versus italic). You might even get differences like [ʒæne:] "women" and [ʒæne:] "women?", with a difference in pitch in the question form. In this case, we have a systematic difference, but it doesn't signal different words as in the case of "eagle" and "plum". It's a difference in pragmatic category. The analysis could be (I emphasize, could be, you would have to work out a system) that the interrogative function is marked by a H boundary tone and the declarative function is marked by a L boundary tone. This would be an example of the kind of contrastive categories that ToBI is designed to capture. If you find that a dozen interrogative utterances have a dozen different pitch profiles, that is beyond what ToBI is designed to capture. You can use a ToBI analysis to group together a set of utterances that have the same analysis (e.g. H boundary tone) and pragmatic function (interrogatives), but if there are still phonetic differences within your corpus, you would have to find a different way to describe the difference, other than with discrete ToBI transcriptional differences. Addressing your specific pitch traces (and assuming you're looking at the difference at the very end), I would not assume that those two utterances have the same boundary tones. I might say, for example, that your first exclamative example has HL boundary tones and the interrogative has just L. But the explanation could also be something more lexical, which is why you have to work out a system of rules to predict where word stress is. A better example would be something like "How big the man is!" vs. "The man is big", assuming you can convey that difference just with intonation. 

The most basic problem is that it is impossible (given any realistic i.e. non-Star Trek technology) to map waveforms to IPA letters for an arbitrary language. It is, however, possible for well-enough studied languages, using Google-grade technology, for example you can speak Norwegian or English to Google, it will return the spelling, and you can use that to get an IPA spelling from a phonetic dictionary of the language (except: when there are multiple pronunciations like "to-MAY-toe" and "to-MAW-toe", when you'd need to know how Google handles many-to-1 mappings of pronunciation to spelling). For the vast majority of languages which are not as well worked out, the problem comes from the fact that mapping between phonetic properties and IPA is many-to-many. For many languages, it is essentially arbitrary whether you will transcribe a given vowel as [i] vs [ɪ], or [ɪ] vs. [e]. Field linguists solve these problems (to the extent that they can be solved – often they cannot) by appeal to myriad poorly- or totally-not understood heuristics, indeed it is well known that the native language as well as fieldwork experience of the fieldworker play a major role in their judgments as to appropriate IPA letter. IPA letters represent ranges of acoustic and articulatory values (the latter being inaccessible to any speech-recognition program): they are not precise things. The IPA (the association) has not promulgated definitive / authoritative reference recordings or values (especially covering the acceptable range of values, such as F1 and F2 defining [e] vs. [ɪ]). It would also be insufficient to extract such values from Peter Ladefoged's IPA performances, since Ladefoged does not define the IPA standard, and in fact he has made a point of the fact that IPA experts do vary in their pronunciations of "the same thing" (he left us with an illustration of this point for vowel here, and wrote a dissertation on the topic). Without such standard values, even if you could segment the waveform into candidate segments, and even if you have formant trajectories, that still doesn't tell you which segment you have. A simple illustration (and challenge) is that nobody can give you the defining formant values of [ɨ ə ʌ ø ʊ ɜ ɤ]. You can perhaps find formant values of [ə] is some specific language, but the goal is to find general values. The above points have focused on the easiest problem to solve: how to map steady-state properties to IPA letters. Segments are rarely "steady" (even within a defined granularity of measurement), so for instance a hallmark of the acoustics of [ɪ ʊ ɛ] in English is that their formant shift over time. This is a language-specific property – [ɪ ʊ ɛ] in Nilotic are reasonably steady-state vowels. Another issue (which is a bit easier to solve) is that IPA transcriptions can be narrow broad. You could write the distinction between "tab" and "stab" in IPA in many ways: [tæb, stæb], [tʰæb, stæb], [tʰæəb, stæəb], [tʰæəbp, stæəbp]. A narrow transcription hugs the phonetic ground and doesn't care if a transcriptional feature is redundant; a broad transcription gets the essential details of the language. To get a broad transcription, you'd need to first work out the phonological system (how did you do that?). There is an interplay between "absolutist" judgments in transcription and "rationalist" judgment. What I'm referring to is, on the one hand, a linguist has some standard in mind for "ɪ" (as an absolute thing), but that judgment is modified via exposure to many tokens and a solution to the phonemicization problem (the variation is reasoned away). This kind of reasoning takes many forms. The main form of reasoning that feeds back into transcription decisions is a phonological analysis. If there is a vowel "I" which could reasonably be spelled [ɪ] or [e], but it behaves in the phonology clearly like [i u ɨ] and not like [ɛ ɔ], then you ought to spell "I" as [ɪ]. Fieldworker decisions involve a continuous loop between conscious reasoning about the phonological system and the notion of an absolute acoustic anchor (which I regret to say is a mythical beast). 

Everybody has to deal with diacritics, be they phoneticians, syntacticians, or politicians, as long as the language involved is one of the majority of languages that uses diacritics. Diacritics are simply little marks put somewhere near a bigger letter which happens to also be useable without the diacritic. For example, the letter usually refers to the sound in English "dog"; whereas <đ> with a diacritic refers to some kind of palatal-ish affricate in some South Slavic languages, a dental fricative in Saami, and [ɗ] in Vietnamese. So diacritics are basically ways of writing speech without inventing a totally new symbol for each sound that isn't found in Latin (at least written Latin, since length in Latin has to be written with a diacritic). You're really asking 3 questions: the function of diacritics; the logical relationship between phonetics and phonology; plus, you're assuming a specific theory of what phonetics vs. phonology without technically asking whether your characterization is correct (I would say it is not, although it's only yards off, not miles off). I kinda think this question needs to be tightened up. 

The name in English (as used in the media) started as Cambodia, and changed briefly during the 70's and 80's after the fall of the Lon Nol government. The name "Kampuchea" went the way of the Khmer Rouge and Vietnamese governments of the country, and can be seen as making a break with those regimes. English usage reflects French usage, so current "Kingdom of Cambodia" = "Royaume du Cambodge", "State of Cambodia" = "État du Cambodge", "People's Republic of Kampuchea" = "République populaire du Kâmpŭchéa" and so on. General use in English (as opposed to official governmental use or use by news agencies) did not follow that political trend, and has remained "Cambodia" throughout. 

This picture may be useful (likewise this article). The thing of interest is the glottal flow derivative, modeled in this paper (fig. 2 has an analogous graph). The flow derivative is held to be the best model of the pressure wave (i.e. the source that is filtered by the vocal tract). The greatest amplitude excursion in the flow derivative is associated with the closing phase, which is fairly brief, and thus rapid. The rightward-leaning asymmetry in the wave shape has to do with glottal and vocal tract acoustic mass, delaying flow increase (when it increases, at the beginning of the cycle), plus rapid decay (when it starts to close). I follow the Liljencrants-Fant-Rotheberg rule on relating glottal flow derivative to the acoustic source: if they agree that that that is the source, I'm gonna believe them. (95% of Fant is way above my head). 

It is not entirely clear what you are asking or presupposing, especially whether you are looking for theoretical possibilities in imaginary languages, or actual counts based on English (or some other language). "Teach" is most verb-like and "teacher" is less verb-like, with "teaching" standing between the two (and potentially divisible into the forms in "I am teaching" versus "Teaching is irritating"). Is "teaching" a verb or is it not a verb? In other words, it's not clear what you mean by "verb". You have a couple answers that imply that you just need one dummy verb: actually you wouldn't need any overt thing that you'd label a verb. You wouldn't even need a verb to distinguish stative and active predicates. All actions involve relations between things ("thing" being a non-technical term meaning "anything you want to talk about"). In English, in order to describe the act of cooking porridge, you need one verb (cook) and a noun that says what you cook. In Logoori, you need just one verb ruga and no object, because the object of the verb is implicit in selecting that particular lexical item (if you want to cook beans or leaves, you would deeka them). The English verbal lexicon is thus somewhat less more compact in not including the "cook+porridge" relationship as a distinct lexical item. So it appears that you're looking for a minimal list of lexical "verbs", shifting the burden to compositional semantics (thus all forms of cooking are "cook + object"). "Fry" would be e.g. "cook with oil", "simmer" would be "cook slowly", "blanch" would be "cook quickly in water" and so on. "Cook" would itself be decomposed into a longer description involving some verbal action plus the nouns "heat" and "food", which in many languages are simply nominalizations of verbs "eat", "be warm". If you count such potential V-to-N nominalizations as not verbs, then you could theoretically eliminate "cook". So it depends on how you dispose of V-to-N derivations, and on whether you're asking for actual language facts or merely an imaginable scenario. 

Whether Norwegians and Danes living in the same place would end up speaking 1 vs. 2 languages depends on the extent to which they remain culturally Norwegians vs. Danes, or simply generalized Scandinavians. Language is one of the most volatile distinguishing cultural features, as witnessed in North America by the loss of indigenous languages where other cultural practices are more robust, also in Scandinavia among the speakers of the southern Saami languages (Pite, Ume, South Saami) where the languages are dying out but cultural distinctness survives. The result is typically not really a merger, rather speakers of one language shift to using the other language (the dominant language, however that is culturally determined). The "abandoned" language will probably leave some traces. As an example, the Kerewe language was introduced to Kerewe Island about 500 years ago, via immigrants from across Lake Victoria who established a centralized kingdom on the island. The number of immigrants was pretty small, and they had a substantial impact due to the new technology of centralized government. The result was that most people adopted that language: but the language that it (partially) displaced, Jita, had a number of influences on Kerewe, so that one might say Kerewe is a version of Haya-Zinza with 10% influence from Jita – not exactly a "merger". Similar (historically less-understood) language shifts affected the blending of the Pare and Shambaa where the tribe meet in the Gonja area: the dialect of Pare spoken there is substantially influenced by Shambaa, indicating that there was some interpenetration of the people. Again, the language is fundamentally Pare, with about 10% influence from Shambaa. Something stronger than "subtle influences" may be found in creole languages, which arise in trade contexts where many cultures meet (esp. for trade purposes) and lacking a common language, a rudimentary pidgin language develops (not a language learned by children), and then this may further develop onto a full-blown language. This process involves boiling a language down to just basic vocabulary and getting rid of complicated grammar. Pidginization especially involves unrelated languages, whereas a pidgin would not develop via Kerewe-Jita contact, since the languages are similar enough that it's easy to become bilingual. 

You can look up PIE roots from Walde-Pokorny here. This contains a link to a language index, which could lead you to the Latin list, although you'd have to know that facio is related to putrefacio and a number of other words (odd that facio itself isn't an entry), which would point you to * dhe, and that would list everything-ish coming from that root. 

This is intended to represent Leben's beliefs, with the condition that for Leben this would be a fact about underlying forms but derived representations. While L does not explicitly say this, it is observationally consistent with what L does, and L has not to my knowledge objected to this as a mischaracterization of his views at the time. G's argument against the OCP is a straightforward necessity argument: in order to account for certain of Tiv's inflectional tone patterns, it was necessary to posit HH as a morpheme-internal sequence. Underlying melodies with OCP violations are also necessitated for Etung, which contrasts the pattern LLH and LH, or HHL and HL, the problem being to say where the first tone is associated. G has available the mechanism of "accent" whereby a given vowel could be "accented" and this the initial locus of tone association. G is not satisfied with the conclusion that Etung is an "accent" language, thus OCP violation is taken to be necessary. G posits that "structure" is more highly valued than "substance", so given a representational ambiguity between treating [bákí] as two H tones vs one H linked to two vowels, the latter representation is preferable (simpler), though language facts could prove that two Hs are necessary – both are possible representations, but OCP-obedience within a morpheme would be the acquisitional default. So: (1) OCP, at the time, had very little explanatory force and was not clearly articulated as a strong principle of universal grammar until later (by McCarthy), and in tonology was there only to explain pattern restrictions – the reasons to assume the OCP were very weak to start with; (2) such pattern restrictions were found to not be universal even in the set of "suprasegmental" languages. The true rise of the OCP as a social phenomenon is McCarthy's analysis of Arabic root patterning, in his dissertation.