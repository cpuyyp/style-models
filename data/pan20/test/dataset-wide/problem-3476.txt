A bit later, amid the Bullionist controversy (regarding whether paper money should be convertible to gold or not), a myriad of writings appeared (see link). For example, in 1801, Walter Boyd wrote a letter to the primer minister William Pitt advocating for full convertibility, in order to avoid high inflation associated with private banks incentive to expand paper money under limited convertibility. A notorious economist at that time was Henry Thornton, who wrote in 1802 the book "An Enquiry into the Nature and Effects of the Paper Credit of Great Britain", partly in response to Boyd's views. Apparently, some (?) consider Thornton as the founder of central banking. Later on, Ricardo also wrote about central banking. In particular, he argued in favour of central bank independence (yet, remaining a for-profit institution). This is a quote from 1824 (emphasis mine): 

Your question is complex. First of all, what is science? Not even methodologists have settled this question. Falseability, testeability, a method? (great read here). Second, what do you mean by Marxism? Marxism is around 150 years old, and has evolved in the process. Karl Popper argued that Marxism became a pseudoscience from the moment that their successors started to add ad-hoc modifications to justify why its predictions were incorrect, a view rejected by another philosopher of science, Thomas Kuhn. If it comes to testeability, well, there are plenty of tests around of the labour theory of value (albeit not originally marxists, a key component imo) (e.g. here), tests about the tendency of the rate of profit to fall (e.g. here), or about Marx's understanding of crises (e.g. here). Under this grounds, it is a bit unfair to broadly define Marxism as unscientific. Keep in mind the two aforementioned caveats though. Finally, about modern mainstream critiques to Marxism, there are of course many. Their existence however does not provide a foundation for its obsolescence per se, since every school of thought or ideology within a discipline faces criticism (see here a critique from a mainstream economist - Paul Romer - about mainstream Macroeconomics). Here are two critiques from two mainstream Nobel prize winners that might be of interest: 

Henry is right. The units of $A$ depends on how you are defining the variables $Y$, $K$ and $L$. You could only define an "absolute value" of $A$ is you have strong reason to believe that your definitions for these variables is the "correct" one (how they are computed, their units, etc). This is not trivial. For example, notice that GDP is, by definition, a nominal variable (that is how the data is recorded/constructed). Real GDP is a derivation from nominal GDP, based on some assumed price index. Therefore, the definition of $A$ will be affected by the choice of such index, which is not trivial. Additionally, such "absolute value" of $A$ might be incorrect because the production function could be wrong. For example, the model does not include human capital, or energy, or land, etc. Or the functional form could be incorrect (CES?), or its parameters might vary over time. But even more severe, an aggregate production function (and therefore $A$) might not even exist. This is the old-but-still-relevant issue of the impossibility of aggregation. 

The are many more papers studying financial frictions and related aspects of the financial market, showing that the role of finance is very important. 

First of all, I'm not sure you are reading that article correctly, as I cannot find any mention of the claim you make. In any case, you can find here a superb comparative analysis of QE for the Fed, the Bank of Japan, the ECB and the Bank of England. Regarding money creation, the key difference between conventional monetary policy and QE is the type of asset being transacted (short-term government debt versus long-term government/private debt, respectively; again it depends on the central bank, see referred document). There is, to my understanding, no difference on how the monetary operations are carried out in each policy. In fact, these operations are quite standard. The central bank buys assets by crediting the seller's bank account, in exchange for the ownership of the assets, which are then added to the central bank assets and eliminated from the sellers assets. This is akin to you receiving a deposit in your current account. You have been credited with some amount. There is no physical money creation or physical money transfered in the process itself. There is indirect money creation (all electronic) if such transactions enable the banks to expand their credit to other institutions or companies (provided demand for such credit exist and banks are willing to do so, e.g. in an economic boom), whilst still complying with the regulation. To learn more about money creating and modern banking, I suggest this page, this page or this post. 

Uber might be just like Amazon. There was a recent Economist issue on Amazon, which, if I understood correctly, concluded that the financial market is expecting Amazon to become a dominant player in several markets in the future. These companies, by operating at the margin or even at net loss, aim to build market share, getting rid of the competition. That is very likely one aim of Uber, namely to displace standard taxis, just like Amazon has been blamed for bankrupting bookshops (e.g. in France and the UK). A bubble in valuation is also possible, just like in the dot-com bubble. Some companies reached huge valuations. For example (taken from Wikipedia): 

I wonder whether there is a single, centralised system which can alert me, say, every month, of the plethora of new economic books published or forthcoming. I know Inomics is a good centralised system for conferences and jobs alert, but I am not aware of book alerts. RePEc does not seem to provide that service either. 

Alternatively, you can use the mean of the period, or fit a linear trend and interpolate. These methods use more information than just two years, which has the benefit of accounting for possible idiosyncratic factors in 2012 or 2014, with the cost of perhaps adding idiosyncratic factors from years as far as 2017. Here is the R code with the three methods (including Alecos's): 

The key to the answer is good data on Capital. There is a project (KLEMS), which is computing harmonised (i.e. comparable) information on capital, labour, energy, etc for many countries. At the moment it has information mainly on developed countries, but data for more developing countries are coming up. For example, this is a calculation of the capital-output ratio for the case of the UK: 

First, notice that there is a trivial equilibrium at $K^*=L^*=0$. Second, there is another equilibrium with positive optimal inputs and production. Naturally, this point crosses the optimal capital-labour ratio. Importantly, the level of capital and labour can be known. For example, capital is given by the formula shown in @denesp's answer. Which equilibrium does the firm choose? As @denesp rightly noticed, the non-trivial equilibrium is preferred only if there are decreasing returns to scale ($\alpha+\beta<1$), as a positive production yield positive profits. This can be confirmed using the SOCs, or calculating optimal profits (see below). In the case of increasing returns to scale ($\alpha+\beta>1$), firms would rather not produce, as any positive level of production yield losses. Yet, the most interesting case - and that which motivated my question - is that of constant returns to scale. It is easy to see in the graph above that under CRS, the two labour functions become linear. Do they have the same slope than the optimal capital/labour ratio? This depends on whether we are thinking from a partial or general equilibrium approach: 

Have very clear which is the "long-run" relationship among variables your theory predicts. The point about cointegration is that there is at least one common trend among the variables. In your case, you would expect that the price of the three stocks move in tandem, based on some underlying market phenomena like economic growth, volatility, etc. In this case, the long run relationship between variables in likely to be in levels (price of stocks). Test for unit root on each variable in levels. There are plenty of tests here (ADF, KPSS, etc). You want to find that these variables are I(1) or maybe I(2). Perform the Johansen cointegration test. If you reject the null hypothesis of cointegration ($r=0$), then there is not a common trend among the variables, and they are not cointegrated. Do not run regressions with them in levels, as any result will be spurious. If you do happen to find cointegration ($0<r\leq n$), then estimate the VECM, from where you can get the cointegration vector(s) that define the common trend(s). 

Apparently not. A 2015 article by the author uses his index to evaluate economic growth across country and time, stopping in 2005, same as the index in the original paper. And yet, many of the other variables are in principle available for more recent years. The first best here would be for you to contact the author. He might be in the process of updating his index. Actually, if you look into his working papers, you can see he is doing some work on patents. Even more, there he offers some (more updated) data on copyright and Trademark. You might contact him to know better what this is. The second best is that you expand the index yourself. For example, you can use the data just mentioned. Alternatively, you can use the subcomponents of this or this index (e.g. property rights or investor protection). There are more indexes here (e.g. intellectual property protection), and here. One way to do this update is to take the original Park's series, and see which one of the above match its better (higher correlation, for instance). From here you obtain your "proxy". Then, simply extend Park's index with the newest data on this proxy. This is simply an exercise of "concatenation". 

As Amit said, your answer is correct. As Toby said, you can rule out negative output. There is no much insight to add as an answer really. However, I was also fairly puzzled that such a simple problem gives a complex answer. As it turns out, you get a linear demand (on income) only if the exponents on both components is the same. Consider the more general case: $$ U(x,y) = \text{min}\{ax^\alpha,by^\beta\} $$ Optimality requires (* notation for optimal values omitted) $$ ax^\alpha = by^\beta\ $$ Replacing one of these in the budget constraint leads to: $$ p_x x + p_y \left(\frac{a}{b}\right)^{\frac{1}{\beta}} x^{\frac{\alpha}{\beta}} = w $$ Here we see that only when $\alpha=\beta$, demand will be linear (on income). In this case: $$ x = \frac{w}{p_x + p_y\left(\frac{a}{b}\right)^{\frac{1}{\alpha}}} $$ It seems an algebraic solution for the general (unrestricted) case cannot be derived, as the polynomial you get might have irrational powers. 

$$ S_{t} = \beta_0 + \beta_1 S_{t-1} + \cdots + \beta_p S_{t-p} + \mu_t $$ After you have estimated the model and obtained values for $\beta_i$, you can forecast oil prices by simply lagging the last $p$ observations one period and put into the model, from where you get $\hat{S}_{t+1|t}$ (assuming $\mu_{t+1} =0$). Then, iterate forward to get forecast for any other future period. 

The Journal of Economic Perspectives is really good. It tends to be less "news", however (but still worth checking out). If your looking for policy papers then you might check out the Peterson Institute. But, frankly, it sounds like your best bet may be The Economist. And if you're not up for shelling out any money for a subscription, then the podcast is great, too. 

I think you should calculate the log of the ratio of two price series. When googling "log of relative price index", I did not find many references, but I found three which seem sufficient to me: 

A "true" labour market shortage as defined by no labour at any price has probably never occurred. Let me argue why: 

This paper presents another model. They show that "embodied" technical change (i.e. technical change which comes with new machines rather than occur amid current machines) decreases the lifetime of capital, thereby increasing the depreciation rate. This paper finds empirical support for a variable depreciation rate. 

The Bank of England also provides its estimates of the past uncertainty (what they call "backcast", in opposition to "forecast"). This is available in the last sheet of this dataset. Another interesting example comes from the Euro Area. Eurostat produces quarterly "Preliminary GDP flash estimates" for the Euro Area for selection of periods: 30, 45, and 60 days after the end of the quarter. As the page states, there is "a trade-off between timeliness and accuracy". An example of subsequent revisions are: