Wind and temperature in the vertical direction: as you increase in height, the temperature decreases due to the conservation of geopotential energy. Also the wind speed increases, due to the lack of friction. In data assimilation, spurious correlations are quite common, especially for large distances. Localization is one way to reduce the influence of spurious correlations. It is not an uncommon rule of thumb that precipitation is associated with low pressure. However, that rule of thumb does not always apply to thunderstorms, where pressure may increase in the downdraft. It is a known fact that pressure and temperature are proportional ($P=\rho R T$). However, if you look at hurricane data, warm air can strengthen a hurricane, according to the wind-induced surface heat exchange. Also, warm advection can also lower the pressure of an extratropical cyclone. Spurious Correlations has a list of examples of correlations that are not many times inexplicable, though not all are relegated to earth science. For example, precipitation in South Carolina is negatively correlated with precipitation in Richland County, Wisconsin (-0.77). 

The major reason isn't Earth's distance to the sun. The major reason is that earth's axis is tilted. The Arctic circle, for example, sees 24 hours of darkness in the winter, and 24 hours of sunlight in the summer. If you go to the other side of the equator, you'll note that the seasons seem switched- the southern hemisphere is warmer in the winter than in the summer. Take a look at how much sunlight each day has- you'll note that in the summer, the northern hemisphere has more hours of sunlight than the winter. The opposite is true for the southern hemisphere. 

I am trying to compare NDVI values computed from MODIS Surface Reflectance(MOD09) with the NDVI product (MOD13). MODIS Surface reflectance Collection 5 products include a Data State QA field that I am using to estimate which pixels are best suited for computing NDVI. A part of this field is labeled Aerosol quantity. The values in this field seem to significantly affect the result of NDVI computation. The explanation of the possible values of this field in the MOD09 Users' Guide seems not entirely clear to me. 

It seems that results with low aerosol quantity (green points) match well with the results obtained with the complex rating algorithm used in the standard NDVI MOD13Q1 product. While the data with climatology state (orange points) seems far off. What does the climatology attribute mean and how to treat it? 

The last three values are clear, while the meaning of the climatology value is somewhat non-intuitive. I have selected a crop field and calculated NDVI for all pixels clear of clouds or cloud shadows during one vegetation period and observed the following: 

That is bands 1 and 2 store and components of the wind speed at the boundary layer. parameters make up the product definition section identifiers. The first kpds number can be mapped to mnemonic GRIB identifiers, i.e. number maps to . The unit is and the data representation is 64-bit floating point. The latter piece of metadata can be extracted from the GRIB file itself e.g. using . 

Well, one side of the earth would heat up, while the other side would not. This would cause a very large thermal gradient, causing extreme wind speeds. Also, you would not have the Coriolis effect, destroying quasi-geostrophic theory. 

Now there are plenty of limitations, such as the nonlinear version of that equation. If I had my notes on hand, then I could address those problems more. I'll edit this when I can. Also, model physics cannot be computed in spectral space, so there has to be conversion from spectral to physical space and vice-versa. I'll check my notes and correct any errors I can find later. 

It is just the Weather Channel. You can tell which businesses frequently watch (or even do business with) the Weather Channel by whether or not they refer to them by these names. The naming of winter storms irks many of the meteorologists I know (including myself). 

The atmosphere, as a whole, is approximately in hydrostatic equilibrium. This means that the gravity of the earth holds the atmosphere to the earth, preventing its escape, though few molecules may escape every so often. Mathematically, this can be described by $$\frac{dP}{dr}=-\rho g$$ where P is the pressure, $\rho$ is the density, and $g$ is gravity. Using the Ideal Gas Law $$P=\rho R T$$, where $T$ is temperature and $R$ is the gas constant for air. Assuming that the temperature in the height of a column of the atmosphere is averaged ($\bar{T}$) an equation for the average height of the atmosphere can be found $$P(r,\phi,\lambda)=P_0(\phi,\lambda)exp(-\frac{(r-r_0)g}{R\bar{T}(\phi,\lambda)})=P_0(\phi,\lambda)exp(-\frac{gz}{R\bar{T}(\phi,\lambda)})$$ where $r_0$ is the radius of the earth,$P_0$ is the surface pressure, $r=z+r_0$, where $z$ is the height above the earth's surface, $\phi$ is the latitude, and $\lambda$ is the longitude. To Summarize: As the average temperature of the atmosphere increases, the height of the atmosphere will generally increase. This means that the height of the atmosphere will generally be the lowest near the poles, but highest near the equator. There are certainly exceptions to this rule, but this generally works. If you were to go around the equator, it will likely not be a "perfect circle" since the average temperature would have to be exactly the same. 

This is not a definitive answer but a few points from someone who has spent some time with remotely sensed temperature data. I guess your best bet to get high spatial and temporal resolution surface air temperature would be to get reanalysis data from weather models and augment it with weather data from ships. Purely satellite retrievals of surface air temperature are not very reliable as this is a rather indirect measurement. Consider Level 2 AIRS 50 km (approx. 0.5 deg) Surface Air Temperature data layer ($URL$ The spatial resolution is much lower compared to surface radiative temperature products. This is in order to compensate for the uncertainty in the atmospheric state and for lower resolution of the sensor that needs to balance its very high spectral resolution with lower spatial resolution to achieve usable SNR levels in each spectral channel. Speaking of the reanalysis data the NCEP Global Data Assimilation System has 1 deg (approx. 100 km) spatial resolution. However depending on the region of study you might get higher resolution from regional weather models provided by local meteorologists. So you could start with the NCEP data spatially interpolated from 1 deg to 0.01 deg as an initial approximation. 

In addition to the rarity, there is also the problem of additional computations. To find out if there is a solar eclipse, you need extra calculations. Since the introduction of additional calculations can slow down the model, especially since radiation is oft quoted as the most expensive physics parameterization, introduction of the astronomy may provide unnecessary calculations. A solar eclipse, while influential, is only influential in the short term, so it is often ignored. Even then, the data assimilation methods can correct for any error after the eclipse (though that would be an interesting project). Plus there is also the complication of clouds, etc. Any forecaster who pays attention to the news knows about the eclipse. Any forecaster who knows the inner workings of models (though many don't) should know that eclipses are ignored in radiation parameterizations. So forecasters should know that it will impact their forecast, but by how much, was uncertain. There are things that are ignored in weather models, but we know we ignore them. We ignore them because the are small, difficult to moddel, poorly understood, etc. Such things include aerosols, tropospheric chemistry, tornadoes, heating from lightning, anthropogenic heat flux, drag from buildings.