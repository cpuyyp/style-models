Does this mean, that if I used same password to create and protect each DMK in my databases across all instances, the key will be the same? If not, it is OK to create a DMK on one database, and to restore it on all instances in order to have the same key everywhere and to need make a backup of only one key? 

I am trying to detect what is causing this huge grow. One of the possible reasons explained here is . I have try to find this particular error: 

This will probably work, but the will be force to learn how to work with the interface, and the application functionalities will consume a lot of time to develop - it will be easier to use tool that is already created and the user is familiar with (for example excel). I know I can export/import data between T-SQL table and excel file, but is there a way to build some kind of collaboration between them - for example the excel to call a stored procedure which is returning the data (it will be nice if I can send a hash of the data or the rows in order to return only the rows that have been changed). And to call a stored procedure which can upload/delete/insert data in the table. If not, I guess I am going to work with files - download the file, edit the data in excel, then upload it, if there aren't other ways? 

It seems when other data is extracted from the table (we are filtering by now) the engine is not able to use the spatial index and performs clustered index seek instead. 

I am wondering if I am using the hint, the table will be blocked for inserts. In the documentation is said that: 

Is each field stored or only the ones? If you are using the of the table to read the whole record, are fields that are stored out of the row read, too? 

I want to create a new column master key using certificate created in advanced. I use the following command to create a certificate in the . 

Its and I want to know if the is using the latest installed version? I want to create function using and the must use this version either. If I install version, will the server automatically start using it? 

I am going to rename all of the above objects in order to match our internal naming conventions but I want to write an automatic script for doing this (as there are thousands of constraints). The only issue I can think off is that if index is used with a in statement it will be broken. Could you tell any other reasons not to use the for such purpose? 

Let's say, I have a database that contains two file groups - and . Then, I am creating a partial backup of the secondary file group. Now, let's create a new database (there might be difference in the database objects in the primary file group, but the tables in the read-only file group will be the same as this in the database). Is it possible to perform piecemeal restore of the file group but using the new database? 

In standard edition we can use dynamic-data-masking. The masking of data is controlled using the built-in security, for example: 

I have found the following queries to detect the CPU usage by database, but they are showing different results: 

I have table with 42 columns and a trigger which should do some stuff when 38 of these columns are updated. So, I need to skip the logic if the rest 4 columns are changed. I can use UPDATE() function and create one big condition, but prefer to do something shorter. Using COLUMNS_UPDATED I can check if all of certain columns are updated? For example, checking if column 3, 5 and 9 are updated: 

Additional Information With we have the availability to use functionality with the of the server. One is able to create a certificate in the and mapped this certificate to - then, use the to protect used for encrypting data in particular column. Using is handy: 

So, using password is not best way to protect it. I was not able to find any pros and cons for using the other techniques. Can anyone tell is there are difference from security or performance matter or it is just a matter of choice? 

I just wanted to check if I can get different dates using sys and not sys date time function. And of course it is possible. 

where the is the current user location. For two different users, I get two different execution times - 0 seconds vs 5 seconds and I want to understand what is causing this. I get two identical execution plans: 

The general goal is to reduce the first database size and put it on SSD drives and put the historical not queried data on slow storage. The tables will have less indexes and can be additionally compressed via row,page,column store or clr compressions. It will be working under simple recovery model. I have never used linked servers and have the following questions: 

I have perform some test with relatively small tables and the encryption took a very long time (maybe because there are many SQL objects that are referring the target table). I am wondering(worrying) when a table is being encrypted, is it possible to read the encrypted data or query the table? I only found this: 

I guess this should be SP1 issue, because I have checked that I had successfully install standard edition of SQL Server 2014 without SP1. 

I know that when columns are used the data is stored - the data row will have a pointer to another location where the 'large value' is stored. I have the following questions: 

Can I create a key vault in (let's say) and used the keys to protect keys in SQL instance running on machine hosted in ? 

I am wondering what could cause such LOB reads when the plan is the same and the data size and rows of each operator is the same (except the one)? There is another difference in the operator - the of the faster query is 1142 and for the slower is 1180. 

Basically, the index is created for 20 minutes, so I need a way to guarantee user experience is not blocked. The alternative is to have separate window for maintenance in which the database to be in single user mode. 

Does this mean that I do not need to care(renew, backup, etc) about Service Master Key and Database Master Key as I am using external keys to protect the Symmetric Keys? When a new key vault is created the location is specified: 

I have found that you can do on columns using the clause(example here and here). Unfortunately, there is nothing mentioned about performance optimization in the official documentation. Could anyone tell is there any optimization at all? Has anyone made some performance test? Also if there is optimization I guess it will affect only specific situations, too. 

When the temporary table is used, the correct count value is used for the rows returned by the function. All involved indexes are rebuilt. Why I am worried about this? This join is part of more complex query which execution is very slow. I believe this is due to the fact that the statistics that are used by the engine are wrong and not the optimal execution plan is used. Even when the query use parallelism again nested loop is used to join the tables. If I use a or hints on join the correct statistics are used and better execution plan is built but I prefer to find a way to provide the correct statistics instead of forcing the engine to do something. Could anyone tell why always one row is expected? Maybe because table-valued function is used, the engine thinks one row is going to be read from the table/index? 

Of course, this is not working for the users who are members of the database role. Is there a list with security roles or cases showing when the is going to work? I have found a database engine permissions but it's too complicated and does not seem to show when data cannot be masked from a user. 

So, it seems that really the first row is now occupying more space (8 KB exactly). So, I add one more row and do the test again: 

I have a databases that are regularly (monthly) updated. An update includes a lot of new or changed T-SQP procedures, functions, indexes creation, etc. A has transaction log file with the following settings: 

Case scenario The functionality is used to encrypted all data in a particular column. I need to encrypted only some of the data, so I need to use a or . I want to protect them with a certificate which is stored in the in order to control the access to the keys from there. Is this possible to create such certificate? 

I need to rebuild the clustered index of the table that has more then 1.6 millions rows, more then 20 non-clustered indexes and 80 columns in order to reclaim space after two columns were deleted. I have try using the following command: 

So, value for column and , and value for column because it is set in the first bit of the second byte. If I change the statement to it will check if columns and or column is/are updated? How can apply logic in the context of one byte? 

Its body statement is executed for 0 seconds (with given parameters). The function itself (with the same parameter is never executed - I was waiting for 20 minutes and only part of the result was returned). There must be something wrong with the statistics and respectively the query execution plan that is created because the function is executed instantly: 

I have checked thet the is decpreated. Does this mean that the results from the second query are wrong? 

I am trying to compress some tables that have fields. Unfortunately, the and the compression do not have the desire impact (only ~100/200 MB saved for 20 GB table). Also, I am not able to apply column store and column store archival compressions because they do not support compression of fields. Can anyone tell if I have any alternatives here? I also guess the and compression do not have effect because the content of the columns is unique. 

Add column to the table called with default constraint and When data is going to be encrypted the flag is raised and the output encrypted value will be set in the existing column 

Knowing the security objects hierarchy, we can see that in order to create a and encrypt data, we need to create: 

I want to either compressed them or move them to cheaper(slow) storage. Because of the tables structure (there are columns) I was not able to apply or achieved good results using row, page, column store or column store archive compressions. So, I have decided to move them to cheaper storage. I guess I have to options here, but let me know if I am wrong about something: 

I want to execute the code above after each update, but sometimes I need to execute the command above many times in order to reduce the database tlog file (each time I have to wait 15 minutes for the tlog backup to complete). The database are in mode with , end the tlog backups are made on the secondary databases. Could anyone tell if the idea for regular shrink is bad and why sometimes I need to executed the shrink several times? 

As you can see each entry is updated in different times on different date. Could anyone tell why this is happening? 

As it is said here it is a good practice to backup the database master key, because when the database is moved to another instance it will need the original DMK that has been used to protect the certificates in order to skip regeneration of the encrypted stuff. In the context of database backup encryption, when the DMK is stored in the database with the rest of the certificates used for backup encryption, do we need to create a backup of the DMK? Also, I am wondering, as it said that 

I have recently learned about sp_tableoption which gives you the ability to store "large value types out of row" for particular table. A colleague of me told me if I enabled it, each field is stored on 8 KB page at least, no matter how large it is. I was not able to find a confirmation of this, so I run the following test: 

I am going to test the both variants of course but before doing that I am wondering is there another option or any issues in the above ones? 

Is it possible to set the name of a unique constraint when it is part of table variable function definition? 

I guess this means I need to use one of these types of encryption in order to be sure the data is protected? 

In , we can use module to simplify this hierarchy. For example, we can store the certificates in the and these certificates are protecting our encryption master keys. I am wondering, if I want to use not built-in encryption functionalities, can I use module to create and manage my symmetric keys (like it is shown on the diagram). In the CREATE SYMMETRIC KEY documentation we have option, but not enough information about possible providers and examples. I am interesting in storing the which is protecting the Symmetric keys or the Symmetric keys in external storage, because in such way the data is separated from the keys and in the database we are storing only references to the keys (like in always encrypted). Windows Certification Store will be best option for me, but Azure Key Vault or something else will work as well, I guess. 

I am going to a large amount of data from one table to other table. The target data source table has foreign key constrains, so I am not able to use , but need to have a table instead. The table is not going to be read by other users, but the (in which I am inserting records will be frequently used during the operation) - so, I need to do the migration without blocking the other users activity. The database is under recovery model, and transaction log backup is made on every minutes (the transaction log growth during the operation is not an issue anyway). I have the following statement: