Let $I$ be a subinterval of $\lbrack 0,1\rbrack$. How well can one hope to approximate it in the $L_1$-norm (on $\mathbb{R}/\mathbb{Z}$) by a trigonometric polynomial of degree $\leq R$, that is, a function $$x\mapsto \sum_{r=-R}^R c_r e(r x),$$ where $e(t) = e^{2 \pi i t}$? Are substantially better approximations possible if we allow a linear combination of any $2 R + 1$ different exponentials $e(r x)$ (for not necessarily consecutive values of $r$)? One can ask the same questions about the sawtooth function x\mapsto {x} instead of $1_{I}$; indeed the two settings feel equivalent. (There are functions related to Beurling-Selberg majorant that may be useful to consider; I've taken a look again at the first chapter of Montgomery's Ten Lectures. It is still unclear to me whether such functions give something approximately optimal with respect to the $L^1$ norm.) 

For $x\geq 1$, $$\rho(x) = \sum_{n\leq x} \frac{\mu(n)}{\sigma(n)} \log \frac{x}{n}.$$ As $x\to\infty$, this sum tends to $\zeta(2) = \pi^2/6$. Is it in fact the case that $\rho(x)\leq \zeta(2)$ for all $x\geq 1$? (Computations confirm this for $x\leq 65000000$; this takes just a minute or two, but I'm using interval arithmetic, and precision issues preclude me from going further.) How would one go about proving such an inequality? (We can derive an asymptotic expression for $\rho(x)$ from an asymptotic expression for $\check{m}(x) = \sum_{n\leq x} (\mu(n)/n) \log x/n$ (bounded by Balazard and Ramaré); the hard part is showing that the error term is negative.) 

Let $f:\mathbb{R}_0^+\to \mathbb{R}$ be defined by some combination of the four basic operations and square roots. (The argument of square-roots is assumed is to be non-negative, and the value of square roots is defined to be non-negative as well.) How can one find the global maximum and minimum of $f$ numerically, with full rigor? (A rigorous numerical solution has to be given as an interval, e.g., [1.500001,1.500002], within which the maximum (say) lies.) Rather convincing but not quite rigorous method: Plot $f'(x)$, see roughly where the zeroes are, and narrow them down using some standard (e.g. SAGE, mathematica). Then verify this (non-rigorous) datum rigorously simply by checking that $f'(x)$ has different signs at $x=x_0-\epsilon$ and at $x=x_0+\epsilon$, where $x_0$ is an alleged zero of $f'(x)$. Compare the values of $f(x)$ at all $x_0$, and also at $x=0$ and as $x\to \infty$. There is really only one way in which this procedure isn't rigorous: there is no guarantee that there aren't any zeroes of $f'(x)$ we have missed. (If $f$ is a polynomial, this can be easily dealt with, but $f$ is not necessarily a polynomial.) Are there any (free) programs out there that for the minimum and maximum of $f$ rigorously? 

Let $K$ be a finite field with $p$ elements. (a) Let $f\in K\lbrack x\rbrack$ be such that (i) $\deg(f)<p$ and (ii) $f(2x) = f(x)$ for $\geq (1-\epsilon) p$ values of $x$ in $K$. What can we say about $f$? (b) Let $F\in K\lbrack x,y\rbrack$ be such that (ii) $\deg(F)\leq \sqrt{2 p}$ and (ii) the curves $F(x,y)=0$, $F(2x,y)=0$ and $F(4x,y)=0$ intersect at $\geq (1-\epsilon) p$ points $(x,y)$ (in $\mathbb{A}^2(K)$ or in $\mathbb{A}^2(\overline{K})$ -- it's equivalent). What can you conclude about $F$? Here is a partial answer to (a) that does not quite satisfy me. Write $f=\sum_{j=0}^{p-1} a_j x^j$. Then the function $f(2x)-f(x) = \sum_{j=0}^{p-1} (2^j-1) a_j x^j$ takes the value $0$ at all but $\epsilon p$ points of K. Hence, $f(2x)-f(x)$ is of the form $P(x) (x^p-x)/\prod_{s\in S} (x-s)$, where $\deg(P)\leq \epsilon p$ and $S$ is a subset of $K$ with $\leq \epsilon p$ elements. (Of course, this says that $f(2x)-f(x)$ has a nice form, but doesn't quite give us a nice form for $f(x)$, at least not that I can see it.) 

My intuition is that the answer to (a) is "no". As for whether the answer to (b) should be "a constant": I do not know! It is easy to show that $\ell\ll \log n$. I can show $\ell\ll \log \log n$ (not a great deal harder). 

How to go about this? One inviting approach is to use the fact that the Mellin transform is an isometry. (That is, if $F$ is the Mellin transform of $f$, then $\frac{1}{2\pi} \int_{-\infty} |F(\sigma+it)|^2 dt=\int_0^\infty |f(x)|^2 x^ {2\sigma-1}$.) Recall that $\zeta(s)/s$ is the Mellin transform of $x\mapsto \lfloor 1/x\rfloor$. An obvious (even crass) sub-approach would be to let $$g(x) = \frac{1}{2 \pi i} \int_{\sigma-iT}^{\sigma+iT} \frac{\zeta(s)}{s} x^{-s} ds,$$ and then note that the Mellin transform of $\lfloor 1/x\rfloor - g(x)$, evaluated at $\sigma+it$, equals $0$ when $|t|< T$ and $\zeta(s)/s$ when $|t|>T$. (I'm papering over the fact that we should check that there is no pole at $s=1$, so that we can analytically continue the Mellin transform to the left of $\Re s=1$.) It follows, by isometry, the quantity in the first displayed equation above (the integral over the tails) equals $$\int_0^\infty |\lfloor 1/x\rfloor - g(x)|\cdot x^{2\sigma-1} dx.$$ We can write down $g(x)$ explicitly, though it is not completely trivial; if I understand correctly (see Theorem 5.2 in Montgomery-Vaughan), $$g(x) = \frac{1}{\pi} \sum_{n\geq 1} \mathrm{si}(T \log n x),$$ where $\mathrm{si}$ is the sine integral function. Of course the problem here is that $g(x)$ is not a particularly pleasant function to work with. My instinct is to work instead with a function of the form $$\frac{1}{2\pi i} \int_{\sigma-iT}^{\sigma+iT} \frac{\zeta(s)}{s} \phi(s) x^{-s} ds,$$ where $\phi(s)$ is close to $0$ for $s = \sigma+i t$, $|t|\geq T$, and is close to $1$ on most of the segment $\lbrack \sigma-iT,\sigma+iT\rbrack$. I haven't hit on the right choice of weight $\phi$, though. It should give some sort of advantage over $g$ above, or else it is pointless. Any other approaches? Is this all known (explicitly)? 

Let $$F(x) = \sum_{d\leq x} \frac{\mu(d)}{d} \log \frac{x}{d}.$$ s it possible/feasible to give an elementary proof of the fact that $F(x)= 1 + o(1)$ (and, ideally, $1+O(1/\log x)$, or better)? By "elementary" I mean "using the properties of $\zeta(s)$ only for $\Re(s)\geq 1$, and of preference only for $s$ real". (Call work with $s$ complex, $\Re(s)\geq 1$, "semi-elementary" if you wish.) I'd also need for it to be possible to make the bounds nicely explicit. [Note: I am well aware of Ramaré's and Balazard's work, which relies on estimates on $\sum_{m\leq x} \mu(m)$ (derived in turn from estimates on $\sum_{m\leq x} \Lambda(m)$). I am looking for (semi-)elementary estimates, in part because I would like something that can be easily adapted to analogous sums.] 

Let H be a subgroup of Sym(n) that is not too large - say $|H| = |Sym(n)|^{o(1)}$. (A group that is especially interesting in this context is the abelian group H = {products of powers of $\pi_1$,...,$\pi_k$}, where $\pi_1$,$\pi_2$,...,$\pi_k$ are disjoint cycles of distinct lengths, k about $\sqrt{n}$, each length also about $\sqrt{n}$.) If I conjugate H by a few random elements, will H stick in different directions? To make this precise: will the map $H \times H \times ... \times H \to Sym(n)$ given by $(h_1,h_2,...,h_l)\mapsto g_1 h_1 g_1^{-1} ... g_l h_l g_l^{-1}$ ($l=O(1)$) be in some sense close to being injective for g_1,...,g_l random elements of Sym(n)? (By "close to being injective", I mean, say, that if two tuples $(h_1,h_2,...,h_l)$, $(h_1',h_2',...,h_l')$ map to the same element of Sym(n), then one of the elements $h_1 h_1'^{-1}$, ... , $h_l h_l'^{-1}$ has small support (support < 0.1n, say).) 

Say you are allowed to use Fourier analysis, complex variables, Euler-Maclaurin, etc., but no complex analysis - no holomorphic continuations, no definition of analytic function, and, in particular, no recourse to the concept that every analytic function $f(s)$ vanishing at $s_0$ behaves like $(s-s_0)$ near $s_0$. (You are still allowed to use that fact for a specific function $f$, if you can prove it for that function $f$.) How would you prove that $lim_{\sigma->1^+} \zeta(\sigma+it) \ne 0$ ($t$ real and fixed)? All the proofs I know (with or without explicit recourse to $\zeta^3(\sigma) |\zeta(\sigma+it)|^4 |\zeta(\sigma+2it)|\geq 0$ or the like) use the fact that, if the limit were 0, then $\zeta(\sigma+it)\sim (\sigma+it-1)$ for $\sigma$ near 1. (Motivation: of course, I am trying to present a proof of the prime number theorem with plenty of analytic ideas but no complex analysis.) 

I am doing something with the curve given parametrically by $y = (-ar+b) r$, $x = \sqrt{r^2-y^2}$ for $r\in \lbrack (b-1)/a,b/a\rbrack$. It is nice enough (and of low enough degree) that I suspect it has been studied before - and, in particular, that it has a name. Does it? 

Let $\Sigma_n\subset G$ be a set of generators of the symmetric group $S_n$. It is a well-known conjecture that the diameter of the Cayley graph $\Gamma(S_n,\Sigma_n)$ is at most $n^C$ for some absolute constant $C$. (The diameter of the Cayley graph is just the maximum of $\ell(g)$ for $g\in S_n$, where $\ell(g)$ is the length of the shortest word on $A \cup A^{-1}$ equal to $g$.) For $\Sigma_n$ of bounded size, the diameter cannot be less than a constant times $\log |S_n|$, i.e., a constant times $n\log n$. It is clear and well-known that, for $\Sigma_n = \{(1 2),(1 2 \dotsb n)\}$, the diameter of $\Gamma(S_n, \Sigma_n)$ is at least a constant times $n^2$. (It is also at most that.) Are there any examples of generating sets $\Sigma_n$ for which the diameter is larger than $n^{2+\epsilon}$ for every (or infinitely many) $n$? Larger than $n^2 (\log n)^A$ for some $A>0$ and infinitely many $n$? 

Let $V$ be an inner-product vector space (real, of very large but finite dimension, if you wish). Let $S:V\to V$ be a symmetric linear operator. Let $v_1,\dotsc,v_k\in V$ be a collection of vectors of norm $1$, orthogonal to each other. (Here $k$ may be thought of as larger than a constant, but possibly much smaller than $\dim V$.) Assume that $(S v_i,v_i)>\delta$ for every $1\leq i\leq k$ and $|S v|_2\leq |v|_2$ for every $v\in V$. Question: Can we conclude that the space W spanned by all eigenspaces with eigenvalue $\geq \delta/2$ (say; $\delta^2$ or $\delta^{100}/100$ would also do) has large dimension? By "large" I mean something in the scale of $k$, $\sqrt{k}$, $\delta k$ or thereabouts. It is easy to give a very weak bound ($\gg \log(O(\delta^2 k))$; see below). Question, version 2: The same question, but with the added condition $(S v_i, v_j)=0$ for all distinct $1\leq i,j\leq k$. 

The aim of this answer is to sketch a proof of the fact that there are at most $\epsilon p$ solutions to $2^{2^{2^x}} = x \mod p$. The original question -- namely, to show the same for $2^{2^{2^{2^x}}} = x \mod p$ -- remains open for now. Suppose there were $\gg p$ (meaning: $> \epsilon p$ for some fixed $\epsilon>0$) solutions to $2^{2^{2^x}} = x \mod p$. Then there would have to be a bounded constant $k$ such that $x$ and $x+k$ are both solutions for $\gg p$ values of $x$. For all such $k$, $$2^{2^{2^x}}+k = x+k = 2^{2^{2^{x+k}}} = 2^{2^{2^k 2^x}} = 2^{(2^{2^x})^{2^k}} \mod p.$$ Writing $y$ for the integer in $\{0,1,...,p-2\}$ congruent to $2^{2^x} \mod p-1$, we obtain that there are $\gg p$ elements $y$ of $\{0,1,...p-2\}$ (or $\{0,1,...,p-1\}$) such that $$2^{y^{2^k}} = 2^y + k \mod p.\;\;\;\;\;\;\;\;\; (*)$$ By the same reasoning as before, this implies that, for any $r$, there is an $(r+1)$-tuple of distinct constants $l_0=0,l_1, l_2,...,l_r$ such that, for $\gg p$ elements $y$ of $\{0,1,...p-1\}$, (*) is true for every $y+l_i$, $0\ll i \ll r$. Now, set $r = 2^k$. The $r+1$ polynomials $$(y+l_i)^{r},\;\;\;\;\;\; 0\leq i\leq r$$ are linearly independent (because this is true over $\mathbb{Z}$ or $\mathbb{R}$: Vandermonde matrix is non-singular), but, since they each have r+1 coefficients, they and any other polynomial in y -- in particular, the polynomial y -- must be linearly dependent. Hence, there are (bounded integer constants) $c$ (not zero) and $c_i$, $0\leq i\leq r$, not all of them zero, such that $c y = \sum_{0\leq i\leq r} c_i (y+l_i)^{2^k} = 0$. Therefore, $$\prod_{0<=i<=r} (2^{(y+l_i)^{2^k}})^{c_i} = 2^{\sum_{0<=i<=r} c_i (y+l_i)^{2^k}} = 2^{c y} \mod p,$$ and so $$\prod_{0<=i<=r} (2^y + k)^{c_i} = 2^{c y} \mod p.$$ Setting $z = 2^y$, we see we have an equation $$(z + k)^{\sum_{0<=i<=r} c_i} = z^c \mod p.\;\;\;\;\;\;\;\; (**)$$ supposedly satisfied by $\gg p$ elements of $\{0,1,...p\}$. Let $C = \sum_{0<=i<=r} c_i$. If $C\geq 0$, (**) is just the equation $$(z+k)^C = z^c \mod p;$$ if $C<0$, (**) is equivalent to the equation $(z+k)^C z^c = 1 \mod p$. In either case, we have an equality between two identical polynomials. Such an equality ($\mod p$) can have at most a bounded number of solutions. Contradiction. 

I need to evaluate some (one-variable) integrals that neither SAGE nor Mathematica can do symbolically. As far as I can tell, I have two options: (a) Use GSL (via SAGE), Maxima or Mathematica to do numerical integration. This is really a non-option, since, if I understand correctly, the "error bound" they give is not really a guarantee. (b) Cobble together my own programs using the trapezoidal rule, Simpson's rule, etc., and get rigorous error bounds using bounds I have for the second (or fourth, or what have you) derivative of the function I am integrating. This is what I have been doing. Is there a third option? Is there standard software that does (b) for me? 

Let $w$ be a word of length $2\ell$ chosen at random on the alphabet $\{x_1,x_1^{-1},x_2,x_2^{-1},\dotsc,x_k,x_k^{-1}\}$. By the reduction $\rho(w)$ I mean what you obtain by deleting substrings of the form $x_i x_i^{-1}$ or $x_i^{-1} x_i$ (and repeating, and repeating, until you cannot do it further). What is the probability that the length of $\rho(w)$ is at most $2 s$, where $s\leq \ell$ is given? (In the special case $s=0$, there is a good (indeed optimal in the limit $\ell\to \infty$) bound due to Kesten ("Symmetric random walks on groups", 1959).) 

Yes - the standard proof of Vinogradov's result by means of the circle method gives this result. You just need to examine an integral $$\int_{\mathbb{R}/\mathbb{Z}} (\widehat{f}(\alpha))^2 \widehat{f}(-\alpha) e(-\alpha N) d\alpha$$ instead of $$\int_{\mathbb{R}/\mathbb{Z}} (\widehat{f}(\alpha))^3 e(-\alpha N) d\alpha.$$ Here $\widehat{f}(\alpha) = \sum_n \Lambda(n) e(\alpha n) \eta(n/N)$, where $\eta$ is any weight supported in $\lbrack 0,1\rbrack$. 

If, starting from a measure $v$, you apply the proposition over and over again, you obtain that v*v*v*...*v is very flat, i.e., has very small $\ell_2$ norm. This is a weak form of "mixing", as in "mixing time". You can then deduce a stronger form of "mixing" (equivalent to the expansion property) from this by using the high multiplicity of non-trivial eigenvalues of $SL_2(\mathbb{Z}/p\mathbb{Z})$ (an idea that already occurs in this context in the work of Sarnak-Xue). Incidentally, there is a simplified proof of the flattening lemma in Tao's notes (referred to below). As before, the main idea is to chop up the measure into chunks to reduce the lemma to, um, Helfgott's theorem on growth in $SL_2(\mathbb{Z}/p\mathbb{Z})$. 

This turns out to be an easy problem. For any $\epsilon>0$, we can cover all of $\mathbb{H}$ except for a domain U of area $\epsilon$ and its (horizontal) translates by $\mathbb{Z}$ using words on $S = \left(\begin{matrix} 0 & -1\\ 1 & 0\end{matrix}\right)$ and $T = \left(\begin{matrix} 1 & 1\\ 0 & 1\end{matrix}\right)$ that involve at most $\ll \log(1/\epsilon)$ appearances of the involution $S$ (which corresponds to the map $w:z\mapsto -1/z$), and at most $\ll 1/\epsilon$ appearances of $T$ (which corresponds to the map $z\mapsto z+1$). This is (a) essentially optimal: $(D+n)^{-1}$ (where $D$ is the fundamental domain) has Euclidean area $\gg 1/n^2$, and $ST^{-1}ST...ST^{-1}ST(D)$ ($(\log \epsilon)$ repetitions) isn't that small either; (b) good enough in practice, if, say, we are producing a poster: powers of $T$ come for free, whereas each appearance of $S$ corresponds to one more step in the continued fraction algorithm (the obvious one; I just checked that it's the same as the one in page 7 of the paper Igor R. kindly linked to in the above). Sketch of proof. Write $V$ for the closure of the standard fundamental domain $D$. The union of $V+\mathbb{Z}$, $V^{-1}+\mathbb{Z}$ and $(V+1)^{-1}+\mathbb{Z}$ covers all of $\mathbb{H}$ except for a set contained in $\{x+i y: 0< y < 1/(2\sqrt{3})\}$. Let us look at the intersection of that set with $\{x+i y: |x|\leq 1/2\}$. All of its elements $z$ satisfy $|z|\leq 1/3$. Hence, in a neighbourhood of $w^{-1}(z) = -z^{-1}$, the map $w$ shrinks everything by a factor of at least $3-\epsilon$. Now repeat. (As for the map $z\mapsto z+1$: the map $w$ shrinks anything in $\{x + i y: y>0, n+1/2\leq x\leq n+3/2\}$ by a factor $\ll 1/n$.) 

Let $G$ be a linear algebraic group over a field $K$. (Say $K=\mathbb{F}_q$ or $K=\mathbb{C}$; do not assume $K$ is algebraically closed or of characteristic $0$.) Let $H_1$, $H_2$ be algebraic subgroups of $G$. Consider the multiplication map $\phi:H_1\times H_2\to G$. The image of $\phi$ is a constructible set, i.e., a variety $H$ with perhaps a few varieties of lower dimension deleted from it. (This is a special case of a result of Chevalley's.) Question: when is $H_1(K) H_2(K)$ equal to $H(K)$? There are two issues here: closure (i.e., really getting a variety rather than a constructible set as the image) and rationality. Getting more specific, since the question above may be too hairy in general: (a) Assume that $G$ is solvable. Does that help? Can we then answer the question in the affirmative? (b) Say, furthermore, that both $H_1$ and $H_2$ are in the same unipotent subgroup of $G$, or that $H_1$ is unipotent and $H_2$ is a subgroup of a corresponding maximal torus. Does that help?