I have created a small SSIS solution to illustrate my problem. Unfortunately it looks like I can't attach it here, so I will try to explain. I can send the solution to people individually. SSIS SQL Server 2008. I have a package called "Parent_Package" with a single variable: "User::Log_Path". During design time I assigned this value to it: "C:\temp\Log_Design_Time". The package has a log provider configured (simple text file). The connection string of the log file has an expression: @[User::Log_Path] + "\" + @[System::PackageName] + [expression for timestamp] + ".txt". As you can see, I would like to create a new log file for each package execution. If I now run this package, I will have two log files generated! One will have a timestamp from around the time I created the package. This file only has the header fields and no other other information. The second file has the correct timestamp and correct output information: 

I would like to ask for help coming up with a query, which can identify groups on non-overlapping records. Here is a sample scenario (admittedly contrived). Let's say I have employees who are assigned to work on various projects. While an employee can be assigned to multiple projects, he/she can only work on one project at any given time (don't you wish we all had this luxury :). I need to find out which projects can be scheduled to be worked on in parallel because they do not share any employees. Here is some code to setup sample tables and data. 

Obviously it's still looking for the path specified earlier, but why? Again, I set "Delay Validation" = True on all my connection managers and the package itself. I appreciate any help on this. Thank you! P.S. Here is a complete expression for log path: @[User::Log_Path] + "\\" + @[System::PackageName] + "_" + (DT_STR, 4, 1252)DATEPART("yyyy", @[System::ContainerStartTime]) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mm", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("dd", @[System::ContainerStartTime]), 2) + "_" + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("hh", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mi", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("ss", @[System::ContainerStartTime]), 2) + ".txt" 

Has anyone experienced behavior such as this, or know what could be causing index statistics to go awry so quickly across so many tables? Any recommendations for diagnosing next time this occurs? Manually running the stored procedures and diagnosing the execution plans have proved rather unhelpful as the statistics its reading (Expected/Actual Rows) are the same. 

I'll end on a capture from RedGate SQL Monitor over the past 24 hours. The primary point of note is the CPU utilization and number of waits - during our peak hours yesterday, we were experiencing heavy CPU use and wait contentions. After this simple fix, we have improved our performance tenfold. Even our disk I/O has reduced significantly. This is a seemingly easily overlooked setting that can improve virtual performance by an order of magnitude. At least, it was overlooked by our engineers and a complete d'oh moment. 

However, even after dropping the trigger, restarting MSSQLSERVER, and even the entire server, events are still being captured. I dug even deeper and I browsed to within the registry, and the was not explicitly specified. So, I manually added it with a value of for . Restarted MSSQLSERVER again to no avail. Set the value to for and restarted... still capturing all login events. What else could contribute to this behavior? 

I am attempting to update a query that utilizes the operator within a clause predicate with to compare potential performance improvements and better understand what is happening behind the scenes when the two are interchanged. It is my understanding that in practice, the query optimizer treats and the same way whenever it can. I'm noticing that when the query is ran with the operator, it returns the desired result set. However, when I replace it with the equivalent, it pulls in all values from the primary table I want to filter. It is ignoring the provided input values passed to and returning all possible distinct values. The use case is relatively straightforward: the query accepts a pipe delimited string that can contain up to 4 values, e.g. or . From here I the input into a table variable to determine the corresponding internal for the . The result set returned is then filtered to exclude the that were not passed. To illustrate, here is a similar table definition: 

Initially everything seems to be ok and the results indicate that all of the tables match. Next I go over to the subscriber and manually delete a few rows from some of the tables. I manually verify that the row counts in my tables are now different between the publisher and the subscriber. Finally I run the sp_publication_validation procedure again and .... it says that everything is still OK. This is wrong! I also tried to return both rowcnt and checksum and it still doesn't detect the fact that there are differences between the publisher and subscriber. I appreciate any ideas. Thank you! 

I think I’ve figured it out (at least partially). It seems that in order to avoid creation of duplicate log file for the child package the value of “Log_Path” variable in the “child” package have to be made blank. If there is no value, the validation process will not create an extra file and will properly inherit value specified in the “parent”. This still doesn’t fully resolve the issue with the “parent” package, because I can’t run it from the development environment without any value specified for the “Log_Path” variable. The only way I found around that is to make it blank, save it and then execute it from the command line (DTExec) while passing the desired variable value via SET option. This finally results in just two files instead of four. I still don’t understand why validation process (at least I think it’s validation process) creates those “extra” files using design-time values. This just seems like a wrong behavior. 

I have a transactional replication setup between the two servers and I noticed that if a run a statement similar to this: UPDATE mytable SET mycolumn = mycolumn the replication somehow knows to ignore this transaction and it does not get applied on the subscriber. I have confirmed it by running SQL Profiler and also by adding TIMESTAMP column to my subscriber table (it does not change). I suspect there is some sort of mechanism, which enables this kind of "smart" behavior and I was wondering if anybody could shed some light on it. Thank you! 

The data it stores is enumerated based on a JavaScript method that uses information from their browser (I don't know much more than that, but could find out if needed): 

We have been experiencing an odd behavior in our application where various modules will begin to timeout in SQL Server 2012. Each time we stumble across this issue, we find that the statistics require update and that running fixes the issue. After updating index statistics, the timeouts go away. However, the frightening issue is the frequency in which we have been experiencing the need to update the statistics, and the fact that nearly all of the statistics are showing a need to be updated across all of our tables related to order processing. Due to the frequency, we have setup a job to run prior to business hours at 7:30 AM. This seemed to have calmed the issue while we continued to investigate until it occurred again today. Not 10 minutes after business opening, they were receiving timeouts. I immediately ran and the timeouts disappeared and application function returned to normal. The order volume since business opening was low (less than 20 rows added to the primary order table), yet the statistics became so bad between 7:30 AM and 8:10 AM that we began experiencing time outs. A couple of notes: 

The execution plan (PasteThePlan) is vastly different with this small change. It is tossing a warning that there is within the predicate. It was my assumption that would implicitly satisfy this argument as it does with . However, given that the operator returns each row from the initial input when there is a matching row in the second input, and because the warning exists, query optimizer assumes that each row is a matching row. Therefore all subsequent batch operations are ran against all 2048 rows in the table. What can I look into to better interpret what the execution plan is describing so that I can understand how to properly resolve the issue? Or, alternatively, am I simply missing the purpose of the operator when filtering result sets based on a static input? 

SQL Server 2014 SP2. As the titles says, we have converted one of our database tables to be in-memory. After we did this, the corresponding memory-optimized filegroup takes up 1GB on the disk, but on a larger server it's up to 4GB). I suspect it has to do with the number of CPUs. The table is EMPTY! The structure of the table is nothing special, something like this: 

Just adding this comment, so it shows up as "answered", but all the credit goes to Martin Smith: Thank you, Martin! Installing SQL2012 SP1 CU8 update did the trick for me! Note that hacking around with files as suggested here did not help 

I am trying to use sp_publication_validation stored procedure to validate that my replication setup is working correctly. This procedure is pretty straightforward and is described here. I am running this command: 

For my next test, I now would like to change the location of the log files, so I modify the value of the [User::Log_Path] variable in the parent package to "C:\temp\Log_Run_Time". If I run now, I will have four log files! Two for the parent package in the "Log_Run_Time" directory and two for the child package - one in "Log_Design_Time" and one in "Log_Run_Time" directory: 

Once again, thank you to Vladimir for providing the formula! The correct way to accomplish what I need is this: 

I just wanted to share what I did to work-around this problem in case someone else runs into it. Once again, the fundamental issue is caused by simultaneous changes being made to multiple publications, which share the same distribution database. After analyzing the code of the Microsoft queries, which are reported by the deadlock trace (see my original post above), I found out that MSsubscriptions table is missing an index, so I went ahead an added it: 

I have a question about whether or not it is necessary to back the tail log up when performing a restore of a database in Full Recovery mode. I've sprawled for a good while now, and have gotten conflicting reports. Consider this scenario in SQL Server 2012: 

I have an issue where within my SQL Server 2016 Standard instance, is set to but within the file, it is capturing . Once I observed the behavior, I thought that the reason for this was that I had an trigger enabled. This trigger was created to prevent usage of a SQL Server credential when connecting with SQL Server Management Studio that originates from a that has not been delegated access. This was added to allow access from specific terminals outside of the domain (but within the same network) that the SQL Server is hosted on. This is the trigger: 

This returns: My clause clearly brings back only the rows that are explicitly , so it is not an issue of being an empty string and evaluating to . 

We have the following (unfortunate) scenario: we have a database where on-the-fly changes are being made by developers as quick remedies to dictionary tables (order types, document types, fee distributions, and so forth). This wouldn't be an issue if these particular developers didn't forget to check their changes into TFS, but alas, we face it. This causes an issue where the database project(s) in TFS do not properly reflect what is in the database. Essentially, I want receive an e-mail or other type of notification (table insert with necessary information) when a change is made to 15-20 tables, what the change was, and who committed the transaction. What is the best method to monitor this type of behavior? We are willing to invest in tools that allow for this ability. I've checked out RedGate SQL Monitor, but it doesn't seem to have this feature set (or I am missing it). I've also seen usage of triggers, but we have a large amount of tables that would require monitoring and I don't know the performance degradation that might incur and if we can collect the appropriate data. I do like the trigger approach where it would insert the information into a table ([dbo].[TABLE_UPDATES]) because it would allow us to build an SSRS report to run against and pull data as needed, but I'm not sure I 100% trust this as a solution. We currently use RedGate DLM Dashboard for monitoring schema changes, and it has proven quite invaluable to receive those updates. We just want to expand it to the data itself. Disclaimer: I know - terrible practice by developers to apply things directly to a database without checking in their changes, but we are currently having to deal with this hurdle and reign these individuals from wildly shooting at the hip. We are wanting a long-term solution for accountability purposes as our database and company grows. Thanks in advance.