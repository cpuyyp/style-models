Which of these measures is most useful is not clear, and I've seen all three used at different times and in different places. This speech by Janet Yellen has some discussion of the different measures, as does this paper by Carola Binder. 

To the extent that there is an economic explanation for their findings, it's something along the lines of costs of changing prices and employment are large enough relative to the increase in the minimum wage observed that producers choose instead to take a large amount of the cost of minimum wage increases on themselves. The alternatives would be 1) they decrease employment 2) they increase prices 3) they close branches or reduce entry. None of that seems to have happened. I think Pburg is right that one possible bone to pick with this result is that the short run response is not representative of the possible long run reaction. Another possibility is that the increase in the minimum wage was too small. \$4.25 to \$5.05 (the increase they observe) is very different from \$7.25 to \$10.10. This is my where my concerns would lie. It's possible there were also confounding legal changes, which would violate the parallel trends assumption, but I find this less plausible. 

More new hires when there are more unemployed (people without work actively looking for some) given the same number of vacancies? More new hires when there are more vacancies given the same number of unemployed? 

Example: Let $f(t, x) = 3t + x^2$. The (total derivative) is $\frac{d}{dt}f(t, x) = 3 + 2x\frac{dx}{dt}$ Whereas the partial derivative is $\frac{\partial}{\partial t}f(t, x) = 3 $ since we are forcing $\frac{dx}{dt}$ to be zero. So why are there 

The Jegadeesh and Titman (1993) paper is usually considered Original Source, though I'm sure you could find something earlier that looks similar if you looked hard enough. I don't think there is a satisfactory explanation. Momentum is not correlated with macroeconomic variables, it does not seem to reflect persistent exposure to other (known) sources of risk, and is driven almost entirely by the 7 to 12 months before stocks are chosen as part of a momentum portfolio. This all makes it extremely difficult to put a solid theoretical model to it that doesn't have a behavioral aspect. Recently (mid 2009), there was a momentum crash in the market. Daniel and Moskowitz argue that momentum has rare "panics" periods like this, where crashes occur for momentum. So during normal periods investors may be being compensated for this risk. The question becomes "what are these crashes caused by?" The authors make the case that following a period of declining stock values, momentum portfolios will be long a lot of low market beta stocks, and short a lot of high beta stocks. If there is a sudden rebound in the market, momentum strategies will experience a crash. So this is at least suggestive of a direction for more formal modeling, but they ultimately suggest a behavioral interpretation may be necessary as well. If you have not seen it, Jegadeesh and Titman (2011) review the evidence for momentum in more detail, and explore behavioral explanations for momentum, of which I have little knowledge. 

According to this theory an increase in money in circulation increases price levels. So an "extremely simple mathematical model" (and extremely simplistic) would be something like: $$ \pi_t = \dfrac{M_t-M_{t-1}}{M_t} $$ where $M$ stands for money supply and $\pi$ stands for inflation. 

The unemployed want to find the vacancies and the companies want to fill them. So the more potential vacancies there are, the easier it is to find one (maybe there is one just down the street) and vice versa. Imagine walking through the streets of a city taking random turns at each intersection, the more jobs ads there are the faster you will come across one. Unless I misremember, the matching functions are parts of models where unemployment is caused by some kind of frictions so maybe reading more on that will help build intuition why the matching function makes sense (or doesn't). 

Michael Woodford's book Interest and Prices, while it may not be explicitly New Keynesian, may have some of the rigor you're looking for applied to this class of models. A more direct alternative would be New Keynesian Economics edited by Mankiw and Romer. While it's a collection of papers not a textbook, if you're looking for underpinnings of New Keynesian models this'd be a good place to start. Also "The Science of Monetary Policy: A New Keynesian Perspective" by Clarida, Gali and Gertler. 

Given a balanced two-period panel data, with lets say 1000 observations on 500 individuals. When you estimate a pooled OLS regression and first-differences regression is there a standard in the economics literature on how you should report number of observations. It seems most natural for me to report 500 observations in the case of first-differences. This might seem trivial. But better follow any standard if it exists. 

According to my personal observation the majority of (prominent) economists prefer use Stata for their statistical analysis and Matlab for other mathematical work. SAS and Excel are also used (especially in finance). In my opinion R is a much better software for data cleaning, manipulation and analysis than Stata (not to mention that Stata costs) it is also seems on par with Matlab in what it does best. But I guess (as a graduate student) it will not make cooperation very smooth using a different statistical program than the rest. Use Stata like everyone else or suffer? Thus a student who is an 'R expert' should, if having to choose between two equal departments, choose the one that uses R. But does such a department exist? A department where at least few researchers use R? 

(Here $\sim$ is set notation instead of preference ordering.) The sense that Karni involves two acts that agree with each other outside of $B$. However Savage's definition of conditional preference is exactly $f\succcurlyeq_E f'$ iff $f_Eh\succcurlyeq f'_Eh$. If there really is no loss of generality in the rest of the proofs it doesn't matter which definition you pick. However Savage's definition is in line with both yours and mine, and the definition of conditional expectation. I'll look to see if that has any real consequences for Karni. ADDENDUM Using equality instead of $\sim$ is plenty general, simply because if the two acts only differ in consequences that do not have an effect on the preference ordering, you could just redefine the consequence space such that the equality was still maintained. Which is a cop out, but also true. I believe I've figure it out. The only place Karni seems to use his conditional preference definition is in P7. Savage uses an analogous P7 but with his own definition of conditional preference. P7 says (Savage's wording): 

No problem. $$Q =\left(\min\{K, L\} \right)^b$$ It just means that first you compare $K$ and $L$ and your quantity $Q$ will be equal the lower one to the power $b$. Example: $b=2$, $K=3$ and $L=7 \implies Q = 3^2 = 9$. 

You often hear economists (proudly) say that they don't take stated preferences seriously, but instead rely on observed preferences. Correct? To me both are problematic if you want to understand preferences, since people often do things they don't want to do. Example: I prefer to go to bed at 22:00 but I observe that I very often get caught up and don't do this. Decision making of individuals is complicated and I think everyone should be careful interpreting observerd preferences? My question is: Can anyone provide further points thinking about this issue and/or articles about the interpretability of observed preferences? I am aware there exists micro-models with "Hyperbolic discounting" making preferences time-inconsistent. I am looking for some more general discussion and frameworks. 

I don't feel these two interpretations are mutually exclusive, they belong to different sides of the same problem - one is empirical the other is theoretical. The conflict you seem to see between these two methods is that the "population" interpretation is individual specific. But the population component is a statistical convenience. It is "everything else" that the researcher has no time to model, and it is variation on the level of the population of observations not people - so it is not an individual specific error. In fact, one acceptable interpretation of this convenience is that conditional on non-random components individuals are the same, and the errors are such that if presented with the same decision again the individual would make a different choice. This is precisely equivalent to your neurological interpretation. Extra structure on the random component may change the degree to which this justification applies - if errors for an individual are assumed to be correlated over time it decreases the relative impact of a neuroeconomic explanation and starts to be about unobservable heterogeneity on the level of the individual. But random utility can still easily encompass both explanations as answers to different sets of questions. The residual error will include both neuroeconomic reasoning and any variables that have been omitted. To the extent that the randomness is specifically designed to capture "everything else" talking about what causes it is something most empiricists don't want to focus on. This being said, Michael Woodford has written something recently about neuroeconomics and random choice. And the "stocastic neural functions imply stochastic choice" is apparently a fairly common assertion in neuroeconomics: (see here, here, and here though some of these references point out that neuroeconomics places constraints on the form of random choice). But given the variables omitted in an average econometric study, I would hazard a guess that the random term captures relatively more of this omission and less of neurological processes. 

The simplest thing is to calculate the expected cost in the two scenarios: Scenario 1: Develop both at the same time $$E[cost]= X + Y$$ Scenario 2: Wait and see if task two is needed $$E[cost]= X + 0.7Z$$ However I would not say expected value calculation represents the "economics view". The general view rather depends on your preferences, often expressed as utility functions. It could be your utility is a function of expected value, but it could also not be the case. What is your risk tolerance? Some people seem risk-seeking. Are you ok with a a small possibility of ruin (bankruptcy)? Preferences can also depend on reference points and more... For other simplistic approaches you could start with value at risk.