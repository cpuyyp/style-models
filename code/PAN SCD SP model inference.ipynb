{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eaef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from itertools import cycle\n",
    "from ast import literal_eval\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, ModelOutput\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1064fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = os.environ[\"scratch_result_folder\"] if \"scratch_result_folder\" in os.environ else '../result'\n",
    "scratch_data_folder = os.environ[\"scratch_data_folder\"] if \"scratch_data_folder\" in os.environ else None\n",
    "repo_folder = os.environ[\"style_models_repo_folder\"] if \"style_models_repo_folder\" in os.environ else None\n",
    "data_folder = f\"{repo_folder}/data\" if repo_folder else '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyTrainingArgs:\n",
    "    # dataset args # it's not very appropriate to put them here, especially the split\n",
    "    dataset_idx: int = 0\n",
    "#     split: str\n",
    "        \n",
    "    # model args\n",
    "    base_model_name: str ='bert-base-uncased'\n",
    "    freeze_bert: bool = 0\n",
    "    use_pooler: bool = False\n",
    "        \n",
    "    # training args    \n",
    "    num_epoch: int = 5\n",
    "    lr: float = 5e-5\n",
    "    num_warmup_steps = 500\n",
    "    warmup_ratio = 0.1\n",
    "    model_folder: str = None # if None, this will be inferred based on tasks\n",
    "    model_name: str = None # if provide, use to name model_folder, otherwise use style to name model_folder\n",
    "    loss_fn: str = None\n",
    "    do_mlm: bool = False\n",
    "    method: str = 'successive paragraph'\n",
    "    # tempurature for cosine similarity. \n",
    "    # simcse uses 0.05. However, that's for crossentropy on single label classification. \n",
    "    # For multi-label where BCEloss(withlogits) is used, due to sigmoid, temp < 1 tends to make it learn slower\n",
    "    cos_temp: float = 1. # not used\n",
    "        \n",
    "    # data loader args\n",
    "    batch_size: int = 32\n",
    "    max_length: int = 64\n",
    "    shuffle: bool = False\n",
    "    num_workers: int = 4\n",
    "    data_limit: int = None # if not None, truncate dataset to keep only top {data_limit} rows\n",
    "    \n",
    "    # post training args\n",
    "    save_best_only: bool = True\n",
    "    load_best_at_end: bool = True\n",
    "    early_stop_patience: int = 1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        excute_time = datetime.now() \n",
    "        model_name = self.model_name if self.model_name else f\"pan22-dataset{self.dataset_idx}\"\n",
    "        model_folder = f\"{result_folder}/{model_name}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "        self.model_folder = model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PastelDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # this works for standard class indices and also class probilities\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, split):\n",
    "#         self.task = task\n",
    "        self.split = split\n",
    "        self.df = pd.read_csv(f\"{data_folder}/pastel/processed/{self.split}/pastel.csv\")\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        item = {'text': dataslice['output.sentences']}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''\n",
    "    Create fake meaningless \"changes\" label. To match with model requirement only\n",
    "    '''\n",
    "    batch_out = {}\n",
    "    texts = []\n",
    "    for item in batch:\n",
    "        texts.append(item['text'])\n",
    "    \n",
    "    batch_out = {k:v for k,v in tokenizer(text = texts, return_tensors='pt', padding=True, truncation=True, max_length=my_training_args.max_length).to(device).items()}\n",
    "    batch_out['changes'] = [0] * (len(batch)-1)\n",
    "    batch_out['texts'] = texts\n",
    "    \n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SCDOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    sent_embs: List[torch.FloatTensor] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee94195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSCD(BertPreTrainedModel):\n",
    "    def __init__(self, config, training_args):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = training_args.use_pooler\n",
    "        self.basemodel = AutoModel.from_pretrained(training_args.base_model_name)\n",
    "        self.do_mlm = training_args.do_mlm\n",
    "#         self.cossim = Similarity(training_args.cos_temp)\n",
    "        \n",
    "        self.hidden1 = nn.Linear(2*768, 256)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.hidden2 = nn.Linear(256, 2)\n",
    "        \n",
    "        if training_args.loss_fn == 'BCEWithLogitsLoss':\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        elif training_args.loss_fn == 'MSELoss':\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # mlm is not finished yet\n",
    "#         if self.do_mlm: \n",
    "#             self.lm_head = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, output_sent_embs=False, output_hidden_states=False, output_attentions=False, **kwargs):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=output_hidden_states, output_attentions=output_attentions)\n",
    "        \n",
    "        if self.use_pooler and ('pooler_output' in output):\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        concat_embs = torch.cat([sent_emb[:-1], sent_emb[1:]], axis=-1)\n",
    "        \n",
    "        logits = self.hidden2(self.gelu(self.hidden1(concat_embs)))\n",
    "        \n",
    "        # get style change labels\n",
    "        labels = torch.LongTensor(kwargs['changes']).to(device)\n",
    "        \n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        \n",
    "        return SCDOutput(loss=loss, logits=logits, sent_embs=sent_emb.detach(), hidden_states=output.hidden_states, attentions=output.attentions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ecac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PastelDataset(split='train')\n",
    "val_set = PastelDataset(split='valid')\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee24f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_folder = f\"{result_folder}/PAN_SCD_CL/sp_run_7\"\n",
    "my_training_args = MyTrainingArgs()\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased') \n",
    "model = BertForSCD(config, my_training_args).to(device)\n",
    "model.load_state_dict(torch.load(f\"{model_folder}/pytorch_model.bin\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88278b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = collections.defaultdict(list)\n",
    "for i_iter,x in enumerate(train_loader):\n",
    "    texts = x['texts']\n",
    "    output = model(**x)\n",
    "    prediction = output.logits.argmax(-1).detach().cpu()\n",
    "    for i in torch.where(prediction==1)[0]:\n",
    "        examples['has change'].append([texts[i], texts[i+1]])\n",
    "    for i in torch.where(prediction==0)[0][::3]:\n",
    "        examples['no change'].append([texts[i], texts[i+1]])\n",
    "    if len(examples['has change']) > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**x)\n",
    "prediction = output.logits.argmax(-1).detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2a720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be96449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 22])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(prediction==1)[0][::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He was welcomed and got comfortable very soon',\n",
       " 'The person went into the really cute bakery.',\n",
       " 'The woodwinds were particularly great at this concert.',\n",
       " 'We saw a cute cat in the window.',\n",
       " 'We gathered in a crowded room to celebrate the life of our friend.',\n",
       " 'The folks in the water participated in the boat portion of the race.',\n",
       " 'Thanks for the beautiful birthday flowers.',\n",
       " 'we came up to a big wall',\n",
       " 'Her groom was nervous.',\n",
       " \"It was a wonderful celebration of Nick's achievements.\",\n",
       " 'The story had a great location with an inviting entrance.',\n",
       " \"The runners in the marathon run a very long way, but at least it's a scenic route that passes by the ocean.\",\n",
       " 'The visitors team was just as pumped as the home team!',\n",
       " 'we had a great time visiting the homes.',\n",
       " 'IT IS A NICE BULIDINGS',\n",
       " 'This multi legged creature is scary to the little girl',\n",
       " 'It took 5 hours.',\n",
       " 'At the beginning of the school year, university students always have some things they need to shop for.',\n",
       " 'The party was out of sight',\n",
       " 'They and their friends are enjoying wine there.',\n",
       " 'We took a group photo after everyone arrived but could not fit everyone in the frame.',\n",
       " 'Planes in the sky',\n",
       " 'After that you need to make sure your audience enjoys the food as well.',\n",
       " 'When I rode the ferris wheel I felt absolutely terrified, but once I pctured myself as one gumball amongst a sea of others, I felt a calm within me.',\n",
       " 'His mom was so proud of him.',\n",
       " 'THE GIRL WAS SOME DOING',\n",
       " 'The big race was that day',\n",
       " 'The swamp area is like no other scene.',\n",
       " 'It was a gorgeous day in Egypt.',\n",
       " 'His mom was so proud of him.',\n",
       " 'Yesterday we went to visit the aquarium.',\n",
       " 'They built a dinosaur out of the snow.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9100648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
