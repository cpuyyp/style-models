{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1c77e",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#functions\" data-toc-modified-id=\"functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#multi-task-dataloader\" data-toc-modified-id=\"multi-task-dataloader-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>multi-task dataloader</a></span></li><li><span><a href=\"#multi-task-model\" data-toc-modified-id=\"multi-task-model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>multi-task model</a></span></li><li><span><a href=\"#train-and-validate\" data-toc-modified-id=\"train-and-validate-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>train and validate</a></span></li><li><span><a href=\"#bertology\" data-toc-modified-id=\"bertology-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>bertology</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484ef75",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, ModelOutput\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5168db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "result_folder = os.environ[\"scratch_result_folder\"] if \"scratch_result_folder\" in os.environ else './result'\n",
    "scratch_data_folder = os.environ[\"scratch_data_folder\"] if \"scratch_data_folder\" in os.environ else None\n",
    "data_folder = '../data'\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8839076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary: task_name: number_of_labels\n",
    "with open(f'{data_folder}/pastel/pastel_tasks2labels.json', 'r') as f:\n",
    "    tasks2labels = json.load(f)\n",
    "# Dictionary: task_name: task index\n",
    "tasks2idx = {k:i for i,k in enumerate(tasks2labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fe632",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArgs:\n",
    "    # training args\n",
    "    selected_tasks: List\n",
    "    base_model_name: str \n",
    "    freeze_bert: bool\n",
    "    use_pooler: bool\n",
    "    num_epoch: int\n",
    "    lr: float = 5e-5\n",
    "    num_warmup_steps = 500\n",
    "    model_folder: str = None # this will be inferred based on tasks\n",
    "    model_name: str = None # if provide, use to name model_folder, otherwise use style to name model_folder\n",
    "        \n",
    "    # data loader args\n",
    "    batch_size: int = 32\n",
    "    max_length: int = 64\n",
    "    shuffle: bool = False\n",
    "    num_workers: int = 4\n",
    "    data_limit: int = None # if not None, truncate dataset to keep only top {data_limit} rows\n",
    "    \n",
    "    # post training args\n",
    "    save_best: bool = True\n",
    "    load_best_at_end: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        excute_time = datetime.now() \n",
    "        model_name = self.model_name if self.model_name else '+'.join(self.selected_tasks)\n",
    "        model_folder = f\"{result_folder}/{model_name}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "        self.model_folder = model_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "## multi-task dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8f997",
   "metadata": {},
   "source": [
    "Test/validation dataloader consume dataset one by one, where as the train dataloader do it randomly. So the train dataloader is more complicated than test/validation dataloader. It must be able to reset a dataset once it is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ce118",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyTrainingArgs:\n",
    "    # training args\n",
    "    selected_tasks: List\n",
    "    base_model_name: str \n",
    "    freeze_bert: bool\n",
    "    use_pooler: bool\n",
    "    num_epoch: int\n",
    "    lr: float = 5e-5\n",
    "    num_warmup_steps = 500\n",
    "    model_folder: str = None # this will be inferred based on tasks\n",
    "    model_name: str = None # if provide, use to name model_folder, otherwise use style to name model_folder\n",
    "        \n",
    "    # data loader args\n",
    "    batch_size: int = 32\n",
    "    max_length: int = 64\n",
    "    shuffle: bool = False\n",
    "    num_workers: int = 4\n",
    "    data_limit: int = None # if not None, truncate dataset to keep only top {data_limit} rows\n",
    "    \n",
    "    # post training args\n",
    "    save_best: bool = True\n",
    "    load_best_at_end: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        excute_time = datetime.now() \n",
    "        model_name = self.model_name if self.model_name else '+'.join(self.selected_tasks)\n",
    "        model_folder = f\"{result_folder}/{model_name}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "        self.model_folder = model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, training_args, split):\n",
    "        self.max_length = training_args.max_length\n",
    "        self.split = split\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        self.df = pd.read_csv(f'{data_folder}/pastel/processed/{self.split}/pastel.csv')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "        if training_args.data_limit:\n",
    "            self.df = self.df.iloc[:training_args.data_limit]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        item = {k: v for k, v in self.tokenizer(dataslice['output.sentences'], truncation=True, padding=True, max_length=self.max_length).items()}\n",
    "        item.update({k: dataslice[k] for k in tasks2labels}) \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTasksDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        return self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "## multi-task model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b24e7",
   "metadata": {},
   "source": [
    "Given selected tasks, the model will add corresponding classification heads on the top of pretrained bert/(other bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb)).squeeze(1)\n",
    "\n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.hidden = nn.Linear(embedding_dim, self.num_labels)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden(self.dropout(sent_emb))\n",
    "        \n",
    "        loss = self.loss_fn(output.view(-1, self.num_labels), label.view(-1))\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21303a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultiTaskOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    sent_emb: torch.FloatTensor = None\n",
    "    bert_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    bert_attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(PreTrainedModel):\n",
    "    def __init__(self, config, training_args):\n",
    "        super().__init__(config)\n",
    "#         self.training_args = training_args\n",
    "        self.use_pooler = training_args.use_pooler\n",
    "        self.basemodel = AutoModel.from_pretrained(training_args.base_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        \n",
    "        for task in training_args.selected_tasks:\n",
    "            if tasks2labels[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks2labels[task]))\n",
    "                \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, **kwargs):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler and ('pooler_output' in output):\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        total_loss = None\n",
    "        for task in kwargs:\n",
    "            i_task = tasks2idx[task]\n",
    "            logits, loss = self.style_heads[i_task](sent_emb, kwargs[task]) \n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "        return MultiTaskOutput(loss=total_loss, sent_emb=sent_emb, bert_hidden_states=output.hidden_states, bert_attentions=output.attentions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1774e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)    \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        return (outputs.loss, outputs) if return_outputs else outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067bbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(training_args):\n",
    "    config = AutoConfig.from_pretrained(training_args.base_model_name) \n",
    "    model = MultiTaskBert(config, training_args).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1302944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert == True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert==True:\n",
    "        for param in model.basemodel.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, int):\n",
    "        for layer in model.basemodel.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df1503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jz17d/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_training_args = MyTrainingArgs(selected_tasks=list(tasks2labels.keys()),\n",
    "                             base_model_name='bert-base-uncased',\n",
    "                             freeze_bert=False,\n",
    "                             use_pooler=False,\n",
    "                             num_epoch=5,\n",
    "                             data_limit=30000,\n",
    "                            )\n",
    "model = init_model(my_training_args)\n",
    "freeze_model(model, my_training_args.freeze_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf57de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "hg_training_args = TrainingArguments(\n",
    "    output_dir=my_training_args.model_folder,   # output directory\n",
    "    num_train_epochs=my_training_args.num_epoch,     # total number of training epochs\n",
    "    per_device_train_batch_size=my_training_args.batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=my_training_args.batch_size,   # batch size for evaluation\n",
    "    warmup_steps=my_training_args.num_warmup_steps,    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f\"{my_training_args.model_folder}/logs\",  # directory for storing logs\n",
    "    logging_first_step = True, \n",
    "    evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "    save_total_limit = 1,\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end=True, # decide on loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec015cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jz17d/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jz17d/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jz17d/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jz17d/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(my_training_args, 'train')\n",
    "# val_dataset = MyDataset(my_training_args.selected_tasks, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 30000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4690\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='4690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  91/4690 00:18 < 15:33, 4.92 it/s, Epoch 0.10/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=model,   # the instantiated Transformers model to be trained\n",
    "    args=hg_training_args,                  # training arguments, defined above\n",
    "    tokenizer=model.tokenizer, \n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "#     eval_dataset=val_dataset,          # evaluation dataset\n",
    "#     compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b900e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98838e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda2b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31dc4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa318c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f025b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b673e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7472f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78c28749",
   "metadata": {},
   "source": [
    "## train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model, model_folder):\n",
    "    model.load_state_dict(torch.load(f\"{model_folder}/pytorch_model.bin\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_args):\n",
    "    \n",
    "    # these two will be used frequently\n",
    "    model_folder = training_args.model_folder\n",
    "    Path(model_folder).mkdir(parents=True, exist_ok=True)\n",
    "    selected_tasks = training_args.selected_tasks\n",
    "    \n",
    "    train_dataloader = MultiTaskTrainDataLoader(training_args)\n",
    "    num_training_steps = training_args.num_epoch*len(train_dataloader)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                                optimizer=optimizer,\n",
    "                                num_warmup_steps=training_args.num_warmup_steps,\n",
    "                                num_training_steps=num_training_steps)\n",
    "\n",
    "    # create dataframes for logging\n",
    "    columns = ['i_epoch', 'train_loss'] + [f'train_loss_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    columns += ['train_acc'] + [f'train_acc_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    columns += ['val_loss'] + [f'val_loss_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    columns += ['val_acc'] + [f'val_acc_{selected_tasks[i]}' for i in range(len(selected_tasks))]\n",
    "    df_evaluation = pd.DataFrame(columns=columns)\n",
    "#     df_loss_per_step = pd.DataFrame(columns=['i_epoch', 'i_iter', 'i_task', 'task_name', 'train_loss'])\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for i_epoch in range(training_args.num_epoch):\n",
    "        for i_iter, data in enumerate(train_dataloader):  \n",
    "            optimizer.zero_grad()\n",
    "            labels = {k:data[k].to(device) for k in training_args.selected_tasks}\n",
    "            for k in training_args.selected_tasks:\n",
    "                del data[k]\n",
    "            tokens = model.tokenizer(**data, return_tensors='pt', padding=True, truncation=True, max_length=training_args.max_length).to(device)\n",
    "            output = model(**tokens)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # log per step\n",
    "#             step_result = {'i_epoch':i_epoch, 'i_iter':i_iter, 'i_task':i_task, 'task_name':selected_tasks[i_task], 'train_loss':loss.item(),}\n",
    "#             df_loss_per_step = df_loss_per_step.append(step_result , ignore_index=True)\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # run evaluation on train and validation set \n",
    "#         train_loss, train_overall_acc, train_task_accs = validate(model, training_args, split='train')\n",
    "#         val_loss, val_overall_acc, val_task_accs = validate(model, training_args, split='dev')\n",
    "        \n",
    "        # save best model and corresponding opt and scheduler states to disk\n",
    "        if training_args.save_best and val_overall_acc.item() > best_accuracy: \n",
    "            torch.save(model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "            torch.save(optimizer.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "            torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")\n",
    "            best_accuracy = val_overall_acc.item()\n",
    "            \n",
    "        # collect result\n",
    "        epoch_result = {'i_epoch':i_epoch, f'train_loss':sum(train_loss.values()), 'train_acc':train_overall_acc.item(), 'val_loss':sum(val_loss.values()), 'val_acc':val_overall_acc.item()}\n",
    "        epoch_result.update({f'train_loss_{selected_tasks[i]}':train_loss[i] for i in train_loss})\n",
    "        epoch_result.update({f'train_acc_{selected_tasks[i]}':train_task_accs[i].item() for i in range(len(train_task_accs))})   \n",
    "        epoch_result.update({f'val_loss_{selected_tasks[i]}':val_loss[i] for i in val_loss})\n",
    "        epoch_result.update({f'val_acc_{selected_tasks[i]}':val_task_accs[i].item() for i in range(len(val_task_accs))})   \n",
    "        df_evaluation = df_evaluation.append(epoch_result , ignore_index=True)\n",
    "#         print('\\n'.join([f\"{k}:{v:.4}\" if isinstance(v, float) else f\"{k}:{v}\" for k,v in result.items()]))\n",
    "    \n",
    "    display(df_evaluation) \n",
    "#     display(df_loss_per_step) # this is too long, not approporate to show directly\n",
    "\n",
    "    # save to disk\n",
    "    if training_args.save_best:\n",
    "        with open(f\"{model_folder}/training_args.json\", \"w\") as outfile:\n",
    "            json.dump(asdict(training_args), outfile)\n",
    "        df_evaluation.to_csv(f\"{model_folder}/evaluation.csv\", index=False)\n",
    "#         df_loss_per_step.to_csv(f\"{model_folder}/loss_per_step.csv\", index=False)\n",
    "    \n",
    "    if training_args.save_best and training_args.load_best_at_end:\n",
    "        model = load_best_model(model, model_folder)\n",
    "    return df_evaluation, df_loss_per_step, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_training_args = MyTrainingArgs(selected_tasks=['VUA'],\n",
    "#                              base_model_name='bert-base-uncased',\n",
    "#                              freeze_bert=True,\n",
    "#                              use_pooler=True,\n",
    "#                              num_epoch=5,\n",
    "#                              data_limit=30000,\n",
    "#                             )\n",
    "\n",
    "# model = init_model(my_training_args)\n",
    "# freeze_model(model, my_training_args.freeze_bert)\n",
    "# df_evaluation, df_loss_per_step, model = train_model(model, my_training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83aaf45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7439d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, training_args, split):\n",
    "    # TODO!\n",
    "    val_loss = collections.defaultdict(float)\n",
    "    val_size = collections.defaultdict(int)\n",
    "    overall_acc = torchmetrics.Accuracy() \n",
    "    task_accs = [torchmetrics.Accuracy() for i in range(len(training_args.selected_task))] \n",
    "    \n",
    "    mt_dataloader = MultiTaskTestDataLoader(training_args, split=split)\n",
    "    \n",
    "    model.eval()\n",
    "    for data in tqdm(mt_dataloader, leave=False):  \n",
    "        i_task, batch = data\n",
    "        label = batch['label'].to(device)\n",
    "        size = len(label)\n",
    "        del batch['label']\n",
    "        tokens = model.tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=training_args.max_length).to(device)\n",
    "        output = model(**tokens, i_task=i_task, label=label)\n",
    "        loss = output.loss\n",
    "#         logits = output.logits\n",
    "        overall_acc.update(logits.to('cpu').detach(), label.to('cpu').detach())\n",
    "        task_accs[i_task].update(logits.to('cpu').detach(), label.to('cpu').detach())\n",
    "        tokens = None\n",
    "        output = None\n",
    "        val_loss[i_task] += loss.detach().item()*size\n",
    "        val_size[i_task] += size\n",
    "    \n",
    "    accs = []\n",
    "    for i_task in val_loss:\n",
    "        val_loss[i_task] /= val_size[i_task]\n",
    "        accs.append(task_accs[i_task].compute())\n",
    "    model.train()\n",
    "    \n",
    "    return val_loss, overall_acc.compute(), accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cbd92",
   "metadata": {},
   "source": [
    "## bertology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa453ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\" Compute the entropy of a probability distribution \"\"\"\n",
    "    plogp = p * torch.log(p)\n",
    "    plogp[p == 0] = 0\n",
    "    return -plogp.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heads_importance(\n",
    "    model, eval_dataloader, training_args, diagnose_per_step=False, diagnose_normalize=True, compute_entropy=True, compute_importance=True, head_mask=None, \n",
    "    dont_normalize_importance_by_layer = True, dont_normalize_global_importance=True\n",
    "):\n",
    "    \"\"\" This method shows how to compute:\n",
    "        - head attention entropy\n",
    "        - head importance scores according to http://arxiv.org/abs/1905.10650\n",
    "    \"\"\"\n",
    "    model_folder = training_args.model_folder\n",
    "    \n",
    "    # Prepare our tensors\n",
    "    n_layers, n_heads = model.basemodel.config.num_hidden_layers, model.basemodel.config.num_attention_heads\n",
    "    head_importance = torch.zeros(n_layers, n_heads).to(device)\n",
    "    attn_entropy = torch.zeros(n_layers, n_heads).to(device)\n",
    "\n",
    "    if head_mask is None:\n",
    "        head_mask = torch.ones(n_layers, n_heads).to(device)\n",
    "    head_mask.requires_grad_(requires_grad=True)\n",
    "    preds = None\n",
    "    labels = None\n",
    "    tot_tokens = 0.0\n",
    "    if diagnose_per_step:\n",
    "        entropy_per_step = None\n",
    "        importance_per_step = None\n",
    "\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Iteration\")):\n",
    "        i_task, batch = batch\n",
    "        label_ids = batch['label'].to(device)\n",
    "        size = len(label_ids)\n",
    "        del batch['label']\n",
    "        batch = model.tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        input_ids, input_mask, segment_ids = batch['input_ids'], batch['attention_mask'], batch['token_type_ids']\n",
    "        \n",
    "        # Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below)\n",
    "        outputs = model(i_task=i_task,\n",
    "            input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, label=label_ids, head_mask=head_mask, \n",
    "            output_attentions = True, \n",
    "        )\n",
    "        loss, logits, all_attentions = (\n",
    "            outputs.loss,\n",
    "            outputs.logits,\n",
    "            outputs.attentions,\n",
    "        )  # Loss and logits are the first, attention the last\n",
    "        loss.backward()  # Backpropagate to populate the gradients in the head mask\n",
    "        \n",
    "        batch_entropy = torch.zeros(n_layers, n_heads).to(device) \n",
    "        if compute_entropy:\n",
    "            for layer, attn in enumerate(all_attentions):\n",
    "                masked_entropy = entropy(attn.detach()) * input_mask.float().unsqueeze(1)\n",
    "                batch_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n",
    "                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n",
    "\n",
    "        if compute_importance:\n",
    "            batch_importance = head_mask.grad.abs().detach()\n",
    "            head_importance += batch_importance\n",
    "\n",
    "        # Also store our logits/labels if we want to compute metrics afterwards\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            labels = label_ids.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            labels = np.append(labels, label_ids.detach().cpu().numpy(), axis=0)\n",
    "        \n",
    "        batch_num_tokens = input_mask.float().detach().sum().item()\n",
    "        tot_tokens += batch_num_tokens\n",
    "        \n",
    "        if diagnose_per_step:\n",
    "            if diagnose_normalize:\n",
    "                batch_entropy = batch_entropy.detach().cpu().unsqueeze(0).numpy()/batch_num_tokens\n",
    "                batch_importance = batch_importance.cpu().unsqueeze(0).numpy()/batch_num_tokens\n",
    "                \n",
    "            else:\n",
    "                batch_entropy = batch_entropy.detach().cpu().unsqueeze(0).numpy()\n",
    "                batch_importance = batch_importance.detach().cpu().unsqueeze(0).numpy()\n",
    "                \n",
    "            if entropy_per_step is None:\n",
    "                entropy_per_step = batch_entropy\n",
    "            else:\n",
    "                entropy_per_step = np.append(entropy_per_step, batch_entropy, axis=0)\n",
    "            if importance_per_step is None:\n",
    "                importance_per_step = batch_importance\n",
    "            else:\n",
    "                importance_per_step = np.append(importance_per_step, batch_importance, axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    attn_entropy /= tot_tokens\n",
    "    head_importance /= tot_tokens\n",
    "    # Layerwise importance normalization\n",
    "    if not dont_normalize_importance_by_layer:\n",
    "        exponent = 2\n",
    "        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n",
    "        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n",
    "\n",
    "    if not dont_normalize_global_importance:\n",
    "        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n",
    "\n",
    "    # save matrices\n",
    "    np.save(os.path.join(model_folder, \"attn_entropy.npy\"), attn_entropy.detach().cpu().numpy())\n",
    "    np.save(os.path.join(model_folder, \"head_importance.npy\"), head_importance.detach().cpu().numpy())\n",
    "\n",
    "    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=device)\n",
    "    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(\n",
    "        head_importance.numel(), device=device\n",
    "    )\n",
    "    head_ranks = head_ranks.view_as(head_importance)\n",
    "    \n",
    "    plt.figure(figsize = (9,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('attn_entropy')\n",
    "    plt.imshow(attn_entropy.detach().cpu().numpy())\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('head_importance')\n",
    "    plt.imshow(head_importance.detach().cpu().numpy())\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    if diagnose_per_step:\n",
    "        return attn_entropy, head_importance, preds, labels, entropy_per_step, importance_per_step\n",
    "    \n",
    "    return attn_entropy, head_importance, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(torch_mat):\n",
    "    plt.imshow(torch_mat.detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArgs(selected_tasks=['VUA'],\n",
    "#                              base_model_name='bert-base-uncased',\n",
    "#                              freeze_bert=True,\n",
    "#                              use_pooler=True,\n",
    "#                              num_epoch=5,\n",
    "#                              data_limit=30000,\n",
    "#                             )\n",
    "\n",
    "# model = init_model(training_args)\n",
    "# freeze_model(model, training_args.freeze_bert)\n",
    "# df_evaluation, df_loss_per_step, model = train_model(model, training_args)\n",
    "\n",
    "# eval_dataloader = MultiTaskTestDataLoader(training_args, split='dev')\n",
    "# attn_entropy, head_importance, preds, labels = compute_heads_importance(model, eval_dataloader, training_args)\n",
    "\n",
    "# imshow(attn_entropy)\n",
    "# imshow(head_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
