{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2938637/1640288973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoConfig, AutoTokenizer, BertModel, RobertaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d955a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(f'{os.getcwd()}/SentEval')\n",
    "PATH_TO_DATA = f'{os.getcwd()}/SentEval/data'\n",
    "\n",
    "# Import SentEval\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29182ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186facb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8839076",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce7ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d23f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {'EmoBank_Valence': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd186",
   "metadata": {},
   "source": [
    "# multi-task dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    def __init__(self, tsv_file):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        sample = {'text':dataslice['text'], 'label':dataslice['label']}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainDataLoader():\n",
    "    '''\n",
    "    Each time, a random integer selects a dataset and load a batch of data {text, label} from it. Return i_task and data\n",
    "    \n",
    "    a iterator\n",
    "    Known issue: large dataset may have not iterate once, small datasets may have been iterated many times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, batch_size, shuffle, num_workers):\n",
    "        self.tasks = tasks\n",
    "        self.split = 'train'\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset('./processed/'+self.split+'/'+task+'.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(self.dataloaders[-1]._get_iterator())\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        dataloaderiter = self.dataloaderiters[i_task]    \n",
    "        try: \n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "#             self.dataloaderiters[i_task]._reset(self.dataloaders[i_task])\n",
    "#             dataloaderiter = self.dataloaderiters[i_task]\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61019a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTestDataLoader():\n",
    "    '''\n",
    "    For dev and test\n",
    "    \n",
    "    a generator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, split, batch_size, shuffle, num_workers):\n",
    "        assert split in ['dev', 'test'], 'not implemented'\n",
    "        self.tasks = tasks\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset('./processed/'+self.split+'/'+task+'.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "        self.i_task = 0\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i_task in range(self.num_tasks):\n",
    "            dataloader = self.dataloaders[i_task]\n",
    "            for batch in dataloader:\n",
    "                yield i_task, batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07c0cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(mt_val_dataloader):\n",
    "    val_loss = collections.defaultdict(float)\n",
    "    val_size = collections.defaultdict(int)\n",
    "    acc = torchmetrics.Accuracy() # todo\n",
    "    mt_model.eval()\n",
    "    for data in tqdm(mt_val_dataloader):  \n",
    "        i_task, batch = data\n",
    "        label = batch['label'].to(device)\n",
    "        size = len(label)\n",
    "        del batch['label']\n",
    "        tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "        output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "        tokens = None\n",
    "        output = None\n",
    "        val_loss[i_task] += loss.detach().item()*size\n",
    "        val_size[i_task] += size\n",
    "    for i_task in val_loss:\n",
    "        val_loss[i_task] /= val_size[i_task]\n",
    "    mt_model.train()\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a28b2",
   "metadata": {},
   "source": [
    "# multi-task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9c05444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ed50b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "#         self.hidden1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.hidden2 = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden = nn.Linear(embedding_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "#         output = self.activation(self.hidden2(self.hidden1(sent_emb))).squeeze(1)\n",
    "        output = self.activation(self.hidden(sent_emb)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c097a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "#         self.hidden1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.hidden2 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.hidden = nn.Linear(embedding_dim, num_labels)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.activation(self.hidden2(self.hidden1(sent_emb)))\n",
    "        \n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(BertPreTrainedModel):\n",
    "    def __init__(self, config, tasks, use_pooler=True):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = use_pooler\n",
    "        self.basemodel = BertModel(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in tasks:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, i_task=None, label=None, return_sent_emb=False):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        if return_sent_emb:\n",
    "            return sent_emb  \n",
    "        output, loss = self.style_heads[i_task](sent_emb, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "605330aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRoberta(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, tasks, use_pooler=True):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = use_pooler\n",
    "        self.basemodel = RobertaModel(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in tasks:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, input_ids, attention_mask, i_task=None, label=None, return_sent_emb=False):\n",
    "        output = self.basemodel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        if return_sent_emb:\n",
    "            return sent_emb\n",
    "        \n",
    "        output, loss = self.style_heads[i_task](sent_emb, label)\n",
    "        \n",
    "        return output, loss, sent_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d73059",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6acf874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(losses):\n",
    "    for k in losses:\n",
    "        print(f'{losses[k]:4.2f}', end=' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e1fda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger batch_size will definitely lead to memory issue\n",
    "mt_dataloader = MultiTaskTrainDataLoader(tasks, batch_size = 16, shuffle = True, num_workers = 4)\n",
    "mt_dev_dataloader = MultiTaskTestDataLoader(tasks, split='dev', batch_size = 16, shuffle = True, num_workers = 4)\n",
    "mt_test_dataloader = MultiTaskTestDataLoader(tasks, split='test', batch_size = 16, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ec272dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 7.92 GiB total capacity; 6.41 GiB already allocated; 90.62 MiB free; 6.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2778475/1939104225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTaskBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pooler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# mt_model = MultiTaskRoberta(config, tasks).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 7.92 GiB total capacity; 6.41 GiB already allocated; 90.62 MiB free; 6.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "base_model = \"bert-base-uncased\"\n",
    "# base_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "mt_model = MultiTaskBert(config, tasks, use_pooler=False).to(device)\n",
    "# mt_model = MultiTaskRoberta(config, tasks).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "884c578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(mt_model.parameters(), lr=0.03)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.03, total_steps=len(mt_dataloader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a516c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in mt_model.basemodel.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "654eadc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e3b38ffa184350a010110a61fbfe23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6222dc36d76f4fd8b6fba1ec6f328ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training iter 300/551\n",
      "0.28 \n"
     ]
    }
   ],
   "source": [
    "losses = collections.defaultdict(list)\n",
    "df_dev = pd.DataFrame(columns=np.arange(0,len(tasks)))\n",
    "training_acc = torchmetrics.Accuracy() \n",
    "\n",
    "\n",
    "test_embs = []\n",
    "\n",
    "for i_iter, data in enumerate(tqdm(mt_dataloader)):  \n",
    "    if i_iter == 1000:\n",
    "        for param in mt_model.basemodel.parameters():\n",
    "            param.requires_grad = True\n",
    "    i_task, batch = data\n",
    "    optimizer.zero_grad()\n",
    "    label = batch['label'].to(device)\n",
    "    del batch['label']\n",
    "    tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=64).to(device)\n",
    "    output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    losses[i_task].append(loss.detach().item())\n",
    "    \n",
    "    if i_iter in [0,10,100,200,210,300,400,500]:\n",
    "        sent_emb = mt_model(**tokens, i_task=i_task,  label=label, return_sent_emb=True)\n",
    "        test_embs.append(sent_emb)\n",
    "#     if i_iter > 500:\n",
    "#         break\n",
    "    if i_iter%300 == 0 and i_iter != 0:\n",
    "        dev_loss = validate(mt_dev_dataloader)\n",
    "        df_dev = df_dev.append(dev_loss , ignore_index=True)\n",
    "        print(f'#####training iter {i_iter}/{len(mt_dataloader)}')\n",
    "        print_loss(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d422c204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskBert(\n",
       "  (basemodel): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (style_heads): ModuleList(\n",
       "    (0): RegressionHead(\n",
       "      (hidden1): Linear(in_features=768, out_features=128, bias=True)\n",
       "      (hidden2): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "      (loss_fn): MSELoss()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd2c16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mt_model.basemodel(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb812a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2252, -0.1909, -0.8188,  ..., -0.5485, -1.8978,  1.4473],\n",
       "        [ 0.4681, -0.0052, -0.6890,  ..., -0.4932, -2.0251,  1.4433],\n",
       "        [ 0.4378, -0.3210, -0.4945,  ..., -0.6542, -2.0153,  1.2084],\n",
       "        ...,\n",
       "        [ 0.6605, -0.3394, -0.4702,  ..., -0.1526, -1.6630,  1.5776],\n",
       "        [ 0.3609, -0.3827, -0.8274,  ..., -0.6290, -1.8401,  1.2784],\n",
       "        [ 0.2989, -0.2560, -0.4781,  ..., -0.4163, -1.9857,  1.4385]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['last_hidden_state'][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c08b2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2188, 0.3438, 0.5000, 0.3125, 0.6375, 0.5000, 0.4688, 0.5312, 0.5938,\n",
       "        0.5000, 0.5938, 0.4688, 0.5312, 0.5344, 0.5938, 0.3750],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a08d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3f9ef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor(0.2813, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_model.style_heads[0](output['last_hidden_state'][:,0,:], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "321621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = mt_model(**tokens, i_task=i_task,  label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57cbe6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor(0.2813, device='cuda:0', grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e63e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 12, 12, 12,  7, 10,  7, 10, 12, 10], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4adf29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0471e-01,  1.2167e+00, -4.7918e-01,  ..., -1.3093e-01,\n",
       "         -4.7092e-03,  1.0579e+00],\n",
       "        [ 4.8265e-01,  1.1955e+00, -4.8833e-01,  ..., -9.8464e-02,\n",
       "         -2.6491e-02,  1.0363e+00],\n",
       "        [ 4.9495e-01,  1.2084e+00, -4.8237e-01,  ..., -8.0255e-02,\n",
       "          1.3555e-02,  1.0508e+00],\n",
       "        ...,\n",
       "        [ 5.0307e-01,  3.3286e-01, -4.7845e-01,  ..., -1.3443e-01,\n",
       "          1.0501e-02,  1.0487e+00],\n",
       "        [ 5.2614e-01,  1.2204e+00, -4.6653e-01,  ..., -7.9352e-02,\n",
       "         -9.1928e-03,  1.0598e+00],\n",
       "        [ 5.0942e-01,  1.2224e+00, -4.7927e-01,  ..., -1.2814e-01,\n",
       "          1.0450e-03,  1.0637e+00]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emb = mt_model(**tokens, i_task=i_task,  label=label, return_sent_emb=True)\n",
    "sent_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a7e7f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5282, -0.6056,  0.0627,  ...,  1.1953, -0.4895, -1.3894],\n",
       "         [-1.1636, -0.2303, -0.3400,  ...,  1.1154, -0.4572, -0.1489],\n",
       "         [ 0.8764, -0.8679, -0.4821,  ...,  0.5399, -0.7865, -0.8485],\n",
       "         ...,\n",
       "         [-0.5016,  0.7449,  0.5383,  ...,  1.0368,  0.0035, -0.8615],\n",
       "         [ 0.3498, -0.2952, -0.1204,  ...,  1.7765, -0.3754, -0.4325],\n",
       "         [ 0.0862, -0.0140,  0.3767,  ...,  0.8000, -0.8127, -0.0237]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.2193, -0.6518,  0.9127,  ..., -0.8741, -0.2050, -1.2656],\n",
       "         [-0.3398, -0.1049,  0.5103,  ...,  1.6578, -0.6212, -0.9448],\n",
       "         [ 0.0572, -1.1511,  0.3264,  ...,  1.1537, -1.6816, -0.7146],\n",
       "         ...,\n",
       "         [ 0.8545, -0.3801,  0.1675,  ...,  1.0831, -0.4000, -0.0018],\n",
       "         [ 0.1369, -0.5419, -0.0932,  ...,  0.6743, -0.0577, -1.0107],\n",
       "         [ 1.2012, -0.2522,  0.3095,  ...,  1.0965, -1.0189, -0.6714]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.3787, -0.5289,  0.9931,  ...,  0.2567, -0.1725, -1.4500],\n",
       "         [ 0.5294,  0.4169,  0.1250,  ...,  0.6285,  0.0500, -0.5599],\n",
       "         [ 1.3748, -0.5715,  0.1246,  ..., -0.6123, -0.3363, -0.7592],\n",
       "         ...,\n",
       "         [-0.1711, -0.6148,  0.1474,  ...,  0.2141, -0.6026, -0.6002],\n",
       "         [-0.2893, -0.4886, -0.0840,  ..., -0.5964, -1.1328, -0.8490],\n",
       "         [ 0.0818,  0.2446, -0.0609,  ...,  0.6065, -1.0805, -0.7792]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-0.6046, -1.3268,  1.0581,  ...,  1.2816, -0.7171, -1.1015],\n",
       "         [-0.7306, -0.1772,  0.0125,  ..., -0.7393, -0.4459, -1.1842],\n",
       "         [ 0.1562, -0.1778,  0.3986,  ...,  1.1915, -1.5479, -0.5208],\n",
       "         ...,\n",
       "         [ 0.2007,  0.1518,  1.0436,  ..., -0.8258, -0.9092, -0.4125],\n",
       "         [ 0.7417, -1.1316,  0.7312,  ...,  0.1688, -0.4556, -0.5033],\n",
       "         [ 0.2867, -0.5362,  0.0728,  ...,  1.6177, -0.5360, -0.4399]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.1364, -0.7160,  1.5700,  ...,  1.6380, -0.1148, -0.4773],\n",
       "         [ 0.1575, -0.7039,  1.3865,  ...,  1.6775, -1.0709, -2.1251],\n",
       "         [ 0.1620, -0.7225,  0.2613,  ...,  1.7107, -1.0127, -2.0125],\n",
       "         ...,\n",
       "         [ 0.1352, -0.6724,  1.6961,  ...,  0.1479, -0.9392, -2.1738],\n",
       "         [ 0.1751, -0.6548,  1.6579,  ...,  1.6154, -1.0255, -2.0592],\n",
       "         [ 0.1495, -0.6364,  1.6225,  ...,  1.6114, -0.9666, -1.7745]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 4.2923e-03, -3.9978e-01,  9.9531e-02,  ...,  2.0483e+00,\n",
       "           2.0760e-02, -2.2946e+00],\n",
       "         [-1.6299e-03,  7.1356e-02,  1.7751e+00,  ...,  2.0819e+00,\n",
       "          -9.5720e-01, -2.4701e+00],\n",
       "         [ 9.0246e-01, -3.2524e-01,  7.7118e-02,  ...,  2.0445e+00,\n",
       "          -1.0140e+00, -2.5167e+00],\n",
       "         ...,\n",
       "         [ 8.8149e-01, -3.6742e-01,  1.7074e+00,  ...,  2.0066e+00,\n",
       "           4.0345e-02, -2.2974e+00],\n",
       "         [ 8.9579e-01, -2.8162e-01,  1.7975e+00,  ...,  2.0241e+00,\n",
       "          -1.0023e+00, -2.4430e+00],\n",
       "         [ 9.4433e-01, -2.2097e-01,  1.6473e-02,  ...,  2.0435e+00,\n",
       "          -9.5563e-01, -2.4858e+00]], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.9195, -0.3340,  1.7843,  ...,  2.0470, -1.0278, -2.4733],\n",
       "         [-0.0139, -0.2917,  1.7202,  ...,  2.0547, -0.9911, -2.4612],\n",
       "         [ 0.8930, -0.2702,  1.7786,  ...,  1.9503, -0.9677, -2.4176],\n",
       "         ...,\n",
       "         [-0.0129, -0.2667,  1.7479,  ...,  2.1046, -0.9842, -2.4939],\n",
       "         [ 0.8818, -0.2506,  1.7564,  ...,  2.0469, -0.9428, -2.4302],\n",
       "         [ 0.9055, -0.3584,  1.7193,  ...,  2.0490, -1.0213, -2.4572]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.0225, -0.5360,  1.8450,  ...,  1.8922, -0.0056, -2.2852],\n",
       "         [ 0.9841, -0.4679,  0.1255,  ...,  1.9438, -1.1060, -0.1063],\n",
       "         [ 0.8987, -0.0461,  1.7826,  ...,  1.8533, -1.1612, -2.2616],\n",
       "         ...,\n",
       "         [ 0.9611, -0.0619,  1.8706,  ...,  1.9383, -1.1585, -2.4115],\n",
       "         [ 0.9637, -0.4945,  1.7904,  ...,  1.9302, -1.1433, -2.4513],\n",
       "         [ 0.9177, -0.5657,  1.8288,  ...,  1.8604, -1.1685, -2.3117]],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77eb8f9",
   "metadata": {},
   "source": [
    "torch metrics\n",
    "https://torchmetrics.readthedocs.io/en/stable/pages/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b20b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = torchmetrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ef7de68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = metric(torch.softmax(output, dim=-1).cpu(), torch.LongTensor([6, 7,  7, 12,  5, 12, 10,  5,  8,  8, 8, 8,  9,  5,  7, 3]))\n",
    "acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "029949b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_accuracy = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45b4e2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0833)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc1c5a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>CrowdFlower</th>\n",
       "      <th>DailyDialog</th>\n",
       "      <th>EmoBank_Valence</th>\n",
       "      <th>EmoBank_Arousal</th>\n",
       "      <th>EmoBank_Dominance</th>\n",
       "      <th>HateOffensive</th>\n",
       "      <th>PASTEL_age</th>\n",
       "      <th>PASTEL_country</th>\n",
       "      <th>PASTEL_education</th>\n",
       "      <th>PASTEL_ethnic</th>\n",
       "      <th>...</th>\n",
       "      <th>PASTEL_tod</th>\n",
       "      <th>SARC</th>\n",
       "      <th>SarcasmGhosh</th>\n",
       "      <th>SentiTreeBank</th>\n",
       "      <th>ShortHumor</th>\n",
       "      <th>ShortJokeKaggle</th>\n",
       "      <th>ShortRomance</th>\n",
       "      <th>StanfordPoliteness</th>\n",
       "      <th>TroFi</th>\n",
       "      <th>VUA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>7</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>8</th>\n",
       "      <th>2</th>\n",
       "      <th>10</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.312776</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.064081</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.832837</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.570806</td>\n",
       "      <td>1.628639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.065751</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012265</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.312776</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.832837</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.570806</td>\n",
       "      <td>1.628639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>1.380987</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.023418</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.312776</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>0.056178</td>\n",
       "      <td>0.019477</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.832837</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.570802</td>\n",
       "      <td>1.628639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.281408</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>10356.683170</td>\n",
       "      <td>3.350560</td>\n",
       "      <td>202.949844</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.876076</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.906558</td>\n",
       "      <td>1.622863</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.243174</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>117.785290</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.586025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.013024</td>\n",
       "      <td>0.009631</td>\n",
       "      <td>0.009826</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.073756</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012265</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>0.007379</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.082852</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.021920</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>44.343632</td>\n",
       "      <td>39.389981</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.066447</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.040102</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.199740</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>2.296593</td>\n",
       "      <td>2.929388</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.869238</td>\n",
       "      <td>1.804569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>243.021282</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.014671</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.575869</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>0.007498</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.563523</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063148</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.563523</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063210</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.553995</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.014559</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.531922</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.291181</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063349</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.669394</td>\n",
       "      <td>3.056301</td>\n",
       "      <td>33.257728</td>\n",
       "      <td>0.413607</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.291181</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.282474</td>\n",
       "      <td>1.289514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063479</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.247142</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.669394</td>\n",
       "      <td>2.795953</td>\n",
       "      <td>0.013106</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.682214</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.779532</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.064578</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011947</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.593174</td>\n",
       "      <td>2.795953</td>\n",
       "      <td>0.013773</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.008857</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.682214</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.779532</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.166203</td>\n",
       "      <td>3.433892</td>\n",
       "      <td>0.013319</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.682214</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.718815</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063461</td>\n",
       "      <td>1.123398</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012515</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.166203</td>\n",
       "      <td>3.433892</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.463943</td>\n",
       "      <td>1.805775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.718815</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.069324</td>\n",
       "      <td>1.123398</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011902</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.741549</td>\n",
       "      <td>1.313582</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>0.009784</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.201590</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>2.618898</td>\n",
       "      <td>3.365728</td>\n",
       "      <td>...</td>\n",
       "      <td>2.133407</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063156</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.675492</td>\n",
       "      <td>1.053234</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.201590</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>2.618898</td>\n",
       "      <td>1.755472</td>\n",
       "      <td>...</td>\n",
       "      <td>2.283431</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.067555</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.014723</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.538020</td>\n",
       "      <td>1.053234</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>0.009674</td>\n",
       "      <td>0.025462</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.201590</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>2.618898</td>\n",
       "      <td>1.579542</td>\n",
       "      <td>...</td>\n",
       "      <td>2.169905</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.080893</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.515313</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>5.145812</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>1.902214</td>\n",
       "      <td>2.464783</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.112872</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156836</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.053090</td>\n",
       "      <td>477.005259</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.515313</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.014636</td>\n",
       "      <td>0.024433</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>2.464783</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.112872</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.970627</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.053090</td>\n",
       "      <td>0.071535</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.360841</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.040487</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.986006</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.918075</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.690410</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.125329</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.360841</td>\n",
       "      <td>1.851276</td>\n",
       "      <td>0.015409</td>\n",
       "      <td>0.028597</td>\n",
       "      <td>0.068424</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>2.038050</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.918075</td>\n",
       "      <td>2.084602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>167.689251</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012074</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.195473</td>\n",
       "      <td>1.983602</td>\n",
       "      <td>0.018592</td>\n",
       "      <td>0.059705</td>\n",
       "      <td>0.112257</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.894972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.093787</td>\n",
       "      <td>2.064906</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006813</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.066633</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.011939</td>\n",
       "      <td>1.052642</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.248536</td>\n",
       "      <td>1.983602</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.054340</td>\n",
       "      <td>0.093445</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.979928</td>\n",
       "      <td>2.064906</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006813</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.128055</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.236341</td>\n",
       "      <td>3.234514</td>\n",
       "      <td>0.016057</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>0.061102</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.979928</td>\n",
       "      <td>2.064906</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006813</td>\n",
       "      <td>1.125411</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.137261</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.893061</td>\n",
       "      <td>1.512960</td>\n",
       "      <td>0.013178</td>\n",
       "      <td>0.052041</td>\n",
       "      <td>0.063899</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>3.273791</td>\n",
       "      <td>3.384352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>8.340908</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>6066.250663</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.465798</td>\n",
       "      <td>1.512960</td>\n",
       "      <td>8964.222899</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.894972</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>3.273791</td>\n",
       "      <td>3.384352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>1.128445</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.063543</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.016995</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.465798</td>\n",
       "      <td>1.512960</td>\n",
       "      <td>0.016301</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.010876</td>\n",
       "      <td>0.701575</td>\n",
       "      <td>1.849361</td>\n",
       "      <td>0.174123</td>\n",
       "      <td>3.273791</td>\n",
       "      <td>1.886398</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576885</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.120865</td>\n",
       "      <td>1.130458</td>\n",
       "      <td>1.128527</td>\n",
       "      <td>1.126928</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrowdFlower DailyDialog EmoBank_Valence EmoBank_Arousal EmoBank_Dominance  \\\n",
       "            13          7               1               1                 1    \n",
       "0     2.312776    0.832633        0.017660        0.064081          0.007568   \n",
       "1     2.312776    0.832633        0.035017        0.012292          0.026210   \n",
       "2     2.312776    0.832633        0.016358        0.056178          0.019477   \n",
       "3     3.281408    3.575869    10356.683170        3.350560        202.949844   \n",
       "4     3.199740    3.575869        0.013024        0.009631          0.009826   \n",
       "5     3.199740    3.575869        0.017520        0.009645          0.007379   \n",
       "6     3.199740    3.575869       44.343632       39.389981          0.009247   \n",
       "7     3.199740    3.575869        0.013002        0.009748          0.008249   \n",
       "8     2.531922    3.575869        0.014539        0.009767          0.007498   \n",
       "9     2.531922    3.056301        0.013438        0.010019          0.008480   \n",
       "10    2.531922    3.056301        0.013254        0.009790          0.007389   \n",
       "11    2.531922    3.056301        0.031359        0.010579          0.012248   \n",
       "12    2.669394    3.056301       33.257728        0.413607          0.007814   \n",
       "13    2.669394    2.795953        0.013106        0.009670          0.007399   \n",
       "14    2.593174    2.795953        0.013773        0.009716          0.008857   \n",
       "15    3.166203    3.433892        0.013319        0.009545          0.008774   \n",
       "16    3.166203    3.433892        0.013053        0.009650          0.007941   \n",
       "17    2.741549    1.313582        0.020966        0.009784          0.009704   \n",
       "18    2.675492    1.053234        0.014367        0.010479          0.014078   \n",
       "19    2.538020    1.053234        0.013331        0.009674          0.025462   \n",
       "20    3.515313    1.851276        0.013314        5.145812          0.017289   \n",
       "21    3.515313    1.851276        0.013080        0.014636          0.024433   \n",
       "22    3.360841    1.851276        0.013085        0.019438          0.040487   \n",
       "23    3.360841    1.851276        0.015409        0.028597          0.068424   \n",
       "24    3.195473    1.983602        0.018592        0.059705          0.112257   \n",
       "25    3.248536    1.983602        0.017954        0.054340          0.093445   \n",
       "26    3.236341    3.234514        0.016057        0.041509          0.061102   \n",
       "27    2.893061    1.512960        0.013178        0.052041          0.063899   \n",
       "28    3.465798    1.512960     8964.222899        0.019575          0.017272   \n",
       "29    3.465798    1.512960        0.016301        0.018517          0.010876   \n",
       "\n",
       "   HateOffensive PASTEL_age PASTEL_country PASTEL_education PASTEL_ethnic  \\\n",
       "              3          8              2                10            10   \n",
       "0       0.701575   1.832837       0.693147         2.570806      1.628639   \n",
       "1       0.701575   1.832837       0.693147         2.570806      1.628639   \n",
       "2       0.701575   1.832837       0.693147         2.570802      1.628639   \n",
       "3       2.296593   2.876076       0.693147         2.906558      1.622863   \n",
       "4       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "5       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "6       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "7       2.296593   2.929388       0.693147         2.869238      1.804569   \n",
       "8       1.902214   2.563523       0.693147         2.282474      1.289514   \n",
       "9       1.902214   2.563523       0.693147         2.282474      1.289514   \n",
       "10      1.902214   2.553995       0.693147         2.282474      1.289514   \n",
       "11      1.902214   2.291181       0.693147         2.282474      1.289514   \n",
       "12      1.902214   2.291181       0.693147         2.282474      1.289514   \n",
       "13      1.098612   1.722609       0.693147         2.682214      1.805775   \n",
       "14      1.098612   1.722609       0.693147         2.682214      1.805775   \n",
       "15      1.098612   1.722609       0.693147         2.682214      1.805775   \n",
       "16      1.098612   1.722609       0.693147         2.463943      1.805775   \n",
       "17      1.902214   2.201590       0.174123         2.618898      3.365728   \n",
       "18      1.902214   2.201590       0.174123         2.618898      1.755472   \n",
       "19      1.902214   2.201590       0.174123         2.618898      1.579542   \n",
       "20      1.902214   2.464783       0.693147         3.112872      2.084602   \n",
       "21      1.098612   2.464783       0.693147         3.112872      2.084602   \n",
       "22      1.098612   1.986006       0.693147         2.918075      2.084602   \n",
       "23      1.098612   2.038050       0.693147         2.918075      2.084602   \n",
       "24      1.098612   1.894972       0.693147         2.093787      2.064906   \n",
       "25      0.701575   1.849361       0.693147         1.979928      2.064906   \n",
       "26      0.701575   1.849361       0.693147         1.979928      2.064906   \n",
       "27      0.701575   1.849361       0.174123         3.273791      3.384352   \n",
       "28      0.701575   1.894972       0.174123         3.273791      3.384352   \n",
       "29      0.701575   1.849361       0.174123         3.273791      1.886398   \n",
       "\n",
       "    ... PASTEL_tod      SARC SarcasmGhosh SentiTreeBank ShortHumor  \\\n",
       "    ...         5         2            2             1          2    \n",
       "0   ...   1.576885  1.125411     0.200766      0.065751   0.693147   \n",
       "1   ...   1.576885  1.125411     0.200766      1.380987   0.693147   \n",
       "2   ...   1.576885  1.125411     0.200766      0.075151   0.693147   \n",
       "3   ...   1.576885  0.693147     0.693147      0.243174   0.693147   \n",
       "4   ...   1.576885  0.693147     0.693147      0.073756   0.693147   \n",
       "5   ...   1.576885  0.693147     0.693147      0.082852   0.693147   \n",
       "6   ...   1.576885  0.693147     0.693147      0.066447   0.693147   \n",
       "7   ...   1.576885  0.693147     0.693147    243.021282   0.693147   \n",
       "8   ...   1.970627  0.693147     0.693147      0.063148   0.693147   \n",
       "9   ...   1.970627  0.693147     0.693147      0.063210   0.693147   \n",
       "10  ...   1.970627  0.693147     0.693147      0.063161   0.693147   \n",
       "11  ...   1.970627  0.693147     0.693147      0.063349   0.693147   \n",
       "12  ...   1.970627  0.693147     0.693147      0.063479   0.693147   \n",
       "13  ...   1.779532  1.128445     0.693147      0.064578   1.130458   \n",
       "14  ...   1.779532  1.128445     0.693147      0.064000   1.130458   \n",
       "15  ...   1.718815  1.128445     0.693147      0.063461   1.123398   \n",
       "16  ...   1.718815  1.128445     0.693147      0.069324   1.123398   \n",
       "17  ...   2.133407  1.125411     0.693147      0.063156   1.130458   \n",
       "18  ...   2.283431  1.125411     0.200766      0.067555   1.130458   \n",
       "19  ...   2.169905  1.125411     0.200766      0.080893   1.130458   \n",
       "20  ...   2.156836  0.693147     2.053090    477.005259   0.693147   \n",
       "21  ...   1.970627  0.693147     2.053090      0.071535   0.693147   \n",
       "22  ...   1.690410  0.693147     0.693147      0.063559   0.693147   \n",
       "23  ...   1.576885  0.693147     0.693147    167.689251   0.693147   \n",
       "24  ...   2.006813  1.125411     0.693147      0.066633   0.693147   \n",
       "25  ...   2.006813  1.125411     0.693147      0.128055   0.693147   \n",
       "26  ...   2.006813  1.125411     0.693147      0.137261   0.693147   \n",
       "27  ...   1.576885  1.128445     0.200766      8.340908   1.130458   \n",
       "28  ...   1.576885  1.128445     0.200766      0.063543   1.130458   \n",
       "29  ...   1.576885  0.693147     0.200766      0.120865   1.130458   \n",
       "\n",
       "   ShortJokeKaggle ShortRomance StanfordPoliteness     TroFi       VUA  \n",
       "                2            2                  1         2         2   \n",
       "0         0.693147     0.693147           0.012265  0.693147  0.693147  \n",
       "1         0.693147     0.693147           0.023418  0.693147  0.693147  \n",
       "2         0.693147     0.693147           0.012740  0.693147  0.693147  \n",
       "3         0.693147     0.693147         117.785290  0.693147  1.586025  \n",
       "4         0.693147     0.693147           0.012265  0.693147  0.693147  \n",
       "5         0.693147     0.693147           0.021920  0.693147  0.693147  \n",
       "6         0.693147     0.693147           0.040102  0.693147  0.693147  \n",
       "7         0.693147     0.693147           0.014671  0.693147  0.693147  \n",
       "8         1.125329     0.693147           0.011814  0.693147  0.693147  \n",
       "9         1.125329     0.693147           0.012274  0.693147  0.693147  \n",
       "10        1.125329     0.693147           0.014559  0.693147  0.693147  \n",
       "11        1.125329     0.693147           0.012060  0.693147  0.693147  \n",
       "12        1.125329     0.693147           0.247142  0.693147  0.693147  \n",
       "13        1.125329     0.693147           0.011947  1.201214  0.693147  \n",
       "14        1.125329     0.693147           0.012716  1.201214  0.693147  \n",
       "15        0.693147     0.693147           0.012515  1.201214  0.693147  \n",
       "16        0.693147     0.693147           0.011902  1.201214  0.693147  \n",
       "17        0.693147     1.126928           0.022923  1.201214  0.693147  \n",
       "18        0.693147     1.126928           0.014723  1.201214  0.693147  \n",
       "19        1.128527     1.126928           0.012073  1.201214  0.693147  \n",
       "20        1.125329     1.126928           0.014370  1.052642  0.693147  \n",
       "21        1.125329     1.126928           0.014299  1.052642  0.693147  \n",
       "22        1.125329     1.126928           0.012019  1.052642  0.693147  \n",
       "23        0.693147     1.126928           0.012074  1.052642  0.693147  \n",
       "24        1.128527     0.693147           0.011939  1.052642  0.693147  \n",
       "25        1.128527     0.693147           0.015505  0.693147  0.693147  \n",
       "26        1.128527     0.693147           0.012144  0.693147  0.693147  \n",
       "27        1.128527     1.126928        6066.250663  0.693147  0.693147  \n",
       "28        1.128527     1.126928           0.016995  0.693147  0.693147  \n",
       "29        1.128527     1.126928           0.012610  0.693147  0.693147  \n",
       "\n",
       "[30 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.columns = tasks.items()\n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f97e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './mt_model_runs/mt_2.bin'\n",
    "torch.save(mt_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6424252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('losses.json', 'w') as f:\n",
    "    json.dump(losses, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0376c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.to_csv('dev_losses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5087e",
   "metadata": {},
   "source": [
    "# senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faa83f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare(params, samples):\n",
    "    return\n",
    "def batcher(params, batch):\n",
    "    sentences = [' '.join(s) for s in batch]\n",
    "    batch = tokenizer(\n",
    "        sentences,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "    with torch.no_grad():\n",
    "        sent_emb = mt_model(**batch, return_sent_emb=True)\n",
    "    \n",
    "    return sent_emb.cpu()\n",
    "\n",
    "# Set params for SentEval (fastmode)\n",
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                    'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "task_set = 'sts'\n",
    "if task_set == 'sts':\n",
    "    senteval_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "elif task_set == 'transfer':\n",
    "    senteval_tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
    "elif task_set == 'full':\n",
    "    senteval_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    senteval_tasks += ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
    "\n",
    "se = senteval.engine.SE(params, batcher, prepare)\n",
    "mt_model.eval()\n",
    "results = se.eval(senteval_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0585c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STS12': {'MSRpar': {'pearson': (0.04511842111176674, 0.21713343634765625),\n",
       "   'spearman': SpearmanrResult(correlation=0.05829879048588996, pvalue=0.11065140077710642),\n",
       "   'nsamples': 750},\n",
       "  'MSRvid': {'pearson': (-0.04719690809332701, 0.19666754119304142),\n",
       "   'spearman': SpearmanrResult(correlation=-0.05791055536831769, pvalue=0.11305028382924873),\n",
       "   'nsamples': 750},\n",
       "  'SMTeuroparl': {'pearson': (0.05465063166493182, 0.24259241634429948),\n",
       "   'spearman': SpearmanrResult(correlation=0.04601455241561682, pvalue=0.3252817731161748),\n",
       "   'nsamples': 459},\n",
       "  'surprise.OnWN': {'pearson': (0.028684333440108387, 0.43280310929667104),\n",
       "   'spearman': SpearmanrResult(correlation=0.0486239359795535, pvalue=0.183456625977699),\n",
       "   'nsamples': 750},\n",
       "  'surprise.SMTnews': {'pearson': (0.004681029491783591, 0.9257362156479326),\n",
       "   'spearman': SpearmanrResult(correlation=-0.013892916608515183, pvalue=0.7820461848374392),\n",
       "   'nsamples': 399},\n",
       "  'all': {'pearson': {'all': 0.014868264700974198,\n",
       "    'mean': 0.017187501523052706,\n",
       "    'wmean': 0.01509226368897569},\n",
       "   'spearman': {'all': 0.015054354649698811,\n",
       "    'mean': 0.01622676138084548,\n",
       "    'wmean': 0.016839296703608394}}},\n",
       " 'STS13': {'FNWN': {'pearson': (0.06002909117909498, 0.4119145612631916),\n",
       "   'spearman': SpearmanrResult(correlation=0.10666278073835941, pvalue=0.14406661717560476),\n",
       "   'nsamples': 189},\n",
       "  'headlines': {'pearson': (-0.016041462614685892, 0.6609448622666173),\n",
       "   'spearman': SpearmanrResult(correlation=-0.013468861132827664, pvalue=0.7126783172726778),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (-0.07275485896320083, 0.08512943410489214),\n",
       "   'spearman': SpearmanrResult(correlation=-0.10822762814993239, pvalue=0.010310445835506298),\n",
       "   'nsamples': 561},\n",
       "  'all': {'pearson': {'all': -0.033342285143532845,\n",
       "    'mean': -0.009589076799597246,\n",
       "    'wmean': -0.027667383071014087},\n",
       "   'spearman': {'all': -0.04431797498580879,\n",
       "    'mean': -0.005011236181466881,\n",
       "    'wmean': -0.03377205312145526}}},\n",
       " 'STS14': {'deft-forum': {'pearson': (-0.021190909391096264,\n",
       "    0.6539171213268393),\n",
       "   'spearman': SpearmanrResult(correlation=0.0029060908432702397, pvalue=0.9509799970865339),\n",
       "   'nsamples': 450},\n",
       "  'deft-news': {'pearson': (-0.04926721632365719, 0.3951653745618607),\n",
       "   'spearman': SpearmanrResult(correlation=-0.015566161047035513, pvalue=0.7883109827723784),\n",
       "   'nsamples': 300},\n",
       "  'headlines': {'pearson': (-0.036271895856946464, 0.3211895362760735),\n",
       "   'spearman': SpearmanrResult(correlation=-0.01334434447258084, pvalue=0.7152184385155735),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (0.04866819404814243, 0.18305765622334177),\n",
       "   'spearman': SpearmanrResult(correlation=0.05836358393088991, pvalue=0.11025498096668766),\n",
       "   'nsamples': 750},\n",
       "  'OnWN': {'pearson': (0.012467595309804187, 0.7331917338366095),\n",
       "   'spearman': SpearmanrResult(correlation=0.0052956944913676056, pvalue=0.8848785081393861),\n",
       "   'nsamples': 750},\n",
       "  'tweet-news': {'pearson': (-0.0484903037253082, 0.18466517099205706),\n",
       "   'spearman': SpearmanrResult(correlation=-0.060365555567131204, pvalue=0.09854921142261619),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': -0.00430607701796145,\n",
       "    'mean': -0.015680755989843583,\n",
       "    'wmean': -0.011209568477685734},\n",
       "   'spearman': {'all': 0.0022572859515260823,\n",
       "    'mean': -0.003785115303536634,\n",
       "    'wmean': -0.002906686306061319}}},\n",
       " 'STS15': {'answers-forums': {'pearson': (0.004816059805726414,\n",
       "    0.9259418669703817),\n",
       "   'spearman': SpearmanrResult(correlation=0.026151520189151158, pvalue=0.6136868898421568),\n",
       "   'nsamples': 375},\n",
       "  'answers-students': {'pearson': (0.03918296576869461, 0.28385863697876323),\n",
       "   'spearman': SpearmanrResult(correlation=0.03984013410841718, pvalue=0.27585421304522323),\n",
       "   'nsamples': 750},\n",
       "  'belief': {'pearson': (0.015021320699688725, 0.7718683584847408),\n",
       "   'spearman': SpearmanrResult(correlation=0.010098170341856312, pvalue=0.8454693854921901),\n",
       "   'nsamples': 375},\n",
       "  'headlines': {'pearson': (0.017554243305938865, 0.6312431198857941),\n",
       "   'spearman': SpearmanrResult(correlation=0.03219384489201663, pvalue=0.3786305234273506),\n",
       "   'nsamples': 750},\n",
       "  'images': {'pearson': (-0.06171300104517855, 0.09124412338374546),\n",
       "   'spearman': SpearmanrResult(correlation=-0.04114135190420947, pvalue=0.26046213846165633),\n",
       "   'nsamples': 750},\n",
       "  'all': {'pearson': {'all': 0.004980804085493675,\n",
       "    'mean': 0.0029723177069740135,\n",
       "    'wmean': 0.0012357245705406233},\n",
       "   'spearman': {'all': 0.016412582538039754,\n",
       "    'mean': 0.01342846352544636,\n",
       "    'wmean': 0.012254368090432018}}},\n",
       " 'STS16': {'answer-answer': {'pearson': (0.09638987265977564,\n",
       "    0.12547448671360714),\n",
       "   'spearman': SpearmanrResult(correlation=0.0704678208720361, pvalue=0.26317243154014536),\n",
       "   'nsamples': 254},\n",
       "  'headlines': {'pearson': (0.09546151397635683, 0.13304278320286977),\n",
       "   'spearman': SpearmanrResult(correlation=0.08769641340272052, pvalue=0.16773378560824984),\n",
       "   'nsamples': 249},\n",
       "  'plagiarism': {'pearson': (-0.023254569313365135, 0.7257391914319138),\n",
       "   'spearman': SpearmanrResult(correlation=-0.036292456678818995, pvalue=0.5839775954283772),\n",
       "   'nsamples': 230},\n",
       "  'postediting': {'pearson': (0.08533342407802708, 0.18400294501056963),\n",
       "   'spearman': SpearmanrResult(correlation=0.10513255618290616, pvalue=0.10135216378278733),\n",
       "   'nsamples': 244},\n",
       "  'question-question': {'pearson': (-0.044619727218692305, 0.5211908361500954),\n",
       "   'spearman': SpearmanrResult(correlation=-0.020283823663301125, pvalue=0.7706604089711906),\n",
       "   'nsamples': 209},\n",
       "  'all': {'pearson': {'all': 0.0474392426460971,\n",
       "    'mean': 0.04186210283642042,\n",
       "    'wmean': 0.04586865613824098},\n",
       "   'spearman': {'all': 0.04372775283752752,\n",
       "    'mean': 0.04134410202310853,\n",
       "    'wmean': 0.04452023015653067}}},\n",
       " 'STSBenchmark': {'train': {'pearson': (0.0029242067551564586,\n",
       "    0.8245692798092561),\n",
       "   'spearman': SpearmanrResult(correlation=0.0036638990419097382, pvalue=0.7812079114872583),\n",
       "   'nsamples': 5749},\n",
       "  'dev': {'pearson': (-0.01772728686373328, 0.49267753704857437),\n",
       "   'spearman': SpearmanrResult(correlation=0.0018255850636838076, pvalue=0.9436797209993755),\n",
       "   'nsamples': 1500},\n",
       "  'test': {'pearson': (0.013181439308982199, 0.6247930962772686),\n",
       "   'spearman': SpearmanrResult(correlation=0.0037039774949266606, pvalue=0.8906968207880507),\n",
       "   'nsamples': 1379},\n",
       "  'all': {'pearson': {'all': 0.0012741367819083397,\n",
       "    'mean': -0.0005405469331982079,\n",
       "    'wmean': 0.0009732891918035476},\n",
       "   'spearman': {'all': 0.0040670140871223335,\n",
       "    'mean': 0.003064487200173402,\n",
       "    'wmean': 0.0033507091044238132}}},\n",
       " 'SICKRelatedness': {'train': {'pearson': (-0.04701125618938638,\n",
       "    0.0016078501873482392),\n",
       "   'spearman': SpearmanrResult(correlation=-0.04303466092832693, pvalue=0.003884543748536068),\n",
       "   'nsamples': 4500},\n",
       "  'dev': {'pearson': (0.03231563834984917, 0.47092220229455317),\n",
       "   'spearman': SpearmanrResult(correlation=-0.006204259523049101, pvalue=0.8899357054569257),\n",
       "   'nsamples': 500},\n",
       "  'test': {'pearson': (-0.013428132806185007, 0.34600898585681605),\n",
       "   'spearman': SpearmanrResult(correlation=-0.004088426444182312, pvalue=0.7741845770018561),\n",
       "   'nsamples': 4927},\n",
       "  'all': {'pearson': {'all': -0.025679792589757226,\n",
       "    'mean': -0.009374583548574072,\n",
       "    'wmean': -0.026347662336394444},\n",
       "   'spearman': {'all': -0.021552217030538626,\n",
       "    'mean': -0.01777578229851945,\n",
       "    'wmean': -0.02184968077258809}}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d93ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
