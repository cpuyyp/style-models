{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8cbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "from itertools import cycle\n",
    "from ast import literal_eval\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchmetrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import *\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, ModelOutput\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'PAN SCD with contrastive learning.ipynb'\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445efe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = os.environ[\"scratch_result_folder\"] if \"scratch_result_folder\" in os.environ else '../result'\n",
    "scratch_data_folder = os.environ[\"scratch_data_folder\"] if \"scratch_data_folder\" in os.environ else None\n",
    "repo_folder = os.environ[\"style_models_repo_folder\"] if \"style_models_repo_folder\" in os.environ else None\n",
    "data_folder = f\"{repo_folder}/data\" if repo_folder else '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ded426",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyTrainingArgs:\n",
    "    # dataset args # it's not very appropriate to put them here, especially the split\n",
    "    dataset_idx: int\n",
    "#     split: str\n",
    "        \n",
    "    # model args\n",
    "    base_model_name: str ='bert-base-uncased'\n",
    "    freeze_bert: bool = 0\n",
    "    use_pooler: bool = False\n",
    "        \n",
    "    # training args    \n",
    "    num_epoch: int = 5\n",
    "    lr: float = 5e-5\n",
    "    num_warmup_steps = 500\n",
    "    warmup_ratio = 0.1\n",
    "    model_folder: str = None # if None, this will be inferred based on tasks\n",
    "    model_name: str = None # if provide, use to name model_folder, otherwise use style to name model_folder\n",
    "    loss_fn: str = None\n",
    "    do_mlm: bool = False\n",
    "    # tempurature for cosine similarity. \n",
    "    # simcse uses 0.05. However, that's for crossentropy on single label classification. \n",
    "    # For multi-label where BCEloss(withlogits) is used, due to sigmoid, temp < 1 tends to make it learn slower\n",
    "    cos_temp: float = 1. \n",
    "        \n",
    "    # data loader args\n",
    "    batch_size: int = 32\n",
    "    max_length: int = 64\n",
    "    shuffle: bool = False\n",
    "    num_workers: int = 4\n",
    "    data_limit: int = None # if not None, truncate dataset to keep only top {data_limit} rows\n",
    "    \n",
    "    # post training args\n",
    "    save_best_only: bool = True\n",
    "    load_best_at_end: bool = True\n",
    "    early_stop_patience: int = 1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        excute_time = datetime.now() \n",
    "        model_name = self.model_name if self.model_name else f\"pan22-dataset{self.dataset_idx}\"\n",
    "        model_folder = f\"{result_folder}/{model_name}/{excute_time.now().strftime('%Y%m%d-%H:%M:%S')}\"\n",
    "        self.model_folder = model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pan22Datasets(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    # this works for standard class indices and also class probilities\n",
    "    # limit: use to truncate dataset. This will drop rows after certain index. May influence label distribution.\n",
    "    def __init__(self, dataset_idx, split, training_args):\n",
    "        self.dataset_idx = dataset_idx # [1,2,3]. Pan22 only has these three.\n",
    "        self.split = split\n",
    "        self.max_length = training_args.max_length\n",
    "        self.dataset_folder = f\"{data_folder}/pan22/dataset{self.dataset_idx}/{self.split}\"\n",
    "        self.num_docs = len(os.listdir(self.dataset_folder))//2 if self.split != 'test' else len(os.listdir(self.dataset_folder))\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(training_args.base_model_name)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_docs\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        text_file = f\"problem-{idx+1}.txt\"\n",
    "        truth_file = f\"truth-problem-{idx+1}.json\"\n",
    "        with open(f\"{self.dataset_folder}/{text_file}\") as f:\n",
    "            doc = f.readlines()\n",
    "        with open(f\"{self.dataset_folder}/{truth_file}\") as f:\n",
    "            truth = json.load(f)\n",
    "            \n",
    "        item = {'paragraph_authors':truth['paragraph-authors'], 'changes':truth['changes'], 'text': doc}\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''\n",
    "    This is only for batchsize==1\n",
    "    '''\n",
    "    batch_out = batch[0]\n",
    "    \n",
    "    batch_out.update({k:v for k,v in tokenizer(text = batch_out['text'], return_tensors='pt', padding=True, truncation=True, max_length=my_training_args.max_length).to(device).items()})\n",
    "    \n",
    "    del batch_out['text']\n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Similarity(nn.Module):\n",
    "    \"\"\"\n",
    "    Dot product or cosine similarity\n",
    "    From SimCSE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.cos(x, y) / self.temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SCDOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    cos_sim: torch.FloatTensor = None\n",
    "    sent_embs: List[torch.FloatTensor] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd67d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSCD(BertPreTrainedModel):\n",
    "    def __init__(self, config, training_args):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = training_args.use_pooler\n",
    "        self.basemodel = AutoModel.from_pretrained(training_args.base_model_name)\n",
    "        self.do_mlm = training_args.do_mlm\n",
    "        self.cossim = Similarity(training_args.cos_temp)\n",
    "        if training_args.loss_fn == 'BCEWithLogitsLoss':\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        elif training_args.loss_fn == 'MSELoss':\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # mlm is not finished yet\n",
    "#         if self.do_mlm: \n",
    "#             self.lm_head = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, output_sent_embs=False, output_hidden_states=False, output_attentions=False, **kwargs):\n",
    "        output1 = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=output_hidden_states, output_attentions=output_attentions)\n",
    "        output2 = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        if self.use_pooler and ('pooler_output' in output):\n",
    "            sent_emb1 = output1['pooler_output']\n",
    "            sent_emb2 = output2['pooler_output']\n",
    "        else:\n",
    "            sent_emb1 = output1['last_hidden_state'][:,0,:]\n",
    "            sent_emb2 = output2['last_hidden_state'][:,0,:]\n",
    "        \n",
    "        # get contrastive_labels\n",
    "        num_paras = len(kwargs['paragraph_authors'])\n",
    "        contrastive_labels = torch.zeros(num_paras,num_paras)\n",
    "        paragraphs_in_group = collections.defaultdict(list)\n",
    "        for i in range(num_paras):\n",
    "            paragraphs_in_group[kwargs['paragraph_authors'][i]].append(i)\n",
    "        for v in paragraphs_in_group.values():\n",
    "            i_grid,j_grid=torch.meshgrid(torch.tensor(v),torch.tensor(v), indexing='ij')\n",
    "            contrastive_labels[i_grid,j_grid] = 1.\n",
    "        contrastive_labels = contrastive_labels.to(device)\n",
    "        \n",
    "        cos_sim = self.cossim(sent_emb1.unsqueeze(1), sent_emb2.unsqueeze(0))\n",
    "        loss = self.loss_fn(cos_sim, contrastive_labels)\n",
    "        if output_sent_embs:\n",
    "            sent_embs = [sent_emb1.detach(), sent_emb2.detach()]\n",
    "        else:\n",
    "            sent_embs = None\n",
    "        \n",
    "        return SCDOutput(loss=loss, cos_sim=cos_sim, sent_embs=sent_embs, hidden_states=output1.hidden_states, attentions=output1.attentions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(model, cycle_valid_loader, num_valid_steps):\n",
    "    '''\n",
    "    cos_sim gives similarity\n",
    "    style change is the opposite of it\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    threshold = 0.5 if my_training_args.loss_fn == 'MSELoss' else 0.0\n",
    "    \n",
    "    f1 = torchmetrics.F1Score(num_classes=2, average='macro')\n",
    "    \n",
    "    for i_step in trange(num_valid_steps, leave=False):\n",
    "        batch = next(cycle_valid_loader)\n",
    "        output = model(**batch)\n",
    "        \n",
    "        prediction = (torch.diag(output.cos_sim.detach().cpu(),-1)<threshold).type(torch.int64)   \n",
    "        style_change_labels = torch.tensor(batch['changes'])\n",
    "        f1.update(prediction, style_change_labels)\n",
    "        \n",
    "    evaluation = {'f1': f1.compute().item()}\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d5dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(training_args):\n",
    "    config = AutoConfig.from_pretrained(training_args.base_model_name) \n",
    "    model = BertForSCD(config, training_args).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507304af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert == True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert==True:\n",
    "        for param in model.basemodel.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif isinstance(freeze_bert, (int, np.int32, np.int64, torch.int32, torch.int64)):\n",
    "        for param in model.basemodel.embeddings.parameters():\n",
    "            param.requires_grad = False  \n",
    "        for layer in model.basemodel.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de252076",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d919e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169cc5c64d64426996176ca5473ad142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221011_151251-3bg55063</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/PAN%20SCD%20with%20contrastive%20learning/runs/3bg55063\" target=\"_blank\">run 0</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/PAN%20SCD%20with%20contrastive%20learning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036bd668feac415ca82d1d461b4a4405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopped run 0!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.025 MB of 0.025 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_f1</td><td>▁█</td></tr><tr><td>f1</td><td>▆█▆▄▅▅▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_f1</td><td>0.7</td></tr><tr><td>f1</td><td>0.64306</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">run 0</strong>: <a href=\"https://wandb.ai/fsu-dsc-cil/PAN%20SCD%20with%20contrastive%20learning/runs/3bg55063\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/PAN%20SCD%20with%20contrastive%20learning/runs/3bg55063</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221011_151251-3bg55063/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7ccf085bec4ed69817b7c2f712013b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.033341018358866374, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20221011_154248-2zawj42i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/PAN%20SCD%20with%20contrastive%20learning/runs/2zawj42i\" target=\"_blank\">run 1</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/PAN%20SCD%20with%20contrastive%20learning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af311dd3f7de4895b0ada2a036e1353e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 1\n",
    "max_length = 128\n",
    "save_best_only = True # if not save the best, save the last\n",
    "monitering_metric = 'f1'\n",
    "\n",
    "early_stop_patience = 5\n",
    "\n",
    "my_training_args = MyTrainingArgs(dataset_idx=1,\n",
    "                                  model_name=f'PAN SCD with contrastive learning',\n",
    "                                  base_model_name='bert-base-uncased',\n",
    "                                  freeze_bert=False, # will be overwriten in the loop\n",
    "                                  use_pooler=False,\n",
    "                                  num_epoch=num_epochs,\n",
    "                                  batch_size=batch_size,\n",
    "                                  max_length=max_length,\n",
    "                                  save_best_only=save_best_only,\n",
    "                                  early_stop_patience=early_stop_patience,\n",
    "                                 )\n",
    "\n",
    "# some parameters to sweep\n",
    "LR = [3e-5, 5e-5, 7e-5]\n",
    "FREEZE_BERT = [9, 11]\n",
    "WARMUP_RATIO = [0.1, 0.15, 0.2]\n",
    "LOSS_FN = ['BCEWithLogitsLoss', 'MSELoss']\n",
    "\n",
    "LR, FREEZE_BERT, WARMUP_RATIO, LOSS_FN = np.meshgrid(LR,FREEZE_BERT, WARMUP_RATIO, LOSS_FN)\n",
    "LR, FREEZE_BERT, WARMUP_RATIO, LOSS_FN = LR.flatten(), FREEZE_BERT.flatten(), WARMUP_RATIO.flatten(), LOSS_FN.flatten()\n",
    "num_runs = len(LR)\n",
    "\n",
    "# create dataset\n",
    "train_set = Pan22Datasets(dataset_idx=1, split='train', training_args=my_training_args)\n",
    "val_set = Pan22Datasets(dataset_idx=1, split='validation', training_args=my_training_args)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=my_training_args.shuffle, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=1, shuffle=my_training_args.shuffle, collate_fn=collate_fn)\n",
    "\n",
    "num_training_steps = len(train_loader)\n",
    "cycle_train_loader = cycle(iter(train_loader))\n",
    "\n",
    "num_valid_steps = len(val_loader)\n",
    "cycle_val_loader = cycle(iter(val_loader))\n",
    "\n",
    "# start runs\n",
    "for i_run in trange(num_runs):\n",
    "    model_folder = f\"{result_folder}/PAN_SCD_CL/run_{i_run}\"\n",
    "    Path(model_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    lr = LR[i_run]\n",
    "    freeze_bert = FREEZE_BERT[i_run]\n",
    "    warmup_ratio = WARMUP_RATIO[i_run]\n",
    "    loss_fn = LOSS_FN[i_run]\n",
    "    \n",
    "    my_training_args.lr = lr\n",
    "    my_training_args.freeze_bert = int(freeze_bert) if freeze_bert != True else freeze_bert # json doesn't work with np.int64\n",
    "    my_training_args.warmup_ratio = warmup_ratio\n",
    "    my_training_args.loss_fn = loss_fn\n",
    "    \n",
    "    # use wandb to track experiments\n",
    "    wconfig = {}\n",
    "    wconfig['lr'] = lr\n",
    "    wconfig['freeze_bert'] = freeze_bert\n",
    "    wconfig['warmup_ratio'] = warmup_ratio\n",
    "    wconfig['loss_fn'] = loss_fn\n",
    "    \n",
    "\n",
    "    run = wandb.init(project=\"PAN SCD with contrastive learning\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'run {i_run}',\n",
    "                     reinit=True)\n",
    "\n",
    "    model = init_model(my_training_args)\n",
    "    model = freeze_model(model, freeze_bert)\n",
    "    \n",
    "    wandb.watch(model, log=\"all\", log_freq=1000, log_graph=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad==True], lr=lr)\n",
    "\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=int(warmup_ratio*num_epochs*num_training_steps),\n",
    "                            num_training_steps=num_epochs*num_training_steps)\n",
    "\n",
    "    # start training and logging\n",
    "    best_metric = 0.0\n",
    "    previous_metric = 0.0\n",
    "    early_stop_counter = 0\n",
    "    df = pd.DataFrame(columns=['global_step'])\n",
    "    pbar = trange(num_epochs*num_training_steps, leave=False)\n",
    "    for i_epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i_step in range(num_training_steps):\n",
    "\n",
    "            batch = next(cycle_train_loader)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            loss = output['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            pbar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        evaluation = do_eval(model, cycle_val_loader, num_valid_steps)\n",
    "        wandb.log(evaluation, step=pbar.n)\n",
    "        evaluation.update({'global_step':pbar.n})\n",
    "        df = df.append(evaluation, ignore_index=True)\n",
    "        \n",
    "        # save best model\n",
    "        if save_best_only and (best_metric < evaluation[monitering_metric]):\n",
    "            best_metric = evaluation[monitering_metric]\n",
    "            wandb.log({f'best_{monitering_metric}': best_metric})\n",
    "            torch.save(model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "            torch.save(optimizer.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "            torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")\n",
    "        \n",
    "        if previous_metric < evaluation[monitering_metric]:\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(f'Early stopped run {i_run}!')\n",
    "                pbar.close()                \n",
    "                break\n",
    "        previous_metric = evaluation[monitering_metric]\n",
    "    # if not save best, save the last\n",
    "    if not save_best_only:\n",
    "        torch.save(model.state_dict(), f\"{model_folder}/pytorch_model.bin\")\n",
    "        torch.save(optimizer.state_dict(), f\"{model_folder}/optimizer.pt\")\n",
    "        torch.save(scheduler.state_dict(), f\"{model_folder}/scheduler.pt\")\n",
    "    with open(f\"{model_folder}/training_args.json\", \"w\") as outfile:\n",
    "        json.dump(asdict(my_training_args), outfile)\n",
    "    df.to_csv(f\"{model_folder}/evaluation.csv\", index=False)\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40071a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
