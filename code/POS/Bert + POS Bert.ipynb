{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d80428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm.auto import tqdm, trange\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "import inspect\n",
    "from itertools import cycle\n",
    "import ast\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6198d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast, AutoTokenizer\n",
    "from transformers import BertForMaskedLM, BertConfig, PreTrainedModel, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import IntervalStrategy\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers import get_scheduler\n",
    "import transformers\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import torch\n",
    "\n",
    "import wandb\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from sklearn import preprocessing\n",
    "import evaluate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005d560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.utils import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef5e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1954c85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1 2.2.0 4.26.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__, pyg.__version__, transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454ddbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc56ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3958bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_min_pool(x: Tensor, batch: Optional[Tensor],\n",
    "                    size: Optional[int] = None) -> Tensor:\n",
    "    r\"\"\"Returns batch-wise graph-level-outputs by taking the channel-wise\n",
    "    maximum across the node dimension, so that for a single graph\n",
    "    :math:`\\mathcal{G}_i` its output is computed by\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{r}_i = \\mathrm{max}_{n=1}^{N_i} \\, \\mathbf{x}_n.\n",
    "\n",
    "    Functional method of the\n",
    "    :class:`~torch_geometric.nn.aggr.MaxAggregation` module.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Node feature matrix\n",
    "            :math:`\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}`.\n",
    "        batch (torch.Tensor, optional): The batch vector\n",
    "            :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns\n",
    "            each element to a specific example.\n",
    "        size (int, optional): The number of examples :math:`B`.\n",
    "            Automatically calculated if not given. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    dim = -1 if x.dim() == 1 else -2\n",
    "\n",
    "    if batch is None:\n",
    "        return x.max(dim=dim, keepdim=x.dim() <= 2)[0]\n",
    "    size = int(batch.max().item() + 1) if size is None else size\n",
    "    return scatter(x, batch, dim=dim, dim_size=size, reduce='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc72eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_eval = ['edge_indexs', 'hetoro_edges', 'pos_seqs', 'upos_seqs', 'num_syllables', 'alignments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e74064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_author_ids(df, col='author'):\n",
    "    assert col in df, f'no column named {col} found in df'\n",
    "    \n",
    "    unique_author = sorted(df['author'].unique())\n",
    "    mapping = {unique_author[i]:i for i in range(len(unique_author))}\n",
    "    df[col] = df[col].map(mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e0c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, freeze_bert):\n",
    "    '''\n",
    "    if freeze_bert is True, freeze all layer. \n",
    "    if freeze_bert is a positive integer, freeze the bottom {freeze_bert} attention layers\n",
    "    negative integer should also work\n",
    "    '''\n",
    "    if freeze_bert is True: # == True is wrong!!!\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif freeze_bert is False: # isinstance(False, int) returns True!\n",
    "        return model\n",
    "    elif isinstance(freeze_bert, (int, np.int32, np.int64, torch.int32, torch.int64)):\n",
    "        for param in model.bert.embeddings.parameters():\n",
    "            param.requires_grad = False  \n",
    "        for layer in model.bert.encoder.layer[:freeze_bert]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c9e35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_to(dic, device):\n",
    "    for k,v in dic.items():\n",
    "        dic[k] = v.to(device)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2f4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English specific denpendency relations: https://universaldependencies.org/en/dep/\n",
    "s = '''nsubj \tcsubj\n",
    "↳nsubj:pass \t↳csubj:pass\n",
    "↳nsubj:outer \t↳csubj:outer\n",
    "obj \tccomp \txcomp\n",
    "iobj\n",
    "obl \tadvcl \tadvmod\n",
    "↳obl:npmod \t↳advcl:relcl\n",
    "↳obl:tmod\n",
    "vocative \taux \tmark\n",
    "discourse \t↳aux:pass\n",
    "expl \tcop\n",
    "nummod \tacl \tamod\n",
    "  \t↳acl:relcl\n",
    "appos \t  \tdet\n",
    "  \t  \t↳det:predet\n",
    "nmod \t  \t \n",
    "↳nmod:npmod\n",
    "↳nmod:tmod\n",
    "↳nmod:poss\n",
    "compound \tflat\n",
    "↳compound:prt \t↳flat:foreign\n",
    "fixed \tgoeswith\n",
    "conj \tcc\n",
    "  \t↳cc:preconj\n",
    "list \tparataxis \torphan\n",
    "dislocated \t\treparandum\n",
    "root \tpunct \tdep'''\n",
    "all_relations = []\n",
    "s = s.split('\\n')\n",
    "for line in s:\n",
    "    if '↳' in line:\n",
    "        continue\n",
    "    line = line.split('\\t')\n",
    "    for r in line:\n",
    "        if r.strip() == '':\n",
    "            continue\n",
    "        all_relations.append(r.split(':')[0].strip())\n",
    "if 'root' in all_relations:\n",
    "    all_relations.remove('root')\n",
    "    all_relations.append('ROOT')\n",
    "    all_relations.append('case')      # manually add relation not in list\n",
    "    all_relations.append('discourse')    # manually add relation not in list\n",
    "all_relations = sorted(all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9664ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation2id = {all_relations[i]:i for i in range(len(all_relations))}\n",
    "relation2id['self'] = 36 # add self loop type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f01e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('&amp;', '')\n",
    "    # corenlp and bert deal with xxxn't differently\n",
    "    # need to add a space inbetween\n",
    "    text = text.replace(\"dont\", \"don't\")\n",
    "    text = text.replace(\"doesnt\", \"doesn't\")\n",
    "    text = text.replace(\"wont\", \"will n't\")\n",
    "    text = text.replace(\"n\\'t\", \" n\\'t\")\n",
    "    text = text.replace(\"N\\'T\", \" N\\'T\")\n",
    "    text = text.replace(\"cannot\", \"can not\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f483746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(df, add_syllables=False, col='pos_seqs', limit=None, batch_size=32, shuffle=True, max_length=128):\n",
    "    data_list = []\n",
    "    if limit is not None:\n",
    "        dfnew = df.sample(frac=1).reset_index(drop=True)[:limit]\n",
    "    else:\n",
    "        dfnew = df\n",
    "    data_list = []\n",
    "    count = 0\n",
    "    for i in trange(len(dfnew), leave=False):\n",
    "        curr = df.iloc[i]\n",
    "        data = Data()\n",
    "        data.edge_index = torch.cat([torch.tensor([[0],[0]]),  # for self loop of CLS token\n",
    "                                     torch.tensor(curr['edge_indexs']).T, \n",
    "                                     # for batching purpose, if data.x is missing, edge_index is used to inference batch\n",
    "                                     # an isolated node (the SEP in this case) will mess all up\n",
    "                                     torch.tensor([[len(curr['edge_indexs'])+1],[len(curr['edge_indexs'])+1]])], \n",
    "                                    axis=1)\n",
    "        \n",
    "        # add self loop only for cls and sep tokens\n",
    "        data.edge_type_ids = torch.tensor([36]+[relation2id[t.split(':')[0]] for t in curr['hetoro_edges']]+[36])\n",
    "        if data.edge_index.shape[1] >= max_length-1:\n",
    "            count += 1\n",
    "#             print(f\"data {i} too long length {data.edge_index.shape[1]}\")\n",
    "            continue\n",
    "        \n",
    "        data.text = clean_text(curr['text'])\n",
    "        data.pos = ' '.join(curr[col])\n",
    "        data.y = torch.tensor([curr['author']])\n",
    "        if add_syllables:\n",
    "            data.num_syllables = torch.tensor([17]+curr['num_syllables']+[17])\n",
    "            \n",
    "        if 'doc_id' in curr:\n",
    "            data.doc_id = torch.tensor([curr['doc_id']])\n",
    "            \n",
    "        data.num_nodes = len(data.edge_type_ids)\n",
    "        data.alignments = curr['alignments']\n",
    "        data_list.append(data)\n",
    "    print(f'{count} data dropped because of exceeding max_length {max_length}')\n",
    "    loader = DataLoader(data_list, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63db8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_author_ids(df):\n",
    "    assert 'author' in df, 'no column named \"author\" found in df'\n",
    "    \n",
    "    max_id, min_id = df['author'].max(), df['author'].min()\n",
    "    mapping = {i+min_id:i for i in range(max_id-min_id+1)}\n",
    "    df['author'] = df['author'].map(mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8ecbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class myGNNoutput:\n",
    "    loss: None\n",
    "    logit: None\n",
    "    emb: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de80e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddensize2checkpoint = {64: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/\",\n",
    "                         48: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_16/checkpoint-95000/\",\n",
    "                         32: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_10/checkpoint-145000/\",\n",
    "                         16: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_5/checkpoint-95000/\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d421ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, TransformerConv, PDNConv, global_mean_pool, global_max_pool\n",
    "\n",
    "GNNtype2layer = {'GATConv':GATConv, 'GATv2Conv':GATv2Conv, 'TransformerConv':TransformerConv, 'PDNConv':PDNConv}\n",
    "\n",
    "class SemSynGNN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 num_classes, \n",
    "                 num_dep_type, \n",
    "                 heads, \n",
    "                 hidden_dim, \n",
    "                 dep_emb_dim=32, \n",
    "                 add_self_loops=False, \n",
    "                 gnntype='GATConv', \n",
    "                 add_syllables=None,\n",
    "                 pos_checkpoint=\"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/\",\n",
    "                 checkpoint='bert-base-uncased',\n",
    "                 max_length=256,\n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.pos_checkpoint = pos_checkpoint\n",
    "        self.pos_tokenizer = AutoTokenizer.from_pretrained(self.pos_checkpoint, local_files_only=True)\n",
    "        self.pos_bert = BertModel.from_pretrained(self.pos_checkpoint, local_files_only=True, add_pooling_layer = False).to(device)\n",
    "        \n",
    "        self.checkpoint = checkpoint\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint, local_files_only=True)\n",
    "        self.bert = BertModel.from_pretrained(self.checkpoint, add_pooling_layer = False).to(device)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.pos_emb_dim = 64 # this is determined by POS Bert\n",
    "        self.heads = heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dep_emb_dim = dep_emb_dim\n",
    "        self.add_syllables = add_syllables\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.gnn_dim = self.pos_emb_dim + 768\n",
    "        \n",
    "        if add_syllables:\n",
    "            self.num_syllables = 18 # the longest word has 17 syllables\n",
    "            self.syllable_emb_layer = nn.Embedding(self.num_syllables, self.pos_emb_dim)\n",
    "            \n",
    "        self.GNNlayer = GNNtype2layer[gnntype]\n",
    "        \n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.dep_emb_layer = nn.Embedding(num_dep_type, self.dep_emb_dim)\n",
    "        \n",
    "        self.gnns = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            self.gnns.append(self.GNNlayer(self.gnn_dim, self.gnn_dim//self.heads, heads = self.heads, edge_dim=self.dep_emb_dim, beta=True))\n",
    "        \n",
    "        self.layernorms = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            self.layernorms.append(nn.LayerNorm(self.gnn_dim))\n",
    "            \n",
    "        self.classifier = nn.Linear(3*self.gnn_dim, self.num_classes)\n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, text, pos, alignments, edge_index, edge_type_ids, batch, y, ptr, num_syllable=None, readout='pool'):\n",
    "        # word embeddings\n",
    "        # merge subwords and concatenate\n",
    "        word_embs = []\n",
    "        for t,al in zip(text, alignments):\n",
    "            word_tokens = self.tokenizer(t, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            word_emb = self.bert(**word_tokens).last_hidden_state.squeeze(0)\n",
    "            al = torch.LongTensor([-1]+al+[al[-1]+1])+1\n",
    "            al = al.to(device)\n",
    "            zero_emb = torch.zeros(al[-1]+1, 768).to(device)\n",
    "            word_embs.append(zero_emb.index_reduce(0, al, word_emb, 'mean', include_self=False))\n",
    "        word_embs = torch.concat(word_embs, axis=0)    \n",
    "        \n",
    "        # pos embeddings\n",
    "        # drop padded tokens then flatten \n",
    "        pos_tokens = self.pos_tokenizer(pos, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt').to(device)\n",
    "        pos_emb = self.pos_bert(**pos_tokens).last_hidden_state\n",
    "        pos_emb = pos_emb.masked_select(pos_tokens.attention_mask.ge(0.5).unsqueeze(2)).reshape((-1,self.pos_emb_dim))\n",
    "        if self.add_syllables:\n",
    "            syllable_emb = self.syllable_emb_layer(num_syllable)\n",
    "            pos_emb = pos_emb + syllable_emb\n",
    "        \n",
    "        x = torch.concat([word_embs, pos_emb], axis=1)\n",
    "        \n",
    "        edge_attr = self.dep_emb_layer(edge_type_ids)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.gnns[i](self.layernorms[i](x), edge_index, edge_attr=edge_attr).relu() + x\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        if readout == 'pool':\n",
    "            x = torch.cat([global_mean_pool(x, batch), global_max_pool(x, batch), global_min_pool(x, batch)], axis=1)\n",
    "        elif readout == 'cls':\n",
    "            x = x[ptr[:-1],:]\n",
    "            \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        logit = self.classifier(x)\n",
    "        loss = self.lossfn(logit, y)\n",
    "        return myGNNoutput(loss=loss, logit=logit, emb=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ad8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a082941f5924f1aa7c8a4a3c1ce7a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data dropped because of exceeding max_length 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 data dropped because of exceeding max_length 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20230216_174344-loh4bu50</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/SemSynGNN%20%28all%20authors%2C%20bert%20unfrozen%29/runs/loh4bu50\" target=\"_blank\">run_0</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/SemSynGNN%20%28all%20authors%2C%20bert%20unfrozen%29\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/fsu-dsc-cil/SemSynGNN%20%28all%20authors%2C%20bert%20unfrozen%29\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/SemSynGNN%20%28all%20authors%2C%20bert%20unfrozen%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/fsu-dsc-cil/SemSynGNN%20%28all%20authors%2C%20bert%20unfrozen%29/runs/loh4bu50\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/SemSynGNN%20%28all%20authors%2C%20bert%20unfrozen%29/runs/loh4bu50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbc09c104524591a566eadbb4156553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "epochs = 100\n",
    "warmup_ratio = 0.15\n",
    "freeze_bert = 11 # 10, 11\n",
    "batch_size = 64\n",
    "pos_bert_dim = 64\n",
    "pos_checkpoint = hiddensize2checkpoint[pos_bert_dim]\n",
    "\n",
    "LIMIT = [None]\n",
    "NUM_LAYERS = [4]\n",
    "LR = [1e-3]\n",
    "HEADS = [4] \n",
    "READOUT = ['pool']\n",
    "GNNTYPE = ['TransformerConv'] # 'GATConv', 'GATv2Conv', 'TransformerConv'\n",
    "HIDDEN_DIM = [(768+64)//4] # not used \n",
    "DEP_EMB_DIM = [64]\n",
    "NUM_SENT = [1,2,3]\n",
    "ADD_SELF_LOOPS = [False]\n",
    "ADD_SYLLABLES = [True]\n",
    "REPEAT = list(range(1))\n",
    "\n",
    "ARGS = itertools.product(LIMIT, NUM_LAYERS, LR, HEADS, READOUT, GNNTYPE, HIDDEN_DIM, DEP_EMB_DIM, NUM_SENT, ADD_SELF_LOOPS, ADD_SYLLABLES, REPEAT)\n",
    "num_runs = len(list(ARGS))\n",
    "run_pbar = trange(num_runs, leave=False)\n",
    "\n",
    "skip_runs = -1\n",
    "ARGS = itertools.product(LIMIT, NUM_LAYERS, LR, HEADS, READOUT, GNNTYPE, HIDDEN_DIM, DEP_EMB_DIM, NUM_SENT, ADD_SELF_LOOPS, ADD_SYLLABLES, REPEAT)\n",
    "for i_run, args in enumerate(ARGS):\n",
    "\n",
    "    if i_run <= skip_runs:\n",
    "        run_pbar.update(1)\n",
    "        continue\n",
    "    limit, num_layers, lr, heads, readout, gnntype, hidden_dim, dep_emb_dim, num_sent_per_text, add_self_loops, add_syllables, repeat = args\n",
    "    \n",
    "    seed = int(datetime.now().timestamp())\n",
    "    set_seed(seed)\n",
    "    \n",
    "    file = f'../../data/CCAT50/processed/author_all_sent_{num_sent_per_text}_0.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    for col in cols_to_eval:\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "    file = f'../../data/CCAT50/processed/author_all_sent_{num_sent_per_text}_1.csv'\n",
    "    df_val = pd.read_csv(file)\n",
    "    for col in cols_to_eval:\n",
    "        df_val[col] = df_val[col].apply(ast.literal_eval)\n",
    "    val_docid2index = {doc_id:i for i,doc_id in enumerate(df_val['doc_id'].unique())}\n",
    "    \n",
    "    valid_loader = get_loader(df_val, add_syllables=add_syllables, max_length=max_length, batch_size=batch_size)\n",
    "    num_valid_steps = len(valid_loader)\n",
    "    train_loader = get_loader(df, limit = limit, add_syllables=add_syllables, max_length=max_length, batch_size=batch_size)\n",
    "    num_training_steps = len(train_loader)\n",
    "    \n",
    "    model = SemSynGNN(num_layers=num_layers,\n",
    "                       num_classes=50, \n",
    "                       num_dep_type=len(relation2id), \n",
    "                       heads=heads,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       dep_emb_dim=dep_emb_dim, \n",
    "                       add_self_loops=add_self_loops,\n",
    "                       gnntype=gnntype,\n",
    "                       add_syllables=add_syllables,\n",
    "                       pos_checkpoint=pos_checkpoint,\n",
    "                      )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = freeze_model(model, freeze_bert)    \n",
    "    \n",
    "    para = []\n",
    "    for name, module in model.named_children():\n",
    "        if name == 'bert':\n",
    "            para.append({\"params\": [p for p in module.parameters() if p.requires_grad==True], 'lr': 5e-5})\n",
    "        else:\n",
    "            para.append({\"params\": module.parameters(), 'lr': lr})\n",
    "\n",
    "    optimizer = torch.optim.Adam(para)\n",
    "    \n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=int(warmup_ratio*epochs*num_training_steps),\n",
    "                            num_training_steps=epochs*num_training_steps)\n",
    "    \n",
    "    wconfig = {}\n",
    "    wconfig['seed'] = seed\n",
    "    wconfig['num_sent_per_text'] = num_sent_per_text\n",
    "    wconfig['limit'] = limit\n",
    "    wconfig['num_layers'] = num_layers\n",
    "    wconfig['lr'] = lr\n",
    "    wconfig['heads'] = heads\n",
    "    wconfig['readout'] = readout\n",
    "    wconfig['GNNtype'] = gnntype\n",
    "    wconfig['add_self_loops'] = add_self_loops\n",
    "    wconfig['add_syllables'] = add_syllables\n",
    "    wconfig['pos_checkpoint'] = pos_checkpoint\n",
    "    \n",
    "    run = wandb.init(project=\"SemSynGNN (all authors, bert unfrozen)\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'run_{i_run}',\n",
    "                     reinit=True,\n",
    "                     settings=wandb.Settings(start_method='thread'))\n",
    "    \n",
    "    best_evaluation = collections.defaultdict(float)\n",
    "    pbar = trange(epochs*num_training_steps, leave=False)\n",
    "    for i_epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if add_syllables:\n",
    "                output = model(data.text, data.pos, data.alignments, data.edge_index, data.edge_type_ids, data.batch, data.y, data.ptr, data.num_syllables, readout=readout)\n",
    "            else:\n",
    "                output = model(data.text, data.pos, data.alignments, data.edge_index, data.edge_type_ids, data.batch, data.y, data.ptr, readout=readout)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            pbar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        doc_score = 1e-8*np.ones((len(val_docid2index),50))\n",
    "        doc_true = np.zeros(len(val_docid2index))\n",
    "        metric = evaluate.load('/home/jz17d/Desktop/metrics/accuracy')\n",
    "        for data in valid_loader:\n",
    "            data.to(device)\n",
    "            if add_syllables:\n",
    "                output = model(data.text, data.pos, data.alignments, data.edge_index, data.edge_type_ids, data.batch, data.y, data.ptr, data.num_syllables, readout=readout)\n",
    "            else:\n",
    "                output = model(data.text, data.pos, data.alignments, data.edge_index, data.edge_type_ids, data.batch, data.y, data.ptr, readout=readout)\n",
    "            metric.add_batch(predictions=output.logit.argmax(axis=-1).cpu().detach().numpy(), references=data.y.cpu().numpy())\n",
    "            \n",
    "            pred = output.logit.argmax(axis=-1).cpu().detach().numpy()\n",
    "            doc_id = np.vectorize(val_docid2index.get)(data.doc_id.cpu().detach().numpy()) \n",
    "            doc_score[doc_id,pred] += 1\n",
    "            doc_true[doc_id] = data.y.cpu().numpy()\n",
    "        \n",
    "        # logging\n",
    "        evaluation = metric.compute()\n",
    "        for k in range(1, 6):\n",
    "            evaluation.update({f'doc_acc@{k}': top_k_accuracy_score(doc_true, doc_score, k=k)})\n",
    "        wandb.log(evaluation, step=pbar.n)\n",
    "        \n",
    "        # logging best\n",
    "        for key in evaluation:\n",
    "            best_evaluation[f'best_{key}'] = max(best_evaluation[f'best_{key}'], evaluation[key])\n",
    "        wandb.log(best_evaluation, step=pbar.n)\n",
    "    \n",
    "    run.finish()\n",
    "    run_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a246842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_grad_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad )\n",
    "def count_all_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061af9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14810354, 109526258)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_grad_parameters(model),count_all_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4d53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14175744, 108891648)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_grad_parameters(model.bert), count_all_parameters(model.bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ded809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559104"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_grad_parameters(model.gnns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b274b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
