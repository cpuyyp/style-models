{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.parse.corenlp import CoreNLPParser,CoreNLPDependencyParser\n",
    "import nltk\n",
    "import stanza\n",
    "from tqdm.auto import tqdm, trange\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from numerize import numerize\n",
    "# stanza.download('en') # download English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3203e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from transformers import BertForMaskedLM, BertConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tag.hunpos import HunposTagger\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bf202",
   "metadata": {},
   "source": [
    "# tagger check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(tag_sent):\n",
    "    words = []\n",
    "    tags = []\n",
    "    for i in range(len(tag_sent)):\n",
    "        words.append(tag_sent[i][0])\n",
    "        tags.append(tag_sent[i][1].decode('utf-8') if isinstance(tag_sent[i][1], bytes) else tag_sent[i][1])  \n",
    "    print('Sentences:\\t', '\\t'.join(words))\n",
    "    print('POS tags:\\t', '\\t'.join(tags))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hunpos\n",
    "ht = HunposTagger('/home/jz17d/bin/english.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanford corenlp \n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 11:46:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a41a6b2ba554c26b8f36929bf6d8a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 11:46:56 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2022-11-02 11:46:56 INFO: Use device: gpu\n",
      "2022-11-02 11:46:56 INFO: Loading: tokenize\n",
      "2022-11-02 11:47:16 INFO: Loading: pos\n",
      "2022-11-02 11:47:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fa344",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['This is how I check. -- - --- :',\n",
    "             'how to use \"-\" correctly?',\n",
    "             \"Nice to meet you. -Mia\",\n",
    "             \"Why is that ( blabla ) happening?\",\n",
    "             'my number is 850-000-0000',\n",
    "             \"How about E=mc^2 and E = mc ^ 2?\",\n",
    "             \"quotes? he said, 'Nice to meet you.'\",\n",
    "             \"' Is it because of nltk.ssplit?' Yes\",\n",
    "             'My 6 y.o. boy like it.',\n",
    "             \"I gave him $200.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae22a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\t This\tis\thow\tI\tcheck\t.\t--\t-\t--\t-\t:\n",
      "POS tags:\t DT\tVBZ\tWRB\tPRP\tVBP\t.\t:\t:\t:\t:\t:\n",
      "Sentences:\t how\tto\tuse\t``\t-\t''\tcorrectly\t?\n",
      "POS tags:\t WRB\tTO\tVB\t``\t:\t''\tRB\t.\n",
      "Sentences:\t Nice\tto\tmeet\tyou\t.\t-Mia\n",
      "POS tags:\t JJ\tTO\tVB\tPRP\t.\tNNP\n",
      "Sentences:\t Why\tis\tthat\t(\tblabla\t)\thappening\t?\n",
      "POS tags:\t WRB\tVBZ\tDT\tJJ\tNN\tVBZ\tVBG\t.\n",
      "Sentences:\t my\tnumber\tis\t850-000-0000\n",
      "POS tags:\t PRP$\tNN\tVBZ\tCD\n",
      "Sentences:\t How\tabout\tE=mc^2\tand\tE\t=\tmc\t^\t2\t?\n",
      "POS tags:\t WRB\tIN\tNN\tCC\tNN\tSYM\tCD\tCD\tCD\t.\n",
      "Sentences:\t quotes\t?\the\tsaid\t,\t'Nice\tto\tmeet\tyou\t.\t'\n",
      "POS tags:\t NNS\t.\tPRP\tVBD\t,\tNNP\tTO\tVB\tPRP\t.\t''\n",
      "Sentences:\t '\tIs\tit\tbecause\tof\tnltk.ssplit\t?\t'\tYes\n",
      "POS tags:\t ''\tVBZ\tPRP\tIN\tIN\tJJ\t.\t''\tUH\n",
      "Sentences:\t My\t6\ty.o\t.\tboy\tlike\tit\t.\n",
      "POS tags:\t PRP$\tCD\tNN\t.\tNN\tIN\tPRP\t.\n",
      "Sentences:\t I\tgave\thim\t$\t200\t.\n",
      "POS tags:\t PRP\tVBD\tPRP\t$\tCD\t.\n"
     ]
    }
   ],
   "source": [
    "# hunpos\n",
    "tagged = ht.tag_sents([word_tokenize(sent) for sent in sentences])\n",
    "for sent in tagged:\n",
    "    pprint(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a19029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\t This\tis\thow\tI\tcheck\t.\t--\t-\t---\t:\n",
      "POS tags:\t DT\tVBZ\tWRB\tPRP\tVBP\t.\t:\tHYPH\tNFP\t:\n",
      "Sentences:\t how\tto\tuse\t\"\t-\t\"\tcorrectly\t?\n",
      "POS tags:\t WRB\tTO\tVB\t''\t,\t``\tRB\t.\n",
      "Sentences:\t Nice\tto\tmeet\tyou\t.\t-\tMia\n",
      "POS tags:\t JJ\tTO\tVB\tPRP\t.\tNFP\tNNP\n",
      "Sentences:\t Why\tis\tthat\t(\tblabla\t)\thappening\t?\n",
      "POS tags:\t WRB\tVBZ\tDT\t-LRB-\tFW\t-RRB-\tVBG\t.\n",
      "Sentences:\t my\tnumber\tis\t850-000-0000\n",
      "POS tags:\t PRP$\tNN\tVBZ\tCD\n",
      "Sentences:\t How\tabout\tE\t=\tmc\t^\t2\tand\tE\t=\tmc\t^\t2\t?\n",
      "POS tags:\t WRB\tIN\tNN\tSYM\tNN\tSYM\tCD\tCC\tNN\tSYM\tNN\tSYM\tCD\t.\n",
      "Sentences:\t quotes\t?\the\tsaid\t,\t'\tNice\tto\tmeet\tyou\t.\t'\n",
      "POS tags:\t NNS\t,\tPRP\tVBD\t,\t''\tJJ\tTO\tVB\tPRP\t.\t''\n",
      "Sentences:\t '\tIs\tit\tbecause\tof\tnltk.ssplit\t?\t'\tYes\n",
      "POS tags:\t ``\tVBZ\tPRP\tIN\tIN\tNNP\t.\t''\tUH\n",
      "Sentences:\t My\t6\ty.o.\tboy\tlike\tit\t.\n",
      "POS tags:\t PRP$\tCD\tNN\tNN\tIN\tPRP\t.\n",
      "Sentences:\t I\tgave\thim\t$\t200\t.\n",
      "POS tags:\t PRP\tVBD\tPRP\t$\tCD\t.\n"
     ]
    }
   ],
   "source": [
    "# standford corenlp\n",
    "tagged = list(pos_tagger.raw_tag_sents(sentences))\n",
    "for sent in tagged:\n",
    "    pprint(sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\t This\tis\thow\tI\tcheck\t.\n",
      "POS tags:\t DT\tVBZ\tWRB\tPRP\tVBP\t.\n",
      "Sentences:\t --\t-\t---\t:\n",
      "POS tags:\t NFP\t,\tNFP\t:\n",
      "Sentences:\t how\tto\tuse\t\"\t-\t\"\tcorrectly\t?\n",
      "POS tags:\t WRB\tTO\tVB\t``\tHYPH\t''\tRB\t.\n",
      "Sentences:\t Nice\tto\tmeet\tyou\t.\n",
      "POS tags:\t JJ\tTO\tVB\tPRP\t.\n",
      "Sentences:\t -\tMia\n",
      "POS tags:\t NFP\tNNP\n",
      "Sentences:\t Why\tis\tthat\t(\tblabla\t)\thappening\t?\n",
      "POS tags:\t WRB\tVBZ\tDT\t-LRB-\tNN\t-RRB-\tVBG\t.\n",
      "Sentences:\t my\tnumber\tis\t850-000-0000\n",
      "POS tags:\t PRP$\tNN\tVBZ\tCD\n",
      "Sentences:\t How\tabout\tE=mc^2\tand\tE\t=\tmc\t^\t2\t?\n",
      "POS tags:\t WRB\tIN\tNNP\tCC\tNNP\tSYM\tNNP\tSYM\tCD\t.\n",
      "Sentences:\t quotes\t?\n",
      "POS tags:\t NNS\t.\n",
      "Sentences:\t he\tsaid\t,\t'\tNice\tto\tmeet\tyou\t.\t'\n",
      "POS tags:\t PRP\tVBD\t,\t``\tJJ\tTO\tVB\tPRP\t.\t''\n",
      "Sentences:\t '\tIs\tit\tbecause\tof\tnltk.ssplit\t?\t'\n",
      "POS tags:\t ``\tVBZ\tPRP\tIN\tIN\tNN\t.\t''\n",
      "Sentences:\t Yes\n",
      "POS tags:\t UH\n",
      "Sentences:\t My\t6\ty.o.\tboy\tlike\tit\t.\n",
      "POS tags:\t PRP$\tCD\tNN\tNN\tVBP\tPRP\t.\n",
      "Sentences:\t I\tgave\thim\t$\t200\t.\n",
      "POS tags:\t PRP\tVBD\tPRP\t$\tCD\t.\n"
     ]
    }
   ],
   "source": [
    "# stanza\n",
    "docs = [nlp(sent) for sent in sentences]\n",
    "doc_upos = []\n",
    "doc_xpos = []\n",
    "for doc in docs:\n",
    "    for sentence in doc.sentences:\n",
    "        upos = []\n",
    "        xpos = []\n",
    "        for word in sentence.words:\n",
    "            upos.append((word.text,word.upos))\n",
    "            xpos.append((word.text,word.xpos))\n",
    "        doc_upos.append(upos)\n",
    "        doc_xpos.append(xpos)    \n",
    "for sent in doc_xpos:\n",
    "    pprint(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52c165",
   "metadata": {},
   "source": [
    "# dependency tree parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "depparser = CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed = depparser.raw_parse(sentences[-1])\n",
    "# parsed = depparser.raw_parse('President Trump likes to make money.')\n",
    "# 'I put the book in the box on the table.'\n",
    "parsed = depparser.raw_parse('President Trump likes to make more and more money.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9349151",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e365c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"264px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,200.0,264.0\" width=\"200px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">likes</text></svg><svg width=\"44%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Trump</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">President</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"44%\" x=\"44%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">make</text></svg><svg width=\"36.3636%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">to</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.1818%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"63.6364%\" x=\"36.3636%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">money</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">more</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">more</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.1818%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"12%\" x=\"88%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"94%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('likes', [Tree('Trump', ['President']), Tree('make', ['to', Tree('money', [Tree('more', [Tree('more', ['and'])])])]), '.'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5983d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph G{\n",
      "edge [dir=forward]\n",
      "node [shape=plaintext]\n",
      "\n",
      "0 [label=\"0 (None)\"]\n",
      "0 -> 3 [label=\"ROOT\"]\n",
      "0 -> 3 [label=\"ROOT\"]\n",
      "0 -> 1 [label=\"compound\"]\n",
      "0 -> 2 [label=\"nsubj\"]\n",
      "0 -> 4 [label=\"mark\"]\n",
      "0 -> 5 [label=\"xcomp\"]\n",
      "0 -> 6 [label=\"amod\"]\n",
      "0 -> 9 [label=\"obj\"]\n",
      "0 -> 7 [label=\"cc\"]\n",
      "0 -> 8 [label=\"conj\"]\n",
      "0 -> 10 [label=\"punct\"]\n",
      "1 [label=\"1 (President)\"]\n",
      "1 -> 0 \n",
      "1 -> 2 [label=\"nsubj\"]\n",
      "1 -> 3 [label=\"ROOT\"]\n",
      "1 -> 4 [label=\"mark\"]\n",
      "1 -> 5 [label=\"xcomp\"]\n",
      "1 -> 6 [label=\"amod\"]\n",
      "1 -> 9 [label=\"obj\"]\n",
      "1 -> 7 [label=\"cc\"]\n",
      "1 -> 8 [label=\"conj\"]\n",
      "1 -> 10 [label=\"punct\"]\n",
      "2 [label=\"2 (Trump)\"]\n",
      "2 -> 1 [label=\"compound\"]\n",
      "2 -> 1 [label=\"compound\"]\n",
      "2 -> 0 \n",
      "2 -> 3 [label=\"ROOT\"]\n",
      "2 -> 4 [label=\"mark\"]\n",
      "2 -> 5 [label=\"xcomp\"]\n",
      "2 -> 6 [label=\"amod\"]\n",
      "2 -> 9 [label=\"obj\"]\n",
      "2 -> 7 [label=\"cc\"]\n",
      "2 -> 8 [label=\"conj\"]\n",
      "2 -> 10 [label=\"punct\"]\n",
      "3 [label=\"3 (likes)\"]\n",
      "3 -> 2 [label=\"nsubj\"]\n",
      "3 -> 2 [label=\"nsubj\"]\n",
      "3 -> 5 [label=\"xcomp\"]\n",
      "3 -> 5 [label=\"xcomp\"]\n",
      "3 -> 10 [label=\"punct\"]\n",
      "3 -> 10 [label=\"punct\"]\n",
      "3 -> 0 \n",
      "3 -> 1 [label=\"compound\"]\n",
      "3 -> 4 [label=\"mark\"]\n",
      "3 -> 6 [label=\"amod\"]\n",
      "3 -> 9 [label=\"obj\"]\n",
      "3 -> 7 [label=\"cc\"]\n",
      "3 -> 8 [label=\"conj\"]\n",
      "4 [label=\"4 (to)\"]\n",
      "4 -> 0 \n",
      "4 -> 1 [label=\"compound\"]\n",
      "4 -> 2 [label=\"nsubj\"]\n",
      "4 -> 3 [label=\"ROOT\"]\n",
      "4 -> 5 [label=\"xcomp\"]\n",
      "4 -> 6 [label=\"amod\"]\n",
      "4 -> 9 [label=\"obj\"]\n",
      "4 -> 7 [label=\"cc\"]\n",
      "4 -> 8 [label=\"conj\"]\n",
      "4 -> 10 [label=\"punct\"]\n",
      "5 [label=\"5 (make)\"]\n",
      "5 -> 4 [label=\"mark\"]\n",
      "5 -> 4 [label=\"mark\"]\n",
      "5 -> 9 [label=\"obj\"]\n",
      "5 -> 9 [label=\"obj\"]\n",
      "5 -> 0 \n",
      "5 -> 1 [label=\"compound\"]\n",
      "5 -> 2 [label=\"nsubj\"]\n",
      "5 -> 3 [label=\"ROOT\"]\n",
      "5 -> 6 [label=\"amod\"]\n",
      "5 -> 7 [label=\"cc\"]\n",
      "5 -> 8 [label=\"conj\"]\n",
      "5 -> 10 [label=\"punct\"]\n",
      "6 [label=\"6 (more)\"]\n",
      "6 -> 8 [label=\"conj\"]\n",
      "6 -> 8 [label=\"conj\"]\n",
      "6 -> 0 \n",
      "6 -> 1 [label=\"compound\"]\n",
      "6 -> 2 [label=\"nsubj\"]\n",
      "6 -> 3 [label=\"ROOT\"]\n",
      "6 -> 4 [label=\"mark\"]\n",
      "6 -> 5 [label=\"xcomp\"]\n",
      "6 -> 9 [label=\"obj\"]\n",
      "6 -> 7 [label=\"cc\"]\n",
      "6 -> 10 [label=\"punct\"]\n",
      "7 [label=\"7 (and)\"]\n",
      "7 -> 0 \n",
      "7 -> 1 [label=\"compound\"]\n",
      "7 -> 2 [label=\"nsubj\"]\n",
      "7 -> 3 [label=\"ROOT\"]\n",
      "7 -> 4 [label=\"mark\"]\n",
      "7 -> 5 [label=\"xcomp\"]\n",
      "7 -> 6 [label=\"amod\"]\n",
      "7 -> 9 [label=\"obj\"]\n",
      "7 -> 8 [label=\"conj\"]\n",
      "7 -> 10 [label=\"punct\"]\n",
      "8 [label=\"8 (more)\"]\n",
      "8 -> 7 [label=\"cc\"]\n",
      "8 -> 7 [label=\"cc\"]\n",
      "8 -> 0 \n",
      "8 -> 1 [label=\"compound\"]\n",
      "8 -> 2 [label=\"nsubj\"]\n",
      "8 -> 3 [label=\"ROOT\"]\n",
      "8 -> 4 [label=\"mark\"]\n",
      "8 -> 5 [label=\"xcomp\"]\n",
      "8 -> 6 [label=\"amod\"]\n",
      "8 -> 9 [label=\"obj\"]\n",
      "8 -> 10 [label=\"punct\"]\n",
      "9 [label=\"9 (money)\"]\n",
      "9 -> 6 [label=\"amod\"]\n",
      "9 -> 6 [label=\"amod\"]\n",
      "9 -> 0 \n",
      "9 -> 1 [label=\"compound\"]\n",
      "9 -> 2 [label=\"nsubj\"]\n",
      "9 -> 3 [label=\"ROOT\"]\n",
      "9 -> 4 [label=\"mark\"]\n",
      "9 -> 5 [label=\"xcomp\"]\n",
      "9 -> 7 [label=\"cc\"]\n",
      "9 -> 8 [label=\"conj\"]\n",
      "9 -> 10 [label=\"punct\"]\n",
      "10 [label=\"10 (.)\"]\n",
      "10 -> 0 \n",
      "10 -> 1 [label=\"compound\"]\n",
      "10 -> 2 [label=\"nsubj\"]\n",
      "10 -> 3 [label=\"ROOT\"]\n",
      "10 -> 4 [label=\"mark\"]\n",
      "10 -> 5 [label=\"xcomp\"]\n",
      "10 -> 6 [label=\"amod\"]\n",
      "10 -> 9 [label=\"obj\"]\n",
      "10 -> 7 [label=\"cc\"]\n",
      "10 -> 8 [label=\"conj\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(x.to_dot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804a4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('likes', 'VBZ'), 'nsubj', ('Trump', 'NNP')),\n",
       " (('Trump', 'NNP'), 'compound', ('President', 'NNP')),\n",
       " (('likes', 'VBZ'), 'xcomp', ('make', 'VB')),\n",
       " (('make', 'VB'), 'mark', ('to', 'TO')),\n",
       " (('make', 'VB'), 'obj', ('money', 'NN')),\n",
       " (('money', 'NN'), 'amod', ('more', 'JJR')),\n",
       " (('more', 'JJR'), 'conj', ('more', 'JJR')),\n",
       " (('more', 'JJR'), 'cc', ('and', 'CC')),\n",
       " (('likes', 'VBZ'), 'punct', ('.', '.'))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3117e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President\tNNP\t2\tcompound\n",
      "Trump\tNNP\t3\tnsubj\n",
      "likes\tVBZ\t0\tROOT\n",
      "to\tTO\t5\tmark\n",
      "make\tVB\t3\txcomp\n",
      "more\tJJR\t9\tamod\n",
      "and\tCC\t8\tcc\n",
      "more\tJJR\t6\tconj\n",
      "money\tNN\t5\tobj\n",
      ".\t.\t3\tpunct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.to_conll(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9dc19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.multidigraph.MultiDiGraph>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = x.nx_graph()\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc1f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((1, 2, 3, 4, 5, 6, 7, 8, 9, 10))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e5c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutMultiEdgeDataView([(1, 2), (2, 3), (4, 5), (5, 3), (6, 9), (7, 8), (8, 6), (9, 5), (10, 3)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee5315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, {2: {'compound': {}}}),\n",
       " (2, {3: {'nsubj': {}}}),\n",
       " (3, {}),\n",
       " (4, {5: {'mark': {}}}),\n",
       " (5, {3: {'xcomp': {}}}),\n",
       " (6, {9: {'amod': {}}}),\n",
       " (7, {8: {'cc': {}}}),\n",
       " (8, {6: {'conj': {}}}),\n",
       " (9, {5: {'obj': {}}}),\n",
       " (10, {3: {'punct': {}}})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(G.adjacency())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7187e87",
   "metadata": {},
   "source": [
    "# amazon review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d45214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '../../data/amazon review 2018/Toys_and_Games_5.json'\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('../../data/amazon review 2018/Toys_and_Games_5.json.gz')\n",
    "df = df.dropna(subset=['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d49dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_tag(df):\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,pos')\n",
    "    all_num_sent = []\n",
    "    all_num_word = []\n",
    "    all_upos = []\n",
    "    all_xpos = []\n",
    "    for text in tqdm(df['reviewText']):\n",
    "        doc = nlp(text)\n",
    "    #     sentences = [sentence.text for sentence in doc.sentences]\n",
    "        num_word = []\n",
    "        doc_upos = []\n",
    "        doc_xpos = []\n",
    "        for sentence in doc.sentences:\n",
    "            num_word.append(len(sentence.words))\n",
    "            upos = []\n",
    "            xpos = []\n",
    "            for word in sentence.words:\n",
    "                upos.append(word.upos)\n",
    "                xpos.append(word.xpos)  \n",
    "            doc_upos.append(upos)\n",
    "            doc_xpos.append(xpos)    \n",
    "        all_num_sent.append(len(doc.sentences))\n",
    "        all_num_word.append(num_word)\n",
    "        all_upos.append(doc_upos)\n",
    "        all_xpos.append(doc_xpos)   \n",
    "    df['all_num_sent'] = all_num_sent\n",
    "    df['all_num_word'] = all_num_word    \n",
    "    df['all_upos'] = all_upos    \n",
    "    df['all_xpos'] = all_xpos    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 15:18:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8a73050dad490f94c26c9f72fffd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 15:18:12 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2022-10-25 15:18:12 INFO: Use device: gpu\n",
      "2022-10-25 15:18:12 INFO: Loading: tokenize\n",
      "2022-10-25 15:18:12 INFO: Loading: pos\n",
      "2022-10-25 15:18:13 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5189f2dfd8443389353d623c0b265be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4345/1336902853.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['all_num_sent'] = all_num_sent\n",
      "/tmp/ipykernel_4345/1336902853.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['all_num_word'] = all_num_word\n",
      "/tmp/ipykernel_4345/1336902853.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['all_upos'] = all_upos\n",
      "/tmp/ipykernel_4345/1336902853.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['all_xpos'] = all_xpos\n"
     ]
    }
   ],
   "source": [
    "df5000 = df[:5000]\n",
    "df5000 = stanza_tag(df5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85791f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df5000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35134/251290121.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../data/amazon review 2018/Toys_and_Games_5_tagged5000.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf5000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df5000' is not defined"
     ]
    }
   ],
   "source": [
    "out_name = '../../data/amazon review 2018/Toys_and_Games_5_tagged5000.csv'\n",
    "df5000.to_csv(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9132eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7918bc6",
   "metadata": {},
   "source": [
    "# bookcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 11:32:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4a419162cd45539361a7a6e279b7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 11:32:18 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2022-10-27 11:32:19 INFO: Use device: gpu\n",
      "2022-10-27 11:32:19 INFO: Loading: tokenize\n",
      "2022-10-27 11:32:55 INFO: Loading: pos\n",
      "2022-10-27 11:32:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (/scratch/data_jz17d/hf_datasets_cache/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b786db956a4aca83872c529a3e157a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['HF_DATASETS_CACHE'] = '/scratch/data_jz17d/hf_datasets_cache'\n",
    "dataset = load_dataset(\"bookcorpus\", cache_dir=\"/scratch/data_jz17d/hf_datasets_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd5b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my room was just up the stairs , and it beckoned to me even though i was more interested in dinner .'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset['train'][134]['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d302bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: my\tupos: PRON\txpos: PRP$\tfeats: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "word: room\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: was\tupos: AUX\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "word: just\tupos: ADV\txpos: RB\tfeats: _\n",
      "word: up\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: the\tupos: DET\txpos: DT\tfeats: Definite=Def|PronType=Art\n",
      "word: stairs\tupos: NOUN\txpos: NNS\tfeats: Number=Plur\n",
      "word: ,\tupos: PUNCT\txpos: ,\tfeats: _\n",
      "word: and\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: it\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "word: beckoned\tupos: VERB\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "word: to\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: me\tupos: PRON\txpos: PRP\tfeats: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "word: even\tupos: ADV\txpos: RB\tfeats: _\n",
      "word: though\tupos: SCONJ\txpos: IN\tfeats: _\n",
      "word: i\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: was\tupos: AUX\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\n",
      "word: more\tupos: ADV\txpos: RBR\tfeats: Degree=Cmp\n",
      "word: interested\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: in\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: dinner\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fa227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 74004228\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging(example):\n",
    "    if isinstance(example['text'], list):\n",
    "        upos = []\n",
    "        xpos = []\n",
    "        for text in example['text']:\n",
    "            doc = nlp(text)\n",
    "            upos.append(' '.join([word.upos for sentence in doc.sentences for word in sentence.words]))\n",
    "            xpos.append(' '.join([word.xpos for sentence in doc.sentences for word in sentence.words]))\n",
    "    else:\n",
    "        doc = nlp(example['text'])\n",
    "        upos = ' '.join([word.upos for sentence in doc.sentences for word in sentence.words])\n",
    "        xpos = ' '.join([word.xpos for sentence in doc.sentences for word in sentence.words])\n",
    "    return {'upos':upos, 'xpos':xpos}\n",
    "dataset['train'].set_transform(POS_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefe8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upos': 'PRON NOUN AUX ADV ADP DET NOUN PUNCT CCONJ PRON VERB ADP PRON ADV SCONJ PRON AUX ADV ADJ ADP NOUN PUNCT',\n",
       " 'xpos': 'PRP$ NN VBD RB IN DT NNS , CC PRP VBD IN PRP RB IN PRP VBD RBR JJ IN NN .'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0950ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28445a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
