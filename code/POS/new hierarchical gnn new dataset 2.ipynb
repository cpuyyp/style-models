{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567d35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined library\n",
    "from data_utils import keystoint, get_seg_loader # other unneeded definitions: MyData, MyDataset, SegmentBatchCollater, SegmentDataLoader, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Sequence, Union\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch_geometric as pyg\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, TransformerConv, PDNConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "import transformers\n",
    "from transformers import get_scheduler, AutoTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "\n",
    "import evaluate\n",
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4aa9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.26.0', '2.2.0', '1.13.1', device(type='cuda'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformers.__version__, pyg.__version__, torch.__version__,device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70da8683",
   "metadata": {},
   "source": [
    "# definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyConfig:\n",
    "    # dataset configs\n",
    "    dataset: str\n",
    "    num_classes: int \n",
    "    segment_length: int = 'doc'\n",
    "\n",
    "    # model archtectures\n",
    "    pos_emb_dim: int = 64 # this is determined by the pretrained pos bert\n",
    "    dep_emb_dim: int = 32 # edge attribute dim\n",
    "    hidden_dim: int = field(init=False)\n",
    "    # these 4 parameters below are not changable\n",
    "    num_dep_type: int = 37 # this is determined by dependency2id\n",
    "    # max syllable for common word is 17. However, the longest word in English is a protein that has 189819 letters! \n",
    "    # That must have much more syllables. Truncate to 32 to simplify.\n",
    "    max_num_syllables: int = 32  \n",
    "    max_sentence_num: int = 64 \n",
    "    # zipf frequency bins. 0-8 stands for its frequency between 10**(x-1) and 10**x. 9 is for punctuations. 10 is for CLS and SEP\n",
    "    num_freq_type: int = 11\n",
    "\n",
    "    num_layers: int = 4\n",
    "    heads: int = 4\n",
    "    num_hierarchy: int = 1\n",
    "\n",
    "    add_self_loops: bool = False\n",
    "    add_syllables: bool = True\n",
    "    add_word_freq: bool = True\n",
    "    add_dep: bool = True\n",
    "    add_sentence_order: bool = True # only if num_hierarchy > 0\n",
    "    \n",
    "    # training configs\n",
    "    max_length: int = 256 # this is only for pos tokenizer, bert tokenizer use default 512\n",
    "    dropout: float = 0.1\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 100\n",
    "    warmup_ratio: float = 0.15\n",
    "    lr: float = 2e-3\n",
    "    save_location: str = None\n",
    "\n",
    "    # pretrained checkpoints\n",
    "    pos_checkpoint: str = field(init=False)\n",
    "    bert_checkpoint: str = 'bert-base-uncased'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.pos_emb_dim % self.heads==0, 'make sure pos_emb_dim is dividable to heads'\n",
    "        self.hidden_dim = self.pos_emb_dim//self.heads\n",
    "\n",
    "        pos_emb_dim2pos_checkpoint = {64: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/\",\n",
    "                                    48: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_16/checkpoint-95000/\",\n",
    "                                    32: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_10/checkpoint-145000/\",\n",
    "                                    16: \"/scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_5/checkpoint-95000/\",}\n",
    "        self.pos_checkpoint = pos_emb_dim2pos_checkpoint[self.pos_emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class myGNNoutput:\n",
    "    loss: None\n",
    "    logit: None\n",
    "    emb: None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac635a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNNtype2layer = {'GATConv':GATConv, 'GATv2Conv':GATv2Conv, 'TransformerConv':TransformerConv, 'PDNConv':PDNConv}\n",
    "\n",
    "class MyGNNBlock(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 heads,\n",
    "                 dropout=0.1,\n",
    "                 dropout_position='last',\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(in_channels)\n",
    "        self.gnnlayer = TransformerConv(in_channels=in_channels, out_channels=out_channels, heads=heads, beta=True, **kwargs)\n",
    "        self.dropout = dropout\n",
    "        self.dropout_position = dropout_position\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        if self.dropout_position=='last':\n",
    "            x = x + self.gnnlayer(self.layernorm(x), edge_index, edge_attr).relu()\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        elif self.dropout_position=='first':\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = x + self.gnnlayer(self.layernorm(x), edge_index, edge_attr).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierGNN(torch.nn.Module):\n",
    "    def __init__(self, myconfig):\n",
    "        super().__init__()\n",
    "        self.num_classes = myconfig.num_classes\n",
    "    \n",
    "        # model archtectures\n",
    "        self.pos_emb_dim = myconfig.pos_emb_dim\n",
    "        self.dep_emb_dim = myconfig.dep_emb_dim\n",
    "        self.hidden_dim = myconfig.hidden_dim\n",
    "        \n",
    "        # these 4 parameters are not changable\n",
    "        self.num_dep_type = myconfig.num_dep_type\n",
    "        self.max_num_syllables = myconfig.max_num_syllables\n",
    "        self.max_sentence_num = myconfig.max_sentence_num\n",
    "        self.num_freq_type = myconfig.num_freq_type\n",
    "\n",
    "        self.num_layers = myconfig.num_layers\n",
    "        self.heads = myconfig.heads\n",
    "        self.num_hierarchy = myconfig.num_hierarchy\n",
    "\n",
    "        self.add_self_loops = myconfig.add_self_loops\n",
    "        self.add_syllables = myconfig.add_syllables\n",
    "        self.add_word_freq = myconfig.add_word_freq\n",
    "        self.add_dep = myconfig.add_dep\n",
    "        self.add_sentence_order = myconfig.add_sentence_order\n",
    "\n",
    "        # model misc\n",
    "        self.max_length = myconfig.max_length # this is for pos tokenizer\n",
    "        self.dropout = myconfig.dropout\n",
    "\n",
    "        # pretrained checkpoints\n",
    "        self.pos_checkpoint = myconfig.pos_checkpoint\n",
    "        # self.bert_checkpoint = myconfig.bert_checkpoint\n",
    "\n",
    "        # loading pretrained models\n",
    "        self.pos_tokenizer = AutoTokenizer.from_pretrained(self.pos_checkpoint, local_files_only=True)\n",
    "        self.pos_bert = BertModel.from_pretrained(self.pos_checkpoint, local_files_only=True, add_pooling_layer = False).to(device)\n",
    "        \n",
    "        # embedding layers\n",
    "        if self.add_syllables:\n",
    "            # the longest word in the world has 17 syllables. However, if either processing error or people speak like that, such there is no space between words, error will arise.\n",
    "            self.syllable_emb_layer = nn.Embedding(self.max_num_syllables, self.pos_emb_dim)\n",
    "        if self.add_word_freq:\n",
    "            self.freq_emb_layer = nn.Embedding(self.num_freq_type, self.pos_emb_dim)\n",
    "        if self.add_dep:\n",
    "            self.dep_emb_layer = nn.Embedding(self.num_dep_type, self.dep_emb_dim)\n",
    "        \n",
    "        # gnns within sentences\n",
    "        self.gnns = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            if self.add_dep:\n",
    "                self.gnns.append(MyGNNBlock(self.pos_emb_dim, self.hidden_dim, heads = self.heads, add_self_loops=self.add_self_loops, dropout=self.dropout, edge_dim=self.dep_emb_dim))\n",
    "            else:\n",
    "                self.gnns.append(MyGNNBlock(self.pos_emb_dim, self.hidden_dim, heads = self.heads, add_self_loops=self.add_self_loops, dropout=self.dropout))\n",
    "\n",
    "        if self.num_hierarchy:\n",
    "            # for sentence order\n",
    "            # the longest text has 104 lines. how to deal with super long text?\n",
    "            if self.add_sentence_order:\n",
    "                self.sentence_position_emb_layer = nn.Embedding(self.max_sentence_num, 2*self.pos_emb_dim) \n",
    "            \n",
    "            # hierarchical layer\n",
    "            self.hierarchy_gnns = nn.ModuleList()\n",
    "            for i in range(self.num_hierarchy):\n",
    "                self.hierarchy_gnns.append(MyGNNBlock(2*self.pos_emb_dim, 2*self.hidden_dim, heads = self.heads, add_self_loops=self.add_self_loops, dropout=self.dropout))\n",
    "            \n",
    "            self.classifier = nn.Linear(4*self.pos_emb_dim, self.num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(2*self.pos_emb_dim, self.num_classes)\n",
    "            \n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, pos, edge_index, edge_type_ids, batch, ptr, y, segment_ids, num_syllable, word_freq):\n",
    "        # get pos embeddings, reshape and squeeze the dimension 0 to match pyg batching fashion\n",
    "        # x.shape = (sum of #sentence, max_length, pos_emb_dim)\n",
    "        tokens = self.pos_tokenizer(pos, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt').to(device)\n",
    "        x = self.pos_bert(**tokens).last_hidden_state\n",
    "        # reshape! drop padded tokens!\n",
    "        # x.shape = (sum of #token, pos_emb_dim)\n",
    "        x = x.masked_select(tokens.attention_mask.ge(0.5).unsqueeze(2)).reshape((-1,self.pos_emb_dim))\n",
    "        \n",
    "        # add syllables embedding to pos embeddings\n",
    "        if self.add_syllables:\n",
    "            x = x + self.syllable_emb_layer(torch.clip(num_syllable, max=self.max_num_syllables-1)) # clip to make sure no error\n",
    "\n",
    "        # add freq embedding to pos embeddings\n",
    "        if self.add_word_freq:\n",
    "            x = x + self.freq_emb_layer(word_freq)\n",
    "\n",
    "        # get edge embeddings\n",
    "        if self.add_dep:\n",
    "            edge_attr = self.dep_emb_layer(edge_type_ids)\n",
    "\n",
    "        # graph conv\n",
    "        for i in range(self.num_layers):\n",
    "            x = x + self.gnns[i](x, edge_index, edge_attr=edge_attr).relu() if self.add_dep else self.gnns[i](x, edge_index).relu()\n",
    "        \n",
    "        if self.num_hierarchy:\n",
    "            # readout to get sentence embeddings\n",
    "            # x.shape = (#sentence, pos_emb_dim*2)\n",
    "            non_zero_i, non_zero_j = tokens.attention_mask.nonzero(as_tuple=True)\n",
    "            # the input batch is segment level batch indices. Need sentence level batch indices here\n",
    "            sent_batch = (((torch.arange(len(text)).to(device)+1).unsqueeze(1)*tokens.attention_mask)[non_zero_i, non_zero_j] - 1)\n",
    "            x = torch.cat([global_mean_pool(x, sent_batch), global_max_pool(x, sent_batch)], axis=1)\n",
    "\n",
    "            # calculate edge_index between sentences from the same paragraph\n",
    "            edges_among_sentences = torch.LongTensor().to(device)\n",
    "            if self.add_sentence_order: \n",
    "                sentence_id = torch.LongTensor()\n",
    "            for i in range(segment_ids.max().item()+1):\n",
    "                idx = (segment_ids==i).nonzero().long().squeeze(1)  # select all sentence id belong to current segment\n",
    "                edge_x, edge_y = torch.meshgrid(idx, idx)\n",
    "                edge = torch.vstack([edge_x.flatten(), edge_y.flatten()])\n",
    "                edges_among_sentences = torch.cat([edges_among_sentences, edge], axis = 1)\n",
    "                if self.add_sentence_order: \n",
    "                    sentence_id = torch.cat([sentence_id, torch.arange(len(idx), dtype=torch.long)])\n",
    "            \n",
    "            # add sentence position\n",
    "            if self.add_sentence_order:\n",
    "                sentence_id = sentence_id.to(device)\n",
    "                x = x + self.sentence_position_emb_layer(torch.clip(sentence_id, max=self.max_sentence_num-1))\n",
    "\n",
    "            # hierarchical layers\n",
    "            for i in range(self.num_hierarchy):\n",
    "                x = + self.hierarchy_gnns[i](x, edges_among_sentences).relu()\n",
    "\n",
    "            # readout to get segment/doc embeddings\n",
    "            # x.shape = (#segment/#doc, pos_emb_dim*4)\n",
    "            x = torch.cat([global_mean_pool(x, segment_ids), global_max_pool(x, segment_ids)], axis=1)\n",
    "\n",
    "        else: \n",
    "            # readout to get segment/doc embeddings\n",
    "            # x.shape = (#segment/#doc, pos_emb_dim*2)\n",
    "            x = torch.cat([global_mean_pool(x, batch), global_max_pool(x, batch)], axis=1)\n",
    "\n",
    "        # prepare logits and output\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        logit = self.classifier(x)\n",
    "        loss = self.lossfn(logit, y)\n",
    "        return myGNNoutput(loss=loss, logit=logit, emb=x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c385e825",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470911fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs(dataset, num_classes, exclude_keys=['repeat'], **kwargs):\n",
    "    '''\n",
    "    If want to try different settings, give a list. Otherwise, just a number/str.\n",
    "    '''\n",
    "    keys = []\n",
    "    values = []\n",
    "    direct_kwargs = {}\n",
    "    for k,v in kwargs.items():\n",
    "        if k.lower() not in exclude_keys:\n",
    "            assert k.lower() in MyConfig.__dict__['__annotations__'], f\"{k} doesn't match any MyConfig option\"\n",
    "        if isinstance(v, list):\n",
    "            keys.append(k)\n",
    "            values.append(v)\n",
    "        else:\n",
    "            direct_kwargs[k]=v\n",
    "\n",
    "    CONFIGS = itertools.product(*values)\n",
    "    config_lists = []\n",
    "    for raw_config in CONFIGS:\n",
    "        myconfig = MyConfig(dataset=dataset, num_classes=num_classes, **direct_kwargs)\n",
    "        for k,v in zip(keys, raw_config):\n",
    "            if k.lower() not in exclude_keys:\n",
    "                myconfig.__dict__[k.lower()] = v\n",
    "        myconfig.__post_init__()\n",
    "        config_lists.append(myconfig)\n",
    "    return config_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe50c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b70f6de282496885fc141ade897876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/data_jz17d/result/pos_mlm_corenlp/retrained_all_pos_mlm_22/checkpoint-95000/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcpuyyp\u001b[0m (\u001b[33mfsu-dsc-cil\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/data_jz17d/wandb_tmp/wandb/run-20230314_151918-3tv8dip6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fsu-dsc-cil/POS%20GNN%20ccat50/runs/3tv8dip6\" target=\"_blank\">run_0</a></strong> to <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20GNN%20ccat50\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20GNN%20ccat50\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20GNN%20ccat50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/fsu-dsc-cil/POS%20GNN%20ccat50/runs/3tv8dip6\" target=\"_blank\">https://wandb.ai/fsu-dsc-cil/POS%20GNN%20ccat50/runs/3tv8dip6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2493ee6ec6d4461b80d64afa48d28f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m     metric\u001b[39m.\u001b[39madd_batch(predictions\u001b[39m=\u001b[39mpred, references\u001b[39m=\u001b[39mbatch\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     80\u001b[0m     doc_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvectorize(test_docid2index\u001b[39m.\u001b[39mget)(batch\u001b[39m.\u001b[39mdoc_id\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()) \n\u001b[0;32m---> 81\u001b[0m     doc_score[doc_id,pred] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[39m# logging current\u001b[39;00m\n\u001b[1;32m     84\u001b[0m evaluation \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mcompute()\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "dataset='ccat50'\n",
    "scratch_data_dir = '/scratch/data_jz17d/data'\n",
    "dataset_dir = f'{scratch_data_dir}/{dataset}'\n",
    "\n",
    "model_name = 'POS GNN'\n",
    "\n",
    "config_lists = get_configs(dataset='ccat50', \n",
    "                           num_classes=50,\n",
    "                           segment_length=[2, 3, 4, 'doc'],\n",
    "                           num_hierarchy=[0, 1, 2],\n",
    "                           add_sentence_order=[True, False],\n",
    "                           batch_size = 64,\n",
    "                           )\n",
    "\n",
    "skip_runs = -1\n",
    "######################## in most cases, no need to edit the section below ##########################\n",
    "run_pbar = trange(len(config_lists), leave=False)\n",
    "for i_run, myconfig in enumerate(config_lists):\n",
    "\n",
    "    if i_run <= skip_runs:\n",
    "        run_pbar.update(1)\n",
    "        continue\n",
    "    \n",
    "    seed = int(datetime.now().timestamp())\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # load necessary files and dataset\n",
    "    doc_true = np.load(f'{dataset_dir}/doc_true.npy')\n",
    "    with open(f'{dataset_dir}/test_docid2index.json') as f:\n",
    "        test_docid2index = json.load(f, object_hook=keystoint)\n",
    "    \n",
    "    train_loader = get_seg_loader(dataset=dataset, segment_length=myconfig.segment_length, split='train', batch_size=myconfig.batch_size, shuffle=True, max_length=myconfig.max_length)\n",
    "    num_training_steps = len(train_loader)\n",
    "    test_loader = get_seg_loader(dataset=dataset, segment_length=myconfig.segment_length, split='test', batch_size=myconfig.batch_size, shuffle=True, max_length=myconfig.max_length)\n",
    "    # num_test_steps = len(test_loader)\n",
    "    \n",
    "    # initialize model, optimizere, and lr scheduler\n",
    "    model = HierGNN(myconfig)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=myconfig.lr)\n",
    "    scheduler = get_scheduler(\"linear\",\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=int(myconfig.warmup_ratio*myconfig.epochs*num_training_steps),\n",
    "                            num_training_steps=myconfig.epochs*num_training_steps)\n",
    "    # start sync to wandb\n",
    "    wconfig = {}\n",
    "    wconfig['seed'] = seed\n",
    "    wconfig.update(myconfig.__dict__)\n",
    "    run = wandb.init(project=f\"{model_name} {dataset}\", \n",
    "                     entity=\"fsu-dsc-cil\", \n",
    "                     dir='/scratch/data_jz17d/wandb_tmp/', \n",
    "                     config=wconfig,\n",
    "                     name=f'run_{i_run}',\n",
    "                     reinit=True,\n",
    "                     settings=wandb.Settings(start_method='thread'))\n",
    "    \n",
    "    best_evaluation = collections.defaultdict(float)\n",
    "    pbar = trange(myconfig.epochs*num_training_steps, leave=False)\n",
    "    for i_epoch in range(myconfig.epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch.pos, batch.edge_index, batch.edge_type_ids, batch.batch, batch.ptr, batch.y, batch.segment_ids, batch.num_syllables, batch.word_freqs)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            pbar.update(1)\n",
    "\n",
    "        # evaluate on test set\n",
    "        model.eval()\n",
    "        doc_score = 1e-8*np.ones((len(test_docid2index), myconfig.num_classes))\n",
    "        metric = evaluate.load('/home/jz17d/Desktop/metrics/accuracy')\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "            output = model(batch.pos, batch.edge_index, batch.edge_type_ids, batch.batch, batch.ptr, batch.y, batch.segment_ids, batch.num_syllables, batch.word_freqs)\n",
    "            pred = output.logit.argmax(axis=-1).cpu().detach().numpy()\n",
    "            metric.add_batch(predictions=pred, references=batch.y.cpu().numpy())\n",
    "            doc_id = np.vectorize(test_docid2index.get)(batch.doc_id.cpu().detach().numpy()) \n",
    "            doc_score[doc_id,pred] += 1\n",
    "        \n",
    "        # logging current\n",
    "        evaluation = metric.compute()\n",
    "        for k in range(1, 6):\n",
    "            evaluation.update({f'test_doc_acc@{k}': top_k_accuracy_score(doc_true, doc_score, k=k)})\n",
    "        wandb.log(evaluation, step=pbar.n)\n",
    "        \n",
    "        # logging best\n",
    "        for key in evaluation:\n",
    "            best_evaluation[f'best_{key}'] = max(best_evaluation[f'best_{key}'], evaluation[key])\n",
    "        wandb.log(best_evaluation, step=pbar.n)\n",
    "    \n",
    "    run.finish()\n",
    "    run_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase needed\n",
    "\n",
    "# with open(f'{dataset_dir}/test_docid2index.json') as f:\n",
    "#     test_docid2index = json.load(f, object_hook=keystoint)\n",
    "# test_docid2index = {v:k for k,v in test_docid2index.items()}\n",
    "# with open(f'{dataset_dir}/test_docid2index.json', 'w') as f:\n",
    "#     json.dump(test_docid2index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "eff459b455d2c0757a4e89f8cb2c37a927eff263cb260980ebfa13f3344fc7e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
